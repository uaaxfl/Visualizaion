2009.iwslt-evaluation.8,P06-1121,0,0.0208436,"ligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four different systems are listed below: 1. Silenus, a linguistically syntax-based system that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b))"
2009.iwslt-evaluation.8,N03-1017,0,0.00351692,"that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b)) by pattern-matching. Finally, Silenus searches for the best derivation on the translation forest and outputs the target string. Beside the features we computed in rule extraction procedure, the additional features used in decoding step are listed here: • The number of rules in the derivation; Figure 1: Forest-based Rule Extraction and Translation • T"
2009.iwslt-evaluation.8,P08-1023,1,0.889278,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,D08-1022,1,0.876308,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,P06-1077,1,0.841838,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P07-1089,1,0.882882,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P89-1018,0,0.0587337,"ection 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four diff"
2009.iwslt-evaluation.8,W05-1506,0,0.153435,"d-side of r, while the root(lhs(r) denotes the root node of the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM"
2009.iwslt-evaluation.8,J07-2003,0,0.10516,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,P07-1019,0,0.0142273,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,J97-3002,0,0.00823424,"uting the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. For more details, please refer to [1] and [2]. Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Bruin Bruin is a formally syntax-based SMT system, which implements the maximum entropy based reordering model on BTG [11] rules. This model considers the reorder as a problem of classification, where the Maximum Entropy model is introduced. To complete the decoding procedure, three BTG rules are used to derive the translation: [] A → (A1 , A2 ) hi (7) A → (A1 , A2 ) (8) A → (x, y) (9) The lexical rule (3) is used to translate source phrase y into target phrase x and generate a block A. The merging rules (1) and (2) are used to merge two consecutive blocks into a single larger block in the straight or inverted order. Three essential elements must be illustrated in Bruin. The first one is a stochastic BTG, whose r"
2009.iwslt-evaluation.8,P03-1021,0,0.0105949,"nguage model score of the two blocks according to their final order, λLM is its weight. For the lexical rule, applying it is assigned a probability P rl (A): P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|x|)λ6 LM ·pλLM (x) (11) where p(·) are the phrase translation probabilities in both directions, plex (·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. The feature weights λs are tuned to maximize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchica"
2009.iwslt-evaluation.8,P06-1066,1,0.846081,"imize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchical phrase-based model [9]. This model can be formalized as a synchronous contextfree grammar, which is automatically acquired from wordaligned parallel data without any syntactic information. X →&lt; γ, α, ∼&gt; (13) Where X is a non-terminal, γ, α are strings of terminals and non-terminals, and ∼ is one-to-one correspondence between the non-terminal in γ, α. Our work faithfully followed Chiang’s [9] work. The only exception is the condition for terminating cube pruning. Chiang’s [9] implementation quits upon considering t"
2009.iwslt-evaluation.8,I05-1007,1,0.670097,"T07 dev IWSLT08 dev selection 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set fo"
2009.iwslt-evaluation.8,P05-1022,0,0.209137,"on 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is"
2009.iwslt-evaluation.8,P08-1067,0,0.0326858,"ric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is selected automatically from all the development sentences according to the n-gram similarity, which is calculated against the current test set sentences. Our method works as follows: Fi"
2020.acl-main.269,D15-1075,0,0.0207773,"h the probability 1 − π. = sigmoid((Es + G0 − G00 )/τ ) NLP Benchmarks (5) where G0 and G00 are two independent Gumbel noises (Gumbel, 1954), and τ ∈ (0, ∞) is a temperature parameter. As τ diminishes to zero, a sample from the Gumbel-Sigmoid distribution becomes cold and resembles the one-hot samples. At training time, we can use Gumbel-Sigmoid to obtain Experimental Setup Natural Language Inference aims to classify semantic relationship between a pair of sentences, i.e., a premise and corresponding hypothesis. We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which has three classes: Entailment, Contradiction and Neutral. We followed Shen et al. (2018b) to use a token2token SAN layer followed by a source2token SAN layer to generate a compressed vector representation of input sentence. The selector is integrated into the token2token SAN layer. Taking the premise representation sp and the hypothesis vector sh as input, their semantic relationship is represented by the concatenation of sp , sh , sp −sh and sp · sh , which is passed to a classification module to generate a categorical distribution over the three classes. We initialize the word embedd"
2020.acl-main.269,P18-1008,0,0.115114,"word order encoding (Yang et al., 2019a) and syntactic structure modeling (Tang et al., 2018). In this work, we concentrate on these two commonly-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representa"
2020.acl-main.269,P18-1198,0,0.0224993,"Missing"
2020.acl-main.269,N19-1423,0,0.0125418,"word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) p"
2020.acl-main.269,C16-1276,1,0.893209,"Missing"
2020.acl-main.269,D19-1082,1,0.814363,"Missing"
2020.acl-main.269,D19-1135,1,0.882131,"urther refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Metho"
2020.acl-main.269,N19-1122,1,0.877134,"Missing"
2020.acl-main.269,D19-1088,1,0.908786,"Missing"
2020.acl-main.269,D18-1317,1,0.881654,"are pretrained on Wikipedia and Gigaword, to initialize our networks, but they are not fixed during training. We choose the better feed-forward networks (FFN) variants of DEEPATT as our standard settings. Machine Translation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datase"
2020.acl-main.269,N19-1359,1,0.870621,"Missing"
2020.acl-main.269,D15-1166,0,0.0438377,"uistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Methodology Self-Attention Networks SANs (Lin et al., 2017), as a variant of attention model (Bahdanau et al., 2015; Luong et al., 2015), compute attention weights between each pair of elements in a single sequence. Given the input layer H = {h1 , · · · , hN } ∈ N ×d , SANs first transform the layer H into the queries Q ∈ N ×d , the keys K ∈ N ×d , and the values V ∈ N ×d with three separate weight matrices. The output layer O is calculated as: R R O = ATT(Q, K)V R R (1) where the alternatives to ATT(·) can be additive attention (Bahdanau et al., 2015) or dot-product attention (Luong et al., 2015). Due to time and space efficiency, we used the dot-product attention in this study, which is computed as: QKT ATT(Q, K) = sof tmax("
2020.acl-main.269,D18-1458,0,0.0466347,"Missing"
2020.acl-main.269,W18-5444,0,0.0310899,"Missing"
2020.acl-main.269,D14-1162,0,0.0828181,"Missing"
2020.acl-main.269,D19-1145,1,0.865883,"Missing"
2020.acl-main.269,N18-1202,0,0.0976303,"Missing"
2020.acl-main.269,D19-1098,0,0.0165419,"y-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (W"
2020.acl-main.269,P16-1162,0,0.016109,"lation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datasets, we followed He et al. (2018) to decrease the layer size to 256 and the number of attention heads to 4. For all the tasks, we applied the selector to the first layer of encoder to better capture lexical and syntactic infor"
2020.acl-main.269,N18-2074,0,0.0355278,"ion (i.e., sequence generation), demonstrate that SSANs consistently outperform the standard SANs (§3). Despite demonstrating the effectiveness of SSANs, the underlying reasons for their strong performance have not been well explained, which poses great challenges for further refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntact"
2020.acl-main.269,D18-1408,0,0.0225505,"9a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (Wu et al., 2018; Hao et al., 2019a; Wang et al., 2019b). These studies show that the introduction of syntactic information can achieve further improvement over SANs, demonstrating its potential weakness on structure modeling. 2.3 Selective Self-Attention Networks In this study, we implement the selective mechanism on SANs by introducing an additional selector, namely SSANs, as illustrated in Figure 1. The selector aims to select a subset of elements from the input sequence, on top of which the standard self-attention (Equation 1) is conducted. We implement the selector with Gumbel-Softmax, which has proven e"
2020.acl-main.269,D18-1475,1,0.809343,"guage representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018, 2019b). This poses a"
2020.acl-main.269,P19-1354,1,0.897977,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,N19-1407,1,0.818121,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,C18-1259,0,0.0489127,"Missing"
2020.acl-main.269,D18-1548,0,0.12373,"to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al.,"
2020.acl-main.269,W13-3516,0,\N,Missing
2020.acl-main.269,D16-1159,0,\N,Missing
2020.acl-main.278,N13-1073,0,0.179842,"Missing"
2020.acl-main.278,P16-1162,0,0.225361,"tion of translation errors. While TER only labels the mis-translation (“S”) and over-translation (“I”) errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label “D” from the ground-truth sequence to the generated sequence. 4 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 EnglishGerman (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the three language pairs. We used BLEU (Papineni et al., 2001) to evaluate the NMT models. We used the TER toolkit (Snover et al., 2006) to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer (Vaswani et al., 2017). We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with β1 = 0.9, β2"
2020.acl-main.278,2006.amta-papers.25,0,0.502853,"019) and Kumar and Sarawagi (2019) studied the calibration of NMT in the training setting, and found that NMT trained with label smoothing (Szegedy et al., 2016) is well-calibrated. We believe that this setting would cover up a central problem of NMT, the exposure bias (Ranzato et al., 2015) – the training-inference discrepancy caused by teacher forcing in the training of auto-regressive models. In response to this problem, this work focuses on the calibration of NMT in inference, which can better reflect the generative capacity of NMT models. To this end, we use translation error rate (TER) (Snover et al., 2006) to automatically annotate the correctness of generated tokens, which makes it feasible to evaluate calibration in infer3070 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070–3079 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ence. Experimental results on several datasets across language pairs show that even trained with label smoothing, NMT models still suffer from miscalibration errors in inference. Figure 1 shows an example. While modern neural networks on classification tasks have been found to be miscalibrated in the"
2020.acl-main.278,D15-1182,0,0.06029,"Missing"
2020.acl-main.278,2001.mtsummit-papers.68,0,0.0152736,"(“I”) errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label “D” from the ground-truth sequence to the generated sequence. 4 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 EnglishGerman (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the three language pairs. We used BLEU (Papineni et al., 2001) to evaluate the NMT models. We used the TER toolkit (Snover et al., 2006) to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer (Vaswani et al., 2017). We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98 and  = 10−9 . We used the same warmup strategy for learning rate as Vaswani et al. (2017"
2020.acl-main.278,P19-1176,0,0.132887,"bration. Szegedy et al. (2016) propose the label smoothing technique which can effectively reduce the calibration error. Ding et al. (2019) extend label smoothing to adaptive label regularization. Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures (Kuleshov and Liang, 2015). Nguyen and O’Connor (2015) verified the finding of NiculescuMizil and Caruana (2005) in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction (Ott et al., 2018; Wang et al., 2019), Kumar and Sarawagi (2019) studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated. M¨uller et al. (2019) investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models (Vaswani et al., 2017). 3 3.1 Definitions of Calibration Neural Machine Translation Training In machine translation task, an NMT model F : x → y maximizes the probabi"
2020.acl-main.278,D19-1073,1,0.935975,"bration. Szegedy et al. (2016) propose the label smoothing technique which can effectively reduce the calibration error. Ding et al. (2019) extend label smoothing to adaptive label regularization. Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures (Kuleshov and Liang, 2015). Nguyen and O’Connor (2015) verified the finding of NiculescuMizil and Caruana (2005) in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction (Ott et al., 2018; Wang et al., 2019), Kumar and Sarawagi (2019) studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated. M¨uller et al. (2019) investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models (Vaswani et al., 2017). 3 3.1 Definitions of Calibration Neural Machine Translation Training In machine translation task, an NMT model F : x → y maximizes the probabi"
2020.acl-main.278,D18-1396,0,0.0500661,"Missing"
2020.acl-main.278,P02-1040,0,\N,Missing
2020.coling-main.288,S17-2126,0,0.0277777,"d Emotion Network and BERT+EmNet for BERT-based Emotion Network. 3238 3 Experiments 3.1 Setup Dataset We use the English subset of Twitter dataset provided by SemEval 2018 (Mohammad et al., 2018). The dataset contains 11 emotions: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise and trust. The training data contains 6,838 tweets. The development and test sets have 886 and 3,259 tweets respectively. The data preprossing in the LSTM and BERT models are different. For LSTM models we preprocess the corpus following (Baziotis et al., 2018) where the ekphrasis2 (Baziotis et al., 2017) tool is used. The preprocessing steps included in ekphrasis are: Twitter-specific tokenization, spell correction, word normalization, word segmentation (for splitting hashtags) and word annotation.The BPE (Sennrich et al., 2015) is not applied and 800K unique words are collected. For BERT models, we just use the default preprocessing procedures in BERT including tokenization and BPE to preprocess the corpus. Evaluation Metrics We use the official competition metric provided by SemEval 2018 for comparison that is the multi-label accuracy (or Jaccard index) (Mohammad et al., 2018). Multi-label"
2020.coling-main.288,N19-1423,0,0.172993,"he sentence emotions based on both the encoded sentence representations and generated word emotions. With the newly introduced emotion generator, our EmNet can alleviate the domain mismatch and emotion ambiguity problems of using external lexicons. For example, the contextual words “how long”, “keeps diving and ducking” can help disambiguate the emotion of the word “joke”, thus improve the accuracy of emotion classification. We validate the proposed approach on the Twitter dataset of SemEval-2018 task (Mohammad et al., 2018) on top of both the LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) architectures. Our approach consistently outperforms the baseline models across model architectures, demonstrating the effectiveness and universality of the proposed approach. In addition, our model also outperforms the SOTA method of leveraging external emotion lexicon. Further analyses reveal that the proposed EmNet can learn reasonable emotion lexicons as expected, which avoids the mismatch problem of using external resource. Contributions. The main contributions of this paper are listed as follows: • We propose a novel emotional network for multi-label emotion classification which jointly"
2020.coling-main.288,P15-1101,0,0.0141223,"lassification task (e.g., positive, negative), emotion classification or affect detection is a multi-label classification task which is to detect a discrete set of emotions present in a given sentence such as anger, joy, sadness etc (Dalgleish and Power, 2000; Plutchik, 2001). Traditional methods such as lexicon, n-gram and graph models have been used. Xu et al. (2012) proposed a coarse-to-fine strategy for multi-label emotion classification. They dealt with the data sparseness problem by incorporating the transfer probabilities from the neighboring sentences to refine the emotion categories. Li et al. (2015) recast multi-label emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions. Yan and Turtle (2016) 3242 long -0.793 0.596 0.987 0.092 1.161 -1.899 0.951 0.015 0.787 -1.26 -1.395 0.014 long will -0.853 0.72 1.025 0.191 1.051 -1.871 0.927 0.018 0.747 -1.324 -1.374 0.011 will he -0.71 0.705 1.072 0.167 1.117 -1.855 0.809 0.034 0.727 -1.303 -1.503 0.013 he keeps -0.715 0.675 1.106 0.119 1.032 -1.927 0.737 0.098 0.873 -1.307 -1.437 0.015 keeps words anger anticipation disgust fear joy love optimism pessimism sadnes"
2020.coling-main.288,S18-1043,0,0.100652,"in previous studies and achieves new state-of-the-art on the benchmark Twitter dataset. 1 Introduction Tweet Emotion The last several years have seen a land rush in research on identification of emotions in short This is a joke really how long will disgust text such as Twitter or product reviews due to he keep diving and ducking. its greatly commercial value. For example, the That’s the joke. I know it’s incense. joy emotions (e.g., anger or joy) expressed in prodTable 1: Example sentences and their emotions. uct reviews can be a major factor in deciding the marketing strategy for a company (Meisheri and Dey, 2018). The SOTA approaches to this task (Baziotis et al., 2018; Meisheri and Dey, 2018) generally employ pre-defined emotion lexicons, which have two major limitations: 1. Most established emotion lexicons were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal short text. 2. The pre-defined lexicons suffer from the ambiguity problem: the emotion of a word is highly influenced by the context. Table 1 shows an example. The word “joke” carries different emotions according to different context. In this work, we tackle these challenges by"
2020.coling-main.288,S18-1001,0,0.372314,"motions, which dynamically adapt to the sentence context. 3. Emotion classifier classifies the sentence emotions based on both the encoded sentence representations and generated word emotions. With the newly introduced emotion generator, our EmNet can alleviate the domain mismatch and emotion ambiguity problems of using external lexicons. For example, the contextual words “how long”, “keeps diving and ducking” can help disambiguate the emotion of the word “joke”, thus improve the accuracy of emotion classification. We validate the proposed approach on the Twitter dataset of SemEval-2018 task (Mohammad et al., 2018) on top of both the LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) architectures. Our approach consistently outperforms the baseline models across model architectures, demonstrating the effectiveness and universality of the proposed approach. In addition, our model also outperforms the SOTA method of leveraging external emotion lexicon. Further analyses reveal that the proposed EmNet can learn reasonable emotion lexicons as expected, which avoids the mismatch problem of using external resource. Contributions. The main contributions of this paper are listed as follows: •"
2020.coling-main.288,D19-5541,0,0.0152637,"odel is a two-layer LSTM network where external emotion lexicons are used to provide word level affective knowledge. • TCS Research The Rank 2 method of SemEval-2018 Task 1 proposed by Meisheri and Dey (2018). The model uses two BiLSTM networks to encode tweets from different aspects. Then they concatenated the hidden states for the final classifications. • DATN-2 A transfer learning method proposed by Yu et al. (2018) for emotion classification in tweets. They used a shared-private architecture with the dual attention mechanism to encode tweets into features. • BERTbase +DK and BERTlarge +DK Ying et al. (2019) proposed to integrate domain knowledge into BERT for emotion classification. We compare with both their BERTbase and BERTlarge models. 2 https://competitions.codalab.org/competitions/17751 3239 ID 1 2 3 4 5 6 7 8 9 Method NTUA-SLP (Baziotis et al., 2018) TCS Research (Meisheri and Dey, 2018) DATN-2 (Yu et al., 2018) BERTbase +DK (Ying et al., 2019) BERTlarge +DK (Ying et al., 2019) Bi-LSTM Baseline 6 + EmNet BERTbase Baseline 8 + EmNet Accuracy 58.8 58.2 58.3 59.1 59.5 56.6 59.0† 58.0 59.6† F1-micro 70.1 69.3 71.3 71.6 68.3 70.1† 70.1 71.6† F1-macro 52.8 53.0 54.4 54.9 56.3 49.2 55.5† 53.0 56"
2020.coling-main.288,D18-1137,0,0.0699925,"aseline, we remove the emotion generator in Figure 1 and use the [CLS] embedding for classification. • NTUA-SLP The Rank 1 method of SemEval-2018 Task 1 proposed by Baziotis et al. (2018). The model is a two-layer LSTM network where external emotion lexicons are used to provide word level affective knowledge. • TCS Research The Rank 2 method of SemEval-2018 Task 1 proposed by Meisheri and Dey (2018). The model uses two BiLSTM networks to encode tweets from different aspects. Then they concatenated the hidden states for the final classifications. • DATN-2 A transfer learning method proposed by Yu et al. (2018) for emotion classification in tweets. They used a shared-private architecture with the dual attention mechanism to encode tweets into features. • BERTbase +DK and BERTlarge +DK Ying et al. (2019) proposed to integrate domain knowledge into BERT for emotion classification. We compare with both their BERTbase and BERTlarge models. 2 https://competitions.codalab.org/competitions/17751 3239 ID 1 2 3 4 5 6 7 8 9 Method NTUA-SLP (Baziotis et al., 2018) TCS Research (Meisheri and Dey, 2018) DATN-2 (Yu et al., 2018) BERTbase +DK (Ying et al., 2019) BERTlarge +DK (Ying et al., 2019) Bi-LSTM Baseline 6"
2020.coling-main.288,D16-1061,1,0.821774,"sgust fear the joy joke love . i know it &apos; s incense optimism pessimism sadness surprise trust (b) Figure 3: Visualization of attention weights and word emotions for tweets in Table 1. (a) the case study of T1 in Table 1. (b) the case study of T2 in Table 1. The color in deep means more weights. We highlight the words with larger attention weights. built a separate binary classifier for each emotion category to detect if an emotion category were present or absent in a tweet with traditional unigram features. The neural network models have also been used in emotion classification. For example, Zhou et al. (2016) proposed an emotion distribution learning (EDL) method, which first used recursive autoencoders (RAEs) to extract features and then conducted multi-label emotion classification by incorporating the label relations into the cost function. He and Xia (2018) provided an end-to-end learning framework by integrating representation learning and multi-label classification in one neural network. Recently, external knowledge has been widely employed for this task. One representative research line is the transfer learning. Yu et al. (2018) proposed a new transfer learning architecture to divide the sen"
2020.coling-main.394,W05-0909,0,0.04992,"context and fine-grained emotional context as system inputs. The target outputs are a coarse-grained emotion label and the listener’s response. For our model, we reserve the next utterance of the target response as user feedback in the training procedure. Finally, we obtain 20,724 dialogues in the training set, 2,972 in the validation set, and 2,713 in the testing set. 5.2 Evaluation Methods Automatic Evaluation. Liu et al. (2016) have verified BLEU might be improper to evaluate the conversation generation problem, as it correlates weakly with human judgements of the response quality; METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) have the same problem. Therefore, following previous emotion-related studies (Zhou et al., 2018; Rashkin et al., 2019; Song et al., 2019; Wei et al., 2019), we employ three evaluation metrics to automatically evaluate the performance of our EmpDG: Perplexity (Serban et al., 2015) measures the high-level general quality of the generation model; Distinct-1 and Distinct-2 (Li et al., 2016a) measure the proportion of the distinct unigrams / bigrams in all the generated results to indicate diversity. To evaluate the model at the emotional level, we adopt Emotion Accuracy as t"
2020.coling-main.394,N19-1374,0,0.045033,"a listener responds to a speaker who is under an emotional situation in an empathetic way. Furthermore, several emotion 4455 lexicons (Mohammad and Turney, 2013; Sedoc et al., 2020) have also been shown to be effective in tracking emotions in texts. In our work, we focus on the task of empathetic dialogue generation on the E MPATHETIC D IALOGUES dataset and emotion lexicon (Mohammad and Turney, 2013). The second line of our related work is the emotional dialogue generation, which has received an increasing amount of attention to address emotion factors (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019). Prior studies on emotion-related conversational systems mainly focused on rule-based systems, which heavily rely on hand-crafted features (Prendinger and Ishizuka, 2005). Recently, many neural emotional dialogue generation approaches (Ghosh et al., 2017; Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019) have been explored to control the emotional expression in the target response. However, as Li et al. (2018) reveal that conventional emotional conversation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, whic"
2020.coling-main.394,D19-1015,0,0.0194639,"is important to make the chatbot become empathetic within dialogue interactions (Prendinger and Ishizuka, 2005). Therefore, in this paper, we focus on the task of empathetic dialogue generation (Rashkin et al., 2019), which automatically tracks and understands the user emotion information in a multi-turn dialogue scenario. Despite the achieved successes (Rashkin et al., 2019; Lin et al., 2019), obstacles to establishing an empathetic conversational system are still far beyond current progress: (1) It is still difficult to accurately capture the nuances of human emotion in dialogue generation (Ghosal et al., 2019). (2) Merely relying on the dialogue history but overlooking the potential of user feedback for the generated responses further aggravates the aforementioned deficiencies, which causes undesirable responses (Zhang et al., 2018a). In Figure 1, we give an example of the benchmark dataset E MPATHETIC D IALOGUES (Rashkin et al., 2019). Note that the emotional words in the sequence of utterances (in Figure 1, they are “new”, “job” in utterance 1, “amazing”, “excited” in utterance 2, and “excited” in utterance 3) have fine-grained emotional connections. Without considering fine-grained emotional wor"
2020.coling-main.394,P17-1059,0,0.0276044,"work, we focus on the task of empathetic dialogue generation on the E MPATHETIC D IALOGUES dataset and emotion lexicon (Mohammad and Turney, 2013). The second line of our related work is the emotional dialogue generation, which has received an increasing amount of attention to address emotion factors (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019). Prior studies on emotion-related conversational systems mainly focused on rule-based systems, which heavily rely on hand-crafted features (Prendinger and Ishizuka, 2005). Recently, many neural emotional dialogue generation approaches (Ghosh et al., 2017; Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019) have been explored to control the emotional expression in the target response. However, as Li et al. (2018) reveal that conventional emotional conversation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et"
2020.coling-main.394,P19-1358,0,0.0596897,"Missing"
2020.coling-main.394,L18-1252,0,0.0538806,"Missing"
2020.coling-main.394,P14-1062,0,0.0951959,"o optimize the content and empathy ability of the empathetic generator, respectively. Both the semantic discriminator and the emotional one are built on the convolutional neural network (CNN) based classifier, so we detail the semantic discriminator first for convenience. 4458 Semantic Discriminator. First, we apply an LSTM encoder (Hochreiter and Schmidhuber, 1997) to respectively encode the generated response and gold response into hidden representations, i.e., dN t and P N P dt . We regard dt as negative vectors and dt as positive vectors. Thereafter, a two-dimensional convolutional layer (Kalchbrenner et al., 2014b) convolves the hidden vecor d∗t with multiple convolutional kernels of different widths, where ∗ ∈ {N, P }. Each kernel corresponds to a linguistic feature detector which extracts a specific pattern of multi-grained n-grams (Kalchbrenner et al., 2014b). A convolutional filter Ws maps hidden states in the receptive field to a single feature. As we slide the filter across the negative or positive sequence, a sequence of new features F∗ = [f1∗ , . . . , fk∗ ] is obtained: ft∗ = ReLU(d∗t ⊗ Ws + bs ), (16) where ReLU is activation function, ⊗ denotes the convolution operation, Ws ∈ Rd×k and bs ∈"
2020.coling-main.394,D18-1071,0,0.0343867,"Missing"
2020.coling-main.394,N16-1014,0,0.488339,"5). Recently, many neural emotional dialogue generation approaches (Ghosh et al., 2017; Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019) have been explored to control the emotional expression in the target response. However, as Li et al. (2018) reveal that conventional emotional conversation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dialogue models usually adopt the sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) fashion. Adversarial learning achieves considerable success in generating higher-quality responses (Goodfellow et al., 2014; Li et al., 2017a) but often leads to gradient vanishing as the discriminator saturates (Gulrajani et al., 2017). To tackle this problem, Gao et al. (2019) utilize Wasserstein GAN (Arjovsky et al., 2017) to enhance response consistency with external facts. Romanov et al. (2019) propose an adversa"
2020.coling-main.394,D16-1127,0,0.365097,"5). Recently, many neural emotional dialogue generation approaches (Ghosh et al., 2017; Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019) have been explored to control the emotional expression in the target response. However, as Li et al. (2018) reveal that conventional emotional conversation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dialogue models usually adopt the sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) fashion. Adversarial learning achieves considerable success in generating higher-quality responses (Goodfellow et al., 2014; Li et al., 2017a) but often leads to gradient vanishing as the discriminator saturates (Gulrajani et al., 2017). To tackle this problem, Gao et al. (2019) utilize Wasserstein GAN (Arjovsky et al., 2017) to enhance response consistency with external facts. Romanov et al. (2019) propose an adversa"
2020.coling-main.394,D17-1230,0,0.199821,"sation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dialogue models usually adopt the sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) fashion. Adversarial learning achieves considerable success in generating higher-quality responses (Goodfellow et al., 2014; Li et al., 2017a) but often leads to gradient vanishing as the discriminator saturates (Gulrajani et al., 2017). To tackle this problem, Gao et al. (2019) utilize Wasserstein GAN (Arjovsky et al., 2017) to enhance response consistency with external facts. Romanov et al. (2019) propose an adversarial decomposition method for fine-grained text representation. Unlike the previous work, we investigate an adversarial approach to improve the empathy quality of neural dialogue models. 3 Problem Formulation Before detailing our method, we introduce our key notations and concepts. A multi-turn dialogue context consis"
2020.coling-main.394,I17-1099,0,0.105048,"sation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dialogue models usually adopt the sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) fashion. Adversarial learning achieves considerable success in generating higher-quality responses (Goodfellow et al., 2014; Li et al., 2017a) but often leads to gradient vanishing as the discriminator saturates (Gulrajani et al., 2017). To tackle this problem, Gao et al. (2019) utilize Wasserstein GAN (Arjovsky et al., 2017) to enhance response consistency with external facts. Romanov et al. (2019) propose an adversarial decomposition method for fine-grained text representation. Unlike the previous work, we investigate an adversarial approach to improve the empathy quality of neural dialogue models. 3 Problem Formulation Before detailing our method, we introduce our key notations and concepts. A multi-turn dialogue context consis"
2020.coling-main.394,D19-1012,0,0.487152,"ucial step towards a more humanized humanmachine conversation, which improves the emotional perceptivity in emotion-bonding social activities (Zech and Rim´e, 2005). To design an intelligent automatic dialogue system, it is important to make the chatbot become empathetic within dialogue interactions (Prendinger and Ishizuka, 2005). Therefore, in this paper, we focus on the task of empathetic dialogue generation (Rashkin et al., 2019), which automatically tracks and understands the user emotion information in a multi-turn dialogue scenario. Despite the achieved successes (Rashkin et al., 2019; Lin et al., 2019), obstacles to establishing an empathetic conversational system are still far beyond current progress: (1) It is still difficult to accurately capture the nuances of human emotion in dialogue generation (Ghosal et al., 2019). (2) Merely relying on the dialogue history but overlooking the potential of user feedback for the generated responses further aggravates the aforementioned deficiencies, which causes undesirable responses (Zhang et al., 2018a). In Figure 1, we give an example of the benchmark dataset E MPATHETIC D IALOGUES (Rashkin et al., 2019). Note that the emotional words in the seque"
2020.coling-main.394,D16-1230,0,0.0497306,". In order to supplement the language gap between the training data and NRC, all adjectives not included in NRC are extracted together with NRC emotional words. We treat the dialogue context and fine-grained emotional context as system inputs. The target outputs are a coarse-grained emotion label and the listener’s response. For our model, we reserve the next utterance of the target response as user feedback in the training procedure. Finally, we obtain 20,724 dialogues in the training set, 2,972 in the validation set, and 2,713 in the testing set. 5.2 Evaluation Methods Automatic Evaluation. Liu et al. (2016) have verified BLEU might be improper to evaluate the conversation generation problem, as it correlates weakly with human judgements of the response quality; METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) have the same problem. Therefore, following previous emotion-related studies (Zhou et al., 2018; Rashkin et al., 2019; Song et al., 2019; Wei et al., 2019), we employ three evaluation metrics to automatically evaluate the performance of our EmpDG: Perplexity (Serban et al., 2015) measures the high-level general quality of the generation model; Distinct-1 and Distinct-2 (Li et al., 20"
2020.coling-main.394,D14-1162,0,0.0916257,"utomatic metrics, while the next three metrics are human metrics. Bold face indicates leading results in terms of the corresponding metric. Models Accuracy Perplexity ↓ Distinct-1 Distinct-2 EmpDG w/o G w/o D 0.3431 0.3281 0.3347 34.18 33.94 32.66 1.81 1.27 1.00 6.94 5.21 3.89 Table 2: Ablation test of different components. • w/o D: The EmpDG model without the interactive discriminators. 5.4 Implementation Details We implement all models using Pytorch (Paszke et al., 2017)1 and optimize the models using Adam (Kingma and Ba, 2015) with a mini-batch size of 16. We use pre-trained Glove vectors (Pennington et al., 2014) to initialize the word embedding. During the training of empathetic generator, the learning rate is initialled as 0.0001 and we vary the learning rate following Vaswani et al. (2017). Early stopping is applied when training. When inference, we set the maximum decoding step as 30. All common hyperparameters are the same as the work in (Lin et al., 2019). During the interactive adversarial training, D-steps (for two interactive discriminators) is set to 1 and G-steps (for empathetic generator) is set to 5. Hyper-parameter β in interactive discriminators is set to 0.1. Meanwhile, we employ the t"
2020.coling-main.394,P19-1534,0,0.292904,"ch significantly outperforms the state-of-the-art baselines in both content quality and emotion perceptivity. 1 Introduction Studies on social psychology suggest that “empathy” is a crucial step towards a more humanized humanmachine conversation, which improves the emotional perceptivity in emotion-bonding social activities (Zech and Rim´e, 2005). To design an intelligent automatic dialogue system, it is important to make the chatbot become empathetic within dialogue interactions (Prendinger and Ishizuka, 2005). Therefore, in this paper, we focus on the task of empathetic dialogue generation (Rashkin et al., 2019), which automatically tracks and understands the user emotion information in a multi-turn dialogue scenario. Despite the achieved successes (Rashkin et al., 2019; Lin et al., 2019), obstacles to establishing an empathetic conversational system are still far beyond current progress: (1) It is still difficult to accurately capture the nuances of human emotion in dialogue generation (Ghosal et al., 2019). (2) Merely relying on the dialogue history but overlooking the potential of user feedback for the generated responses further aggravates the aforementioned deficiencies, which causes undesirable"
2020.coling-main.394,N19-1088,0,0.0174873,"s (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dialogue models usually adopt the sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) fashion. Adversarial learning achieves considerable success in generating higher-quality responses (Goodfellow et al., 2014; Li et al., 2017a) but often leads to gradient vanishing as the discriminator saturates (Gulrajani et al., 2017). To tackle this problem, Gao et al. (2019) utilize Wasserstein GAN (Arjovsky et al., 2017) to enhance response consistency with external facts. Romanov et al. (2019) propose an adversarial decomposition method for fine-grained text representation. Unlike the previous work, we investigate an adversarial approach to improve the empathy quality of neural dialogue models. 3 Problem Formulation Before detailing our method, we introduce our key notations and concepts. A multi-turn dialogue context consists of M utterances between two interlocutors. We assume both semantic context and emotional context exist in such dialogue. The semantic context U refers to the sequence of utterances, i.e., U = [U1 , ..., UM ]. Following Lin (2019), we flat U into a long token"
2020.coling-main.394,2019.ccnlg-1.3,0,0.0307884,"Missing"
2020.coling-main.394,2020.lrec-1.206,0,0.34323,"Missing"
2020.coling-main.394,P19-1359,0,0.18016,"ve the next utterance of the target response as user feedback in the training procedure. Finally, we obtain 20,724 dialogues in the training set, 2,972 in the validation set, and 2,713 in the testing set. 5.2 Evaluation Methods Automatic Evaluation. Liu et al. (2016) have verified BLEU might be improper to evaluate the conversation generation problem, as it correlates weakly with human judgements of the response quality; METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) have the same problem. Therefore, following previous emotion-related studies (Zhou et al., 2018; Rashkin et al., 2019; Song et al., 2019; Wei et al., 2019), we employ three evaluation metrics to automatically evaluate the performance of our EmpDG: Perplexity (Serban et al., 2015) measures the high-level general quality of the generation model; Distinct-1 and Distinct-2 (Li et al., 2016a) measure the proportion of the distinct unigrams / bigrams in all the generated results to indicate diversity. To evaluate the model at the emotional level, we adopt Emotion Accuracy as the agreement between the ground truth emotion labels and the predicted emotion labels by the empathetic generator. Human Evaluation. To qualitatively examine m"
2020.coling-main.394,2020.acl-main.516,0,0.0201981,"l., 2017; Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019) have been explored to control the emotional expression in the target response. However, as Li et al. (2018) reveal that conventional emotional conversation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dialogue models usually adopt the sequence-to-sequence (Seq2Seq) (Sutskever et al., 2014) fashion. Adversarial learning achieves considerable success in generating higher-quality responses (Goodfellow et al., 2014; Li et al., 2017a) but often leads to gradient vanishing as the discriminator saturates (Gulrajani et al., 2017). To tackle this problem, Gao et al. (2019) utilize Wasserstein GAN (Arjovsky et al., 2017) to enhance response consistency with external facts. Romanov et al. (2019) propose an adversarial decomposition method for fine-grained text representation. Unlike the previo"
2020.coling-main.394,C18-1206,0,0.0412983,"Missing"
2020.coling-main.394,P18-1104,0,0.0402025,"e task of empathetic dialogue generation on the E MPATHETIC D IALOGUES dataset and emotion lexicon (Mohammad and Turney, 2013). The second line of our related work is the emotional dialogue generation, which has received an increasing amount of attention to address emotion factors (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019). Prior studies on emotion-related conversational systems mainly focused on rule-based systems, which heavily rely on hand-crafted features (Prendinger and Ishizuka, 2005). Recently, many neural emotional dialogue generation approaches (Ghosh et al., 2017; Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019) have been explored to control the emotional expression in the target response. However, as Li et al. (2018) reveal that conventional emotional conversation systems aim to produce more emotion-rich responses according to a given specific user-input emotion, which inevitably leads to an emotional inconsistency problem. Our research also aligns with recent advances in open-domain dialogue generation models (Vinyals and Le, 2015; Li et al., 2016b; Zhang et al., 2018b; Hancock et al., 2019; Li et al., 2020; Song et al., 2020). These dia"
2020.coling-main.529,P18-1008,0,0.0586543,"Missing"
2020.coling-main.529,P18-1167,0,0.0325142,"Missing"
2020.coling-main.529,2020.acl-main.269,1,0.875136,"Missing"
2020.coling-main.529,N19-1357,0,0.0616084,"Missing"
2020.coling-main.529,D18-1317,1,0.895028,"Missing"
2020.coling-main.529,P19-1124,0,0.0542325,"Missing"
2020.coling-main.529,P02-1040,0,0.108335,"contribution score measures the effect of fully ablating the component on model performance (i.e. hard metric), while the criticality score measures how much the component can be rewound while maintaining the model performance (i.e. soft metric). 3 Experiments Data and Setup We conducted experiments on the benchmarking WMT2014 English-German (EnDe) and English-French (En-Fr) translation datasets, which consist of 4.6M and 35.5M sentence pairs respectively. We employed BPE (Sennrich et al., 2016) with 32K merge operations for both language pairs, and used case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as our evaluation metric. Unless otherwise stated, the Transformer model consists of 6-layer encoder and decoder. The layer size is 512, the size of feed-forward sub-layer is 2048, and the number of attention heads is 8. We followed the settings in (Vaswani et al., 2017) to train the Transformer models on the En-De and En-Fr datasets. We set the dropout as 0.1 and the initialization seed as 1 for all Transformer models . 3.1 Observing Component Importance In this section, we first measure the component importance of trained Transformer models. Then we vary some settings, which are threats to"
2020.coling-main.529,W18-5431,0,0.0450532,"Missing"
2020.coling-main.529,P16-1162,0,0.0856563,"cs evaluate the component importance in terms of its effect on model performance, there are considerable differences. The contribution score measures the effect of fully ablating the component on model performance (i.e. hard metric), while the criticality score measures how much the component can be rewound while maintaining the model performance (i.e. soft metric). 3 Experiments Data and Setup We conducted experiments on the benchmarking WMT2014 English-German (EnDe) and English-French (En-Fr) translation datasets, which consist of 4.6M and 35.5M sentence pairs respectively. We employed BPE (Sennrich et al., 2016) with 32K merge operations for both language pairs, and used case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as our evaluation metric. Unless otherwise stated, the Transformer model consists of 6-layer encoder and decoder. The layer size is 512, the size of feed-forward sub-layer is 2048, and the number of attention heads is 8. We followed the settings in (Vaswani et al., 2017) to train the Transformer models on the En-De and En-Fr datasets. We set the dropout as 0.1 and the initialization seed as 1 for all Transformer models . 3.1 Observing Component Importance In this section,"
2020.coling-main.529,D19-1149,0,0.0364078,"Missing"
2020.coling-main.529,R19-1136,0,0.0420395,"Missing"
2020.coling-main.529,P19-1580,0,0.111927,"b) En-Fr: Contribution (c) En-De: Criticality (d) En-Fr: Criticality Seed 66 1 Figure 3: Importance of individualSeed components measured by (a, b) contribution in information flow, (c, d) criticality in representation generalization. Y-axis is the layer id and X-axis is the type of components. ”E”, ”D”, ”SA”, ”EA” and ”FF” represent Encoder, Decoder, Self-attention, Encoder-attention and Feedforward layer respectively. Darker cells denote more important components. • Higher encoder-attention (“D:EA”) layers in decoder are more important than lower encoderattention layers. This is the same in Voita et al. (2019) which claims that lower part of decoder is more like a language model. For the other components, the bottom and top layers are more important than the intermediate layer. Seed 66 Seed 99 1 difference between the results of two We noticeSeed the main metrics is on bottom feed-forward layers in decoder. The contribution score is high but criticality score is low. It is because the performance are bad when α = 0 and 1, but the performance are dramatically good when α ≥ 3. So the contribution is high but criticality is relatively low, according to the definition in Section 2. In the following exp"
2020.coling-main.529,P19-1624,1,0.842391,"e: • Initialization Seed: Recent works have shown that neural models are very sensitive to the initialization seeds: even with the same hyper-parameter values, distinct random seeds can lead to substantially different results (Dodge et al., 2020). 6022 • Model Capacity: Depth and width are two key aspects in the design of a neural network architecture. Lu et al. (2017) claimed that the depth of a network may determine the abstraction level, and the width may influence the loss of information in the forwarding pass. Recent studies have also demonstrated the significant effect of varying depth (Wang et al., 2019) and width (Vaswani et al., 2017) on NMT models. Figure 4 shows the results of Transformer models with different initialization seeds and model capacities on the En-De dataset. Specifically, we used two other different initialization seeds (i.e., “66” and “99”). For the model capacity setting, we used deeper Transformer (i.e., 12 layer) and wider Transformer (i.e., layer size be 1024). Clearly, the above conclusions hold in all cases, demonstrating the robustness of our findings. In the following experiments, we use Transformer-base with initialization seed 1 as the default model. Results on T"
2020.coling-main.529,P19-1354,1,0.894285,"Missing"
2020.emnlp-main.176,N19-4007,0,0.0735179,"Missing"
2020.emnlp-main.176,N19-4009,0,0.028415,"lely on attention mechanisms. Rejuvenation Model Inspired by recent successes on data augmentation for NMT, we adopt the widely-used backtranslation (Sennrich et al., 2016a) and forwardtranslation (Zhang and Zong, 2016) approaches to implement the rejuvenation model. After the active examples are distinguished from the training data, we use them to train an NMT model in Experiment • DYNAMIC C ONV (Wu et al., 2019) that is implemented with lightweight and dynamic convolutions, which can perform competitively to the best reported T RANSFORMER results. We adopted the open-source toolkit Fairseq (Ott et al., 2019) to implement the above NMT models. 2257 Probability Probability Distribution Distribution En-De: Transformer-Base En-De En-De En-FrEn-Fr 28 En-De: En-De: Transformer-Base Transformer-Base 0.2 0.2 En-Fr 27 Data Bin Data Bin Inactive Inactive Active Active Inactive Inactive Active Active Figure 2: Probability diagram on (a) En⇒De and (b) En⇒Fr datasets. Training examples in smaller bins (e.g., 1, 2) are regarded as inactive examples due to their lower probabilities. 1 2 3 4 5 6 7 8 9 10 Data Bin Data Bin 25 Inactive 25 Inactive Most Inactive Examples Examples Random Random Examples Examples Ran"
2020.emnlp-main.176,P18-1167,0,0.28978,"iant to specific NMT models and depends on the data distribution itself. We further propose data rejuvenation to rejuvenate the inactive examples to improve the performance of NMT models. Specifically, we train an NMT model on the active examples as the rejuvenation model to re-label the inactive examples, resulting in the rejuvenated examples (§3.2). The final NMT model is trained on the combination of the active examples and rejuvenated examples. Experimental results show that the data rejuvenation approach consistently and significantly improves performance on SOTA NMT models (e.g., L STM (Domhan, 2018), T RANSFORMER (Vaswani et al., 2017), and DYNAMIC C ONV (Wu et al., 2255 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2255–2266, c November 16–20, 2020. 2020 Association for Computational Linguistics 2019)) on the benchmark WMT14 English-German and English-French datasets (§4.4). Encouragingly, our approach is also complementary to existing data manipulation methods (e.g., data diversification (Nguyen et al., 2019) and data denoising (Wang et al., 2018)), and combining them can further improve performance. Finally, we conduct extensive analyses"
2020.emnlp-main.176,N13-1073,0,0.133504,"Missing"
2020.emnlp-main.176,kocmi-bojar-2017-curriculum,0,0.0354003,"Missing"
2020.emnlp-main.176,W04-3250,0,0.589545,"Missing"
2020.emnlp-main.176,W17-3204,0,0.0388685,"d examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability. 1 1 Introduction Neural machine translation (NMT) is a data-hungry approach, which requires a large amount of data to train a well-performing NMT model (Koehn and Knowles, 2017). However, the complex patterns and potential noises in the large-scale data ∗ Work was mainly done when Wenxiang Jiao and Shilin He were interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/Data-Rejuvenation make training NMT models difficult. To relieve this problem, several approaches have been proposed to better exploit the training data, such as curriculum learning (Platanios et al., 2019), data diversification (Nguyen et al., 2019), and data denoising (Wang et al., 2018). In this paper, we explore an interesting alternative which is to reactivate the"
2020.emnlp-main.176,W18-6301,0,0.020817,"the translation performance, and rejuvenating them leads to a further decrease of performance. In contrast, the proposed data rejuvenation improves performance as expected. These results provide empirical support for our claim that the improvement comes from the proposed data rejuvenation rather than forward translation. 4.4 Main Results Table 3 lists the results across model architectures and language pairs. Our T RANSFORMER models achieve better results than that reported in previous work (Vaswani et al., 2017), especially on the large-scale En⇒Fr dataset (e.g., more than 1.0 BLEU points). Ott et al. (2018) showed that models of larger capacity benefit from training with large batches. Analogous to DYNAMIC C ONV, we trained another T RANSFORMER -B IG model with 2260 Model T RANSFORMER -BASE + Data Rejuvenation + Data Diversification-BT + Data Rejuvenation + Data Diversification-FT + Data Rejuvenation + Data Denoising + Data Rejuvenation BLEU 27.5 28.3 26.9 27.9 28.1 28.5 28.1 28.6 4 – +0.8 3000 -0.6 2000 +0.4 +0.6 1000 +1.0 +0.60 +1.1 RankingRankingRanking 4000 4000 Coverage Coverage Coverage Uncertainty Uncertainty Uncertainty 4000 1.0 1.0 1.0 5.0 3000 3000 0.8 0.8 0.8 4.0 2000 2000 0.6 0.6 0.6"
2020.emnlp-main.176,P02-1040,0,0.107232,"on) side. Benefiting from the knowledge distillation based on active examples, the rejuvenated examples consist of simpler patterns than the original examples (Edunov et al., 2019), thus are more likely to be learned by NMT models. 4 4.1 Experimental Setup Data. We conducted experiments on the widely used WMT14 English⇒German (En⇒De) and English⇒French (En⇒Fr) datasets, which consist of about 4.5M and 35.5M sentence pairs, respectively. We applied BPE (Sennrich et al., 2016b) with 32K merge operations for both language pairs. The experimental results were reported in casesensitive BLEU score (Papineni et al., 2002). Model. We validated our approach on a couple of representative NMT architectures: • L STM (Domhan, 2018) that is implemented in the T RANSFORMER framework. • T RANSFORMER (Vaswani et al., 2017) that is based solely on attention mechanisms. Rejuvenation Model Inspired by recent successes on data augmentation for NMT, we adopt the widely-used backtranslation (Sennrich et al., 2016a) and forwardtranslation (Zhang and Zong, 2016) approaches to implement the rejuvenation model. After the active examples are distinguished from the training data, we use them to train an NMT model in Experiment • DY"
2020.emnlp-main.176,N19-1119,0,0.0609194,"Missing"
2020.emnlp-main.176,P16-1009,0,0.301762,"enate the inactive examples to improve the training of NMT models. 2 Related Work Data Manipulation. Our work is closely related to previous studies on manipulating training data for NMT models, which focuses on exploiting the original training data without augmenting additional data. For example, the data denoising approach (Wang et al., 2018) aims to identify and clean the noise training examples. Data diversification (Nguyen et al., 2019) tries to diversify the training data by applying forward-translation (Zhang and Zong, 2016) to the source side of the parallel data, or back-translation (Sennrich et al., 2016a) to the target side of parallel data in a reverse translation direction. Our approach is complementary to theirs, and using them together can further improve translation performance (Table 4). Another distantly related direction is to simplify the source sentences so that a black-box machine translation system can better translate them (Mehta et al., 2020), which is out of scope in this work. Distinguishing Training Examples. Our work is also related to previous work on distinguishing training examples in machine learning. One stream is to re-weight training examples with different choices o"
2020.emnlp-main.176,P16-1162,0,0.590137,"enate the inactive examples to improve the training of NMT models. 2 Related Work Data Manipulation. Our work is closely related to previous studies on manipulating training data for NMT models, which focuses on exploiting the original training data without augmenting additional data. For example, the data denoising approach (Wang et al., 2018) aims to identify and clean the noise training examples. Data diversification (Nguyen et al., 2019) tries to diversify the training data by applying forward-translation (Zhang and Zong, 2016) to the source side of the parallel data, or back-translation (Sennrich et al., 2016a) to the target side of parallel data in a reverse translation direction. Our approach is complementary to theirs, and using them together can further improve translation performance (Table 4). Another distantly related direction is to simplify the source sentences so that a black-box machine translation system can better translate them (Mehta et al., 2020), which is out of scope in this work. Distinguishing Training Examples. Our work is also related to previous work on distinguishing training examples in machine learning. One stream is to re-weight training examples with different choices o"
2020.emnlp-main.176,P16-1008,1,0.919616,"Missing"
2020.emnlp-main.176,D19-1570,0,0.026952,"Missing"
2020.emnlp-main.176,2020.acl-main.278,1,0.754425,"igure 1: The framework of data rejuvenation. The inactive examples from the original training data are identified by the identification model, then rejuvenated by the rejuvenation model. The rejuvenated examples along with the active examples are used together to train the NMT model. Best view in color. of the training data{[xn , yn ]}N n=1 : L(θ) = N X log P (yn |xn ). (1) n=1 The trained NMT model assigns a sentence-level probability P (y|x) to each sentence pair (x, y), indicating the confidence of the model to generate the target sentence y from the source one x (Kumar and Sarawagi, 2019; Wang et al., 2020). Intuitively, if a training example has a low sentence-level probability, it is less likely to provide useful information for improving model performance, and thus is regarded as an inactive example. Therefore, we adopt sentence-level probability P (y|x) as the metric to measure the activeness level of each training example: I(y|x) = T Y p(yt |x, y<t ), (2) t=1 where T is the number of target words in the training example. I(y|x) is normalized by the length of target sentence y to avoid length bias. We train an NMT model on the original training data and use it to score each training example."
2020.emnlp-main.176,2020.acl-main.41,0,0.0309809,". Concerning the training loss (Figure 7(a)), our approach converges faster and presents much less fluctuation than the baseline model during the whole training process. Correspondingly, the BLEU score on the validation set is significantly boosted (Figure 7(b)). These results suggest that data rejuvenation is able to accelerate and stabilize the training process. Generalization Capability In this section, we investigated how data rejuvenation affected the generalization capability of NMT models with two measures, namely, Margin (Bartlett et al., 2017) and Gradient Signal-toNoise Ratio (GSNR, Liu et al., 2020a). Table 5 lists the results, in which the GSNR values are at the same order of magnitude as that reported by Liu et al. (2020a). As seen, our approach achieves noticeably larger Margin and GSNR values, demonstrating that data rejuvenation improves the generalization capability of NMT models. 5.4 Speeding Up The pipeline of data rejuvenation in Figure 1 is time-consuming: training the identification and rejuvenation models in sequence as well as the scoring and rejuvenating procedures make the time cost of data rejuvenation more than 3X that of the standard NMT system. To save the time cost,"
2020.emnlp-main.176,P19-1123,0,0.0248496,"Missing"
2020.emnlp-main.176,W18-6314,0,0.0177199,"ion approach consistently and significantly improves performance on SOTA NMT models (e.g., L STM (Domhan, 2018), T RANSFORMER (Vaswani et al., 2017), and DYNAMIC C ONV (Wu et al., 2255 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2255–2266, c November 16–20, 2020. 2020 Association for Computational Linguistics 2019)) on the benchmark WMT14 English-German and English-French datasets (§4.4). Encouragingly, our approach is also complementary to existing data manipulation methods (e.g., data diversification (Nguyen et al., 2019) and data denoising (Wang et al., 2018)), and combining them can further improve performance. Finally, we conduct extensive analyses to better understand the inactive examples and the proposed data rejuvenation approach. Quantitative analyses reveal that the inactive examples are more difficult to learn than active ones, and rejuvenation can reduce the learning difficulty (§5.1). The rejuvenated examples stabilize and accelerate the training process of NMT models (§5.2), resulting in final models with better generalization capability (§5.3). Our contributions of this work are as follows: • Our study demonstrates the existence of in"
2020.emnlp-main.78,P18-1198,0,0.0573306,"Missing"
2020.emnlp-main.78,D18-1457,1,0.883746,"Missing"
2020.emnlp-main.78,D15-1166,0,0.0620799,"et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “# Para.” denotes the trainable parameter size of each model. “+” denotes appending new features to the above row. “↑/⇑” indicates statistical significance (p &lt; 0.05/0.01) over the baseline. the open-source toolkit – fairseq (Ott"
2020.emnlp-main.78,N19-4009,0,0.0341847,"015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “# Para.” denotes the trainable parameter size of each model. “+” denotes appending new features to the above row. “↑/⇑” indicates statistical significance (p &lt; 0.05/0.01) over the baseline. the open-source toolkit – fairseq (Ott et al., 2019). For Transformer, we investigate big, base and small settings. About RNNSearch and LightConv, we employ corresponding configurations in fairseq. The implementation is detailed in Appendix §A.1. All baseline models are trained for 100K updates using Adam optimizer (Kingma and Ba, 2015). Based on the baselines, the proposed pruning and rejuvenation methods are trained with additional 100K updates (i.e. 50K for each one). To rule out the circumstance that more training steps may bring improvements, we also conduct continuous training (ConTrain) as strong baselines and they employ the same traini"
2020.emnlp-main.78,P02-1040,0,0.106342,"(De⇒En) and English⇒French (En⇒Fr) translation tasks. For En⇒De task, we use WMT14 corpus which contains 4 million sentence pairs. The Zh⇒En task is conducted on WMT17 corpus, consisting of 21 million sentence pairs. We follow Dou et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “"
2020.emnlp-main.78,W19-5211,0,0.0345998,"Missing"
2020.emnlp-main.78,K16-1029,0,0.211111,"Missing"
2020.emnlp-main.78,P16-1162,0,0.117646,"rameters and training entire networks (RejTrain). 3 3.1 Experiments Setup Data We conduct experiments on English⇒ German (En⇒De), Chinese⇒English (Zh⇒En), German⇒English (De⇒En) and English⇒French (En⇒Fr) translation tasks. For En⇒De task, we use WMT14 corpus which contains 4 million sentence pairs. The Zh⇒En task is conducted on WMT17 corpus, consisting of 21 million sentence pairs. We follow Dou et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.7"
2020.emnlp-main.78,P16-1008,1,0.860236,"Missing"
2020.emnlp-main.78,D19-1088,1,0.857064,"Missing"
2020.emnlp-main.78,W04-3250,0,0.0607804,"e task, we use WMT14 corpus which contains 4 million sentence pairs. The Zh⇒En task is conducted on WMT17 corpus, consisting of 21 million sentence pairs. We follow Dou et al. (2018) to select the development and test sets. Furthermore, we evaluate low-resourced translation on IWSLT14 De⇒En and IWSLT17 En⇒Fr corpora. We preprocess our data using byte-pair encoding (Sennrich et al., 2016) with 40K merge operations for En⇒De, 32K for Zh⇒En, and 10K for De⇒En and En⇒Fr, and keep all tokens in the vocabulary. We use 4-gram BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Koehn, 2004) for statistical significance. Models We implement our approach on top of three popular architectures, namely Transformer (Vaswani et al., 2017), RNNSearch (Luong et al., 2015) and LightConv (Wu et al., 2019) with 1061 # 1 2 3 4 5 6 7 Model BASE + ConTrain + RejTrain + RejTrain B IG + ConTrain + RejTrain # Para. 108.6M 108.6M 108.6M 108.6M 305.3M 305.3M 305.3M BLEU 27.54 27.74 28.12⇑ 28.33⇑ 28.55 28.81 29.12⇑ ∆ – +0.20 +0.58 +0.79 – +0.26 +0.57 Data Zh-En (21M) De-En (0.16M) Table 2: Translation quality of Transformer model on WMT14 En⇒De. “# Para.” denotes the trainable parameter size of each"
2020.emnlp-main.78,2020.tacl-1.47,0,0.0432741,"Missing"
2020.emnlp-main.78,D19-1145,1,0.875297,"Missing"
2020.emnlp-main.78,2020.findings-emnlp.432,1,0.837561,"Missing"
2020.emnlp-main.78,D18-1041,0,0.0986037,"ent training phases. For each phase, we select sequentially three models. The solid arrow represents the changes in each phase. The dotted arrow represents the changes from the baseline to the pruning phase. ways: representation visualization and linguistic probing. Furthermore, we study the translation outputs in terms of adequacy and fluency. Escaping from Local Optimum To study how our method help models to escape from local optimum, we analyze the change of source representations during different training phases. The analysis is conducted on the Transformer BASE model and En⇒De. Following Zeng et al. (2018), we feed source sentences in the development set into a checkpoint and output an element-wise averaged vector from representations of the last encoder layer. With the dimension-reduction technique of TruncatedSVD (Du et al., 2017), we can plot the dimensionally reduced values in Figure 2. Among the training phases (i.e. Baseline, ConTrain, PruTrain, RejTrain), we select checkpoints at which interval training updates are equal. As seen, within each phase, the representations change smoothly in direction and quantity. The continuous training still transforms the representations in the same dire"
2020.findings-emnlp.432,D18-1457,1,0.829088,"ution of source words. Related to our work, Li et al. (2019) and Tang et al. (2019b) also conducted word alignment analysis on the same De-En and Zh-En datasets with Transformer models8 . We use similar techniques to examine word alignment in our context; however, we also introduce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed"
2020.findings-emnlp.432,W18-5431,0,0.0330701,"Missing"
2020.findings-emnlp.432,W19-5211,0,0.125059,"duce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed a graduated label smoothing that can improve the inference calibration. In this work, based on our information probing analysis, we simplified the decoder by removing the residual feedforward module in totality, with minimal loss of translation quality and a significant boost of"
2020.findings-emnlp.432,D16-1159,0,0.0466695,"Missing"
2020.findings-emnlp.432,2020.acl-main.269,1,0.828464,"peed (words per second) and “#Infer.” denotes the inference speed (sentences per second). Results are averages of three runs. preting the behaviors of attention modules. Previous studies generally focus on the self-attention in the encoder, which is implemented as multi-head attention. For example, Li et al. (2018) showed that different attention heads in the encoder-side self-attention generally attend to the same position. Voita et al. (2019) and Michel et al. (2019) found that only a few attention heads play consistent and often linguistically-interpretable roles, and others can be pruned. Geng et al. (2020) empirically validated that a selective mechanism can mitigate the problem of word order encoding and structure modeling of encoder-side self-attention. In this work, we investigated the functionalities of decoder-side attention modules for exploiting both source and target information. Interpreting Encoder Attention The encoderattention weights are generally employed to interpret the output predictions of NMT models. Recently, Jain and Wallace (2019) showed that atten4806 IFM -IFM -- 3.5 3.0 2.5 SEM 3.0 2 5.0 TEM 4.010: Comparison Figure of IFM information evolution between the standard and s"
2020.findings-emnlp.432,D18-1548,0,0.0198969,"and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance – a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed. 1 Introduction Transformer models have advanced the state-ofthe-art on a variety of natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019"
2020.findings-emnlp.432,D19-1149,0,0.26795,", 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the e"
2020.findings-emnlp.432,D19-1088,1,0.802727,"cePPL PPL Source AddAdd & Norm & Norm 5.0 5.0 En-Zh Source PPL 3.5 4.0 4.5 4.5 IFM SEM TEM En-De Source PPL 4.0 Softmax Softmax 4.5 5.5 5.5 IFM SEM TEM En-Fr 5.0 Output Output 4.5 Probabilities Probabilities Target TargetPPL PPL IFM SEM TEM EnDe-w/oFFN 5.5 2.0 1 2 3 Decoder 4 Model Standard (Base) Simplified (Base) Fluency 4.00 4.01 Adequacy 3.86 3.87 Table 5: Human evaluation of translation performance of both standard and simplified decoders on 100 samples from En-Zh test set, on the scale of 1 to 5. tion weights are weakly correlated with the contribution of source words to the prediction. He et al. (2019) used the integrated gradients to better estimate the contribution of source words. Related to our work, Li et al. (2019) and Tang et al. (2019b) also conducted word alignment analysis on the same De-En and Zh-En datasets with Transformer models8 . We use similar techniques to examine word alignment in our context; however, we also introduce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different"
2020.findings-emnlp.432,R19-1136,0,0.340244,", 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the e"
2020.findings-emnlp.432,N19-1357,0,0.0626198,"Missing"
2020.findings-emnlp.432,P19-1452,0,0.0230096,".1 Representation Evolution Across Layers In order to quantify and visualize the representation evolution, we design a universal probing scheme to quantify the source (or target) information stored in network representations. Task Description Intuitively, the more the source (or target) information stored in a network representation, the more probably a trained reconstructor could recover the source (or target) sequence. Since the lengths of source sequence and decoder representations are not necessarily the same, the widely-used classification-based probing approaches (Belinkov et al., 2017; Tenney et al., 2019b) cannot be applied to this task. Accordingly, we cast this task as a generation problem – evaluating the likelihood of generating the word sequence conditioned on the input representation. Figure 2 illustrates the architecture of our probing scheme. Given a representation sequence from decoder H = {h1 , . . . , hM } and the source (or target) word sequence to be recovered x = {x1 , . . . , xN } the recovery likelihood is calculated as the perplexity (i.e. negative log-likelihood) of forced-decoding the word sequence: P P L(x|H) = N X − log P (xn |x&lt;n , H) n=1 3 More implementation details ar"
2020.findings-emnlp.432,P16-1008,1,0.925135,"e functionalities of the three modules are well-separated. 2.3 Research Questions Modern Transformer decoder is implemented as multiple identical layers, in which the source and target information are exploited and evolved layerby-layer. One research question arises naturally: RQ1. How do source and target information evolve within the decoder layer-by-layer and module-by-module? RQ2. How does SEM exploit the source information in different layers? In Section 3.2, we investigate how the SEMs transform the source information to the target side in terms of alignment accuracy and coverage ratio (Tu et al., 2016). Experimental results show that higher layers of SEM modules accomplish word alignment, while lower layer ones exploit necessary contexts. This also explains the representation evolution of source information: lower layers collect more source information to obtain a global view of source input, and higher layers extract less aligned source input for accurate translation. Of the three sub-layers, IFM modules conceptually appear to play a key role in merging source and target information – raising our final question: RQ3. How does IFM fuse source and target information on the operation level? I"
2020.findings-emnlp.432,P19-1580,0,0.0203504,"bell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the encoder is fed to the decoder and the training signals for the encoder are back-propaga"
2020.findings-emnlp.432,D19-1085,1,0.891412,"Missing"
2020.findings-emnlp.432,D17-1301,1,0.848009,"ope that our analysis and findings could inspire architectural changes for further improvements, such as 1) improving the word alignment of higher SEMs by incorporating external alignment signals; 2) exploring the stacking order of SEM, TEM and IFM sub-layers, which may provide a more effective way to transform information; 3) further pruning redundant sub-layers for efficiency. Since our analysis approaches are not limited to the Transformer model, it is also interesting to explore other architectures such as RNMT (Chen et al., 2018), ConvS2S (Gehring et al., 2017), or on document-level NMT (Wang et al., 2017, 2019). In addition, our analysis methods can be applied to other sequence-to-sequence tasks such as summarization and grammar error correction, whose source and target sides are in the same language. We leave those tasks for future work. Acknowledgments Tadepalli acknowledges the support of DARPA under grant number N66001-17-2-4030. The authors thank the anonymous reviewers for their insightful and helpful comments. References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine trans"
2020.findings-emnlp.432,2020.acl-main.278,1,0.810191,", 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed a graduated label smoothing that can improve the inference calibration. In this work, based on our information probing analysis, we simplified the decoder by removing the residual feedforward module in totality, with minimal loss of translation quality and a significant boost of both training and inference speeds. 5 Conclusions In this paper, we interpreted NMT Transformer decoder by assessing the evolution of both source 8 We find our results are more similar to that of Tang et al. (2019b). Also, o"
2020.findings-emnlp.432,P19-1354,1,0.925372,"ion (Vaswani et al., 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that"
2020.findings-emnlp.432,D19-1083,0,0.0287233,"To verify that, we remove TEM from the decoder (“SEM⇒IFM”), which significantly increases the alignment error from 0.37 to 0.54 (in Figure 5), and leads to a serious decrease of translation performance (BLEU: 27.45 ⇒ 22.76, in Table 1) on En-De, while results on En-Zh also confirms it (in Figure 6). This indicates that TEM is essential for building word alignment. However, reordering the stacking of TEM and SEM (“SEM⇒TEM⇒IFM”) does not affect the alignment or translation qualities (BLEU: 27.45 vs. 27.61). These results provide empirical support for recent work on merging TEM and SEM modules (Zhang et al., 2019). Robustness to Decoder Depth To verify the robustness of our conclusions, we vary the depth of NMT decoder and train it from scratch. Table 2 demonstrates the results on translation quality, which generally show that more decoder layers bring better performance. Figure 7 shows that SEM behaves similarly regardless of depth. These results demonstrate the robustness of our conclusions. 3.3 Information Fusion in Decoder We now turn to the analysis of IFM. Within the Transformer decoder, IFM plays the critical role of fusing the source and target information by merg4804 0.9 0.9 0.9 0.9 0.8 0.8 0."
2020.wmt-1.34,D18-1457,1,0.850332,"Missing"
2020.wmt-1.34,D18-1045,0,0.0270558,"arge-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target monolingual corpus into source. Then the synthetic parallel corpus is used to train models together with the bilingual data. In this work we apply the noise back-translations method as introduced in (Lample et al., 2018). When translating monolingual data we use an ensemble of two models to get better source translations. We follow (Edunov et al., 2018) to add noise to the synthetic source data. Furthermore, we use a tag at the head of each synthetic source sentence as Caswell et al. (2019) does. To filter the pseudo corpus, we translate the synthetic source into target and calculate a Round-Trip BLEU score, the synthetic pairs are dropped if the BLEU score is lower than 30. Notably, we only apply back translation to the English → German task. We find that back translation decrease the translation quality to Chinese ↔ English tasks in our experiments. 2 https://github.com/mosessmt/mosesdecoder/tree/master/scripts/tokenizer/tokenizer.perl 3 3"
2020.wmt-1.34,D19-1135,1,0.88946,"Missing"
2020.wmt-1.34,N19-4009,0,0.0304603,", 2019) encoder. • B IG D EEP T RANSFORMER is the T RANSFORMER - BIG model with 20 encoder layers. • L ARGER T RANSFORMER is similar to B IG D EEP model except that it uses 8192 as the FFN inner width. The main differences between these models are presented in Table 1. To stabilize the training of deep model, we use the Pre-Norm strategy (Li et al., 2019). The layer normalization was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize → Transform → dropout → residual-add. All models are implemented on top of the open-source toolkit Fairseq3 (Ott et al., 2019). 3.2 Data Augmentation Data augmentation is a commonly used technique to improve the translation quality. There are various of methods to conduct data augmentation such as back-translation (Sennrich et al., 2016a), joint training (Zhang et al., 2018) etc. In this section, we will introduce the methods we used in WMT2020. 3.2.1 Large-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target mono"
2020.wmt-1.34,P16-1009,0,0.261753,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,P16-1162,0,0.542977,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,W19-5341,0,0.0876143,"to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the b"
2020.wmt-1.34,2020.wmt-1.60,1,0.813015,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,W18-6429,1,0.42399,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,W18-6430,0,0.0695928,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,P19-1176,0,0.0535969,"Missing"
2020.wmt-1.34,2020.wmt-1.97,1,0.841946,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,J82-2005,0,0.607939,"Missing"
2020.wmt-1.60,W18-6404,0,0.0212518,"this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 4.1 Data Selection Inspired by Ding and Tao (2019), multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM (Devlin et al., 2019b), Transformer LM (Bei et al., 2018), N-gram LM (Stolcke, 2002) and FDA (Bic¸ici and Yuret, 2011). Three LMs are complement each other on measuring qualities of sentences while FDA can measure its domain relevance given a in-domain dataset. Sentence pairs in the out-of-domain corpus Checkpoint Average and Model Ensemble 4.3 Finetuning We employ various finetuning strategies at different phases of training. For Sent-Out→Sent-In finetune (same architecture but different data), we first train a sentence-level model on large pseudo-indomain data and then continuously train it on small in-domain data. We apply similar strategy for Do"
2020.wmt-1.60,W11-2131,0,0.0808625,"Missing"
2020.wmt-1.60,N12-1047,0,0.0441032,"decoding. X LM→S ENT B ERT→D OC M BART→S ENT Integration Models IN I N+O UT IN I N+O UT IN I N+O UT I N→I N I N +O UT ∗ I N +O UT Pretrain O UT→I N O UT→I N +O UT I N +O UT I N +O UT I N +O UT BLEU 42.56 59.81 41.87 58.62 45.65 51.12 51.93 54.01 54.59 49.77 51.58 59.61 56.01 57.48 Table 4: BLEU scores of S ENT, D OC, NAT and P RE T RAIN with different finetuning strategies on En⇒De. the sentence reranker contains the best left-to-right (L2R) translation model, R2L (right-to-left) translation model and T2S (target-to-source) translation model. They are integrated by K-best batch MIRA training (Cherry and Foster, 2012) on valid set. 5 Experimental Results Unless otherwise specified, reported BLEU scores are calculated based on combined and tokenized validation set by muti-bleu.perl, which is different from the official evaluation method. 5.1 Ablation Study Table 2 investigates effects of different settings on translation quality. We then apply the best hyperparameters to the models in Section 4 if applicable. Effects of Model Average and Ensemble Following Section 4.2, we averaged top-L checkpoints in S ENT-B model and found that it performs best when L = 5. We followed the same operation for S ENT-S model"
2020.wmt-1.60,N19-1423,0,0.432471,"em combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. 1 We experimented mBART after the official submission. 483 Proceedings of the 5th Conference on Machine Translation (WMT), pages 483–491 c Online, November 19–20, 2020. 2020 Associatio"
2020.wmt-1.60,W19-5314,1,0.774159,"tion set and generated a final checkpoint with averaged weights to avoid stochasticity. To combine different models (maybe different architectures), we further ensembled the averaged checkpoints in each model. In our preliminary experiments, we find that this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 4.1 Data Selection Inspired by Ding and Tao (2019), multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM (Devlin et al., 2019b), Transformer LM (Bei et al., 2018), N-gram LM (Stolcke, 2002) and FDA (Bic¸ici and Yuret, 2011). Three LMs are complement each other on measuring qualities of sentences while FDA can measure its domain relevance given a in-domain dataset. Sentence pairs in the out-of-domain corpus Checkpoint Average and Model Ensemble 4.3 Finetuning We employ various finetuning strategies at dif"
2020.wmt-1.60,2020.wmt-1.3,0,0.0819563,"Missing"
2020.wmt-1.60,2020.tacl-1.47,0,0.105944,"l-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. 1 We experimented mBART after the official submission. 483 Proceedings of the 5th Conference on Machine Translation (WMT), pages 483–491 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Tes"
2020.wmt-1.60,W18-6311,0,0.171527,"transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while"
2020.wmt-1.60,D15-1130,0,0.0686515,"ask. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. W"
2020.wmt-1.60,W18-6301,0,0.136336,"y proposed evolved cross-attention (Ding et al., 2020). Technically, we used the most recent effective strategies including back/forward translation, data selection, domain adaptation, batch learning, finetuning, model ensemble and system combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensem"
2020.wmt-1.60,P16-1009,0,0.431512,"w.statmt. org/wmt20/chat-task_results_DA.html. 484 3 https://github.com/Unbabel/BConTrasT. http://www.statmt.org/wmt20/ translation-task.html. 5 https://github.com/ google-research-datasets/Taskmaster. 4 We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 2.2 Source Sentence Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE (Sennrich et al., 2016b) with 32K merge operations. 3.1 Sentence-level NMT (S ENT) We use standard T RANSFORMER models (Vaswani et al., 2017) with two customized settings. Due to data limitation, we use the small settings (S ENT-S)6 with regular batch size (4096 tokens × 8 GPUs). Based on the base settings (S ENT-B),7 we also empirically adopt big batch learning (Ott et al., 2018) (16348 tokens × 4 GPUs) with larger dropout (0.3). 3.2 Document-level NMT (D OC) To improve discourse properties for chat translation, we re-implement our document-level model (Wang et al., 2017b) on top of T RANS FORMER . Its addition en"
2020.wmt-1.60,P16-1162,0,0.599377,"w.statmt. org/wmt20/chat-task_results_DA.html. 484 3 https://github.com/Unbabel/BConTrasT. http://www.statmt.org/wmt20/ translation-task.html. 5 https://github.com/ google-research-datasets/Taskmaster. 4 We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 2.2 Source Sentence Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE (Sennrich et al., 2016b) with 32K merge operations. 3.1 Sentence-level NMT (S ENT) We use standard T RANSFORMER models (Vaswani et al., 2017) with two customized settings. Due to data limitation, we use the small settings (S ENT-S)6 with regular batch size (4096 tokens × 8 GPUs). Based on the base settings (S ENT-B),7 we also empirically adopt big batch learning (Ott et al., 2018) (16348 tokens × 4 GPUs) with larger dropout (0.3). 3.2 Document-level NMT (D OC) To improve discourse properties for chat translation, we re-implement our document-level model (Wang et al., 2017b) on top of T RANS FORMER . Its addition en"
2020.wmt-1.60,D19-1633,0,0.0234314,"ls that generate each target word conditioned on previously generated ones, NAT models break the autoregressive factorization and produce target words in parallel (Gu et al., 2018). Although NAT is proposed to speed up the inference, we exploit it to alleviate sequential error accumulation and improve the diversity in conversational translation. To adequately capture the source contexts, we proposed evolved cross-attention for NAT decoder by modeling the local and global attention simultaneously (Ding et al., 2020). Accordingly, we implement our method based on the advanced MaskPredict model (Ghazvininejad et al., 2019)8 , which uses the conditional mask LM (Devlin et al., 2019a) to iteratively generate the target sequence from the masked input. 3.4 Pretraining NMT (P RETRAIN) To transfer the general knowledge to chat translation models, we explore to initialize (part of) model parameters with different pretrained language/generation models. Li et al. (2019) showed 8 https://github.com/facebookresearch/ Mask-Predict. 485 #CP 1 5 10 15 20 ENS En-De 60.32 60.33 60.26 60.19 60.23 60.49 De-En 59.51 59.53 59.42 59.34 59.22 60.08 (a) Model average and ensemble. #BM 4 8 12 14 16 20 En-De 60.33 60.33 60.33 60.34 60."
2020.wmt-1.60,P14-6007,0,0.0288008,"the Tencent AI Lab’s entry into the WMT2020 Chat Translation Task. We explore a breadth of established techniques for building chat translation systems. The paper includes numerous models making use of sentence-level, document-level, non-autoregressive NMT. It also investigates a number of advanced techniques including data selection, model ensemble, finetuing, back/forward translation and initialization using a pretrained LMs. We present extensive experimental results and hope that this work could help both MT researchers and industries to boost the performance of discourse-aware MT systems (Hardmeier, 2014; Wang, 2019). Acknowledgments The authors wish to thank the organizers of WMT2020 Chat Translation for their prompt responses on our questions. The authors also specially thank Dr. Xuebo Liu (University of Macau) and Dr. Siyou Liu (Macao Polytechnic Institute), who kindly support us by their engineering and linguistic suggestions, respectively. Official Results The official automatic evaluation results of our submissions for WMT 2020 are presented in Table 8. For the primary submission, the S YS-1 combines S ENT (ensembled S ENT-B and S ENT-S), D OC and NAT models. As contrastive submissions,"
2020.wmt-1.60,D16-1139,0,0.0422246,"el model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 4.4 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT models (Sennrich et al., 2016a). About forward translation (FT), we employ forward translation model to perform sequence distillation for NAT models (Kim and Rush, 2016) . 4.5 System Combination As shown in Figure 1, in order to take full advantages of different systems (Model1 , Model2 and Model3 ), we explore both token- and sentencelevel combination strategies. Token-level We perform token-level combination with confusion network. Concretely, our method follows Consensus Network Minimum Bayes Risk (ConMBR) (Sim et al., 2007), which can be modeled as EConM BR = argminE 0 L(E 0 , Econ ), where Econ was obtained as backbone through performing consensus network decoding. X LM→S ENT B ERT→D OC M BART→S ENT Integration Models IN I N+O UT IN I N+O UT IN I N+O UT"
2020.wmt-1.60,D19-6503,0,0.128451,"9 https://github.com/google-research/ bert. 12 https://github.com/marian-nmt/marian. 13 https://github.com/bicici/FDA. https://github.com/facebookresearch/ XLM. 10 https://github.com/pytorch/fairseq/ tree/master/examples/mbart. 486 Method S ENT-B +Bi-FDA +Bi-FDA-XL +Mono-FDA-XL # Sent. 10K 300K 500K 1M 500K 800K 1M 800K 1M BLEU 41.87 59.36 59.81 59.96 59.86 59.95 59.68 60.36 59.80 Systems S ENT-B S ENT-S D OC NAT Table 3: BLEU scores of S ENT-BASE model on En⇒De task with different FDA variants (three LMs scoring are consistent). S ENT→D OC and we use “h/si” symbols as their pseudo contexts (Kim et al., 2019; Li et al., 2020). Besides, we conduct Sent-Out→Doc-In finetuning (different architectures and data). Specifically, we first train a sentence-level model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 4.4 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT mode"
2020.wmt-1.60,2020.acl-main.322,0,0.0386177,"Missing"
2020.wmt-1.60,I17-3009,1,0.817542,". Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth of established techniques for building Chat NMT systems. Specifica"
2020.wmt-1.60,D19-1085,1,0.805322,"owledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in Germa"
2020.wmt-1.60,D17-1301,1,0.937396,". Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth of established techniques for building Chat NMT systems. Specifica"
2020.wmt-1.60,L16-1436,1,0.906273,"resent extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth"
2020.wmt-1.60,2020.wmt-1.97,1,0.642141,"s 483–491 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Test 2,100 Out-of-domain 46,074,573 +filter 33,293,382 +select 1,000,000 Monolingual Out-of-domain De 58,044,806 +filter 56,508,715 +select 1,000,000 Out-of-domain En 34,209,709 +filter 32,823,301 +select 1,000,000 According to the official evaluation results, our systems in De⇒En and De⇐En are respectively ranked 2nd and 4th.2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation (Wang et al., 2020) and news translation (Wu et al., 2020) tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. Data and Processing"
2020.wmt-1.60,2020.wmt-1.34,1,0.813015,"2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Test 2,100 Out-of-domain 46,074,573 +filter 33,293,382 +select 1,000,000 Monolingual Out-of-domain De 58,044,806 +filter 56,508,715 +select 1,000,000 Out-of-domain En 34,209,709 +filter 32,823,301 +select 1,000,000 According to the official evaluation results, our systems in De⇒En and De⇐En are respectively ranked 2nd and 4th.2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation (Wang et al., 2020) and news translation (Wu et al., 2020) tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. Data and Processing 2.1 Data The parallel data we use to t"
2020.wmt-1.60,N19-1095,0,0.0264596,"dels to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our sub"
2020.wmt-1.97,D11-1033,0,0.0270018,"-domain monolingual data from bilingual data in other language pair. Specifically, we collect the English side of the bilingual sentence pairs from Biomedical Translation and UFAL Medical Corpus. The statistics of the in-domain bilingual and monolingual data is listed in Table 2. Bilingual Data 3.3 In-domain bilingual data The in-domain bilingual data is provided by WMT20 biomedical translation shared task. For German-English, we choose Biomedical Translation3 and UFAL Medical Corpus4 to use as the in-domain training data. For Chinese-English out-of-domain (OOD) data, we adopt data selection (Axelrod et al., 2011; Liu et al., 2014) to select the in-house data (8.5M sentence pairs) as the in-domain training data. General-domain bilingual data To alleviate the data scarce problem, we collect generaldomain bilingual data from WMT20 news translation shared task5 . For German-English, we use Europarl-v106 , ParaCrawl-v5.17 , News Commentary-v158 and Wiki Titles-v29 . For 2 https://github.com/pytorch/fairseq (Ott et al., 2019) 3 https://github.com/ biomedical-translation-corpora/corpora 4 https://ufal.mff.cuni.cz/ufal_ medical_corpus 5 http://www.statmt.org/wmt18/ translation-task.html 6 http://www.statmt.o"
2020.wmt-1.97,W19-5403,0,0.0635797,"n shuffled.cs-en shuffled.es-en shuffled.fr-en shuffled.hu-en shuffled.pl-en shuffled.ro-en shuffled.sv-en n/a n/a n/a n/a n/a n/a n/a n/a 37,814,533 n/a n/a n/a n/a n/a n/a n/a 37,814,533 48,243,170 92,999,169 88,526,658 48,783,611 39,442,076 62,034,179 23,142,661 Table 2: The detailed statistics of in-domain training data used in our system. “Zh/En” and “De/En” denote the Chinese-English and German-English bilingual data, respectively. “En” denotes the monolingual English data. use Moses scripts14 to preprocess15 the data and filter the bilingual data with following heuristics rules: Follow Bawden et al. (2019), we use multibleu.perl from Moses16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets. • Filter out duplicate sentence pairs (Khayrallah and Koehn, 2018; Ott et al., 2018). • Filter out sentence pairs with wrong language (Khayrallah and Koehn, 2018). • Filter out sentences pairs containing more than 120 tokens or fewer than 3. • Filter out sentence pairs with source/target length ratio exceeding 1.5 (Ott et al., 2018). 4.2 Evaluation For German-English, we use the Khresmoi development data as the development set, and use the sentence pairs with the corr"
2020.wmt-1.97,C18-1111,0,0.045754,"Missing"
2020.wmt-1.97,D18-1457,1,0.838269,"on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), p"
2020.wmt-1.97,D19-1082,1,0.890519,"Missing"
2020.wmt-1.97,D19-1135,1,0.809118,"the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), pages 881–886 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Encoder Layer Decoder Layer Attention Heads Emb"
2020.wmt-1.97,N19-1122,1,0.887317,"Missing"
2020.wmt-1.97,2020.emnlp-main.176,1,0.763879,"inal model for the testing. For model inference, the length penalty is set to 0.6 and the beam size is set to 4. The German-English results are listed in Table 3. Our observations are: • Due to the largest model capacity, L ARGE model obtains the best translation performance among the four model variants. • Ensemble decoding with different transformer architectures (E NSEMBLE in Table 3) achieves best translation performance. 4.5 • Leveraging in-domain bilingual data (“+Indomain”) and synthetic bilingual data (“+BT In-domain”) achieves significant translation improvement. Data rejuvenation18 (Jiao et al., 2020) is an approach which exploits the inactive training examples for neural machine translation on large-scale datasets. We adopt the data rejuvenation approach to German⇒English translation task. Experimental results are presented in Tale 7 and the data rejuvenation approach achieves significant improvement over the baseline L ARGE model. 4.4 train the models from scratch. Since the development set and test set have different data distribution, we save checkpoints every epoch and average the last 5 checkpoints rather than choose the model with best validation loss. For model inference, the lengt"
2020.wmt-1.97,W18-2709,0,0.0186969,"3 48,243,170 92,999,169 88,526,658 48,783,611 39,442,076 62,034,179 23,142,661 Table 2: The detailed statistics of in-domain training data used in our system. “Zh/En” and “De/En” denote the Chinese-English and German-English bilingual data, respectively. “En” denotes the monolingual English data. use Moses scripts14 to preprocess15 the data and filter the bilingual data with following heuristics rules: Follow Bawden et al. (2019), we use multibleu.perl from Moses16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets. • Filter out duplicate sentence pairs (Khayrallah and Koehn, 2018; Ott et al., 2018). • Filter out sentence pairs with wrong language (Khayrallah and Koehn, 2018). • Filter out sentences pairs containing more than 120 tokens or fewer than 3. • Filter out sentence pairs with source/target length ratio exceeding 1.5 (Ott et al., 2018). 4.2 Evaluation For German-English, we use the Khresmoi development data as the development set, and use the sentence pairs with the correct alignment in WMT19 biomedical translation task test set as our test set. For Chinese-English, we use the in-house bilingual test set (1,000 sentence pairs) and the sentence pairs with the c"
2020.wmt-1.97,W17-3204,0,0.0209667,"English bilingual data for the community. The rest of this paper is organized as follows. Section 2 presents our system with four different transformer architectures: D EEP, H YBRID, B IG, L ARGE Transformers. Section 3 describes the training data used in our system, including bilingual data, monolingual data and synthetic bilingual data. Section 4 reports experimental results in two language directions. Finally, we conclude our work in Section 5. Introduction Neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017, NMT) has achieved great progress in recent years. However, as Koehn and Knowles (2017) pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach ("
2020.wmt-1.97,W19-5325,0,0.0377866,"Missing"
2020.wmt-1.97,2020.wmt-1.60,1,0.704639,"Missing"
2020.wmt-1.97,P19-1176,0,0.0310034,"opt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), pages 881–886 c Onli"
2020.wmt-1.97,D17-1155,0,0.0610201,"Missing"
2020.wmt-1.97,P14-2093,1,0.694985,"Missing"
2020.wmt-1.97,N19-4009,0,0.0654804,"Missing"
2020.wmt-1.97,W19-5420,0,0.0561124,"e lack of sufficient in-domain bilingual data, we use an online translation system TranSmart13 to translate the in-domain monolingual English back to Chinese. For German-English, we train a English-German L ARGE model on the combination of in-domain and general-domain bilingual data, and use the model to generate synthetic bilingual data. 4 Experiment We report experimental results in four language pairs: German-English (de/en), English-German (en/de), Chinese-English (zh/en) and EnglishChinese (en/zh). 4.1 Experimental Setup Data Pre-Processing We follow previous work (Saunders et al., 2019; Peng et al., 2019) to 10 http://mteval.cipsc.org.cn:81/ agreement/description 11 https://conferences.unite.un.org/ UNCorpus/ 12 http://data.statmt.org/wikititles/v2/ 13 transmart.qq.com 882 Corpus File Zh/En De/En En Biomedical Translation wmt18training/es-en wmt18training/fr-en wmt18training/pt-en wmt19training/de-en wmt19training/fr-en wmt19training/es-en wmt19training/pt-en wmt20training/it-en wmt20training/ru-en n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 40,398 n/a n/a n/a n/a n/a 287,811 627,576 74,645 40,398 75,049 100,257 49,918 14,756 46,782 UFAL Medical Corpus shuffled.de-en shuffled.cs-en shuffle"
2020.wmt-1.97,W19-5421,0,0.0186626,"r Chinese-English, as we lack of sufficient in-domain bilingual data, we use an online translation system TranSmart13 to translate the in-domain monolingual English back to Chinese. For German-English, we train a English-German L ARGE model on the combination of in-domain and general-domain bilingual data, and use the model to generate synthetic bilingual data. 4 Experiment We report experimental results in four language pairs: German-English (de/en), English-German (en/de), Chinese-English (zh/en) and EnglishChinese (en/zh). 4.1 Experimental Setup Data Pre-Processing We follow previous work (Saunders et al., 2019; Peng et al., 2019) to 10 http://mteval.cipsc.org.cn:81/ agreement/description 11 https://conferences.unite.un.org/ UNCorpus/ 12 http://data.statmt.org/wikititles/v2/ 13 transmart.qq.com 882 Corpus File Zh/En De/En En Biomedical Translation wmt18training/es-en wmt18training/fr-en wmt18training/pt-en wmt19training/de-en wmt19training/fr-en wmt19training/es-en wmt19training/pt-en wmt20training/it-en wmt20training/ru-en n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 40,398 n/a n/a n/a n/a n/a 287,811 627,576 74,645 40,398 75,049 100,257 49,918 14,756 46,782 UFAL Medical Corpus shuffled.de-en sh"
2020.wmt-1.97,P16-1009,0,0.709327,"pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder sta"
2020.wmt-1.97,P16-1162,0,0.826805,"pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder sta"
2020.wmt-1.97,D19-1145,1,0.895388,"Missing"
2020.wmt-1.97,D17-1149,1,0.899559,"Missing"
2020.wmt-1.97,2020.wmt-1.34,1,0.89255,"Missing"
2021.acl-long.221,W19-5301,0,0.0422466,"Missing"
2021.acl-long.221,2021.acl-long.567,0,0.0323651,"(NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large"
2021.acl-long.221,W19-5206,0,0.218083,"ems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava"
2021.acl-long.221,2020.acl-main.532,0,0.253958,"suggest that emphasizing the learning on uncertain monolingual sentences also brings additional benefits for the learning of low-frequency words at the target side. 4 Related Work Synthetic Parallel Data. Data augmentation by synthetic parallel data has been the most simple and effective way to utilize monolingual data for NMT, 2847 which can be achieved by self-training (He et al., 2019) and back-translation (Sennrich et al., 2016a). While back-translation has dominated the NMT area for years (Fadaee and Monz, 2018; Edunov et al., 2018; Caswell et al., 2019), recent works on translationese (Marie et al., 2020; Graham et al., 2019) suggest that NMT models trained with backtranslation may lead to distortions in automatic and human evaluation. To address the problem, starting from WMT2019 (Barrault et al., 2019), the test sets only include naturally occurring text at the sourceside, which is a more realistic scenario for practical translation usage. In this new testing setup, the forward-translation (Zhang and Zong, 2016), i.e., self-training in NMT, becomes a more promising method as it also introduces naturally occurring text at the source-side. Therefore, we focus on the data sampling strategy in"
2021.acl-long.221,D18-1040,0,0.361354,"tions; ST: pseudo-sentences) on WMT En⇒De newstest2019 and newstest2020. certainty. Another interesting finding is that using the pseudo-sentences outperforms using the manual translations (Rows 4 vs. 2, 5 vs. 3). One possible reason is that the T RANSFORMER - BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data, which serves as self-training to decently improve the supervised baseline (He et al., 2019). Comparison with Related Work. We compared our sampling approach with two related works, i.e., difficult word by frequency (DWF, Fadaee and Monz, 2018) and source language model (S RC LM, Lewis, 2010). The former one was proposed for monolingual data selection for back-translation, in which sentences with lowfrequency words were selected to boost the performance of back-translation. The latter one was proposed for in-domain data selection for in-domain language models. Details of the implementation of related work are in Appendix A.3. Table 3 listed the results. For DWF, it brings no improvement over R AND S AMP, indicating that the Table 3: Comparison of the proposed uncertaintybased sampling strategy with related methods on WMT En⇒De newst"
2021.acl-long.221,N19-4007,0,0.0367251,"per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To ensure the quality of synthetic parallel data, we trained 2 BLEU+case.mixed+lang.[Task]+numrefs.1 +smooth.exp++test.wmt[Year]+tok.[Tok]+ver sion.1.4.14, Task=en-de/en-zh, Year=19/20, Tok=13a/zh 3 https://githu"
2021.acl-long.221,2020.emnlp-main.76,0,0.0302764,"one reflects the quality of synthetic sentence pairs. We also presented the results of the synthetic target sentences for reference. Details of the linguistic properties are in Appendix A.2. The results are reported in Figure 3. For the length property, we find that monolingual sentences with higher uncertainty are usually longer except for those with excessively high uncertainty (e.g., bin 5). The monolingual sentences in the last data bin noticeably contain more rare words than other bins in Figure 3(b), and the rare words in the sentences pose a great challenge in the NMT training process (Gu et al., 2020). In Figure 3(c), the overall coverage in bin 5 is the lowest among the self-training bins. In contrast, bin 1 with the lowest uncertainty has the highest coverage. These observations suggest that monolingual sentences in bin 1 indeed contain the easiest patterns while 2843 50 50 50 8.5 8.5 8.5 Source Source Source Target TargetTarget 1.001.001.00 Source Source Source Target TargetTarget Source Source Source Target TargetTarget 0.950.950.95 25 25 25 8.0 8.0 8.0 0.900.900.90 0 0 0 3Uncertainty UncertaintyDistribution Distribution 1 1 21 2 32 3 43 4 54 5 7.5 7.5 7.5 1 1 21 2 32 3 43 4 54 5 5 bin"
2021.acl-long.221,W11-2123,0,0.0291568,"Missing"
2021.acl-long.221,2020.findings-emnlp.435,1,0.846505,"lity. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.1 1 Introduction Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020"
2021.acl-long.221,2020.emnlp-main.176,1,0.887935,"lity. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.1 1 Introduction Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020"
2021.acl-long.221,D16-1139,0,0.0227609,"ct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer 2840 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2840–2850 August 1–6, 2021. ©2021 Association for Computational Linguistics vision also reveals that easy patterns in unlabeled data with the deterministic prediction may not provide"
2021.acl-long.221,W04-3250,0,0.579118,"steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To ensure the quality of synthetic parallel data, we trained 2 BLEU+case.mixed+lang.[Task]+numrefs.1 +smooth.exp++test.wmt[Year]+tok.[Tok]+ver sion.1.4.14, Task=en-de/en-zh"
2021.acl-long.221,P10-2041,0,0.131195,"newstest2020. certainty. Another interesting finding is that using the pseudo-sentences outperforms using the manual translations (Rows 4 vs. 2, 5 vs. 3). One possible reason is that the T RANSFORMER - BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data, which serves as self-training to decently improve the supervised baseline (He et al., 2019). Comparison with Related Work. We compared our sampling approach with two related works, i.e., difficult word by frequency (DWF, Fadaee and Monz, 2018) and source language model (S RC LM, Lewis, 2010). The former one was proposed for monolingual data selection for back-translation, in which sentences with lowfrequency words were selected to boost the performance of back-translation. The latter one was proposed for in-domain data selection for in-domain language models. Details of the implementation of related work are in Appendix A.3. Table 3 listed the results. For DWF, it brings no improvement over R AND S AMP, indicating that the Table 3: Comparison of the proposed uncertaintybased sampling strategy with related methods on WMT En⇒De newstest2019 and newstest2020. technique developed for"
2021.acl-long.221,2020.tacl-1.47,0,0.0257659,"iao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model t"
2021.acl-long.221,W19-5333,0,0.0111278,"ges. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strate"
2021.acl-long.221,N19-4009,0,0.01743,"yj |xi ). yj ∈Ab (xi ) (4) 2.2 Experimental Setup Data. We conducted experiments on two large-scale benchmark translation datasets, i.e., WMT English⇒German (En⇒De) and WMT English⇒Chinese (En⇒Zh). The authentic parallel data for the two tasks consists of about 36.8M and 22.1M sentence pairs, respectively. The monolingual data we used is from newscrawl released by WMT2020. We combined the Model. We chose the state-of-the-art T RANS FORMER (Vaswani et al., 2017) network as our model, which consists of an encoder of 6 layers and a decoder of 6 layers. We adopted the open-source toolkit Fairseq (Ott et al., 2019) to implement the model. We used the T RANSFORMER -BASE model for preliminary experiments (§2.3) and the constrained scenario (§3.2) for efficiency. For the unconstrained scenario (§3.3), we adopted the T RANSFORMER -B IG model. Results on these models with different capacities can also reflect the robustness of our approach. For the T RANSFORMER BASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16"
2021.acl-long.221,P02-1040,0,0.109387,"scenario (§3.3), we adopted the T RANSFORMER -B IG model. Results on these models with different capacities can also reflect the robustness of our approach. For the T RANSFORMER BASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual s"
2021.acl-long.221,N19-1119,0,0.0226628,"These results corroborate with prior studies (Chang et al., 2017; Mukherjee and Awadallah, 2020) such that learning on certain examples brings little gain while on the excessively uncertain examples may also hurt the model training. 2.4 Linguistic Properties of Uncertain Data We further analyzed the differences between the monolingual sentences with varied uncertainty to gain a deeper understanding of the uncertain data. Specifically, we performed linguistic analysis on the five data bins in terms of three properties: 1) sentence length that counts the tokens in the sentence, 2) word rarity (Platanios et al., 2019) that measures the frequency of words in a sentence with a higher value indicating a more rare sentence, and 3) translation coverage (Khadivi and Ney, 2005) that measures the ratio of source words being aligned with any target words. The first two reflect the properties of monolingual sentences while the last one reflects the quality of synthetic sentence pairs. We also presented the results of the synthetic target sentences for reference. Details of the linguistic properties are in Appendix A.2. The results are reported in Figure 3. For the length property, we find that monolingual sentences"
2021.acl-long.221,W18-6319,0,0.016797,"-B IG model. Results on these models with different capacities can also reflect the robustness of our approach. For the T RANSFORMER BASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To"
2021.acl-long.221,P16-1009,0,0.652024,"le unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data aug"
2021.acl-long.221,P16-1162,0,0.714753,"le unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data aug"
2021.acl-long.221,2020.wmt-1.30,0,0.0364461,"Missing"
2021.acl-long.221,P19-1177,0,0.014441,"pectively. Generally, a high H(Y|X = x) denotes that a source sentence x would have more possible translation candidates. Equation (2) estimates the translation uncertainty of a source sentence with all possible translation candidates in the parallel corpus. It can not be directly applied to the sentences in monolingual data due to the lack of corresponding translation candidates. One potential solution to the problem is utilizing a trained model to generate multiple translation candidates. However, generation may lead to bias estimation due to the generation diversity issue (Li et al., 2016; Shu et al., 2019). More importantly, generation is extremely time-consuming for large-scale monolingual data. Monolingual Uncertainty. To address the problem, we modified Equation (2) to reflect the uncertainty of monolingual sentences. We estimate the target word distribution conditioned on each source word based on the authentic parallel corpus, and then use the distribution to measure the translation uncertainty of the monolingual example. Specifically, we measure the uncertainty of monolingual sentences based on the bilingual dictionary. 2841 37 38 newscrawl data from year 2011 to 2019 for the English mono"
2021.acl-long.221,2020.acl-main.252,0,0.0210192,"; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “t"
2021.acl-long.221,P16-1008,1,0.892962,"Missing"
2021.acl-long.221,D19-1073,0,0.0341507,"21). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer 2840 Proceedings of the 59th Annu"
2021.acl-long.221,2020.acl-main.278,1,0.895865,"Missing"
2021.acl-long.221,2020.emnlp-main.216,0,0.0635692,"Missing"
2021.acl-long.221,D19-1430,0,0.0751742,"; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer 2840 Proceed"
2021.acl-long.221,2020.wmt-1.34,1,0.766284,"oaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization ("
2021.acl-long.221,D16-1160,0,0.321979,"come an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, se"
2021.acl-long.221,2020.acl-main.620,0,0.0581449,"Missing"
2021.acl-long.266,D16-1162,0,0.0173145,"lists in Table 11. As seen, AT models also suffer from the problem of low-frequency words when using knowledge distillation, and our approach also works for them. Takeaway: Our method works well for general cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulati"
2021.acl-long.266,P05-1066,0,0.372859,"Missing"
2021.acl-long.266,N19-1423,0,0.0209994,"knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks employ Transformer-B IG as their strong teacher except for Ro-En and Small En-De, which are distilled by Transformer-BASE. 4 http://www.statmt.org/wmt19/ translation-task.html Training Traditionally, NAT models are usually trained for 300K steps on regular batch size (i.e. • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from the masked input. We followed its optimal settings to keep the iteration number as 10 and length beam as 5. • Levenshtein Transformer (LevT, Gu et al. 2019) that introduces three steps: deletion, placeholder and token prediction. The decoding iterations adaptively depends on certain conditions. 3435 Model Zh-En Ja-En Law Med. IT Kor. Sub. BLEU ALF BLEU ALF AT 41.5 30.8 27.5 8.6 15.4 AT 25.3 66.2 29.8 70.8 MaskT +LFR 24.2 25.1† 61.5 64.8 28.9 29.6† 66.9 68.9 MaskT +LFR 37.3 38.1† 28.2 28.8 24.6 25.4† 7.3 8.9† 11.2 14.3† LevT +LFR 24.4 25.1† 62.7"
2021.acl-long.266,2021.findings-acl.247,1,0.886121,"Missing"
2021.acl-long.266,2020.coling-main.389,1,0.552002,"Missing"
2021.acl-long.266,2020.emnlp-main.176,1,0.799552,"model for the rest steps. For fair comparison, the total training steps of the proposed method are same as the traditional one. In general, we expect that this training recipe can provide a good trade-off between raw and distilled data (i.e. high-modes and complete vs. low-modes and incomplete). 2.3 Bidirectional Distillation Training Analyzing Bilingual Links in Data KD simplifies the training data by replacing low-frequency target words with high-frequency ones (Zhou et al., 2020). This is able to facilitate easier aligning source words to target ones, resulting in high bilingual coverage (Jiao et al., 2020). Due to the information loss, we argue that KD makes lowfrequency target words have fewer opportunities to align with source ones. To verify this, we propose a method to quantitatively analyze bilingual links from two directions, where low-frequency words similar performance. 3433 are aligned from source to target (s 7→ t) or in an opposite direction (t 7→ s). The method can be applied to different types of data. Here we take s 7→ t links in Raw data as an example to illustrate the algorithm. Given the WMT14 En-De parallel corpus, we employ an unsupervised word alignment method2 (Och and Ney,"
2021.acl-long.266,N13-1073,0,0.0320442,"duct ←− the same analysis method on KD data, and found better t 7→ s links but worse s 7→ t links compared with Raw. Take the Zh-En sentence pair in Ta−→ ble 2 for example, KD retains the source side lowfrequency Chinese words “海克曼” (RawS ) but generates the high-frequency English words “Heck−→ man” instead of the golden “Hackman” (KDT ). On ←− the other hand, KD preserves the low-frequency English words “Hackman” (RawT ) but produces the ←− high-frequency Chinese words “哈克曼” (KDS ). Our Approach Based on analysis results, we propose to train NAT models on bidirectional distil2 The FastAlign (Dyer et al., 2013) was employed to build word alignments for the training datasets. lation by concatenating two kinds of distilled data. The reverse distillation is to replace the source sentences in the original training data with synthetic ones generated by a backward AT teacher.3 Ac←− cording to Equation 3, KD can be formulated as: ←− KD = {(yi , ft7→s (yi ))|yi ∈ Rawt }N i=1 (4) where ft7→s represents an AT-based translation model trained on Raw data for translating text from the target to the source language. Figure 1(c) illustrates the training strategy. First, we employ both fs7→t and ft7→s AT models to"
2021.acl-long.266,D18-1045,0,0.0446041,"mbined pipeline: Raw → KD + KD → KD as out best training strategy. There are many possible ways to implement the general idea of combining two approaches. The aim of this paper is not to explore the whole space but simply to show that one fairly straightforward implementation works well and the idea is reasonable. Nonetheless, we compare possible strategies of combination two approaches as well as demonstrate their complementarity in §3.3. While in main experiments (in §3.2), we valid the combination strategy, namely Low-Frequency Rejuvenation (LFR). 3 This is different from back-translation (Edunov et al., 2018), which is an alternative to leverage monolingual data. 3434 Model Iteration Speed En-De Ro-En BLEU ALF BLEU ALF AT Models Transformer-BASE (Ro-En Teacher) n/a Transformer-B IG (En-De Teacher) n/a 1.0× 27.3 0.8× 29.2 Existing NAT Models NAT (Gu et al., 2018) 1.0 2.4× Iterative NAT (Lee et al., 2018) 10.0 2.0× DisCo (Kasai et al., 2020) 4.8 3.2× Mask-Predict (Ghazvininejad et al., 2019) 10.0 1.5× Levenshtein (Gu et al., 2019) 2.5 3.5× 19.2 21.6 26.8 27.0 27.3 70.5 34.1 73.0 n/a n/a 31.4 30.2 33.3 33.3 33.3 73.6 n/a n/a Our NAT Models Mask-Predict (Ghazvininejad et al., 2019) 27.0 10.0 1.5× +Low"
2021.acl-long.266,D18-1040,0,0.0208174,"rious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulation Our work is related to previous studies on manipulating training data for NMT. Bogoychev and Sennrich (2019) show that forwardand backward-translations (FT/ BT) could both boost the model performances, where FT plays the role of domain adaptation and BT makes the translation fluent. Fadaee and Monz (2018) sample the monolingual data with more difficult words (e.g. rare words) to perform BT, achieving significant improvements compared with randomly sampled BT. Nguyen et al. (2020) diversify the data by applying FT and BT multiply times. However, different from AT, the prerequisite of training a well-performed NAT model is to perform KD. We compared with related works in Table 10 and found that our approach consistently outperforms them. Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data. Non-Autoregressive Translation A variety of approac"
2021.acl-long.266,D19-1633,0,0.280947,"e complementary approaches (i.e. raw pretraining, 3431 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3431–3441 August 1–6, 2021. ©2021 Association for Computational Linguistics bidirectional distillation training and KD finetuning) as a new training strategy for further boosting NAT performance (§2.4). We validated our approach on five translation benchmarks (WMT14 En-De, WMT16 Ro-En, WMT17 Zh-En, WAT17 Ja-En and WMT19 EnDe) over two advanced architectures (Mask Predict, Ghazvininejad et al., 2019; Levenshtein Transformer, Gu et al., 2019). Experimental results show that the proposed method consistently improve translation performance over the standard NAT models across languages and advanced NAT architectures. Extensive analyses confirm that the performance improvement indeed comes from the better lexical translation accuracy especially on low-frequency tokens. Knowledge Distillation Gu et al. (2018) pointed out that NAT models suffer from the multimodality problem, where the conditional independence assumption prevents a model from properly capturing the highly multimodal distributio"
2021.acl-long.266,2020.emnlp-main.76,0,0.0249342,"eneral cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulation Our work is related to previous studies on manipulating training data for NMT. Bogoychev and Sennrich (2019) show that forwardand backward-translations (FT/ BT) could both boost the model perfo"
2021.acl-long.266,2021.naacl-main.313,1,0.78714,"t our approach consistently outperforms them. Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data. Non-Autoregressive Translation A variety of approaches have been exploited to bridge the performance gap between NAT and AT models. Some researchers proposed new model architectures (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Kasai et al., 2020), aided with additional signals (Wang et al., 2019; Ran et al., 2019; Ding et al., 2020), introduced sequential information (Wei et al., 2019; Shao et al., 2019; Guo et al., 2020; Hao et al., 2021), and explored advanced training objectives (Ghazvininejad et al., 2020; Du et al., 2021). Our work is close to the research line on training methods. Ding et al. (2021b) revealed the low-frequency word problem in distilled training data, and introduced an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Ding et al. (2021a) propose a simple and effective training strategy, which progressively feeds different granularity of data into NAT models by leveraging curriculum learning. 5 Conclusion In this study, we propose"
2021.acl-long.266,D16-1139,0,0.116243,"e.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. The translation accuracy of lowfrequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established We closely followed previous works to apply sequence-level knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks employ Transformer-B IG as their strong teacher except for Ro-En and Small En-De, which are distilled by Transformer-BASE. 4 http://www.statmt.org/wmt19/ translation-task.html Training Traditionally, NAT models are usually trained for 300K steps on regular batch size (i.e. • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the t"
2021.acl-long.266,W17-3204,0,0.0572941,"the teacher model. For fair comparison, we leverage the TransformerBASE as the student model, which shares the same model capacity with NAT student (i.e. MaskT). The result lists in Table 11. As seen, AT models also suffer from the problem of low-frequency words when using knowledge distillation, and our approach also works for them. Takeaway: Our method works well for general cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and"
2021.acl-long.266,D18-1149,0,0.314945,"reasonable. Nonetheless, we compare possible strategies of combination two approaches as well as demonstrate their complementarity in §3.3. While in main experiments (in §3.2), we valid the combination strategy, namely Low-Frequency Rejuvenation (LFR). 3 This is different from back-translation (Edunov et al., 2018), which is an alternative to leverage monolingual data. 3434 Model Iteration Speed En-De Ro-En BLEU ALF BLEU ALF AT Models Transformer-BASE (Ro-En Teacher) n/a Transformer-B IG (En-De Teacher) n/a 1.0× 27.3 0.8× 29.2 Existing NAT Models NAT (Gu et al., 2018) 1.0 2.4× Iterative NAT (Lee et al., 2018) 10.0 2.0× DisCo (Kasai et al., 2020) 4.8 3.2× Mask-Predict (Ghazvininejad et al., 2019) 10.0 1.5× Levenshtein (Gu et al., 2019) 2.5 3.5× 19.2 21.6 26.8 27.0 27.3 70.5 34.1 73.0 n/a n/a 31.4 30.2 33.3 33.3 33.3 73.6 n/a n/a Our NAT Models Mask-Predict (Ghazvininejad et al., 2019) 27.0 10.0 1.5× +Low-Frequency Rejuvenation 27.8† 68.4 33.3 72.3 33.9† 70.9 72.4 Levenshtein (Gu et al., 2019) +Low-Frequency Rejuvenation 69.2 33.2 72.8 33.8† 71.1 72.7 2.5 3.5× 27.4 28.2† Table 3: Comparison with previous work on WMT14 En-De and WMT16 Ro-En. “Iteration” indicates the number of iterative refinement wh"
2021.acl-long.266,W17-5706,0,0.0753165,"ding. “ALF” is the translation accuracy on low-frequency words. “† ” indicates statistically significant difference (p &lt; 0.05) from corresponding baselines. 3 3.1 Experiment based on the widely-used automatic alignment tool GIZA++ (Och and Ney, 2003). Setup Models We validated our research hypotheses on two state-of-the-art NAT models: Data Main experiments are conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De, Vaswani et al. 2017), WMT16 Romanian-English (Ro-En, Gu et al. 2018), WMT17 Chinese-English (Zh-En, Hassan et al. 2018), and WAT17 Japanese-English (Ja-En, Morishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the universality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and s"
2021.acl-long.266,N18-1031,0,0.0156507,"words when using knowledge distillation, and our approach also works for them. Takeaway: Our method works well for general cases through rejuvenating more low-frequency words. 4 Related Work Low-Frequency Words Benefiting from continuous representation learned from the training data, NMT models have shown the promising performance. However, Koehn and Knowles (2017) point 3438 that low-frequency words translation is still one of the key challenges for NMT according to the Zipf’s law (Zipf, 1949). For AT models, Arthur et al. (2016) address this problem by integrating a count-based lexicon, and Nguyen and Chiang (2018) propose an additional lexical model, which is jointly trained with the AT model. Recently, Gu et al. (2020) adaptively re-weight the rare words during training. The lexical choice problem is more serious for NAT models, since 1) the lexical choice errors (low-resource words in particular) of AT distillation will propagate to NAT models; and 2) NAT lacks target-side dependencies thus misses necessary target-side context. In this work, we alleviate this problem by solving the first challenge. Data Manipulation Our work is related to previous studies on manipulating training data for NMT. Bogoyc"
2021.acl-long.266,J03-1002,0,0.127279,"t al., 2020). Due to the information loss, we argue that KD makes lowfrequency target words have fewer opportunities to align with source ones. To verify this, we propose a method to quantitatively analyze bilingual links from two directions, where low-frequency words similar performance. 3433 are aligned from source to target (s 7→ t) or in an opposite direction (t 7→ s). The method can be applied to different types of data. Here we take s 7→ t links in Raw data as an example to illustrate the algorithm. Given the WMT14 En-De parallel corpus, we employ an unsupervised word alignment method2 (Och and Ney, 2003) to produce a word alignment, and then we extract aligned links whose source words are low-frequency (called s 7→ t LFW Links). Second, we randomly select a number of samples from the parallel corpus. For better comparison, the subset should contains the same i in Equation (2) as that of other type of datasets (e.g. i in Equation −→ (3) for KD). Finally, we calculate recall, precision, F1 scores based on low-frequency bilingual links for the subset. Recall (R) represents how many low-frequency source words can be aligned to targets. Precision (P) means how many aligned low-frequency links are"
2021.acl-long.266,P02-1040,0,0.109214,"WAT17 Japanese-English (Ja-En, Morishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the universality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. The translation accuracy of lowfrequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established We closely followed previous works to apply sequence-level knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks employ Transformer-B IG as their strong teacher except for Ro-En and Small"
2021.acl-long.266,N19-1119,0,0.0994126,"Missing"
2021.acl-long.266,2020.acl-main.15,0,0.260218,"he performance improvement indeed comes from the better lexical translation accuracy especially on low-frequency tokens. Knowledge Distillation Gu et al. (2018) pointed out that NAT models suffer from the multimodality problem, where the conditional independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations. Thus, the sequence-level knowledge distillation is introduced to reduce the modes of training data by replacing their original target-side samples with sentences generated by an AT teacher (Gu et al., 2018; Zhou et al., 2020; Ren et al., 2020). Formally, the original parallel data Raw and the −→ distilled data KD can be defined as follows: Contributions Our main contributions are: where fs7→t represents an AT-based translation model trained on Raw data for translating text from the source to the target language. N is the total number of sentence pairs in training data. As shown in Figure 1 (a), well-performed NAT models −→ are generally trained on KD data instead of Raw. • We show the effectiveness of rejuvenating lowfrequency information by pretraining NAT models from raw data. • We provide a quantitative analysis of bilingual lin"
2021.acl-long.266,P16-1162,0,0.0534523,", Gu et al. 2018), WMT17 Chinese-English (Zh-En, Hassan et al. 2018), and WAT17 Japanese-English (Ja-En, Morishita et al. 2017), which consist of 4.5M, 0.6M, 20M, and 2M sentence pairs, respectively. We use the same validation and test datasets with previous works for fair comparison. To prove the universality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De.4 The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations. We use tokenized BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. The translation accuracy of lowfrequency words is measured by AoLC (Ding et al., 2021b), where word alignments are established We closely followed previous works to apply sequence-level knowledge distillation to NAT (Kim and Rush, 2016). Specifically, we train both BASE and B IG Transformer as the AT teachers. For B IG model, we adopt large batch strategy (i.e. 458K tokens/batch) to optimize the performance. Most NAT tasks e"
2021.acl-long.266,P19-1125,0,0.0195216,"e compared with related works in Table 10 and found that our approach consistently outperforms them. Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data. Non-Autoregressive Translation A variety of approaches have been exploited to bridge the performance gap between NAT and AT models. Some researchers proposed new model architectures (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Kasai et al., 2020), aided with additional signals (Wang et al., 2019; Ran et al., 2019; Ding et al., 2020), introduced sequential information (Wei et al., 2019; Shao et al., 2019; Guo et al., 2020; Hao et al., 2021), and explored advanced training objectives (Ghazvininejad et al., 2020; Du et al., 2021). Our work is close to the research line on training methods. Ding et al. (2021b) revealed the low-frequency word problem in distilled training data, and introduced an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Ding et al. (2021a) propose a simple and effective training strategy, which progressively feeds different granularity of data into NAT models by leveraging curr"
2021.acl-long.266,D19-1430,0,0.20018,"erent seeds) improves data diversification for NMT, and we leave this for future work. 2.4 Combining Both of Them: Low-Frequency Rejuvenation (LFR) We have proposed two parallel approaches to rejuvenate low-frequency knowledge from authentic (§2.2) and synthetic (§2.3) data, respectively. Intuitively, we combine both of them to further improve the model performance. From data view, two presented training strategies −→ −→ ←− are: Raw → KD (Raw Pretraining) and KD + KD (Bidirectional Distillation Training). Considering the effectiveness of pretraining (Mathis et al., 2021) and clean finetuning (Wu et al., 2019), we introduce −→ ←− −→ a combined pipeline: Raw → KD + KD → KD as out best training strategy. There are many possible ways to implement the general idea of combining two approaches. The aim of this paper is not to explore the whole space but simply to show that one fairly straightforward implementation works well and the idea is reasonable. Nonetheless, we compare possible strategies of combination two approaches as well as demonstrate their complementarity in §3.3. While in main experiments (in §3.2), we valid the combination strategy, namely Low-Frequency Rejuvenation (LFR). 3 This is diffe"
2021.emnlp-main.402,2020.tacl-1.36,0,0.0661528,"Missing"
2021.emnlp-main.402,W14-3346,0,0.0961204,"ulti-task sequence tagging. In particular, for each input word, we decide whether to delete it or not, and at the same time, we choose what span from the dialogue context need to be inserted to the front of the current word. In this way, our solution enjoys a far smaller search space than the generation based approaches. Since our model does not directly take features from the word-to-word interactions of its output utterances, this may cause the lack of fluency. To encourage more fluent outputs, we propose to inject additional supervisions from two popular metrics, i.e., sentence-level BLEU (Chen and Cherry, 2014) and the perplexity of a pretrained GPT-2 (Radford et al., 2019) model, using the framework of “REINFORCE with a baseline” (Williams, 1992). Sentence-level BLEU is computationally efficient, but it requires references and thus may only provide domain-specific knowledge. Conversely, the perplexity by GPT-2 is reference-free, giving more guidance on open-domain scenarios benefiting from the large-scale pretraining. Experiments on two dialogue rewriting benchmarks show that our model can give huge improvements (14.6 in BLEU4 score and 18.9 percent of exact match) over the current state-of-the-art"
2021.emnlp-main.402,2020.emnlp-main.651,0,0.074226,"Missing"
2021.emnlp-main.402,N19-1423,0,0.0777142,"able has limited coverage. Though we also convert our original problem into a multi-task tagging problem, we predict what span to be inserted, avoiding the issues caused by using a phrase table. Besides, we study injecting richer supervision signals to improve the fluency of outputs, which is a common issue for tagging based approaches on text generation, as they do not directly model wordto-word dependencies. Finally, we are the first to apply sequence tagging on dialogue rewriting, showing much better performances than those of BERT-based strong baselines. 3 Our baseline consists of a BERT (Devlin et al., 2019) encoder and a Transformer (Vaswani et al., 2017) decoder with a copy mechanism. Given input tokens X = (x1 , . . . , xN ) that is the concatenation of the current dialogue context c = (u1 , . . . , ui−1 ) and the latest utterance ui , the BERT encoder is firstly adopted to represent the input with contextualized embeddings: (1) Next, the Transformer decoder with copy mechanism is adopted to generate a rewriting output u0 = (y1 , . . . , yM ) one token at a time: p(yt |y<t , X) = θt pvocab + (1 − θt )pattn t t pattn , st t vocab pt (2) = TransDecoder(y<t , E) (3) = Softmax(Linear(st )) (4) whe"
2021.emnlp-main.402,D19-1605,0,0.0392318,"Missing"
2021.emnlp-main.402,P16-1154,0,0.0773972,"Missing"
2021.emnlp-main.402,P16-1014,0,0.0473789,"Missing"
2021.emnlp-main.402,I17-1099,0,0.0609614,"Missing"
2021.emnlp-main.402,P19-1003,0,0.339307,"r the current state-of-the-art systems when transferring to another dataset. u1 上海最近天气怎么样？ (How is the recent weather in Shanghai?) u2 最近经常阴天下雨。 (It is always raining recently.) u3 冬天就是这样。 (Winter is like this.) u03 上海冬天就是经常阴天下雨。 (It is always raining in winter Shanghai.) Table 1: An example dialogue including the context utterances (u1 and u2 ), the latest utterance (u3 ) and the rewritten utterance (u03 ). on these tasks are still far from satisfactory, not to mention their uncovered situations, such as when a whole verb phrase is omitted. Recently, the task of dialogue utterance rewriting (Su et al., 2019; Pan et al., 2019; Elgohary et al., 1 Introduction 2019) was proposed as for explicitly representing multi-turn dialogues. The task aims to reconstruct Recent years have witnessed increasing attention the latest dialogue utterance into a new utterance in conversation-based tasks, such as conversational question answering (Choi et al., 2018; Reddy et al., that is semantically equivalent to the original one 2019; Sun et al., 2019), dialogue response genera- and can be understood without referring to the contion (Li et al., 2017; Zhang et al., 2018; Wu et al., text. In another point of view, it"
2021.emnlp-main.402,Q19-1014,1,0.890552,"Missing"
2021.emnlp-main.402,2020.emnlp-main.227,0,0.0155517,"method, where all context words that need to be inserted during rewriting are identified in the first step. The second step adopts a pointer generator that takes the outputs of the first step as additional features to produce the output. Xu et al. (2020) train a model of semantic role labeling (SRL) to highlight the core meaning (e.g., who did what to whom) of each input dialogue to prevent their rewriter from violating this information. To obtain an accurate SRL model on dialogues, they manually annotate SRL information for more than 27,000 dialogue turns, which is timeconsuming and costly. Liu et al. (2020) casts this task into a semantic segmentation problem, a major task in computer vision. In particular, their model generates a word-level matrix, which contains the operations of substitution and insertion, for each original utterance. They adopt a heavy model that takes 10 convolution layers in addition to the BERT encoder. None of the existing efforts mention the robustness issue, a critical aspect for the usability of this task. Besides, they only compare performances under automatic metrics (e.g., BLEU). We take the first step to address this severe robustness issue, and we adopt multiple"
2021.emnlp-main.402,D19-1510,0,0.0209559,"to address this severe robustness issue, and we adopt multiple measures for comprehensive evaluation. Besides, we propose a novel model based on sequence tagging for solving this task, and our model takes a much smaller search space than previous models. Sequence tagging for text generation Given the intrinsic nature of typical text-generation problems (e.g., machine translation), i.e. (1) the number of predictions cannot be determined by inputs, and (2) the candidate space for each prediction is usually very large, sequence tagging is not commonly adopted on text-generation tasks. Recently, Malmi et al. (2019) proposed a model based on sequence tagging for sentence fusion and sentence splitting, and they show that their model outperforms a vanilla sequence-to-sequence baseline. In particular, their model can decide whether to keep or delete each input word and what phrase needs 2 Related Work to be inserted in front of it. As a result, they have Initial efforts (Su et al., 2019; Elgohary et al., to extract a large phrase table from the training 2019) treat dialogue utterance rewriting as a stan- data, causing inevitable computation for choosing 4914 phrases from the table. Their approach also faces"
2021.emnlp-main.402,D19-1191,0,0.228972,"te-of-the-art systems when transferring to another dataset. u1 上海最近天气怎么样？ (How is the recent weather in Shanghai?) u2 最近经常阴天下雨。 (It is always raining recently.) u3 冬天就是这样。 (Winter is like this.) u03 上海冬天就是经常阴天下雨。 (It is always raining in winter Shanghai.) Table 1: An example dialogue including the context utterances (u1 and u2 ), the latest utterance (u3 ) and the rewritten utterance (u03 ). on these tasks are still far from satisfactory, not to mention their uncovered situations, such as when a whole verb phrase is omitted. Recently, the task of dialogue utterance rewriting (Su et al., 2019; Pan et al., 2019; Elgohary et al., 1 Introduction 2019) was proposed as for explicitly representing multi-turn dialogues. The task aims to reconstruct Recent years have witnessed increasing attention the latest dialogue utterance into a new utterance in conversation-based tasks, such as conversational question answering (Choi et al., 2018; Reddy et al., that is semantically equivalent to the original one 2019; Sun et al., 2019), dialogue response genera- and can be understood without referring to the contion (Li et al., 2017; Zhang et al., 2018; Wu et al., text. In another point of view, it integrates the rec"
2021.emnlp-main.402,P02-1040,0,0.1101,"2016; See et al., 2017). They have demonstrated recovery. But, the state-of-the-art performances almost ready-to-use performances on the test set ∗ Work done while J. Hao was interning and L. Wang was from the same data source as the training set. Howworking at Tencent AI Lab. † Corresponding author. ever, they are not robust, as our experiments show 4913 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4913–4924 c November 7–11, 2021. 2021 Association for Computational Linguistics that their performances can drop dramatically (by roughly 33 BLEU4 (Papineni et al., 2002) points and 44 percent of exact match) on another test set created from a different data source (not necessarily from a totally different domain). We argue that it may not be the best practice to model utterance rewriting as standard text generation. One main reason is that text generation introduces an overly large search space, while a rewriting output (e.g., u03 in Table 1) always keeps the core semantic meaning of its input (e.g., u3 ). Besides, exposure bias (Wiseman and Rush, 2016) can further exacerbate the problem for test cases that are not similar to the training set, resulting in ou"
2021.emnlp-main.402,D16-1264,0,0.0452181,"of our model. For a fair comparison, it takes the same BERT-based encoder (Equation 1) as the baseline to represent each input. For simplicity, we directly apply classifiers to predict the corresponding tags for each input word. In particular, to determine whether each word xn in the current utterance ui should be kept or deleted, we use a binary classifier: p(dn |X, n) = Softmax(Wd en + bd ) (7) where Wd and bd are learnable parameters, dn is the binary classification result, and en is the BERT embedding for xn . 4916 Moreover, we cast span prediction as machine reading comprehension (MRC) (Rajpurkar et al., 2016), where a predicted span corresponds to an MRC target answer. For each input token xn ∈ ui , we follow the previous work on MRC to predict ed the start position sst n and end position sn for the target span sn , performing separate self-attention mechanisms for them: p(sst n |X, n) = Attnstart (E, en ) (8) p(sed n |X, n) = Attnend (E, en ) (9) where Attnstart and Attnend are the self-attention layers for predicting the start and end positions of a span. We use the standard additive attention mechanism (Bahdanau et al., 2014) to perform the attention function. The probability for the whole span"
2021.emnlp-main.402,Q19-1016,0,0.0404538,"Missing"
2021.emnlp-main.402,P17-1099,0,0.0428704,", . . . , ui−1 ) and the latest utterance ui , the BERT encoder is firstly adopted to represent the input with contextualized embeddings: (1) Next, the Transformer decoder with copy mechanism is adopted to generate a rewriting output u0 = (y1 , . . . , yM ) one token at a time: p(yt |y<t , X) = θt pvocab + (1 − θt )pattn t t pattn , st t vocab pt (2) = TransDecoder(y<t , E) (3) = Softmax(Linear(st )) (4) where TransDecoder is the Transformer decoder that returns the attention probability distribution pattn over the encoder states E and the latest det coder state st for each step t. Following See et al. (2017), the generation probability θt for timestep t is calculated from the weighted sum for the encoder-decoder cross attention distribution and the encoder hidden states. X θt = σ(w| (pattn [n] · en )) (5) t n∈[1..N ] where w represents the model parameter. In this way, the copy mechanism encourages copying words from the input tokens. The T RANS -PG baseline is trained with standard cross-entropy loss: X Lgen = − log p(yt |y<t , X; θ) (6) t∈[1..M ] where θ represents all model parameters. 5 06 06 27 Shanghai recently recently Reference <start> Input <start> Deletion Insertion often 5 Shanghai 9 w"
2021.emnlp-main.402,D16-1137,0,0.0229302,"Missing"
2021.emnlp-main.402,P19-1369,0,0.0548787,"Missing"
2021.emnlp-main.402,2020.emnlp-main.537,1,0.851999,"Missing"
2021.emnlp-main.402,2020.acl-main.444,1,0.888547,"Missing"
2021.emnlp-main.402,P18-1205,0,0.0608904,"Missing"
2021.emnlp-main.402,2020.acl-demos.30,0,0.0330435,".7 76.0 69.8 67.6 76.5 78.8 79.3 80.5 7.4 8.6 12.9 24.5 26.8 31.8 Table 3: Test results of all comparing models trained on the R EWRITE dataset. Model settings We implement the baseline and our model on top of a BERT-base model (Devlin et al., 2019), and we use Adam (Kingma and Ba, 2015) as the optimizer, setting the learning rate to 3e−5 as determined by a development experiment. For the reinforcement learning stage, we respectively use the sentence-level BLEU score with “Smoothing 3” (Chen and Cherry, 2014) or the perplexity score based on a Chinese GPT-2 model trained on massive dialogues (Zhang et al., 2020)4 as the reward function. It is worth noting that the GPT-2 model is not fine-tuned during the reinforcement learning stage. 6.2 Main Results Training on R EWRITE Table 3 shows the results when all comparing models are trained on the R EWRITE dataset, before evaluating on the indomain R EWRITE and the R ESTORATION test data for robustness examination. On the R EWRITE test set, our tagging-based models (Rows 4-6) are much better than the T RANS -PG+BERT baseline, and they can get comparable performances with RUN, the previous state-of-the-art model. RUN usually gets high numbers on BLEU1 withou"
2021.emnlp-main.402,2020.acl-main.635,0,0.0367031,"Missing"
2021.emnlp-main.402,D19-1192,0,0.0176334,"n open-domain scenarios benefiting from the large-scale pretraining. Experiments on two dialogue rewriting benchmarks show that our model can give huge improvements (14.6 in BLEU4 score and 18.9 percent of exact match) over the current state-of-the-art model for cross-dataset evaluation. More analysis shows that the outputs of our model keep more semantic information from the inputs. Our code is available at https://github.com/ freesunshine0316/RaST-plus. dard text generation problem, adopting sequenceto-sequence models with copy mechanism to tackle this problem. Later work (Pan et al., 2019; Zhou et al., 2019; Huang et al., 2021) explores taskspecific features for additional gains in performance. For instance, Pan et al. (2019) adopts a pipeline-based method, where all context words that need to be inserted during rewriting are identified in the first step. The second step adopts a pointer generator that takes the outputs of the first step as additional features to produce the output. Xu et al. (2020) train a model of semantic role labeling (SRL) to highlight the core meaning (e.g., who did what to whom) of each input dialogue to prevent their rewriter from violating this information. To obtain an"
2021.findings-acl.247,P05-1066,0,0.812321,"Missing"
2021.findings-acl.247,N19-1423,0,0.028792,"e preprocessed data via bytepair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations. We evaluated the translation quality with BLEU (Papineni et al., 2002) with statistical significance test (Collins et al., 2005). For fine-grained bilingual knowledge, e.g. word alignment and phrase table, to ensure the source to target mapping more deterministic, we set 0.05 as the probability threshold. Taking WMT14 EnDe for example, there are 3M words and 156M phrases in the original phrase table extracted by • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from the masked input; • Levenshtein Transformer (LevT, Gu et al. 2019) that introduces three steps: deletion, placeholder prediction and token prediction. For regularization, we empirically set the dropout rate as 0.2, and apply weight decay with 0.01 and label smoothing with  = 0.1. We train batches of approximately 128K tokens using Adam (Kingma and Ba, 2015). The learning rate warms up to 5 × 10−4 in the first 10K steps, and then decays with the inverse square-root schedule. We train 50k steps on word-level data and 50k steps on phrase-level da"
2021.findings-acl.247,2021.acl-long.266,1,0.886121,"Missing"
2021.findings-acl.247,2020.acl-main.153,1,0.822376,"Missing"
2021.findings-acl.247,2020.coling-main.389,1,0.381955,"Missing"
2021.findings-acl.247,N10-1140,0,0.0461511,"essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work can be seen as a typical determinism-based curriculum learning (CL) (Bengio et al., 2009) method, where the finer granular2798 Models BLEU Speed Ro-En En-De Zh-En Ja-En AT Models Transformer-BASE (Ro-En Teacher) 1.0× Transformer"
2021.findings-acl.247,D19-1633,0,0.451065,"“high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work can be seen as a typical determinism-based curriculum learning (CL) (Bengio et al., 2009) method, where the finer granular2798 Models BLEU Speed Ro-En En-De Zh-En Ja-En AT Models Transformer-BASE (Ro-En Teacher) 1.0× Transformer-B IG (En-De / Zh-En / Ja-En Teacher) 0.8× 34.1 n/a 27.3 29.2 24.4 25.3 29.2 29.8 Existing NAT Models NAT (Gu et al., 2018) 2.4× Iterative NAT (Lee et al., 2018) 2.0× DisCo (Kasai et al., 2020) 3.2× Levenshtein (Gu et al., 2019) 3.5× Mask-Predict (Ghazvininejad et al., 2019) 1.5× Context-aware NAT (Ding et al., 2020b) 1.5× 31.4 30.2 33.3 33.3 33.3 33.2 19.2 21.6 26.8 27.3 27.0 27.5 n/a n/a n/a n/a 23.2 24.6 n/a n/a n/a n/a n/a 29.4 Our NAT Models Levenshtein (Gu et al., 2019) 3.5× +PMG Training Mask-Predict (Ghazvininejad et al., 2019) 1.5× +PMG Training 33.2 33.8† 33.3 33.7 27.4 27.8 27.0 27.6† 24.4 25.0† 24.0 24.5 29.1 29.6 28.9 29.5† Table 3: Comparison with previous work on WMT16 Ro-En, WMT14 En-De, WMT17 Zh-En and WAT17 Ja-En datasets. “† ” indicates that the proposed method was significantly better than baseline at significance level p<0.05. ities are more"
2021.findings-acl.247,D16-1139,0,0.0522007,"lity against strong NAT baselines. Also, we show that more deterministic fine-grained knowledge can further enhance performance. 1 W ORD P HRASE S ENTENCE 59.8 36.0 29.2 Raw 4 KD 4 57.1 31.7 24.5 -2.7 -4.3 -4.7 59.0 34.2 27.0 -0.8 -1.8 -2.2 Table 1: Translation performance at different granularity on the WMT14 English⇒German dataset. “4” indicates the performance gap between the NAT and AT. independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations. To reduce the modes of training data, sequence-level knowledge distillation (KD) (Kim and Rush, 2016) is widely employed via replacing their original target samples with sentences generated from an AT teacher (Gu et al., 2018; Zhou et al., 2020; Ren et al., 2020). Introduction Non-autoregressive translation (NAT, Gu et al., 2018) has been proposed to improve the decoding efficiency by predicting all tokens independently and simultaneously. Different from autoregressive translation (AT, Vaswani et al., 2017) models that generate each target word conditioned on previously generated ones, NAT models suffer from the multimodality problem (i.e. multiple translations for a single input), in which t"
2021.findings-acl.247,N03-1017,0,0.175871,"gaps between NAT and AT are significant than that of word and phrase in Table 1. Based on the above evidence, it is natural to suspect that the existing sentence-level NAT training is sub-optimal. 2.2 Fine-grained Bilingual Knowledge Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for"
2021.findings-acl.247,D18-1149,0,0.110574,"Missing"
2021.findings-acl.247,2020.acl-main.41,1,0.866952,"al. (2021) delivered the knowledge from pretrained language models to the NAT models. Above works improve the NAT at the model level, while we improve NAT at the data level. Most related to our work, Ding et al. (2021a) proposed data-level strategies, including reverse distillation and bidirectional distillation, to make the Curriculum Learning Our proposed training strategy is a novel technique for NAT by exploiting curriculum learning (CL). Recent works have shown that CL can help the autoregressive translation (AT) models achieve fast convergence and better results (Platanios et al., 2019; Liu et al., 2020b; Zhan et al., 2021; Zhou et al., 2021). However, CL for non-autoregressive translation (NAT) models has not been well studied. Among the few attempts, Guo et al. (2020a); Liu et al. (2020a) respectively investigated “parameter- and task-level” curriculum learning approaches, while we proposed progressive multi-granularity training for NAT at “datalevel”. To the best of our knowledge, this is the first work to investigate the effects of different granularities of data on NAT models. 5 Conclusion In this paper, we investigated the translation accuracy of different granularities in NAT, and fou"
2021.findings-acl.247,P06-1077,0,0.129713,"rained Bilingual Knowledge Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work can be seen as a typical determinism-based curriculum learning (CL) (Bengio et al., 2009) method, where the finer granular2798 Models BLEU Speed Ro-En En-De Zh-En Ja-En AT Mod"
2021.findings-acl.247,J03-1002,0,0.0727205,"ificant than that of word and phrase in Table 1. Based on the above evidence, it is natural to suspect that the existing sentence-level NAT training is sub-optimal. 2.2 Fine-grained Bilingual Knowledge Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexicons (Koehn and CallisonBurch, 2009). For each training example in the original training set, we sample its all possible intersentence bi-lingual phrases from the phrase table that obtained with phrase-based statistical machine translation (PBSMT) model (Koehn et al., 2003). The GIZA++ (Och and Ney, 2003) was employed to build word alignments for the training datasets. We leave the exploitation of more advanced forms bilingual knowledge such as syntax rules (Liu et al., 2006) and discontinuous phrases (Galley and Manning, 2010) for future work. Take the sentence phrase boundaries. 2.3 Progressive Multi-Granularity Training We present an extremely simple progressive multigranularity (PMG) training fashion. Concretely, we progressively schedule the PMG: learn from “low” to “high” granularity, i.e. word→phrase→sentence. And we empirically set the training steps for each training stage. Our work c"
2021.findings-acl.247,P02-1040,0,0.112217,"lishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence pairs, respectively. It is worthy noting that Ro-En, En-De and Zh-En are low-, medium- and high- resource language pairs, and Ja-En is word order divergent language direction. We use the same validation and test datasets with previous works for fair comparison. To avoid unknown works, we preprocessed data via bytepair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations. We evaluated the translation quality with BLEU (Papineni et al., 2002) with statistical significance test (Collins et al., 2005). For fine-grained bilingual knowledge, e.g. word alignment and phrase table, to ensure the source to target mapping more deterministic, we set 0.05 as the probability threshold. Taking WMT14 EnDe for example, there are 3M words and 156M phrases in the original phrase table extracted by • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from the masked input; • Levenshtein Transformer (LevT, Gu et al. 2019) that introduces three steps: del"
2021.findings-acl.247,N19-1119,0,0.490511,"26.8 27.3 27.0 27.5 n/a n/a n/a n/a 23.2 24.6 n/a n/a n/a n/a n/a 29.4 Our NAT Models Levenshtein (Gu et al., 2019) 3.5× +PMG Training Mask-Predict (Ghazvininejad et al., 2019) 1.5× +PMG Training 33.2 33.8† 33.3 33.7 27.4 27.8 27.0 27.6† 24.4 25.0† 24.0 24.5 29.1 29.6 28.9 29.5† Table 3: Comparison with previous work on WMT16 Ro-En, WMT14 En-De, WMT17 Zh-En and WAT17 Ja-En datasets. “† ” indicates that the proposed method was significantly better than baseline at significance level p<0.05. ities are more deterministic than sentences. Thus we compare with typical CL works (Zhang et al., 2019; Platanios et al., 2019) in Section 3.2. SMT methodology. We then filter the items whose translation probability is lower than 0.05 and obtain 0.3M words and 56.5M phrases as the final data. 3 Non-Autoregressive Models We validated our progressive multi-granularity training strategy on two state-of-the-art NAT model structures: 3.1 Experiment Setup Data Experiments were conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence pairs, respectively. It"
2021.findings-acl.247,2020.acl-main.15,0,0.114674,"29.2 Raw 4 KD 4 57.1 31.7 24.5 -2.7 -4.3 -4.7 59.0 34.2 27.0 -0.8 -1.8 -2.2 Table 1: Translation performance at different granularity on the WMT14 English⇒German dataset. “4” indicates the performance gap between the NAT and AT. independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations. To reduce the modes of training data, sequence-level knowledge distillation (KD) (Kim and Rush, 2016) is widely employed via replacing their original target samples with sentences generated from an AT teacher (Gu et al., 2018; Zhou et al., 2020; Ren et al., 2020). Introduction Non-autoregressive translation (NAT, Gu et al., 2018) has been proposed to improve the decoding efficiency by predicting all tokens independently and simultaneously. Different from autoregressive translation (AT, Vaswani et al., 2017) models that generate each target word conditioned on previously generated ones, NAT models suffer from the multimodality problem (i.e. multiple translations for a single input), in which the conditional ∗ NAT AT Liang Ding and Longyue Wang contributed equally to this work. Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab"
2021.findings-acl.247,2020.emnlp-main.83,0,0.0136977,"ni et al., 2017) models that generate each target word conditioned on previously generated ones, NAT models suffer from the multimodality problem (i.e. multiple translations for a single input), in which the conditional ∗ NAT AT Liang Ding and Longyue Wang contributed equally to this work. Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab. Although KD reduces the learning difficulty for NAT, there are still complicated word orders and structures (Gell-Mann and Ruhlen, 2011) in the synthetic sentences, making the NAT performance sub-optimal. To answer this challenge, Saharia et al. (2020); Ran et al. (2021) propose to lowers the bilingual modeling difficulties under the monotonicity assumption, where bilingual sentences are in the same word order. However, they make extensive modifications to model structures or objectives, limiting the applicability of their methods to a boarder range of tasks and languages. Accordingly, we turn to break down the sentencelevel high modes into finer granularities, i.e. bilingual words and phrases, where we assume that finer granularities are easy to be learned by NAT. As shown in Table 1, we analyzed the translation accuracy at three linguisti"
2021.findings-acl.247,P16-1162,0,0.121428,"periment Setup Data Experiments were conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence pairs, respectively. It is worthy noting that Ro-En, En-De and Zh-En are low-, medium- and high- resource language pairs, and Ja-En is word order divergent language direction. We use the same validation and test datasets with previous works for fair comparison. To avoid unknown works, we preprocessed data via bytepair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations. We evaluated the translation quality with BLEU (Papineni et al., 2002) with statistical significance test (Collins et al., 2005). For fine-grained bilingual knowledge, e.g. word alignment and phrase table, to ensure the source to target mapping more deterministic, we set 0.05 as the probability threshold. Taking WMT14 EnDe for example, there are 3M words and 156M phrases in the original phrase table extracted by • Mask-Predict (MaskT, Ghazvininejad et al. 2019) that uses the conditional mask LM (Devlin et al., 2019) to iteratively generate the target sequence from t"
2021.findings-acl.247,2021.eacl-main.18,0,0.030813,"will leave the exploration of high-quality bilingual knowledge for NAT as a future work. 4 Related Works Non-Autoregressive Translation There still exists a performance gap between AT teacher and its NAT student. To bridge this gap, many studies have been proposed. Ghazvininejad et al. (2019); Gu et al. (2019); Kasai et al. (2020) designed novel model structures to considerably improve the NAT model capacity. Wang et al. (2019); Ran et al. (2021); Ding et al. (2021b); Du et al. (2021) explored to improve the model performance with additional training signals or objectives. Guo et al. (2020b); Su et al. (2021) delivered the knowledge from pretrained language models to the NAT models. Above works improve the NAT at the model level, while we improve NAT at the data level. Most related to our work, Ding et al. (2021a) proposed data-level strategies, including reverse distillation and bidirectional distillation, to make the Curriculum Learning Our proposed training strategy is a novel technique for NAT by exploiting curriculum learning (CL). Recent works have shown that CL can help the autoregressive translation (AT) models achieve fast convergence and better results (Platanios et al., 2019; Liu et al."
2021.findings-acl.247,P16-1008,1,0.849057,"y be sub-optimal. • We propose PMG training to encourage NAT models to learn from easy to hard. The finegrained knowledge distilled by SMT will be dynamically transferred during training. • Experiments across language pairs and model structures show the effectiveness and universality of PMG training. 2 2.1 Methodology Motivation We investigated theories in second-language acquisition: one usually learns a foreign language from word-to-word translation to sentence-to-sentence translation, namely from local to global (Onnis et al., 2008). Bilingual knowledge is at the core of adequacy modeling (Tu et al., 2016), which is a major weakness of the NAT models due to the lacks of autoregressive factorization. Table 2 demonstrates the English⇒Chinese multimodality at different granularities (i.e. word, phrase, sentence levels). As seen, the sentence-level consists of various kinds of modes, including word alignment (“English” vs. “英语”/“英文”), phrase translation (“be good at” vs. “...非常 擅长...”/“...水平 很 高”), and even reordering (“英语” can be subject or object). However, phrase-level modes are less complex with similar structure and word-level modes are simple with token-to-token mapping. Generally, the lower"
2021.findings-acl.247,2002.tmi-tutorials.2,0,0.0906952,"结构 镂空 结构 他 英文 很 好。 他 非常 擅长 英语 。 他的 英语 水平 很 高 。 Table 2: Examples of different translation granularities. ities, there are still some gaps with AT teacher. Also, we showed that finer granularities are easier to be learned, that is, accuracy gap “∆” of WORD is small than that of PHRASE , and SEN TENCE (0.8<1.8<2.2). Thus, we propose a simple and effective training strategy to enhance the ability to handle the sentence-level high modes. More specifically, we generate bilingual lexicons from parallel data by leveraging word alignment and phrase extraction in statistical machine translation (SMT, Zens et al., 2002). Then we guide the NAT model to progressively learn the bilingual knowledge from low to high granularity. Experimental results on four commonly-cited translation benchmarks show that our proposed PROGRESSIVE MULTI - GRANULARITY (PMG) training strategy consistently improves the translation performance. The main contributions are: • Our study reveals that NAT is better at learning fine-grained knowledge. Training with sentences merely may be sub-optimal. • We propose PMG training to encourage NAT models to learn from easy to hard. The finegrained knowledge distilled by SMT will be dynamically t"
2021.findings-acl.247,N19-1189,0,0.112105,"33.3 33.2 19.2 21.6 26.8 27.3 27.0 27.5 n/a n/a n/a n/a 23.2 24.6 n/a n/a n/a n/a n/a 29.4 Our NAT Models Levenshtein (Gu et al., 2019) 3.5× +PMG Training Mask-Predict (Ghazvininejad et al., 2019) 1.5× +PMG Training 33.2 33.8† 33.3 33.7 27.4 27.8 27.0 27.6† 24.4 25.0† 24.0 24.5 29.1 29.6 28.9 29.5† Table 3: Comparison with previous work on WMT16 Ro-En, WMT14 En-De, WMT17 Zh-En and WAT17 Ja-En datasets. “† ” indicates that the proposed method was significantly better than baseline at significance level p<0.05. ities are more deterministic than sentences. Thus we compare with typical CL works (Zhang et al., 2019; Platanios et al., 2019) in Section 3.2. SMT methodology. We then filter the items whose translation probability is lower than 0.05 and obtain 0.3M words and 56.5M phrases as the final data. 3 Non-Autoregressive Models We validated our progressive multi-granularity training strategy on two state-of-the-art NAT model structures: 3.1 Experiment Setup Data Experiments were conducted on four widely-used translation datasets: WMT14 EnglishGerman (En-De), WMT16 Romanian-English (RoEn), WMT17 Chinese-English (Zh-En) and WAT17 Japanese-English (Ja-En), which consist of 4.5M, 0.6M, 20M and 2M sentence"
2021.findings-acl.247,2021.iwslt-1.25,1,0.813384,"m pretrained language models to the NAT models. Above works improve the NAT at the model level, while we improve NAT at the data level. Most related to our work, Ding et al. (2021a) proposed data-level strategies, including reverse distillation and bidirectional distillation, to make the Curriculum Learning Our proposed training strategy is a novel technique for NAT by exploiting curriculum learning (CL). Recent works have shown that CL can help the autoregressive translation (AT) models achieve fast convergence and better results (Platanios et al., 2019; Liu et al., 2020b; Zhan et al., 2021; Zhou et al., 2021). However, CL for non-autoregressive translation (NAT) models has not been well studied. Among the few attempts, Guo et al. (2020a); Liu et al. (2020a) respectively investigated “parameter- and task-level” curriculum learning approaches, while we proposed progressive multi-granularity training for NAT at “datalevel”. To the best of our knowledge, this is the first work to investigate the effects of different granularities of data on NAT models. 5 Conclusion In this paper, we investigated the translation accuracy of different granularities in NAT, and found that the NAT models are better at dea"
2021.findings-acl.373,2020.acl-main.705,0,0.0141674,"copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample,"
2021.findings-acl.373,N19-1423,0,0.355934,"ussein Tantawi war anwesend. Table 1: Training objective gap between Seq2Seq LM pre-training and NMT training. LM learns to reconstruct a few source tokens and copy most of them, while NMT learns more translation rather than copying. Underlines denote artificial noises, and highlights indicate expected copying tokens. 2020). As a range of surface, syntactic and semantic information has been encoded in the initialized parameters (Jawahar et al., 2019; Goldberg, 2019), they are expected to bring benefits to NMT models and hence the translation quality. Introduction Self-supervised pre-training (Devlin et al., 2019; Song et al., 2019), which acquires general knowledge from a large amount of unlabeled data to help better and faster learning downstream tasks, has an intuitive appeal for neural machine translation (NMT; Bahdanau et al., 2015; Vaswani et al., 2017). One direct way to utilize pre-trained knowledge is initializing the NMT model with a pre-trained language model (LM) before training it on parallel data (Conneau and Lample, 2019; Liu et al., ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. However, there is a discrepancy between the training objective of sequence-"
2021.findings-acl.373,2021.acl-long.266,1,0.684327,"of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test sets of the IT, Koran, law, and subtitle domains are with 2,000 examples respectively. For training R ANDOM, we used the"
2021.findings-acl.373,2020.emnlp-main.6,0,0.476957,"Missing"
2021.findings-acl.373,P16-1154,0,0.027074,", 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even t"
2021.findings-acl.373,2020.acl-main.244,0,0.0313561,"Missing"
2021.findings-acl.373,P19-1356,0,0.0328339,"Missing"
2021.findings-acl.373,W18-2709,0,0.0142241,"wles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text contains more copying tokens, which sheds light upon future works. 5 Conclusion and Future Work We find that NMT models with pre-training are prone to generate more copying tokens. We introduce a copying ratio and a copying error rate to quantitatively analyze copying behaviors in NMT evaluation. In addition, a simple and effective copying penalty is proposed to enhance the copying behaviors during model inference. Experimental results prove th"
2021.findings-acl.373,D18-1339,0,0.0316042,"is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text c"
2021.findings-acl.373,W17-3204,0,0.0160039,"raining on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah"
2021.findings-acl.373,D11-1034,0,0.0335393,"Missing"
2021.findings-acl.373,2020.acl-main.703,0,0.0349923,"re-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answerin"
2021.findings-acl.373,2020.emnlp-main.210,0,0.43631,"NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural lang"
2021.findings-acl.373,2020.tacl-1.47,0,0.0560822,"Missing"
2021.findings-acl.373,2021.ccl-1.108,0,0.0281972,"Missing"
2021.findings-acl.373,2020.amta-research.14,0,0.0673361,"Missing"
2021.findings-acl.373,W19-5333,0,0.0331562,"Missing"
2021.findings-acl.373,N19-4009,0,0.0143054,"of pre-training on NMT in the perspective of copying behaviors. We expect to provide more evidence for controlling the copying behaviors of NMT models. 2.1 Experimental Setup Data We conducted experiments on the widelyused WMT14 English-German benchmark. We used the processed data provided by Vaswani et al. (2017), which consists of 4.5M sentence pairs.1 We used all the training data for model training. The validation set is newstest2013 of 3,000 examples and the test set is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a tra"
2021.findings-acl.373,W18-6301,0,0.122055,"is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a training batch size of 131K tokens. The hyperparameters keep the same with R ANDOM except the 0.2 label smoothing, 2500 warm-up steps, and 1e-4 maximum learning rate. Evaluation For each model, we selected the checkpoint with the lowest perplexity on the validation set for testing. The beam size is 5 and the length penalty is 0.6. In addition to report1 https://drive.google.com/uc?id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 2 https://github.com/pytorch/fairseq 3 https://github.com/pytor"
2021.findings-acl.373,P02-1040,0,0.11011,"rseq/ blob/master/examples/scaling_nmt/README. md#3-train-a-model 4266 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of"
2021.findings-acl.373,N18-1202,0,0.00927479,"020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural language inference (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monoli"
2021.findings-acl.373,W18-6319,0,0.0179225,"P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. The denominator is the total number of tokens in output sentences. In general, higher Ratio values indicate more copying behaviors produced by the NMT model, and vice versa. Copying E"
2021.findings-acl.373,2020.tacl-1.40,0,0.0209978,"ying errors and thus the BLEU scores get a sharp degradation when setting the copying penalty Figure 3: BLEU scores of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test se"
2021.findings-acl.373,2020.tacl-1.18,0,0.136916,"ll the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of f"
2021.findings-acl.373,W16-2323,0,0.0591289,"tion, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding"
2021.findings-acl.373,2006.amta-papers.25,0,0.166465,"6 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. T"
2021.findings-acl.373,N03-1033,0,0.0595807,"Missing"
2021.findings-acl.373,2020.emnlp-main.208,0,0.123844,"setting CP to 1.2). One possible reason is that the IT domain needs to copy more tokens from the source sentence than translating sentences from other domains, thus the copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Tr"
2021.findings-acl.373,W19-5208,0,0.238508,"Missing"
2021.findings-acl.422,W19-5206,0,0.108278,"e bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect"
2021.findings-acl.422,C18-1111,0,0.0181722,"performance drop of backtranslation, as well as the different performances between tagged forward-translation and tagged back-translation (Caswell et al., 2019). In addition, we show that our approach is also beneficial for data augmentation approaches, which can further improve the translation performance over both back-translation and forward-translation. 5.3 Domain Adaptation Since high-quality and domain-specific parallel data is usually scarce or even unavailable, domain adaptation approaches are generally employed for translation in low-resource domains by leveraging out-of-domain data (Chu and Wang, 2018). Languages can be also regarded as different domains, since articles in different languages cover different topics (Bogoychev and Sennrich, 2019). Starting from this intuition, we distinguish examples with different original languages with tagging (Aharoni et al., 2019) and fine-tuning (Luong and Manning, 2015), which are commonly-used in domain adaptation and multi-lingual translation tasks. Our work also benefits domain adaptation: distinguishing original languages in general domain 4785 data consistently improves translation performance of NMT models in several specific domains (Table 16 i"
2021.findings-acl.422,D18-1045,0,0.0469535,"this section, we aim to provide some insights where monolingual data augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6"
2021.findings-acl.422,N19-1388,0,0.0767514,"validation sets. the monolingual data augmentation scenario (Section 4.2), where the language coverage bias problem is more severe due to the newly introduced dataset in source or target language. 4.1 Bilingual Data Utilization In this section, we aim to improve bilingual data utilization through explicitly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total trai"
2021.findings-acl.422,P07-2045,0,0.0080886,"Missing"
2021.findings-acl.422,2021.eacl-main.130,0,0.0136043,"(2020) show that the source-side translationese texts can potentially lead to distortions in automatic and human evaluations. Accordingly, the WMT competition starts to use only source-original test sets for most translation directions since 2019. Our study reconfirms the necessity of distinguishing the source- and target-original examples and takes one step further to distinguish examples in training data. Complementary to previous works, we investigate the effect of language coverage bias on machine translation, which is related to the content bias rather than the language style difference. Shen et al. (2021) also reveal the context mismatch between texts from different original languages. To alleviate this problem, they proposed to combine back- and forward-translation by introducing additional monolingual data, while we focus on better exploiting bilingual data by distinguishing the original languages, which is also helpful for back- and forward-translation. Lembersky et al. (2011, 2012) propose to adapt machine translation systems to generate texts that are more similar to human-translations, while Riley et al. (2020) propose to model human-translated texts and original texts as separate langua"
2021.findings-acl.422,2020.wmt-1.30,0,0.0343531,"Missing"
2021.findings-acl.422,D11-1034,0,0.080019,"y crabs are the most well-known image spokesmen of Bacheng. Both It is the best-known icon of Bacheng. Table 5: An example of the outputs of NMT models trained on different sets of data. Using the targetoriginal data tends to omit content words. naturally arises: can target-original bilingual data improve the fluency of NMT models? To answer the above question, we measure the fluency of outputs with language models trained on the monolingual data as described in Section 2. Previous study finds that different perplexities could be caused by specific contents rather than structural differences (Lembersky et al., 2011). Specifically, some source-original contents are of low frequency in the target-language monolingual data (e.g., “Bacheng” in Table 5), thus the language model trained on the target-language monolingual data tends to assign higher perplexities to outputs containing more source-original content words. To rule out this possibility and check whether the outputs are structurally different, we follow Lembersky et al. (2011) to abstract away from the contentspecific features of the outputs to measure their fluency at the syntactic level. Table 6 shows the results. Although using only the source-ori"
2021.findings-acl.422,W19-6627,0,0.115311,"t the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,E12-1026,0,0.0480962,"Missing"
2021.findings-acl.422,2015.iwslt-evaluation.11,0,0.170579,"ly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total training steps of the pre-training and fine-tuning stages are the same as the baseline. Analysis Recent studies have shown that generating human-translation like texts as opposed to original texts can improve the BLEU score (Riley et al., 2020). To validate that the improvement is partially from alleviating the c"
2021.findings-acl.422,2020.acl-main.532,0,0.193192,"the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted"
2021.findings-acl.422,W19-5208,0,0.392807,"plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,2020.wmt-1.25,0,0.0648905,"Missing"
2021.findings-acl.422,N19-4007,0,0.0685171,"ly, if the language coverage bias exists, the vocabulary distributions of the source- and target-original data should differ greatly from each other, since the covered issues tend to have different frequencies between them (D’Alessio and Allen, 2000). We use the Jensen-Shannon (JS) divergence (Lin, 1991) to measure the difference between two vocabulary En⇒Zh En⇐Zh Origin noun verb adj noun verb adj Target Source Both 67.6 69.7 69.9 52.0 54.0 54.1 64.3 66.2 65.9 53.8 61.8 61.2 38.0 44.1 43.8 57.0 63.9 63.4 Table 4: Translation adequacy of different types of content words measured by F-measure (Neubig et al., 2019). The results are reported on the validation sets. distributions p and q:   p+q p+q 1 KL(p|| ) + KL(q|| ) , JS (p||q) = 2 2 2 where KL(·||·) is the KL divergence (Kullback and Leibler, 1951) of two distributions. Table 2 shows the JS divergence of the vocabulary distributions between the source- and targetoriginal data. We also divide the words into content words and functions words based on their POS tags, since content words are more related to the language coverage bias, while the function words are more related to the stylistic and structural differences between the translationese and or"
2021.findings-acl.422,W19-5333,0,0.0130493,"each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1"
2021.findings-acl.422,W18-6301,0,0.0117812,". For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Se"
2021.findings-acl.422,W18-6319,0,0.0201539,"nce pairs for En⇔De, En⇔Zh, and En⇔Ja, respectively. We used the monolingual data that is publicly available in WMT20 to train the proposed original language detection model (Section 3.1) and data augmentation (Section 4.2). The Appendix lists details about the data preprocessing. For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original la"
2021.findings-acl.422,2020.findings-emnlp.276,0,0.057633,"Missing"
2021.findings-acl.422,2020.acl-main.691,0,0.33119,"followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1 Detecting Original Languages Detection Method Intuitively, we use a largescale monolingual dataset to estimate the distribution of the contents covered by each language. For each training example, we compare its similarities 4779 WMT20 WMT20 En=&gt;Zh En=&gt;Zh En-Zh En-Ja En-De FT Ours 83.6 84.4 83.7 91.5 86.6 88.7 WMT20 WMT20 Zh=&gt;En Zh=&gt;En 37 37 28 28 36.5 36.5 32 32 Table 1: F1 scores of detecting original languages in the test sets. “FT” denotes the forward translation classifier proposed by Riley et al. (2020). 33.2 33.2 31.0 31.0 P (Ds )P (hx, yi|Ds ) , P (hx, yi) P (Dt )P (hx, yi|Dt ) P"
2021.findings-acl.422,P16-1009,0,0.0613289,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-acl.422,P16-1162,0,0.0390796,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-emnlp.247,W19-5206,0,0.177765,"T and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and EnglishRussian benchmarks show that PT can nicely cowork with BT, leading to state-of-the-art model performances. Extensive analyses show that the tagging mechanism is helpful for enhancing the complementarity between PT and BT by improving the translation of source-original sentences and low-frequency words. Our main contributions are as follows: • We design two probing tasks to investigate the impact of PT and BT on NMT models. • We empirically demonstrate the complementarity between PT and BT. • We show that Tagged BT further improve"
2021.findings-emnlp.247,2021.emnlp-main.263,1,0.833334,"Missing"
2021.findings-emnlp.247,D18-1045,0,0.452877,"Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the"
2021.findings-emnlp.247,2020.acl-main.253,0,0.0509777,"Missing"
2021.findings-emnlp.247,D18-1040,0,0.0187186,"l parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with p"
2021.findings-emnlp.247,2020.emnlp-main.6,0,0.0112886,"ed source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, especially on source-original sentences. Takeaway: 1) PT and BT complementary in terms of originality of sentences; 2) Tagged BT can allevi"
2021.findings-emnlp.247,2021.acl-long.221,1,0.810105,"Missing"
2021.findings-emnlp.247,2020.emnlp-main.210,0,0.605565,"laborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/2K sentence pairs, while those of En-Ru include 2M/3K/3K pairs. Towards better reproducibility, we directly used the BT data provided by Sennrich et al. (2016a)1 , consisting of 2.3M synt"
2021.findings-emnlp.247,2020.acl-main.41,1,0.673903,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2021.findings-acl.373,1,0.886854,"o learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/"
2021.findings-emnlp.247,2020.tacl-1.47,0,0.265899,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2020.acl-main.532,0,0.0613034,"Missing"
2021.findings-emnlp.247,N19-4007,0,0.0122229,"riginal while “All” means the whole testset. Table 2: Translation quality on the En-Ro and En-Ru benchmarks. “+” means incorporating PT and (Tagged) BT into NMT models. 4.2 All Src-Ori denotes the testing data originating in the source language, while Tgt-Ori denotes the data translating from the target language. 2903 Effects of Word Frequency Data augmentation is an effective way to improve the translation quality of low-frequency words (Sennrich et al., 2016b). Thus, we compare the performance of the models on translating different frequencies of words. Specifically, we employed compare-mt (Neubig et al., 2019) to calculate the f-measure of translating low- and high-frequency words (&lt;50 vs. ≥50). As shown in Table 4, PT improves more on translating low-frequency words (58.2 vs. 57.5 scores) while BT performs better on high-frequency words (67.3 vs. 66.7 scores). Furthermore, the combination of PT and tagged BT achieves the best performance on both low- and high-frequency words, leading to an overall improvement on the whole words. Similar phenomenons can be observed by combining self-training and BT (Ding et al., 2021b). Takeaway: 1) PT and BT complementary in terms of frequency of words; 2) Tagged"
2021.findings-emnlp.247,P02-1040,0,0.109581,"ir comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand th"
2021.findings-emnlp.247,W18-6319,0,0.0200641,"., 2020b). Setting To make a fair comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In th"
2021.findings-emnlp.247,2020.tacl-1.18,0,0.187012,"tiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conduct"
2021.findings-emnlp.247,W16-2323,0,0.284038,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1009,0,0.470578,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1162,0,0.668047,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,2006.amta-papers.25,0,0.0314185,"hitectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences"
2021.findings-emnlp.247,D19-1149,0,0.0464751,"Missing"
2021.findings-emnlp.247,D19-1073,0,0.0181896,"n the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with parameter initializa"
2021.findings-emnlp.247,2021.findings-acl.422,1,0.73879,"29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, esp"
2021.findings-emnlp.247,2020.emnlp-main.208,0,0.143525,"al., 2019), which can ac2900 ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2900–2907 November 7–11, 2021. ©2021 Association for Computational Linguistics quire knowledge from unlabeled monolingual data, has shown its effectiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation direct"
2021.findings-emnlp.247,D16-1160,0,0.0238903,"Missing"
2021.findings-emnlp.247,W19-5208,0,0.0274486,"e-art performances on the two benchmarks. Similar tendencies are observed in terms of the TER scores. The above results illustrate the better complementarity between PT and Tagged BT on improving translation quality for NMT models. Analysis We conducted extensive analyses to better understand the improvement of our approach. All results are reported on the En-Ro benchmark. Effects of Sentence Type Recent studies have shown that the evaluation of BT is sensitive to the sentences types, thus we report BLEU scores on the subsets of source-original (Src-Ori) and targetoriginal (Tgt-Ori) datasets (Zhang and Toral, 2019; Model Vanilla + PT + BT + BT + PT + Tagged BT + Tagged BT + PT Tgt 33.7 37.7 38.4 41.2 38.6 41.6 29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and B"
2021.naacl-main.313,2020.emnlp-main.176,1,0.896512,"Missing"
2021.naacl-main.313,D19-1448,0,0.0232308,"Missing"
2021.naacl-main.313,D19-1145,1,0.871511,"Missing"
2021.naacl-main.313,D19-1573,0,0.039367,"Missing"
2021.naacl-main.313,P19-1125,0,0.0559868,"coding. A line of research (Wang et al., 2019b; Guo et al., 2020; Ding et al., 2020) holds the opinion that the lack of contextual dependency on target sentences potentially leads to the deteriorated performance of NAT models. To boost the NAT translation performance, many recent works resort to the knowledge transfer from a well-trained AT model. Typical knowledge transfer methods include sequence-level knowledge distillation with translation outputs generated by strong AT models (Gu et al., 2019; Ghazvininejad et al., 2019), word-level knowledge distillation with AT decoder representations (Wei et al., 2019; Li et al., 2019), and fine-tuning on AT model by curriculum learning (Guo et al., 2020), etc. In this work, we first verify our our hypothesis that AT and NAT encoders – although they belong ∗ The first two authors contributed equally to this work. to the same sequence-to-sequence learning task This work was conducted when Yongchang Hao, Shilin He, – capture different linguistic properties of source and Wenxiang Jiao were interning at Tencent AI Lab. 1 Code is publicly available at https://github. sentences. We conduct our verification by evaluatcom/yongchanghao/multi-task-nat ing the encode"
2021.naacl-main.313,2020.autosimtrans-1.4,0,0.0333223,"erate output tokens one by one following the left to right direction (Vaswani et al., 2017; Bahdanau et al., 2015), but it is often criticized for its slow inference speed (Gu et al., 2018). The second type non-autoregressive translation (NAT) models adopt a parallel decoding algorithm to produce output tokens simultaneously (Gu et al., 2019; Ghazvininejad et al., 2019; Ma et al., 2020), but the translation quality of it is often inferior to auto-regressive models (Gu et al., 2018). Many researchers have investigated the collaboration between AT and NAT models. For instance, E NCODER -NAD-AD (Zhou et al., 2020) leverages NAT models to improve the performance of AT. Specifically, their method inserts a NAT decoder between the conventional AT encoder and decoder to generate coarse target sequences for the final autoregressive decoding. A line of research (Wang et al., 2019b; Guo et al., 2020; Ding et al., 2020) holds the opinion that the lack of contextual dependency on target sentences potentially leads to the deteriorated performance of NAT models. To boost the NAT translation performance, many recent works resort to the knowledge transfer from a well-trained AT model. Typical knowledge transfer met"
2021.naacl-main.313,N19-4007,0,0.0141058,"ements demonstrate the effectiveness of our proposed model using multi-task learning. 4.2 Main Result We further evaluate the proposed M ULTI -TASK NAT model with the standard practice of knowledge distillation. Table 3 depicts the performances of our model as well as strong baseline models. Our proposed M ULTI -TASK NAT model achieves a significant improvement of 0.80 and 0.41 BLEU point over the strong baseline M ASK -P REDICT model on En⇒De and De⇒En translation. On En⇔Ro translation, our model outperforms the baseline model by 0.77 and 0.89 BLEU scores respectively. We use the compare-mt (Neubig et al., 2019)2 to determine the significance. Details for significance tests are described in Appendix A.4. 2 https://github.com/neulab/compare-mt Task O URS ∆AT ∆N AT Surface SeLen WC 94.4 79.3 2.7 3.3 1.0 0.2 Syntactic TrDep ToCo BShif 47.2 79.3 74.7 1.4 1.0 -0.1 1.2 -0.4 1.3 Semantic Tense SubN ObjN SoMo CoIn 88.9 87.1 85.8 54.8 63.3 -0.3 0.9 0.6 0.8 -1.6 -0.3 -0.4 0.5 1.8 0.5 Table 4: Performance on the probing tasks of our M ULTI - TASK NAT model. ∆AT and ∆N AT denote the relative increase over the AT and NAT probing performance, respectively. 4.3 Analysis We conduct probing tasks to empirically recon"
2021.naacl-main.313,N19-4009,0,0.0570055,"Missing"
2021.naacl-main.313,P02-1040,0,0.110044,"sentence with knowledge distillation (4.2). During the model inference, we only use the NAT decoder to generate the target tokens simultaneously while ignoring the AT decoder. Therefore, the inference overhead is the same as the NAT model before sharing. 4 Experiment We conducted experiments on two widely used WMT14 English⇔German and WMT16 English⇔Romanian benchmark datasets, which consist of 4.5M and 610K sentence pairs, respectively. We applied BPE (Sennrich et al., 2016) with 32K merge operations for both language pairs. The experimental results are evaluated in case-sensitive BLEU score (Papineni et al., 2002). We use T RANSFORMER (Vaswani et al., 2017) as our baseline autoregressive translation model and the M ASK -P REDICT (Ghazvininejad et al., 2019) as our baseline non-autoregressive model. We integrate the T RANSFORMER decoder into the M ASK -P REDICT to implement the proposed M ULTI -TASK NAT model. For λt , we use the annealing scheme described in Section 3. Since the major NAT architecture of our method is exactly the M ASK -P REDICT model, any established decoding latency results (Kasai et al., 2021) for M ASK -P REDICT can also be applied to ours. All of the parameters are randomly initia"
2021.naacl-main.313,W18-5431,0,0.0256365,"nd NAT encoders from another perspective. The AT and NAT models referred to in the following experiments are T RANSFORMER and M ASK -P REDICT. We train the models on the WMT14 English⇒German dataset, and the details of the experiments are introduced in the Appendix. Surface SeLen WC 91.7 76.0 93.4 79.1 Syntactic TrDep ToCo BShif 45.8 78.3 74.8 46.0 79.7 73.4 Semantic Tense SubN ObjN SoMo CoIn 89.2 86.2 85.2 54.0 64.9 89.2 87.5 85.3 53.0 62.8 Table 1: Performance on the probing tasks of evaluating linguistic properties embedded in the learned representations of AT and NAT models. et al., 2018; Raganato and Tiedemann, 2018) for AT and NAT models. Further, by leveraging the linguistic differences, we then adopt a multi-task learning framework with a shared encoder (i.e., M ULTI TASK NAT) to transfer the AT model knowledge into the NAT model. Specifically, we employ an additional AT task as the auxiliary task of which the encoder parameters are shared with the NAT task while parameters of the decoder are exclusive. Since many works (Cipolla et al., 2018; Liu et al., 2019) suggest that the weights for each task are critical to the multi-task learning, in this work, the multi-task weight assigned to the AT task is d"
2021.naacl-main.313,N19-1329,0,0.0451126,"Missing"
2021.naacl-main.313,P16-1162,0,0.0207897,"T14 De⇒En test sets without knowledge distillation. Y can be either the target sentence in the raw training data (4.1) or the generated target sentence with knowledge distillation (4.2). During the model inference, we only use the NAT decoder to generate the target tokens simultaneously while ignoring the AT decoder. Therefore, the inference overhead is the same as the NAT model before sharing. 4 Experiment We conducted experiments on two widely used WMT14 English⇔German and WMT16 English⇔Romanian benchmark datasets, which consist of 4.5M and 610K sentence pairs, respectively. We applied BPE (Sennrich et al., 2016) with 32K merge operations for both language pairs. The experimental results are evaluated in case-sensitive BLEU score (Papineni et al., 2002). We use T RANSFORMER (Vaswani et al., 2017) as our baseline autoregressive translation model and the M ASK -P REDICT (Ghazvininejad et al., 2019) as our baseline non-autoregressive model. We integrate the T RANSFORMER decoder into the M ASK -P REDICT to implement the proposed M ULTI -TASK NAT model. For λt , we use the annealing scheme described in Section 3. Since the major NAT architecture of our method is exactly the M ASK -P REDICT model, any estab"
C10-1123,P05-1067,0,0.0626954,"string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rul"
C10-1123,P08-1115,0,0.0433796,"endency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significant improvements over that of using 1-best trees on the NIST 2004/200"
C10-1123,N04-1035,0,0.0493122,".append(r) keep k-best dependency structures for v It is difficult to assign a probability to each hyperedge. The current method is arbitrary, and we will improve it in the future. 4 Forest-based Rule Extraction In tree-based rule extraction, one just needs to first enumerate all bilingual phrases that are consistent with word alignment and then check whether the dependency structures over the target phrases are well-formed. However, this algorithm fails to work in the forest scenario because there are usually exponentially many well-formed structures over a target phrase. The GHKM algorithm (Galley et al., 2004), which is originally developed for extracting treeto-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). The algorithm distinguishes between minimal and composed rules. Although there are exponentially many composed rules, the number of minimal rules extracted from each node is rather limited (e.g., one or zero). Therefore, one can obtain promising composed rules by combining minimal rules. Unfortunately, the GHKM algorithm cannot be applied to extracting string-to-dependency rules from dependency forests. This is because the GHKM al"
C10-1123,W05-1506,0,0.411678,"i denotes that he0,1 , boy2,4 , and with4,7 are dependants (from left to right) of saw0,7 . More formally, a dependency forest is a pair hV, Ei, where V is a set of nodes, and E is a set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of wi,j , which denotes that w dominates the substring from positions i through j (i.e., wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head(e)i, where head(e) ∈ V is the head and tails(e) ∈ V are its dependants. A dependency forest has a structure of a hypergraph such as packed forest (Klein and Manning, 2001; Huang and Chiang, 2005). However, while each hyperedge in a packed forest naturally treats the corresponding PCFG rule probability as its weight, it is challenging to make dependency forest to be a weighted hypergraph because dependency parsers usually only output a score, which can be either positive or negative, for each edge in a dependency tree rather than a hyperedge in a 1094 saw0,7 e1 he0,1 Algorithm 1 Forest-based Initial Phrase Extraction e2 boy2,4 e3 boy2,7 e4 a2,3 with4,7 e5 he ta saw a boy kandao yige dai with 1: 2: 3: telescope5,7 4: e6 5: 6: a5,6 7: 8: a telescope 9: 10: 11: wangyuanjing de nanhai Figu"
C10-1123,D09-1127,1,0.908687,"Missing"
C10-1123,W01-1812,0,0.060847,"y2,4 , with4,7 ), saw0,7 i denotes that he0,1 , boy2,4 , and with4,7 are dependants (from left to right) of saw0,7 . More formally, a dependency forest is a pair hV, Ei, where V is a set of nodes, and E is a set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of wi,j , which denotes that w dominates the substring from positions i through j (i.e., wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head(e)i, where head(e) ∈ V is the head and tails(e) ∈ V are its dependants. A dependency forest has a structure of a hypergraph such as packed forest (Klein and Manning, 2001; Huang and Chiang, 2005). However, while each hyperedge in a packed forest naturally treats the corresponding PCFG rule probability as its weight, it is challenging to make dependency forest to be a weighted hypergraph because dependency parsers usually only output a score, which can be either positive or negative, for each edge in a dependency tree rather than a hyperedge in a 1094 saw0,7 e1 he0,1 Algorithm 1 Forest-based Initial Phrase Extraction e2 boy2,4 e3 boy2,7 e4 a2,3 with4,7 e5 he ta saw a boy kandao yige dai with 1: 2: 3: telescope5,7 4: e6 5: 6: a5,6 7: 8: a telescope 9: 10: 11: wa"
C10-1123,D09-1106,1,0.801307,"we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significant improvements over that of using 1-best trees on the NIST 2004/2005/2006 Chinese-English test sets. 2 Background Figure 1 shows a dependency tree o"
C10-1123,D08-1022,0,0.607088,"rk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significan"
C10-1123,P00-1056,0,0.135647,"Each n-gram (e.g., “boy-as-head a”) is assigned the same fractional count of the hyperedge it belongs to. We also tried training dependency language model as in (Shen et al., 2008), which means all hyperedges were on equal footing without regarding probabilities. However, the performance is about 0.8 point lower in BLEU. One possbile reason is that hyperedges with probabilities could distinguish high quality structures better. 6 Experiments 6.1 Results on the Chinese-English Task We used the FBIS corpus (6.9M Chinese words + 8.9M English words) as our bilingual training corpus. We ran GIZA++ (Och and Ney, 2000) to obtain word alignments. We trained a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2004/2005/2006 test sets. To obtain dependency trees and forests, we parsed the English sentences of the FBIS corpus using a shift-reduce dependency"
C10-1123,P02-1038,0,0.220964,"Missing"
C10-1123,J04-4002,0,0.0457485,"dency structure di..j is fixed on head h, where h ∈ / [i, j], or fixed for short, if and only if it meets the following conditions telescope a (b) (c) and the word alignments between them. To facilitate identifying the correspondence between the English and Chinese words, we also gives the English sentence. Extracting string-to-dependency rules from aligned string-dependency pairs is similar to extracting SCFG (Chiang, 2007) except that the target side of a rule is a well-formed structure. For example, we can first extract a string-todependency rule that is consistent with the word alignment (Och and Ney, 2004): with ((a) telescope) → dai wangyuanjing de Then a smaller rule (a) telescope → wangyuanjing can be subtracted to obtain a rule with one nonterminal: • dh ∈ / [i, j] with (X1 ) → dai X1 de • ∀k ∈ [i, j] and k 6= h, dk ∈ [i, j] • ∀k ∈ / [i, j], dk = h or dk ∈ / [i, j] Definition 2. A dependency structure di..j is floating with children C, for a non-empty set C ⊆ {i, ..., j}, or floating for short, if and only if it meets the following conditions • ∃h ∈ / [i, j], s.t.∀k ∈ C, dk = h • ∀k ∈ [i, j] and k ∈ / C, dk ∈ [i, j] • ∀k ∈ / [i, j], dk ∈ / [i, j] A dependency structure is well-formed if and"
C10-1123,P02-1040,0,0.0903191,"Missing"
C10-1123,W06-1608,0,0.0253466,"age of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed fo"
C10-1123,P05-1034,0,0.308079,"s. Our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits"
C10-1123,P08-1066,0,0.574261,"stem obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency la"
C10-1123,D07-1078,0,0.013782,"ructures of a head can be constructed from those of its dependants. For example, in Figure 4, as the fixed structure rooted at telescope 5,7 is (a) telescope we can obtain a fixed structure rooted for the node with4,7 by attaching the fixed structure of its dependant to the node (EnumFixed in line 4). Figure 2(b) shows the resulting fixed structure. Similarly, the floating structure for the node saw0,7 can be obtained by concatenating the fixed structures of its dependants boy2,4 and with4,7 (EnumFloating in line 5). Figure 2(c) shows the resulting fixed structure. The algorithm is similar to Wang et al. (2007), which binarize each constituent node to create some intermediate nodes that correspond to the floating structures. Therefore, we can find k-best fixed and floating structures for a node in a dependency forest by manipulating the fixed structures of its dependants. Then we can extract string-to-dependency rules if the dependency structures are consistent with the word alignment. How to judge a well-formed structure extracted from a node is better than others? We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. Given a tree fragment t, we use the inside-ou"
C10-1123,P08-1023,1,\N,Missing
C10-1123,P03-1041,0,\N,Missing
C10-1123,P08-1067,0,\N,Missing
C10-1123,W06-1606,0,\N,Missing
C10-1123,P05-1033,0,\N,Missing
C10-1123,P08-1010,0,\N,Missing
C10-1123,J07-2003,0,\N,Missing
C10-1123,2008.amta-papers.18,0,\N,Missing
C12-2122,P06-1002,0,0.0193855,"nother alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or re"
C12-2122,H05-1009,0,0.0216,"stem combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (D"
C12-2122,P06-1009,0,0.0532677,"Missing"
C12-2122,J93-2003,0,0.0478199,"Missing"
C12-2122,P05-1033,0,0.441175,"robabilities. Instead of extracting phrase pairs that respect the word alignment, Tu et al. (2011) enumerate all potential phrase pairs and calculate their fractional counts. As they soften the alignment consistency constraint, there exists a massive number of phrase pairs extracted from the training corpus. To maintain a reasonable phrase table size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments"
C12-2122,P05-1066,0,0.166329,"Missing"
C12-2122,P11-1043,0,0.0377862,"he process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weigh"
C12-2122,C10-1036,0,0.01903,"2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset"
C12-2122,P08-1115,0,0.0280219,"5; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minim"
C12-2122,D09-1115,1,0.846211,"). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering mo"
C12-2122,N09-2064,0,0.13425,"Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary nu"
C12-2122,J07-3002,0,0.0988242,"igne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or removed from the set of phr"
C12-2122,P06-1121,0,0.10519,"Missing"
C12-2122,P11-1127,0,0.0185151,"s high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; F"
C12-2122,D08-1011,0,0.0397351,"Missing"
C12-2122,W99-0623,0,0.0194602,"eference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alig"
C12-2122,2006.amta-papers.8,0,0.0607293,"Missing"
C12-2122,N03-1017,0,0.0694144,"nd phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-be"
C12-2122,N04-1022,0,0.0650294,"ell together: alignment refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to"
C12-2122,N06-1014,0,0.0889621,"ce pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 a"
C12-2122,P06-1077,1,0.905513,"Missing"
C12-2122,J10-3002,1,0.857717,"n the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 1254 Alignments GIZA++ Berkeley Vigne Selecti"
C12-2122,D09-1106,1,0.959517,"t al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alig"
C12-2122,D08-1022,0,0.0291746,"en explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment s"
C12-2122,P06-1065,0,0.0455271,"Missing"
C12-2122,P03-1021,0,0.0759237,"size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined al"
C12-2122,J03-1002,0,0.167522,"at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), wor"
C12-2122,J04-4002,0,0.308906,"Missing"
C12-2122,P02-1040,0,0.0852613,"ts 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice,"
C12-2122,N07-1029,0,0.0897098,"s calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, wh"
C12-2122,N06-2033,0,0.0331853,"kel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approa"
C12-2122,P08-1066,0,0.102941,"Missing"
C12-2122,P12-1025,0,0.0250082,"using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into ac"
C12-2122,H05-1010,0,0.073942,"Missing"
C12-2122,D08-1065,0,0.016802,"refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard"
C12-2122,C10-1123,1,0.860759,"ly (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to"
C12-2122,I11-1145,1,0.883984,"ur technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alignment ai using the"
C12-2122,C96-2141,0,0.530719,"Missing"
C12-2122,C10-2154,0,0.0147392,"ave the same value. 2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note"
C12-2122,P06-1066,1,0.854268,"Missing"
D17-1149,P17-2021,0,0.00734708,"ts of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016)"
D17-1149,D16-1162,0,0.0486271,"ropose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpo"
D17-1149,J93-2003,0,0.107649,"e, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine tra"
D17-1149,P05-1033,0,0.0429496,"the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavi"
D17-1149,P16-1160,0,0.0905162,"o a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework"
D17-1149,P16-2058,0,0.0325284,"Missing"
D17-1149,P15-1002,0,0.0266067,"rporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT."
D17-1149,P16-1078,0,0.0231179,"istic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of appro"
D17-1149,D16-1249,0,0.0212281,"ich dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or"
D17-1149,P16-5005,0,0.0185392,"Missing"
D17-1149,C16-1172,0,0.0273232,"Missing"
D17-1149,P16-1154,0,0.0169774,"e log-likelihood: C(θ) = Ty N X X n=1 i=1 n log P (yin |y&lt;i , xn ) (5) given the training data with N bilingual sentences (Cho, 2015). In the testing phase, given a source sentence x, we use beam search strategy to search a target senˆ that approximately maximizes the conditence y tional probability P (y|x) ˆ = argmax P (y|x) y y 3 (6) Approach In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT. 1422 The balancing weight λ is produced by the balancer – a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci , the previous decoding state si−1 and the previous generated word yi−1 : λi = σ(fb (si , yi−1 , ci )) (8) where σ(·) is a sigmoid function and fb (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be high if the phra"
D17-1149,P16-1014,0,0.00748876,"model by maximizing the log-likelihood: C(θ) = Ty N X X n=1 i=1 n log P (yin |y&lt;i , xn ) (5) given the training data with N bilingual sentences (Cho, 2015). In the testing phase, given a source sentence x, we use beam search strategy to search a target senˆ that approximately maximizes the conditence y tional probability P (y|x) ˆ = argmax P (y|x) y y 3 (6) Approach In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT. 1422 The balancing weight λ is produced by the balancer – a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci , the previous decoding state si−1 and the previous generated word yi−1 : λi = σ(fb (si , yi−1 , ci )) (8) where σ(·) is a sigmoid function and fb (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be"
D17-1149,P15-1001,0,0.0571924,"ral sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT po"
D17-1149,D13-1176,0,0.164447,"Missing"
D17-1149,P03-1021,0,0.0613915,"n source and target words, which is derived from attention distribution produced by the NMT model (Wang et al., 2017). SMT coverage vector in (Wang et al., 2017) is also introduced to avoid repeat phrasal recommendations. In our work, the potential phrase is phrase with high SMT score which is defined as following: SM Tscore (pl |y&lt;t , x) = M X m=1 wm hm (pl , x(pl )) (10) where pl is a target phrase and x(pl ) is its corresponding source span. hm (pl , x(pl )) is a SMT feature function and wm is its weight. The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och, 2003). This leads to a better interaction between SMT and NMT models. It should be emphasized that our memory is dynamically updated at each decoding step based on the decoding history from both SMT and NMT models. The proposed model is very flexible, where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary, or any other bilingual resources storing idiomatic translations or bilingual multi-word expressions, which may lead to a further improvement. 2 Reading Phrase Memory When phrases are read from the memory, they are rescor"
D17-1149,W16-2209,0,0.161524,"d model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below. Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase “ &E S (information security)” is tagged as a noun phrase “NP”, and the tag sequence should be “NP B NP”. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows: [E w xi , E t ti ] 1 (9) Overlapped phrases may result in a high dimensionality in translation hypothesis representation and make it hard to employ shared fragments for efficient dynamic programming. 1423 NMT |is a word embedding where E w ∈ Rdw×|V matrix and dw is the word embedding dimensionT AG | ality, E t ∈ Rdt×|V is a tag embedding matrix and dt is the tag embedding dimensionality. [·] is the"
D17-1149,P16-1162,0,0.0342203,"ory that stores phrase pairs in symbolic forms for NMT. During decoding, the NMT decoder enquires the phrase memory and properly generates phrase translations. The significant differences between these efforts and ours are 1) that we dynamically generate phrase translations via an SMT model, and 2) that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation. Incorporating linguistic information into NMT NMT is essentially a sequence to sequence mapping network that treats the input/output units, eg., words, subwords (Sennrich et al., 2016), characters (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propo"
D17-1149,E17-2058,0,0.0163956,"Missing"
D17-1149,N03-1017,0,0.0969388,"enerates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al."
D17-1149,P16-2049,0,0.093448,"phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-byword generation framework, we propose a novel architecture that integrates a phrase-based SMT 1421 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics model into NMT. Specifically, we add an auxiliary phrase memory to store target phrases i"
D17-1149,P17-1064,1,0.855535,"this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NM"
D17-1149,C16-1291,0,0.0145267,"ssigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or a word from the voc"
D17-1149,P16-1100,0,0.024674,"uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-by"
D17-1149,Q17-1007,1,0.899484,"Missing"
D17-1149,P16-1008,1,0.871968,"Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations"
D17-1149,1983.tc-1.13,0,0.583759,"Missing"
D17-1149,P16-1007,0,0.0372004,"the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and"
D17-1149,W03-1728,0,0.0345713,"c programming, we restrict ourselves to non-overlap phrases.1 (2) We explicitly utilize the boundary information of the source-side chunk phrases, to better guide the proposed model to adopt a target phrase at an appropriate decoding step. (3) We enable the model to exploit the syntactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below. Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase “ &E S (information security)” is tagged as a noun phrase “NP”, and the tag sequence should be “NP B NP”. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows: [E w xi , E t ti ] 1 (9) Overlapped phrases may result i"
D17-1149,C16-1170,0,0.0367497,"Missing"
D17-1149,P17-2060,0,0.0224158,"2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the en"
D17-1149,P15-4025,0,0.0387348,"Missing"
D17-1149,P07-2045,0,\N,Missing
D17-1301,D17-1105,1,0.644977,"Missing"
D17-1301,W12-3156,0,0.0426269,"ation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received relatively little attention fr"
D17-1301,P05-1066,0,0.303284,"Missing"
D17-1301,P15-1166,0,0.0160558,"de and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role."
D17-1301,D13-1176,0,0.00976873,"rical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol en"
D17-1301,P07-2045,0,0.00724727,"Missing"
D17-1301,D15-1166,0,0.0309999,"s been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT model"
D17-1301,P02-1040,0,0.101177,"Missing"
D17-1301,E17-3017,0,0.0145815,"Missing"
D17-1301,P16-1008,1,0.843029,"NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding m"
D17-1301,2011.mtsummit-papers.13,0,0.0990302,"ical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received rel"
D17-1301,N16-1004,0,0.0544309,"or example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduc"
D18-1317,H93-1035,0,0.704743,"Missing"
D18-1317,P05-1066,0,0.251246,"Missing"
D18-1317,D18-1457,1,0.832959,"Missing"
D18-1317,N03-1017,0,0.0296402,"isagreement directly applies regularization on the outputs of each attention head, by maximizing the difference among them. Similar to the subspace strategy, we employ negative cosine similarity to measure the distance: Doutput # H H 1 X X Oi · Oj =− 2 . H kOi kkOj k (3) i=1 j=1 Related Work The regularization on attended positions is inspired by agreement learning in prior works, which encourages alignments or hidden variables of multiple models to be similar. Liang et al. (2006) first assigned agreement terms for jointly training word alignment in phrase-based statistic machine translation (Koehn et al., 2003). The idea was further extended into other natural language processing tasks such as grammar induction (Liang et al., 2008). Levinboim et al. (2015) extended the agreement for general bidirectional sequence alignment models with model inevitability regularization. Cheng et al. (2016) further explored the agreement on modeling the source-target and target-source alignments in neural machine translation model. In contrast to the mentioned approaches which assigned agreement terms into loss function, we deploy an alignment disagreement regularization by maximizing the distance among multiple atte"
D18-1317,N15-1063,0,0.0187409,"ace strategy, we employ negative cosine similarity to measure the distance: Doutput # H H 1 X X Oi · Oj =− 2 . H kOi kkOj k (3) i=1 j=1 Related Work The regularization on attended positions is inspired by agreement learning in prior works, which encourages alignments or hidden variables of multiple models to be similar. Liang et al. (2006) first assigned agreement terms for jointly training word alignment in phrase-based statistic machine translation (Koehn et al., 2003). The idea was further extended into other natural language processing tasks such as grammar induction (Liang et al., 2008). Levinboim et al. (2015) extended the agreement for general bidirectional sequence alignment models with model inevitability regularization. Cheng et al. (2016) further explored the agreement on modeling the source-target and target-source alignments in neural machine translation model. In contrast to the mentioned approaches which assigned agreement terms into loss function, we deploy an alignment disagreement regularization by maximizing the distance among multiple attention heads. As standard multi-head attention model lacks effective control on the influence of different attention heads, Ahmed et al. (2017) used"
D18-1317,N06-1014,0,0.398539,"V i and V j in different value subspaces, through the dot product of the normalized vectors1 , which measures the cosine of the angle between V i and V j . Thus, the cosine distance is defined as negative similarity, i.e, − cos(·). Our training objective is to enlarge the average cosine distance among all head pairs. The regularization term is formally expressed as: Dsubpace = − H H 1 XX V i · V j . H2 kV i kkV j k (1) i=1 j=1 Disagreement on Attended Positions (Pos.) Another strategy is to disperse the attended positions predicted by multiple heads. Inspired by the agreement regularization (Liang et al., 2006; Cheng et al., 2016) which encourages multiple alignments to be similar, in this work, we deploy a variant of the original term by introducing an alignment disagreement regularization. Formally, we employ the sum of element-wise multiplication of corresponding matrix cells2 , to measure the 1 We did not employ the Euler Distance between vectors since we do not care the absolute value in each vector. 2 We also used the squared element-wise subtraction of two matrices in our preliminary experiments, and found it underperforms its multiplication counterpart, which is consistent with the results"
D18-1317,D15-1166,0,0.12947,"pecifically, we propose three types of disagreement regularization, which respectively encourage the subspace, the attended positions, and the output representation associated with each attention head to be different from other heads. Experimental results on widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness and universality of the proposed approach. 1 Introduction Attention model is now a standard component head2 of the deep learning networks, contributing to impressivehead results in neural machine transla1 tion (Bahdanau et al., 2015; Luong et al., 2015), image captioning (Xu et al., 2015), speech recogBush held a talk with Sharon nition (Chorowski et al., 2015), among many other applications. Recently, Vaswani et al. (2017) introduced a multi-head attention mechanism to capture different context with multiple individual attention functions. One strong point of multi-head attention is the ability to jointly attend to information from different representation subspaces at different positions. However, there is no mechanism to guarantee that different attention heads indeed capture distinct features. In response to this problem, we introduce a"
D18-1317,P02-1040,0,0.100753,"are with the results reported by previous work (Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018), we conduct experiments on both WMT2017 Chinese⇒English (Zh⇒En) and WMT2014 English⇒German (En⇒De) translation tasks. The Zh⇒En corpus consists of 20M sentence pairs, and the En⇒De corpus consists of 4M sentence pairs. We follow previous work to select the validation and test sets. Byte-pair encoding (BPE) is employed to alleviate the Out-ofVocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We use the case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. We evaluate the proposed approaches on the advanced T RANSFORMER model (Vaswani et al., 2017), and implement on top of an open-source toolkit – THUMT (Zhang et al., 2017). We follow Vaswani et al. (2017) to set the configurations and have reproduced their reported results on the En⇒De task. All the evaluations are conducted on the test sets. We have tested both Base and Big models, which differ at hidden size (512 vs. 1024) and number of attention heads (8 vs. 16). We study model variations with Base"
D18-1317,P16-1162,0,0.129459,"ed (steps/second). 5 5.1 4 Regularization Sub. Pos. Out. × × × X × × × X × × × X X × X X X × X X X Experiments Setup To compare with the results reported by previous work (Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018), we conduct experiments on both WMT2017 Chinese⇒English (Zh⇒En) and WMT2014 English⇒German (En⇒De) translation tasks. The Zh⇒En corpus consists of 20M sentence pairs, and the En⇒De corpus consists of 4M sentence pairs. We follow previous work to select the validation and test sets. Byte-pair encoding (BPE) is employed to alleviate the Out-ofVocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We use the case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. We evaluate the proposed approaches on the advanced T RANSFORMER model (Vaswani et al., 2017), and implement on top of an open-source toolkit – THUMT (Zhang et al., 2017). We follow Vaswani et al. (2017) to set the configurations and have reproduced their reported results on the En⇒De task. All the evaluations are conducted on the test sets. We have tested both Base and Big mode"
D18-1317,N18-2074,0,0.0987686,"Missing"
D18-1317,P16-1008,1,0.847337,"res on attended positions across encoder layers. Except for the 1st layer that attends to the input word embeddings, the disagreement scores on other layers (i.e. ranging from the 2nd to 6th layer) are very low, which confirms out above hypothesis. Concerning the regularization terms, except that on position, the other two regularization terms (i.e. “Sub.” and “Out.”) do not increase the disagreement score on the attended positions. This can explain why positional regularization term does not work well with the other two terms, as shown in Table 1. This is also consistent with the finding in (Tu et al., 2016), which indicates that neural networks can model linguistic information in their own way. In contrast to attended positions, it seems that the multi-head attention prefer to encoding the differences among multiple heads in the learned representations. 6 Conclusion In this work, we propose several disagreement regularizations to augment the multi-head attention model, which encourage the diversity among attention heads so that different head can learn distinct features. Experimental results across language pairs validate the effectiveness of the proposed approaches. The models also suggest a wi"
D18-1317,1983.tc-1.13,0,0.486168,"Missing"
D18-1317,D18-1475,1,0.709862,"Missing"
D18-1317,P17-4012,0,0.056906,"ence pairs, and the En⇒De corpus consists of 4M sentence pairs. We follow previous work to select the validation and test sets. Byte-pair encoding (BPE) is employed to alleviate the Out-ofVocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We use the case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test. We evaluate the proposed approaches on the advanced T RANSFORMER model (Vaswani et al., 2017), and implement on top of an open-source toolkit – THUMT (Zhang et al., 2017). We follow Vaswani et al. (2017) to set the configurations and have reproduced their reported results on the En⇒De task. All the evaluations are conducted on the test sets. We have tested both Base and Big models, which differ at hidden size (512 vs. 1024) and number of attention heads (8 vs. 16). We study model variations with Base model on the Zh⇒En task (Section 5.2 and 5.3), and evaluate overall performance with Big model on both Zh⇒En and En⇒De tasks (Section 5.4). 5.2 Effect of Regularization Terms In this section, we evaluate the impact of different regularization terms on the Zh⇒En ta"
D18-1333,N18-1008,0,0.0193306,"rn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reconstruction score is computed by R(ˆ x"
D18-1333,P05-1066,0,0.120726,"tperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the translation results. It is clear that the proposed m"
D18-1333,P15-1166,0,0.0298915,"ch higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotat"
D18-1333,N16-1101,0,0.0216158,"r. Fortunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction scor"
D18-1333,W10-1737,0,0.232395,"Missing"
D18-1333,P02-1040,0,0.100689,"the DPP-annotated data (“Baseline (+DPPs)”, Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the"
D18-1333,D17-1301,1,0.910434,"Missing"
D18-1333,N16-1113,1,0.686079,"Missing"
D18-1333,P13-1081,0,0.46132,"Missing"
D18-1333,N16-1004,0,0.0282224,"em. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reco"
D18-1457,P18-1008,0,0.090299,"Missing"
D18-1457,P05-1066,0,0.0379572,"maximum length limited to 50, consisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions. In such case, the two vectors are in fact linearly dependent, while we aim at encouraging the vectors independent from each other. their reported"
D18-1457,P17-1106,0,0.0203105,"ks with advanced connecting strategies outperform their shallow counterparts. Due to its simplicity and effectiveness, skip connection becomes a standard component of state-of-the-art NMT models (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). In this work, we prove that deep representation exploitation can further improve performance over simply using skip connections. Representation Interpretation Several researchers have tried to visualize the representation of each layer to help better understand what information each layer captures (Zeiler and Fergus, 2014; Li et al., 2016; Ding et al., 2017). Concerning natural language processing tasks, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. Anastasopoulos and Chiang (2018) show that higher level layers are more representative than lower level layers. Peters et al. (2018) demonstrate that higher-level layers capture context-dependent aspects of word meaning while lower-level layers model aspects of syntax. Inspired by these observations, we propose to expose all of these representations to better 4260 fuse information across la"
D18-1457,D18-1317,1,0.832724,"Missing"
D18-1457,N18-1008,0,0.0250029,"Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers. Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018). In natural language processing community, Peters et al. (2"
D18-1457,D16-1025,0,0.0266353,"× × X X X Length of Source Sentence Figure 4: BLEU scores on the En⇒De test set with respect to various input sentence lengths. “Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization. 4.3.1 Length Analysis Following Bahdanau et al. (2015) and Tu et al. (2016), we grouped sentences of similar lengths together and computed the BLEU score for each group, as shown in Figure 4. Generally, the performance of T RANSFORMER -BASE goes up with the increase of input sentence lengths, which is superior to the performance of RNN-based NMT models on long sentences reported by (Bentivogli et al., 2016). We attribute this to the strength of self-attention mechanism to model global dependencies without regard to their distance. Clearly, the proposed approaches outperform the baseline model in all length segments, while there are still considerable differences between the two variations. Hierarchical aggregation consistently outperforms the baseline model, and the improvement goes up on long sentences. One possible reason is that long sentences indeed require deep aggregation mechanisms. Introducing diversity regularization further improves performance on most sentences (e.g. ≤ 45), while the"
D18-1457,D15-1166,0,0.263339,"Missing"
D18-1457,P02-1040,0,0.100765,"the Zh⇒En task, we used all of the available parallel data with maximum length limited to 50, consisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions. In such case, the two vectors are in fact linearly dependent, while we aim at enco"
D18-1457,N18-1202,0,0.722334,"hdanau et al., 2015; Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers. Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018). In natural language proc"
D18-1457,P16-1162,0,0.213758,"e conducted experiments on both Chinese⇒English (Zh⇒En) and English⇒German (En⇒De) translation tasks. For the Zh⇒En task, we used all of the available parallel data with maximum length limited to 50, consisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are"
D18-1457,N18-2074,0,0.0542048,"Missing"
D18-1457,1983.tc-1.13,0,0.597316,"Missing"
D18-1457,D18-1475,1,0.796356,"Missing"
D18-1457,D16-1159,0,0.234053,"ttention model (Bahdanau et al., 2015; Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers. Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018). In"
D18-1457,Q17-1007,1,0.866873,"r with a residual connection: l l−1 H = Layer(H )+ l−1 X Hi . where {W1 , . . . , WL } are trainable matrices. While the strategy is similar in spirit to (Peters et al., 2018), there are two main differences: (1) they use normalized weights while we directly use parameters that could be either positive or negative numbers, which may benefit from more modeling flexibility. (2) they use a scalar that is shared by all elements in the layer states, while we use learnable matrices. The latter offers a more precise control of the combination by allowing the model to be more expressive than scalars (Tu et al., 2017). We also investigate strategies that iteratively and hierarchically merge layers by incorporating more depth and sharing, which have proven effective for computer vision tasks (Yu et al., 2018). Iterative Aggregation. As illustrated in Figure 1d, iterative aggregation follows the iterated stacking of the backbone architecture. Aggregation begins at the shallowest, smallest scale and then iteratively merges deeper, larger scales. The iterative deep aggregation function I for a series of layers Hl1 = {H1 , · · · , Hl } with increasingly deeper and semantic information is formulated as b l = I(H"
D18-1457,P17-4012,0,0.0732084,"set consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions. In such case, the two vectors are in fact linearly dependent, while we aim at encouraging the vectors independent from each other. their reported results on the En⇒De task. The parameters of the proposed models were initialized by the pre-trained model. We tried k = 2 and k = 3 for the multi-layer attention model, which allows to attend to the lower two or t"
D18-1457,P16-1008,1,0.868416,"h 30 Model BLEU 29 Ours Base BASE 28 O URS 27 Hier.+Div. Hier. Base 26 26.13 26.32 26.41 26.69 archical aggregation to different components on En⇒De validation set. 45 > ] (3 0, 45 ] 30 5, (1 5] ,1 (0 45 > ] 45 (3 0, 0] BLEU Table 3: Experimental results of applying hier25 Source Sentence Applied to Encoder Decoder N/A N/A X × × X X X Length of Source Sentence Figure 4: BLEU scores on the En⇒De test set with respect to various input sentence lengths. “Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization. 4.3.1 Length Analysis Following Bahdanau et al. (2015) and Tu et al. (2016), we grouped sentences of similar lengths together and computed the BLEU score for each group, as shown in Figure 4. Generally, the performance of T RANSFORMER -BASE goes up with the increase of input sentence lengths, which is superior to the performance of RNN-based NMT models on long sentences reported by (Bentivogli et al., 2016). We attribute this to the strength of self-attention mechanism to model global dependencies without regard to their distance. Clearly, the proposed approaches outperform the baseline model in all length segments, while there are still considerable differences betw"
D18-1457,Q16-1027,0,0.347851,"ral machine translation (NMT) models have advanced the machine translation community in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). NMT models generally consist of two components: an encoder network to summarize the input sentence into sequential representations, based on which a decoder network generates target sentence word by word with an attention model (Bahdanau et al., 2015; Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at"
D19-1082,P02-1040,0,0.109143,"chine Translation Implementation Detail We conduct the experiments on the WMT14 English-to-German (En⇒De) and NIST Chinese-to-English (Zh⇒En) translation tasks. For En⇒De, the training dataset consists of 4.56M sentence pairs. We use the newstest2013 and newstest2014 as development set and test set respectively. For Zh⇒En, the training dataset consists of about 1.25M sentence pairs. We used NIST MT02 dataset as development set, and MT 03-06 datasets as test sets. Byte pair encoding (BPE) toolkit1 (Sennrich et al., 2016) is used with 32K merge operations. We used casesensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the relevant tags. We test both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16). All models are trained on eight NVIDIA Tesla P40 GPUs where each is allocated with a batch size of 4096 tokens. We implement the proposed approaches on top of T RANS FORMER (Vaswani et al., 2017) – a state-of-theart S ANs-based model"
D19-1082,D17-1209,0,0.0192451,"., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b), prior attention bias (Yang et a"
D19-1082,N18-1202,0,0.107117,"Missing"
D19-1082,P05-1033,0,0.586135,"Missing"
D19-1082,P18-1198,0,0.045558,"lest phrase constituent that above each word, “POS”: Part-of-Speech tags for each words. The tasks for predicting larger labels require models to capture and record larger granularity of phrase information of sentences (Shi et al., 2016). We conduct these tasks to study whether the proposed M G -S A benefits the multi-granularity phrase modeling to produce more useful and informative representation. 4.2 Multi-Granularity Phrases Evaluation Data and Models We extracted the sentences from the Toronto Book Corpus (Zhu et al., 2015).We sample and pre-process 120k sentences for each task following Conneau et al. (2018). By instruction of Shi et al. (2016), we label these sentences for each task. The train/valid/test dataset ratios are set to 10/1/1. For pre-trained NMT encoders, we use the pretrained encoders of model variations in Table 3 followed by a MLP classifier, which are used to In this section, we conduct multi-granularity label prediction tasks to the proposed models in terms of whether the proposed model is effective as expected to capture different levels of granularity phrase information of sentences. We analyze the impact of multi-granularity self-attention based on 2 Since the attention weigh"
D19-1082,W18-5431,0,0.120452,"proach consistently improves performance. Targeted linguistic analysis reveals that M G -S A indeed captures useful phrase information at various levels of granularities. 1 Introduction Recently, T RANSFORMER (Vaswani et al., 2017), implemented as deep multi-head self-attention networks (S ANs), has become the state-of-the-art neural machine translation (NMT) model in recent years. The popularity of S ANs lies in its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. More recently, an in-depth study (Raganato and Tiedemann, 2018) reveals that S ANs generally focus on disperse words and ignore continuous phrase patterns, which have proven essential in both statistical machine translation (SMT, Koehn ∗ Work done when interning at Tencent AI Lab. 887 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 887–897, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics speeds marginally decrease. Analysis on multigranularity label prediction tasks reveals that M G S A indeed captu"
D19-1082,N19-1423,0,0.0237641,"syntactic phrase based models, we only apply syntactic boundary of phrases and do not use any tag supervision for fair comparison. carry out probing tasks. phrase interaction by using O N -L STM. We use same assignments of heads for multi-granularity phrases as machine translation task for all model variants. For models trained from scratch, each of our model consists of 3 encoding layers followed by a MLP classifier. For each encoding layer, we employ a multi-head self-attention block and a feed-forward block as in T RANSFORMER, which have shown significant performance on several NLP tasks (Devlin et al., 2019). The difference between the compared models merely lies in the self-attention mechanism: “BASE” denotes standard M H -S A, “N-Gram Phrase” and “Syntactic Phrase” are the proposed M G -S A under N-gram phrase and syntactic phrase partition, and “Syntactic Phrase + Interaction” denotes M G -S A with Results Analysis Table 5 lists the prediction accuracies of five syntactic labels on test. Several observations can be made here. 1). Comparing the two set of experiments, the experimental results from models trained from scratch consistently outperform the results from NMT encoder probing on all ta"
D19-1082,P16-1162,0,0.128307,"illion), “Speed” denotes the training speed (steps/second). Encoder Layers [1 − 6] [1 − 3] [1] Machine Translation Implementation Detail We conduct the experiments on the WMT14 English-to-German (En⇒De) and NIST Chinese-to-English (Zh⇒En) translation tasks. For En⇒De, the training dataset consists of 4.56M sentence pairs. We use the newstest2013 and newstest2014 as development set and test set respectively. For Zh⇒En, the training dataset consists of about 1.25M sentence pairs. We used NIST MT02 dataset as development set, and MT 03-06 datasets as test sets. Byte pair encoding (BPE) toolkit1 (Sennrich et al., 2016) is used with 32K merge operations. We used casesensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the relevant tags. We test both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16). All models are trained on eight NVIDIA Tesla P40 GPUs where each is allocated with a batch size of 4096 tokens. We implement the proposed"
D19-1082,P16-1078,0,0.0161954,"regularization (Li et al., 2018; Tao et al., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et a"
D19-1082,D16-1159,0,0.470117,"arget phrases? Q3. Does M G -S A capture more phrase information at the various granularity levels? In Section 4.1, we demonstrate that integrating the proposed M G -S A into the T RANS FORMER consistently improves the translation quality on both WMT14 English⇒German and (11) 890 Phrase Modeling n/a M AX -P OOLING S ANs L STM NIST Chinese⇒English (Q1). Further analysis reveals that our approach has stronger ability of capturing the phrase information and promoting the generation of the target phrases (Q2). In Section 4.2, we conduct experiments on the multi-granularity label prediction tasks (Shi et al., 2016), and investigate the representations of NMT encoders trained on both translation data and the training data of the label prediction tasks. Experimental results show that the proposed M G -S A indeed captures useful phrase information at various levels of granularities in both scenarios (Q3). 4.1 Speed 1.28 1.27 1.26 1.14 BLEU 27.31 27.56 27.69 27.58 Table 1: Evaluation of various phrase composition strategies under N-gram phrase partition. “# Para” denotes the trainable parameter size of each model (M=million), “Speed” denotes the training speed (steps/second). Encoder Layers [1 − 6] [1 − 3]"
D19-1082,P19-1254,0,0.0154434,"so on monolingual tasks. 5 Multi-Head Attention Multi-head attention mechanism has shown its effectiveness in machine translation (Vaswani et al., 2017) and generative dialog (Tao et al., 2018) systems. Recent studies shows that the modeling ability of multi-head attention has not been completely developed. Several specific guidance cues of different heads without breaking the vanilla multi-head attention mechanism can further boost the performance, e.g., disagreement regularization (Li et al., 2018; Tao et al., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. T"
D19-1082,D18-1548,0,0.424859,"ssing, pages 887–897, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics speeds marginally decrease. Analysis on multigranularity label prediction tasks reveals that M G S A indeed captures and stores the information of different granularity phrases as expected. 2 hand, recent study (Vaswani et al., 2017) implicitly hint that attention heads are underutilized as increasing number of heads from 4 to 8 or even 16 can hardly improve the translation performance. Several attention heads can be further exploited under specific guidance to improve the performance (Strubell et al., 2018). We expect the inductive bias for multi-granularity phrase can further improve the performance of S ANs and meanwhile maintain its simplicity and flexibility. Background Multi-Head Self-attention Instead of performing a single attention, Multi-Head Self-attention Networks (M H -S A), which are the defaults setting in T RANSFORMER (Vaswani et al., 2017), project the queries, keys and values into multiple subspaces and performs attention on the projected queries, keys and values in each subspace. In the standard M H -S A, it jointly attends to information from different representation subspaces"
D19-1082,D19-1135,1,0.884992,"c phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one commonly-cited weakness of S ANs (Tran et al., 2018; Hao et al., 2019b). We evaluate the proposed model on two widely-used translation tasks: WMT14 Englishto-German and NIST Chinese-to-English. Experimental results demonstrate that our approach consistently improves translation performance over strong T RANSFORMER baseline model (Vaswani et al., 2017) across language pairs, while Current state-of-the-art neural machine translation (NMT) uses a deep multi-head selfattention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced subs"
D19-1082,N19-1122,1,0.804029,"Missing"
D19-1082,D18-1503,0,0.182789,"words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one commonly-cited weakness of S ANs (Tran et al., 2018; Hao et al., 2019b). We evaluate the proposed model on two widely-used translation tasks: WMT14 Englishto-German and NIST Chinese-to-English. Experimental results demonstrate that our approach consistently improves translation performance over strong T RANSFORMER baseline model (Vaswani et al., 2017) across language pairs, while Current state-of-the-art neural machine translation (NMT) uses a deep multi-head selfattention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases"
D19-1082,P03-1054,0,0.0349375,"s. For En⇒De, the training dataset consists of 4.56M sentence pairs. We use the newstest2013 and newstest2014 as development set and test set respectively. For Zh⇒En, the training dataset consists of about 1.25M sentence pairs. We used NIST MT02 dataset as development set, and MT 03-06 datasets as test sets. Byte pair encoding (BPE) toolkit1 (Sennrich et al., 2016) is used with 32K merge operations. We used casesensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the relevant tags. We test both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16). All models are trained on eight NVIDIA Tesla P40 GPUs where each is allocated with a batch size of 4096 tokens. We implement the proposed approaches on top of T RANS FORMER (Vaswani et al., 2017) – a state-of-theart S ANs-based model on machine translation, and followed the setting in previous work (Vaswani et al., 2017) to train the models. We incorporate the proposed model into the encoder"
D19-1082,N03-1017,0,0.30611,"rases and meanwhile maintain their simplicity and flexibility. The starting point for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one common"
D19-1082,D18-1317,1,0.93476,"openg Tu Tencent AI Lab shumingshi@tencent.com Florida State University jinfeng@stat.fsu.edu Tencent AI Lab zptu@tencent.com Abstract et al., 2003; Chiang, 2005; Liu et al., 2006) and NMT (Eriguchi et al., 2016; Wang et al., 2017; Yang et al., 2018; Zhao et al., 2018). To alleviate this problem, in this work we propose multi-granularity self-attention (M G -S A), which offers S ANs the ability to model phrases and meanwhile maintain their simplicity and flexibility. The starting point for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input senten"
D19-1082,P19-1580,0,0.0230715,"ent AI Lab zptu@tencent.com Abstract et al., 2003; Chiang, 2005; Liu et al., 2006) and NMT (Eriguchi et al., 2016; Wang et al., 2017; Yang et al., 2018; Zhao et al., 2018). To alleviate this problem, in this work we propose multi-granularity self-attention (M G -S A), which offers S ANs the ability to model phrases and meanwhile maintain their simplicity and flexibility. The starting point for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence in"
D19-1082,N19-1359,1,0.833903,"-S A is not limited to machine translation, but also on monolingual tasks. 5 Multi-Head Attention Multi-head attention mechanism has shown its effectiveness in machine translation (Vaswani et al., 2017) and generative dialog (Tao et al., 2018) systems. Recent studies shows that the modeling ability of multi-head attention has not been completely developed. Several specific guidance cues of different heads without breaking the vanilla multi-head attention mechanism can further boost the performance, e.g., disagreement regularization (Li et al., 2018; Tao et al., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are pro"
D19-1082,D17-1149,1,0.772626,"take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b), prior attention bias (Yang et al., 2018, 2019; Guo et al., 2019). Our work differs at that we explicitly model phrase patterns at different granularities, which is then attended by different attention heads. 6 Conclusion In this paper, we propose multi-granularity selfattention model, a novel attention mechanism to simultaneously attend different granularity phrase. We study effective phrase representation for Ngram phrase and syntactic phrase, and find that a syntactic phrase based mechanism obtains the best result due to effectively incorporating r"
D19-1082,D18-1408,0,0.0826882,"Missing"
D19-1082,P06-1077,0,0.069506,"oint for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one commonly-cited weakness of S ANs (Tran et al., 2018; Hao et al., 2019b). We evalu"
D19-1082,D18-1475,1,0.737412,"l., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b), prior attention bias (Yang et al., 2018, 2019; Guo et al., 2019). Our work differs at that we explicitly model phrase patterns at different granularities, which is then attended by different attention heads. 6 Conclusion In this paper, we propose multi-granularity selfattention model, a novel attention mechanism to simultaneously attend different granularity phrase. We study effective phrase representation for Ngram phrase and syntactic phrase, and find that a syntactic phrase based mechanism obtains the best result due to effectively incorporating rich syntactic information. To evaluate the effectiveness of the proposed model, we"
D19-1082,N19-1407,1,0.86432,"Missing"
D19-1082,D16-1100,0,0.0286164,". Targeted multi-granularity phrases evaluation shows that our model indeed capture useful phrase information. As our approach is not limited to specific tasks, it is interesting to validate the proposed model in other tasks, such as reading comprehension, language inference, and sentence classification. Multi Granularity Representation Multigranularity representation, which is proposed to make full use of subunit composition at different levels of granularity, has been explored in various NLP tasks, such as paraphrase identification (Yin and Sch¨utze, 2015), Chinese word embedding learning (Yin et al., 2016), universal sentence encoding (Wu et al., 2018) and machine translation (Nguyen and Joty, 2018; Li et al., 2019b). The major difference between our work and Nguyen and Joty (2018); Li et al. (2019b) lies in that we successfully introduce syntactic information into our multi-granularity representation. Furthermore, it is not well measured how much phrase information are stored in multi-granularity Acknowledgments J.Z. was supported by the National Institute of General Medical Sciences of the National Institute of Health under award number R01GM126558. We thank the anonymous reviewers for their"
D19-1082,N15-1091,0,0.0712827,"Missing"
D19-1082,N19-1118,0,0.0190421,"phrase benefits to translation quality. In addition, incorporating tag loss (Row 4) in training stage can further boost the translation performance. This indicates the auxiliary syntax objective is necessary, which is consistent with the results in other NLP task (Strubell et al., 2018). We use syntactic phrase partition with tag supervision as the default setting for subsequent experiments unless otherwise stated. Main Results Table 4 lists the results on WMT14 En⇒De and NIST Zh⇒En translation tasks. Our baseline models, outperform the reported results on the same data (Vaswani et al., 2017; Zhang et al., 2019), which we believe make the evaluation convincing. As seen, in terms of BLEU score, the proposed M G -S A consistently improves translation performance across language pairs, which demonstrates the effectiveness and universality of the proposed approach. Phrasal Pattern Evaluation As aforementioned, the proposed M G -S A aims to simultaneously model different granularities of phrases with different heads in S ANs. To investigate whether the proposed M G -S A improves the generation of phrases in the output, we calculate the improvement of the proposed models over multiple N-grams, as shown in"
D19-1085,N18-1118,0,0.038634,"wouldn’t let you buy it? Non-Fixed Error 我 和 露西 只是 要 搬 到 对门。 我们 一 分手 (我) 就 搬 回去。 Once we broke up, I’ll move back. Once we broke up, she’ll move back. Once we broke up, we moved back. Once we broke up, we’ll move back. bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model. More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018). They mainly exploited general anaphora in non-pro-drop languages such as English⇒Russian. 6 Conclusion In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourselevel information for better ZP prediction. Experimental results on both Chinese⇒English and Japanese⇒English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translat"
D19-1085,D13-1135,0,0.0234044,"e hidden states of both the encoder-decoder and the reconstructor to embed the ZPs in the source sentence. Although the calculation of labeling loss relies on explicitly annotated labels, it is only used in training to guide the parameters to learn ZP-enhanced representations. Benefiting from the implicit integration of ZP information, we release the reliance on external ZP prediction model in testing. P (zpt |hrec t ) t=1 T Y (5) 3.2 (4) Discourse-Aware ZP Prediction Discourse information have proven useful for predicting antecedents, which may occur in previous sentences (Zhao and Ng, 2007; Chen and Ng, 2013). Therefore, we further improve ZP prediction with discourse-level context, which is learned together with the joint model. gl (zpt , hrec t ) t=1 where gl (·) is softmax for the ZP labeler. As seen, we integrate the ZP generation component into the ZP translation model. There is no reliance on external ZP prediction models in decoding phase. Encoding Discourse-Level Context Hierarchical structure networks are usually used for modelling discourse context on various natural language processing tasks such query suggestion (Sordoni et al., 2015), dialogue modeling (Serban et al., 2016) and MT (Wa"
D19-1085,W12-4213,0,0.496166,"Missing"
D19-1085,D10-1062,0,0.0858334,"Missing"
D19-1085,P05-1066,0,0.33376,"Missing"
D19-1085,W17-5702,0,0.0150206,"that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translation models in previous work, and achieve a new state-of-the-art on the widelyused subtitle corpus. Manual evaluation confirms that the performance improvement comes from the alleviation of translation errors, which are mainly caused by subjective, objective as well as discourse-aware ZPs. There are two potential extensions to our work. First, we will evaluate our method on other implication phenomena (or called unaligned words (Takeno et al., 2017)) such as tenses and article words for NMT. Second, we will investigate the impact of different contextaware models on ZP translation, including multiattention (Jean et al., 2017b) and context-aware Transformer(Voita et al., 2018). Table 7: Example translations where pronouns in brackets are dropped in original inputs (“I NP.”) but labeled by humans according to references (“R EF.”) and previous sentence (“P RE .”). We italicize some mistranslated errors and highlight the correct ones in bold. overcome the data-level gap, Wang et al. (2016) proposed an automatic approach of ZP annotation by ut"
D19-1085,tiedemann-2012-parallel,0,0.0363956,"Missing"
D19-1085,Q18-1029,1,0.853259,"nces (ˆ x, y) whose source-side sentences are auto-annotated with ZPs. The “+ Reconstruction” is the best model reported in Wang et al. (2018a), which employs two reconstructors to reconstruct the x ˆ from Experiments 4.1 Results on Chinese⇒English Task Setup We conducted translation experiments on both Chinese⇒English and Japanese⇒English translation tasks, since Chinese and Japanese are pro-drop languages while English is not. For Chinese⇒English translation task, we used the data of auto-annotated ZPs (Wang et al., 2018a).3 4 http://www.opensubtitles.org. We followed Wang et al. (2017) and Tu et al. (2018) to use 3 previous sentences as discourse context. 3 5 https://github.com/longyuewangdcu/ tvsub. 925 # Model 1 Baseline 2 3 4 5 Translation #Params BLEU Prediction P R F1 86.7M 31.80 n/a External ZP Prediction (Wang et al., 2018a) + ZP-Annotated Data +0M 32.67 0.67 + Reconstruction +73.8M 35.08 This Work: Joint ZP Prediction and Translation Joint Model +35.6M 36.04† 0.72 + Discourse-Level Context +56.6M 37.11† 0.76 n/a n/a 0.65 0.66 0.68 0.77 0.70 0.77 Table 2: Evaluation of ZP translation and prediction on the Chinese–English data. “#Params” represents the number of parameters used in differe"
D19-1085,W17-4806,0,0.120064,"buy one? Sure. Joey wouldn’t let you buy it? Non-Fixed Error 我 和 露西 只是 要 搬 到 对门。 我们 一 分手 (我) 就 搬 回去。 Once we broke up, I’ll move back. Once we broke up, she’ll move back. Once we broke up, we moved back. Once we broke up, we’ll move back. bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model. More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018). They mainly exploited general anaphora in non-pro-drop languages such as English⇒Russian. 6 Conclusion In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourselevel information for better ZP prediction. Experimental results on both Chinese⇒English and Japanese⇒English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform th"
D19-1085,P18-1117,0,0.418324,"yielded best performances on validation sets. For training the proposed models, the hidden layer sizes of hierarchical model and reconstruction model are 1,000 and 2,000, respectively. We modeled previous three sentences as discourse-level context.5 (6) After we can obtain all sentence-level representations HX = {h−K , . . . , h−1 }, we feed them into a sentence-level encoder to produce a vector that represents the discourse-level context: C = E NCODERsentence (HX ) (7) Here the summary C consists of not only the dependencies between words, but also the relations between sentences. Following Voita et al. (2018), we share the parameters of word-level encoder E NCODERword with the encoder component in standard NMT model. Note that, E NCODERword and E NCODERsentence can be implemented as arbitrary networks, such as recurrent networks (Cho et al., 2014), convolutional networks (Gehring et al., 2017), or self-attention networks (Vaswani et al., 2017). In this study, we used recurrent networks to implement our E NCODER. Integrating Discourse into ZP Prediction We directly feed the discourse-level context to the reconstructor to improve ZP prediction. Specifically, we combine the context vector and the rec"
D19-1085,D10-1086,0,0.182667,"Missing"
D19-1085,W10-1737,0,0.0613995,"Missing"
D19-1085,D17-1301,1,0.929721,"3). Therefore, we further improve ZP prediction with discourse-level context, which is learned together with the joint model. gl (zpt , hrec t ) t=1 where gl (·) is softmax for the ZP labeler. As seen, we integrate the ZP generation component into the ZP translation model. There is no reliance on external ZP prediction models in decoding phase. Encoding Discourse-Level Context Hierarchical structure networks are usually used for modelling discourse context on various natural language processing tasks such query suggestion (Sordoni et al., 2015), dialogue modeling (Serban et al., 2016) and MT (Wang et al., 2017). Therefore, we employ hierarchical encoder (Wang et al., 2017) to encoder discourseTraining and Testing The newly introduced prediction component is trained together with the 1 We introduce “ heosi” to cover the case that a pronoun is missing at the end of a sentence. 2 We employ the pronoun vocabulary used in Wang et al. (2016), which contains 30 distinct Chinese pronouns. 924 level context for NMT. More specifically, we use the previous K source sentences X = {x−K , . . . , x−1 } as the discourse information, which is summarized with a two-layer hierarchical encoder, as shown in Figure 2. F"
D19-1085,D18-1333,1,0.851182,"interactive attention models: enc α ˆ enc = ATTenc (xt−1 , hrec ) t−1 , h α ˆ dec = dec enc ˆt ) ATTdec (xt−1 , hrec ,c t−1 , h Figure 2: Architecture of hierarchical neural encoder. x−K , . . . , x−1 are K previous sentences before the current source sentence “你 烤 的 吗 ?” in a text. (2) (3) encoder-decoder-reconstructor:  J(θ, γ, ψ) = arg max log L(y|x; θ) {z } | θ,γ,ψ likelihood + log R(x|henc , hdec ; θ) | {z } reconstruction  rec + log P (zp|h ; θ, γ) {z } | ZP labeling The interaction between two attention models leads to a better exploitation of the encoder and decoder representations (Wang et al., 2018b). ZP Prediction as Sequence Labelling We cast ZP prediction as a sequence labelling task, where each word is labelled if there is a pronoun missing before it. Given the input x = {x1 , x2 , . . . , xT } with the last word xT being the end-of-sentence tag “ heosi”,1 the output to be labelled is a sequence of labels zp = {zp1 , zp2 , . . . , zpT } with zpt ∈ {N } ∪ Vzp . Among the label set, “N ” denotes no ZP, and Vzp is the vocabulary of pronouns.2 Taking Figure 1 as an example, the label sequence “N N N 它 N N” indicates that the pronoun “它” is missing before the fourth word “吗” in the sourc"
D19-1085,C10-1080,0,0.0468499,"Missing"
D19-1085,N16-1113,1,0.922694,"o components to interact with each other. 2. Our study demonstrates the effectiveness of discourse-level context for ZP prediction. 3. Based on our manually-annotated testset, we conduct extensive analyses to assess ZP prediction and translation. 2 2.1 等 我 搬进来，(我 我) 能 买 台 电视 吗？ Can I get a TV when I move in? When I move in to buy a TV. 这块 蛋糕 很 美味！你 烤 的 (它 它) 吗？ The cake is very tasty! Did you bake it? The cake is delicious! Are you baked? 2.2 Background Bridging Data Gap Between ZP Prediction and Translation Recent efforts have explored ways to bridge the gap of ZP prediction and translation (Wang et al., 2016, 2018a,b) by training both models on the homologous data. The pipeline involves two phases, as described below. Zero Pronoun In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English (Zhao and Ng, 2007). As seen in Table 1, the subject pronoun (“我”) and the object pronoun (“它”) are omitted in Chinese sentences (“Inp.”) while these pronouns are all compulsory Translation-Oriented ZP Prediction Its goal is to recall the ZPs in the source sentence (i.e. prodrop language) with the information of the target sentence (i.e."
D19-1085,P02-1040,0,0.103671,"Missing"
D19-1085,P13-1081,0,0.388518,"Missing"
D19-1085,Y15-1050,0,0.345278,"Missing"
D19-1085,C10-1135,0,0.0554474,"Missing"
D19-1085,N15-1052,0,0.216225,"Missing"
D19-1085,D17-1135,0,0.0842924,"Missing"
D19-1085,D07-1057,0,0.489977,"Can I get a TV when I move in? When I move in to buy a TV. 这块 蛋糕 很 美味！你 烤 的 (它 它) 吗？ The cake is very tasty! Did you bake it? The cake is delicious! Are you baked? 2.2 Background Bridging Data Gap Between ZP Prediction and Translation Recent efforts have explored ways to bridge the gap of ZP prediction and translation (Wang et al., 2016, 2018a,b) by training both models on the homologous data. The pipeline involves two phases, as described below. Zero Pronoun In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English (Zhao and Ng, 2007). As seen in Table 1, the subject pronoun (“我”) and the object pronoun (“它”) are omitted in Chinese sentences (“Inp.”) while these pronouns are all compulsory Translation-Oriented ZP Prediction Its goal is to recall the ZPs in the source sentence (i.e. prodrop language) with the information of the target sentence (i.e. non-pro-drop language) in a paral922 lel corpus. Taking the second case (assuming that Inp. and Ref. are sentence pair in a parallel corpus) in Table 1 for instance, the ZP “它 (it)” is dropped in the Chinese side while its equivalent “it” exists in the English side. It is possib"
D19-1086,D18-1048,0,0.07512,"4. 938 studies mainly use capsule network for information aggregation, where the capsules could have a less interpretable meaning. In contrast, our model learns what we expect by the aid of auxiliary learning signals, which endows our model with better interpretability. RNN layers, nor compatible with the state-of-theart Transformer for the additional recurrences prevent Transformer decoder from being parallelized. Another direction is to introduce global representations. Lin et al. (2018) model a global source representation by deconvolution networks. Xia et al. (2017); Zhang et al. (2018); Geng et al. (2018) propose to provide a holistic view of target sentence by multi-pass decoding. Zhou et al. (2019) improve Zhang et al. (2018) to a synchronous bidirectional decoding fashion. Similarly, Weng et al. (2019) deploy bidirectional decoding in interactive translation setting. Different from these work aiming at providing static global information in the whole translation process, our approach models a dynamically global (holistic) context by using capsules network to separate source contents at every decoding steps. 6 Conclusion In this paper, we propose to recognize the translated PAST and untransl"
D19-1086,D16-1096,0,0.044511,"sentences become longer, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may be not easy to show the direct correspondence between the source contents and learned representations in the past/future Does guided dynamic routing really matter? Despite the promis"
D19-1086,C18-1232,0,0.0294723,"cally, an encoder first maps the source sentence into a sequence of encoded representations: Figure 1: An example of separation of PAST and F U TURE in machine translation. When generating the current translation “his”, the source tokens “hBOSi”, “布什(Bush)” and phrase “为...辩护(defend)” are the translated contents (PAST), while the remaining tokens are untranslated contents (F UTURE). sule Network (Hinton et al., 2011) with routingby-agreement mechanism (Sabour et al., 2017), which has demonstrated its appealing strength of solving the problem of parts-to-wholes assignment (Hinton et al., 2018; Gong et al., 2018; Dou et al., 2019; Li et al., 2019), to model the separation of the PAST and F UTURE: 1. We first cast the PAST and F UTURE source contents as two groups of capsules. 2. We then design a novel variant of the routingby-agreement mechanism, called Guided Dynamic Routing (G DR), which is guided by the current translating status at each decoding step to assign each source word to its associated capsules by assignment probabilities for several routing iterations. 3. Finally, the PAST and F UTURE capsules accumulate their expected contents from representations, and are fed into the decoder to provi"
D19-1086,W17-4123,0,0.0805954,"Missing"
D19-1086,P16-1008,1,0.931586,"et al., 2015). Like human translators, NMT systems should have the ability to know the relevant source-side context for the current word (P RESENT), as well as recognize what parts in the source contents have been translated (PAST) and what parts have not (F UTURE), at each decoding step. Accordingly, the PAST, P RESENT and F U TURE are three dynamically changing states during the whole translation process. Previous studies have shown that NMT models are likely to face the illness of inadequate translation (Kong et al., 2019), which is usually embodied in over- and under-translation problems (Tu et al., 2016, 2017). This issue may be attributed to the poor ability of NMT of recognizing the dynamic translated and untranslated contents. To remedy this, Zheng et al. (2018) first demonstrate that explicitly tracking PAST and F UTURE contents helps NMT models alleviate this issue and generate better translation. In their work, the running PAST and F UTURE contents are modeled as recurrent states. However, the recurrent process is still non-trivial to determine which parts of the source words are the PAST and which are the F U TURE , and to what extent the recurrent states represent them respectively,"
D19-1086,D19-1074,0,0.035318,"Missing"
D19-1086,D19-1087,0,0.0241637,"dynamic PAST and F UTURE. (a) Translation length v.s source length (b) BLEU v.s source length Figure 5: Comparison regarding source length. 5 gets a larger improvement when the input sentences become longer, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may b"
D19-1086,D17-1013,1,0.941176,"the encoder leverages N stacked identical layers to map the sentence into contextual representations: The NMT model is now able to employ the dynamic holistic context for better generation. 3.3 Learning PAST and F UTURE as Expected h(l) = EncoderLayer(h(l−1) ), Auxiliary Guided Losses To ensure that the dynamic routing process runs as expected, we introduce the following auxiliary guided signals to assist the learning process. where the superscript l indicates layer depth. Based on the encoded source representations hN , a decoder generates translation word by word. The Bag-of-Word Constraint Weng et al. (2017) propose a multitasking scheme to boost NMT by predicting the bag-of-words of target sentence using the Word Predictions approach. Inspired by 934 trained by minimizing the loss L(θ), where θ is the set of all the parameter of the proposed model: this work, we introduce a B OW constraint to encourage the PAST and F UTURE capsules to be predictive of the preceding and subsequent bag-ofwords regarding each decoding step respectively: L B OW M L(θ) = T 1X = − log pPRE (y≤t |ΩPt ) T t=0  − log pSUB (y≥t |ΩFt ) , where λ1 and λ2 are hyper-parameters. 4 where ppre (y≤t |ΩPt ) and psub (y≥t |ΩFt ) a"
D19-1086,N19-1359,1,0.823548,"ce sentence into a sequence of encoded representations: Figure 1: An example of separation of PAST and F U TURE in machine translation. When generating the current translation “his”, the source tokens “hBOSi”, “布什(Bush)” and phrase “为...辩护(defend)” are the translated contents (PAST), while the remaining tokens are untranslated contents (F UTURE). sule Network (Hinton et al., 2011) with routingby-agreement mechanism (Sabour et al., 2017), which has demonstrated its appealing strength of solving the problem of parts-to-wholes assignment (Hinton et al., 2018; Gong et al., 2018; Dou et al., 2019; Li et al., 2019), to model the separation of the PAST and F UTURE: 1. We first cast the PAST and F UTURE source contents as two groups of capsules. 2. We then design a novel variant of the routingby-agreement mechanism, called Guided Dynamic Routing (G DR), which is guided by the current translating status at each decoding step to assign each source word to its associated capsules by assignment probabilities for several routing iterations. 3. Finally, the PAST and F UTURE capsules accumulate their expected contents from representations, and are fed into the decoder to provide a time-dependent holistic view of"
D19-1086,P18-2047,0,0.01658,"ger, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may be not easy to show the direct correspondence between the source contents and learned representations in the past/future Does guided dynamic routing really matter? Despite the promising numbers of the G"
D19-1086,1983.tc-1.13,0,0.706531,"Missing"
D19-1086,C18-1276,0,0.0187648,"milar to Equation 6. The PAST and F UTURE representations are computed by weighted summation, which is similar to Equation 4. 938 studies mainly use capsule network for information aggregation, where the capsules could have a less interpretable meaning. In contrast, our model learns what we expect by the aid of auxiliary learning signals, which endows our model with better interpretability. RNN layers, nor compatible with the state-of-theart Transformer for the additional recurrences prevent Transformer decoder from being parallelized. Another direction is to introduce global representations. Lin et al. (2018) model a global source representation by deconvolution networks. Xia et al. (2017); Zhang et al. (2018); Geng et al. (2018) propose to provide a holistic view of target sentence by multi-pass decoding. Zhou et al. (2019) improve Zhang et al. (2018) to a synchronous bidirectional decoding fashion. Similarly, Weng et al. (2019) deploy bidirectional decoding in interactive translation setting. Different from these work aiming at providing static global information in the whole translation process, our approach models a dynamically global (holistic) context by using capsules network to separate so"
D19-1086,D15-1166,0,0.0656013,"urce contents in the Section 3.3. Note that we employ G DR at every decoding step t to obtain the time-dependent PAST and F UTURE and omit the subscript t for simplicity. In the dynamic routing process, each vector output of capsule j is calculated with a non-linear (7) where Wb ∈ Rd+dc ∗2 and w ∈ Rdc are learnable parameters. Instead of using simple scalar prod> Ω (Sabour et al., 2017), which uct, i.e., bij = vij j could not consider the current decoding state as a condition signal, we resort to the MLP to take zi into account inspired by MLP-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). That is why we call it “guided” dynamic routing. 2 Note that unlike Sabour et al. (2017), where each pair of input capsule i and output capsule j has a distinct transformation matrix Wij as their numbers are predefined (I × J transformation matrices in total), here we share the transformation matrix Wj of output capsule j among all the input capsules due to the varied amount of the source words. So there are J transformation matrices in our model. 933 Output probabilities Softmax Algorithm 1 Guided Dynamic Routing (G DR) Past Capsules Input: Encoder hidden state h, current decoding hidden Ou"
D19-1086,1981.tc-1.7,0,0.677566,"Missing"
D19-1086,D18-1350,0,0.0406832,"Missing"
D19-1086,Q18-1011,1,0.439939,"ecognize what parts in the source contents have been translated (PAST) and what parts have not (F UTURE), at each decoding step. Accordingly, the PAST, P RESENT and F U TURE are three dynamically changing states during the whole translation process. Previous studies have shown that NMT models are likely to face the illness of inadequate translation (Kong et al., 2019), which is usually embodied in over- and under-translation problems (Tu et al., 2016, 2017). This issue may be attributed to the poor ability of NMT of recognizing the dynamic translated and untranslated contents. To remedy this, Zheng et al. (2018) first demonstrate that explicitly tracking PAST and F UTURE contents helps NMT models alleviate this issue and generate better translation. In their work, the running PAST and F UTURE contents are modeled as recurrent states. However, the recurrent process is still non-trivial to determine which parts of the source words are the PAST and which are the F U TURE , and to what extent the recurrent states represent them respectively, this less interpretable nature is probably not the best way to model and exploit the dynamic PAST and F UTURE. We argue that an explicit separation of the source wor"
D19-1086,P02-1040,0,\N,Missing
D19-1086,W04-1013,0,\N,Missing
D19-1086,Q19-1006,0,\N,Missing
D19-1086,P16-1162,0,\N,Missing
D19-1086,D15-1229,0,\N,Missing
D19-1088,D17-1042,0,0.384726,"ral Machine Translation with Word Importance Shilin He1,2 Zhaopeng Tu3∗ Xing Wang3 Longyue Wang3 Michael R. Lyu1,2 Shuming Shi3 1 Department of Computer Science and Engineering, The Chinese University of Hong Kong 2 Shenzhen Research Institute, The Chinese University of Hong Kong 1,2 {slhe,lyu}@cse.cuhk.edu.hk 3 Tencent AI Lab 3 {zptu,brightxwang,vinnylywang,shumingshi}@tencent.com Abstract 2017) or hidden units (Bau et al., 2019; Ding et al., 2017). Another direction focuses on understanding the importance of input words by interpreting the input-output behavior of NMT models. Previous work (Alvarez-Melis and Jaakkola, 2017) treats NMT models as black-boxes and provides explanations that closely resemble the attention scores in NMT models. However, recent studies reveal that attention does not provide meaningful explanations since the relationship between attention scores and model output is unclear (Jain and Wallace, 2019). In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better. Specifically, we employ the integrated gradients method (Sundararajan et al., 2017) to attribute the output to the input"
D19-1088,W16-1601,0,0.0612708,"Missing"
D19-1088,P17-1080,0,0.0264362,"understanding NMT by identifying undertranslated words. • We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universa"
D19-1088,C08-1018,0,0.0267726,"n) as introduced in Section 3.2. In Section 4.1, to ensure that the translation performance decrease attributes to the selected words instead of the perturbation operations, we randomly select the same number of words to perturb (Random), which serves as a baseline. Since there is no ranking for content words, we randomly select a set of content words as important words. To avoid the potential bias introduced by randomness (i.e., Random and Con• Deletion perturbation removes the selected words from the input sentence, and it can be regarded as a specific instantiation of sentence compression (Cohn and Lapata, 2008). • Mask perturbation replaces embedding vectors of the selected words with all-zero vectors (Arras et al., 2016), which is similar to Deletion perturbation except that it retains the placeholder. • Grammatical Replacement perturbation replaces a word by another word of the same linguistic role (i.e., POS tags), yielding a sentence that is grammatically correct but semantically nonsensical (Chomsky and Lightfoot, 2002; Gulordava et al., 2018), such as “colorless green ideas sleep furiously”. Figure 2 illustrates the experimental results on Chinese⇒English translation with Transformer. It 956 D"
D19-1088,P17-1106,0,0.0239557,"rovide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings. Concerning interpreting the input-out"
D19-1088,1997.mtsummit-papers.1,0,0.269192,"Missing"
D19-1088,D18-1548,0,0.0196634,"n, which we leave to the future work. 6 Acknowledgement Discussion and Conclusion We approach understanding NMT by investigating the word importance via a gradient-based method, which bridges the gap between word importance and translation performance. Empirical results show that the gradient-based method is superior to several black-box methods in estimating the word importance. Further analyses show that important words are of distinct syntactic categories on different language pairs, which might support the viewpoint that essential inductive bias should be introduced into the model design (Strubell et al., 2018). Our study also suggests the possibility of detecting the notorious under-translation problem via the gradient-based method. This paper is an initiating step towards the general understanding of NMT models, which may bring some potential improvements, such as Shilin He and Michael R. Lyu were supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14210717 of the General Research Fund), and Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award). We thank the anonymous reviewers for their insightful comments and sugg"
D19-1088,N18-1108,0,0.0186535,"Con• Deletion perturbation removes the selected words from the input sentence, and it can be regarded as a specific instantiation of sentence compression (Cohn and Lapata, 2008). • Mask perturbation replaces embedding vectors of the selected words with all-zero vectors (Arras et al., 2016), which is similar to Deletion perturbation except that it retains the placeholder. • Grammatical Replacement perturbation replaces a word by another word of the same linguistic role (i.e., POS tags), yielding a sentence that is grammatically correct but semantically nonsensical (Chomsky and Lightfoot, 2002; Gulordava et al., 2018), such as “colorless green ideas sleep furiously”. Figure 2 illustrates the experimental results on Chinese⇒English translation with Transformer. It 956 Deletion Mask Deletion Mask 2 2525 202020 202020 2020 151515 3 4 er of Operations 5 Random Random Random Frequency Frequency Frequency Content Content Content Attention Attention Attention Attribution Attribution Attribution 101010 0 0 0 1 11 2 22 3 33 4 4 4 5 55 Number Number ofOperations Operations Number of of Operations (a) Deletion 151515 BLEU BLEU 252525 BLEU BLEU BLEU 252525 BLEU BLEU BLEU m ncy t on tion Replacement (Same POS) Replacem"
D19-1088,P19-1354,1,0.774881,"dentifying undertranslated words. • We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings"
D19-1088,P17-1141,0,0.0261181,"comparing words of least importance and human-annotated under-translated words. As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding (Hokamp and Liu, 2017; Post and Vilar, 2018). In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section 5.1) and design better architectures for specific languages (Section 5.2). Due to the space limitation, we only analyze the results of Chinese⇒English, English⇒French, and English⇒Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold. 5.1 Type Noun Verb Adj. Prep. Dete. Punc. Others ≥2 1 (0, 1) 0 Low Middle High Effect on Detecting Translation Errors In this experiment, we propose to use the"
D19-1088,N19-1357,0,0.368665,"encent AI Lab 3 {zptu,brightxwang,vinnylywang,shumingshi}@tencent.com Abstract 2017) or hidden units (Bau et al., 2019; Ding et al., 2017). Another direction focuses on understanding the importance of input words by interpreting the input-output behavior of NMT models. Previous work (Alvarez-Melis and Jaakkola, 2017) treats NMT models as black-boxes and provides explanations that closely resemble the attention scores in NMT models. However, recent studies reveal that attention does not provide meaningful explanations since the relationship between attention scores and model output is unclear (Jain and Wallace, 2019). In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better. Specifically, we employ the integrated gradients method (Sundararajan et al., 2017) to attribute the output to the input words with the integration of first-order derivatives. We justify the gradient-based approach via quantitative comparison with black-box methods on a couple of perturbation operations, several language pairs, and two representative model architectures, demonstrating its superiority on estimating word im"
D19-1088,D18-1512,0,0.0622219,"Missing"
D19-1088,W17-5706,0,0.0176538,"first choose two large-scale datasets that are publicly available, i.e., Chinese-English and EnglishFrench. Since English, French, and Chinese all belong to the subject-verb-object (SVO) family, we choose another very different subject-object-verb (SOV) language, Japanese, which might bring some interesting linguistic behaviors in EnglishJapanese translation. For Chinese-English task, we use WMT17 Chinese-English dataset that consists of 20.6M sentence pairs. For English-French task, we use WMT14 English-French dataset that comprises 35.5M sentence pairs. For English-Japanese task, we follow (Morishita et al., 2017) to use the first two sections of WAT17 English-Japanese dataset that consists of 1.9M sentence pairs. Following the standard NMT procedure, we adopt the standard byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all language pairs. We believe that these datasets are large enough to confirm the rationality and validity of our experimental analyses. Evaluation We evaluate the effectiveness of estimating word importance by the translation performance decrease. More specifically, unlike the usual way, we measure the decrease of translation performance when perturbing"
D19-1088,N18-1119,0,0.0171115,"st importance and human-annotated under-translated words. As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding (Hokamp and Liu, 2017; Post and Vilar, 2018). In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section 5.1) and design better architectures for specific languages (Section 5.2). Due to the space limitation, we only analyze the results of Chinese⇒English, English⇒French, and English⇒Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold. 5.1 Type Noun Verb Adj. Prep. Dete. Punc. Others ≥2 1 (0, 1) 0 Low Middle High Effect on Detecting Translation Errors In this experiment, we propose to use the estimated word importan"
D19-1088,N19-4006,0,0.0510171,"Missing"
D19-1088,P16-1162,0,0.147475,"very different subject-object-verb (SOV) language, Japanese, which might bring some interesting linguistic behaviors in EnglishJapanese translation. For Chinese-English task, we use WMT17 Chinese-English dataset that consists of 20.6M sentence pairs. For English-French task, we use WMT14 English-French dataset that comprises 35.5M sentence pairs. For English-Japanese task, we follow (Morishita et al., 2017) to use the first two sections of WAT17 English-Japanese dataset that consists of 1.9M sentence pairs. Following the standard NMT procedure, we adopt the standard byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all language pairs. We believe that these datasets are large enough to confirm the rationality and validity of our experimental analyses. Evaluation We evaluate the effectiveness of estimating word importance by the translation performance decrease. More specifically, unlike the usual way, we measure the decrease of translation performance when perturbing a set of important words that are of top-most word importance in a sentence. The more translation performance degrades, the more important the word is. We use the standard BLEU score as the evaluation metric for"
D19-1088,D16-1159,0,0.194425,"-the-art results on a mass of language pairs with varying structural differences, such as English-French (Bahdanau et al., 2014; Vaswani et al., 2017) and Chinese-English (Hassan et al., 2018). However, so far not much is known about how and why NMT works, which pose great challenges for debugging NMT models and designing optimal architectures. The understanding of NMT models has been approached primarily from two complementary perspectives. The first thread of work aims to understand the importance of representations by analyzing the linguistic information embedded in representation vectors (Shi et al., 2016; Belinkov et al., ∗ Zhaopeng Tu is the corresponding author. Work was mainly done when Shilin He was interning at Tencent AI Lab. 953 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 953–962, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the feature importance. Starting from this observation, we exploit the intermediate gradients to better estimate word importance, which consistently outperforms its attention counterpart across model a"
D19-1135,P18-1008,0,0.310848,"e state of the art on a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Tan et al., 2018), and language representations (Devlin et al., 2018). However, a previous study empirically reveals that the hierarchical structure of the input sentence, which is essential for language understanding, is not well modeled by S ANs (Tran et al., 2018). Recently, hybrid models which combine the strengths of S ANs and recurrent neural networks (R NNs) have outperformed both individual architectures on a machine translation task (Chen et al., 2018). We attribute the improvement to that R NNs complement S ANs on the representation limitation of hi∗ erarchical structure, which is exactly the strength of R NNs (Tran et al., 2018). Starting with this intuition, we propose to further enhance the representational power of hybrid models with an advanced R NNs variant – Ordered Neurons L STM (O N -L STM, Shen et al., 2019). O N -L STM is better at modeling hierarchical structure by introducing a syntax-oriented inductive bias, which enables R NNs to perform tree-like composition by controlling the update frequency of neurons. Specifically, we s"
D19-1135,P18-1198,0,0.0353809,"ed with sequential context (Chen et al., 2018). Moreover, to dispel the doubt that whether the improvement of hybrid model comes from the increasement of parameters. We investigate the 8layers L STM and 10-layers S ANs encoders (Rows 3-4) which have more parameters compared with the proposed hybrid model. The results show that the hybrid model consistently outperforms these model variants with less parameters and the improvement should not be due to more parameters. 3.2 0.7 Targeted Linguistic Evaluation To gain linguistic insights into the learned representations, we conducted probing tasks (Conneau et al., 2018) to evaluate linguistics knowledge embedded in the final encoding representation learned by model, as shown in Table 3. We evaluated S ANs and proposed hybrid model with Short-Cut connection on these 10 targeted linguistic evaluation tasks. The tasks and model details are described in Appendix A.2. Experimental results are presented in Table 3. Several observations can be made here. The proposed hybrid model with short-cut produces more informative representation in most tasks (“Final” in “S” vs. in “Hybrid+Short-Cut”), indicating that the effectiveness of the model. The only exception are sur"
D19-1135,P19-1032,0,0.0251266,"e a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studied for a long time as the na"
D19-1135,D18-1457,1,0.795963,"pdate frequencies. It also equals to the probability of each position’s value being 1 in the ideal master gate. Since this ideal master gate is binary, CU(·) is the expectation of the ideal master gate. Based on this activation function, the master gates are defined as f˜t = CUf (xt , ht−1 ), ˜it = 1 − CUi (xt , ht−1 ), (8) (9) where xt is the current input and ht−1 is the hidden state of previous step. CUf and CUi are two individual activation functions with their own trainable parameters. Short-Cut Connection Inspired by previous work on exploiting deep representations (Peters et al., 2018; Dou et al., 2018), we propose to simultaneously expose both types of signals by explicitly combining them with a simple short-cut connection (He et al., 2016). Similar to positional encoding injection in Transformer (Vaswani et al., 2017), we add the output of the O N -L STM encoder to the output of S ANs encoder: 1337 L b = HK H O N -L STM + HS ANs , (10) # 1 2 3 4 5 6 7 8 9 Encoder Architecture Base Model 6L S ANs 6L L STM 6L O N -L STM 6L L STM + 4L S ANs 6L O N -L STM + 4L S ANs 3L O N -L STM + 3L S ANs + Short-Cut Big Model 6L S ANs Hybrid Model + Short-Cut Para. BLEU 88M 97M 110M 104M 123M 99M 99M 27.31"
D19-1135,P15-1150,0,0.201741,"Missing"
D19-1135,D19-1082,1,0.811413,"Missing"
D19-1135,D18-1503,0,0.151578,"rence tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure. 1 Introduction Self-attention networks (S ANs, Lin et al., 2017) have advanced the state of the art on a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Tan et al., 2018), and language representations (Devlin et al., 2018). However, a previous study empirically reveals that the hierarchical structure of the input sentence, which is essential for language understanding, is not well modeled by S ANs (Tran et al., 2018). Recently, hybrid models which combine the strengths of S ANs and recurrent neural networks (R NNs) have outperformed both individual architectures on a machine translation task (Chen et al., 2018). We attribute the improvement to that R NNs complement S ANs on the representation limitation of hi∗ erarchical structure, which is exactly the strength of R NNs (Tran et al., 2018). Starting with this intuition, we propose to further enhance the representational power of hybrid models with an advanced R NNs variant – Ordered Neurons L STM (O N -L STM, Shen et al., 2019). O N -L STM is better at mo"
D19-1135,N19-1122,1,0.818568,"Missing"
D19-1135,P19-1624,1,0.822984,"odel. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studie"
D19-1135,C16-1229,0,0.0215293,"eling of hierarchical structure is an essential strength of hybrid models over the vanilla S ANs. • Our study proves that the idea of augmenting R NNs with ordered neurons (Shen et al., 2019) produces promising improvement on machine translation, which is one potential criticism of O N -L STM. 1336 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1336–1341, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Approach Partially motivated by Wang et al. (2016) and Chen et al. (2018), we stack a S ANs encoder on top of a R NNs encoder to form a cascaded encoder. In the cascaded encoder, hierarchical structure modeling is enhanced in the bottom R NNs encoder, based on which S ANs encoder is able to extract representations with richer hierarchical information. Let X = {x1 , . . . , xN } be the input sequence, the representation of the cascaded encoder is calculated by HK R NNs = E NC R NNs (X), (1) HL S ANs (2) = E NC S ANs (HK R NNs ), where E NC R NNs (·) is a K-layer R NNs encoder that reads the input sequence, and E NC S ANs (·) is a Llayer S ANs"
D19-1135,D18-1408,0,0.161685,"mechanism, as the R NNs counterpart to boost the hybrid model. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neura"
D19-1135,D18-1475,1,0.736722,"structured gating mechanism, as the R NNs counterpart to boost the hybrid model. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure M"
D19-1135,N18-1202,0,0.023472,"s denotes different update frequencies. It also equals to the probability of each position’s value being 1 in the ideal master gate. Since this ideal master gate is binary, CU(·) is the expectation of the ideal master gate. Based on this activation function, the master gates are defined as f˜t = CUf (xt , ht−1 ), ˜it = 1 − CUi (xt , ht−1 ), (8) (9) where xt is the current input and ht−1 is the hidden state of previous step. CUf and CUi are two individual activation functions with their own trainable parameters. Short-Cut Connection Inspired by previous work on exploiting deep representations (Peters et al., 2018; Dou et al., 2018), we propose to simultaneously expose both types of signals by explicitly combining them with a simple short-cut connection (He et al., 2016). Similar to positional encoding injection in Transformer (Vaswani et al., 2017), we add the output of the O N -L STM encoder to the output of S ANs encoder: 1337 L b = HK H O N -L STM + HS ANs , (10) # 1 2 3 4 5 6 7 8 9 Encoder Architecture Base Model 6L S ANs 6L L STM 6L O N -L STM 6L L STM + 4L S ANs 6L O N -L STM + 4L S ANs 3L O N -L STM + 3L S ANs + Short-Cut Big Model 6L S ANs Hybrid Model + Short-Cut Para. BLEU 88M 97M 110M 104M"
D19-1135,N19-1407,1,0.797897,"R NNs counterpart to boost the hybrid model. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP S"
D19-1135,P16-1162,0,0.0710345,"slation task. “↑ / ⇑”: significant over the conventional self-attention counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling. “6L S ANs” is the state-of-the-art Transformer model. “nL L STM + mL S ANs” denotes stacking n L STM layers and m S ANs layers subsequently. “Hybrid Model” denotes “3L O N -L STM + 3L S ANs”. 3 Encoder Architecture 3L O N -L STM → 3L S ANs 3L S ANs → 3L O N -L STM 8L L STM 10L S ANs Machine Translation For machine translation, we used the benchmark WMT14 English⇒German dataset. Sentences were encoded using byte-pair encoding (BPE) with 32K word-piece vocabulary (Sennrich et al., 2016). We implemented the proposed approaches on top of T RANSFORMER (Vaswani et al., 2017) – a state-of-the-art S ANs-based model on machine translation, and followed the setting in previous work (Vaswani et al., 2017) to train the models, and reproduced their reported results. We tested on both the Base and Big models which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and number of attention heads (8 vs. 16). All the model variants were implemented on the encoder. The implementation details are introduced in Appendix A.1. Table 1 lists the results. Baselines (Rows 1-3) Follow"
D19-1135,D13-1170,0,0.00435154,"ormation. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studied for a long time as the natural language sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). With the emergence of deep learning, tree-based models have been proposed to integrate syntactic tree structure into Recursive Neural Networks (Socher et al., 2013), L STMs (Tai et al., 2015), C NNs (Mou et al., 2016). As for S ANs, Hao et al. (2019a), Ma et al. (2019) and Wang et al. (2019b) enhance the S ANs with neural syntactic distance, multigranularity attention scope and structural position representations, which are generated from the syntactic tree structures. Closely related to our work, Hao et al. (2019b) find that the integration of the recurrence in S ANs encoder can provide more syntactic structure feaSamuel R. Bowman, Christopher D. Manning, and Christopher Potts. 2015. Tree-structured composition in neural networks without tree-structured"
D19-1145,W13-2322,0,0.0389766,"of relationship path between words, sequential PE measures the sequential distance between the words. As shown in Figure 1 (a), for each word, absolute sequential position represents the sequential distance to the beginning of the sentence, while relative sequential position measures the relative distance to the queried word (“talk” in the example). The latent structure can be interpreted in various ways, from syntactic tree structures, e.g., constituency tree (Collins, 2003) or dependency tree (K¨ubler et al., 2009), to semantic graph structures, e.g., abstract meaning representation graph (Banarescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the tree depth of the word in the dependency tree as its absolute structural position. Specifically, we treat the main verb (Tapanainen and Jarvi"
D19-1145,J03-4003,0,0.205527,"ctic relationships among input words. Figure 1 shows an example to illustrate the idea of the proposed approach. From the perspective of relationship path between words, sequential PE measures the sequential distance between the words. As shown in Figure 1 (a), for each word, absolute sequential position represents the sequential distance to the beginning of the sentence, while relative sequential position measures the relative distance to the queried word (“talk” in the example). The latent structure can be interpreted in various ways, from syntactic tree structures, e.g., constituency tree (Collins, 2003) or dependency tree (K¨ubler et al., 2009), to semantic graph structures, e.g., abstract meaning representation graph (Banarescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the"
D19-1145,P18-1198,0,0.197153,"we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to encode the distance of each word pair in the tree. We implement our structural encoding strategies on top of T RANSFORMER (Vaswani et al., 2017) and conduct experiments on both NIST Chinese⇒English and WMT14 English⇒German translation tasks. Experimental results show that exploiting structural position encoding strategies consistently boosts performance over both the absolute and relative sequential position representations across language pairs. Linguistic analyses (Conneau et al., 2018) reveal that the proposed structural position representation improves the translation performance with richer syntactic information. Our main contributions are: • Our study demonstrates the necessity and effectiveness of exploiting structural position encoding for S ANs, which benefits from modeling syntactic depth and distance under the latent structure of the sentence. • We propose structural position representations for S ANs to encode the latent structure of the input sentence, which are complementary to their sequential counterparts. 2 Background Self-Attention SANs produce representation"
D19-1145,N19-1423,0,0.0455592,"e to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese⇒English and WMT14 English⇒German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to"
D19-1145,D18-1457,1,0.850735,"and structure position representations1 : asb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utili"
D19-1145,N16-1024,0,0.0650603,"Missing"
D19-1145,P17-2012,0,0.0278999,"Missing"
D19-1145,D19-1082,1,0.844139,"Missing"
D19-1145,D19-1135,1,0.917158,", 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et al. (2019c,b) integrate the recurrence into the S ANs and empirically demonstrate that the hybrid models achieve better performances by modeling structure of sentences. Hao et al. (2019a) further make use of the multi-head attention to form the multi-granularity self-attention, to capture the different granularity phrases in source sentences. The difference is that we treat the position representation as a medium to transfer the structure information from the dependency tree into the S ANs. 1 We also use parameter-free element-wise addition method to combine two absolute position embedding and get 0.28"
D19-1145,P03-1054,0,0.105534,"ST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed position encoding strategies on T RANSFORMER (Vaswani et al., 2017) and implement them on top of THUMT (Zhang et al., 2017). We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the structural structural absolute and relative position as described in Section 3. When using relative structural position encoding, we use clipping distance r = 16. To make a fair comparison, we valid different position encoding strategies on the encoder and keep the T RANSFORMER decoder unchanged. Effect of Position Encoding We first remove the sequential encoding from the Transformer encoder (Model #1) and observe the translation performance degrades dramatically (28.33 − 44.31 = −15.98), which demonstrates the necessity of the position encoding strategie"
D19-1145,W04-3250,0,0.0760548,"t al. (2019c) Transformer-Big + Structural PE + Relative Sequential PE + Structural PE MT04 46.49 47.12↑ 47.01 47.37⇑ Zh⇒En MT05 45.21 45.84 45.65 46.20⇑ MT06 44.87 45.64⇑ 45.87⇑ 46.18⇑ Avg 45.47 46.06 46.00 46.40 En⇒De WMT14 28.98 28.58 28.88 28.90 29.19⇑ Table 2: Evaluation of translation performance on NIST Zh⇒En and WMT14 En⇒De test sets. Hao et al. (2019c) is a Transformer-Big model which adopted an additional recurrence encoder with the attentive recurrent network to model syntactic structure. “↑ / ⇑”: significant over the Transformer-Big (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). Model BASE + Rel. Seq. PE + Stru. PE SeLen 92.20 89.82 89.54 Surface WC Avg 63.00 77.60 63.17 76.50 62.90 76.22 TrDep 44.74 45.09 46.12 Syntactic ToCo BShif 79.02 71.24 78.45 71.40 79.12 72.36 Avg 65.00 64.98 65.87 Tense 89.24 88.74 89.30 SubN 84.69 87.00 85.47 Semantic ObjN SoMo 84.53 52.13 85.53 51.68 84.94 52.90 CoIn 62.47 62.21 62.99 Avg 74.61 75.03 75.12 Table 3: Performance on linguistic probing tasks. The probing tasks were conducted by evaluating linguistics embedded in the Transformer-Base encoder outputs. “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” denote TransformerBase, Transformer-Ba"
D19-1145,P02-1040,0,0.103911,". The experimental results on the development set are shown in Table 1. Table 1: Impact of the position encoding components on Chinese⇒English NIST02 development dataset using Transformer-Base model. “Abs.” and “Rel.” denote absolute and relative position encoding, respectively. “Spd.” denotes the decoding speed (sentences/second) on a Tesla M40, the speed of structural position encoding strategies include the step of dependency parsing. 5 Model Variations Experiment We conduct experiments on the widely used NIST Chinese⇒English and WMT14 English⇒German data, and report the 4-gram BLEU score (Papineni et al., 2002). Chinese⇒English We use the training dataset consists of about 1.25 million sentence pairs. NIST 2002 (MT02) dataset is used as development set. NIST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed posit"
D19-1145,D16-1244,0,0.107899,"Missing"
D19-1145,N18-2074,0,0.405265,"n tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structur"
D19-1145,Q19-1002,0,0.0399602,"Missing"
D19-1145,D18-1548,0,0.0412553,", we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese⇒English and WMT14 English⇒German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this"
D19-1145,P19-1032,0,0.0226086,"ere fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et al. (2019c,b) integrate the"
D19-1145,N19-1122,1,0.884589,"Missing"
D19-1145,P15-1150,0,0.0469481,"ks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structural depths and distances (Hewitt and Manning, 2019). Accordingly, we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to enc"
D19-1145,N19-1419,0,0.0324287,"pture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structural depths and distances (Hewitt and Manning, 2019). Accordingly, we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to encode the distance of each word pair in the tree. We implement our structural encoding strategies on top of T RANSFORMER (Vaswani et al., 2017) and conduct experiments on both NIST Chinese⇒English and WMT14 English⇒German translation tasks. Experimental results show that exploiting structural position encoding strategies consistently boosts performance over both the absolute and relative sequential position representations across language pairs. Lingui"
D19-1145,A97-1011,0,0.169453,"arescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the tree depth of the word in the dependency tree as its absolute structural position. Specifically, we treat the main verb (Tapanainen and Jarvinen, 1997) of the sentence as the origin and use the distance of the dependency path from the target word to the origin as the absolute structural position absstru (xi ) = distancetree (xi , origin), (5) where xi is the target word, tree is the given dependency structure and the origin is the main verb of the tree. In the field of NMT, BPE sub-words and endof-sentence symbol should be carefully handled as they do not appear in the conventional dependency tree. In this work, we assign the BPE sub-words share the absolute structural position of the original word and set the the first larger integer than t"
D19-1145,C18-1255,0,0.0502484,"sb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structur"
D19-1145,P19-1624,1,0.835259,"uctural PE) achieves further improvement up to +0.40 BLEU points and outperforms the Transformer-Big by 0.93 BLEU points. For English⇒German, similar phenomenon is observed, which reveals that the proposed structural position encoding strategy can consistently boost translation performance over both the absolute and relative sequential position representations. 5.3 Linguistic Probing Evaluation We conduct probing tasks3 (Conneau et al., 2018) to evaluate structure knowledge embedded in the encoder output in the variations of the Base model that are trained on En⇒De translation task. We follow Wang et al. (2019) to set model configurations. The experimental results on probing tasks are shown in Table 3, and the BLEU scores of “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” are 3 https://github.com/facebookresearch/ SentEval/tree/master/data/probing 27.31, 27.99 and 28.30. From the table, we can see 1) adding the relative sequential positional embedding achieves improvement over the baseline on semantic tasks (75.03 vs. 74.61). This may indicate the model benefits more from semantic modeling; 2) with the structural positional embedding, the model obtains improvement on syntactic tasks (65.87 v.s. 64.98), which"
D19-1145,D18-1408,0,0.0426954,"PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representation"
D19-1145,D18-1475,1,0.83168,"epresentations1 : asb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree t"
D19-1145,N19-1407,1,0.85324,"S PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et"
D19-1145,P17-4012,0,0.105233,"002 (MT02) dataset is used as development set. NIST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed position encoding strategies on T RANSFORMER (Vaswani et al., 2017) and implement them on top of THUMT (Zhang et al., 2017). We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the structural structural absolute and relative position as described in Section 3. When using relative structural position encoding, we use clipping distance r = 16. To make a fair comparison, we valid different position encoding strategies on the encoder and keep the T RANSFORMER decoder unchanged. Effect of Position Encoding We first remove the sequential encoding from the Transformer encoder (Model #1) and observe the translation performance degrades dramatically (28.33 − 44.31 = −15.98), which demonst"
D19-1195,P19-1389,1,0.832148,"律的 What is irregular? 有啥不好意思的 Why you are shy? 我也不规律 I am irregular too. See a doctor, the easiest way is with Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our model. Darker color indicates bigger matching scores and the words being selected for skeleton are in red boxes. sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response. Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Bi et al., 2019; Tian et al., 2019; Gao et al., 2019) have been proposed for this problem. Some previous studies have been about using the results of traditional retrieval systems for informative response generation. Song et al. (2016) introduced an extra encoder for the retrieved response. The encoder’s output, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical diff"
D19-1195,D18-1297,1,0.852868,"rregular too. 有啥不规律的 What is irregular? 有啥不好意思的 Why you are shy? 我也不规律 I am irregular too. See a doctor, the easiest way is with Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our model. Darker color indicates bigger matching scores and the words being selected for skeleton are in red boxes. sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response. Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Bi et al., 2019; Tian et al., 2019; Gao et al., 2019) have been proposed for this problem. Some previous studies have been about using the results of traditional retrieval systems for informative response generation. Song et al. (2016) introduced an extra encoder for the retrieved response. The encoder’s output, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes"
D19-1195,D15-1166,0,0.0206424,"ators in unsure cases. A set of 300 different query samples are used for evaluation. We recruit five experienced annotators and take the average score among them. Besides, we also use dist-1/dist-2 (Li et al., 2016a) to examine a model’s ability for generating diverse responses. which is the number of distinct unigrams/bi-grams divided by the total number. 3.2 Compared Methods To show the effectiveness of our proposed methods, we compare it with the following methods. • Retrieval The underlying retrieval system used in our experiments. • Seq2Seq The basic Seq2Seq model (Bahdanau et al., 2014; Luong et al., 2015) that only takes the query as input. • Seq2Seq-MMI A variant of the basic Seq2Seq model that uses Maximum Mutual Information (MMI) for filtering out generic responses (Li et al., 2016a). Concretely, a response-toquery Seq2Seq model is trained and used to 4 Douban https://www.douban.com/ and Weibo https://www.weibo.com/ 5 https://ai.qq.com/product/nlpchat. shtml • EditVec The model proposed in Wu et al. (2019). In addition to the retrieved response, the lexical difference (insert words and delete words) between the query and the retrieved query is also encoded (in a so-called edit vector) to fe"
D19-1195,N18-1204,0,0.0168843,"context similarity, yet their work is done in close domain conversation. The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks. For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation. Wiseman et al. (2017, 2018) used either fixed template or learned templates for data-to-text generation. Xu et al. (2018) conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling. Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input. Gu et al. (2018) uses retrieved translation as a reference to the generative translation model. 5 Conclusion In this paper, we presented a novel framework, matching-to-generation, for retrieval-guided response generation. Our method uses an interpretable matching model for response skeleton extraction and a robust response generator for response completion. The two components are trained separately to allow more flexibility. Experiments show our method significantly outperforms several strong baselines. 1873 Refer"
D19-1195,C16-1316,0,0.0421746,"Missing"
D19-1195,Q18-1031,0,0.0661934,"decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical differences between the current query and the retrieved query. Besides, Pandey et al. (2018) proposed to weight different training instances by context similarity, yet their work is done in close domain conversation. The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks. For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation. Wiseman et al. (2017, 2018) used either fixed template or learned templates for data-to-text generation. Xu et al. (2018) conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling. Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input. Gu et al. (2018) uses retrieved translation as a reference to the generative translation model. 5 Conclusion In this paper, we presented a n"
D19-1195,P18-1123,0,0.153056,"l., 2016a). This problem is avoided in traditional retrieval systems (Ji et al., 2014; Hu et al., 2014) by preceding the selection of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decoder framework, early attempts have either used an extra encoder for the retrieved response (Song et al., 2016; ∗ Vanilla seq2seq retrieve This work was mainly done when Deng Cai was an intern at Tencent AI Lab. Yan Wang is the corresponding author. Pandey et al., 2018; Wu et al., 2019) or a unified encoder for the concatenation of the query and the retrieved response (Weston et al., 2018). To prevent the inflow of erroneous information, Cai et al. (2019) proposed a general framework that first extracts a skeleton from the retrieved response and then generates the response based on the extracted skeleton. Despite their differences, a common issue is that the generation model easily learns to ignore the retrieved response entirely and collapses to a vanilla seq2seq model. As shown in Figure 1, this happens with improper training instances. Given the large sp"
D19-1195,P19-1372,1,0.886718,"Missing"
D19-1195,P15-1152,0,0.16558,"Missing"
D19-1195,N15-1020,0,0.209982,"neration is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs. 1 collapse Bad, I hate the weather. mismatch generate generate Response: Great, I get promotion today. Figure 1: The common problem for training a retrievalguided generation model in previous work. The model is forced to neglect the retrieved response even though it is a proper response, due to the mismatch between the retrieved response and the target response. Introduction Sequence-to-sequence (seq2seq) neural models (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a) have been popular for single-turn dialogue response generation. However, many of the generated responses (e.g., “I don’t know” and “I think so”) appear to be generic and dull (safe response problem) (Li et al., 2016a). This problem is avoided in traditional retrieval systems (Ji et al., 2014; Hu et al., 2014) by preceding the selection of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decod"
D19-1195,N16-1014,0,0.69571,"ained generator. Extensive experiments demonstrate the effectiveness of our model designs. 1 collapse Bad, I hate the weather. mismatch generate generate Response: Great, I get promotion today. Figure 1: The common problem for training a retrievalguided generation model in previous work. The model is forced to neglect the retrieved response even though it is a proper response, due to the mismatch between the retrieved response and the target response. Introduction Sequence-to-sequence (seq2seq) neural models (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a) have been popular for single-turn dialogue response generation. However, many of the generated responses (e.g., “I don’t know” and “I think so”) appear to be generic and dull (safe response problem) (Li et al., 2016a). This problem is avoided in traditional retrieval systems (Ji et al., 2014; Hu et al., 2014) by preceding the selection of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decoder framework, early attempts have eith"
D19-1195,P16-1094,0,0.0645175,"Missing"
D19-1195,P19-1371,1,0.839076,"lar? 有啥不好意思的 Why you are shy? 我也不规律 I am irregular too. See a doctor, the easiest way is with Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our model. Darker color indicates bigger matching scores and the words being selected for skeleton are in red boxes. sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response. Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Bi et al., 2019; Tian et al., 2019; Gao et al., 2019) have been proposed for this problem. Some previous studies have been about using the results of traditional retrieval systems for informative response generation. Song et al. (2016) introduced an extra encoder for the retrieved response. The encoder’s output, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical differences between the"
D19-1195,W18-5713,0,0.28376,"election of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decoder framework, early attempts have either used an extra encoder for the retrieved response (Song et al., 2016; ∗ Vanilla seq2seq retrieve This work was mainly done when Deng Cai was an intern at Tencent AI Lab. Yan Wang is the corresponding author. Pandey et al., 2018; Wu et al., 2019) or a unified encoder for the concatenation of the query and the retrieved response (Weston et al., 2018). To prevent the inflow of erroneous information, Cai et al. (2019) proposed a general framework that first extracts a skeleton from the retrieved response and then generates the response based on the extracted skeleton. Despite their differences, a common issue is that the generation model easily learns to ignore the retrieved response entirely and collapses to a vanilla seq2seq model. As shown in Figure 1, this happens with improper training instances. Given the large space of possible responses, it happens frequently that a retrieved response (extracted skeleton) is suitable for responding"
D19-1195,D18-1356,0,0.0916317,"Missing"
D19-1195,D18-1462,0,0.0231513,"ther introduced to encodes the lexical differences between the current query and the retrieved query. Besides, Pandey et al. (2018) proposed to weight different training instances by context similarity, yet their work is done in close domain conversation. The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks. For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation. Wiseman et al. (2017, 2018) used either fixed template or learned templates for data-to-text generation. Xu et al. (2018) conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling. Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input. Gu et al. (2018) uses retrieved translation as a reference to the generative translation model. 5 Conclusion In this paper, we presented a novel framework, matching-to-generation, for retrieval-guided response generation. Our method uses an interpretable matching model for response skeleton extraction and a robust response generato"
D19-1195,P18-1101,0,0.0560756,"Missing"
I11-1145,P05-1033,0,0.668587,"i Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji"
I11-1145,P05-1066,0,0.271267,"Missing"
I11-1145,N03-1017,0,0.106149,"han economy ’s China fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not e"
I11-1145,P06-1077,1,0.914482,"Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy"
I11-1145,D10-1053,0,0.130861,"Missing"
I11-1145,D09-1106,1,0.815845,"Missing"
I11-1145,P08-1010,0,0.0172715,"the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different translation models (e.g"
I11-1145,D08-1022,0,0.184832,"Missing"
I11-1145,P08-1115,0,0.0604813,"s BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different translation models (e.g. phrase-based, hierarchical phrase-based and tree-based models). They use phrase po"
I11-1145,P02-1038,0,0.0206047,"p(′ s|N U LL) × 0.36 ×   p(economy|jingji) × 1.0 ′ ′ Here the probability that economy translates a source NULL token is 0.0. 4 Experiments 4.1 Data Preparation Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system (Liu et al., 2006). We train a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995). We optimize feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2003/2004/2005 test sets. To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used “grow-diag-finaland” (Koehn et al., 2003) to all 20 × 20 bidirectional alignment pairs. We follow Liu et al. (2009) to use ps2t × pt2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignmen"
I11-1145,P06-1121,0,0.373917,"and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this p"
I11-1145,P09-1104,0,0.0325122,"Missing"
I11-1145,2006.amta-papers.8,0,0.437981,"Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural so"
I11-1145,J04-4002,0,0.0845271,"o-string model. fazhan economy ’s China fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de,"
I11-1145,P02-1040,0,0.0830624,"oken is 0.0. 4 Experiments 4.1 Data Preparation Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system (Liu et al., 2006). We train a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995). We optimize feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2003/2004/2005 test sets. To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used “grow-diag-finaland” (Koehn et al., 2003) to all 20 × 20 bidirectional alignment pairs. We follow Liu et al. (2009) to use ps2t × pt2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair. We obtained n-best lists"
I11-1145,P08-1066,0,0.158557,"Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural solution is to extract"
I11-1145,J10-4005,0,0.0532568,"s the complement. Note that the probabilities of “Shared” rules are different for the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignm"
I11-1145,P03-1041,0,0.0272029,"ed” rules are different for the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different"
I11-1145,2008.amta-papers.18,0,0.410017,"ord aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural solution is to extract rules from nbest alignments (Venugopal et al., 2008). However, using n-best alignments still face two major challenges. First, n-best alignments have to be processed individually although they share many links, see (zhongguo, China) and (jingji, economy) in Figure 1. Second, regardless of probabilities of links in each alignment, numerous wrong rule would be extracted from n-best alignments. For example, a wrong rule (X1 de jingji, of X1 ’s economy) would be extracted from the alignment in Figure 1(a). Since Liu et al. (2009) show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical"
I11-1145,P06-1066,1,0.872693,"fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hi"
I11-1145,P05-1059,0,0.0789146,"Missing"
I11-1145,J07-2003,0,\N,Missing
I17-3009,P07-2045,0,0.0193009,"Missing"
I17-3009,P03-1021,0,0.0209504,"SLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4, Recog indicates NE recognition on the"
I17-3009,J03-1002,0,0.0059894,"20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4,"
I17-3009,D17-1301,1,0.67952,"Missing"
I17-3009,N16-1113,1,0.838057,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
I17-3009,L16-1436,1,0.749687,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
L16-1436,P12-2040,0,0.0758794,"consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Bu"
L16-1436,W11-0609,0,0.0490398,"2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing."
L16-1436,itamar-itai-2008-using,0,0.207421,"MT) of conversational material by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discou"
L16-1436,P07-2045,0,0.0127474,"Missing"
L16-1436,matsubara-etal-2002-bilingual,0,0.0781911,"Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Dane"
L16-1436,W12-0117,0,0.0160222,"ed Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged"
L16-1436,J03-1002,0,0.00968861,"Missing"
L16-1436,P03-1021,0,0.0215298,"Missing"
L16-1436,prasad-etal-2008-penn,0,0.0125033,"ue MT system. The rest of the paper is organized as follows. In Section 2, we describe related work. Section 3 describes in detail our approaches to build a dialogue corpus as well as the structure of the generated database. The experimental results for both corpus annotation and translation are reported in Section 4. Finally, Section 5 presents our conclusions and future work plans. 2. Related Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue co"
L16-1436,schmitt-etal-2012-parameterized,0,0.0221098,"g three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags using clues such as format and story structure tags in the script; (2) for a bilingual subtitle, we align each sentence with its tra"
L16-1436,tiedemann-2008-synchronizing,0,0.197336,"aterial by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bri"
L16-1436,tiedemann-2012-parallel,0,0.140544,"tructure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bridging the information in these two kin"
L16-1436,walker-etal-2012-annotated,0,0.171098,"013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags"
L16-1436,O12-1015,1,0.894007,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W12-6310,1,0.929649,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W14-3331,1,0.893281,"Missing"
L16-1436,zhang-etal-2014-dual,0,0.0684567,"eyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ c"
N16-1113,P11-2037,0,0.0300195,"Missing"
N16-1113,D13-1135,0,0.169061,"Missing"
N16-1113,D10-1062,0,0.769955,"hat there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our"
N16-1113,P07-2045,0,0.0104744,"ing the approach described in Section 2.1. There are two different language models for the DP annotation (Section 2.1) and translation tasks, respectively: one is trained on the 2.13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016). Corpus Lang. Sentents Train Dev Test ZH EN ZH EN ZH EN 1,037,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5,"
N16-1113,D10-1086,0,0.132,"Missing"
N16-1113,W10-1737,0,0.380169,"Missing"
N16-1113,D09-1106,1,0.87907,"Missing"
N16-1113,P13-2064,1,0.842545,"missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our task is that ZP cont"
N16-1113,C14-1003,0,0.0608436,"Missing"
N16-1113,J03-1002,0,0.0251063,"on. Related work is described in Section 3. The experimental results for both the DP generator and translation are reported in Section 4. Section 5 analyses some real examples which is followed by our conclusion in Section 6. 2 Methodology The architecture of our proposed method is shown in Figure 2, which can be divided into three phases: DP corpus annotation, DP generation, and SMT integration. 2.1 DP Training Corpus Annotation We propose an approach to automatically annotate DPs by utilizing alignment information. Given a parallel corpus, we first use an unsupervised word alignment method (Och and Ney, 2003; Tu et al., 2012) to produce a word alignment. From observing of the alignment matrix, we found it is possible to detect DPs by projecting misaligned pronouns from the non-pro-drop target side (English) to the pro-drop source side (Chinese). In this work, we focus on nominative and accusative pronouns including personal, possessive and reflexive instances, as listed in Table 1. Figure 2: Architecture of proposed method. Category Subjective Personal Objective Personal Possessive Objective Possessive Reflexive Pronouns 我 (I), 我们 (we), 你/你们 (you), 他 (he), 她 (she), 它 (it), 他们/她们/它 们 (they). 我 (me"
N16-1113,P03-1021,0,0.094528,"37,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5, the size of the single hidden layer = 200, iterations = 10, embeddings = 200. The MLP classifier use random initialized embeddings, with the following settings: the size of the single hidden layer = 200, embeddings = 100, iterations = 200. For end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure 5 Available at http://www.sogou"
N16-1113,P02-1040,0,0.0971952,"on, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, the contributions of this paper include the following: • We propose an automatic method to build a large-scale DP training corpus. Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task; • Benefit"
N16-1113,W12-4501,0,0.0351916,"Missing"
N16-1113,N07-1029,0,0.0127834,"anslation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences. More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a “pronoun-complete” translation model. In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, th"
N16-1113,W12-4213,0,0.348159,"Missing"
N16-1113,C10-1123,1,0.824671,"et language, so that the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The di"
N16-1113,I11-1145,1,0.817536,"hat the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our t"
N16-1113,C12-2122,1,0.905413,"Missing"
N16-1113,L16-1436,1,0.843791,"Missing"
N16-1113,P13-1081,0,0.692401,"se pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our DP generator. We div"
N16-1113,N13-1125,0,0.0308356,"Missing"
N16-1113,C10-2158,0,0.0510881,"Missing"
N16-1113,P15-2051,0,0.709643,"consists of 1M sentence pairs extracted from movie and TV episode subtitles. We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. Aft"
N16-1113,zhang-etal-2014-dual,0,0.0371089,"Missing"
N16-1113,D07-1057,0,0.339977,"Missing"
N18-1125,P05-1033,0,0.188078,"dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation error due to the incorrect attention. (b) With the help of the target foresight information “VBZ”, TFA-NMT is likely to figure out the exact translation as the reference in (c). The light font denotes the target words to be translated in"
N18-1125,P11-2031,0,0.0255199,"Missing"
N18-1125,2016.amta-researchers.10,0,0.161257,"auxiliary information from a target foresight word into the attention model. However, there is a significant difference between our approach and their approaches. Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs. The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning. In contrast, Mi et al. (2016); Liu et al. (2016b); Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning. They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention. 6 Conclusion It has been argued that the traditional attention model in neural machine translation suf1387 System Model Dev MT05 MT06 MT08 Ave. (Liu et al., 2016b) Moses NMT-J – – 35.4 36.8 33.7 36.9 25.0 28.5 31.37 34.07 (Liu et al., 20"
N18-1125,C16-1290,0,0.0446104,"sh and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation er"
N18-1125,P07-2045,0,0.0110597,".1 1844.9 72.0 1666.1 70.1 1485.2 59.1 Performance BLEU FPA 38.65 – 38.57 – 38.83 69.03 39.26 69.95 40.63 71.91 Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a"
N18-1125,N03-1017,0,0.0510404,"shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation error due to the incorrect attention. (b) With the help of the target foresight information “VBZ”, TFA-NMT is likely to figure out the exact translation as the reference in (c). The light font denotes the target words to be"
N18-1125,C16-1291,1,0.743925,"Missing"
N18-1125,N16-1046,1,0.904938,"tly the same as the original concept in alignment task (Peter et al., 2017). However, both of them share a common idea that foresight word should be at a later time step, and thus we respect the work in Peter et al. (2017) and maintain the same concept for easier understanding. 1380 Proceedings of NAACL-HLT 2018, pages 1380–1390 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics i.e. the first light color word in Figure 1(a), is not known but to be translated at the next time step. Therefore, this may lead to inadequate modeling for attention mechanism (Liu et al., 2016a; Peter et al., 2017). Regarding to this, Peter et al. (2017) explicitly feed this target word into the attention model, and demonstrate the significant improvements in alignment accuracy. Unfortunately, this approach relies on the premise that the target foresight word is available in advance in its alignment scenario, and thus it can not be used in the translation scenario. To address this issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks. Its basic idea includes two steps: it firstly designs an auxiliary mechani"
N18-1125,P05-1057,0,0.582372,"is issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks. Its basic idea includes two steps: it firstly designs an auxiliary mechanism to predict some information for the target foresight word which is helpful for alignment; and then it feeds the predicted result into the attention model for translation. For the sake of efficiency, instead of predicting the target foresight word with large vocabulary size, we only predict its partial information, i.e. partof-speech tag, which is proved to be helpful for word alignment (Liu et al., 2005). Figure 1(b) shows the main idea of TFA based on NMT. In order to remit the negative effects due to the prediction errors, we feed the distribution of the prediction result instead of the maximum a posteriori result into the attention model. In addition, since the target foresight words are available during the training, we jointly learn the prediction model for the target foresight words and the translation model in a supervised manner. This paper makes the following contributions: • It proposes a novel TFA-NMT for neural machine translation by using an auxiliary mechanism to predict the tar"
N18-1125,D15-1166,0,0.658725,"pirical experiments on chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a)"
N18-1125,C16-1205,0,0.0527507,"Missing"
N18-1125,D16-1249,0,0.0306058,"e propose approach, since we introduce auxiliary information from a target foresight word into the attention model. However, there is a significant difference between our approach and their approaches. Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs. The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning. In contrast, Mi et al. (2016); Liu et al. (2016b); Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning. They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention. 6 Conclusion It has been argued that the traditional attention model in neural machine translation suf1387 System Model Dev MT05 MT06 MT08 Ave. (Liu et al., 2016b) Moses NMT-J – – 35.4 36.8 33.7 36"
N18-1125,P03-1021,0,0.0158574,"ent of our model is not only from the POS tagging. In order to further validate the improvements of variant proposed models, we evaluate the foresight prediction accuracy (FPA) for three proposed prediction models. We found that the fine-grained Model3 achieves the best FPA, indicating a good estimated foresight is very important to obtain the gains in terms of BLEU. 4.2.2 Decode Exp Exp Map Analysis on Syntactic Categories In this experiment, we investigate which category of generated words benefit most from the proposed approach in terms of alignments measured by alignment error rate (AER) (Och, 2003). We carry out experiments on the evaluation dataset from (Liu and Sun, 2015), which contains 900 manually aligned ChineseEnglish sentence pairs. Following (Luong et al., 2015b), we force-decode both the bilingual sentences including source and reference sentences to obtain the attention matrices, and then we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. As shown in Table 2, the AER improvements are modest for content words such as Noun, Verb, and adjective (“Adj.”) words; but there are substantial improvements for func"
N18-1125,C04-1156,0,0.0908774,"Missing"
N18-1125,E17-3017,0,0.0271867,".91 Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a batch. The dimension of word embedding is 620. The dimensions of both feed forward NN and RNN hidden layer are"
N18-1125,W16-2209,0,0.0362855,"epresentations in decoding: Exp for expectation and Map for maximum a posteriori. Table 2: Performances on syntactic categories. “Base” denotes “Nematus”, and Ours denotes the proposed model. icantly better than baseline, but Model2 is significantly better with p&lt;0.05 and Model3 is significantly better with p&lt;0.01. Given that simply introducing an additional layer (“+2Layer”) does not produce any improvement on this data, we believe the gain of our model is not only from the more introduced parameters. Besides, we augment the word embedding by concatenating the POS tag embedding, proposed by (Sennrich and Haddow, 2016), the BLEU is 38.96, which indicating the improvement of our model is not only from the POS tagging. In order to further validate the improvements of variant proposed models, we evaluate the foresight prediction accuracy (FPA) for three proposed prediction models. We found that the fine-grained Model3 achieves the best FPA, indicating a good estimated foresight is very important to obtain the gains in terms of BLEU. 4.2.2 Decode Exp Exp Map Analysis on Syntactic Categories In this experiment, we investigate which category of generated words benefit most from the proposed approach in terms of a"
N18-1125,N03-1033,0,0.0381169,"d in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a batch. The dimension of word embedding is 620. The dimensions of both feed forward NN and RNN hidden layer are 1000. The beam size for decoding is 12, and the cost function is optimized by Adadelta with hyper-parameters suggested by Zeiler (2012). Particularly for TFA"
N18-1125,P16-1008,1,0.922677,"n chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtai"
N18-1125,D17-1013,0,0.0215987,"n. βi is derived from Figure 3, zi is from Eq.(10-11), and other nodes are similar to ones in Figure 2. obtained from Eq.(10-11) and the prediction model as shown in Figure 3. Note that the proposed TFA-NMT models the target foresight word, which is a future word regarding to the current time step, to conduct attention calculation. In this sense, it employs the idea of modeling future and thus resembles to the work in (Zheng et al., 2017). The main difference is that TFA-NMT models the future from the target side whereas Zheng et al. (2017) models the future from the source side. In addition, Weng et al. (2017) imposes a regularization term by using future words during training. Unlike our approach, their approach does not use future words during the inference because these words are unavailable. Anyway, it is possible to put both their approach and our approach together for further improvements. 3.3 Suppose {⟨ k k a kset ⟩ of training data } is denoted by x , y , u |k = 1, · · · , K . Here xk , yk and uk denotes a source sentence, a target sentence and a POS tag sequence of yk , respectively. Then one can jointly train both the translation model for yk and the prediction model for uk by minimizing"
N18-1125,1983.tc-1.13,0,0.573571,"Missing"
N18-1125,Q18-1011,1,0.889688,"Missing"
N19-1122,D18-1457,1,0.810585,"Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the position information respectively. Shen et al. (2018) introduce a directiona"
N19-1122,P18-1008,0,0.111193,"ang Baosong Yang Florida State University haoj8711@gmail.com Tencent AI Lab brightxwang@tencent.com University of Macau nlp2ct.baosong@gmail.com Longyue Wang Jinfeng Zhang Zhaopeng Tu∗ Tencent AI Lab vinnylywang@tencent.com Florida State University jinfeng@stat.fsu.edu Tencent AI Lab zptu@tencent.com Abstract Recently, the Transformer model (Vaswani et al., 2017) that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence hinders its further improvement of translation capacity (Chen et al., 2018; Dehghani et al., 2019). In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences"
N19-1122,P18-1198,0,0.269586,"and target sequences with shorter path. Among all the model variants, the implementation with shortest path performs best, in which the recurrence encoder is single layer and its output is only fed to the top decoder layer. It consistently outperforms its multiple deep counterparts, such as multiple-layer recurrence encoder and feeding the output of recurrence encoder to all the decoder layers. In addition, our approach indeed generates more informative encoder representations, especially representative on syntactic structure features, through conducting linguistic analyses on probing tasks (Conneau et al., 2018). 2 Background Figure 1 shows the model architecture of Transformer. The encoder is composed of a stack of N identical layers, each of which has two sub-layers. The first sub-layer is a self-attention network, and the second one is a position-wise fully connected feed-forward network. A residual connection (He et al., 2016) is employed around each of two sublayers, followed by layer normalization (Ba et al., Add & Norm Recurrence Encoder Positional Encoding Add & Norm Self-Attention Encoder N× Add & Norm Multi-Head QK> Attention sof tmax( √ )V dk Norm Recurrence (3)Modeling ☯ ⊕ where {Q, K, V}"
N19-1122,N03-1017,0,0.0277516,"r the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentence pairs, En⇒De) and WMT17 Chinese-to-English (20.6M sentence pairs, Zh⇒En) translation tasks. All the data had been tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations2 . We used case-sensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We implemented the proposed approaches on top of the Transformer model (Vaswani et al., 2017). Both in our model and related model of Subsection 5.3, the RNN is implemented with GRU (Cho et al., 2014) for fair comparison. We followed the configurations in Vaswani et al. (2017), and reproduced their reported results on the En⇒De task. We initialized parameters of the proposed models by the pre-trained baseline model. We have tested both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096), and number of attention heads"
N19-1122,D18-1317,1,0.864009,"nce encoder to all the decoder layers. 4 Related Work Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the position information r"
N19-1122,P02-1040,0,0.103507,"lement the Transformer model. In our preliminary experiments, attending over the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentence pairs, En⇒De) and WMT17 Chinese-to-English (20.6M sentence pairs, Zh⇒En) translation tasks. All the data had been tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations2 . We used case-sensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We implemented the proposed approaches on top of the Transformer model (Vaswani et al., 2017). Both in our model and related model of Subsection 5.3, the RNN is implemented with GRU (Cho et al., 2014) for fair comparison. We followed the configurations in Vaswani et al. (2017), and reproduced their reported results on the En⇒De task. We initialized parameters of the proposed models by the pre-trained baseline model. We have tested both Base and Big models, which differ at hidden size (51"
N19-1122,N18-1202,0,0.0618655,"mpirically show that the proposed model benefits from the short-cut effect. Comparison to Reviewer Network Attentive recurrent network are inspired by the reviewer network, which is proposed by Yang et al. (2016) for the image caption generation task. There are two key differences which reflect how we have generalized from the original model. First, we perform attention steps over the source embeddings instead of the encoder representations. The main reason is that the Transformer encoder is implemented as multiple layers, and higher layers generally encode global information, as indicated by Peters et al. (2018). Second, we feed the feature vectors together with the original encoder representations to the decoder. In image caption generation, the source side (i.e. image) contains much more information than the target side (i.e. caption) (Tu et al., 2017). Therefore, they aim at learning a compact and abstractive representation from the source information, which serves as the only input to the decoder. In this work, we focus on leveraging the attention model to better learn 1202 Model BASE the recurrence, which we expect to complement the Transformer model. In our preliminary experiments, attending ov"
N19-1122,P16-1162,0,0.128779,"the attention model to better learn 1202 Model BASE the recurrence, which we expect to complement the Transformer model. In our preliminary experiments, attending over the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentence pairs, En⇒De) and WMT17 Chinese-to-English (20.6M sentence pairs, Zh⇒En) translation tasks. All the data had been tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations2 . We used case-sensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We implemented the proposed approaches on top of the Transformer model (Vaswani et al., 2017). Both in our model and related model of Subsection 5.3, the RNN is implemented with GRU (Cho et al., 2014) for fair comparison. We followed the configurations in Vaswani et al. (2017), and reproduced their reported results on the En⇒De task. We initialized parameters of the proposed models by the pre-trai"
N19-1122,N18-2074,0,0.439798,"work was conducted when Jie Hao and Baosong Yang were interning at Tencent AI Lab. down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance. However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality (Dehghani et al., 2019). Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations (Tran et al., 2016) and positional encoding (Shaw et al., 2018), which are exactly the weaknesses of SAN (Tran et al., 2018). Recently, Chen et al. (2018) show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models. Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder. The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional information source to the Transformer decoder. In ad"
N19-1122,D18-1548,0,0.049565,"g the output of recurrence encoder to all the decoder layers. 4 Related Work Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the posi"
N19-1122,P18-1117,0,0.0204601,"es Hd to make a target word prediction. It is much simpler than that of the conventional Transformer, which transfers information learned from input sequences across multiple stacking encoder and decoder layers. We expect it outperforms its multiple deep counterparts, such as multiple-layer recurrence encoder and feeding the output of recurrence encoder to all the decoder layers. 4 Related Work Improving Transformer Encoder From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they"
N19-1122,N16-1036,0,0.016905,"the corresponding author of the paper. This work was conducted when Jie Hao and Baosong Yang were interning at Tencent AI Lab. down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance. However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality (Dehghani et al., 2019). Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations (Tran et al., 2016) and positional encoding (Shaw et al., 2018), which are exactly the weaknesses of SAN (Tran et al., 2018). Recently, Chen et al. (2018) show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models. Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder. The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional informat"
N19-1122,C18-1255,0,0.042848,"r From the perspective of representation learning, there has been an increasing amount of work on improving the representation power of SAN encoder. Bawden et al. (2018) and Voita et al. (2018) exploit external context for SAN encoder, while Yang et al. (2019) leverage the intermediate representations to contextualize the transformations in SAN. A number of recent efforts have explored ways to improve multi-head SAN by encouraging individual attention heads to extract distinct information (Strubell et al., 2018; Li et al., 2018). Concerning multi-layer SAN encoder, Dou et al. (2018, 2019) and Wang et al. (2018) propose to aggregate the multi-layer representations, and Dehghani et al. (2019) recurrently refine these representations. Our approach is complementary to theirs, since they focus on improving the representation power of SAN encoder, while we aim to complement SAN encoder with an additional recurrence encoder. Along the direction of modeling recurrence for SAN, Vaswani et al. (2017) and Shaw et al. (2018) inject absolute position encoding and relative positional encoding to consider the position information respectively. Shen et al. (2018) introduce a directional self-attention network (DiSA"
N19-1122,D18-1503,0,0.0424227,"ng at Tencent AI Lab. down the sequential assumption to obtain the ability of highly parallel computation: input elements interact with each other simultaneously without regard to their distance. However, prior studies empirically show that the lack of recurrence modeling hinders Transformer from further improvement of translation quality (Dehghani et al., 2019). Modeling recurrence is crucial for capturing several essential properties of input sequence, such as structural representations (Tran et al., 2016) and positional encoding (Shaw et al., 2018), which are exactly the weaknesses of SAN (Tran et al., 2018). Recently, Chen et al. (2018) show that the representations learned by SAN-based and RNNbased encoders are complementary to each other, and merging them can improve translation performance for RNN-based NMT models. Starting from these findings, we propose to directly model recurrence for Transformer with an additional recurrence encoder. The recurrence encoder recurrently reads word embeddings of input sequence and outputs a sequence of hidden states, which serves as an additional information source to the Transformer decoder. In addition to the standard RNN, we propose to implement recurrenc"
N19-1122,Q17-1007,1,0.85447,"There are two key differences which reflect how we have generalized from the original model. First, we perform attention steps over the source embeddings instead of the encoder representations. The main reason is that the Transformer encoder is implemented as multiple layers, and higher layers generally encode global information, as indicated by Peters et al. (2018). Second, we feed the feature vectors together with the original encoder representations to the decoder. In image caption generation, the source side (i.e. image) contains much more information than the target side (i.e. caption) (Tu et al., 2017). Therefore, they aim at learning a compact and abstractive representation from the source information, which serves as the only input to the decoder. In this work, we focus on leveraging the attention model to better learn 1202 Model BASE the recurrence, which we expect to complement the Transformer model. In our preliminary experiments, attending over the encoder representations does not improve performance, while feeding the feature vectors only to the decoder seriously harms performance. 5 Experiment 5.1 Setup We conducted experiments on the widelyused WMT14 English-to-German (4.6M sentenc"
N19-1124,E17-2029,0,0.0279951,"l techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re"
N19-1124,P18-1063,0,0.0252571,"latter took advantages of both sides. In a closed domain conversation setting, Pandey et al. (2018) further proposed to weight different training instances by context similarity. Our model differs from them in that we take an extra intermediate step for skeleton generation to filter the retrieval information before use, which shows the effectiveness in avoiding the erroneous copy in our experiments. Multi-step Language Generation Our work is also inspired by the recent success of decomposing an end-to-end language generation task into several sequential sub-tasks. For document summarization, Chen and Bansal (2018) first select salient sentences and then rewrite them in parallel. For sentiment-to-sentiment translation, Xu et al. (2018) first use a neutralization module to remove emotional words and then add sentiment to the neutralized content. Not only does their decomposition improve the overall performance, but also makes the whole generation process more interpretable. Our skeleton-to-response framework also sheds some light on the use of retrieval memories. 5 Experiments 5.1 Data We use the preprocessed data in (Wu et al., 2019) as our test bed. The total dataset consists of about 20 million single"
N19-1124,N16-1014,0,0.719468,"he IR-based models (Ji et al., 2014; Hu et al., 2014) directly copy an existing response from a training corpus when receiving a response request. Since the training corpus is usually collected from real-world conversations and possibly post-edited ∗ † Work done while DC was interning at Tencent AI Lab. Corresponding author. by a human, the retrieved responses are informative and grammatical. However, the performance of such systems drops when a given dialogue history is substantially different from those in the training corpus. The generative models (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), on the other hand, generate a new utterance from scratch. While those generative models have better generalization capacity in rare dialogue contexts, the generated responses tend to be universal and noninformative (e.g., “I don’t know”, “I think so” etc.) (Li et al., 2016a). It is partly due to the diversity of possible responses to a single query (i.e., the one-to-many problem). The dialogue query alone cannot decide a meaningful and specific response. Thus a well-trained model tends to generate the most frequent (safe but boring) responses instead. To summarize, IR-based models may give"
N19-1124,P18-1123,0,0.554103,"eters. The weight βw2 is obtained in a similar way with another set of parameters vD and WD . After acquiring the edit vector, we transform the prototype response r0 to a skeleton t by the follow( &lt; blank &gt; if m ˆ i = 0, φ(ri0 , hi , z) = , 0 ri else (3) where m ˆ i is the indicator and equals 0 if ri0 is replaced with a placeholder “&lt;blank&gt;” and 1 otherwise. The probability of m ˆ i = 1 is computed by P (m ˆ i = 1) = sigmoid(Wm [hi ⊕ z] + bm ). (4) 2.2 Response Generator The response generator can be implemented using most existing IR-augmented models (Song et al., 2016; Weston et al., 2018; Pandey et al., 2018), just by replacing the retrieved response input with the corresponding skeleton. We discuss our choices below. Encoders Two separate bidirectional LSTM (biLSTM) networks are used to obtain the distributed representations of the query memories and the skeleton memories, respectively. For biLSTM, 1221 the concatenation of the forward and the backward hidden states at each token position is considered a memory slot, producing two memory pools: Mq = {h1 , h2 , . . . , h|q |} for the input query, and Mt = {h01 , h02 , . . . , h0|t |} for the skeleton.1 Decoder During the generation process, our de"
N19-1124,P17-2079,0,0.0121808,"l., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re-rank the output from both models. However, the performance of such models is limited by the capacity of individual methods. Most related to our work, Song et al. (2016); Weston et al. (2018) and Wu et al. (2019) encoded the retrieved result into distributed representation and used it as the additional conditionals along with the standard query representation. While the former two only used the target side of the retrieved pairs, the latter took advantages of both sides. In a closed domain conversation setting, Pandey et al. (2018) further proposed to weight different training inst"
N19-1124,P15-1152,0,0.176413,"Missing"
N19-1124,N15-1020,0,0.0309983,"following objective is maximized: log D(r|q, rˆ, r, r) = log exp(hr T MD hq ) , P exp(hx T MD hq ) x∈{ˆ r ,r,r} (8) where hx is a vector representation of x, produced by a bidirectional LSTM (the last hidden state), and MD is a trainable matrix.2 4 Related Work Multi-source Dialogue Generation Chit-chat style dialogue system dates back to ELIZA (Weizenbaum, 1966). Early work uses handcrafted rules, while modern systems usually use data-driven approaches, e.g., information retrieval techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker"
N19-1124,P16-1094,0,0.0661705,"Missing"
N19-1124,D15-1166,0,0.129849,"cts a response skeleton. Lower: The response generator generates a response based on both the skeleton and the query. word embeddings to get the dense representations of I and D. The edit vector is computed as: X X z= αw1 Φ(w1 ) ⊕ βw2 Φ(w2 ), (1) w1 ∈I ing equations: 0 t = (φ(r10 , h1 , z), φ(r20 , h2 , z), · · · , φ(r|r 0 |, h|r 0 |, z)), w2 ∈D where ⊕ is the concatenation operation. Φ maps a word to its corresponding embedding vector, αw1 and βw2 are the weights of an insertion word w1 and a deletion word w2 respectively. The weights of different words are derived by an attention mechanism (Luong et al., 2015). Formally, the 0 ) is proretrieved response r0 = (r10 , r20 . . . , r|r 0| cessed by a bidirectional GRU network (biGRU). We denote the states of the biGRU (i.e. concatenation of forward and backward GRU states) as (h1 , h2 , . . . , h|r0 |). The weight αw1 is calculated by: exp(sw1 ) αw1 = P , w∈I exp(sw ) sw1 = vI&gt; tanh(WI [Φ(w1 ) ⊕ h|r0 |]), (2) where vI and WI are learnable parameters. The weight βw2 is obtained in a similar way with another set of parameters vD and WD . After acquiring the edit vector, we transform the prototype response r0 to a skeleton t by the follow( &lt; blank &gt; if m ˆ"
N19-1124,W18-5713,0,0.391089,"uery (i.e., the one-to-many problem). The dialogue query alone cannot decide a meaningful and specific response. Thus a well-trained model tends to generate the most frequent (safe but boring) responses instead. To summarize, IR-based models may give informative but inappropriate responses while generative models often do the opposite. It is desirable to combine both merits. Song et al. (2016) used an extra encoder for the retrieved response. The resulted dense representation, together with the original query, is used to feed the decoder in a standard S EQ 2S EQ model (Bahdanau et al., 2014). Weston et al. (2018) used a single encoder that takes the concatenation of the original query and the retrieved as input. Wu et al. (2019) noted that the retrieved information should be used in awareness of the context difference, and further proposed to construct an edit vector by explicitly encoding the lexical differences between the input query and the retrieved query. However, in our preliminary experiments, we found that the IR-guided models are inclined to degenerate into a copy mechanism, in which the generative models simply repeat the retrieved response without necessary modifications. Sharp performance"
N19-1124,P18-1101,0,0.0275646,"nformation retrieval techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et a"
N19-1124,P17-1061,0,0.0360971,"2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re-rank the output from both models. However, the performance of such"
N19-1359,P02-1040,0,\N,Missing
N19-1359,N18-1202,0,\N,Missing
N19-1359,D18-1548,0,\N,Missing
N19-1359,P18-1167,0,\N,Missing
N19-1359,P18-1198,0,\N,Missing
N19-1359,D18-1317,1,\N,Missing
N19-1359,N19-1407,1,\N,Missing
N19-1359,W18-5431,0,\N,Missing
N19-1359,P16-1162,0,\N,Missing
N19-1359,D16-1044,0,\N,Missing
N19-1407,D18-1457,1,0.873171,"ng efficiency. Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018). Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018, 2019), Li et al. (2019) proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads. 5 Experiments We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. 4042 For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectiv"
N19-1407,D16-1044,0,0.0672508,"N2 ] (8) V [V b h, V b h are elements in the h-th subspace, where K which are calculated by Equations S 4 and 5 respectively. The union operation means combining the keys and values in different subspaces. The corresponding output is calculated as: e h )V eh ohi = ATT(qhi , K (9) The 2D convolution allows SANs to build relevance between elements across adjacent heads, thus flexibly extract local features from different subspaces rather than merely from an unique head. The vanilla SAN models linearly aggregate features from different heads, and this procedure limits the extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the o"
N19-1407,W18-5431,0,0.423676,"aces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attention heads in an unified framework. Specifically, in order to pay more attention to a local part of the input sequence, we restrict the attention scope to a window of neighboring"
N19-1407,P16-1162,0,0.152427,"eriments We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. 4042 For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018), we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; Yang et al., 2018), we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017), and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. 5.1 Effects of Window/Area Size We first investigated the"
N19-1407,N18-2074,0,0.478472,"l allows each head to interact local features with its adjacent subspaces at attention time. We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong T RANSFORMER model (Vaswani et al., 2017) across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Yang et al., 2018; Sperber et al., 2018), our model boosts performance on both translation quality and training efficiency. 4040 Proceedings of NAACL-HLT 2019, pages 4040–4045 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held a talk with SharonBush (a) Vanilla SANs held Bush aheld talka with talkSharon with Sharon (b) 1D"
N19-1407,D14-1181,0,0.00321668,"e extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is fed to the subsequent SAN layer. Several researches proposed to revise the attention distribution with a parametric localness bias, and succeed on machine translation (Yang et al., 2018) and natural language inference (Guo et al., 2019). While both models introduce additional parameters, our approach is a more lightweight solution without introducing any new parameters. Closely related to this work, Shen et al. (2018a) applied a positional mask to encode temporal order, which only allows SANs to attend to the previous or following tokens in th"
N19-1407,D18-1548,0,0.0713839,"we improve locality modeling from revising attention scope. To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018). Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018, 2019), Li et al. (2019) proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads. 5 Experiments We conducted experiments with the Transformer model (Vaswani et al., 2017)"
N19-1407,W04-3250,0,0.0278748,"nce improvements over the Transformer baseline. Model T RANSFORMER -BASE + CS ANs T RANSFORMER -B IG + CS ANs WMT14 En⇒De Speed BLEU 1.28 27.31 1.22 28.18⇑ 0.61 28.58 0.50 28.74 WMT17 Zh⇒En Speed BLEU 1.21 24.13 1.16 24.80⇑ 0.58 24.56 0.48 25.01↑ WAT17 Ja⇒En Speed BLEU 1.33 28.10 1.28 28.50↑ 0.65 28.41 0.55 28.73↑ Table 2: Experimental results on WMT14 En⇒De, WMT17 Zh⇒En and WAT17 Ja⇒En test sets. “Speed” denotes the training speed (steps/second). “↑ / ⇑” indicates statistically significant difference from the vanilla self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). C NNs, revealing that extracting local features with dynamic weights is superior to assigning fixed parameters. Moreover, while most of the existing approaches (except for Shen et al. (2018a)) introduce new parameters, our methods are parameter-free and thus only marginally affect training efficiency. 5.4 Universality of The Proposed Model To validate the universality of our approach on MT tasks, we evaluated the proposed approach on different language pairs and model settings. Table 2 lists the results on En⇒De, Zh⇒En and Ja⇒En translation tasks. As seen, our model consistently improves tra"
N19-1407,D18-1317,1,0.920185,"ann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato and Tiedemann, 2018; Li et al., 2018), especially in diverse local contexts (Yang et al., 2018). We hypothesis that exploiting local properties across heads can further improve the performance of SANs. To this end, we expand the 1-dimensional window to a 2-dimensional area with the new dimension being the index of attention head. Suppose that the area size is (N + 1) × (M + 1) (N ≤ H), the keys and values in the area are: [ eh = b h− N2 , . . . , K b h, . . . , K b h+ N2 ] (7) K [K [ eh = b h− N2 , . . . , V b h, . . . , V b h+ N2 ] (8) V [V b h, V b h are elements in the h-th subspace, where K which are calculated by Equations S"
N19-1407,N19-1359,1,0.91234,"V b h are elements in the h-th subspace, where K which are calculated by Equations S 4 and 5 respectively. The union operation means combining the keys and values in different subspaces. The corresponding output is calculated as: e h )V eh ohi = ATT(qhi , K (9) The 2D convolution allows SANs to build relevance between elements across adjacent heads, thus flexibly extract local features from different subspaces rather than merely from an unique head. The vanilla SAN models linearly aggregate features from different heads, and this procedure limits the extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is"
N19-1407,D18-1408,0,0.159869,"tion, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017), which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attention heads in an u"
N19-1407,D15-1166,0,0.0950172,"ers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; Yang et al., 2018), we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017), and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. 5.1 Effects of Window/Area Size We first investigated the effects of window size (1D-CS ANs) and area size (2D-CS ANs) on En⇒De validation set, as plotted in Figure 2. For 1D-CS ANs, the local size with 11 is superior to other settings. This is consistent with Luong et al. (2015) who found that 10 is the best window size in their local attention experiments. Then, we fixed the number of neighboring tokens being 11 and varied the number of heads. As seen, by considering the features across heads (i.e. > 1), 2D-CS ANs further improve the translation quality. However, when the number of heads in attention goes up, the translation quality inversely drops. One possible reason is that the model still has the flexibility of learning a different distribution for each head with few interactions, while a large amount of interactions assumes more heads make “similar contribution"
N19-1407,D16-1244,0,0.171283,"Missing"
N19-1407,N18-1202,0,0.0838108,"the model to attend to a local region via convolution operations (1D-CS ANs, Figure 1(b)). Accordingly, it provides distance-aware 2 Accordingly, the calculation of corresponding output in Equation (2) is modified as: b h )V bh ohi = ATT(qhi , K (3) Approach 2 (6) As seen, SANs are only allowed to attend to the b h, V b h ), instead of all neighboring tokens (e.g., K the tokens in the sequence (e.g., Kh , Vh ). The SAN-based models are generally implemented as multiple layers, in which higher layers tend to learn semantic information while lower layers capture surface and lexical information (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato"
N19-1407,D18-1475,1,0.240925,"e elements. In addition, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017), which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attent"
N19-1407,D17-1150,1,0.868524,"n, SANs are only allowed to attend to the b h, V b h ), instead of all neighboring tokens (e.g., K the tokens in the sequence (e.g., Kh , Vh ). The SAN-based models are generally implemented as multiple layers, in which higher layers tend to learn semantic information while lower layers capture surface and lexical information (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato and Tiedemann, 2018; Li et al., 2018), especially in diverse local contexts (Yang et al., 2018). We hypothesis that exploiting local properties across heads can further improve the performance of SANs. To this end, we expand the 1-dimensional window to a 2-dimensional a"
P12-2007,2011.eamt-1.38,0,0.0724794,"of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides fl"
P12-2007,P07-1005,0,0.31714,"ianming 支持/VV 美国/NR 立场/NN zhichi meiguo lichang Eight European countries jointly support America’s stand Figure 1: An example word alignment for a ChineseEnglish sentence pair with the dependency parse tree for the Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin. ible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang’s HPB as well as a SAMT-style refined version of HPB. 2 Head-Driven HPB Translation Model Like Chiang (2005) and Chiang (2007), our HDHPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007), given a word sequence f ij from position i to position j, we first find heads and then concatenate the POS tags of these heads as f ij ’s non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: Definition 1. For word sequence f ij , word fk (i ≤"
P12-2007,A00-2018,0,0.186505,"ion is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2"
P12-2007,P05-1033,0,0.82208,"iak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a"
P12-2007,J07-2003,0,0.732834,"lins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way w"
P12-2007,J03-4003,0,0.0227783,"how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 200"
P12-2007,D11-1079,0,0.0366988,"Missing"
P12-2007,D10-1054,0,0.020809,"inals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics root 欧洲/NR Ouzhou 八国/NN 联名/AD baguo"
P12-2007,D10-1014,0,0.0889748,"Missing"
P12-2007,N03-1017,0,0.0465152,"respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule t"
P12-2007,W04-3250,0,0.126206,"nn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is ˜1.5 times that of Chiang’s, largely due to refining the non-terminal symbol X in Chiang’s model into head-informed ones in our model. It is also unsurprising, that the test set-filtered rule table size of our model is only ˜0.7 times that of Chiang’s: this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training d"
P12-2007,P08-1114,0,0.0400471,"which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics"
P12-2007,P11-1065,0,0.0933141,"Missing"
P12-2007,P00-1056,0,0.331027,"Missing"
P12-2007,J04-4002,0,0.151762,"t in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in 34 translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004) and Chiang’s HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang’s HPB model (Chiang, 2007), except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 d"
P12-2007,P03-1021,0,0.0579183,"and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is ˜1.5 times that of Chiang’s, largely due to refining the non-te"
P12-2007,N07-1051,0,0.0482787,"rt cell contains at most b derivations). For Moses HPB, we use “grow-diag-final-and” to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters. We train our model on a dataset with ˜1.5M sentence pairs from the LDC dataset.2 We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the 2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gig"
P12-2007,D09-1008,0,0.0548675,"minals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics root 欧洲/NR Ouzhou 八"
P12-2007,W06-3119,0,0.0463139,"lled Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important"
P12-2007,P11-1001,0,0.0533361,"syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads us"
P12-2066,W11-0705,0,0.149937,"which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and convolution kernels to exploit information on surface and syn"
P12-2066,H05-1091,0,0.0191743,") represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context contai"
P12-2066,W08-1301,0,0.0692146,"Missing"
P12-2066,W10-2910,0,0.0159615,"gly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and co"
P12-2066,P09-2079,0,0.0921626,"-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types"
P12-2066,P03-1054,0,0.00724357,"directly connected to the subjective word. For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1 , respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degre"
P12-2066,D09-1017,0,0.350748,"ion Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in docum"
P12-2066,P08-2029,0,0.0249684,"rom dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively."
P12-2066,J08-2003,0,0.102698,"e docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe"
P12-2066,P06-2079,0,0.451789,"Missing"
P12-2066,D09-1143,0,0.0168412,"frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et"
P12-2066,P04-1035,0,0.860926,"xicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus. 1 Introduction An important subtask in sentiment analysis is sentiment classification. Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraphlevel, sentence-level and phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment clas"
P12-2066,H05-1044,0,0.861528,"tics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009). Our research is inspired by these observations. Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words. More specifically, we use the terms in the lexicon constructed from (Wilson et al., 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is a"
P12-2066,W03-1017,0,0.174347,"categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Ha"
P12-2066,P06-1104,0,0.0670835,"istic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective parag"
P12-2066,N10-1121,0,\N,Missing
P13-2064,J93-2003,0,0.0364286,"proach outperforms both 1-best and n-best alignments. Figure 1: A bigraph constructed from an alignment (a), and its disjoint MCSs (b). of independent subhypergraphs, which is computationally feasible in practice (§ 3.2). Experimental results show that our approach significantly improves translation performance by up to 1.3 BLEU points over 1-best alignments (§ 4.3). 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al., 1993). Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (§ 2.2). The main challenge of this research is to effic"
P13-2064,J07-2003,0,0.301186,"w well it is. Formally, a weighted bipartite hypergraph H is a triple hS, T, Ei where S and T are two sets of vertices on the source and target sides, and E are hyperedges associated with weights. Currently, we estimate the weights of hyperedges from an nbest list by calculating relative frequencies: w(ei ) = P BG∈N P 3 Graph-based Rule Extraction In this section we describe how to extract translation rules from a hypergraph (§ 3.1) and how to estimate their probabilities (§ 3.2). 3.1 Extraction Algorithm We extract translation rules from a hypergraph for the hierarchical phrase-based system (Chiang, 2007). Chiang (2007) describes a rule extraction algorithm that involves two steps: (1) extract phrases from 1-best alignments; (2) obtain variable rules by replacing sub-phrase pairs with nonterminals. Our extraction algorithm differs at the first step, in which we extract phrases from hypergraphs instead of 1-best alignments. Rather than restricting ourselves by the alignment consistency in the traditional algorithm, we extract all possible candidate target phrases for each source phrase. To maintain a reasonable rule table size, we filter out less promising candidates that have a fractional coun"
P13-2064,J04-4002,0,0.289326,"alignments, it is unrealistic to enumerate all consistent alignments explicitly for each phrase pair. Recall that a hypergraph can be decomposed to a list of independent subhypergraphs, and an alignment is a combination of the sub-alignments from the decompositions. We observe that a phrase pair is absolutely consistent with the subalignments from some subhypergraphs, while possibly consistent with the others. As an example, p(A|H, P ) c(P |H) = = p(A|H) Q p(A|hi , P ) hi ∈OS p(A|hi ) h ∈OS Qi After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. 360 Rules from. . . 1-best 10-best Hypergraph Rules 257M 427M 426M MT03 33.45 34.10 34.71 MT04 35.25 35.71 36.24 MT05 33.63 34.04 34.41 Avg. 34.11 34.62 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 1 vertices hyperedges Setup 0.8 4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs f"
P13-2064,P03-1021,0,0.0636987,"6.24 MT05 33.63 34.04 34.41 Avg. 34.11 34.62 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 1 vertices hyperedges Setup 0.8 4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct"
P13-2064,P02-1040,0,0.0864087,"4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 0.6 0.4 0.2 0 1 2 3 4"
P13-2064,P05-1066,0,0.134044,"Missing"
P13-2064,D10-1053,0,0.0313696,"Missing"
P13-2064,P08-1115,0,0.0231616,"ergraphs outperform n-best lists, indicating: (1) our approach has a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presented a novel compact representation of word alignment, named weighted bipartite h"
P13-2064,N03-1017,0,0.00715537,"train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 number of vertices (hyperedges) Figure 4: The distribution of vertices (hyperedges) number of the subhypergraphs. peredges on average. This suggests that the divideand-conquer strategy makes the extraction computationally"
P13-2064,W02-1019,0,0.0329766,"opose a computationally tractable divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. Experimental results show that our approach outperforms both 1-best and n-best alignments. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Acknowledgement The authors are supported by 863 State Key Project No. 2011AA01A207, National Key Technology R&D Program No. 2012BAH39B03 and National Natural Science Foundation of China (Contracts 61202216). Qun Liu’s work is partially supported by Science"
P13-2064,D09-1106,1,0.910519,"Missing"
P13-2064,P10-1017,0,0.0188702,"ndependent subhypergraphs. Experimental results show that our approach outperforms both 1-best and n-best alignments. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Acknowledgement The authors are supported by 863 State Key Project No. 2011AA01A207, National Key Technology R&D Program No. 2012BAH39B03 and National Natural Science Foundation of China (Contracts 61202216). Qun Liu’s work is partially supported by Science Foundation Ireland (Grant No.07/CE/I1142) as part of the CNGL at Dublin City University. We thank Junhui"
P13-2064,C10-1123,1,0.883284,"es. We can see that both the “Shared” and “Non-shared” rules learned from hypergraphs outperform n-best lists, indicating: (1) our approach has a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presen"
P13-2064,I11-1145,1,0.901381,"nsistent alignments explicitly for each phrase pair. Recall that a hypergraph can be decomposed to a list of independent subhypergraphs, and an alignment is a combination of the sub-alignments from the decompositions. We observe that a phrase pair is absolutely consistent with the subalignments from some subhypergraphs, while possibly consistent with the others. As an example, p(A|H, P ) c(P |H) = = p(A|H) Q p(A|hi , P ) hi ∈OS p(A|hi ) h ∈OS Qi After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. 360 Rules from. . . 1-best 10-best Hypergraph Rules 257M 427M 426M MT03 33.45 34.10 34.71 MT04 35.25 35.71 36.24 MT05 33.63 34.04 34.41 Avg. 34.11 34.62 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 1 vertices hyperedges Setup 0.8 4.2 percentage We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language mo"
P13-2064,C12-2122,1,0.832873,"Missing"
P13-2064,2008.amta-papers.18,0,0.0162348,"sing a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 number of vertices (hyperedges) Figure 4: The distribution of vertices"
P13-2064,J10-3002,1,0.85755,"is an NP-complete problem, we propose a computationally tractable divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. Experimental results show that our approach outperforms both 1-best and n-best alignments. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Acknowledgement The authors are supported by 863 State Key Project No. 2011AA01A207, National Key Technology R&D Program No. 2012BAH39B03 and National Natural Science Foundation of China (Contracts 61202216). Qun Liu’s w"
P13-2064,D08-1022,0,0.02252,"for the two approaches. We can see that both the “Shared” and “Non-shared” rules learned from hypergraphs outperform n-best lists, indicating: (1) our approach has a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusi"
P13-2064,H05-1011,0,0.0325573,"d from an alignment (a), and its disjoint MCSs (b). of independent subhypergraphs, which is computationally feasible in practice (§ 3.2). Experimental results show that our approach significantly improves translation performance by up to 1.3 BLEU points over 1-best alignments (§ 4.3). 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al., 1993). Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (§ 2.2). The main challenge of this research is to efficiently calculate the fractional counts for rules extracted from hypergraphs."
P15-2088,N03-1017,0,0.028722,"pture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗ * Corresponding author 536 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th"
P15-2088,P07-2045,0,0.0101836,"00. We set the sliding window k = 3, and the learning rate η = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to Yang et al. (2013), and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. 6 Conclusion In this paper, we propose a context-dependent convolutional matching model to capture semantic similarities between phrase pairs that are sensitive to contexts. Experimental results show that our approach significantly improves the translation performance and obtains improvement of 1.0 BLEU scores on the overall test data. Integrating deep architecture into contextdependent translation selection is a promising way"
P15-2088,D08-1010,0,0.0187228,"4, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34.95α 35.01α 35.21α"
P15-2088,P08-1114,0,0.0247046,"tion test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34.95α 35.01α 35.21α 35.40αβ Table 1 summaries the"
P15-2088,P15-1003,1,0.855887,"ases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 1: procedure CURRICULUM-TRAINING(T , W ) 2: N1 ← easy negative(T ) 3: N2 ← medium negative(T ) 4: N3 ← difficult negative(T ) 5: T ← N1 6: CURRICULUM(T , n · t) . CUR. easy 7: T ← MIX([N1 , N2 ]) 8: CURRICULUM(T , n · t) . CUR. medium 9: for step ← 1 . . . n do 10: T ← MIX("
P15-2088,P05-1066,0,0.0850318,"Missing"
P15-2088,P03-1021,0,0.00843428,"terations reach the predefined number, we terminate this curriculum. 4 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of disc"
P15-2088,P02-1040,0,0.0960297,"Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the so"
P15-2088,P14-1129,0,0.0895301,"Missing"
P15-2088,D14-1015,0,0.0151334,"or evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34.95α 35.01α 35.21α 35.40αβ Table 1 summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the C"
P15-2088,P14-1066,0,0.386412,"ntencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗ * Corresponding author 536 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 536–541, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Matching))Score) matching(model( convolu6onal( sentence(model( representation representat"
P15-2088,P13-1017,0,0.0539848,"pendent counterpart, and “All” is the combined test sets. The superscripts α and β indicate statistically significant difference (p &lt; 0.05) from Baseline and CICM, respectively. For training the neural networks, we use 4 convolution layers for source sentences and 3 convolution layers for target phrases. For both of them, 4 pooling layers (pooling size is 2) are used, and all the feature maps are 100. We set the sliding window k = 3, and the learning rate η = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to Yang et al. (2013), and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. 6 Conclusion In this paper,"
P15-2088,C08-1041,0,0.0247345,"data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34"
P15-2088,P14-1011,0,0.391613,"ts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗ * Corresponding author 536 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 536–541, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Matching))Score) matching(model( convolu6onal( sentence(model( representation representation … … pooling Laye"
P15-2088,D13-1176,0,\N,Missing
P15-2088,P12-1079,0,\N,Missing
P15-2088,D13-1141,0,\N,Missing
P15-2088,P14-1013,0,\N,Missing
P16-1008,J93-2003,0,0.0568553,"Missing"
P16-1008,P15-1001,0,0.0941512,"Missing"
P16-1008,N03-1017,0,0.0513269,"Missing"
P16-1008,P07-2045,0,0.0228456,"Missing"
P16-1008,N06-1014,0,0.143259,"Missing"
P16-1008,D15-1166,0,0.112,"Missing"
P16-1008,J07-2003,0,0.0902386,"Missing"
P16-1008,J03-1002,0,0.011396,"Missing"
P16-1008,W14-4012,0,0.186136,"Missing"
P16-1008,D14-1179,0,0.0785675,"Missing"
P16-1008,P05-1066,0,0.0753936,"Missing"
P16-1008,P03-1021,0,0.0087897,"Missing"
P16-1008,P02-1040,0,0.100194,"Missing"
P16-1008,P16-1159,1,0.746732,"Missing"
P16-1008,koen-2004-pharaoh,0,\N,Missing
P16-1008,D08-1060,0,\N,Missing
P16-1008,P05-1033,0,\N,Missing
P16-1008,P09-1065,1,\N,Missing
P16-1008,N13-1073,0,\N,Missing
P16-1008,D11-1125,0,\N,Missing
P16-1008,D13-1052,0,\N,Missing
P17-1064,J07-2003,0,0.0490256,"Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6"
P17-1064,W14-4012,0,0.0168517,"Missing"
P17-1064,D14-1179,0,0.0247785,"Missing"
P17-1064,D16-1257,0,0.0120836,"any two different words. However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence. For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order. Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2 There is no doubt that the structural label sequence is much longer than its word sequence. In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi ’s part-of-speech (POS) tag in the label sequence and view the tag’s annotation vector as wi ’s label annotation vector. This is because wi ’s POS tag location can also represent wi ’s location in the parse tree. For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1 . L"
P17-1064,P10-4002,0,0.0137269,"ur method with two state-of-theart models of SMT and NMT: • Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on th"
P17-1064,P16-1078,0,0.160511,"Missing"
P17-1064,P16-1162,0,0.0130766,"Missing"
P17-1064,W15-3014,0,0.0140583,"n-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT exp"
P17-1064,P08-1066,0,0.0134567,"improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax. Figure 1 (a) shows a Chinese-to-English translation example of NMT. In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., 新 生/xinsheng 银 行/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence. Statistics on our development set show that one forth of Chinese noun phrases are translate"
P17-1064,W04-3250,0,0.104681,"Missing"
P17-1064,D16-1159,0,0.168192,"e syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translati"
P17-1064,N13-1060,1,0.934388,"Missing"
P17-1064,Q17-1007,1,0.0517642,"Missing"
P17-1064,P06-1077,0,0.0689507,"training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT"
P17-1064,P16-1008,1,0.860335,"we group sentences of similar lengths together and compute BLEU scores. Figure 6 presents the BLEU scores over different lengths of input sentences. It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths. It also shows that the performance drops substantially 693 System RNNSearch Mixed RNN AER 50.1 47.9 System RNNSearch Table 2: Evaluation of alignment quality. The lower the score, the better the alignment quality. when the length of input sentences increases. This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a). We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec). We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences. 5.2 Mixed RNN Cont. 57.3 59.8 47.3 54.0 58.1 63.3 63.1 54.5 56.2 60.4 Dis. 33."
P17-1064,2015.iwslt-evaluation.11,0,0.0637674,"ves is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,D15-1166,0,0.0233336,"vided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,P08-1114,0,0.0438962,"m to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find tha"
P17-1064,D16-1249,0,0.0283303,"Missing"
P17-1064,J03-1002,0,0.00476306,"Word Alignment Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment. To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs. We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2. Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word. 5.3 XP PP NP CP QP ALL PP NP CP QP ALL 5.4 Analysis on Over Translation To estimate the over translation generated by NMT, we propose ratio of over translation (ROT): Analysis on Phrase Alignment ROT = The above subsection examines the alignment performance at the word level. In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit. Given a source phrase XP, we use w"
P17-1064,P02-1040,0,0.13215,"Missing"
P17-1064,N07-1051,0,0.0913188,"Missing"
P17-1064,W16-2209,0,0.284718,"tion, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence. In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation. Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general. Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information. On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model. Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy. 2 h h1 h1 yi hm h hm si-1 h1 h1 x1 x2 ….. xm (a) encoder Atten ci MLP si RNN yi-1 (b) decoder Figure 2: Attention-based NMT model. mulated using a pair of neural networks, i.e.,"
P17-2092,P07-2045,0,0.006204,"on in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated withIn typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granular"
P17-2092,P05-1033,0,0.138079,"raged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguisti"
P17-2092,N03-1017,0,0.0576836,"ularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computat"
P17-2092,D14-1179,0,0.106361,"Missing"
P17-2092,P16-1160,0,0.309404,"ut the phrasal component may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, o"
P17-2092,P06-1077,0,0.211161,"Missing"
P17-2092,P16-2058,0,0.0225803,"Missing"
P17-2092,P16-1078,0,0.311835,"h global reordering of phrases and local translation inside phrases. Our model has following benefits: 1. The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible. 2. Our model recognizes phrase structures explicitly. Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words. 3. Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars. 4. Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart. Experiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously. 2 bush s"
P17-2092,D15-1166,0,0.0422838,"ine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2092 out attention. The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary"
P17-2092,P08-1114,0,0.19138,"Missing"
P17-2092,P17-1174,0,0.0518004,"f previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results. Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages. Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate. Table 4: Subjective evaluation results. System dl4mt This Work DE-14 16.53 17.40 DE-1213 16.78 17.45 Table 5: Results on German-English the translation is translated by. The human evaluator is asked to give 4 scores: adequacy score and fluency score, which are between 0 and 5, the larger, the better; under-translation score and overtranslation score, wh"
P17-2092,P02-1040,0,0.102378,"from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the"
P17-2092,D13-1176,0,0.0742459,"updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work"
P17-2092,W16-2209,0,0.184245,"ponent may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a ch"
P17-2092,Q17-1007,1,0.89772,"Missing"
P17-2092,P16-1162,0,0.0186724,". The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk"
P17-2092,P16-1008,1,0.910907,"Missing"
P17-2092,P08-1066,0,0.0644196,"Missing"
P17-2092,P16-1218,0,0.0232133,"to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the same as the previous step. In the U PDATE operation, ept−1 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016): Ts ! log P (yn |xn ) + log P (ln |xn ) + log P (bn |xn ) bt will be 0 or 1, where 1 denotes this is the boundary of a new chunk while 0 denotes not. Two different operations would be executed: # pt−1 , bt = 0 (C OPY ) pt = g(pt−1 , ept−1 , pct ), bt = 1 (U PDATE ) ept−1 = m(st−1 , eyt−1 ) − m(st′ , eyt′ ) N & ! (10) The chunk attention model differs from the standard word attention model (i.e., Equation 3) at: 1) it reads chunk state pt−1 rather than word state st−1 , and 2) it is only executed at boundary of each chunk rather than at each decoding step. In this way, our model only extracts"
P17-2092,D16-1159,0,0.0691766,"Missing"
P17-2092,1983.tc-1.13,0,0.186514,"Missing"
P17-2092,P16-2049,0,0.0402049,"Missing"
P17-2092,P15-1117,1,0.838643,"Chinese-English translation task. Our training data consists of 1.16M2 sentence pairs extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag seq"
P17-2092,Q17-1026,0,\N,Missing
P18-1163,D18-1549,0,0.0245783,"k that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models. 6 Conclusion We have proposed adversarial stability training to improve the robustness of NMT models. The basic idea is to train both the encoder and decoder robust to input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. We propose two approac"
P18-1163,P17-1176,1,0.843537,"ugmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models. 6 Conclusion We have proposed adversarial stability training to improve the robustness of NMT models. The basic idea is to train both the encoder and decoder robust to input perturbations by en"
P18-1163,D14-1181,0,0.00580016,"f model parameters using Adam SGD (Kingma and Ba, 2015). Its learning rate is initially set to 0.05 and varies according to the formula in Vaswani et al. (2017). Our adversarial stability training initializes the model based on the parameters trained by maximum likelihood estimation (MLE). We denote adversarial stability training based on lexical-level perturbations and feature-level perturbations respectively as ASTlexical and ASTfeature . We only sample one perturbed neighbour x0 ∈ N (x) for training efficiency. For the discriminator used in Linv , we adopt the CNN discriminator proposed by Kim (2014) to address the variable-length problem of the sequence generated by the encoder. In the CNN discriminator, the filter windows are set to 3, 4, 5 and rectified linear units are applied after convolution operations. We tune the hyperparameters on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam"
P18-1163,P16-1185,1,0.558429,"robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complic"
P18-1163,D17-1230,0,0.0323464,"bout 100K iterations, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it cannot distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x0 perform nearly consistently. 5 Related Work Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation. Adversarial Learning Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing (Li et al., 2017; Yang et al., 2018). Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capabi"
P18-1163,D15-1166,0,0.12942,"Missing"
P18-1163,P02-1040,0,0.102639,"of x and y, we also construct a mini-batch consisting of the perturbed neighbour x0 and y. We propagate the information to calculate these three loss functions according to arrows. Then, gradients are collected to update three sets of model parameters. Except for the gradients of Linv with respect to θenc are multiplying by −1, other gradients are normally backpropagated. Note that we update θinv and θenc simultaneously for training efficiency. 4 4.1 Experiments Setup We evaluated our adversarial stability training on translation tasks of several language pairs, and reported the 4-gram BLEU (Papineni et al., 2002) score as calculated by the multi-bleu.perl script. Chinese-English We used the LDC corpus consisting of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words respectively. We selected the best model using the NIST 2006 set as the validation set (hyper-parameter optimization and model selection). The NIST 2002, 2003, 2004, 2005, and 2008 datasets are used as test sets. English-German We used the WMT 14 corpus containing 4.5M sentence pairs with 118M English words and 111M German words. The validation set is newstest2013, and the test set is newstest2014. English-French We used"
P18-1163,P16-1009,0,0.199849,"on set is newstest2013, and the test set is newstest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the nonnormative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set. For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow Sennrich et al. (2016b) to split words into subword units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We report the case-sensitive tokenized BLEU score for English-German and English-French and the caseinsensitive tokenized BLEU score for ChineseEnglish. Our baseline system is an in-house NMT system. Following Bahdanau et al. (2015), we implement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers (He et al., 2016b). The gating mechanism o"
P18-1163,P16-1162,0,0.426482,"on set is newstest2013, and the test set is newstest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the nonnormative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set. For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow Sennrich et al. (2016b) to split words into subword units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We report the case-sensitive tokenized BLEU score for English-German and English-French and the caseinsensitive tokenized BLEU score for ChineseEnglish. Our baseline system is an in-house NMT system. Following Bahdanau et al. (2015), we implement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers (He et al., 2016b). The gating mechanism o"
P18-1163,P16-1159,1,0.736098,"on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam size for decoding is 10. 4.2 4.2.1 Translation Results NIST Chinese-English Translation Table 2 shows the results on Chinese-English translation. Our strong baseline system significantly outperforms previously reported results on 1760 System Shen et al. (2016) Wang et al. (2017) Zhang et al. (2018) this work Training MRT MLE MLE MLE ASTlexical ASTfeature MT06 37.34 37.29 38.38 41.38 43.57 44.44 MT02 40.36 – – 43.52 44.82 46.10 MT03 40.93 39.35 40.02 41.50 42.95 44.07 MT04 41.37 41.15 42.32 43.64 45.05 45.61 MT05 38.81 38.07 38.84 41.58 43.45 44.06 MT08 29.23 – – 31.60 34.85 34.94 Table 2: Case-insensitive BLEU scores on Chinese-English translation. System Shen et al. (2016) Luong et al. (2015) Kalchbrenner et al. (2017) Wang et al. (2017) Wu et al. (2016) Gehring et al. (2017) Vaswani et al. (2017) Architecture Gated RNN with 1 layer LSTM with 4 la"
P18-1163,P17-1013,0,0.0910566,"set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam size for decoding is 10. 4.2 4.2.1 Translation Results NIST Chinese-English Translation Table 2 shows the results on Chinese-English translation. Our strong baseline system significantly outperforms previously reported results on 1760 System Shen et al. (2016) Wang et al. (2017) Zhang et al. (2018) this work Training MRT MLE MLE MLE ASTlexical ASTfeature MT06 37.34 37.29 38.38 41.38 43.57 44.44 MT02 40.36 – – 43.52 44.82 46.10 MT03 40.93 39.35 40.02 41.50 42.95 44.07 MT04 41.37 41.15 42.32 43.64 45.05 45.61 MT05 38.81 38.07 38.84 41.58 43.45 44.06 MT08 29.23 – – 31.60 34.85 34.94 Table 2: Case-insensitive BLEU scores on Chinese-English translation. System Shen et al. (2016) Luong et al. (2015) Kalchbrenner et al. (2017) Wang et al. (2017) Wu et al. (2016) Gehring et al. (2017) Vaswani et al. (2017) Architecture Gated RNN with 1 layer LSTM with 4 layers ByteNet with 3"
P18-1163,N18-1122,0,0.0232541,"ons, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it cannot distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x0 perform nearly consistently. 5 Related Work Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation. Adversarial Learning Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing (Li et al., 2017; Yang et al., 2018). Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the"
P18-1163,D16-1160,0,0.0309436,"al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data an"
P19-1354,P18-1008,0,0.33076,"@gmail.com, {derekfw,lidiasc}@umac.mo ‡ Tencent AI Lab {vinnylywang,zptu}@tencent.com Abstract its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding (Gehring et al., 2017) is generally deployed to capture sequential information for SAN (Vaswani et al., 2017; Shaw et al., 2018). Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling (Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). However, such claims are mainly based on a theoretical argument, which have not been empirically validated. In addition, this can not explain well why SAN-based models outperform their RNN counterpart in machine translation – a benchmark sequence modeling task (Vaswani et al., 2017). Our goal in this work is to empirically assess the ability of SAN to learn word order. We focus on asking the following research questions: Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. m"
P19-1354,P18-1163,1,0.851012,"whether NMT model has the ability to tackle the wrong order noises. As a results, we make erroneous word order noises on English-German development set by moving one word to another position, and evaluate the drop of the translation quality of each model. As listed in Figure 6, SAN and DiSAN yield less drops on translation quality than their RNN counterpart, demonstrating the effectiveness of self-attention on ablating wrong order noises. We attribute this to the fact that models (e.g. RNN-based models) will not learn to be robust to errors since they are never observed (Sperber et al., 2017; Cheng et al., 2018). On the contrary, since SAN-based NMT encoder is good at recognizing and reserving anomalous word order information under NMT context, it may raise the ability of decoder on handling noises occurred in the training set, thus to be more robust in translating sentences with anomalous word order. Related Work Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019). In response to these impressive results, several studies have emerged with the goal of understanding SAN on many"
P19-1354,P18-1198,0,0.188484,"es across language pairs and model variants. This is consistent with prior observation on NMT systems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recov"
P19-1354,N19-1423,0,0.651089,"rder, and does the conclusion hold in different scenarios (e.g., translation)? Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation? Q3: Is position embedding powerful enough to capture word order information for SAN? Introduction Self-attention networks (SAN, Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Strubell et al., 2018), and language representations (Devlin et al., 2019). The popularity of SAN lies in ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. We approach these questions with a novel probing task – word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence. We compare SAN with RNN, as well as directional SAN (DiSAN, Shen et al., 2018a) that augments SAN with recurrence modeling. In this study, we focus on the encoders implemented with different architectures, so as to investigate their abilities to learn 3635 Proce"
P19-1354,P17-1080,0,0.0251829,"tence embedding. However, analysis on sentence encodings may introduce confounds, making it difficult to infer whether the relevant information is encoded within the specific position of interest or rather inferred from diffuse information elsewhere in the sentence (Tenney et al., 2019). In this study, we directly probe the token representations for word- and phrase-level properties, which has been widely used for probing token-level representations learned in neural machine translation systems, e.g. part-of-speech, semantic tags, morphology as well as constituent structure (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). 6 Conclusion In this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information. With the help of the proposed task, we evaluate RNN, 3642 SAN and DiSAN upon Transformer framework to empirically test the theoretical claims that SAN lacks the ability to learn word order. The results reveal that RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task. However, there is no evidence that SAN learns less word order inform"
P19-1354,P18-2003,0,0.0207368,"r, analysis on sentence encodings may introduce confounds, making it difficult to infer whether the relevant information is encoded within the specific position of interest or rather inferred from diffuse information elsewhere in the sentence (Tenney et al., 2019). In this study, we directly probe the token representations for word- and phrase-level properties, which has been widely used for probing token-level representations learned in neural machine translation systems, e.g. part-of-speech, semantic tags, morphology as well as constituent structure (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). 6 Conclusion In this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information. With the help of the proposed task, we evaluate RNN, 3642 SAN and DiSAN upon Transformer framework to empirically test the theoretical claims that SAN lacks the ability to learn word order. The results reveal that RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task. However, there is no evidence that SAN learns less word order information under the machine"
P19-1354,D17-1219,0,0.0270644,"models have to learn to recognize both the normal and abnormal word order in a sentence. Position Detector Figure 1 (a) depicts the architecture of the position detector. Let the sequential representations H = {h1 , ..., hN } be the output of each encoder noted in Section 3, which are fed to the output layer (Figure 1 (b)). Since only one pair of “I” and “O” labels should be generated in the output sequence, we cast the task as a pointer detection problem (Vinyals et al., 2015). To this end, we turn to an output layer that commonly used in the reading comprehension task (Wang and Jiang, 2017; Du and Cardie, 2017), which aims to identify the start and end positions of the answer in the given text.2 The output layer consists of two sub-layers, which progressively predicts the prob2 Contrary to reading comprehension in which the start and end positions are ordered, “I” and “O” do not have to be ordered in our tasks, that is, the popped word can be inserted to either left or right position. 3636 • Q1: We compare SAN with two recurrence architectures – RNN and DiSAN on the WRD task, thus to quantify their abilities on learning word order (Section 3.1). abilities of each position being labelled as “I” and “"
P19-1354,D16-1011,0,0.0320337,"rmation between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the learning objective indeed affects more on learning word order information than model architecture in our case. Accuracy According to Layer Several researchers may doubt that the parallel structure of SAN may lead to failure on capturing word order information at higher layers, si"
P19-1354,D18-1317,1,0.861132,"fact that models (e.g. RNN-based models) will not learn to be robust to errors since they are never observed (Sperber et al., 2017; Cheng et al., 2018). On the contrary, since SAN-based NMT encoder is good at recognizing and reserving anomalous word order information under NMT context, it may raise the ability of decoder on handling noises occurred in the training set, thus to be more robust in translating sentences with anomalous word order. Related Work Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019). In response to these impressive results, several studies have emerged with the goal of understanding SAN on many properties. For example, Tran et al. (2018) compared SAN and RNN on language inference tasks, and pointed out that SAN is weak at learning hierarchical structure than its RNN counterpart. Moreover, Tang et al. (2018) conducted experiments on subject-verb agreement and word sense disambiguation tasks. They found that SAN is good at extracting semantic properties, while underperforms RNN on capturing long-distance dependencies. This is in contrast to our intuit"
P19-1354,N19-1359,1,0.842245,"for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises. As a results, we make erroneous word order noi"
P19-1354,P15-1150,0,0.147118,"Missing"
P19-1354,D15-1166,0,0.07096,"ystems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results ve"
P19-1354,D18-1458,0,0.146942,"Missing"
P19-1354,P11-2093,0,0.0246417,"of WMT14 En⇒De data with maximum length to 80. For each sentence in different sets (i.e. training, validation, and test sets), we construct an instance by randomly moving a word to another position. Finally we construct 7M, 10K and 10K samples for training, validating and testing, respectively. Note that a sentence can be sampled multiple times, thus each dataset in the WRD data contains more instances than that in the machine translation data. All the English and German data are tokenized using the scripts in Moses. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To reduce the vocabulary size, all the sentences are processed by byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the data. 4 Experimental Results We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that 3638 Models RNN SAN DiSAN Insert 78.4 73.2 79.6"
P19-1354,D16-1244,0,0.0955198,"Missing"
P19-1354,N18-1202,0,0.0553809,"reserving more word order information, thus to help for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises"
P19-1354,W18-5431,0,0.0453698,"rder information, thus to help for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises. As a results, we make errone"
P19-1354,P16-1162,0,0.0652226,"test sets), we construct an instance by randomly moving a word to another position. Finally we construct 7M, 10K and 10K samples for training, validating and testing, respectively. Note that a sentence can be sampled multiple times, thus each dataset in the WRD data contains more instances than that in the machine translation data. All the English and German data are tokenized using the scripts in Moses. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To reduce the vocabulary size, all the sentences are processed by byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the data. 4 Experimental Results We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that 3638 Models RNN SAN DiSAN Insert 78.4 73.2 79.6 Original 73.4 66.0 70.1 Both 68.2 60.1 68.0 ability of SAN to learn word order. The consistency between prior studi"
P19-1354,N18-2074,0,0.225183,"he Ability of Self-Attention Networks to Learn Word Order Baosong Yang† Longyue Wang‡ Derek F. Wong† Lidia S. Chao† Zhaopeng Tu‡∗ † NLP2 CT Lab, Department of Computer and Information Science, University of Macau nlp2ct.baosong@gmail.com, {derekfw,lidiasc}@umac.mo ‡ Tencent AI Lab {vinnylywang,zptu}@tencent.com Abstract its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding (Gehring et al., 2017) is generally deployed to capture sequential information for SAN (Vaswani et al., 2017; Shaw et al., 2018). Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling (Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). However, such claims are mainly based on a theoretical argument, which have not been empirically validated. In addition, this can not explain well why SAN-based models outperform their RNN counterpart in machine translation – a benchmark sequence modeling task (Vaswani et al., 2017). Our goal in this work is to empirically assess the ability of SAN to"
P19-1354,D18-1548,0,0.0539845,": Is recurrence structure obligate for learning word order, and does the conclusion hold in different scenarios (e.g., translation)? Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation? Q3: Is position embedding powerful enough to capture word order information for SAN? Introduction Self-attention networks (SAN, Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Strubell et al., 2018), and language representations (Devlin et al., 2019). The popularity of SAN lies in ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. We approach these questions with a novel probing task – word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence. We compare SAN with RNN, as well as directional SAN (DiSAN, Shen et al., 2018a) that augments SAN with recurrence modeling. In this study, we focus on the encoders implemented with different architectures, so a"
P19-1354,D18-1503,0,0.316521,"ordering detection objective. WRD Encoders We first directly train the encoders on the WRD data, to evaluate the abilities of model architectures. The WRD encoders are randomly initialized and co-trained with the output layer. Accordingly, the detection accuracy can be treated as the learning objective of this group of encoders. Meanwhile, we can investigate the reliability of the proposed WRD task by checking whether the performances of different architectures (i.e. RNN, SAN, and DiSAN) are consistent with previous findings on other benchmark NLP tasks (Shen et al., 2018a; Tang et al., 2018; Tran et al., 2018; Devlin et al., 2019). NMT Encoders To quantify how well different architectures learn word order information with the learning objective of machine translation, we first train the NMT models (both encoder and decoder) on bilingual corpus using the same configuration reported by Vaswani et al. (2017). Then, we fix the parameters of the encoder, and only train the parameter associated with the output layer on the WRD data. In this way, we can probe the representations learned by NMT models, on their abilities to learn word order of input sentences. To cope with WRD task, all the models were tr"
P19-1354,D17-1065,0,0.0520865,"Missing"
P19-1354,D18-1475,1,0.836265,"and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the lea"
P19-1354,N19-1407,1,0.841914,"Missing"
P19-1354,D17-1150,1,0.862188,"ducted on the test set. Clearly, the accuracy of SAN gradually increased with the stacking of layers and consistently outperform that of other models across layers. ever, this kind of stability is destroyed when we pre-train each encoder with a learning objective of machine translation. As seen in Figure 4 (b) and (c), the performance of pre-trained NMT encoders obviously became worse on long-distance cases across language pairs and model variants. This is consistent with prior observation on NMT systems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivia"
P19-1354,P17-1172,0,0.0274143,"urce and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the learning objective indeed affects more on learning word order information than model architecture in our case. Accuracy According to Layer Several researchers may doubt that the parallel structure of SAN may lead to failure on capturing word order information at higher layers, since the position"
P19-1354,D10-1092,0,\N,Missing
P19-1354,D16-1159,0,\N,Missing
P19-1354,W17-5706,0,\N,Missing
P19-1624,D18-1338,0,0.0718853,"+ D EEP (R NN) + D EEP (TAM) T RANSFORMER -B IG + D EEP (R NN) + D EEP (TAM) En⇒De 27.31 28.38⇑ 28.33⇑ 28.58 29.04↑ 29.19⇑ En⇒Fr 39.32 40.15⇑ 40.27⇑ 41.41 41.87 42.04⇑ Table 2: Case-sensitive BLEU scores on WMT14 En⇒De and En⇒Fr test sets. “↑ / ⇑”: significant over T RANSFORMER counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling. vs. 264.1M, not shown in the table). Furthermore, D EEP (TAM) consistently outperforms D EEP (RNN) in the T RANSFORMER -B IG configuration. One possible reason is that the big models benefit more from the improved gradient flow with the transparent attention (Bapna et al., 2018). 3.3 Linguistic Analysis To gain linguistic insights into the global and deep sentence representation, we conducted probing tasks1 (Conneau et al., 2018) to evaluate linguistics knowledge embedded in the encoder output and the sentence representation in the variations of the Base model that are trained on En⇒De translation task. The probing tasks are classification problems that focus on simple linguistic properties of sentences. The 10 probing tasks are categories into three groups: (1) Surface information. (2) Syntactic information. (3) Semantic information. For each task, we trained the cl"
P19-1624,P18-1198,0,0.194725,"of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguistic information. The contributions of this paper are: • Our study demonstrates the necessity and effectiveness of exploiting source-side sentential context for NMT, which benefits from fusing useful contextual information across encoder layers. • We propose several strategies to better capture useful sentential context for neural machine translation. Experimental results empirically show that the proposed approaches achieve improvement over the strong baseline model T RANSFORMER. 6197 Proceedings of"
P19-1624,D18-1457,1,0.59051,"ial context representation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1 , . . . , HL }: g = g(H1 , . . . , HL ) = D EEP(g1 , . . . , gL ), (8) where gl is the sentence representation of the l-th layer Hl , which is calculated by Equation 3. The motivation for this mechanism is that recent studies reveal that different encoder layers capture linguistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of D EEP(·) In this work, we investigate two representative functions to aggregate information across layers, which differ at whether the decoding information is taken into account. RNN Intuitively, we can treat G = {g1 , . . . , gL } as a sequence of representations, and recurring all the representations with an RNN: L X βi,l gl , (10) l=1 βi = ATTg (dli−1 , G), (11) where ATTg (·) is an attention model with its own parameters, that specifics which context representations is"
P19-1624,W07-0717,0,0.0324893,"ng the source sentence representation. The deep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the enco"
P19-1624,C18-1276,0,0.0898512,"SFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target languages (Nida, 1969), is crucial for machine translation. In statistical machine translation (SMT), the sentential context has proven beneficial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bahdanau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the translation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guidance of decoding. In this work, we propose simple yet effective approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the sourceside context, which are summarized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the w"
P19-1624,P15-1003,0,0.0729807,"context representation. Experimental results on the WMT14 English⇒German and English⇒French benchmarks show that our model consistently improves performance over the strong T RANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target languages (Nida, 1969), is crucial for machine translation. In statistical machine translation (SMT), the sentential context has proven beneficial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bahdanau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the translation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guidance of decoding. In this work, we propose simple yet effective approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the source"
P19-1624,N18-1202,0,0.408191,"ized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer ling"
P19-1624,W18-5431,0,0.211353,"l context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguistic information. The contrib"
P19-1624,P16-1162,0,0.0894149,"ee appealing strengths. First, TAM dynamically generates the weights βi based on the decoding information at every decoding step dli−1 , while R NN is unaware of the decoder states and the associated parameters are fixed after training. Second, TAM allows the model to adjust the gradient flow to different layers in the encoder depending on its training phase. 3 Experiment We conducted experiments on WMT14 En⇒De and En⇒Fr benchmarks, which contain 4.5M and 35.5M sentence pairs respectively. We reported experimental results with case-sensitive 4gram BLEU score. We used byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations to alleviate the out-of-vocabulary problem. We implemented the proposed approaches on top of T RANSFORMER model (Vaswani et al., 2017). We followed Vaswani et al. (2017) to set the model configurations, and reproduced their reported results. We tested both Base and Big models, which differ at the layer size (512 vs. 1024) and the number of attention heads (8 vs. 16). 3.1 Ablation Study (9) We first investigated the effect of components in the proposed approaches, as listed in Table 1. We use the last RNN state as the sentence representation: g = rL . As seen, the RNN"
P19-1624,N18-1117,0,0.0609503,"Missing"
P19-1624,D16-1159,0,0.0282864,", which are summarized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed"
P19-1624,Q18-1029,1,0.852228,"n which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specif"
P19-1624,2014.amta-researchers.11,0,0.0241312,"representation. The deep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations f"
P19-1624,P15-1162,0,0.0746289,"Missing"
P19-1624,D17-1301,1,0.612456,"eep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent d"
P19-1624,P14-1062,0,0.02126,"the choice of g(·), namely shallow sentential context (Figure 1b) and deep sentential context (Figure 1c), which differ at the encoder layers to be exploited. It should be pointed out that the new parameters introduced in the proposed approach are jointly updated with NMT model parameters in an endto-end manner. 2.2 Shallow Sentential Context Shallow sentential context is a function of the top encoder layer output HL : g = g(HL ) = G LOBAL(HL ), (3) where G LOBAL(·) is the composition function. Choices of G LOBAL(·) Two intuitive choices are mean pooling (Iyyer et al., 2015) and max pooling (Kalchbrenner et al., 2014): G LOBAL MEAN = M EAN(HL ), L G LOBAL MAX = M AX(H ). (4) (5) Recently, Lin et al. (2017) proposed a selfattention mechanism to form sentence representation, which is appealing for its flexibility on extracting implicit global features. Inspired by this, 6198 g3g3 r3 r3 g2g2 r2 r2 g1g1 r1 r1 di-1di-1 g3g3 βi,3βi,3 g2g2 βi,2βi,2 g1g1 r0 r0 (a) R NN gi gi βi,1βi,1 (b) TAM Figure 2: Illustration of the deep functions. “TAM” model dynamically aggregates sentence representations at each decoding step with state di−1 . aggregation repeatedly revises the sentence representations of the sequence with"
P19-1624,C18-1255,0,0.0348102,"entation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1 , . . . , HL }: g = g(H1 , . . . , HL ) = D EEP(g1 , . . . , gL ), (8) where gl is the sentence representation of the l-th layer Hl , which is calculated by Equation 3. The motivation for this mechanism is that recent studies reveal that different encoder layers capture linguistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of D EEP(·) In this work, we investigate two representative functions to aggregate information across layers, which differ at whether the decoding information is taken into account. RNN Intuitively, we can treat G = {g1 , . . . , gL } as a sequence of representations, and recurring all the representations with an RNN: L X βi,l gl , (10) l=1 βi = ATTg (dli−1 , G), (11) where ATTg (·) is an attention model with its own parameters, that specifics which context representations is relevant for each d"
P19-1624,W04-3250,0,0.0589519,"M +18.9M +18.9M +19.9M +26.8M +26.4M n/a R NN TAM Train 1.39 1.08 1.35 1.34 1.22 1.03 1.07 Decode 3.85 3.09 3.45 3.43 3.23 3.14 3.03 BLEU 27.31 27.81 27.58 27.81↑ 28.04⇑ 28.38⇑ 28.33⇑ Table 1: Impact of components on WMT14 En⇒De translation task. BLEU scores in the table are case sensitive. “Train” denotes the training speed (steps/second), and “Decode” denotes the decoding speed (sentences/second) on a Tesla P40. “TAM” denotes the transparent attention model to implement the function D EEP(·). “↑ / ⇑”: significant over T RANSFORMER counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). Model baseline Base model, validating the importance of sentential context in NMT. Among them, attentive mechanism (Row 5) obtains the best performance in terms of BLEU score, while maintains the training and decoding speeds. Therefore, we used the attentive mechanism to implement the function G LOBAL(·) as the default setting in the following experiments. Deep Sentential Context (Rows 6-7) As seen, both R NN and TAM consistently outperform their shallow counterparts, proving the effectiveness of deep sentential context. Introducing deep context significantly improves translation performance"
P19-1624,N19-1359,1,0.869305,"Missing"
P19-1624,P17-2089,0,0.0424251,"Missing"
P19-1624,P12-1079,0,0.0231744,"m all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generati"
P19-1624,D18-1475,1,0.81888,"a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively. Experimental results on W"
P19-1624,C16-1170,0,0.0197359,"ation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al."
P19-1624,Q18-1011,1,0.817489,"t al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively. Experimental results on WMT14 benchmarks show that exploiting sentential context improves performances over the state-of-theart T RANSFORMER model. Linguistic analyses reveal that the proposed approach indeed captures more linguistic information as exp"
Q17-1007,J93-2003,0,0.0952604,"ments in Section 5.2 show that our gating mechanism significantly outperforms linear interpolation when combining contexts. Comparison to Handling Null-Generated Words in SMT: In machine translation, there are certain syntactic elements of the target language that are missing in the source (i.e., null-generated words). In fact this was the preliminary motivation for our approach: current attention models lack a mechanism to control the generation of words that do not have a strong correspondence on the source side. The model structure of NMT is quite similar to the traditional word-based SMT (Brown et al., 1993). Therefore, techniques that have proven effective in SMT may also be applicable to NMT. Toutanova et al. (2002) extend the calculation of translation probabilities to include null-generated target words in word-based SMT. These words are generated based on both the special source token null and the neighbouring word in the target language by a mixture model. We have simplified and generalized their approach: we use context gates to dynamically control the contribution of source context. When producing null-generated words, the context gate can as93 sign lower weights to the source context, by"
Q17-1007,P05-1066,0,0.0240416,"Missing"
Q17-1007,P15-1001,0,0.0454042,"SMT and NMT6 models: • Moses (Koehn et al., 2007): an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data; • GroundHog (Bahdanau et al., 2015): an open source attention-based NMT model with default setting. We have two variants that differ in the activation function used in the decoder 5 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 6 There is some recent progress on aggregating multiple models or enlarging the vocabulary(e.g.,, in (Jean et al., 2015)), but here we focus on the generic models. # 1 2 3 4 5 6 7 8 9 System Moses GroundHog (vanilla) 2 + Context Gate (both) GroundHog (GRU ) 4 + Context Gate (source) 4 + Context Gate (target) 4 + Context Gate (both) GroundHog-Coverage (GRU ) 8 + Context Gate (both) #Parameters – 77.1M 80.7M 84.3M 87.9M 87.9M 87.9M 84.4M 88.0M MT05 31.37 26.07 30.86∗ 30.61 31.96∗ 32.38∗ 33.52∗ 32.73 34.13∗ MT06 30.85 27.34 30.85∗ 31.12 32.29∗ 32.11∗ 33.46∗ 32.47 34.83∗ MT08 23.01 20.38 24.71∗ 23.23 24.97∗ 23.78 24.85∗ 25.23 26.22∗ Ave. 28.41 24.60 28.81 28.32 29.74 29.42 30.61 30.14 31.73 Table 2: Evaluation of t"
Q17-1007,D13-1176,0,0.0685886,"ear , the export of new high level technology product was UNK - billion us dollars china ’s guangdong hi - tech exports hit 58 billion dollars china ’s export of high and new hi - tech exports of the export of the export of the export of the export of the export of the export of the export of the export of · · · Table 1: Source and target contexts are highly correlated to translation adequacy and fluency, respectively. 5src and 5tgt denote halving the contributions from the source and target contexts when generating the translation, respectively. Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years. Its goal is to construct and utilize a single large neural network to accomplish the entire translation task. One great advantage of NMT is that the translation system can be completely constructed by learning from data without human involvement (cf., feature engineering in statistical machine translation (SMT)). The encoderdecoder architecture is widely employed (Cho et al., 2014; Sutskever et al., 2014), in which the encoder summarizes the source sentence into a vector representation, an"
Q17-1007,P07-2045,0,0.0649955,"Missing"
Q17-1007,D15-1166,0,0.200096,"Missing"
Q17-1007,J03-1002,0,0.0544157,"Missing"
Q17-1007,P02-1040,0,0.118366,"Missing"
Q17-1007,W09-0441,0,0.0117211,"t words in the target sentence are more related to the translation adequacy, and thus should depend more on the source context. In contrast, function words in the target sentence are often more related to the translation fluency (e.g., “of” after “is fond”), and thus should depend more on the target context. In this work, we propose to use context gates to control the contributions of source and target contexts on the generation of target words (decoding) 1 Fluency measures whether the translation is fluent, while adequacy measures whether the translation is faithful to the original sentence (Snover et al., 2009). 88 Figure 1: Architecture of decoder RNN. in NMT. Context gates are non-linear gating units which can dynamically select the amount of context information in the decoding process. Specifically, at each decoding step, the context gate examines both the source and target contexts, and outputs a ratio between zero and one to determine the percentages of information to utilize from the two contexts. In this way, the system can balance the adequacy and fluency of the translation with regard to the generation of a word at each position. Experimental results show that introducing context gates lead"
Q17-1007,W02-1012,0,0.167148,"Missing"
Q17-1007,P16-1008,1,0.93311,"ted.4 The decoding state implicitly models the notion of “coverage” by recurrently reading the time-dependent source context si . Lowering its contribution weakens the “coverage” effect and encourages the decoder to regenerate phrases multiple times to achieve the desired translation length. 2. The translation is incomplete. As shown in Table 1, NMT can get stuck in an infinite loop repeatedly generating a phrase due to the overwhelming influence of the source context. As a result, generation terminates early because 4 The recently proposed coverage based technique can alleviate this problem (Tu et al., 2016). In this work, we consider another approach, which is complementary to the coverage mechanism. 90 Figure 3: Architecture of context gate. the translation reaches the maximum length allowed by the implementation, even though the decoding procedure is not finished. The quantitative (Figure 2) and qualitative (Table 1) results confirm our hypothesis, i.e., source and target contexts are highly correlated to translation adequacy and fluency. We believe that a mechanism that can dynamically select information from source context and target context would be useful for NMT models, and this is exactl"
Q17-1007,P16-1125,0,0.0156616,"context gate controls the effect of the source context based on its relative importance. Experiments in Section 5.2 show that combining the two methods can further improve translation performance. There is another difference as well: the coverage mechanism is only applicable to attention-based NMT models, while the context gate is applicable to all NMT models. Comparison to Exploiting Auxiliary Contexts in Language Modeling: A thread of work in language modeling (LM) attempts to exploit auxiliary sentence-level or document-level context in an RNN LM (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2016). Independent of our work, Wang and Cho (2016) propose “early fusion” models of RNNs where additional information from an intersentence context is “fused” with the input to the RNN. Closely related to Wang and Cho (2016), our approach aims to dynamically control the contributions of required source and target contexts for machine translation, while theirs focuses on integrating auxiliary corpus-level contexts for language modelling to better approximate the corpus-level probability. In addition, we employ a gating mechanism to produce a dynamic weight at different decoding steps to combine sou"
Q18-1011,D17-1151,0,0.0132202,"aseline. Comparison with Other Work. (Rows 9-11). We also conduct experiments with multi-layer decoders (Wu et al., 2016) to see whether the NMT system can automatically model the translated and untranslated contents with additional decoder lay152 ers (Rows 9-10). However, we find that the performance is not improved using a two-layer decoder (Row 9), until a deeper version (three-layer decoder, Row 10) is used. This indicates that enhancing performance by simply adding more RNN layers into the decoder without any explicit instruction is nontrivial, which is consistent with the observation of Britz et al. (2017). Our model also outperforms the word-level C OVERAGE (Tu et al., 2016), which considers the coverage information of the source words independently. Our proposed model can be regarded as a high-level coverage model, which captures higher level coverage information, and gives more specific signals for the decision of attention and target prediction. Our model is more deeply involved in generating target words, by being fed not only to the attention model as in Tu et al. (2016), but also to the decoder state. 5.1.2 Subjective Evaluation Following Tu et al. (2016), we conduct subjective evaluatio"
Q18-1011,W17-3203,0,0.0258306,",000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate"
Q18-1011,W17-4725,0,0.0491104,"Missing"
Q18-1011,D13-1176,0,0.0776419,"TURE contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.† 1 Introduction Neural machine translation (NMT) generally adopts an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the given source. During translation, the decoder implicitly serves several functionalities at the same time: * Equal contributions. Our code can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT)."
Q18-1011,D15-1166,0,0.161208,"de can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT). 3. Maintaining what parts in the source have been translated (PAST) and what parts have not (F UTURE). However, it may be difficult for a single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously. A recent successful extension of NMT models is the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation"
Q18-1011,C16-1205,0,0.043448,"del both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients for the success in this work. In addition, we use two separate layers to explicitly model translated and untranslated contents, which is another distinguishing feature of the proposed approach. Future Modeling. Standard neural sequence decoders"
Q18-1011,D16-1096,0,0.0876841,"ly helps the prediction at the beginning of the sentence. We attribute the vanishing of such signals to the overloaded use of decoder states (e.g., L M, PAST, and F UTURE functionalities), and hence we propose to explicitly model the holistic source summarization by PAST and F UTURE contents at each decoding step. 3 Related Work Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. Coverage Modeling. Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not. These vectors are updated by accumulating attention probabilities at each decoding step, which provides an opportunity for the attention model to distinguish translated source words from untranslated ones. Viewing coverage vectors as a (soft) indicator of translated source contents, following this idea, we take one step further. We model translated and untranslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) ins"
Q18-1011,D16-1147,0,0.0206683,"o predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rockt¨aschel et al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rockt¨aschel et al. (2017) propose a keyvalue-predict attention model, which outputs three vectors at each step: the first is used to predict the next-word distribution; the second serves as the key for decoding; and the third is used for the attention mechanism. In this work, we further separate PAST and F UTURE functionalities from the decoder’s hidden representations. 4 Modeling PAST and F UTURE fo"
Q18-1011,J03-1002,0,0.0224634,") . | {z } PAST loss Dataset. We conduct experiments on ChineseEnglish (Zh-En), German-English (De-En), and English-German (En-De) translation tasks. For Zh-En, the training set consists of 1.6m sentence pairs, which are extracted from the LDC corpora3 . The NIST 2003 (MT03) dataset is our development set; the NIST 2002 (MT02), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards port"
Q18-1011,P02-1040,0,0.100819,"rformance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of join"
Q18-1011,E17-2025,0,0.0188295,"14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of joint BPE operations is 90,000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does"
Q18-1011,W17-4738,0,0.0132824,"uishes the PAST and F UTURE directly, which is a higher level coverage mechanism than the word coverage model. 153 ∆ – -1.00 -3.83 Table 4: Evaluation of the alignment quality. The lower the score, the better the alignment quality. 5.2 Results on German-English We also evaluate our model on the WMT17 benchmarks for both De-En and En-De. As shown in Table 5, our baseline gives comparable BLEU scores to the state-of-the-art NMT systems of WMT17. Our proposed model improves the strong baseline on both De-En and En-De. This shows that our proposed model works well across different language pairs. Rikters et al. (2017) and Sennrich et al. (2017a) obtain higher BLEU scores than our model, because they use additional large scale synthetic data (about 10M) for training. It maybe unfair to compare our model to theirs directly. 5.3 Alignment Quality AER 39.73 38.73 35.90 Analysis We conduct analyses on Zh-En, to better understand our model from different perspectives. Parameters and Speeds. As shown in Table 6, the baseline model (BASE) has 80M parameters. A single F UTURE or PAST layer introduces 15M to 17M parameters, and the corresponding objective introduces 18M parameters. In this work, the most complex mod"
Q18-1011,P16-1162,0,0.121344,"2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For"
Q18-1011,E17-3017,0,0.0569003,"Missing"
Q18-1011,P16-1159,0,0.0842396,"4 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate the proposed model on the Chinese-English translation and alignment tasks. 5.1.1 Translation Quality Table 2 shows the translation performances on Chinese-English. Clearly the proposed approach significantly improves the translation quality in all cases, although there are still considerable differences among different variants. F UTURE Layer. (Rows 1-4). All the activation functions for the F UTURE layer obtain BLEU score im"
Q18-1011,P16-1008,1,0.87499,"election over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation and over-translation (Tu et al., 2016). Ideally, PAST grows and F UTURE declines during the translation process. However, it may be difficult for a single RNN to explicitly model the above processes. In this paper, we propose a novel neural machine translation system that explicitly models PAST and F UTURE contents with two additional RNN layers. The RNN modeling the PAST contents (called PAST layer) starts from scratch and accumulates the in145 Transactions of the Association for Computational Linguistics, vol. 6, pp. 145–157, 2018. Action Editor: Philipp Koehn. Submission batch: 6/2017; Revision batch: 9/2017; Published 3/2018."
Q18-1011,D16-1027,0,0.0173536,"nslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) instead of attention probability (i.e., the probability of a source word being translated). In addition, we explicitly model both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients"
Q18-1011,D17-1013,1,0.785512,"g feature of the proposed approach. Future Modeling. Standard neural sequence decoders generate target sentences from left to right, thus failing to estimate some desired properties in the future (e.g., the length of target sentence). To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017; Bahdanau et al., 2017), in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making. Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder’s hidden states to not only generate the current target word, but also predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be allevi"
Q18-1029,D12-1108,0,0.113404,"Missing"
Q18-1029,E14-1035,0,0.0675092,"Missing"
Q18-1029,P15-1001,0,0.0995313,"Missing"
Q18-1029,P14-1062,0,0.0258353,"Missing"
Q18-1029,P17-1137,0,0.0519107,"Missing"
Q18-1029,W17-3204,0,0.0627173,"Missing"
Q18-1029,P17-1064,1,0.884515,"Missing"
Q18-1029,D16-1147,0,0.0753802,"Missing"
Q18-1029,W04-3225,0,0.187657,"Missing"
Q18-1029,P02-1040,0,0.101239,"Missing"
Q18-1029,P16-1159,1,0.900068,"Missing"
Q18-1029,W10-2602,0,0.214099,"Missing"
Q18-1029,N16-1036,0,0.0609243,"Missing"
Q18-1029,P16-1008,1,0.898208,"Missing"
Q18-1029,Q17-1007,1,0.91196,"Missing"
Q18-1029,P16-1125,0,0.0781792,"Missing"
Q18-1029,D17-1301,1,0.617798,"Missing"
Q18-1029,D17-1149,1,0.87385,"Missing"
Q18-1029,2011.mtsummit-papers.13,0,0.403824,"Missing"
Q18-1029,P12-1079,0,0.0962214,"Missing"
Q18-1029,P17-2092,1,0.892768,"Missing"
W12-3128,N07-1051,0,\N,Missing
W12-3128,A00-2018,0,\N,Missing
W12-3128,D10-1014,0,\N,Missing
W12-3128,D10-1054,0,\N,Missing
W12-3128,D11-1079,0,\N,Missing
W12-3128,W05-1506,0,\N,Missing
W12-3128,J03-4003,0,\N,Missing
W12-3128,D11-1020,0,\N,Missing
W12-3128,P11-1065,0,\N,Missing
W12-3128,P11-1001,0,\N,Missing
W12-3128,J04-4002,0,\N,Missing
W12-3128,D09-1008,0,\N,Missing
W12-3128,P07-1005,0,\N,Missing
W12-3128,P96-1021,0,\N,Missing
W12-3128,P08-1114,0,\N,Missing
W12-3128,P05-1033,0,\N,Missing
W12-3128,N03-1017,0,\N,Missing
W12-3128,P02-1038,0,\N,Missing
W12-3128,W06-3119,0,\N,Missing
W12-3128,W04-3250,0,\N,Missing
W12-3128,J07-2003,0,\N,Missing
W12-3128,2011.eamt-1.38,0,\N,Missing
W12-3128,P00-1056,0,\N,Missing
W12-3128,P03-1021,0,\N,Missing
