2021.wanlp-1.11,Automatic Difficulty Classification of {A}rabic Sentences,2021,-1,-1,2,0,518,nouran khallaf,Proceedings of the Sixth Arabic Natural Language Processing Workshop,0,"In this paper, we present a Modern Standard Arabic (MSA) Sentence difficulty classifier, which predicts the difficulty of sentences for language learners using either the CEFR proficiency levels or the binary classification as simple or complex. We compare the use of sentence embeddings of different kinds (fastText, mBERT , XLM-R and Arabic-BERT), as well as traditional language features such as POS tags, dependency trees, readability scores and frequency lists for language learners. Our best results have been achieved using fined-tuned Arabic-BERT. The accuracy of our 3-way CEFR classification is F-1 of 0.80 and 0.75 for Arabic-Bert and XLM-R classification respectively and 0.71 Spearman correlation for regression. Our binary difficulty classifier reaches F-1 0.94 and F-1 0.98 for sentence-pair semantic similarity classifier."
2020.lrec-1.229,Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks,2020,31,0,2,1,17072,yu yuan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper explores the use of Deep Learning methods for automatic estimation of quality of human translations. Automatic estimation can provide useful feedback for translation teaching, examination and quality control. Conventional methods for solving this task rely on manually engineered features and external knowledge. This paper presents an end-to-end neural model without feature engineering, incorporating a cross attention mechanism to detect which parts in sentence pairs are most relevant for assessing quality. Another contribution concerns oprediction of fine-grained scores for measuring different aspects of translation quality, such as terminological accuracy or idiomatic writing. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available."
2020.lrec-1.298,Know thy Corpus! Robust Methods for Digital Curation of Web corpora,2020,30,0,1,1,519,serge sharoff,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper proposes a novel framework for digital curation of Web corpora in order to provide robust estimation of their parameters, such as their composition and the lexicon. In recent years language models pre-trained on large corpora emerged as clear winners in numerous NLP tasks, but no proper analysis of the corpora which led to their success has been conducted. The paper presents a procedure for robust frequency estimation, which helps in establishing the core lexicon for a given corpus, as well as a procedure for estimating the corpus composition via unsupervised topic models and via supervised genre classification of Web pages. The results of the digital curation study applied to several Web-derived corpora demonstrate their considerable differences. First, this concerns different frequency bursts which impact the core lexicon obtained from each corpus. Second, this concerns the kinds of texts they contain. For example, OpenWebText contains considerably more topical news and political argumentation in comparison to ukWac or Wikipedia. The tools and the results of analysis have been released."
2020.lrec-1.715,Recognizing Semantic Relations by Combining Transformers and Fully Connected Models,2020,-1,-1,2,0,18061,dmitri roussinov,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Automatically recognizing an existing semantic relation (e.g. {``}is a{''}, {``}part of{''}, {``}property of{''}, {``}opposite of{''} etc.) between two words (phrases, concepts, etc.) is an important task affecting many NLP applications and has been subject of extensive experimentation and modeling. Current approaches to automatically telling if a relation exists between two given concepts X and Y can be grouped into two types: 1) those modeling word-paths connecting X and Y in text and 2) those modeling distributional properties of X and Y separately, not necessary in the proximity to each other. Here, we investigate how both types can be improved and combined. We suggest a distributional approach that is based on an attention-based transformer. We have also developed a novel word path model that combines useful properties of a convolutional network with a fully connected language model. While our transformer-based approach works better, both our models significantly outperform the state-of-the-art within their classes of approaches. We also demonstrate that combining the two approaches results in additional gains since they use somewhat different data sources."
2020.bucc-1.2,Overview of the Fourth {BUCC} Shared Task: Bilingual Dictionary Induction from Comparable Corpora,2020,-1,-1,3,0,20907,reinhard rapp,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"The shared task of the 13th Workshop on Building and Using Comparable Corpora was devoted to the induction of bilingual dictionaries from comparable rather than parallel corpora. In this task, for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, the participants were supposed to determine automatically the target language translations of several thousand source language test words of three frequency ranges. We describe here some background, the task definition, the training and test data sets and the evaluation used for ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition are the results of a number of systems which provide surprisingly good solutions to the ambitious problem."
R19-1069,Towards Functionally Similar Corpus Resources for Translation,2019,0,0,2,0,5502,maria kunilovskaya,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"The paper describes a computational approach to produce functionally comparable monolingual corpus resources for translation studies and contrastive analysis. We exploit a text-external approach, based on a set of Functional Text Dimensions to model text functions, so that each text can be represented as a vector in a multidimensional space of text functions. These vectors can be used to find reasonably homogeneous subsets of functionally similar texts across different corpora. Our models for predicting text functions are based on recurrent neural networks and traditional feature-based machine learning approaches. In addition to using the categories of the British National Corpus as our test case, we investigated the functional comparability of the English parts from the two parallel corpora: CroCo (English-German) and RusLTC (English-Russian) and applied our models to define functionally similar clusters in them. Our results show that the Functional Text Dimensions provide a useful description for text categories, while allowing a more flexible representation for texts with hybrid functions."
L18-1135,Language adaptation experiments via cross-lingual embeddings for related languages,2018,0,2,1,1,519,serge sharoff,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1312,Investigating the Influence of Bilingual {MWU} on Trainee Translation Quality,2018,0,0,2,1,17072,yu yuan,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1596,Cross-lingual Terminology Extraction for Translation Quality Estimation,2018,0,0,4,1,17072,yu yuan,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1605,A Multilingual Dataset for Evaluating Parallel Sentence Extraction from Comparable Corpora,2018,0,0,2,0,8591,pierre zweigenbaum,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-2512,Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora,2017,9,7,2,0,8591,pierre zweigenbaum,Proceedings of the 10th Workshop on Building and Using Comparable Corpora,0,"This paper presents the BUCC 2017 shared task on parallel sentence extraction from comparable corpora. It recalls the design of the datasets, presents their final construction and statistics and the methods used to evaluate system results. 13 runs were submitted to the shared task by 4 teams, covering three of the four proposed language pairs: French-English (7 runs), German-English (3 runs), and Chinese-English (3 runs). The best F-scores as measured against the gold standard were 0.84 (German-English), 0.80 (French-English), and 0.43 (Chinese-English). Because of the design of the dataset, in which not all gold parallel sentence pairs are known, these are only minimum values. We examined manually a small sample of the false negative sentence pairs for the most precise French-English runs and estimated the number of parallel sentence pairs not yet in the provided gold standard. Adding them to the gold standard leads to revised estimates for the French-English F-scores of at most +1.5pt. This suggests that the BUCC 2017 datasets provide a reasonable approximate evaluation of the parallel sentence spotting task."
W17-1401,Toward Pan-{S}lavic {NLP}: Some Experiments with Language Adaptation,2017,8,0,1,1,519,serge sharoff,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"There is great variation in the amount of NLP resources available for Slavonic languages. For example, the Universal Dependency treebank (Nivre et al., 2016) has about 2 MW of training resources for Czech, more than 1 MW for Russian, while only 950 words for Ukrainian and nothing for Belorussian, Bosnian or Macedonian. Similarly, the Autodesk Machine Translation dataset only covers three Slavonic languages (Czech, Polish and Russian). In this talk I will discuss a general approach, which can be called Language Adaptation, similarly to Domain Adaptation. In this approach, a model for a particular language processing task is built by lexical transfer of cognate words and by learning a new feature representation for a lesser-resourced (recipient) language starting from a better-resourced (donor) language. More specifically, I will demonstrate how language adaptation works in such training scenarios as Translation Quality Estimation, Part-of-Speech tagging and Named Entity Recognition."
W16-2611,Genre classification for a corpus of academic webpages,2016,11,0,2,0,33868,erika dalan,Proceedings of the 10th Web as Corpus Workshop,0,None
L16-1581,{M}o{B}i{L}: A Hybrid Feature Set for Automatic Human Translation Quality Assessment,2016,18,1,2,1,17072,yu yuan,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we introduce MoBiL, a hybrid Monolingual, Bilingual and Language modelling feature set and feature selection and evaluation framework. The set includes translation quality indicators that can be utilized to automatically predict the quality of human translations in terms of content adequacy and language fluency. We compare MoBiL with the QuEst baseline set by using them in classifiers trained with support vector machine and relevance vector machine learning algorithms on the same data set. We also report an experiment on feature selection to opt for fewer but more informative features from MoBiL. Our experiments show that classifiers trained on our feature set perform consistently better in predicting both adequacy and fluency than the classifiers trained on the baseline feature set. MoBiL also performs well when used with both support vector machine and relevance vector machine algorithms."
W15-5710,Large Scale Translation Quality Estimation,2015,-1,-1,2,0,36427,miguel gaona,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-5311,Applying Multi-Dimensional Analysis to a {R}ussian Webcorpus: Searching for Evidence of Genres,2015,23,1,2,0,36487,anisya katinskaya,The 5th Workshop on {B}alto-{S}lavic Natural Language Processing,0,None
W15-3410,Obtaining {SMT} dictionaries for related languages,2015,17,0,2,0,25928,miguel rios,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"This study explores methods for developing Machine Translation dictionaries on the basis of word frequency lists coming from comparable corpora. We investigate (1) various methods to measure the similarity of cognates between related languages, (2) detection and removal of noisy cognate translations using SVM ranking. We show preliminary results on several Romance and Slavonic languages."
W15-3411,{BUCC} Shared Task: Cross-Language Document Similarity,2015,3,5,1,1,519,serge sharoff,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"We summarise the organisation and results of the first shared task aimed at detecting the most similar texts in a large multilingual collection. The dataset of the shared was based on Wikipedia dumps with interlanguage links with further filtering to ensure comparability of the paired articles. The eleven system runs we received have been evaluated using the TREC evaluation metrics. 1 Task description Parallel corpora of original texts with their translations provide the basis for multilingual NLP applications since the beginning of the 1990s. Relative scarcity of such resources led to greater attention to comparable (=less parallel) resources to mine information about possible translations. Many studies have been produced within the paradigm of comparable corpora, including publications in"
J15-1009,"Book Reviews: Web Corpus Construction by Roland Sch{\\\a}fer and Felix Bildhauer""",2015,-1,-1,1,1,519,serge sharoff,Computational Linguistics,0,None
W14-4912,Multiple views as aid to linguistic annotation error analysis,2014,13,0,2,0,38346,marilena bari,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"This paper describes a methodology for supporting the task of annotating sentiment in natural language by detecting borderline cases and inconsistencies. Inspired by the co-training strategy, a number of machine learning models are trained on different views of the same data. The predictions obtained by these models are then automatically compared in order to bring to light highly uncertain annotations and systematic mistakes. We tested the methodology against an English corpus annotated according to a fine-grained sentiment analysis annotation schema (SentiML). We detected that 153 instances (35%) classified differently from the gold standard were acceptable and further 69 instances (16%) suggested that the gold standard should have been improved."
W14-4811,Evaluating Term Extraction Methods for Interpreters,2014,14,1,2,0,38363,ran xu,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,"The study investigates term extraction methods using comparable corpora for interpreters. Simultaneous interpreting requires efficient use of highly specialised domain-specific terminology in the working languages of an interpreter with limited time to prepare for new topics. We evaluate several terminology extraction methods for Chinese and English using settings which replicate real-life scenarios, concerning the task difficulty, the range of terms and the amount of materials available, etc. We also investigate interpretersxe2x80x99 perception on the usefulness of automatic termlists. The results show the accuracy of the terminology extraction pipelines is not perfect, as their precision ranges from 27% on short texts to 83% on longer corpora for English, 24% to 31% on Chinese. Nevertheless, the use of even small corpora for specialised topics greatly facilitates interpreters in their preparation."
W14-3706,Semi-supervised Graph-based Genre Classification for Web Pages,2014,28,5,3,0,38520,noushin asheghi,Proceedings of {T}ext{G}raphs-9: the workshop on Graph-based Methods for Natural Language Processing,0,"Until now, it is still unclear which set of features produces the best result in automatic genre classification on the web. Therefore, in the first set of experiments, we compared a wide range of contentbased features which are extracted from the data appearing within the web pages. The results show that lexical features such as word unigrams and character n-grams have more discriminative power in genre classification compared to features such as part-of-speech n-grams and text statistics. In a second set of experiments, with the aim of learning from the neighbouring web pages, we investigated the performance of a semi-supervised graphbased model, which is a novel technique in genre classification. The results show that our semi-supervised min-cut algorithm improves the overall genre classification accuracy. However, it seems that some genre classes benefit more from this graph-based model than others."
W14-1016,Extracting Multiword Translations from Aligned Comparable Documents,2014,21,4,2,0,20907,reinhard rapp,Proceedings of the 3rd Workshop on Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"Most previous attempts to identify translations of multiword expressions using comparable corpora relied on dictionaries of single words. The translation of a multiword was then constructed from the translations of its components. In contrast, in this work we try to determine the translation of a multiword unit by analyzing its contextual behaviour in aligned comparable documents, thereby not presupposing any given dictionary. Whereas with this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results."
asheghi-etal-2014-designing,Designing and Evaluating a Reliable Corpus of Web Genres via Crowd-Sourcing,2014,36,3,2,0,38520,noushin asheghi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Research in Natural Language Processing often relies on a large collection of manually annotated documents. However, currently there is no reliable genre-annotated corpus of web pages to be employed in Automatic Genre Identification (AGI). In AGI, documents are classified based on their genres rather than their topics or subjects. The major shortcoming of available web genre collections is their relatively low inter-coder agreement. Reliability of annotated data is an essential factor for reliability of the research result. In this paper, we present the first web genre corpus which is reliably annotated. We developed precise and consistent annotation guidelines which consist of well-defined and well-recognized categories. For annotating the corpus, we used crowd-sourcing which is a novel approach in genre annotation. We computed the overall as well as the individual categories{'} chance-corrected inter-annotator agreement. The results show that the corpus has been annotated reliably."
P13-2047,{E}nglish-to-{R}ussian {MT} evaluation campaign,2013,16,0,4,0,25781,pavel braslavski,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,This paper presents the settings and the results of the ROMIP 2013 MT shared task for the English!Russian language direction. The quality of generated translations was assessed using automatic metrics and human evaluation. We also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations.
W12-0114,Design of a hybrid high quality machine translation system,2012,37,5,7,0.698259,12054,bogdan babych,Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"This paper gives an overview of the ongoing FP7 project HyghTra (2010--2014). The HyghTra project is conducted in a partnership between academia and industry involving the University of Leeds and Lingenio GmbH (company). It adopts a hybrid and bootstrapping approach to the enhancement of MT quality by applying rule-based analysis and statistical evaluation techniques to both parallel and comparable corpora in order to extract linguistic information and enrich the lexical and syntactic resources of the underlying (rule-based) MT system that is used for analysing the corpora. The project places special emphasis on the extension of systems to new language pairs and corresponding rapid, automated creation of high quality resources. The techniques are fielded and evaluated within an existing commercial MT environment."
rapp-etal-2012-identifying,Identifying Word Translations from Comparable Documents Without a Seed Lexicon,2012,8,11,2,0,20907,reinhard rapp,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The extraction of dictionaries from parallel text corpora is an established technique. However, as parallel corpora are a scarce resource, in recent years the extraction of dictionaries using comparable corpora has obtained increasing attention. In order to find a mapping between languages, almost all approaches suggested in the literature rely on a seed lexicon. The work described here achieves competitive results without requiring such a seed lexicon. Instead it presupposes mappings between comparable documents in different languages. For some common types of textual resources (e.g. encyclopedias or newspaper texts) such mappings are either readily available or can be established relatively easily. The current work is based on Wikipedias where the mappings between languages are determined by the authors of the articles. We describe a neural-network inspired algorithm which first characterizes each Wikipedia article by a number of keywords, and then considers the identification of word translations as a variant of word alignment in a noisy environment. We present results and evaluations for eight language pairs involving Germanic, Romanic, and Slavic languages as well as Chinese."
2012.tc-1.14,Beyond translation memories: finding similar documents in comparable corpora,2012,-1,-1,1,1,519,serge sharoff,Proceedings of Translating and the Computer 34,0,None
W11-3603,Cross Language {POS} Taggers (and other Tools) for {I}ndian Languages: An Experiment with {K}annada using {T}elugu Resources,2011,17,37,2,0,3549,siva reddy,Proceedings of the Fifth International Workshop On Cross Lingual Information Access,0,"Indian languages are known to have a large speaker base, yet some of these languages have minimal or non-efficient linguistic resources. For example, Kannada is relatively resource-poor compared to Malayalam, Tamil and Telugu, which in-turn are relatively poor compared to Hindi. Many Indian language pairs exhibit high similarities in morphology and syntactic behaviour e.g. Kannada is highly similar to Telugu. In this paper, we show how to build a cross-language part-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools."
Y10-1089,Advanced Corpus Solutions for Humanities Researchers,2010,11,5,3,0,45052,james wilson,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"This paper describes the design and implementation of an interface to corpora in 12 languages, stemming from the analysis of the needs of a diverse group of users: language teachers and language students, (non-computational) linguists, researchers in history and translation studies. We identified a set of requirements shared across the disciplines, as well as more specific requirements from the targeted user groups. The interface is designed to handle large-scale corpora of 20-500 million words."
P10-1077,Fine-Grained Genre Classification Using Structural Learning Algorithms,2010,28,11,3,0,45696,zhili wu,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Prior use of machine learning in genre classification used a list of labels as classification categories. However, genre classes are often organised into hierarchies, e.g., covering the subgenres of fiction. In this paper we present a method of using the hierarchy of labels to improve the classification accuracy. As a testbed for this approach we use the Brown Corpus as well as a range of other corpora, including the BNC, HGC and Syracuse. The results are not encouraging: apart from the Brown corpus, the improvements of our structural classifier over the flat one are not statistically significant. We discuss the relation between structural learning performance and the visual and distributional balance of the label hierarchy, suggesting that only balanced hierarchies might profit from structural learning."
sharoff-etal-2010-web,The Web Library of Babel: evaluating genre collections,2010,19,32,1,1,519,serge sharoff,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present experiments in automatic genre classification on web corpora, comparing a wide variety of features on several different genreannotated datasets (HGC, I-EN, KI-04, KRYS-I, MGC and SANTINIS).We investigate the performance of several types of features (POS n-grams, character n-grams and word n-grams) and show that simple character n-grams perform best on current collections because of their ability to generalise both lexical and syntactic phenomena related to genres. However, we also show that these impressive results might not be transferrable to the wider web due to the lack of comparability between different annotation labels (many webpages cannot be described in terms of the genre labels in individual collections), lack of representativeness of existing collections (many genres are represented by webpages coming from a small number of sources) as well as problems in the reliability of genre annotation (many pages from the web are difficult to interpret in terms of the labels available). This suggests that more research is needed to understand genres on the Web."
2009.eamt-1.6,Evaluation-Guided Pre-Editing of Source Text: Improving {MT}-Tractability of Light Verb Constructions,2009,6,2,3,0.990418,12054,bogdan babych,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper reports an experiment on evaluating and improving MT quality of light-verb construction (LVCs) xe2x80x93 combinations of a xe2x80x98semantically depletedxe2x80x99 verb and its complement. Our method uses construction-level human evaluation for systematic discovery of mistranslated contexts and creating automatic pre-editing rules, which make the constructions more tractable for Rule-Based Machine Translation (RBMT) systems. For rewritten phrases we achieve about 40% reduction in the number of incomprehensible translations into English from both French and Russian. The proposed method can be used for enhancing automatic pre-editing functionality of state-of-theart MT systems. It will allow MT users to create their own rewriting rules for frequently mistranslated constructions and contexts, going beyond existing systemsxe2x80x99 capabilities offered by user dictionaries and do-not translate lists."
babych-etal-2008-generalising,Generalising Lexical Translation Strategies for {MT} Using Comparable Corpora,2008,15,3,2,1,12054,bogdan babych,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We report on an on-going research project aimed at increasing the range of translation equivalents which can be automatically discovered by MT systems. The methodology is based on semi-supervised learning of indirect translation strategies from large comparable corpora and applying them in run-time to generate novel, previously unseen translation equivalents. This approach is different from methods based on parallel resources, which currently can reuse only individual translation equivalents. Instead it models translation strategies which generalise individual equivalents and can successfully generate an open class of new translation solutions. The task of the project is integration of the developed technology into open-source MT systems."
baroni-etal-2008-cleaneval,{C}leaneval: a Competition for Cleaning Web Pages,2008,5,65,4,0,12129,marco baroni,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Cleaneval is a shared task and competitive evaluation on the topic of cleaning arbitrary web pages, with the goal of preparing web data for use as a corpus for linguistic and language technology research and development. The first exercise took place in 2007. We describe how it was set up, results, and lessons learnt"
sharoff-etal-2008-designing,Designing and Evaluating a {R}ussian Tagset,2008,15,39,1,1,519,serge sharoff,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper reports the principles behind designing a tagset to cover Russian morphosyntactic phenomena, modifications of the core tagset, and its evaluation. The tagset is based on the MULTEXT-East framework, while the decisions in designing it were aimed at achieving a balance between parameters important for linguists and the possibility to detect and disambiguate them automatically. The final tagset contains about 500 tags and achieves about 95{\%} accuracy on the disambiguated portion of the Russian National Corpus. We have also produced a test set that can be shared with other researchers."
kurella-etal-2008-corpus,Corpus-Based Tools for Computer-Assisted Acquisition of Reading Abilities in Cognate Languages,2008,10,0,2,0,48474,svitlana kurella,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents an approach to computer-assisted teaching of reading abilities using corpus data. The approach is supported by a set of tools for automatically selecting and classifying texts retrieved from the Internet. The approach is based on a linguistic model of textual cohesion which describes relations between larger textual units that go beyond the sentence level. We show that textual connectors that link such textual units reliably predict different types of texts, such as ÂinformationÂ and ÂopinionÂ: using only textual connectors as features, an SVM classifier achieves an F-score of between 0.85 and 0.93 for predicting these classes. The tools are used in our project on teaching reading skills in a cognate foreign language (L3) which is cognate to a known foreign language (L2)."
P07-1018,Assisting Translators in Indirect Lexical Transfer,2007,9,11,3,1,12054,bogdan babych,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present the design and evaluation of a translatorxe2x80x99s amenuensis that uses comparable corpora to propose and rank nonliteral solutions to the translation of expressions from the general lexicon. Using distributional similarity and bilingual dictionaries, the method outperforms established techniques for extracting translation equivalents from parallel corpora. The interface to the system is available at: http://corpus.leeds.ac.uk/assist/v05/"
2007.tc-1.3,A dynamic dictionary for discovering indirect translation equivalents,2007,-1,-1,3,1,12054,bogdan babych,Proceedings of Translating and the Computer 29,0,None
2007.mtsummit-papers.5,Translating from under-resourced languages: comparing direct transfer against pivot translation,2007,-1,-1,3,1,12054,bogdan babych,Proceedings of Machine Translation Summit XI: Papers,0,None
P06-2095,Using Comparable Corpora to Solve Problems Difficult for Human Translators,2006,12,15,1,1,519,serge sharoff,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"In this paper we present a tool that uses comparable corpora to find appropriate translation equivalents for expressions that are considered by translators as difficult. For a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation of the performance of the tool, which highlight its usefulness when dictionary solutions are lacking."
sharoff-2006-uniform,A Uniform Interface to Large-Scale Linguistic Resources,2006,8,5,1,1,519,serge sharoff,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In the paper we address two practical problems concerning the use of corpora in translation studies. The first stems from the limited resources available for targeted languages and genres within languages, whereas translation researchers and students need: sufficiently large modern corpora, either reflecting general language or specific to a problem domain. The second problem concerns the lackof a uniform interface for accessing the resources, even when the yexist. We deal with the first problem by developing a framework for semi-automatic acquisition of large corpora from the Internet for the languages relevant for our research and training needs. We outline the methodology used and discuss the composition of Internet-derived corpora. We deal with the second problem by developing a uniform interface to our corpora. In addition to standard options for choosingcorpora and sorting concordance lines, the interface can compute the list of collocations and filter the results according touser-specified patterns in order to detect language-specific syntacticstructures."
sharoff-etal-2006-using,Using collocations from comparable corpora to find translation equivalents,2006,14,4,1,1,519,serge sharoff,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,In this paper we present a tool for finding appropriate translation equivalents for words from the general lexicon using comparable corpora. For a phrase in the source language the tool suggests arange of possible expressions used in similar contexts in target language corpora. In the paper we discuss the method and present results of human evaluation of the performance of the tool.
ciobanu-etal-2006-using,Using Richly Annotated Trilingual Language Resources for Acquiring Reading Skills in a Foreign Language,2006,13,1,3,0,50358,dragocs ciobanu,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In an age when demand for innovative and motivating language teaching methodologies is at a very high level, TREAT - the Trilingual REAding Tutor - combines the most advanced natural language processing (NLP) techniques with the latest second and third language acquisition (SLA/TLA) research in an intuitive and user-friendly environment that has been proven to help adult learners (native speakers of L1) acquire reading skills in an unknown L3 which is related to (cognate with) an L2 they know to some extent. This corpus-based methodology relies on existing linguistic resources, as well as materials that are easy to assemble, and can be adapted to support other pairs of L2-L3 related languages, as well. A small evaluation study conducted at the Leeds University Centre for Translation Studies indicates that, when using TREAT, learners feel more motivated to study an unknown L3, acquire significant linguistic knowledge of both the L3 and L2 rapidly, and increase their performance when translating from L3 into L1."
E06-2014,{ASSIST}: Automated Semantic Assistance for Translators,2006,10,4,1,1,519,serge sharoff,Demonstrations,0,"The problem we address in this paper is that of providing contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus."
W04-0403,What is at Stake: a Case Study of {R}ussian Expressions Starting with a Preposition,2004,13,16,1,1,519,serge sharoff,Proceedings of the Workshop on Multiword Expressions: Integrating Processing,0,"The paper describes an experiment in detecting a specific type of multiword expressions in Russian, namely expressions starting with a preposition. This covers not only prepositional phrases proper, but also fixed syntactic constructions like v techenie ('in the course of'). First, we collect lists of such constructions in a corpus of 50 mln words using a simple mechanism that combines statistical methods with knowledge about the structure of Russian prepositional phrases. Then we analyse the results of this data collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus."
sharoff-2004-towards,Towards Basic Categories for Describing Properties of Texts in a Corpus,2004,5,7,1,1,519,serge sharoff,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,The paper discusses the basic principles for describing proper ties of texts to be stored in a corpus and suggests the st andard that is used in the majority of corpora developed at the University of L eeds and can be potentially employed for describing texts in any co rpus collecting activity. The standard defines the minimal subset of tags and attributes that are necessary for describing texts stored in a corpus. The proposed text typology helps to position a corpus under development with respect to a reference corpus cover ing all possible features by explicit selection of a subset of featur es to be considered in the study.
sharoff-2002-meaning,Meaning as use: exploitation of aligned corpora for the contrastive study of lexical semantics,2002,8,16,1,1,519,serge sharoff,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The paper discusses the use of corpora for experimental studies in contrastive lexical semantics, in particular, for comparing how a state of affairs is expressed in different languages and by different translators. Three topics are addressed: (1) a lexicographic database, which is aimed at storing and maintaining contrastive descriptions of a class of lexical items in several languages; (2) an aligned parallel English-Russian corpus, including several literary texts and software manuals (the total size is about one million words), together with tools for querying the corpus by means of Perl-based regular expressions; and (3) an example of development of a lexicographical database of the most frequent size adjectives in English, German and Russian. * The research was supported by the Alexander von Humboldt Foundation, Germany."
bateman-etal-2000-resources,Resources for Multilingual Text Generation in Three {S}lavic Languages,2000,9,8,5,0,27622,john bateman,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The paper discusses the methods followed to re-use a large-scale, broad-coverage English grammar for constructing similar scale grammars for Bulgarian, Czech and Russian for the fast prototyping of a multilingual generation system. We present (1) the theoretical and methodological basis for resource sharing across languages, (2) the use of a corpus-based contrastive register analysis, in particular, contrastive analysis of mood and agency. Because the study concerns reuse of the grammar of a language that is typologically quite different from the languages treated, the issues addressed in this paper appear relevant to a wider range of researchers in need of largescale grammars for less-researched languages."
C00-1069,Multilinguality in a Text Generation System For Three {S}lavic Languages,2000,3,11,6,0,45127,geertjan kruijff,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper describes a multilingual text generation system in the domain of CAD/CAM software instructions for Bulgarian, Czech and Russian. Starting from a language-independent semantic representation, the system drafts natural, continuous text as typically found in software manuals. The core modules for strategic and tactical generation are implemented using the KPML platform for linguistic resource development and generation. Prominent characteristics of the approach implemented are a treatment of multilinguality that makes maximal use of the commonalities between languages while also accounting for their differences and a common representational strategy for both text planning and sentence generation."
