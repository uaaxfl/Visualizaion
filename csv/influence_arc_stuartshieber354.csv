2004.tmi-1.8,P03-1021,0,0.0545324,"ber of subjects is usually preferable. Therefore, while manual methods may remain viable for isolated, large-scale evaluations, researchers should ideally benefit from the ability to quickly and accurately assess their own systems repeatedly as new ideas are implemented; in these situations, automatic methods have the potential to expedite the development of successful MT systems by greatly reducing the resources required for evaluation. In addition, new statistical MT systems have successfully incorporated training methods that directly optimize their results on automatic evaluation metrics (Och 2003). Having reliable metrics therefore allows MT systems to be not only tested but trained very rapidly. In recent years, a number of automatic metrics have been proposed and used for evaluating the overall performance of MT systems. General-purpose measures like word error rate (WER) and position-independent word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics B LEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a"
2004.tmi-1.8,2001.mtsummit-papers.68,0,0.0403344,"y incorporated training methods that directly optimize their results on automatic evaluation metrics (Och 2003). Having reliable metrics therefore allows MT systems to be not only tested but trained very rapidly. In recent years, a number of automatic metrics have been proposed and used for evaluating the overall performance of MT systems. General-purpose measures like word error rate (WER) and position-independent word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics B LEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a set of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT results. The F-Measure has been proposed as a more comprehensible alternative for MT evaluation (Turian et al. 2003), and can be defined as a simple composite of unigram precision and recall. These metrics have demonstrated some success; the B LEU metric, for example, has been shown to correlate highly with the judgments of bilingual human evaluators (a correlation coefficient of 0.96) when averaged ov"
2004.tmi-1.8,2003.mtsummit-papers.51,0,0.551931,"l performance of MT systems. General-purpose measures like word error rate (WER) and position-independent word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics B LEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a set of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT results. The F-Measure has been proposed as a more comprehensible alternative for MT evaluation (Turian et al. 2003), and can be defined as a simple composite of unigram precision and recall. These metrics have demonstrated some success; the B LEU metric, for example, has been shown to correlate highly with the judgments of bilingual human evaluators (a correlation coefficient of 0.96) when averaged over a 500-sentence corpus (Papineni et al. 2001). However, though useful for system discrimination, metrics correlating with human evaluations only over long texts (which tend to average out the “noise” of evaluation) are relatively useless for purposes of error analysis; they are too blunt to provide specific"
2004.tmi-1.8,P02-1040,0,\N,Missing
2004.tmi-1.8,C04-1046,1,\N,Missing
2006.amta-papers.15,J93-2003,0,0.00490333,"be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between parsing efficiency on one hand and the rest of the desired characteristics on the other. However, even among formalisms with the same parse complexity, some formalisms better satisfy the desiderata than others. Finite-state word-based models, such as IBM Model 5 (Brown et al., 1993), use a base formalism that allows for synchronization, probabilistic variants, very efficient processing, and good ability to capture lexical and bilexical relationships. However, they are limited by the inability to use hierarchical information in the interlingual mapping. That bilingual dictionaries describe the mappings between languages in terms of constructions, not individual words, suggests that this information would be useful. For instance, the HarperCollins Italian College Dictionary (HCICD) translates the English “to take advantage of” as “sfruttare”, although that word is a direct"
2006.amta-papers.15,P05-1033,0,0.00464896,"are well known to perform poorly as language models compared to finite-state models; they gain the ability to substitute according to abstract categories at the expense of stating lexical relationships directly. Although arbitrary CFGs can be weakly lexicalized by other CFGs, this can require changing the shape of the derived trees produced, and more critically, changes the structure of the derivation (Schabes and Waters, 1993a; Schabes and Waters, 1995). Because synchronization requires substantial isomorphism of the derivation trees, synchronization of lexicalized CFGs becomes problematic. Chiang (2005) overcomes this short1 For example, “The US took advantage yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchronous CFG-based system by making it both hierachical and phrase-based so that n-grams used in phrasal mappings could still capture some of the lexical dependencies. His system outper"
2006.amta-papers.15,P05-1067,0,0.0161726,"more parsing efficiency for greater expressivity (Melamed, 2004; Melamed et al., 2004). Formalisms such as Generalized Multitext Grammars (GMTG) do in principle satisfy all of the desiderata if the higher time and space complexity of the parsing algorithms for them does not make training prohibitively expensive (Melamed et al., 2004). It is an empirical question whether systems with a high degree of parse complexity can be induced in practice. Burbank et al. (2005) implemented a framework in which base formalisms such as GMTG can be tested though no substantial results have yet been reported. Ding and Palmer (2005) also employ a more expressive formalism but use heuristic approaches to limit the complexity of the processing. All of these considerations led us to seek a more expressive formalism that could still be parsed efficiently. As we will argue, probabilistic synchronous tree-insertion grammar substantially satisfies each of the desiderata without increasing parse complexity. We present an MT system based on it in the remainder of this paper. tution of sub-parts. Due to space limitations, for a detailed description of the TAG formalism we refer readers to the introduction by Joshi (1985). Importan"
2006.amta-papers.15,P99-1059,0,0.0716925,"Missing"
2006.amta-papers.15,J99-4004,0,0.121599,"Missing"
2006.amta-papers.15,W05-0833,0,0.164752,"antially higher quality translations can be produced by unsupervised induction systems that can easily be hybridized with 128 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 128-137, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabili"
2006.amta-papers.15,C04-1154,0,0.381389,"ce a particular formalism, probabilistic synchronous treeinsertion grammar (PSTIG) that we argue satisfies the desiderata optimally within the class of formalisms that can be parsed no less efficiently than context-free grammars and demonstrate that it outperforms state-of-the-art word-based and phrasebased finite-state translation models on training and test data taken from the EuroParl corpus (Koehn, 2005). We then argue that a higher level of translation quality can be achieved by hybridizing our induced model with elementary structures produced using supervised techniques such as those of Groves et al. (2004). 1 Introduction In this paper we identify a base formalism, probabilistic synchronous tree-insertion grammar (PSTIG), for a statistical machine translation system that we propose: 1. maximizes, within its efficiency class, the quality of the MT system induced unsupervised from aligned sentence pairs; and We begin with an argument from first principles for the choice of PSTIG as a base formalism for syntax-aware statistical machine translation (SMT). We then present our implementation of a system that induces a PSTIG unsupervised from data and show that it outperforms a state-of-the-art phrase"
2006.amta-papers.15,N03-1017,0,0.00282311,"joint probability distributions over frequently co-occuring n-grams to find multiword translations, thereby improving on the performance of IBM Model 5. Such an approach does allow multiword relationships to be induced, but does not in any sense incorporate syntactic structure to do so. Indeed the natural way to augment the multiword approach to incorporate syntactic constraints is to restrict the multiword sequences to syntactic constituents (as determined by a statistical parser for instance) (Yamada and Knight, 2002). Yet this augmentation turns out to underperform the syntax-free variant (Koehn et al., 2003). The reason is not hard to understand: the word sequences that map well in translation—such as the German-English example of Koehn et al. (2003) “es gibt”/“there is”—are not themselves syntactic constituents, but rather syntactic templates (“es gibt. . . ”/“there is. . . ”) with “holes” (marked here by ellipses) that might be substituted for in some uniform manner. Bilingual dictionaries even make the mapping between such constructions explicit through the use of place fillers like “sb” (“somebody”) or “qn” (“qualcuno”), as in the HCICD entry “to drive sb mad”/“far impazzire qn”. Secondarily,"
2006.amta-papers.15,N03-1021,0,0.0173718,"ricas, pages 128-137, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between"
2006.amta-papers.15,P04-1083,0,0.420773,"8-137, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between parsing effici"
2006.amta-papers.15,J03-1002,0,0.00221192,"Model 2 only in that it does not allow for one-to-many mappings. We evaluated our system on a set of 15,277 sentences extracted from the over 600,000 sentence pairs in the EuroParl German-English corpus (Koehn, 2005). We selected sentence pairs in which the German sentence contained only words within the 1000 most frequent German words in the corpus, in order to limit the size of the vocabulary. We then further limited the set by using only sentence pairs with combined length less than 25.7 We held out 100 sentence pairs for evaluation of the resulting system. As baselines we trained GIZA++ (Och and Ney, 2003) using the CMU-Cambridge Statistical Language Modeling Toolkit (CMU Toolkit) and the ISI ReWrite Decoder for testing, and Pharaoh using alignments generated according to the algorithm given in Koehn (2003) based on GIZA++ word alignments and using the CMU Toolkit for language modeling.8 We then evaluated all three systems on the test set automatically using BLEU score. We also ran a human evaluation in which three subjects evaluated all of the 100 translations produced by each system, in random order and with no indication of which system generated the translation, against the “gold standard”"
2006.amta-papers.15,1993.iwpt-1.20,0,0.589423,"ext-free grammars. A natural approach, then is to incorporate synchronization of contextfree structures to allow for these kinds of mappings. However, probabilistic context-free grammars (PCFG) are well known to perform poorly as language models compared to finite-state models; they gain the ability to substitute according to abstract categories at the expense of stating lexical relationships directly. Although arbitrary CFGs can be weakly lexicalized by other CFGs, this can require changing the shape of the derived trees produced, and more critically, changes the structure of the derivation (Schabes and Waters, 1993a; Schabes and Waters, 1995). Because synchronization requires substantial isomorphism of the derivation trees, synchronization of lexicalized CFGs becomes problematic. Chiang (2005) overcomes this short1 For example, “The US took advantage yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchr"
2006.amta-papers.15,J95-4002,0,0.850329,"l approach, then is to incorporate synchronization of contextfree structures to allow for these kinds of mappings. However, probabilistic context-free grammars (PCFG) are well known to perform poorly as language models compared to finite-state models; they gain the ability to substitute according to abstract categories at the expense of stating lexical relationships directly. Although arbitrary CFGs can be weakly lexicalized by other CFGs, this can require changing the shape of the derived trees produced, and more critically, changes the structure of the derivation (Schabes and Waters, 1993a; Schabes and Waters, 1995). Because synchronization requires substantial isomorphism of the derivation trees, synchronization of lexicalized CFGs becomes problematic. Chiang (2005) overcomes this short1 For example, “The US took advantage yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchronous CFG-based system by ma"
2006.amta-papers.15,koen-2004-pharaoh,0,0.01532,"e yesterday of the political and military momentum in its Afghan campaign. . . ” is one of many Google hits on the phrase “took advantage yesterday of”. 129 VP V NP take advantage VP PP P of V N P↓ NP NP take advantage Jean PP P of PP N P↓ Adv P P∗ yesterday Figure 1: An example TAG/TIG substitution Figure 2: An example TAG/TIG adjunction coming in his synchronous CFG-based system by making it both hierachical and phrase-based so that n-grams used in phrasal mappings could still capture some of the lexical dependencies. His system outperformed Pharaoh, a state-of-the-art phrase-based decoder (Koehn, 2004), on several translation tasks. Although systems such as Chiang’s are the current state-of-the-art, because of the limitations of CFGs as a base formalism Melamed and others continue to explore the possibility of trading off more parsing efficiency for greater expressivity (Melamed, 2004; Melamed et al., 2004). Formalisms such as Generalized Multitext Grammars (GMTG) do in principle satisfy all of the desiderata if the higher time and space complexity of the parsing algorithms for them does not make training prohibitively expensive (Melamed et al., 2004). It is an empirical question whether sy"
2006.amta-papers.15,C88-2121,0,0.293951,"employ a more expressive formalism but use heuristic approaches to limit the complexity of the processing. All of these considerations led us to seek a more expressive formalism that could still be parsed efficiently. As we will argue, probabilistic synchronous tree-insertion grammar substantially satisfies each of the desiderata without increasing parse complexity. We present an MT system based on it in the remainder of this paper. tution of sub-parts. Due to space limitations, for a detailed description of the TAG formalism we refer readers to the introduction by Joshi (1985). Importantly, Schabes et al. (1988) show that TAG can lexicalize CFG without changing the trees produced. That is, given a CFG a lexicalized TAG can be constructed that will produce the same set of derived structures produced by the CFG. Because each elementary tree contains a lexical item, the operations of substitution and adjunction implicitly manifest a lexical relationship. In addition, the two operations of TAG, substitution and adjunction, are exactly what is needed to handle noncontiguity, as shown in Figures 1 and 2. However, the TAG formalism’s additional expressivity leads to additional processing complexity. TAG par"
2006.amta-papers.15,2005.mtsummit-papers.11,0,0.0828704,"rsing of it will be. However, even among formalisms with the same parse complexity, some formalisms better realize the desired characteristics for machine translation formalisms than others. We introduce a particular formalism, probabilistic synchronous treeinsertion grammar (PSTIG) that we argue satisfies the desiderata optimally within the class of formalisms that can be parsed no less efficiently than context-free grammars and demonstrate that it outperforms state-of-the-art word-based and phrasebased finite-state translation models on training and test data taken from the EuroParl corpus (Koehn, 2005). We then argue that a higher level of translation quality can be achieved by hybridizing our induced model with elementary structures produced using supervised techniques such as those of Groves et al. (2004). 1 Introduction In this paper we identify a base formalism, probabilistic synchronous tree-insertion grammar (PSTIG), for a statistical machine translation system that we propose: 1. maximizes, within its efficiency class, the quality of the MT system induced unsupervised from aligned sentence pairs; and We begin with an argument from first principles for the choice of PSTIG as a base fo"
2006.amta-papers.15,C90-3045,1,0.484228,"as determined by the location of the non-foot material. To maintain the invariant that textual material falls only on a single side of the spine, adjunction is restricted so that left auxiliary trees may not adjoin into a node on the spine of a right auxiliary tree and vice versa. This prevents the formation of “wrapping” trees in which there are terminal symbols on both sides of the foot node. This restriction, coupled with the requirement 3 Synchronous Tree-Insertion Grammar Tree-adjoining grammars (TAG), introduced in monolingual form by Joshi (1985), and in a synchronous variant (STAG) by Shieber and Schabes (1990), are natural choices to capture lexicallybased dependencies while also allowing the substi130 WordAx AuxAx ![ηS , (i, i + 1)], [ηT , (l, l + 1)], ∅, 1# ![ηS , (i, i)], [ηT , (l, l)], ∅, 1# EmptyAx ![η! , (i, i)], [η! , (j, j)], {(x, y)}, 1# wi+1 = Label(ηS ) vl+1 = Label(ηT ) F oot(ηS ) F oot(ηT ) x, y ∈ {L, R} EmptyT ree(η! ) Figure 3: Axioms for CKY-style PSTIG parsing that all elementary auxiliary trees be non-wrapping, is sufficient to limit the formalism to context-free expressivity and O(n3 ) parsability. In addition, Schabes and Waters (Schabes and Waters, 1995) demonstrate that TIG, l"
2006.amta-papers.15,W02-1018,0,0.0201298,"ey are limited by the inability to use hierarchical information in the interlingual mapping. That bilingual dictionaries describe the mappings between languages in terms of constructions, not individual words, suggests that this information would be useful. For instance, the HarperCollins Italian College Dictionary (HCICD) translates the English “to take advantage of” as “sfruttare”, although that word is a direct translation of neither “take” nor “advantage”. Retaining the same finite-state base formalism, these models can be augmented to allow multiword (in addition to single word) mapping. Marcu and Wong (2002), among others, use joint probability distributions over frequently co-occuring n-grams to find multiword translations, thereby improving on the performance of IBM Model 5. Such an approach does allow multiword relationships to be induced, but does not in any sense incorporate syntactic structure to do so. Indeed the natural way to augment the multiword approach to incorporate syntactic constraints is to restrict the multiword sequences to syntactic constituents (as determined by a statistical parser for instance) (Yamada and Knight, 2002). Yet this augmentation turns out to underperform the s"
2006.amta-papers.15,P04-1084,0,0.0905368,"e, August 2006. ©2006 The Association for Machine Translation in the Americas linguistically-motivated elementary structures generated manually or though a supervised process. In particular, we propose a method of hybridizing our system by adding elementary structures generated using the methods of Groves et al. (2004) in a manner similar to that used by Groves and Way (2005). 2 Motivation and Related Work Recent work in statistical machine translation by parsing has identified a set of characteristics an ideal base formalism should have for the translation task (Melamed, 2003; Melamed, 2004; Melamed et al., 2004). What is desired is a formalism that has the substitution-based hierarchical structure of contextfree grammars and the lexical relationship potential of n-gram models. Further, it should allow for discontinuity in phrases and be synchronizable, to allow for multilinguality. Finally, in order to support automated induction, it should allow for a probabilistic variant, and a reasonably efficient parsing algorithm. The more expressive and flexible a formalism is, the less efficient parsing of it will be. Therefore, the primary trade-off to be made is between parsing efficiency on one hand and th"
2006.amta-papers.15,P02-1039,0,0.0109959,"to allow multiword (in addition to single word) mapping. Marcu and Wong (2002), among others, use joint probability distributions over frequently co-occuring n-grams to find multiword translations, thereby improving on the performance of IBM Model 5. Such an approach does allow multiword relationships to be induced, but does not in any sense incorporate syntactic structure to do so. Indeed the natural way to augment the multiword approach to incorporate syntactic constraints is to restrict the multiword sequences to syntactic constituents (as determined by a statistical parser for instance) (Yamada and Knight, 2002). Yet this augmentation turns out to underperform the syntax-free variant (Koehn et al., 2003). The reason is not hard to understand: the word sequences that map well in translation—such as the German-English example of Koehn et al. (2003) “es gibt”/“there is”—are not themselves syntactic constituents, but rather syntactic templates (“es gibt. . . ”/“there is. . . ”) with “holes” (marked here by ellipses) that might be substituted for in some uniform manner. Bilingual dictionaries even make the mapping between such constructions explicit through the use of place fillers like “sb” (“somebody”)"
2006.amta-papers.15,P93-1017,0,\N,Missing
2006.amta-papers.15,W90-0102,1,\N,Missing
2020.bea-1.1,W13-1504,0,0.0192833,"analyses relevant to readability. In addition to utilizing features posed in the existing readability research, we investigated formulating new features with a focus on syntactic ambiguity and syntactic diversity. This challenging aspect of language appeared to be underutilized in existing readability literature. Often, readability classes within a corpus are treated as unrelated. These approaches use raw labels as distinct unordered classes. However, readability labels are ordinal, ranging from lower to higher readability. Some work has addressed this issue such as the readability models of Flor et al. (2013) which predict grade levels via linear regression. To test different approaches to acknowledging this ordinality, we devised three methods for labeling the documents: “classification”, “age regression”, and “ordered class regression”. The classification approach uses the classes originally given. This approach does not suppose any ordinality of the classes. Avoiding such ordinality may be desirable for the sake of simplicity. “Age regression” applies the mean of the age ranges given by the constituent datasets. For instance, in this approach Level 2 documents from Weekly Reader would be given"
2020.bea-1.1,D14-1181,0,0.00530477,"ends to the different words within each sentence while the second mechanism attends to the sentences within the document. These hierarchical attention mechanisms are thought to better mimic the structure of documents and consequently produce superior classification results. The implementation of the model used in this paper is identical to the original architecture described by Yang et al. (2016) and was provided by the authors of Martinc et al. (2019) based on code by Nguyen (2020). CNN Convolutional neural networks were selected for their demonstrated performance on sentence classification (Kim, 2014). The CNN model used in this paper is based on the one described by Kim (2014) and implemented using the Keras (Chollet and others, 2015), Tensorflow (Abadi et al., 2015), and Magpie libraries. 3.4 Incorporating Linguistic Features with Neural Models The neural network models thus far described take either the raw text or word vector embeddings of the text as input. They make no use of linguistic features such as those described in section 3.2. We hypothesized that combining these linguistic features with the deep neural models may improve their performance on readability assessment. Although"
2020.bea-1.1,W16-0502,0,0.444089,"ity level. The size of readability corpora expanded significantly with the introduction of the WeeklyReader corpus by Schwarm and Ostendorf (2005). Composed of articles from an educational magazine, the WeeklyReader corpus contains roughly 2,400 articles. The WeeklyReader corpus was then built upon by Vajjala and Meurers (2012) by adding data from the BBC Bitesize website to form the WeeBit corpus. This WeeBit corpus is larger, containing roughly 6,000 documents, while also spanning a greater range of readability levels. Within these corpora, topic and readability are highly correlated. Thus, Xia et al. (2016) constructed the Newsela corpus in which each article is represented at multiple reading levels thereby diminishing this correlation. Early work on readability assessment, such as that of Flesch (1948), extracted simple textual features like character count. More recently, Schwarm and Ostendorf (2005) analyzed a broader set of features including out-of-vocabulary scores and syntactic features such as average parse tree height. Vajjala and Meurers (2012) assembled perhaps the broadest class of features. They incorporated measures shown by Lu (2010) to correlate well with second language acquisi"
2020.bea-1.1,N16-1174,0,0.0765873,"verage gradient descent (“sag”) optimizer. HAN The Hierarchical attention network involves feeding the input through two bidirectional RNNs each accompanied by a separate attention mechanism. One attention mechanism attends to the different words within each sentence while the second mechanism attends to the sentences within the document. These hierarchical attention mechanisms are thought to better mimic the structure of documents and consequently produce superior classification results. The implementation of the model used in this paper is identical to the original architecture described by Yang et al. (2016) and was provided by the authors of Martinc et al. (2019) based on code by Nguyen (2020). CNN Convolutional neural networks were selected for their demonstrated performance on sentence classification (Kim, 2014). The CNN model used in this paper is based on the one described by Kim (2014) and implemented using the Keras (Chollet and others, 2015), Tensorflow (Abadi et al., 2015), and Magpie libraries. 3.4 Incorporating Linguistic Features with Neural Models The neural network models thus far described take either the raw text or word vector embeddings of the text as input. They make no use of"
2020.nlp4convai-1.15,P18-1198,0,0.110912,"the generated text. In this study, we explore internal representations instead, motivated by the fact that reasonable internal behavior is crucial for interpretability and is often a prerequisite for effective external behavior. Outside of open-domain dialog, probing has been applied for analyzing natural language processing models in machine translation (Belinkov et al., 2017) and visual question answering (Subramanian et al., 2019). Probing is also commonly used for evaluating the quality of “universal” sentence representations which are trained once and used for a variety of applications (Conneau et al., 2018; Adi et al., 2016) (for example, InferSent (Conneau et al., 2017), SkipThought (Kiros et al., 2015), USE (Cer et al., 2018)). Along the same lines, natural language understanding benchmarks such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) propose a set of diverse tasks for evaluating general linguistic knowledge. Our analysis differs from previous work since it is focused on probing for conversational skills that are particularly relevant to dialog generation. With regard to perturbation experiments, Sankar et al. (2019) found that standard dialog models are largely insensit"
2020.nlp4convai-1.15,W19-3646,0,0.0143832,"l., 2017), a popular open-source platform for building dialog systems. We also publicly release all our code at https://github.com/ AbdulSaleh/dialog-probing, hoping that probing will become a standard method for interpreting and analyzing open-domain dialog systems. 2 Related Work Evaluating and interpreting open-domain dialog models is notoriously challenging. Multiple studies have shown that standard evaluation metrics such as perplexity and BLEU scores (Papineni et al., 2002) correlate very weakly with human judgements of conversation quality (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et al., 2019). This has inspired multiple new approaches for evaluating dialog systems. One popular evaluation metric involves calculating the semantic similarity between the user input and generated response in high-dimensional embedding space (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et al., 2019; Park et al., 2018; Zhao et al., 2017; Xu et al., 2018). Ghandeharioun et al. (2019) proposed calculating conversation metrics such as sentiment and coherence on self-play conversations generated by trained models. Similarly, Dziri et al. (2019) use neural classifiers to identify whether the modelgene"
2020.nlp4convai-1.15,P17-1080,1,0.843058,"put in a natural language inference setting. To the best of our knowledge, all existing approaches for evaluating the performance of opendomain dialog systems only consider external model behavior in the sense that they analyze properties of the generated text. In this study, we explore internal representations instead, motivated by the fact that reasonable internal behavior is crucial for interpretability and is often a prerequisite for effective external behavior. Outside of open-domain dialog, probing has been applied for analyzing natural language processing models in machine translation (Belinkov et al., 2017) and visual question answering (Subramanian et al., 2019). Probing is also commonly used for evaluating the quality of “universal” sentence representations which are trained once and used for a variety of applications (Conneau et al., 2018; Adi et al., 2016) (for example, InferSent (Conneau et al., 2017), SkipThought (Kiros et al., 2015), USE (Cer et al., 2018)). Along the same lines, natural language understanding benchmarks such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) propose a set of diverse tasks for evaluating general linguistic knowledge. Our analysis differs from p"
2020.nlp4convai-1.15,Q19-1004,1,0.783782,"results on a variety of language generation tasks including machine translation (Bahdanau et al., 2014), abstractive summarization (Rush et al., 2015), and text simplification (Wang et al., 2016). However, current generative models for dialog suffer from several shortcomings that limit their usefulness in the real world. Neural models can be opaque and difficult to interpret, posing barriers to their deployment in safety-critical applications such as mental health or customer service ∗ Second author equal contribution. Our code is available at https://github.com/ AbdulSaleh/dialog-probing 1 (Belinkov and Glass, 2019). End-to-end training provides little insight as to what these models learn about engaging in dialog. Open-domain dialog systems also struggle to maintain basic conversations, frequently ignoring user input (Sankar et al., 2019) while generating irrelevant, repetitive, and contradictory responses (Saleh et al., 2019; Li et al., 2016, 2017a; Welleck et al., 2018). Table 1 shows examples from standard dialog models which fail at basic interactions – struggling to answer questions, detect intent, and understand conversational context. In light of these limitations, we aim to answer the following"
2020.nlp4convai-1.15,D16-1127,0,0.0303883,"opaque and difficult to interpret, posing barriers to their deployment in safety-critical applications such as mental health or customer service ∗ Second author equal contribution. Our code is available at https://github.com/ AbdulSaleh/dialog-probing 1 (Belinkov and Glass, 2019). End-to-end training provides little insight as to what these models learn about engaging in dialog. Open-domain dialog systems also struggle to maintain basic conversations, frequently ignoring user input (Sankar et al., 2019) while generating irrelevant, repetitive, and contradictory responses (Saleh et al., 2019; Li et al., 2016, 2017a; Welleck et al., 2018). Table 1 shows examples from standard dialog models which fail at basic interactions – struggling to answer questions, detect intent, and understand conversational context. In light of these limitations, we aim to answer the following questions: (i) Do neural dialog models effectively encode information about the conversation history? (ii) Do neural dialog models learn basic conversational skills through end-to-end training? (iii) And to what extent do neural dialog models leverage the dyadic, turn-taking structure of dialog to learn these skills? To answer these"
2020.nlp4convai-1.15,D18-2029,0,0.0165447,"behavior is crucial for interpretability and is often a prerequisite for effective external behavior. Outside of open-domain dialog, probing has been applied for analyzing natural language processing models in machine translation (Belinkov et al., 2017) and visual question answering (Subramanian et al., 2019). Probing is also commonly used for evaluating the quality of “universal” sentence representations which are trained once and used for a variety of applications (Conneau et al., 2018; Adi et al., 2016) (for example, InferSent (Conneau et al., 2017), SkipThought (Kiros et al., 2015), USE (Cer et al., 2018)). Along the same lines, natural language understanding benchmarks such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) propose a set of diverse tasks for evaluating general linguistic knowledge. Our analysis differs from previous work since it is focused on probing for conversational skills that are particularly relevant to dialog generation. With regard to perturbation experiments, Sankar et al. (2019) found that standard dialog models are largely insensitive to perturbations of the input text. Here we introduce an alternative set of perturbation experiments to similarly explor"
2020.nlp4convai-1.15,D17-1230,0,0.0293516,"similarly explore the extent to which dialog structure is being leveraged by these models. 3 3.1 Methodology Models and Data In this study, we focus on the three most widespread dialog architectures: recurrent neural networks (RNNs) (Sutskever et al., 2014), RNNs with attention (Bahdanau et al., 2014), and Transformers (Vaswani et al., 2017). We use the ParlAI platform (Miller et al., 2017) for building and training the models. We train models of two different sizes and initialization strategies. Small models (≈ 14M parameters) are initialized randomly and trained from scratch on DailyDialog (Li et al., 2017b). Large models (≈ 70M parameters) are 133 Question Answering Inference & Entailment Intent Detection Emotional Understanding [Usr]: What do you do for a living? [Bot]: I’m from south america, lol [Usr]: what’s your favorite food? [Bot]: I love food [Usr]: you like food? [Bot]: no, I don’t [Usr]: Give me music recommendations. [Bot]: I don’t know what I want to do with my life. [Usr]: I feel so miserable [Bot]: I’m afraid it’s too much for me. Table 1: Examples from open-domain dialog systems struggling with simple conversational skills. Figure 1: Probing setup. Dotted arrows emphasize that p"
2020.nlp4convai-1.15,D17-1070,0,0.0157137,"ations instead, motivated by the fact that reasonable internal behavior is crucial for interpretability and is often a prerequisite for effective external behavior. Outside of open-domain dialog, probing has been applied for analyzing natural language processing models in machine translation (Belinkov et al., 2017) and visual question answering (Subramanian et al., 2019). Probing is also commonly used for evaluating the quality of “universal” sentence representations which are trained once and used for a variety of applications (Conneau et al., 2018; Adi et al., 2016) (for example, InferSent (Conneau et al., 2017), SkipThought (Kiros et al., 2015), USE (Cer et al., 2018)). Along the same lines, natural language understanding benchmarks such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) propose a set of diverse tasks for evaluating general linguistic knowledge. Our analysis differs from previous work since it is focused on probing for conversational skills that are particularly relevant to dialog generation. With regard to perturbation experiments, Sankar et al. (2019) found that standard dialog models are largely insensitive to perturbations of the input text. Here we introduce an alter"
2020.nlp4convai-1.15,I17-1099,0,0.0119748,"similarly explore the extent to which dialog structure is being leveraged by these models. 3 3.1 Methodology Models and Data In this study, we focus on the three most widespread dialog architectures: recurrent neural networks (RNNs) (Sutskever et al., 2014), RNNs with attention (Bahdanau et al., 2014), and Transformers (Vaswani et al., 2017). We use the ParlAI platform (Miller et al., 2017) for building and training the models. We train models of two different sizes and initialization strategies. Small models (≈ 14M parameters) are initialized randomly and trained from scratch on DailyDialog (Li et al., 2017b). Large models (≈ 70M parameters) are 133 Question Answering Inference & Entailment Intent Detection Emotional Understanding [Usr]: What do you do for a living? [Bot]: I’m from south america, lol [Usr]: what’s your favorite food? [Bot]: I love food [Usr]: you like food? [Bot]: no, I don’t [Usr]: Give me music recommendations. [Bot]: I don’t know what I want to do with my life. [Usr]: I feel so miserable [Bot]: I’m afraid it’s too much for me. Table 1: Examples from open-domain dialog systems struggling with simple conversational skills. Figure 1: Probing setup. Dotted arrows emphasize that p"
2020.nlp4convai-1.15,D16-1230,0,0.033189,"ntegrates with and extends ParlAI (Miller et al., 2017), a popular open-source platform for building dialog systems. We also publicly release all our code at https://github.com/ AbdulSaleh/dialog-probing, hoping that probing will become a standard method for interpreting and analyzing open-domain dialog systems. 2 Related Work Evaluating and interpreting open-domain dialog models is notoriously challenging. Multiple studies have shown that standard evaluation metrics such as perplexity and BLEU scores (Papineni et al., 2002) correlate very weakly with human judgements of conversation quality (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et al., 2019). This has inspired multiple new approaches for evaluating dialog systems. One popular evaluation metric involves calculating the semantic similarity between the user input and generated response in high-dimensional embedding space (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et al., 2019; Park et al., 2018; Zhao et al., 2017; Xu et al., 2018). Ghandeharioun et al. (2019) proposed calculating conversation metrics such as sentiment and coherence on self-play conversations generated by trained models. Similarly, Dziri et al. (2019) use neu"
2020.nlp4convai-1.15,D15-1166,0,0.166644,"Missing"
2020.nlp4convai-1.15,D17-2014,0,0.0543425,"ode information about the conversation history and the current utterance. In most cases, simply averaging the word embeddings is superior to using the learned encoder representations. This performance gap is smaller for large, pre-trained models. 3. Neural dialog models do not leverage the dyadic, turn-taking nature of conversation. Shuffling conversations in the training data had little impact on perplexity and probing performance. This suggests that breaking the dialog structure did not significantly affect the quality of learned representations. Our code integrates with and extends ParlAI (Miller et al., 2017), a popular open-source platform for building dialog systems. We also publicly release all our code at https://github.com/ AbdulSaleh/dialog-probing, hoping that probing will become a standard method for interpreting and analyzing open-domain dialog systems. 2 Related Work Evaluating and interpreting open-domain dialog models is notoriously challenging. Multiple studies have shown that standard evaluation metrics such as perplexity and BLEU scores (Papineni et al., 2002) correlate very weakly with human judgements of conversation quality (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et"
2020.nlp4convai-1.15,P02-1040,0,0.109982,"og structure did not significantly affect the quality of learned representations. Our code integrates with and extends ParlAI (Miller et al., 2017), a popular open-source platform for building dialog systems. We also publicly release all our code at https://github.com/ AbdulSaleh/dialog-probing, hoping that probing will become a standard method for interpreting and analyzing open-domain dialog systems. 2 Related Work Evaluating and interpreting open-domain dialog models is notoriously challenging. Multiple studies have shown that standard evaluation metrics such as perplexity and BLEU scores (Papineni et al., 2002) correlate very weakly with human judgements of conversation quality (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et al., 2019). This has inspired multiple new approaches for evaluating dialog systems. One popular evaluation metric involves calculating the semantic similarity between the user input and generated response in high-dimensional embedding space (Liu et al., 2016; Ghandeharioun et al., 2019; Dziri et al., 2019; Park et al., 2018; Zhao et al., 2017; Xu et al., 2018). Ghandeharioun et al. (2019) proposed calculating conversation metrics such as sentiment and coherence on self-"
2020.nlp4convai-1.15,N18-1162,0,0.0422419,"Missing"
2020.nlp4convai-1.15,D14-1162,0,0.0858223,"g to the previous utterances, [u1 , . . . , ut−1 ], and then we separately averaged the encoder hidden states corresponding to the current utterance, ut , and concatenated the two resulting, equal-length vectors. We also concatenated the last cell state. Similarly, for Transformers, we averaged the encoder outputs corresponding to the previous utterances and separately averaged encoder outputs corresponding to the current utterance and concatenated them. Combined: The combined representations are the concatenation of of the word embeddings and encoder state representations. We also use GloVe (Pennington et al., 2014) word embeddings as a simple baseline. We encode the probing task inputs using the word embeddings approach described above. We ensure that GloVe and all models of a certain size (small vs large) share the same vocabulary for comparability. 3.3 MultiWOZ: Every utterance in a conversation can be considered as an action or a dialog act performed by the speaker. A speaker could be making a request, providing information, or simply greeting the system. MultiWOZ 2.1 (Eric et al., 2019) is a dataset of multi-domain, goal-oriented conversations. Human turns are labeled with dialog acts and the associ"
2020.nlp4convai-1.15,D15-1044,0,0.134145,"Missing"
2020.nlp4convai-1.15,P19-1004,0,0.0540052,"Missing"
2020.nlp4convai-1.15,W18-5446,0,0.0600171,"Missing"
2021.acl-long.144,2020.acl-main.493,0,0.0145923,"el, but does not inform one of how the model does this or which components are responsible for the observed behavior. 2.2 Probing A separate line of analysis work has investigated representations associated with syntactic dependencies by defining a family of functions (probes) that map from model representations to some phenomenon that those representations are expected to encode. For instance, several studies have mapped LM representations to either independent syntactic dependencies (Belinkov, 2018; Liu et al., 2019; Tenney et al., 2019b) or full dependency parses (Hewitt and Manning, 2019; Chi et al., 2020) as a proxy for discovering latent syntactic knowledge within the model. Most related, Giulianelli et al. (2018) use probes to investigate how LSTMs handle agreement. Probing is more difficult to interpret than behavioral approaches because the addition of a trained classifier introduces confounds (Hewitt and Liang, 2019): most notably, whether the probe maps from model representations to the desired output, or learns the task itself. Probes also only give correlational evidence, rather than causal evidence (Belinkov and Glass, 2019). See Belinkov (2021) for a review of the shortcomings of pro"
2021.acl-long.144,P19-1285,0,0.0713695,"Missing"
2021.acl-long.144,N19-1112,1,0.815376,"inflection given the same context. This approach investigates the output behavior of the model, but does not inform one of how the model does this or which components are responsible for the observed behavior. 2.2 Probing A separate line of analysis work has investigated representations associated with syntactic dependencies by defining a family of functions (probes) that map from model representations to some phenomenon that those representations are expected to encode. For instance, several studies have mapped LM representations to either independent syntactic dependencies (Belinkov, 2018; Liu et al., 2019; Tenney et al., 2019b) or full dependency parses (Hewitt and Manning, 2019; Chi et al., 2020) as a proxy for discovering latent syntactic knowledge within the model. Most related, Giulianelli et al. (2018) use probes to investigate how LSTMs handle agreement. Probing is more difficult to interpret than behavioral approaches because the addition of a trained classifier introduces confounds (Hewitt and Liang, 2019): most notably, whether the probe maps from model representations to the desired output, or learns the task itself. Probes also only give correlational evidence, rather than causal ev"
2021.acl-long.144,N19-1002,0,0.252743,"example, which demonstrates subject-verb agreement across an agreement attractor. Here, a model using a linear ∗ Equal contribution. Work done while visiting Google Research. ‡ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. † While we have a reasonable understanding of the generally correct behavior of LMs in such contexts, the mechanisms that underlie models’ sensitivity to syntactic agreement are still not well understood. Recent work has performed causal analyses of syntactic agreement units in LSTM (Hochreiter and Schmidhuber, 1997)-based LMs (Lakretz et al., 2019; Lu et al., 2020) or causal analyses of LSTM hidden representations’ impact on syntactic agreement (Giulianelli et al., 2018), but the agreement mechanisms of Transformer-based LMs have not been as extensively investigated. Transformerbased LMs’ syntactic generalization abilities are superior to those of LSTMs (Hu et al., 2020), which makes Transformer-based models enticing candidates for further analysis. We apply the behavioral-structural method of causal mediation analysis (Pearl, 2001) to investigate syntactic agreement in Transformers, following the approach used by Vig et al. (2020a) fo"
2021.acl-long.144,D18-1151,1,0.857953,"d that language models rely on similar sets of neurons when given sentences with similar syntactic structure. 1 (1) The key to the cabinets is/*are next to the coins. Introduction Targeted syntactic evaluations have shown that neural language models (LMs) are able to predict the correct token from a set of grammatically minimally different continuations with high accuracy, even in difficult contexts (Linzen et al., 2016; Gulordava et al., 2018), for constructions such as subject-verb agreement (van Schijndel et al., 2019), filler-gap dependencies (Wilcox et al., 2018), and reflexive anaphora (Marvin and Linzen, 2018). As an illustration of the targeted syntactic evaluation paradigm, consider the following example, which demonstrates subject-verb agreement across an agreement attractor. Here, a model using a linear ∗ Equal contribution. Work done while visiting Google Research. ‡ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. † While we have a reasonable understanding of the generally correct behavior of LMs in such contexts, the mechanisms that underlie models’ sensitivity to syntactic agreement are still not well understood. Recent work has performed causal an"
2021.acl-long.144,2020.acl-main.490,1,0.768172,"well with human intuitions of syntactic similarity between structures. 2 2.1 Related Work Targeted Syntactic Evaluation Many recent studies have treated neural LMs and contextualized word prediction models—primarily LSTM LMs (Sundermeyer et al., 2012), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019)—as psycholinguistic subjects to be studied behaviorally (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019). Some have studied whether models prefer grammatical completions in subjectverb agreement contexts (Marvin and Linzen, 2018; van Schijndel et al., 2019; Goldberg, 2019; Mueller et al., 2020; Lakretz et al., 2021; Futrell et al., 2019), as well as in filler-gap dependencies (Wilcox et al., 2018, 2019). These are based on the approach of Linzen et al. (2016), where a model’s ability to syntactically generalize is measured by its ability to choose the correct inflection in difficult structural contexts instantiated by tokens that the model has not seen together during training. In other words, this approach tests whether the model assigns the correct inflection a higher probability than an incorrect inflection given the same context. This approach investigates the output behavior o"
2021.acl-long.144,2021.naacl-main.290,0,0.031086,"sal mediation analysis to discover and interpret the mechanisms behind syntactic agreement in pre-trained neural language models. Our results reveal the location and importance of various neurons within various models, and provide insights into the inner workings of these LMs. For future work, we suggest intervening on groups of neurons and attention heads to see how these components work together, and extending the analysis to phenomena such as filler-gap dependencies and negative polarity items. Further work should also explore the impact of specific verbs on syntactic agreement mechanisms (Newman et al., 2021). Lastly, we suggest examining examples where the model makes incorrect predictions to determine how models misuse the mechanisms from Section 6.1. Acknowledgements Y.B. was supported in part by the ISRAEL SCIENCE FOUNDATION (grant no. 448/20) and by an Azrieli Foundation Early Career Faculty Fellowship. A.M. was supported by a National Science Foundation Graduate Research Fellowship (grant no. 1746891). 1836 Impact Statement chine Translation and Speech Recognition. Ph.D. thesis, Massachusetts Institute of Technology. In this paper, we apply causal mediation analysis in order to study the sub"
2021.acl-long.144,D19-1592,1,0.931743,"ment in Transformers, following the approach used by Vig et al. (2020a) for interpreting gender bias in pre-trained English LMs. This method allows us to implicate specific model components in the observed behavior of a model. If we view a neural LM as a causal graph proceeding from inputs to outputs, we can view each model component (e.g., a neuron) as a mediator. We measure the contribution of a mediator to the observed output behavior by performing controlled interventions on input sentences and observing how they change the probabilities of continuation pairs. We focus primarily on GPT-2 (Radford et al., 2019), although we also analyze TransformerXL (Dai et al., 1828 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1828–1843 August 1–6, 2021. ©2021 Association for Computational Linguistics 2019) and XLNet (Yang et al., 2019). We find that both GPT-2 and Transformer-XL use two distinct mechanisms to accomplish subjectverb agreement, one of which is active only when the subject and verb are adjacent. Conversely, XLNet uses one unified mechanism across syntactic structures. Even tho"
2021.acl-long.144,2021.emnlp-main.230,0,0.0821816,"Missing"
2021.acl-long.144,P19-1452,0,0.0617523,"the same context. This approach investigates the output behavior of the model, but does not inform one of how the model does this or which components are responsible for the observed behavior. 2.2 Probing A separate line of analysis work has investigated representations associated with syntactic dependencies by defining a family of functions (probes) that map from model representations to some phenomenon that those representations are expected to encode. For instance, several studies have mapped LM representations to either independent syntactic dependencies (Belinkov, 2018; Liu et al., 2019; Tenney et al., 2019b) or full dependency parses (Hewitt and Manning, 2019; Chi et al., 2020) as a proxy for discovering latent syntactic knowledge within the model. Most related, Giulianelli et al. (2018) use probes to investigate how LSTMs handle agreement. Probing is more difficult to interpret than behavioral approaches because the addition of a trained classifier introduces confounds (Hewitt and Liang, 2019): most notably, whether the probe maps from model representations to the desired output, or learns the task itself. Probes also only give correlational evidence, rather than causal evidence (Belinkov and"
2021.acl-long.144,W18-5423,0,0.0872603,"ructure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure. 1 (1) The key to the cabinets is/*are next to the coins. Introduction Targeted syntactic evaluations have shown that neural language models (LMs) are able to predict the correct token from a set of grammatically minimally different continuations with high accuracy, even in difficult contexts (Linzen et al., 2016; Gulordava et al., 2018), for constructions such as subject-verb agreement (van Schijndel et al., 2019), filler-gap dependencies (Wilcox et al., 2018), and reflexive anaphora (Marvin and Linzen, 2018). As an illustration of the targeted syntactic evaluation paradigm, consider the following example, which demonstrates subject-verb agreement across an agreement attractor. Here, a model using a linear ∗ Equal contribution. Work done while visiting Google Research. ‡ Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. † While we have a reasonable understanding of the generally correct behavior of LMs in such contexts, the mechanisms that underlie models’ sensitivity to syntactic agreement are still not we"
2021.acl-long.144,N19-1334,0,0.039094,"Missing"
C02-2025,J97-4005,0,0.0188522,"d information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these pr"
C02-2025,W97-1502,0,0.122641,"nks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relatively easy to judge. All disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the annotated profile or (ii) dynamic prop"
C02-2025,P97-1003,0,0.0128863,"t the simplest end, we might look only at the lexical type sequence assigned to the words by each parse and rank the parse based on the likelihood of that sequence. These lexical types – the preterminals in the derivation – are essentially part-of-speech tags, but encode considerably finer-grained information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). The"
C02-2025,P01-1019,1,0.256862,"with an enhanced version of the grammar in an automated fashion, viz. by re-applying the disambiguating decisions on the corpus with an updated version of the grammar. Depth of Representation and Transformation of Information Internally, the [incr tsdb()] database records analyses in three different formats, viz. (i) as a derivation tree composed of identifiers of lexical items and constructions used to build the analysis, (ii) as a traditional phrase structure tree labeled with an inventory of some fifty atomic labels (of the type ‘S’, ‘NP’, ‘VP’ et al.), and (iii) as an underspecified MRS (Copestake, Lascarides, & Flickinger, 2001) meaning representation. While representation (ii) will in many cases be similar to the representation found in the Penn Treebank, representation (iii) subsumes the functor – argument (or tectogrammatical) structure advocated in the Prague Dependency Treebank or the German TiGer corpus. Most importantly, however, representation (i) provides all the information required to replay the full HPSG analysis (using the original grammar and one of the open-source HPSG processing environments, e.g., the LKB or PET, which already have been interfaced to [incr tsdb()]). Using the latter approach, users"
C02-2025,W00-1908,0,0.0559015,"Missing"
C02-2025,P99-1069,0,0.0373806,"including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection perfo"
C02-2025,2000.iwpt-1.19,1,0.696136,"SG framework and a generally-available broad-coverage grammar of English, the LinGO English Resource Grammar (Flickinger, 2000) as implemented with the LKB grammar development environment (Copestake, 2002). Unlike existing treebanks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relat"
C02-2025,W02-2030,1,0.780762,"ermining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection performance as percentage of test sentences for which the correct parse was highest ranked by the model. (We restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.) We examine four models: an HMM tagging model, a simple PCFG, a PCFG with grandparent annotation, and a hybrid model that combines predictions from the PCFG and the tagger. Thes"
C86-1050,P84-1075,1,0.908562,"neralized Phrase Structure Grammar [2] are impressive, in part because of the complexity of the axiomatic system developed by the authors. In this paper, we examine the possibility that simpler descriptions of the same theory can be achieved through a slightly different, albeit still axiomatic, method. Rather than characterize the well-formed trees directly, we progress in two stages by procedurally characterizing the well-formedness axioms themselves, which in turn characterize the trees. in a unification-b~qed formalism, the PATR-II formalism developed at SRI International (henceforth PATR) [5], which h~s its own declarative semmltics, and which can therefore be viewed &s an axiomatization of string well-formedness constraints. 2 The characterization of GPSG thus obtained is simpler and better defined than the version described by Gazdar et al. The semantics of the formalism is given directly through the reduction to PATR. Also, the PATR axiomatization has a clear construetire interpretation, unlike that used in Gazdar et al., thus making the system more amenable to computational implementation. Finally, the characteristics of the coml~ilation--the difficulty or ease with which the"
C86-1050,P85-1018,1,\N,Missing
C86-1050,P84-1027,1,\N,Missing
C88-2128,J85-4002,0,0.0200647,"Missing"
C88-2128,C67-1009,0,0.36013,"Missing"
C88-2128,P83-1021,0,0.690783,"Missing"
C88-2128,P83-1017,1,0.846086,"Missing"
C88-2128,P85-1018,1,0.859989,"Missing"
C88-2128,E83-1016,0,0.0489593,"Missing"
C88-2128,T87-1042,0,\N,Missing
C88-2128,T75-1004,0,\N,Missing
C90-3045,J87-1005,1,0.295617,"Missing"
C90-3045,P88-1032,1,0.741633,"Missing"
C90-3045,C88-2121,1,0.346774,"Missing"
C90-3045,C88-2128,1,0.185062,"one-to-one mapping between the source and target derivation. When partial source derivations are recognized by the parser, the corresponding partial target derivation (for example semantic inteq)retation) can be incrementally compuled: as the input is read from left to right, interpretations of the partial target derivations corresponding to partial source derivations can be combined in one step to buikl a larger partial target derivation. In previous work, one of us noted that generation according to an augmented context-free grammar can be made more efficient by requiring the grammar to be (Shieber, 1988); the derived semantics for an expression must include, in an appropriate sense, the semantic material of all its subconstituents. It is interesting to note that synchronous ""FAGs are inherently semantically monotonic. Furthermore, it is reasonable to require that the semantic component of a synchronous TAG t~ (in the sense of Schabes et manticallymonotonic selexicalized 5 257 When the synchronous TAG is order-sensitive, however, there may be a many-to-many correspondence between source derivations and target derivations. This is the case, for instance, in a grammar in which alternative quanti"
C90-3045,P85-1011,0,0.0107604,"Missing"
C90-3045,H86-1020,0,\N,Missing
C90-3045,C90-3001,1,\N,Missing
C90-3045,E89-1001,1,\N,Missing
C90-3045,P89-1027,0,\N,Missing
D16-1255,J90-2002,0,0.547167,"Missing"
D16-1255,E14-1028,0,0.382265,"Missing"
D16-1255,E14-3010,0,0.158376,"Missing"
D16-1255,J10-4005,0,0.0688143,"Missing"
D16-1255,D15-1043,0,0.242476,"Missing"
D16-1255,N15-1012,0,0.251442,"Missing"
D16-1255,J93-2004,0,0.0549901,"Missing"
D16-1255,W12-3018,0,0.0346934,"Missing"
D16-1255,P02-1040,0,0.0966085,"Missing"
D16-1255,D11-1106,0,0.339341,"Missing"
D16-1255,J15-3005,0,0.10736,"Missing"
D16-1255,E12-1075,0,0.350454,"Missing"
D16-1255,C92-2092,0,\N,Missing
D17-1239,D10-1049,0,0.028878,"2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation"
D17-1239,H05-1042,0,0.899964,"enial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to evaluate generations in terms of their fidelity to the database. One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a long history of NLG work that generates sports game 2253 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005). To this end, we make the following contributions: • We introduce a new large-scale corpus consisting of textual descriptions of basketball games paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient. • We introduce a series of extractive evaluation models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself. • We apply a series of state-of-the-art neural methods, as well as a simple templated generation system"
D17-1239,K16-1002,0,0.0220758,"Missing"
D17-1239,P00-1037,0,0.25372,"Missing"
D17-1239,P17-4012,1,0.161462,"ets 49 49 , giving them just enough of an advantage to secure the victory in front of their home crowd . The Jazz were led by the duo of Derrick Favors and James Harden . Favors went 2 - for - 6 from the field and 0 - for - 1 from the three - point line to score a game - high of 15 points , while also adding four rebounds and four assists .... The <team1>’ next game will be at home against the Dallas Mavericks, while the <team2> will travel to play the Bulls. Code implementing all models can be found at https://github.com/harvardnlp/ data2text. Our encoder-decoder models are based on OpenNMT (Klein et al., 2017). 6 Results We found that all models performed quite poorly on the SBNATION data, with the best model achieving a validation perplexity of 33.34 and a BLEU score of 1.78. This poor performance is presumably attributable to the noisy quality of the SBNATION data, and the fact that many documents in the dataset focus on information not in the box- and line-scores. Accordingly, we focus on ROTOW IRE in what follows. The main results for the ROTOW IRE dataset are shown in Table 2, which shows the performance of the models in Section 4 in terms of the metrics defined in Section 3.2, as well as in t"
D17-1239,E17-1060,0,0.0174651,"ng (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation by introducing a new dataset, and proposing various metrics for automatically evaluating content selection, generation, and ordering. We see that recent ideas in copying and reconstruction lead to improvements on this task, but that there is a significant gap even between these neural models and templa"
D17-1239,P83-1022,0,0.289198,"ard generating longer outputs in response to longer and more complicated inputs, however, the generated texts begin to display reference errors, intersentence incoherence, and a lack of fidelity to the source material. The goal of this paper is to suggest a particular, long-form generation task in which these challenges may be fruitfully explored, to provide a publically available dataset for this task, to suggest some automatic evaluation metrics, and finally to establish how current, neural text generation methods perform on this task. A classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output. Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation (Reiter and Dale, 1997; Jurafsky and Martin, 2014). Traditionally, these two challeng"
D17-1239,W14-4012,0,0.111212,"Missing"
D17-1239,D16-1128,0,0.120481,"Missing"
D17-1239,W03-1016,0,0.0314109,"Rockets’ rebounds could manifest in a lower CO score, and incorrectly indicating the win/loss records is a CS error. 7 Related Work In this section we note additional related work not noted throughout. Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on"
D17-1239,P16-1154,0,0.518854,"are defined with respect to the 2 DLD is a variant of Levenshtein distance that allows transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same set (which is a requirement for measures like Kendall’s Tau). 2256 predictions of an information extraction system. Accordingly, our metrics are quite interpretable, since by construction it is always possible to determine which fact (i.e., entity-value pair) in the generation is determined by the extractor to not match the database or the gold generation. 4 Joint Copy Model The models of Gu et al. (2016) and Yang et al. (2016) parameterize the joint distribution table over yˆt and zt directly: p(ˆ yt , zt |yˆ1:t−1 , s) ∝   yt , yˆ1:t−1 , s) copy(ˆ 0   gen(ˆ yt , yˆ1:t−1 , s) Neural Data-to-Document Models In this section we briefly describe the neural generation methods we apply to the proposed task. As a base model we utilize the now standard attentionbased encoder-decoder model (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). We also experiment with several recent extensions to this model, including copy-based generation, and training with a source reconstruction term"
D17-1239,P16-1014,0,0.0223001,"Missing"
D17-1239,P16-1002,0,0.0168363,"n represented as s r j }Jj=1 . ˜, we use an LSTM decoder with attenGiven s tion and input-feeding, in the style of Luong et al. (2015), to compute the probability of each target word, conditioned on the previous words and on s. The model is trained end-to-end to minimize the negative log-likelihood of the words in the gold text y1:T given corresponding source material s. Copying There has been a surge of recent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G¨ulc¸ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016). These models typically introduce an additional binary variable zt into the per-timestep target word distribution, which indicates whether the target word yˆt is copied from the source or generated: p(ˆ yt |yˆ1:t−1 , s) = X p(ˆ yt , zt = z |yˆ1:t−1 , s). z∈{0,1} In our case, we assume that target words are copied from the value portion of a record r; that is, a copy implies yˆt = r.m for some r and t. 3 We also include an additional feature for whether the player is on the home- or away-team. zt = 1, yˆt ∈ s zt = 1, yˆt 6∈ s zt = 0, where copy and gen are functions paramet"
D17-1239,C10-2062,0,0.0918002,"research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation by introducing a new d"
D17-1239,D14-1181,0,0.00296209,"he scores used in copy or pcopy . We train the generation models using SGD and truncated BPTT (Elman, 1990; Mikolov et al., 2010), as in language modeling. That is, we split each y1:T into contiguous blocks of length 100, and backprop both the gradients with respect to the current block as well as with respect to the encoder parameters for each block. Our extractive evaluator consists of an ensemble of 3 single-layer convolutional and 3 singlelayer bidirectional LSTM models. The convolutional models concatenate convolutions with kernel widths 2, 3, and 5, and 200 feature maps in the style of (Kim, 2014). Both models are trained with SGD. Templatized Generator In addition to neural baselines, we also use a problem-specific, template-based generator. The template-based generator first emits a sentence about the teams playing in the game, using a templatized sentence taken from the training set: The <team1> (<wins1>-<losses1>) delog pk (r.x |bi ; θ), feated the <team2> (<wins2>-<losses2>) x∈{e,m,t} <pts1>-<pts2>. where pk is the k’th predicted distribution over records, and where we have modeled each component of r independently. This loss attempts to make the most probable record in s given bi"
D17-1239,D17-1230,0,0.0221655,"Damerau-Levenshtein Distance (Brill and Moore, 2000)2 between the sequences of records extracted from y1:T and that extracted from yˆ1:T . This measures how well the system orders the records it chooses to discuss. We note that CS primarily targets the “what to say” aspect of evaluation, CO targets the “how to say it” aspect, and RG targets both. We conclude this section by contrasting the automatic evaluation we have proposed with recently proposed adversarial evaluation approaches, which also advocate automatic metrics backed by classification (Bowman et al., 2016; Kannan and Vinyals, 2016; Li et al., 2017). Unlike adversarial evaluation, which uses a blackbox classifier to determine the quality of a generation, our metrics are defined with respect to the 2 DLD is a variant of Levenshtein distance that allows transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same set (which is a requirement for measures like Kendall’s Tau). 2256 predictions of an information extraction system. Accordingly, our metrics are quite interpretable, since by construction it is always possible to determine which fact (i.e., entity-value pair) in the gener"
D17-1239,P09-1011,0,0.703039,"improvements in BLEU and in our proposed extractive evaluations, current models are still quite far from producing human-level output, and are significantly worse than templated systems in terms of content selection and realization. Overall, we believe this problem of data-to-document generation highlights important remaining challenges in neural generation systems, and the use of extractive evaluation reveals significant issues hidden by standard automatic metrics. 2 Data-to-Text Datasets We consider the problem of generating descriptive text from database records. Following the notation in Liang et al. (2009), let s = {rj }Jj=1 be a set of records, where for each r ∈ s we define r.t ∈ T to be the type of r, and we assume each r to be a binarized relation, where r.e and r.m are a record’s entity and value, respectively. For example, a database recording statistics for a basketball game might have a record r such that r.t = POINTS, r.e = RUSSELL W ESTBROOK, and r.m = 50. In this case, r.e gives the player in question, and r.m gives the number of points the player scored. From these records, we are interested in generating descriptive text, yˆ1:T = yˆ1 , . . . , yˆT of T words such that yˆ1:T is an a"
D17-1239,D16-1230,0,0.0225098,"are typically evaluated using a combination of automatic measures, such as BLEU (Papineni et al., 2002), and human evaluation. While BLEU is perhaps a reasonably effective way of evaluating short-form text generation, we found it to be unsatisfactory for document generation. In particular, we note that it primarily rewards fluent text generation, rather than generations that capture the most important information in the database, or that report the information in a particularly coherent way. While human evaluation, on the other hand, is likely ultimately necessary for evaluating generations (Liu et al., 2016; Wu et al., 2016), it is much less convenient than using automatic metrics. Furthermore, we believe that current text generations are sufficiently bad in sufficiently obvious ways that automatic metrics can still be of use in evaluation, and we are not yet at the point of needing to rely solely on human evaluators. 3.1 Extractive Evaluation To address this evaluation challenge, we begin with the intuition that assessing document quality is easier than document generation. In particular, it is much easier to automatically extract information from documents than to generate documents that accur"
D17-1239,D11-1149,0,0.0176997,"on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems. 8 Conclusion and Future Work This work explores the challenges facing neural data-to-document generation by introducing a new dataset, and propo"
D17-1239,D15-1166,0,0.0514046,"2015). We also experiment with several recent extensions to this model, including copy-based generation, and training with a source reconstruction term in the loss (in addition to the standard per-target-word loss). Base Model For our base model, we map each record r ∈ s into a vector r˜ by first embedding r.t (e.g., POINTS), r.e (e.g., RUSSELL W ESTBROOK), and r.m (e.g., 50), and then applying a 1-layer MLP (similar to Yang et al. (2016)).3 Our source ˜ = {˜ data-records are then represented as s r j }Jj=1 . ˜, we use an LSTM decoder with attenGiven s tion and input-feeding, in the style of Luong et al. (2015), to compute the probability of each target word, conditioned on the previous words and on s. The model is trained end-to-end to minimize the negative log-likelihood of the words in the gold text y1:T given corresponding source material s. Copying There has been a surge of recent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G¨ulc¸ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016). These models typically introduce an additional binary variable zt into the per-timestep targe"
D17-1239,N16-1086,0,0.603298,"d in generating descriptive text, yˆ1:T = yˆ1 , . . . , yˆT of T words such that yˆ1:T is an adequate and fluent summary of s. A dataset for training data-to-document systems typically consists of (s, y1:T ) pairs, where y1:T is a document consisting of a gold (i.e., human generated) summary for database s. Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being W EATHER G OV (Liang et al., 2009) and ROBOCUP (Chen and Mooney, 2008). Recently, neural generation systems have show strong results on these datasets, with the system of Mei et al. (2016) achieving BLEU scores in the 60s and 70s on W EATHER G OV, and BLEU scores of almost 30 even on the smaller ROBOCUP dataset. These results are quite promising, and suggest that neural models are a good fit for text generation. However, the statistics of these datasets, shown in Table 1, indicate that these datasets use relatively simple language and record structure. Furthermore, there is reason to believe that W EATHER G OV is at least partially machinegenerated (Reiter, 2017). More recently, Lebret et al. (2016) introduced the W IKI B IO dataset, which is at least an order of magnitude larg"
D17-1239,P02-1040,0,0.110284,"aset is significantly larger, but also much more challenging, as the language is very informal, and often tangential to the statistics themselves. We show some sample text from ROTOW IRE in Figure 1. Our primary focus will be on the RO TOW IRE data. 3 Evaluating Document Generation We begin by discussing the evaluation of generated documents, since both the task we introduce and the evaluation methods we propose are motivated by some of the shortcomings of current approaches to evaluation. Text generation systems are typically evaluated using a combination of automatic measures, such as BLEU (Papineni et al., 2002), and human evaluation. While BLEU is perhaps a reasonably effective way of evaluating short-form text generation, we found it to be unsatisfactory for document generation. In particular, we note that it primarily rewards fluent text generation, rather than generations that capture the most important information in the database, or that report the information in a particularly coherent way. While human evaluation, on the other hand, is likely ultimately necessary for evaluating generations (Liu et al., 2016; Wu et al., 2016), it is much less convenient than using automatic metrics. Furthermore"
D17-1239,P15-1061,0,0.0110588,"ity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (“Miami Heat”, “95”) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (M IAMI H EAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015). More concretely, given a document yˆ1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t |e, m; θ) for each pair, using r.t =  to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix. Importantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the information extraction model presented above, since"
D17-1239,P06-1139,0,0.104479,"and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the"
D17-1239,N07-1022,0,0.0118997,"has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard"
D17-1239,1983.tc-1.13,0,0.327159,"Missing"
D17-1239,C14-1220,0,0.011714,"t extract candidate entity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (“Miami Heat”, “95”) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (M IAMI H EAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015). More concretely, given a document yˆ1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t |e, m; θ) for each pair, using r.t =  to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix. Importantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the information extraction mo"
D17-1239,I08-1005,0,0.00919431,"gure 1. We may first extract candidate entity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (“Miami Heat”, “95”) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (M IAMI H EAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015). More concretely, given a document yˆ1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t |e, m; θ) for each pair, using r.t =  to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix. Importantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the inform"
D17-1239,P98-2209,0,0.262177,"t the task is somewhat congenial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to evaluate generations in terms of their fidelity to the database. One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a long history of NLG work that generates sports game 2253 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005). To this end, we make the following contributions: • We introduce a new large-scale corpus consisting of textual descriptions of basketball games paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient. • We introduce a series of extractive evaluation models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself. • We apply a series of state-of-the-art neural methods, as well as a simple"
D17-1239,2007.mtsummit-ucnlg.4,0,0.039326,"of sports games has been a topic of interest for almost as long (Robin, 1994; TanakaIshii et al., 1998; Barzilay and Lapata, 2005). Historically, research has focused on both content selection (“what to say”) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (“how to say it”) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoderdecoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challe"
D17-1298,D16-1195,0,0.0113331,"only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary classification task of grammatical error identification. Experiments demo"
D17-1298,N12-1067,0,0.434547,"Missing"
D17-1298,W13-1703,0,0.20543,"ntences as the dev set3 (of which 53,502 contain edits). The test set contains 146,478 sentences. The primary focus of the present study is conducting controlled experiments on the AESW dataset, but we also investigate results on the CoNLL-2014 shared task data in light of recent neural results (Ji et al., 2017) and to serve as a baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al., 2016). We use the common sets of public data appearing in past work for training: the National University of Singapore (NUS) Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the publicly available Lang-8 so document-level context is not available. 2 Characteristics of the dataset preclude experiments with additional paragraph context features. (See Appendix A.) 3 The dev set contains 13,562 unique deletion types, 29,952 insertion types, and 39,930 replacement types. data (Tajiri et al., 2012; Mizumoto et al., 2012). The Lang-8 dataset of corrections is large4 but is crowd-sourced5 and is thus of a different nature than the professionally annotated AESW and NUCLE datasets. We use the revised CoNLL2013 test set as a tuning/dev set and the CoNLL2014 test set (wi"
D17-1298,W12-2006,0,0.0185518,"ints. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-"
D17-1298,W11-2838,0,0.0450534,"ata, by 6 M 2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific featur"
D17-1298,W16-0506,0,0.0201803,"42.78 41.21 44.62 W ORD + DOM W ORD +B I + DOM C HAR CNN+B I + DOM C HAR CNN+ DOM C HAR +B I + DOM 91.25 91.45 91.15 91.35 91.64 − − − − 91.39 43.12 44.33 40.79 43.94 47.25 − − − − 46.72 Table 1: AESW development/test set correction results. GLEU and M 2 differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.05 level, resampling the full set 50 times. models we propose modeling the input and output sequences with a special initial token representing the journal domain (+ DOM).2 3 Experiments Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task. The training set contains 1,182,491 sentences, of which 460,901 sentences have edits. We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 contain edits), and use the remaining 137,446 sentences as the dev set3 (of which 53,502 contain edits). The test set contains 146,478 sentences. The primary focus of the present study is conducting controlled experiments on the AESW dataset, but we also investigate results on the"
D17-1298,W14-3333,0,0.019636,"DIFFS +M 2 SMT– DIFFS +BLEU W ORD +B I – DIFFS C HAR +B I – DIFFS 90.44 90.90 91.18 91.28 − − − − 38.55 37.66 38.88 40.11 − − − − SMT+BLEU W ORD +B I C HAR CNN C HAR +B I 90.95 91.34 91.23 91.46 90.70 91.05 90.96 91.22 38.99 43.61 42.02 44.67 38.31 42.78 41.21 44.62 W ORD + DOM W ORD +B I + DOM C HAR CNN+B I + DOM C HAR CNN+ DOM C HAR +B I + DOM 91.25 91.45 91.15 91.35 91.64 − − − − 91.39 43.12 44.33 40.79 43.94 47.25 − − − − 46.72 Table 1: AESW development/test set correction results. GLEU and M 2 differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.05 level, resampling the full set 50 times. models we propose modeling the input and output sequences with a special initial token representing the journal domain (+ DOM).2 3 Experiments Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task. The training set contains 1,182,491 sentences, of which 460,901 sentences have edits. We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 contain edits), and use th"
D17-1298,P17-1070,0,0.41752,"s by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary clas"
D17-1298,D16-1161,0,0.625423,"4). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million se"
D17-1298,W04-3250,0,0.0652137,"0 00.00 SMT– DIFFS +M 2 SMT– DIFFS +BLEU W ORD +B I – DIFFS C HAR +B I – DIFFS 90.44 90.90 91.18 91.28 − − − − 38.55 37.66 38.88 40.11 − − − − SMT+BLEU W ORD +B I C HAR CNN C HAR +B I 90.95 91.34 91.23 91.46 90.70 91.05 90.96 91.22 38.99 43.61 42.02 44.67 38.31 42.78 41.21 44.62 W ORD + DOM W ORD +B I + DOM C HAR CNN+B I + DOM C HAR CNN+ DOM C HAR +B I + DOM 91.25 91.45 91.15 91.35 91.64 − − − − 91.39 43.12 44.33 40.79 43.94 47.25 − − − − 46.72 Table 1: AESW development/test set correction results. GLEU and M 2 differences on test are statistically significant via paired bootstrap resampling (Koehn, 2004; Graham et al., 2014) at the 0.05 level, resampling the full set 50 times. models we propose modeling the input and output sequences with a special initial token representing the journal domain (+ DOM).2 3 Experiments Data AESW (Daudaravicius, 2016; Daudaravicius et al., 2016) consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task. The training set contains 1,182,491 sentences, of which 460,901 sentences have edits. We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 cont"
D17-1298,D15-1166,0,0.0927899,"Missing"
D17-1298,C12-2084,0,0.46323,"Missing"
D17-1298,W14-1701,0,0.28744,"r or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe"
D17-1298,W13-3601,0,0.200026,"in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches. 1 Introduction The task of sentence correction is to convert a natural language sentence that may or may not have errors into a corrected version. The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014). Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the t"
D17-1298,P16-1208,0,0.121171,"addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously proposed for the distinct binary classification task of grammatical error identification. Experiments demonstrate that pure characterlevel sequence-to-sequence models are more effective on AESW than word-based models and models that encode subword information via convolutions over characters, and t"
D17-1298,W16-0528,1,0.83144,"rate that on a large, professionally annotated dataset, the most effective sequence-to-sequence approach can significantly outperform a state-of-the-art SMT system without augmenting the sequence-to-sequence model with a secondary model to handle lowfrequency words (Yuan and Briscoe, 2016) or an additional model to improve precision or intersecting a large language model (Xie et al., 2016). We also demonstrate improvements over these previous sequence-to-sequence approaches on the CoNLL-2014 data and competitive results with Ji et al. (2017), despite using significantly less data. The work of Schmaltz et al. (2016) applies W ORD and C HAR CNN models to the distinct binary classification task of error identification. Additional Approaches The standard formulation of the correction task is to model the output sequence as t above. Here, we also propose modeling the diffs between s and t. The diffs are provided in-line within t and are described via tags marking the starts and ends of insertions and deletions, with replacements represented as deletioninsertion pairs, as in the following example selected from the training set: “Some key points are worth &lt;del&gt; emphasiz &lt;/del&gt; &lt;ins&gt; emphasizing &lt;/ins&gt; .”. Here"
D17-1298,P12-2039,0,0.145359,"baseline of comparison against existing sequenceto-sequence approaches (Yuan and Briscoe, 2016; Xie et al., 2016). We use the common sets of public data appearing in past work for training: the National University of Singapore (NUS) Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) and the publicly available Lang-8 so document-level context is not available. 2 Characteristics of the dataset preclude experiments with additional paragraph context features. (See Appendix A.) 3 The dev set contains 13,562 unique deletion types, 29,952 insertion types, and 39,930 replacement types. data (Tajiri et al., 2012; Mizumoto et al., 2012). The Lang-8 dataset of corrections is large4 but is crowd-sourced5 and is thus of a different nature than the professionally annotated AESW and NUCLE datasets. We use the revised CoNLL2013 test set as a tuning/dev set and the CoNLL2014 test set (without alternatives) for testing. We do not make use of the non-public Cambridge Learner Corpus (CLC) (Nicholls, 2003), which contains over 1.5 million sentence pairs. Evaluation We follow past work and use the Generalized Language Understanding Evaluation (GLEU) (Napoles et al., 2016) and MaxMatch (M 2 ) metrics (Dahlmeier an"
D17-1298,N16-1042,0,0.214811,"Ng et al., 2014), a set of corrected essays by second-language learners. The CoNLL2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data. In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task (Junczys-Dowmunt and Grundkiewicz, 2016), outperforming wordand character-based sequence-to-sequence models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017), phrase-based systems with neural features (Chollampatt et al., 2016b,a), re-ranking output from phrase-based systems (Hoang et al., 2016), and combining phrase-based systems with classifiers trained for hand-picked subsets of errors (Rozovskaya and Roth, 2016). We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches. The dataset was previously p"
D18-1356,P16-1014,0,0.0593699,"Missing"
D18-1356,W13-0113,0,0.0286323,"elieve the proposed methodology represents a compelling approach to learning discrete, latent-variable representations of conditional text. 2 Related Work A core task of NLG is to generate textual descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013). There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with discriminative classifiers or rerankers in order to actually accomplish the generation (Angeli et al., 2010; Konstas and Lapata, 2013). In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over ge"
D18-1356,P09-1011,0,0.750449,"descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013). There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with discriminative classifiers or rerankers in order to actually accomplish the generation (Angeli et al., 2010; Konstas and Lapata, 2013). In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over generation segments. Recently, a new paradigm has emerged around neural text generation systems based on machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). Most"
D18-1356,W09-0613,0,0.0640977,"nd ability the records in Figure 2: ”frederick parker-rhodes (21 november 1914 - 2 march 1987) was an english mycology and plant pathology, mathematics at the university of uk.” In addition to not being fluent, it is unclear what the end of this sentence is even attempting to convey: it may be attempting to convey a fact not actually in the knowledge base (e.g., where Parker-Rhodes studied), or perhaps it is simply failing to fluently realize information that is in the knowledge base (e.g., ParkerRhodes’s country of residence). Traditional NLG systems (Kukich, 1983; McKeown, 1992; Belz, 2008; Gatt and Reiter, 2009), in contrast, largely avoid these problems. Since they typically employ an explicit planning component, which decides which knowledge base records to Parker-Rhodes (21ofMarch 1914 -Parker-Rhodes. 21 November FigureFrederick 1: Wikipedia infobox Frederick The focus on, and a surface realization component, 1987) was an English linguist, plant pathologist, computer introduction of his article reads: “Frederick Parker-Rhodes (21 which realizes the chosen records, the intent of the scientist, mathematician, mystic, and mycologist. system is always explicit, and it may be modified March 1914 – 21 N"
D18-1356,P16-1154,0,0.0590413,"rd-algorithm familiar from HMMs (Rabiner, 1989). It is actually more convenient to use the backward-algorithm formulation when using RNNs to parameterize the emission distributions, and we briefly review the backward recurrences here, again following Murphy (2002). We have: βt (j) = p(yt+1:T |zt = j, ft = 1, x) with parameters g k1 ∈ R2d and W ∈ RV ×2d . Note that there is a g k1 vector for each of K discrete states. To additionally implement a kind of slot filling, we allow emissions to be directly copied from the value portion of the records rj using copy attention (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Yang et al., 2016). Define copy scores, = K X βt∗ (k) p(zt+1 = k |zt = j) k=1 βt∗ (k) = p(yt+1:T |zt+1 = k, ft = 1, x) L h X βt+l (k) p(lt+1 = l |zt+1 = k) = l=1 ρj = k rT j tanh(g 2 ◦ hki−1 ), i p(yt+1:t+l |zt+1 = k, lt+1 = l) , where g k2 ∈ Rd . We then normalize the outputvocabulary and copy scores together, to arrive at ei−1 = softmax([v i−1 , ρ1 , . . . , ρJ ]), v and thus with base case βT (j) = 1. We can now obtain theP marginal probability of y as ∗ where we p(y |x) = K k=1 β0 (k) p(z1 = k), have used the fact that f0 must be 1, and we therefore train to maximize the log-marginal lik"
D18-1356,D15-1166,0,0.0917572,"ead to states that specialize to specific emission lengths. 3177 where &lt;/seg&gt; is an end of segment token. The RNN decoder uses attention and copy-attention over the embedded records r j , and is conditioned on zt = k by concatenating an embedding corresponding to the k’th latent state to the RNN’s input; the RNN is also conditioned on the entire x by initializing its hidden state with xa . More concretely, let hki−1 ∈ Rd be the state of an RNN conditioned on x and zt = k (as above) run over the sequence yt−lt +1:t−lt +i−1 . We let the model attend over records r j using hki−1 (in the style of Luong et al. (2015)), producing a context vector cki−1 . We may then obtain scores v i−1 for each word in the output vocabulary, v i−1 = W tanh(g k1 ◦ [hki−1 , cki−1 ]), 5.2 Learning The model requires fitting a large set of neural network parameters. Since we assume z, l, and f are unobserved, we marginalize over these variables to maximize the log marginal-likelihood of the observed tokens y given x. The HSMM marginal-likelihood calculation can be carried out efficiently with a dynamic program analogous to either the forward- or backward-algorithm familiar from HMMs (Rabiner, 1989). It is actually more conveni"
D18-1356,W00-1437,0,0.518905,"iments indicate that we can induce explicit templates (as shown in Figure 1) while achieving competitive automatic scores, and that we can control and interpret our generations by manipulating these templates. Finally, while our experiments focus on the data-to-text regime, we believe the proposed methodology represents a compelling approach to learning discrete, latent-variable representations of conditional text. 2 Related Work A core task of NLG is to generate textual descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013). There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with disc"
E06-1021,W99-0625,0,\N,Missing
E06-1021,J93-1004,0,\N,Missing
E06-1021,W04-3219,0,\N,Missing
E06-1021,moore-2002-fast,0,\N,Missing
E06-1021,C04-1051,0,\N,Missing
E06-1021,W03-1004,0,\N,Missing
E06-1021,J02-4006,0,\N,Missing
E06-1021,J03-1002,0,\N,Missing
E06-1048,P90-1035,0,0.0833559,"tion trees and derived trees. In Section 5 below, we show that linear complete embedded tree homomorphisms, which we introduce next, serve this purpose. 4 Embedded Tree Transducers Embedded tree transducers are a generalization of tree transducers in which states are allowed to take a single additional argument in a restricted manner. They correspond to a restrictive subcase of macro tree transducers with one recursion variable. We use the term “embedded tree transducer” rather than the more cumbersome “monadic macro tree transducer” for brevity and by analogy with embedded pushdown automata (Schabes and Vijay-Shanker, 1990), another automata-theoretic characterization of the tree-adjoining languages. We modify the grammar of transducer equations to add an extra argument to each occurrence of a state q. To highlight the special nature of the extra argument, it is written in angle brackets before the input tree argument. We uniformly use the otherwise unused variable x0 for this argument in the left-hand side, and add x0 as a possible right-hand side itself. Finally, right-hand-side occurrences of states may be passed an arbitrary further righthand-side tree in this argument. f (n) q∈Q ∈ F(n) xi ∈ X ::= x0 |x1 |x2"
E06-1048,W04-3312,1,0.587348,"a novel direct inter-reduction of TAG and monadic macro tree transducers. Tree transformation systems such as tree transducers and synchronous grammars have seen renewed interest, based on a perceived relevance to new applications, such as importing syntactic structure into statistical machine translation models or founding a formalism for speech command and control. The exact relationship among a variety of formalisms has been unclear, with a large number of seemingly unrelated formalisms being independently proposed or characterized. An initial step toward unifying the formalisms was taken (Shieber, 2004) in making use of the formallanguage-theoretic device of bimorphisms, previously used to characterize the tree relations definable by tree transducers. In particular, the tree relations definable by synchronous tree-substitution grammars (STSG) were shown to be just those definable by linear complete bimorphisms, thereby providing for the first time a clear relationship between synchronous grammars and tree transducers. In this work, we show how the bimorphism framework can be used to capture a more powerful formalism, synchronous tree-adjoining grammars, providing a further uniting of the var"
J10-3006,J85-4001,0,0.613197,"n problem for TL-MCTAG with rank 1 and unbounded fan-out is NP-complete. Proof We provide a reduction from 3PAR.6 Let s1 , . . . , s3m , t be an input instance of the 3PAR problem, with all of the integers si represented in unary notation. Our target grammar G is deﬁned as follows. We use a set of nonterminal symbols {S, A}, with S being the start symbol. We take the set of terminal symbols to be {a, $}. G contains two elementary tree sets. The ﬁrst set has a single elementary tree γ, corresponding to a context-free production of the form S → (AAA$)m−1 AAA: 6 We follow the proof strategy of Barton (1985) in this proof. 456 Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG Tree γ has a unique link impinging on all of the 3m occurrences of nonterminal A. The second (multi)set of G contains elementary trees γi , 1 ≤ i ≤ 3m. Each γi corresponds to a context-free production of the form A → asi : We also construct a string w = (at $)m−1 at . If there exists a partition for multiset Q = {s1 , . . . , s3m } satisfying the 3PAR requirement, we can directly construct a derivation for w in G, by sorting the elementary trees in the second set accordingly, and by inserting thes"
J10-3006,P81-1022,0,0.758747,"Missing"
J10-3006,W06-1506,0,0.0131711,"ementary structures, such as the syntax and the semantics of a single word or construction or the syntax of a single word or construction and its translation into another language, with a pair of elementary trees. This ﬂexibility permits conceptually simple, highly expressive, and tightly coupled modeling of the relationship between the syntax and semantics of a language or the syntax and semantics of two languages. As a result, it has frequently been put to use in a growing body of research into incorporating semantics into the TreeAdjoining Grammar (TAG) framework (Kallmeyer and Joshi 2003; Han 2006; Nesson and Shieber 2006, 2007). It is also under investigation as a possible base formalism for use in synchronous-grammar based machine translations systems (Nesson 2009). Similar pairing of elementary structures of the TAG formalism is too constrained to capture the inherent divergence in structure between different languages or even between the syntax and semantics of a language. Pairing of more expressive formalisms is too ﬂexible to provide appropriate constraints and has unacceptable consequences for processing efﬁciency. Although TL-MCTAG was ﬁrst introduced by Weir (1988) and shown a"
J10-3006,P92-1012,1,0.603392,"L-MCTAG membership is in NP, we construct a Turing machine that will non-deterministically guess a truncated derivation tree of size no larger than |G |· |w|. It then checks that the guessed derivation successfully derives w. Because the correctness of the derivation can be checked in linear time, this is sufﬁcient to show that TL-MCTAG membership is in NP.  We know from the equivalence of LCFRS and SL-MCTAG (and the rule-to-treetuple conversion method used to prove equivalency) (Weir 1988) and the fact that LCFRS membership is PSPACE-complete that SL-MCTAG membership is also PSPACEcomplete (Kaji et al. 1992, 1994). Until the results shown in Theorems 1 and 2 it was not known whether TL-MCTAG was in NP. Although the difference in generative capacity between TL-MCTAG and SL-MCTAG is well known, this proven difference in complexity (assuming NP = PSPACE) is novel. To understand the reason underlying the difference, we note that the bound on the length of non-splitting chains does not hold for set-local MCTAG. In set-local MCTAG a tree tuple may be non-splitting while also performing a permutation of the order of the lexical output generated by its children. Permutation is possible because set-loca"
J10-3006,J05-2003,0,0.0633171,"Missing"
J10-3006,P08-1069,1,0.869211,"Missing"
J10-3006,W07-0402,1,0.908429,"Missing"
J10-3006,H05-1101,1,0.84571,"e and one corresponding to an assignment of false. The links in the single initial tree permit only one of these two sets to be used. The tree set for a particular truth assignment for a particular variable vi makes it possible to introduce, by means of another adjunction, terminal symbols taken from the set {1, . . . , n} that correspond to each clause in C that would be satisﬁed by the given assignment to vi . In this way, the string w = 1 · · · n can be generated if and only if all clauses are satisﬁed by the truth assignment to some variable they contain. 1 We follow the proof strategy of Satta and Peserico (2005) in this and the proof of Theorem 3. 449 Computational Linguistics Volume 36, Number 3 We deﬁne a tree-local MCTAG G containing the following tree sets. The initial tree set S contains the single tree: In this tree, the “rows” correspond to the variables and the “columns” to the clauses. Each non-terminal node within a row is labeled with the same link to ensure that a tree set representing a single variable’s effect on each clause will adjoin at each link. For every variable vi , 1 ≤ i ≤ p, tree set Ti , used when representing an assignment of the value true to vi , contains n trees, one for"
J10-3006,J95-4002,0,0.0617667,"ess result still holds. The rank, r, of a grammar is the maximum number of derivational children possible for any tree in the grammar, or in other words, the maximum number of links in any tree in the grammar. We show that when rank is bounded, the NP-hardness result also holds. A notable aspect of all of the proofs given here is that they do not make use of the additional expressive power provided by the adjunction operation of TAG. Put simply, the trees in the tree sets used in our constructions meet the constraints of Tree Insertion Grammar (TIG), a known context-free–equivalent formalism (Schabes and Waters 1995). As a result, we can conclude that the increase in complexity stems from the multi-component nature of the formalism rather than from the power added by an unconstrained adjunction operation. 3.1 Universal Recognition of TL-MCTAG is NP-Complete In this section we prove that universal recognition of TL-MCTAG is NP-complete when neither the rank nor the fan-out of the grammar is bounded. Recall the 3SAT decision problem, which is known to be NP-complete. Let V = {v1 , . . . , vp } be a set of variables and C = {c1 , . . . , cn } be a set of clauses. Each clause in C is a disjunction of three li"
J10-3006,J94-1004,1,0.61007,"igatory adjunction constraint indicates that at least one link at a given node must be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate obligatory adjunction constraints by underlining the label of the node to which the constraint applies. Because we use explicit links, the edges in the derivation tree are labeled with the number of the link used rather than the traditional label of the address at which the operation takes place. Multiple adjunction refers to permitting an unbounded number of adjunctions to occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In the standard deﬁnition of TAG, multiple adjunction is disallowed to ensure that each derivation tree unambiguously speciﬁes a single derived tree (Vijay-Shanker 1987). Because each available adjunction is explicitly notated with a numbered link, our notation implicitly disallows multiple adjunction but permits a third possibility: bounded multiple adjunction. Bounded multiple adjunction permits the formalism to obtain some of the potential linguistic advantages of allowing multiple adjunction while preventing unbounded multiple adjunction. The usual constraint of allowing only one adjunct"
J10-3006,P85-1011,0,0.659159,"the derivation and derived trees are distinct. We depart from the traditional deﬁnition in notation only by specifying adjunction sites explicitly with numbered links in order to simplify the presentation of the issues raised by multi-component adjunctions. Each link may be used only once in a derivation. Adjunctions may only occur at nodes marked with a link. A numbered link at a single site in a tree speciﬁes that a single adjunction is available at that site. An obligatory adjunction constraint indicates that at least one link at a given node must be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate obligatory adjunction constraints by underlining the label of the node to which the constraint applies. Because we use explicit links, the edges in the derivation tree are labeled with the number of the link used rather than the traditional label of the address at which the operation takes place. Multiple adjunction refers to permitting an unbounded number of adjunctions to occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In the standard deﬁnition of TAG, multiple adjunction is disallowed to ensure that each derivation tree unambiguously speciﬁes a"
J10-3006,W07-0404,0,0.016427,"dy of work on induction of TAGs from a treebank exempliﬁed by Chen and Shanker (2004). The factorization performed in their work is done on the basis of syntactic constraints rather than with the goal of reducing complexity. Working from a treebank of actual natural language sentences, their work does not have the beneﬁt of explicitly labeled adjunction sites but rather must attempt to reconstruct a derivation from complete derived trees. The factorization problem we address is more closely related to work on factorizing synchronous context-free grammars (CFGs) (Gildea, Satta, and Zhang 2006; Zhang and Gildea 2007) and on factorizing synchronous TAGs (Nesson, Satta, and Shieber 2008). Synchronous grammars are a special case of multicomponent grammars, so the problems are quite similar to the TL-MCTAG factorization problem. However, synchronous grammars are fundamentally set-local rather than tree-local formalisms, which in some cases simpliﬁes their analysis. In the case of CFGs, the problem reduces to one of identifying problematic permutations of non-terminals (Zhang and Gildea 2007) and can be done efﬁciently by using a sorting algorithm to binarize any non-problematic permutations until only the int"
J10-3006,H86-1020,0,\N,Missing
J10-3006,P06-2036,1,\N,Missing
J87-1005,P81-1028,0,\N,Missing
J87-1005,P83-1009,1,\N,Missing
J90-1004,E89-1032,0,0.0953576,"d&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of Computational Linguistics Volume 16, Number 1, March 1990 Shieber et al. Semantic Head-Driven Grammar PATR-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 1.2 PRELIMINARIES Despite the general applicability of the algorithm, we will, for the sake of concreteness, describe it and other generation algorithms in terms of their implementation for definiteclause grammars (DCG). For ease of exposition, the encoding will be a bit more cumbersome than is typically found in Prolog DCG interpreters. The standard DCG encoding in Prolog uses the notation (cat o) --&gt; (cat I ) . . . . . (cat,). where the (cat i) are terms representing the grammatica"
J90-1004,J87-1005,1,0.800251,"they might be because during the distribution of store elements among the subject and complements of a verb no check is performed as to whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (1987) would not be difficult. 38 4 EXTENSIONS Tile basic semantic-head-driven generation algorithm can be augmented in various ways so as to encompass some important analyses and constraints. In particular, we discuss the incorporation of • completeness and coherence constraints, • the postponing of lexical choice, and • the ability to handle certain problematic empty-headed phrases 4.1 COMPLETENESS AND COHERENCE Wedckind (1988) defines completeness and coherence of a generation algorithm as follows. Suppose a generator derives a string w from a logical form s, and the grammar assigns to w the logi"
J90-1004,P83-1021,1,0.813733,"INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with"
J90-1004,J81-4003,1,0.825833,"Missing"
J90-1004,C88-2128,1,0.954033,"evious bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensi"
J90-1004,C88-2150,0,0.136143,"a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose well-foundedness relies on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property; further discussion can be found in Section 2.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called---has its 30 own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. Tile algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be s"
J93-1008,T87-1042,0,0.544151,"Missing"
J93-1008,E89-1032,0,0.0373278,"Missing"
J93-1008,C88-2128,1,0.882749,"ieber The Problem of Logical-Form Equivalence must allow for the representation of all and only the semantic distinctions that ramify syntactically, s It may make these distinctions by representing them explicitly or by having them be efficiently computable (that is, not by arbitrary reasoning), but they must be manifestable somehow. It seems that a resolution of the logical-form equivalence problem must wait until a representation language has been invented that characterizes exactly the semantic distinctions that ramify in natural language syntax. But this, reiterating my previous argument (Shieber 1988), is one manifestation of the AI knowledge representation problem. There, I used the fact that the knowledge representation problem has not been solved (and might be considered AI-complete) as a proof, by reduction so to speak, of the AI-completeness of the logical-form equivalence problem. Certainly, the argument is not a proof in principle, but merely one in practice; such a representation has not been developed so far but perhaps one will be devised some day. Nonetheless, I am not sanguine about the prospects. 8. Relations to Philosophy One reason that ready solution of the problem of logic"
J94-1004,C69-0101,0,0.73786,"Missing"
J94-1004,P83-1021,0,0.0102503,"use the following notations in this and later sections. The symbol P will serve as a variable over the two LIG grammar nonterminals t and b. The substring of the string wl ... Wn being parsed between indices i and j will be notated as wi+t &quot; . wj, which we take to be the empty string when i is greater than or equal to j. We will use p, A, and {9 for sequences containing terminals and LIG nonterminals with their stack specifications. For instance, F might be t[rll]t[..rl2]t[rl3]. The parsing algorithm can be seen as a tabular parsing method based on deduction of items, as in Earley deduction (Pereira and Warren 1983). We will so describe it, by presenting inference rules over items of the form (e[r/] --* r • A , i , j , k , l ) . Such items play the role of the items of Earley's algorithm. Unlike the items of Earley's algorithm, however, an item of this form does not embed a grammar rule proper; that is, P[7/] --+ pA is not necessarily a rule of the grammar. Rather, it is what we will call a reduced rule; for reasons described below, the nonterminals in F and A as well as the nonterminal P[~/] record only the top element of each stack of indices. We will use the notation P[~] --+ pA for the unreduced form"
J94-1004,C92-2065,0,0.133512,"Kosaraju, and Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber In summary, the interpretation of adjoining constraints particular notion of derivation that is used. Therefore, it test for an appropriate definition of derivation. As such, it independent notion of derivation for modifier auxiliary trees notion for predicative trees. Tree-Adjoining Derivation in TAG is sensitive to the can be used as a litmus argues for a nonstandard and a standard dependent 3.2 Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the"
J94-1004,1991.iwpt-1.4,1,0.811146,"Missing"
J94-1004,C92-2066,1,0.68126,"Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber In summary, the interpretation of adjoining constraints particular notion of derivation that is used. Therefore, it test for an appropriate definition of derivation. As such, it independent notion of derivation for modifier auxiliary trees notion for predicative trees. Tree-Adjoining Derivation in TAG is sensitive to the can be used as a litmus argues for a nonstandard and a standard dependent 3.2 Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the standard defin"
J94-1004,P92-1022,1,0.188092,"ees incrementally as parsing proceeds. Once this has been demonstrated, it should be obvious that these derivation trees could be transferred to target derivation trees during the parsing process and immediately generated from. Thus, incremental interpretation is demonstrated to be possible in the synchronous TAG framework. In fact, the technique presented in this section has allowed for the first implementation of synchronous TAG processing, by Onnig Dombalagian. This implementation was directly based on the inference-based TAG parser mentioned in Section 6.5 and presented in full elsewhere (Schabes and Shieber 1992). We associate with each item a set of operations that have been implicitly carried out by the parser in recognizing the substring covered by the item. An operation can be characterized by a derivation tree and a tree address at which the derivation tree is 113 Computational Linguistics Volume 20, N u m b e r 1 • Scanner: (b[r/] --+ F • aA, i,j, k, I) (b[z/] ~ r a . A , i , j , k , l + l ) a ~ Wl+l (P[r/] --, P • P' [r/lA, i,j, k, I) (P'[r/']--~ • O,/, - , - , l ) P' [~/'] ~ 0 • Predictor: Type 1 and 2 Completor: (b[rh] -+ r • t[r/]A, m,j', k', i) (t[rl] -+ 0 • , i,j, k, l) (b[rh] -+ Pt[r/] •"
J94-1004,J92-2002,0,0.0380292,"ernatively, the axioms can be stated as if there were extra rules S --* t[r/s] for each ~/s a start-nonterminal-labeled root node of an initial tree. In this case, the axioms are items of the form (S --~ • t[~s], 0, - , - , 0) and the string is accepted u p o n proving IS --+ t[~/s] • , 0 , - , - , n). In this case, an extra prediction and completion rule is n e e d e d just for these rules, since the normal rules do not allow S on the left-hand side. This point is taken u p further in Section 6.4. Generation of items can be cached in the standard w a y for inference-based parsing algorithms (Shieber 1992); this leads to a tabular or chart-based parsing algorithm. 6.2 The Algorithm Invariant The algorithm maintains an invariant that holds of all items a d d e d to the chart. We will describe the invariant using some additional notational conventions. Recall that P[~] -+ 1~ is the LIG production in the g r a m m a r w h o s e reduced form is P[~] --+ P. The notation F[7] where 7 is a sequence of stack symbols (i.e., nodes), specifies the sequence F with 7 replacing the occurrence of .. in the stack specifications. For example, if P is the sequence t[rll]t[..rl2]t[~13 ], then F[3,] = t[r]l]t['yrl"
J94-1004,C90-3045,1,0.920026,"g,&quot; whereas the appropriate term for the corresponding cooking process applied to peppers is &quot;roasting,&quot; would be more determining of the expected overall probabilities. Note again that the distinction between modifier and predicative trees is important. The standard definition of derivation is entirely appropriate for adjunction probabilities for predicative trees, but not for modifier trees. 3.3 Adding Semantics Finally, the formation of synchronous TAGs has been proposed to allow use of TAGs in semantic interpretation, natural language generation, and machine translation. In previous work (Shieber and Schabes 1990), the definition of synchronous TAG derivation is given in a manner that requires multiple adjunctions at a single node. The need for such derivations follows from the fact that synchronous derivations are intended to model semantic relationships. In cases of multiple adjunction of modifier trees at 9 Intuition is an appropriate guide in the design of the SLTAG framework, as the idea is to set up a linguistically plausible infrastructure on top of which a lexically based statistical model can be built. In addition, suggestive (though certainly not conclusive) evidence along these lines can be"
J94-1004,P85-1011,0,0.0109463,"r. However, tree-adjoining grammars are almost universally extended with augmentations that make the issue apposite. We discuss three such variations here, all of which argue for the use of independent derivations under certain circumstances. 4 3.1 Adding Adjoining Constraints Already in very early work on tree-adjoining grammars (Joshi, Levy, and Takahashi 1975) constraints were allowed to be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi 1985). As an application of this capability, we consider the traditional grammatical view that directional adjuncts can be used only with certain verbs. 5 This would account 4 The formulation of derivation for tree-adjoining grammars is also of significance for other grammatical formalisms based on weaker forms of adjunction such as lexicalized context-free grammar (Schabes and Waters 1993a) and its stochastic extension (Schabes and Waters 1993b), though we do not discuss these arguments here. 5 For instance, Quirk, Greenbaum, Leech, and Svartvik (1985, page 517) remark that &quot;direction adjuncts of"
J94-1004,C88-2147,0,0.0524701,"of the original node, rather than those of the root and foot of the modifier tree, are manifest in the corresponding nodes in the derived tree, the adjoining constraints would propagate appropriately to handle the examples above. This alternative leads, however, to a formalism for which derivation trees are no longer context-free, with concomitant difficulties in designing parsing algorithms. Instead, the extended definition of derivation effectively allows use of a Kleene-* in the &quot;grammar&quot; of derivation trees. Adjoining constraints can also be implemented using feature structure equations (Vijay-Shanker and Joshi 1988). It is possible that judicious use of such techniques might prevent the particular problems noted here. Such an encoding of a solution requires consideration of constraints that pass among many trees just to limit the cooccurrence of a pair of trees. However, it more closely follows the spirit of TAGs to state such intuitively local limitations locally. 7 We use the term 'predication' in its logical sense, that is, for auxiliary trees that serve as logical predicates over the trees into which they adjoin, in contrast to the term's linguistic sub-sense in which the argument of the predicate is"
J94-1004,C90-3001,1,\N,Missing
J94-1004,P93-1017,1,\N,Missing
J94-1004,W90-0102,1,\N,Missing
J97-3005,E95-1032,0,0.012108,"olume 23, Number 3 Notice that the two sets of readings are disjoint and depend crucially on the antecedent of the pronoun in the source clause. 1 Past approaches to recovering these readings fall into two categories, source-determined analyses and discourse-determined analyses. We describe these in the sections that follow. 2. Source-Determined Analyses The conventional approaches to recovering the elided property in VP ellipsis have been source-determined (Sag 1976; Williams 1977; Gawron and Peters 1990; Priest, Scha, and van den Berg 1991; Dalrymple, Shieber, and Pereira 1991; Kehler 1993; Crouch 1995). Common to these approaches is the idea that at some level of representation (surface syntactic, deep syntactic, or semantic) the anaphoric relationships for the source are marked, and that the target is interpreted as if it were constructed with relationships determined in some uniform manner by those of the source clause at that level of representation. In this paper, we will use the equational analysis of Dalrymple, Shieber, and Pereira (1991, henceforth DSP) as the exemplar of these approaches. In this account, the uniformity is specified by the solving of a certain equation in which, rou"
J97-3005,C92-1048,0,0.13713,"g. After one round with Spinksi, Tysonj beat him/. Now people think that no one can beat himj. 4 We use the # symbolto mark examples that are infelicitousunder the intended interpretation. 460 Kehler and Shieber Anaphoric Dependencies in Ellipsis b. # Mike Tyson will always be considered one of the greats of professional boxing. After one round with Spinksi, Tysonj beat him/. N o w people think that no one can. Example (9) demonstrates that pronouns within copied VPs are not as free to seek extrasentential referents as their unelided VP counterparts. Example (10), a variant of an example that Hardt (1992a) provides to argue against source-determined analyses (see Section 5.1), shows that this is also the case for intrasentential referents. The reading where Mary asked out Bob at Bob&apos;s party, while readily available with light accent on the p r o n o u n in example (10a), is not available in its elided counterpart (10b). (10) a. Every boyi was hoping that Mary would ask himi out, but the waiting is over. Last night at Bob&apos;sj party, she asked himj out. b . # Every boyi was hoping that Mary would ask him/ out, but the waiting is over. Last night at Bob&apos;sj party, she did. Rooth (1993) gives a sim"
J97-3005,E93-1025,1,0.796033,"Linguistics Volume 23, Number 3 Notice that the two sets of readings are disjoint and depend crucially on the antecedent of the pronoun in the source clause. 1 Past approaches to recovering these readings fall into two categories, source-determined analyses and discourse-determined analyses. We describe these in the sections that follow. 2. Source-Determined Analyses The conventional approaches to recovering the elided property in VP ellipsis have been source-determined (Sag 1976; Williams 1977; Gawron and Peters 1990; Priest, Scha, and van den Berg 1991; Dalrymple, Shieber, and Pereira 1991; Kehler 1993; Crouch 1995). Common to these approaches is the idea that at some level of representation (surface syntactic, deep syntactic, or semantic) the anaphoric relationships for the source are marked, and that the target is interpreted as if it were constructed with relationships determined in some uniform manner by those of the source clause at that level of representation. In this paper, we will use the equational analysis of Dalrymple, Shieber, and Pereira (1991, henceforth DSP) as the exemplar of these approaches. In this account, the uniformity is specified by the solving of a certain equation"
N03-1029,J95-4004,0,0.0870481,"Missing"
N03-1029,P97-1003,0,0.136176,"Missing"
N03-1029,A94-1013,0,0.0360548,"on reconstruction problem especially with regard to predicting sentence boundary punctuation such as periods, question marks, and exclamation marks. (This problem is distinct from the problem of sentence boundary disambiguation, where punctuation is provided, but the categorization of the punctuation as to whether or not 9 An alternative method of resolving the data sparsity issues is to back off the model p(yi |yi−2 yi−1 eci ), for instance to p(yi |yi−2 yi−1 ) or to p(yi |yi−1 eci ). Both of these perform less well than the approximation in model 8. it marks a sentence boundary is at issue (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997).) Stolcke and Shriberg (1996) used HMMs for the related problem of linguistic segmentation of text, where the segments corresponded to sentences and other self-contained units such as disfluencies and interjections. They argue that a linguistic segmentation is useful for improving the performance and utility of language models and speech recognizers. Like the present work, they segment clean text rather than automatically transcribed speech. Stevenson and Gaizauskas (Stevenson and Gaizauskas, 2000) and Goto and Renals (Gotoh and Renals, 2000) address the sentenc"
N03-1029,A97-1004,0,0.028801,"especially with regard to predicting sentence boundary punctuation such as periods, question marks, and exclamation marks. (This problem is distinct from the problem of sentence boundary disambiguation, where punctuation is provided, but the categorization of the punctuation as to whether or not 9 An alternative method of resolving the data sparsity issues is to back off the model p(yi |yi−2 yi−1 eci ), for instance to p(yi |yi−2 yi−1 ) or to p(yi |yi−1 eci ). Both of these perform less well than the approximation in model 8. it marks a sentence boundary is at issue (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997).) Stolcke and Shriberg (1996) used HMMs for the related problem of linguistic segmentation of text, where the segments corresponded to sentences and other self-contained units such as disfluencies and interjections. They argue that a linguistic segmentation is useful for improving the performance and utility of language models and speech recognizers. Like the present work, they segment clean text rather than automatically transcribed speech. Stevenson and Gaizauskas (Stevenson and Gaizauskas, 2000) and Goto and Renals (Gotoh and Renals, 2000) address the sentence boundary detection problem di"
N03-1029,A00-1012,0,0.0127211,"the approximation in model 8. it marks a sentence boundary is at issue (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997).) Stolcke and Shriberg (1996) used HMMs for the related problem of linguistic segmentation of text, where the segments corresponded to sentences and other self-contained units such as disfluencies and interjections. They argue that a linguistic segmentation is useful for improving the performance and utility of language models and speech recognizers. Like the present work, they segment clean text rather than automatically transcribed speech. Stevenson and Gaizauskas (Stevenson and Gaizauskas, 2000) and Goto and Renals (Gotoh and Renals, 2000) address the sentence boundary detection problem directly, again using lexical and, in the latter, prosodic cues. 6 Future Work and Conclusion The experiments reported here — like much of the previous work in comma restoration (Beeferman et al., 1998) and sentence boundary disambiguation and restoration (Stolcke and Shriberg, 1996; Shriberg et al., 2001; Gotoh and Renals, 2000; Stevenson and Gaizauskas, 2000) (though not all (Christensen et al., 2001; Stolcke et al., 1998; Kim and Woodland, 2001)) — assume an ideal reference transcription of the tex"
N09-1011,W08-2303,0,0.23597,"in terms of efficient processing. Much work in TAG semantics makes use of tree-local MCTAG (TL-MCTAG) to model phenomena such as quantifier scoping, Wh-question formation, and many other constructions (Kallmeyer and Romero, 2004; Romero et al., 2004). Certain applications, however, appear to require even more flexibility than is provided by TL-MCTAG. Scrambling is one well-known example (Rambow, 1994). In addition, in the semantics domain, the use of a new TAG operation, flexible composition, is used to perform certain semantic operations that seemingly cannot be modeled with TL-MCTAG alone (Chiang and Scheffler, 2008) and in work in synchronous TAG semantics, constructions such as nested quantifiers require a set-local MCTAG (SL-MCTAG) analysis (Nesson and Shieber, 2006). Recent applications of Tree-Adjoining Grammar (TAG) to the domain of semantics as well as new attention to syntactic phenomena have given rise to increased interested in more expressive and complex multicomponent TAG formalisms (MCTAG). Although many constructions can be modeled using tree-local MCTAG (TL-MCTAG), certain applications require even more flexibility. In this paper we suggest a shift in focus from constraining locality and co"
N09-1011,W04-3321,0,0.0518743,"Missing"
N09-1011,W04-3325,0,0.0327897,"Missing"
N09-1011,J10-3006,1,\N,Missing
N16-1114,D08-1031,0,0.543141,"scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requiring only a left-to-right pass through a document’s mentions during which a mention’s antecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention xn to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair models (e.g., Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model 1 We assume nested mentions are ordered by their syntactic heads. 995 as linking to a single nominal or proper antecedent, for example, but not necessarily to all other coreferent mentions. Accordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing th"
N16-1114,P14-1005,0,0.11608,"Missing"
N16-1114,C10-1017,0,0.0193937,"into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of Dyer e"
N16-1114,D13-1057,0,0.0610294,"Missing"
N16-1114,P15-1136,0,0.656832,"2015) Peng et al. (2015) Wiseman et al. (2015) This work P MUC R F1 74.30 76.72 76.12 76.23 77.49 67.46 68.13 69.38 69.31 69.75 70.72 72.17 72.59 72.22 72.60 73.42 P B3 R F1 P CEAFe R F1 CoNLL 62.71 66.12 65.64 66.07 66.83 54.96 54.22 56.01 55.83 56.95 58.58 59.58 60.44 60.50 60.52 61.50 59.40 59.47 59.44 59.41 62.14 52.27 52.33 52.98 54.88 53.85 55.61 55.67 56.02 56.37 57.05 57.70 61.63 62.47 63.02 63.03 63.39 64.21 Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al. (2015). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al. (2015) for all metrics. genre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to φp and φa . • We add features indicating if a mention has a substring overlap with the current speaker (φp and φa ), and if an antecedent has a substring overlap with a speaker distinct from the current mention’s speaker (φp ). • We add a single centered, rescaled document position feature to each mention when learning hc . We calculate a mention xn ’s rescaled"
N16-1114,N07-1011,0,0.293996,"rst, inference is relatively simple and efficient, requiring only a left-to-right pass through a document’s mentions during which a mention’s antecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention xn to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair models (e.g., Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model 1 We assume nested mentions are ordered by their syntactic heads. 995 as linking to a single nominal or proper antecedent, for example, but not necessarily to all other coreferent mentions. Accordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing the model to utilize global, structural information relating to z in making its predictions. We therefore investigate objective functions"
N16-1114,D08-1069,0,0.183256,"and so we may represent a clustering with a vector z ∈ {1, . . . , M }N , where zn = m iff xn is a member of X (m) . Coreference systems attempt to find the best clustering z ∗ ∈ Z under some scoring function, with Z the set of valid clusterings. One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention xn ; because xn may not be anaphoric (and therefore have no antecedents), a “dummy” antecedent  may also be predicted. The aforementioned strategy is adopted by “mention-ranking” systems (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013), which, formally, predict an antecedent yˆ ∈ Y(xn ) for each mention xn , where Y(xn ) = {1, . . . , n − 1, }. Through transitivity, these decisions induce a clustering over the document. Mention-ranking systems make their antecedent predictions with a local scoring function f (xn , y) defined for any mention xn and any antecedent y ∈ Y(xn ). While such a scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requ"
N16-1114,D13-1203,0,0.681581,"cal classifier with fixed context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster"
N16-1114,Q14-1037,0,0.0979,"Missing"
N16-1114,P15-1033,0,0.0249897,"Missing"
N16-1114,N10-1061,0,0.0687172,"structured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it"
N16-1114,N06-2015,0,0.116337,"5: 6: 7: 8: 9: 10: 11: 12: y∈Y(xn ) m ← zy ∗ if y ∗ =  then M ←M +1 m←M append xn to X (m) zn ← m h(m) ← RNN(hc (xn ), h(m) ) return X (1) , . . . , X (M ) is shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates the current RNN representation based on the previous decision that was made, and line 4, which then uses this cluster representation as part of scoring. 6 Experiments 6.1 Methods We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012). The task uses the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the shared task. For all experiments, we use the Berkeley Coreference System (Durrett and Klein, 2013) for mention extraction and to compute features φa and φp . Features We use the raw BASIC + feature sets described by Wiseman et al. (2015), with the following modifications: • We remove all features from φp that concatenate a feature of the antecedent with a feature of the current mention, such as bi-head features. • We add true-cased head features, a current speaker indicator feature, an"
N16-1114,W04-3250,0,0.0132488,"59 72.22 72.60 73.42 P B3 R F1 P CEAFe R F1 CoNLL 62.71 66.12 65.64 66.07 66.83 54.96 54.22 56.01 55.83 56.95 58.58 59.58 60.44 60.50 60.52 61.50 59.40 59.47 59.44 59.41 62.14 52.27 52.33 52.98 54.88 53.85 55.61 55.67 56.02 56.37 57.05 57.70 61.63 62.47 63.02 63.03 63.39 64.21 Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al. (2015). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al. (2015) for all metrics. genre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to φp and φa . • We add features indicating if a mention has a substring overlap with the current speaker (φp and φa ), and if an antecedent has a substring overlap with a speaker distinct from the current mention’s speaker (φp ). • We add a single centered, rescaled document position feature to each mention when learning hc . We calculate a mention xn ’s rescaled doc−1 ument position as 2n−N N −1 . These modifications result in there being approximately 14K distinct features in φa and approx"
N16-1114,D13-1027,0,0.0294159,"context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster 994 Proceedings of NAACL-HLT"
N16-1114,N16-1082,0,0.0151961,"since companies are generally not coreferent with pronouns like “his.” Figure 4 shows an example (consisting of a telephone conversation between “A” and “B”) in which the bracketed pronoun “It’s” is being used pleonastically. Whereas the baseline MR model predicts “It’s” to corefer with a previous “it” — thus making a FL error — the greedy RNN model does not. In Figure 4 the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the NA term in g with respect to that mention. This visualization resembles the “saliency” technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score. We see that the potential antecedent “S-Bahn” has a large gradient, but also that the initial, obviously pleonastic use of “it’s” has a large gradient, 1002 Related Work In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 20"
N16-1114,P14-2005,0,0.0223759,"forming the dot-product scores. 1000 MR Avg, OH RNN, GH RNN, OH MUC B3 CEAFe CoNLL 73.06 73.30 73.63 74.26 62.66 63.06 63.23 63.89 58.98 58.85 59.56 59.54 64.90 65.07 65.47 65.90 Table 2: F1 scores of models described in text on CoNLL 2012 development set. Rows in grey highlight models using oracle history. Following Wiseman et al. (2015) we use the costweights α = h0.5, 1.2, 1i in defining ∆, and we use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official CoNLL 2012 script (Pradhan et al., 2014; Luo et al., 2014). Code for our system is available at https: //github.com/swiseman/nn_coref. The system makes use of a GPU for training, and trains in about two hours. 6.2 Results In Table 1 we present our main results on the CoNLL English test set, and compare with other recent stateof-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F1 scores to date on all three CoNLL metrics. We now consider in more detail the impact of global features and RNNs on performance. For these experiments, we report MUC, B3 , and CEAFe F1"
N16-1114,Q15-1029,0,0.81448,"uch, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster 994 Proceedings of NAACL-HLT 2016, pages 994–1004, c San Diego, California, Jun"
N16-1114,N15-3002,0,0.0202065,"Missing"
N16-1114,C02-1139,0,0.175186,"e “saliency” technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score. We see that the potential antecedent “S-Bahn” has a large gradient, but also that the initial, obviously pleonastic use of “it’s” has a large gradient, 1002 Related Work In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat"
N16-1114,K15-1002,0,0.291432,"vel features improved their results, whereas Martschat and Strube (2015) found that they did not. Clark and Manning (2015) found that incorporating cluster-level features beyond those involving the precomputed mention-pair and mention-ranking probabilities that form the basis of their agglomerative clustering coreference system did not improve performance. Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015). 3.2 Issues with Global Features We believe a major reason for the relative ineffectiveness of global features in coreference problems is that, as noted by Clark and Manning (2015), cluster-level features can be hard to define. Specif996 ically, it is difficult to define discrete, fixed-length features on clusters, which can be of variable size (or shape). As a result, global coreference features tend to be either too coarse or too sparse. Thus, early attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most to the mention-level features defin"
N16-1114,D08-1068,0,0.0359802,"ntary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is ap"
N16-1114,W12-4501,0,0.271279,". . N do 4: y ∗ ← arg max f (xn , y) + g(xn , y, z 1:n−1 ) 5: 6: 7: 8: 9: 10: 11: 12: y∈Y(xn ) m ← zy ∗ if y ∗ =  then M ←M +1 m←M append xn to X (m) zn ← m h(m) ← RNN(hc (xn ), h(m) ) return X (1) , . . . , X (M ) is shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates the current RNN representation based on the previous decision that was made, and line 4, which then uses this cluster representation as part of scoring. 6 Experiments 6.1 Methods We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012). The task uses the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the shared task. For all experiments, we use the Berkeley Coreference System (Durrett and Klein, 2013) for mention extraction and to compute features φa and φp . Features We use the raw BASIC + feature sets described by Wiseman et al. (2015), with the following modifications: • We remove all features from φp that concatenate a feature of the antecedent with a feature of the current mention, such as bi-head features. • We add true-case"
N16-1114,P14-2006,0,0.0831968,"the LSTM states before forming the dot-product scores. 1000 MR Avg, OH RNN, GH RNN, OH MUC B3 CEAFe CoNLL 73.06 73.30 73.63 74.26 62.66 63.06 63.23 63.89 58.98 58.85 59.56 59.54 64.90 65.07 65.47 65.90 Table 2: F1 scores of models described in text on CoNLL 2012 development set. Rows in grey highlight models using oracle history. Following Wiseman et al. (2015) we use the costweights α = h0.5, 1.2, 1i in defining ∆, and we use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official CoNLL 2012 script (Pradhan et al., 2014; Luo et al., 2014). Code for our system is available at https: //github.com/swiseman/nn_coref. The system makes use of a GPU for training, and trains in about two hours. 6.2 Results In Table 1 we present our main results on the CoNLL English test set, and compare with other recent stateof-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F1 scores to date on all three CoNLL metrics. We now consider in more detail the impact of global features and RNNs on performance. For these experiments, we report MUC"
N16-1114,D09-1101,0,0.730904,"lustering with a vector z ∈ {1, . . . , M }N , where zn = m iff xn is a member of X (m) . Coreference systems attempt to find the best clustering z ∗ ∈ Z under some scoring function, with Z the set of valid clusterings. One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention xn ; because xn may not be anaphoric (and therefore have no antecedents), a “dummy” antecedent  may also be predicted. The aforementioned strategy is adopted by “mention-ranking” systems (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013), which, formally, predict an antecedent yˆ ∈ Y(xn ) for each mention xn , where Y(xn ) = {1, . . . , n − 1, }. Through transitivity, these decisions induce a clustering over the document. Mention-ranking systems make their antecedent predictions with a local scoring function f (xn , y) defined for any mention xn and any antecedent y ∈ Y(xn ). While such a scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requiring only a left-to-"
N16-1114,J01-4004,0,0.413935,"zation resembles the “saliency” technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score. We see that the potential antecedent “S-Bahn” has a large gradient, but also that the initial, obviously pleonastic use of “it’s” has a large gradient, 1002 Related Work In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and"
N16-1114,C12-1154,0,0.0479564,"oreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering. The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it bears some similarity to th"
N16-1114,P15-1137,1,0.110816,"ory-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model. Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics. 2 Background and Notation Coreference resolution is fundamentally a clustering task. Given a sequence (xn )N n=1 of (intra-document) mentions – that is, syntactic units that can refer or be referred to – coreference resolution involves partitioning (xn ) into a sequence of clusters (X (m) )M m=1 such that all the mentions in any particular cluster 994 Proceedings of NAACL-HLT 2016, pages 994–1004,"
N16-1114,D08-1067,0,\N,Missing
P08-1069,P06-2036,1,0.639552,"d. (a) S (b) S (c) S likes 1 N P↓ 1 N P↓ 1 VP V N P↓ 2 N Adj N∗ N∗ Adj red John candies 1 N P↓ 2 V NP VP John V rouges NP V P N P Jean V red likes aime likes N VP S 2 NP N 1 NP Det NP N NP John Jean 1 Adj NP aime Det N N red candies les N N Adj bonbons rouges candies les bonbons Figure 2: An example STAG derivation of the English/French sentence pair “John likes red candies”/“Jean aime les bonbons rouges”. The figure is divided as follows: (a) the STAG grammar, (b) the derivation tree for the sentence pair, and (c) the derived tree pair for the sentences. and Gildea, 2007; Zhang et al., 2006; Gildea, Satta, and Zhang, 2006). The methods for k-arization of SCFG cannot be directly applied to STAG because of the additional complexity introduced by the expressivity-increasing adjunction operation of TAG. In SCFG, where substitution is the only available operation and the depth of elementary structures is limited to one, the k-arization problem reduces to analysis of permutations of strings of nonterminal symbols. In STAG, however, the arbitrary depth of the elementary structures and the lack of restriction to contiguous strings of nonterminals introduced by adjunction substantially complicate the task. In this pape"
P08-1069,W06-1506,0,0.0647525,"ity Cambridge, MA 02138 Giorgio Satta Department of Information Engineering University of Padua I-35131 Padova, Italy Stuart M. Shieber School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 nesson@seas.harvard.edu satta@dei.unipd.it shieber@seas.harvard.edu Abstract the application of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-langua"
P08-1069,W06-1505,0,0.0133878,"ity Cambridge, MA 02138 Giorgio Satta Department of Information Engineering University of Padua I-35131 Padova, Italy Stuart M. Shieber School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 nesson@seas.harvard.edu satta@dei.unipd.it shieber@seas.harvard.edu Abstract the application of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-langua"
P08-1069,W07-0402,1,0.850392,"d Sciences Harvard University Cambridge, MA 02138 Giorgio Satta Department of Information Engineering University of Padua I-35131 Padova, Italy Stuart M. Shieber School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138 nesson@seas.harvard.edu satta@dei.unipd.it shieber@seas.harvard.edu Abstract the application of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of nat"
P08-1069,2006.amta-papers.15,1,0.905421,"Missing"
P08-1069,P92-1012,1,0.761867,"ication of synchronous tree-adjoining grammar (STAG) to this problem (Nesson, Shieber, and Rush, 2006; Chiang and Rambow, 2006). In a parallel development, interest in incorporating semantic computation into the TAG framework has led to the use of STAG for this purpose (Nesson and Shieber, 2007; Han, 2006b; Han, 2006a; Nesson and Shieber, 2006). Although STAG does not increase the expressivity of the underlying formalisms (Shieber, 1994), STAG parsing is known to be NPhard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures (Satta, 1992; Weir, 1988). Without efficient algorithms for processing it, its potential for use in machine translation and TAG semantics systems is limited. Synchronous Tree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-language syntax and semantics. Current research in both of these areas is actively pursuing its incorporation. However, STAG parsing is known to be NP-hard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures. Given a particular grammar, the p"
P08-1069,C90-3045,1,0.737602,"only once in a derivation. Operations may only occur at nodes marked with a link. For simplicity of presentation we provisionally assume that only one link is permitted at a node. We later drop this assumption. In a synchronous TAG (STAG) the elementary structures are ordered pairs of TAG trees, with a linking relation specified over pairs of nonterminal nodes. Each link has two locations, one in the left tree in a pair and the other in the right tree. An example of an STAG derivation including both substitution and adjunction is given in Figure 2. For further background, refer to the work of Shieber and Schabes (1990) and Shieber (1994). 606 k-arization Algorithm For a synchronous tree pair γ = hγL , γR i, a fragment of γL (or γR ) is a complete subtree rooted at some node n of γL , written γL (n), or else a subtree rooted at n with a gap at node n0 , written γL (n, n0 ); see Figure 3 for an example. We write links(n) and links(n, n0 ) to denote the set of links of γL (n) and γL (n, n0 ), respectively. When we do not know the root or gap nodes of some fragment αL , we also write links(αL ). We say that a set of links Λ from γ can be isolated if there exist fragments αL and αR of γL and γR , respectively, b"
P08-1069,W07-0404,0,0.0804827,"Missing"
P08-1069,N06-1033,0,0.129869,"four may be isolated. (a) S (b) S (c) S likes 1 N P↓ 1 N P↓ 1 VP V N P↓ 2 N Adj N∗ N∗ Adj red John candies 1 N P↓ 2 V NP VP John V rouges NP V P N P Jean V red likes aime likes N VP S 2 NP N 1 NP Det NP N NP John Jean 1 Adj NP aime Det N N red candies les N N Adj bonbons rouges candies les bonbons Figure 2: An example STAG derivation of the English/French sentence pair “John likes red candies”/“Jean aime les bonbons rouges”. The figure is divided as follows: (a) the STAG grammar, (b) the derivation tree for the sentence pair, and (c) the derived tree pair for the sentences. and Gildea, 2007; Zhang et al., 2006; Gildea, Satta, and Zhang, 2006). The methods for k-arization of SCFG cannot be directly applied to STAG because of the additional complexity introduced by the expressivity-increasing adjunction operation of TAG. In SCFG, where substitution is the only available operation and the depth of elementary structures is limited to one, the k-arization problem reduces to analysis of permutations of strings of nonterminal symbols. In STAG, however, the arbitrary depth of the elementary structures and the lack of restriction to contiguous strings of nonterminals introduced by adjunction substantially c"
P08-1069,W06-1501,0,\N,Missing
P08-1069,W90-0102,1,\N,Missing
P10-1096,P03-2041,0,0.349608,"as providing syntactic analyses for our predictions, which is desirable for automatic evaluation purposes. A straightforward extension of the popular EM algorithm for probabilistic context free grammars (PCFG), the inside-outside algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus of parallel parse trees t = t1 , . . . , tN where tn = tn,s /tn,t for n = 1, . . . , N . Similarly, an The STSG Model Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). Every grammar rule is a pair of elementary trees aligned at the leaf level at their frontier nodes, which we will denote using the form cs /ct → es /et , γ (indices s for source, t for target) where cs , ct are root nonterminals of the elementary trees es , et respectively and γ is a 1-to-1 correspondence between the frontier nodes in es and et . For example, the rule S / S → (S (PP (IN Like) NP[] ) NP[1] VP[2] ) / (S NP[1] VP[2] ) 939 Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a target node (top row in blue), and split/merge a deletio"
P10-1096,P06-1048,0,0.402144,"VP[2] , NP[] aligns with the special symbol  denoting a deletion from the source tree. Symmetrically -aligned target nodes are used to represent insertions into the target tree. Similarly, the rule where the underlined words were deleted. In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. An unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints (Clarke and Lapata, 2006a; Turner and Charniak, 2005). Because these methods rely on dynamic programming to efficiently consider hypotheses over the space of all possible compressions of a sentence, they may be harder to extend to general paraphrasing. 3 NP /  → (NP (NN FaceLift)) /  can be used to continue deriving the deleted subtree. See Figure 1 for an example of how an STSG with these rules would operate in synchronously generating our example sentence pair. STSG is a convenient choice of formalism for a number of reasons. First, it eliminates the isomorphism and strong independence assumptions of SCFGs. Secon"
P10-1096,N07-1023,0,0.304,"e of grammars as opposed to sparse parametric inference with a fixed grammar. 1 Introduction Given an aligned corpus of tree pairs, we might want to learn a mapping between the paired trees. Such induction of tree mappings has application in a variety of natural-language-processing tasks including machine translation, paraphrase, and sentence compression. The induced tree mappings can be expressed by synchronous grammars. Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown, 2007). More expressive formalisms such as syn1 Throughout the paper we will use the word STSG to refer to the tree-to-tree version of the formalism, although the string-to-tree version is also commonly used. 937 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937–947, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics possibility of searching over the infinite space of grammars (and, in machine translation, possible word alignments), thus side-stepping the narrowness problem outlined above as well. In this work, we use a"
P10-1096,D09-1037,0,0.0593039,"ithin this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the 2 Sentence compression Sentence compression is the task of summarizing a sentence while retaining"
P10-1096,N04-1035,0,0.0393607,"rmalism then restricts us to deriving isomorphic tree pairs. STSG is much more expressive, especially if we allow some elementary trees on the source or target side to be unsynchronized, so that insertions and deletions can be modeled, but the segmentation and alignment problems become nontrivial. Previous approaches to this problem have treated the two steps — grammar extraction and weight estimation — with a variety of methods. One approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules (Och and Ney, 2004; Galley et al., 2004) but this leaves open the question of finding the right level of generality of the rules — how deep the rules should be and how much lexicalization they should involve — necessitating resorting to heuristics such as minimality of rules, and leading to We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavail"
P10-1096,D07-1008,0,0.0541834,"ults were not significantly different from those using MPD. 4 The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. However, given the nature of the data that we provided, it can only learn to compress by deleting words. Since the system is less specialized to the task, their model requires additional heuristics in decoding not needed for extractive compression, which might cause a reduction in performance. Nonetheless, because the comparison system is a generalization of the extractive SVM compressor of Cohn and Lapata (2007), we do not expect that the results would differ qualitatively. We discuss the problem of predicting a target tree tN +1,t that corresponds to a source tree tN +1,s unseen in the observed corpus t. The maximum probability tree (MPT) can be found by considering all possible ways to derive it. However a much simpler alternative is to choose the target tree implied by the maximum probability derivasible to prove that our approach is equivalent up to a rescaling of the concentration parameters. Since we fit these parameters to the data, our approach is equivalent. 942 Precision Recall Relational F"
P10-1096,P06-1085,0,0.0982665,"ch is to use a generative model of synchronous derivation and simultaneously segment and weight the elementary tree pairs to maximize the probability of the training data under that model; the simplest exemplar of this approach uses expectation maximization (EM) (Dempster et al., 1977). This approach has two frailties. First, EM search over the space of all possible rules is computationally impractical. Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data (Goldwater et al., 2006; DeNero et al., 2006). Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule. Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting"
P10-1096,C08-1018,0,0.33524,"es with large elementary trees; in turn, the sampling procedure is prone to local modes. In order to counteract this and to improve mixing we used simulated annealing. The probability mass function (5-7) was raised to the power 1/T with T dropping linearly from T = 5 to T = 0. Furthermore, using a final temperature of zero, we recover a maximum a posteriori (MAP) estimate which we denote eMAP . 3.3 e 4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). EM is a natural benchmark, while SVM is also appropriate since it can be taken as the state of the art for our task.4 We used a publicly available extractive sentence compression corpus: the Broadcast News compressions corpus (BNC) of Clarke and Lapata (2006a). This corpus consists of 1370 sentence pairs that were manually created from transcribed Broadcast News stories. We split the pairs into training, development, and testing sets of 1000, Prediction 3 We experimented with MPT using Monte Carlo integration over possible derivations; the results were not significantly different from"
P10-1096,N09-1062,0,0.0394759,"task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the 2 Sen"
P10-1096,A00-1043,0,0.0265991,"ing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the 2 Sentence compression Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). In extractive sentence compression, which we focus on in this paper, an order-preserving subset of the words in the sentence are selected to form the summary, that is, we summarize by deleting words (Knight and Marcu, 2002). An example sentence pair, which we use as a running example, is the following: • Like FaceLift, much of ATM’s screen performance depends on the underlying application. • ATM’s screen performance depends on the underlying application. 938 Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. can be used to delete a subtree roote"
P10-1096,W06-3105,0,0.0297975,"e model of synchronous derivation and simultaneously segment and weight the elementary tree pairs to maximize the probability of the training data under that model; the simplest exemplar of this approach uses expectation maximization (EM) (Dempster et al., 1977). This approach has two frailties. First, EM search over the space of all possible rules is computationally impractical. Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data (Goldwater et al., 2006; DeNero et al., 2006). Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule. Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models"
P10-1096,D08-1033,0,0.0531215,"Missing"
P10-1096,D09-1136,0,0.0870277,"ummary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the 2 Sentence compression Sentence compression is the task of summarizing a sentence while retaining most of the information"
P10-1096,J04-4002,0,0.036448,"y tree — but the formalism then restricts us to deriving isomorphic tree pairs. STSG is much more expressive, especially if we allow some elementary trees on the source or target side to be unsynchronized, so that insertions and deletions can be modeled, but the segmentation and alignment problems become nontrivial. Previous approaches to this problem have treated the two steps — grammar extraction and weight estimation — with a variety of methods. One approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules (Och and Ney, 2004; Galley et al., 2004) but this leaves open the question of finding the right level of generality of the rules — how deep the rules should be and how much lexicalization they should involve — necessitating resorting to heuristics such as minimality of rules, and leading to We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word align"
P10-1096,P09-2012,0,0.103154,"recomputed subset of rules, and with EM used just to assign weights within this grammar. In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al., 2006), to parsing (Cohn et al., 2009; Post and Gildea, 2009) and machine translation (DeNero et al., 2008; Cohn and Blunsom, 2009; Liu and Gildea, 2009). Segmentation is achieved by introducing a prior bias towards grammars that are compact representations of the data, namely by enforcing simplicity and sparsity: preferring simple rules (smaller segments) unless the use of a complex rule is evidenced by the data (through repetition), and thus mitigating the overfitting problem. A Dirichlet process (DP) prior is typically used to achieve this interplay. Interestingly, samplingbased nonparametric inference further allows the 2 Sentence compression Senten"
P10-1096,N03-1026,0,0.175971,"Missing"
P10-1096,P05-1036,0,0.0606811,"the special symbol  denoting a deletion from the source tree. Symmetrically -aligned target nodes are used to represent insertions into the target tree. Similarly, the rule where the underlined words were deleted. In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. An unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints (Clarke and Lapata, 2006a; Turner and Charniak, 2005). Because these methods rely on dynamic programming to efficiently consider hypotheses over the space of all possible compressions of a sentence, they may be harder to extend to general paraphrasing. 3 NP /  → (NP (NN FaceLift)) /  can be used to continue deriving the deleted subtree. See Figure 1 for an example of how an STSG with these rules would operate in synchronously generating our example sentence pair. STSG is a convenient choice of formalism for a number of reasons. First, it eliminates the isomorphism and strong independence assumptions of SCFGs. Second, the ability to have rules"
P10-1096,P08-2035,1,0.869762,"ence, they may be harder to extend to general paraphrasing. 3 NP /  → (NP (NN FaceLift)) /  can be used to continue deriving the deleted subtree. See Figure 1 for an example of how an STSG with these rules would operate in synchronously generating our example sentence pair. STSG is a convenient choice of formalism for a number of reasons. First, it eliminates the isomorphism and strong independence assumptions of SCFGs. Second, the ability to have rules deeper than one level provides a principled way of modeling lexicalization, whose importance has been emphasized (Galley and McKeown, 2007; Yamangil and Nelken, 2008). Third, we may have our STSG operate on trees instead of sentences, which allows for efficient parsing algorithms, as well as providing syntactic analyses for our predictions, which is desirable for automatic evaluation purposes. A straightforward extension of the popular EM algorithm for probabilistic context free grammars (PCFG), the inside-outside algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus of parallel parse trees t = t1 , . . . , tN where tn = tn,s /tn,t for n = 1, . . . , N . Similarly, an The STSG Model Synchron"
P10-1096,P06-2019,0,\N,Missing
P12-2022,W08-2102,0,0.0780983,"R NP* NN the NNL IN ε NN NPR PP NPR NP NP of NNR NP* ε PP IN the ε NP former DT NPL (a) NP NP NPL S WHNP NN NP president NNL NP NN of Figure 1: Example TIG derivation of an NP constituent: One left insertion (at NN) and two simultaneous right insertions (at NP). practical as compared to CFG and TSG’s O(n3 ). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.1 This has led researchers to resort to heuristic grammar extraction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguisti"
P12-2022,P00-1058,0,0.568901,"ident DT NP NPR NP* NN the NNL IN ε NN NPR PP NPR NP NP of NNR NP* ε PP IN the ε NP former DT NPL (a) NP NP NPL S WHNP NN NP president NNL NP NN of Figure 1: Example TIG derivation of an NP constituent: One left insertion (at NN) and two simultaneous right insertions (at NP). practical as compared to CFG and TSG’s O(n3 ). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.1 This has led researchers to resort to heuristic grammar extraction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still"
P12-2022,P10-2042,0,0.349281,"ing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures: We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data. 1 • [N P the [N N [N N [N N president] of the university] who resigned yesterday]] • [N P the [N N former [N N [N N president] of the university]]] • [N P the [N N [N N president] who resigned yesterday]] Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse gr"
P12-2022,N09-1062,0,0.228449,"the university]]] • [N P the [N N [N N president] who resigned yesterday]] Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse grammars — and low complexity — making induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009; Cohn and Blunsom, 2010; Post and Gildea, 2009). DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so A natural recurring structure here would be the structure “[N P the [N N president]]”, yet it occurs not at all in the data. TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, al"
P12-2022,P98-1091,0,0.767565,"r DT NPL (a) NP NP NPL S WHNP NN NP president NNL NP NN of Figure 1: Example TIG derivation of an NP constituent: One left insertion (at NN) and two simultaneous right insertions (at NP). practical as compared to CFG and TSG’s O(n3 ). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.1 This has led researchers to resort to heuristic grammar extraction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some e"
P12-2022,N07-1018,0,0.0874242,"imal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011). This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007). Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a CFG rule CFG probability Base distribution: P0init NP → NPinit init init αinit c /(nNP + αc ) NPinit → NPL NPinit NPR 1.0 init NP NPinit NPinit NPinit → DT NN → DT NNinit → DTinit NN → DTinit NNinit ˜ (NP P ˜ (NP P ˜ (NP P ˜ (NP P → → → → DT NN) × (1 − βDT ) × (1 − βNN ) DT NN) × (1 − βDT ) × βNN DT NN) × βDT × (1 − βNN ) DT NN) × βDT × βNN right NPR → NP Base distribution: P0   right right right µNP × αright c /(nNP + αc ) right right NPR →  1 − µNP NPright → NPright NPR right NP init → NP"
P12-2022,P09-2012,0,0.0923256,"sident] who resigned yesterday]] Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse grammars — and low complexity — making induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009; Cohn and Blunsom, 2010; Post and Gildea, 2009). DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so A natural recurring structure here would be the structure “[N P the [N N president]]”, yet it occurs not at all in the data. TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments"
P12-2022,J95-4002,0,0.804007,"ery small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some examples of TIG derivations. Sharing the same intuitions, Shindo et al. (2011) have provided a previous attempt at combining TIG and Bayesian nonparametric principles, albeit with severe limitations. Their TIG variant (which we will refer to as TIG0 ) is highly constrained in the following ways. 1. The foot node in an auxiliary tree must be the immediate child of the root node. 2. Only one adjunction can occur at a given node. 1 This can be seen by the fact that tree-path languages under TAG are context free, whereas they are regular for TSG. (Schabes and Waters, 1995)"
P12-2022,P11-2036,0,0.571065,"s (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some examples of TIG derivations. Sharing the same intuitions, Shindo et al. (2011) have provided a previous attempt at combining TIG and Bayesian nonparametric principles, albeit with severe limitations. Their TIG variant (which we will refer to as TIG0 ) is highly constrained in the following ways. 1. The foot node in an auxiliary tree must be the immediate child of the root node. 2. Only one adjunction can occur at a given node. 1 This can be seen by the fact that tree-path languages under TAG are context free, whereas they are regular for TSG. (Schabes and Waters, 1995) 111 JJ NN* former ε NNR NPR ε NP NP ε IN of S who SBAR PP NP* SBAR WHNP WHNP NP who S (b) Figure 2: TI"
P12-2022,C98-1088,0,\N,Missing
P13-1030,P03-1054,0,0.00786232,"If we decide on adjunction, one of the available type (3) rules is chosen from a multinomial. By conditioning the probability of adjunction on varying amounts of information about the node, alternative models can easily be defined. Experiments As a proof of concept, we investigate OS TAG in the context of the classic Penn Treebank statistical parsing setup; training on section 2-21 and testing on section 23. For preprocessing, words that occur only once in the training data are mapped to the unknown categories employed in the parser of Petrov et al. (2006). We also applied the annotation from Klein and Manning (2003) that appends “-U” to each nonterminal node with a single child, drastically reducing the presence of looping unary chains. This allows the use of a coarse to fine parsing strategy (Charniak et al., 2006) in which a sentence is first parsed with the Maximum Likelihood PCFG and only constituents whose probability exceeds a cutoff of 10−4 are allowed in the OS TAG chart. Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @, record both the parent and head child of the rule. NP @NN-NP @NN-NP DT SBAR @NN-NP JJ NN A compact"
P13-1030,P06-1055,0,0.42664,"sirable for systems that seek to extract coherent and concise information from text. Introduction While it is widely accepted that natural language is not context-free, practical limitations of existing algorithms motivate Context-Free Grammars (CFGs) as a good balance between modeling power and asymptotic performance (Charniak, 1996). In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al., 2006). Another approach is to employ a more powerful grammatical formalism and devise constraints and transformations that allow use of essential efficient algorithms such as the Inside-Outside algorithm (Lari and Young, 1990) and CYK parsing. Tree-Adjoining Grammar (TAG) is a natural TAG allows a linguistically motivated treatment of the example sentences above by generating the last two sentences through modification of the first, applying operations corresponding to negation and the use of a subordinate clause. Unfortunately, the added expressive power of TAG comes with O(n6 ) time complexity fo"
P13-1030,P00-1058,0,0.0849415,"ter than or equal to 1, we 303 VP + VP always VP* S NP quickly S ⇒ NP VP VP VP always runs VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary"
P13-1030,P10-2042,0,0.103092,"re of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3 ). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. that it can be reduced to a CFG, allowing the use of traditional cubic-time algorithms. The reduction is reversible, so that the original TAG derivation can be recovered exa"
P13-1030,J95-4002,0,0.535819,"bidden in TIG. This targeted blocking of recursion has similar motivations and benefits to the approximation of CFGs with regular languages (Mohri and jan Nederhof, 2000). The following sections discuss in detail the context-free nature of OS TAG and alternative probabilistic models for its equivalent CFG form. We propose a simple but empirically effective heuristic for grammar induction for our experiments on Penn Treebank data. Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity of TAG and the algorithmic simplicity of CFGs. Schabes and Waters (1995) showed that by restricting the form of the auxiliary trees in A and the set of auxiliary trees that may adjoin at particular nodes, a TAG generates only context-free languages. The TIG restriction on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. This introduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. Furthermore, TIG disallows adjunction of left auxiliary trees on the spines of right auxiliary trees, and vice versa. This is to prevent the construction of wr"
P13-1030,N09-1062,0,0.0639577,"record the structure of the original elementary trees. A more compact CFG representation can be obtained by marking each node in each elementary tree with a signature of its subtree. This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3 ). A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al., 2009; Cohn and Blunsom, 2010). This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. that it can be reduced to a CFG, allowing the use of traditional cubic-time algorithms. The reduction is reversible, so that the original TAG deriva"
P13-1030,P11-2036,0,0.0683419,"The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary tree labeled with X by splicing the auxiliary tree in at that internal node, as pictured in Figure 2. We refer to the path"
P13-1030,P12-2022,1,0.643971,"tion combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary tree labeled with X by splicing the auxiliary tree in at that internal node, as pictured in Figure 2. We refer to the path between the root and foot no"
P13-1030,P98-1091,0,0.0874012,"e of depth greater than or equal to 1, we 303 VP + VP always VP* S NP quickly S ⇒ NP VP VP VP always runs VP quickly runs Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle) to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. The OS TAG constraint would disallow further adjunction at the bold VP node only, as it is along the spine of the auxiliary tree. complexity of TIG. Several probabilistic models have been proposed for TIG. While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. Later approaches (Shindo et al., 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. set of auxiliary trees A and the adjunction operation that governs their use. An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. An auxiliary tree with root and foot node X can be adjoined into an internal node"
P13-1030,C94-2149,0,\N,Missing
P13-1030,C98-1088,0,\N,Missing
P13-1030,N06-1022,1,\N,Missing
P13-2106,W08-2102,0,\N,Missing
P13-2106,N09-1062,0,\N,Missing
P13-2106,P10-2042,0,\N,Missing
P13-2106,P12-2022,1,\N,Missing
P13-2106,W07-0412,1,\N,Missing
P13-2106,P00-1058,0,\N,Missing
P13-2106,P98-1091,0,\N,Missing
P13-2106,C98-1088,0,\N,Missing
P13-2106,C92-2065,0,\N,Missing
P13-2106,P06-1055,0,\N,Missing
P13-2106,P09-2012,0,\N,Missing
P13-2106,P11-2036,0,\N,Missing
P13-2106,P12-1046,0,\N,Missing
P13-2106,J94-1004,1,\N,Missing
P13-2106,W97-1505,0,\N,Missing
P13-2106,N07-1018,0,\N,Missing
P15-1137,D13-1203,0,0.841381,"ge by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as a result, state-of-the-ar"
P15-1137,Q14-1037,0,0.108586,"Missing"
P15-1137,D08-1031,0,0.0330564,"1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not k"
P15-1137,W12-4503,0,0.0829107,"Missing"
P15-1137,P14-1005,0,0.290072,"Missing"
P15-1137,D13-1057,0,0.0221907,"inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which mentions in a document participate in coreference clusters, and so (all) mentions must be automatically extrac"
P15-1137,D08-1069,0,0.0483972,"et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” s"
P15-1137,N07-1030,0,0.0359765,"Missing"
P15-1137,N06-2015,0,0.277262,"Missing"
P15-1137,W04-3250,0,0.0267784,"Missing"
P15-1137,D13-1027,0,0.0563879,"nt ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as"
P15-1137,P13-1049,0,0.0594376,"Missing"
P15-1137,J13-4004,0,0.205374,"Missing"
P15-1137,P14-2005,0,0.101775,"Missing"
P15-1137,H05-1004,0,0.414638,"Missing"
P15-1137,D14-1225,0,0.389411,"Missing"
P15-1137,C02-1139,0,0.844818,"Missing"
P15-1137,P04-1020,0,0.110193,"presentations for natural language tasks (Collobert et al., 2011), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically. In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention. Adequately capturing anaphoricity information has long been thought to be an important aspect of the coreference task (see Ng (2004) and Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test se"
P15-1137,W12-4501,0,0.684515,"d Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test set (Pradhan et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Bald"
P15-1137,D09-1101,0,0.0965473,"e-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which me"
P15-1137,N13-1071,0,0.160357,"Missing"
P15-1137,D11-1014,0,0.0258548,"Missing"
P15-1137,D12-1110,0,0.0274344,"Missing"
P15-1137,J01-4004,0,0.741286,"Missing"
P15-1137,C12-1154,0,0.0202981,"Missing"
P15-1137,M95-1005,0,0.672142,"Missing"
P15-1137,P12-1040,0,0.0193087,"Missing"
P15-1137,P14-2006,0,0.333577,"Missing"
P15-1137,N07-1011,0,\N,Missing
P15-1137,W12-4502,0,\N,Missing
P15-1137,N10-1061,0,\N,Missing
P15-1137,Q14-1043,0,\N,Missing
P15-1137,D08-1067,0,\N,Missing
P15-1137,W11-1902,0,\N,Missing
P19-1084,D17-1070,0,0.0437902,"the baseline model learned to rely on the presence/absence of the bias term c, always predicting T RUE/FALSE respectively. Table 1 shows the results of our two proposed methods. As we increase the hyper-parameters α and β, our methods initially behave like the baseline, learning the training set but failing on the test set. However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Baseline & Implementation Details We use InferSent (Conneau et al., 2017) as our baseline model because it has been shown to work well on popular NLI datasets and is representative of many NLI models. We use separate BiLSTM encoders to learn vector representations of P and H.11 The vector representations are combined following Mou et al. (2016),12 and passed to an MLP classifier with one hidden layer. Our proposed 9 Detailed descriptions of these datasets can be found in Poliak et al. (2018b). 10 We leave additional NLI datasets, such as the Diverse NLI Collection (Poliak et al., 2018a), for future work. 11 Many NLI models encode P and H separately (Rockt¨aschel et"
P19-1084,S19-1028,1,0.7647,"Missing"
P19-1084,D15-1075,0,0.462661,"taset B), a letter c is appended to the hypothesis side in the T RUE examples, but not in the FALSE examples. In order to transfer well to the test set, a model that is trained on this training set needs to learn the underlying relationship—that P entails H if and only if their first letter is identical—rather than relying on the presence of c in the hypothesis side. max L1 (θ) = (1 − α) log pθ (y |P, H) θ − α log pθ,φ (y |P 0 , H) max L2 (φ) = β log pθ,φ (y |P 0 , H) Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), since it is known to contain significant annotation artifacts. We evaluate the robustness of our methods on other, target datasets. As target datasets, we use the 10 datasets investigated by Poliak et al. (2018b) in their hypothesisonly study, plus two test sets: GLUE’s diagnostic test set, which was carefully constructed to not contain hypothesis-biases (Wang et al., 2018), and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases (Gururangan et al., 2018). The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, a"
P19-1084,C18-1055,0,0.0314895,"ur methods with known biases in NLI datasets, the effects of stronger bias removal, and the possibility of fine-tuning on the target datasets. Our methodology can be extended to handle biases in other tasks where one is concerned with finding relationships between two objects, such as visual question answering, story cloze completion, and reading comprehension. We hope to encourage such investigation in the broader community. Improving model robustness Neural networks are sensitive to adversarial examples, primarily in machine vision, but also in NLP (Jia & Liang, 2017; Belinkov & Bisk, 2018; Ebrahimi et al., 2018; Heigold et al., 2018; Mudrakarta et al., 2018; Ribeiro et al., 2018; Belinkov & Glass, 2019). A common approach to improving robustness is to include adversarial examples in training (Szegedy et al., 2014; Goodfellow et al., 2015). However, this may not generalize well to new types of examples (Xiaoyong Yuan, 2017; Tramr et al., 2018). Domain-adversarial neural networks aim to increase robustness to domain change, by learning to be oblivious to the domain using gradient reversals (Ganin et al., 2016). Our methods rely similarly on gradient reversals when encouraging models to ignore dataset-"
P19-1084,P17-2097,0,0.232016,"ctic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answering (VQA) by using only the question, without access to the image, due to question biases (Zhang et al., 2016; Kafle & Kanan, 2016, 2017; Goyal et al., 2017; Agrawal et al., 2017). Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-spe"
P19-1084,P18-1225,0,0.0192681,"niques developed for textual entailment“ datasets, e.g., RTE-3, do not transfer well to other domains, specifically conversational entailment (Zhang & Chai, 2009, 2010). Bowman et al. (2015) and Williams et al. (2018) demonstrated (specifically in their respective Tables 7 and 4) how models trained on SNLI and MNLI may not transfer well across other NLI datasets like SICK. Talman & Chatzikyriakidis (2018) recently reported similar findings using many advanced deep-learning models. versarial examples that do not conform to logical rules and regularize models based on those examples. Similarly, Kang et al. (2018) incorporate external linguistic resources and use a GAN-style framework to adversarially train robust NLI models. In contrast, we do not use external resources and we are interested in mitigating hypothesisonly biases. Finally, a similar approach has recently been used to mitigate biases in VQA (Ramakrishnan et al., 2018; Grand & Belinkov, 2019). 8 Conclusion Biases in annotations are a major source of concern for the quality of NLI datasets and systems. We presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an e"
P19-1084,D18-1546,0,0.0793546,"exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answering (VQA) by using only the question, without access to the image, due to question biases (Zhang et al., 2016; Kafle & Kanan, 2016, 2017; Goyal et al., 2017; Agrawal et al., 2017). Our main goal is to determine whether our methods help a model perform well across multiple datasets by ignoring dataset-specific artifacts. In turn, we did not update the models’ parameters on other datasets. But, what if we are given different amounts of training data for a new NLI dataset? To determine if our approach is still help"
P19-1084,W19-1801,1,0.837619,"ss other NLI datasets like SICK. Talman & Chatzikyriakidis (2018) recently reported similar findings using many advanced deep-learning models. versarial examples that do not conform to logical rules and regularize models based on those examples. Similarly, Kang et al. (2018) incorporate external linguistic resources and use a GAN-style framework to adversarially train robust NLI models. In contrast, we do not use external resources and we are interested in mitigating hypothesisonly biases. Finally, a similar approach has recently been used to mitigate biases in VQA (Ramakrishnan et al., 2018; Grand & Belinkov, 2019). 8 Conclusion Biases in annotations are a major source of concern for the quality of NLI datasets and systems. We presented a solution for combating annotation biases by proposing two training methods to predict the probability of a premise given an entailment label and a hypothesis. We demonstrated that this discourages the hypothesis encoder from learning the biases to instead obtain a less biased representation. When empirically evaluating our approaches, we found that in a synthetic setting, as well as on a wide-range of existing NLI datasets, our methods perform better than the tradition"
P19-1084,N18-2017,0,0.0783303,"Missing"
P19-1084,I17-1011,0,0.118985,"Missing"
P19-1084,P18-2005,0,0.0390164,"vious to the domain using gradient reversals (Ganin et al., 2016). Our methods rely similarly on gradient reversals when encouraging models to ignore dataset-specific artifacts. One distinction is that domain-adversarial networks require knowledge of the domain at training time, while our methods learn to ignore latent artifacts and do not require direct supervision in the form of a domain label. Others have attempted to remove biases from learned representations, e.g., gender biases in word embeddings (Bolukbasi et al., 2016) or sensitive information like sex and age in text representations (Li et al., 2018). However, removing such attributes from text representations may be difficult (Elazar & Goldberg, 2018). In contrast to this line of work, our final goal is not the removal of such attributes per se; instead, we strive for more robust representations that better transfer to other datasets, similar to Li et al. (2018). Recent work has applied adversarial learning to NLI. Minervini & Riedel (2018) generate adAcknowledgements We would like to thank Aviad Rubinstein and Cynthia Dwork for discussing an earlier version of this work and the anonymous reviewers for their useful comments. Y.B. was sup"
P19-1084,P16-1204,0,0.135588,"Missing"
P19-1084,marelli-etal-2014-sick,0,0.149192,"Missing"
P19-1084,P15-2067,1,0.883981,"Missing"
P19-1084,K18-1007,0,0.0984265,"in label. Others have attempted to remove biases from learned representations, e.g., gender biases in word embeddings (Bolukbasi et al., 2016) or sensitive information like sex and age in text representations (Li et al., 2018). However, removing such attributes from text representations may be difficult (Elazar & Goldberg, 2018). In contrast to this line of work, our final goal is not the removal of such attributes per se; instead, we strive for more robust representations that better transfer to other datasets, similar to Li et al. (2018). Recent work has applied adversarial learning to NLI. Minervini & Riedel (2018) generate adAcknowledgements We would like to thank Aviad Rubinstein and Cynthia Dwork for discussing an earlier version of this work and the anonymous reviewers for their useful comments. Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative. A.P. and B.V.D were supported by JHU-HLTCOE and DARPA LORELEI. A.M.R gratefully acknowledges the support of NSF 1845664. Views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government. 885 References URL http://ww"
P19-1084,D14-1162,0,0.0849444,"Missing"
P19-1084,N16-1098,0,0.0286455,"ises will lead to a model’s degradation. 6.3 7 Fine-tuning on target datasets Related Work Biases and artifacts in NLU datasets Many natural language undersrtanding (NLU) datasets contain annotation artifacts. Early work on NLI, also known as recognizing textual entailment (RTE), found biases that allowed models to perform relatively well by focusing on syntactic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found no"
P19-1084,W18-5441,1,0.895818,"Missing"
P19-1084,P16-2022,0,0.0707933,"ng the training set but failing on the test set. However, with strong enough hyperparameters (moving towards the bottom in the tables), they perform perfectly on both the biased training set and the unbiased test set. For Method 1, stronger hyper-parameters work better. Baseline & Implementation Details We use InferSent (Conneau et al., 2017) as our baseline model because it has been shown to work well on popular NLI datasets and is representative of many NLI models. We use separate BiLSTM encoders to learn vector representations of P and H.11 The vector representations are combined following Mou et al. (2016),12 and passed to an MLP classifier with one hidden layer. Our proposed 9 Detailed descriptions of these datasets can be found in Poliak et al. (2018b). 10 We leave additional NLI datasets, such as the Diverse NLI Collection (Poliak et al., 2018a), for future work. 11 Many NLI models encode P and H separately (Rockt¨aschel et al., 2016; Mou et al., 2016; Liu et al., 2016; Cheng et al., 2016; Chen et al., 2017), although some share information between the encoders via attention (Parikh et al., 2016; Duan et al., 2018). 12 Specifically, representations are concatenated, subtracted, and multiplie"
P19-1084,P18-1176,0,0.0561621,"Missing"
P19-1084,D12-1071,0,0.092754,"are provided in Appendix A.2. For both methods, we sweep hyper-parameters α, β over {0.05, 0.1, 0.2, 0.4, 0.8, 1.0}. For each target dataset, we choose the best-performing model on its development set and report results on the test set.13 sense Inference (JOCI; Zhang et al., 2017), Multiple Premise Entailment (MPE; Lai et al., 2017),and Sentences Involving Compositional Knowledge (SICK; Marelli et al., 2014). The target datasets also include datasets recast by White et al. (2017) to evaluate different semantic phenomena: FrameNet+ (FN+; Pavlick et al., 2015), Definite Pronoun Resolution (DPR; Rahman & Ng, 2012), and Semantic Proto-Roles (SPR; Reisinger et al., 2015).9 As many of these datasets have different label spaces than SNLI, we define a mapping (Appendix A.1) from our models’ predictions to each target dataset’s labels. Finally, we also test on the Multi-genre NLI dataset (MNLI; Williams et al., 2018), a successor to SNLI.10 5 Results 5.1 Synthetic Experiments To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A). As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize t"
P19-1084,D16-1244,0,0.170648,"Missing"
P19-1084,Q15-1034,1,0.916901,"Missing"
P19-1084,P18-1079,0,0.45999,"sentations of P and H, and a classification layer, gθ , which learns a distribution over y. Typically, this is done by maximizing this discriminative likelihood directly, which will act as our baseline (Figure 1a). However, many NLI datasets contain biases that allow models to perform non-trivially well when accessing just the hypotheses (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). This allows models to leverage hypothesis-only biases that may be present in a dataset. A model may perform well on a specific dataset, without identifying whether P entails H. Gururangan et al. (2018) argue that “the bulk” of many models’ “success [is] attribute[d] to the easy examples”. Consequently, this may limit how well a model trained on one dataset would perform on other datasets that may have different artifacts. Consider an example where P and H are strings from {a, b, c}, and an environment where P enIn summary, in this paper we make the follow878 tails H if and only if the first letters are the same, as in synthetic dataset A. In such a setting, a model should be able to learn the correct condition for P to entail H.4 3.1 Our first approach is to estimate the term p(y |H) direct"
P19-1084,L18-1239,0,0.442811,"se for Granted: Mitigating Artifacts in Natural Language Inference Yonatan Belinkov13∗ Adam Poliak2∗ Stuart M. Shieber1 Benjamin Van Durme2 Alexander M. Rush1 1 2 3 Harvard University Johns Hopkins University Massachusetts Institute of Technology {belinkov,shieber,srush}@seas.harvard.edu {azpoliak,vandurme}@cs.jhu.edu Abstract many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018).3 For instance, in some datasets, negation words like “not” and “nobody” are often associated with a relationship of contradiction. As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases. Recent studies have tried to create new NLI datasets that do not contain such artifacts, but many approaches to dealing with this issue remain unsatisfactory: constructing new datasets (Sharma et al., 2018) is costly and may still result in other artifacts; filtering “easy” examples and defining a harder subset is useful for evaluation purp"
P19-1084,W18-5446,0,0.0389124,"= (1 − α) log pθ (y |P, H) θ − α log pθ,φ (y |P 0 , H) max L2 (φ) = β log pθ,φ (y |P 0 , H) Common NLI datasets Moving to existing NLI datasets, we train models on the Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), since it is known to contain significant annotation artifacts. We evaluate the robustness of our methods on other, target datasets. As target datasets, we use the 10 datasets investigated by Poliak et al. (2018b) in their hypothesisonly study, plus two test sets: GLUE’s diagnostic test set, which was carefully constructed to not contain hypothesis-biases (Wang et al., 2018), and SNLI-hard, a subset of the SNLI test set that is thought to have fewer biases (Gururangan et al., 2018). The target datasets include humanjudged datasets that used automatic methods to pair premises and hypotheses, and then relied on humans to label the pairs: SCITAIL (Khot et al., 2018), ADD-ONE-RTE (Pavlick & CallisonBurch, 2016), Johns Hopkins Ordinal Commonφ Finally, we share the classifier weights between pθ (y |P, H) and pφ,θ (y |P 0 , H). In a sense this is counter-intuitive, since pθ is being trained to unlearn bias, while pφ,θ is being trained to learn it. However, if the models"
P19-1084,K17-1004,0,0.222659,"-tuning on target datasets Related Work Biases and artifacts in NLU datasets Many natural language undersrtanding (NLU) datasets contain annotation artifacts. Early work on NLI, also known as recognizing textual entailment (RTE), found biases that allowed models to perform relatively well by focusing on syntactic clues alone (Snow et al., 2006; Vanderwende & Dolan, 2006). Recent work also found artifacts in new NLI datasets (Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018b). Other NLU datasets also exhibit biases. In ROC Stories (Mostafazadeh et al., 2016), a story cloze dataset, Schwartz et al. (2017b) obtained a high performance by only considering the candidate endings, without even looking at the story context. In this case, stylistic features of the candidate endings alone, such as the length or certain words, were strong indicators of the correct ending (Schwartz et al., 2017a; Cai et al., 2017). A similar phenomenon was observed in reading comprehension, where systems performed non-trivially well by using only the final sentence in the passage or ignoring the passage altogether (Kaushik & Lipton, 2018). Finally, multiple studies found non-trivial performance in visual question answe"
P19-1084,I17-1100,1,0.922317,"Missing"
P19-1084,W17-0907,0,0.0297645,"Missing"
P19-1084,N18-1101,0,0.316511,"Entailment (MPE; Lai et al., 2017),and Sentences Involving Compositional Knowledge (SICK; Marelli et al., 2014). The target datasets also include datasets recast by White et al. (2017) to evaluate different semantic phenomena: FrameNet+ (FN+; Pavlick et al., 2015), Definite Pronoun Resolution (DPR; Rahman & Ng, 2012), and Semantic Proto-Roles (SPR; Reisinger et al., 2015).9 As many of these datasets have different label spaces than SNLI, we define a mapping (Appendix A.1) from our models’ predictions to each target dataset’s labels. Finally, we also test on the Multi-genre NLI dataset (MNLI; Williams et al., 2018), a successor to SNLI.10 5 Results 5.1 Synthetic Experiments To examine how well our methods work in a controlled setup, we train on the biased dataset (B), but evaluate on the unbiased test set (A). As expected, without a method to remove hypothesisonly biases, the baseline fails to generalize to the test set. Examining its predictions, we found that the baseline model learned to rely on the presence/absence of the bias term c, always predicting T RUE/FALSE respectively. Table 1 shows the results of our two proposed methods. As we increase the hyper-parameters α and β, our methods initially b"
P19-1084,N06-1005,0,0.725739,"Missing"
P19-1084,D10-1074,0,0.0834512,"Missing"
P19-1084,W09-3930,0,0.0870273,"Missing"
P19-1084,Q17-1027,1,0.9051,"Missing"
P19-1084,D16-1053,0,\N,Missing
P19-1084,P17-1152,0,\N,Missing
P19-1084,D18-1007,1,\N,Missing
P19-1084,Q19-1004,1,\N,Missing
P82-1001,P81-1028,0,\N,Missing
P82-1001,P80-1013,0,\N,Missing
P83-1004,P82-1014,0,\N,Missing
P84-1027,P84-1008,0,0.0587832,"the effect of the CRP and the old HFC without any notion of the NCP. Our final version of the HFC merely requires that the parent's head features be the generalization of the head (i head) In the case of parents with one head child, this final HFC reduces to the old HFC requiring identity; it reduces to the newer one, however, in cases {like coordinate structures} where there are several head constituents. Furthermore, by utilizing an order structure on the domain of constants C, it may be possible to model that troublesome coordination phenomenon, number agreement in coordinated noun phrases [8,15]. 7. Conclusion We have approached the problem of analyzing the meaning of grammar formalisms by applying the techniques of denotational semantics taken from work on the semantics of computer languages. This has enabled us to In addition to formal insights, linguistic insights have also been gleaned from this work. First of all, we note 'that while the systems make crucial use of unification, generalization is also a well-defined notion therein and might indeed be quite useful. In fact, it was this availability of the generalization operation that suggested a simplified account of coordination"
P84-1027,J81-4003,1,0.82584,"Missing"
P84-1027,P83-1021,1,0.835293,"b 1 agr hum) = pl . . . . }) (&quot;many knights storms Cornwall&quot;, T) Applications We have used the techniques discussed here to analyze the feature systems of GPSG [15], LFG [2] and PATR-II [17]. All of them turn out to be specializations of our domain D of descriptions. Figure 1 provides a summary of two of the most critical formal properties of context-free-based grammar formalisms, the domains of their feature systems (full F~ finite elements of F, or elements of F based on nonrecursive domain equations) and whether the contextfree skeletons of grammars are constrained to be off-line paraeable [13] thereby guaranteeing decidability. 0 &lt; i &lt; r. and only then applying the rule constraints--now viewed as constraining parts of a single description. This is done by the indexing and combination steps described below. The 127 DCG-IIa PATR-II LFG GPSGb FEATURE SYSTEM full finite finite nonrec. CF SKELETON full full off-line full aDCGs based on Prolog-lI which allows cyclic terms. bHPSG, the current Hewlett-Packard implementation derived from GPSG, would come more accurately under the PATR-II classification. Figure 1: Summary of Grammar System Properties Though notational differences and some gr"
P84-1027,P84-1075,1,0.949993,"al possibilities--among them that of imposing a type discipline on the use of a formalism, with all the attendant advantages of compile-time error checking, modularity, and optimized compilation techniques for grammar rules, and that of relating grammar formalisms to other knowledge representation languages [l]. As a specific contribution of this study, we elucidate the nature of the feature systems used in augmented phrasestructure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar (GPSG) [5,15], lexical functional grammar (LFG) [2] and PATR-II [18,17]; we find that the mathematical structures developed for this purpose contain an operation of feature generalization, not available in those grammar formalisms, that can be used to give a partial account of the effect of coordination on syntactic features. Just as studies in the semantics of programming languages start by giving semantics for simple languages, so we will start with simple grammar formalisms that capture the essence of the method without an excess of obscuring detail. The present enterprise should be contrasted with studies of the generative capacity of formalisms using the tec"
P84-1027,P83-1020,0,\N,Missing
P84-1075,C82-1015,0,0.0235127,"Missing"
P84-1075,P83-1021,0,0.0346514,"Missing"
P84-1075,P84-1027,1,0.882891,"Missing"
P84-1075,P84-1008,0,\N,Missing
P85-1018,P84-1027,1,0.854058,"Missing"
P85-1018,P83-1021,0,0.184596,"Missing"
P85-1018,P84-1075,1,0.807785,"m a finite domain of atomic elements to a possibly infinite domain of directed graph structures of a certain sort. Many of tile sm'fa,',,-bast,,I grammatical formalisms explicitly dvfin,,,I ,,r pr~&quot;~Ul~p,~'.,'.l in linguistics can be characterized in this way ,,.~.. It.xi , I functional grammar (I,F(;} [5], generalizt,I I,hr:~,' ~l rlt,'l ur,. grammar (GPSG) [.1], even categorial systems such ,as M,,ntague grammar [81 and Ades/Steedman grammar Ill --,~s can several of the grammar formalisms being used in naturallanguage processing research--e.g., definite clause grammar (DCG) [9], and PATR-II [13]. Unfortunately, in moving to an infinite nonlermiual de,main, standard methods of parsing may no h,ngvr t~, applicable to the formalism. ~k~r instance, the application of techniques for preprocessing of grantmars in ,,rder t,, gain efficiency may fail to terminate, ~ in left-c,~rner and LR algorithms. Algorithms performing top-dc~wn prediction (e.g. top-down backtrack parsing, Earley's algorithm) may not terminate at parse time. Implementing backtracking regimens~useful for instance for generating parses in some particular order, say, in order of syntactic preference--is in general difficult"
P85-1018,P83-1017,1,0.359476,"in many different ways. Traditional solutions have involved limiting in some way the class of grammars that can be parsed. 2.1 In fact, by making these stringent requirements on what information is used to guide parsing, we have to a certain extent thrown the baby out with the bathwater. These formalisms were intended to free us from the tyranny of atomic nonterminal symbols, but for good performance, we are forced toward analyses putting more and more information in an atomic category feature. An example of this phenomenon can be seen in the author's paper on L R syntactic preference parsing [14]. Because the L A L R table building algorithm does not in general terminate for complex-featurebased grammar formalisms, the grammar used in that paper was a simple context-free grammar with subcategorization and gap information placed in the atomic nonterminal symbol. Limiting the formalism The limitations can be applied to the formalism by, for instance, adding a context-free &quot;backbone.&quot; If we require that a context-free subgrammar be implicit in every grammar, the subgrammar can be used for parsing and the rest of the grammar used az a filterduring or aRer parsing. This solution has been r"
P85-1018,P82-1014,0,0.0466296,"Missing"
P89-1002,E89-1032,0,0.151303,"ieve the underlying method to be more broadly applicable. A variant of our method is used in Van Noord&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of PATI~-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase-structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 3 Problems with Generators Existing Existing generation algorithms have efficiency or termination problems with respect to certain classes of grammars. We review the problems of both top-down and bottom-up regimes in this section. 3.1 Problems with Top-Down Generators Consider a naive top-down generation mechanism that takes as input the semantics to generate from and a corresponding syntactic category and builds a complete tree, top-down, left-to-right by applying rules of the gramm"
P89-1002,1988.tmi-1.12,0,0.737425,"ction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for pa"
P89-1002,J87-1005,1,0.63248,"than necessary because the distribution of store elements among the subject and complements of a verb does not check whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (Hobbs and Shieber, 1987) would not be difficult. 6.3 Postponing Lexical Choice As it stands, the generation algorithm chooses particular lexical forms on-line. This approach can lead to a certain amount of unnecessary nondeterminism. For instance, the choice of verb form might depend on syntactic features of the verb&apos;s subject available only after the subject has been generated. This nondeterminism can be eliminated by deferring lexical choice to a postprocess. The generator will yield a list of lexical items instead of a list of words. To this list a small phonological front end is applied. BUG uses such a mechanism"
P89-1002,P83-1021,1,0.865954,"iven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with r"
P89-1002,P85-1018,1,0.928086,"ortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture= our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. We will have little to say about its relation to parsing, leaving such questions for later research.1 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminMs. Furthermore, and again consistent with previous work, we assume that the nonterminals"
P89-1002,C88-2128,1,0.294441,"n Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensio"
P89-1002,C88-2150,0,0.450177,"ns, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance"
P89-1002,J81-4003,1,\N,Missing
P92-1022,P85-1011,0,0.245597,"th respect to the roles of the two auxiliary trees (by inspection), whereas the derived tree is not. By symmetry, therefore, it must be the case that the same independent derivation tree specifies the alternative derived tree in Figure 2(b). Motivation Derivations for Adjoining Constraints Already in very early work on tree-adjoining grammars (Joshi et al., 1975) constraints were allowed to be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi, 1985). As an application of this capability, we consider the remark by Quirk et al. (1985, page 517) that &quot;direction adjuncts of both goal and source can normally be used only with verbs of motion&quot;, which accounts for the distinction between the following sentences: (b) Figure 3: Derivation trees for the derived tree of Figure 2(a) according to the grammar of Figure 1 3 Adding Brockway escorted his sister to the annual cotillion. b. #Brockway resembled his sister to the annual cotillion. This could be modeled by disallowing through selective adjoining constraints the adjunction of the elementary tr"
P92-1022,P92-1022,1,0.0528665,"the foot of a modifier auxiliary tree to the top (rather than the bottom) of the node at which it adjoined (Figure 6b). Recognition and Parsing Following Schabes (1991), the LIG generated by compiling a TAG can be used as the basis for EarIcy recognition. Schabes&apos;s original method must be modified to respect the differences in compilation engendered by extended derivations. Such parsing rules, along with an extension that allows building of explicit derivation trees on-line as a basis for incremental interpretation, have been developed, and are presented in an extended version of this paper (Schabes and Shieber, 1992). In summary, the algorithm operates as a variant of Earley parsing on the corresponding LIG. The set of extended derivations can subsequently be recovered from the set of Earley items generated by the algorithm. The resultant algorithm can be further modified so as to build an explicit derivation tree incrementally as parsing proceeds; this modification, which is a novel result in its own right, allows the parsing algorithm to be used by systems that require incremental processing with respect to tree-adjoining grammars. As a proof of concept, the parsing algorithm just described was implemen"
P92-1022,C92-2066,1,0.80066,"be used as a litmus test for an appropriate definition of derivation. As such, it argues for a nonstandard, independent, notion of derivation for modifier auxiliary trees and a standard, dependent, notion for predicative trees. Brockway conjectured that Harrison wanted to escort his sister. 3.2 [Brockway conjectured that] [Harrison wanted] [to escort his sister] (5)a. Brockway wanted to try to escort his sister. b. [Srockway wanted] [to try] [to escort his sister] Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik, 1992; Schabes, 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the standard defin"
P92-1022,C90-3045,1,0.851113,"er and predicative trees is important. The standard definition of derivation is entirely appropriate for adjunction probabilities for predicative trees, but not for modifier trees. 3.3 Adding (7)a. Brockway paid for the tickets twice intentionally. b. Brockway paid for the tickets intentionally twice. We hope to address this issue in greater detail in future work on synchronous tree-adjoining grammars. Semantics 4 Finally, the formation of synchronous TAGs has been proposed to allow use of TAGs in semantic interpretation, natural language generation, and machine translation. In previous work (Shieber and Schabes, 1990), the definition of synchronous TAG derivation is given in a manner that requires multiple adjunctions at a single node. The need for such derivations follows from the fact that synchronous derivations are intended to model semantic relationships. In cases of multiple adjunction of modifier trees at a single node, the appropriate semantic relationships comprise separate modifications rather than cascaded ones, and this is reflected in the definition of synchronous TAG derivation. 6 Because of this, a parser for synchronous TAGs must recover, at least implicitly, the extended derivations of TAG"
P92-1022,H86-1020,0,\N,Missing
P92-1022,C88-2147,0,\N,Missing
P92-1022,C92-2065,0,\N,Missing
P92-1022,P93-1017,1,\N,Missing
P92-1022,P83-1021,0,\N,Missing
P92-1022,W90-0102,1,\N,Missing
pon-barry-etal-2014-eliciting,forbes-riley-etal-2008-uncertainty,0,\N,Missing
pon-barry-etal-2014-eliciting,W10-0701,0,\N,Missing
S19-1028,N18-2017,0,0.0824448,"Missing"
S19-1028,D15-1075,0,0.21254,"Missing"
S19-1028,N18-1111,0,0.0169731,"ils another (hypothesis) - contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises. For instance, such a method correctly predicted the examples in Table 1 as contradictions. As datasets may always contain biases, it is important to analyze whether, and to what extent, models are immune to or rely on known biases. Furthermore, it is important to build models that can overcome these biases. Recent work in NLP aims to build more robust systems using adversarial methods (Alzantot et al., 2018; Chen & Cardie, 2018; Belinkov & Bisk, 2018, i.a.). In particular, Elazar & Goldberg (2018) attempted to use adversarial training to remove demographic attributes from text data, with limited success. Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations (Conneau et al., 2017). The adversarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypoth"
S19-1028,P18-1225,0,0.0559285,"ude (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning to NLI (Minervini & Riedel, 2018; Kang et al., 2018), this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases. 2 Methods We consider two types of adversarial methods. In the first method, we incorporate an external classifier to force the hypothesis-encoder to ignore hypothesis-only biases. In the second method, we randomly swap premises in the training set to create noisy examples. Equal contribution 256 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 256–262 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics"
S19-1028,D17-1070,0,0.235347,"e immune to or rely on known biases. Furthermore, it is important to build models that can overcome these biases. Recent work in NLP aims to build more robust systems using adversarial methods (Alzantot et al., 2018; Chen & Cardie, 2018; Belinkov & Bisk, 2018, i.a.). In particular, Elazar & Goldberg (2018) attempted to use adversarial training to remove demographic attributes from text data, with limited success. Inspired by this line of work, we use adversarial learning to add small components to an existing and popular NLI system that has been used to learn general sentence representations (Conneau et al., 2017). The adversarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning"
S19-1028,K18-1007,0,0.120137,"ersarial ∗ techniques include (1) using an external adversarial classifier conditioned on hypotheses alone, and (2) creating noisy, perturbed training examples. In our analyses we ask whether hidden, hypothesisonly biases are no longer present in the resulting sentence representations after adversarial learning. The goal is to build models with less bias, ideally while limiting the inevitable degradation in task performance. Our results suggest that progress on this goal may depend on which adversarial learning techniques are used. Although recent work has applied adversarial learning to NLI (Minervini & Riedel, 2018; Kang et al., 2018), this is the first work to our knowledge that explicitly studies NLI models designed to ignore hypothesis-only biases. 2 Methods We consider two types of adversarial methods. In the first method, we incorporate an external classifier to force the hypothesis-encoder to ignore hypothesis-only biases. In the second method, we randomly swap premises in the training set to create noisy examples. Equal contribution 256 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 256–262 c Minneapolis, June 6–7, 2019. 2019 Association for Comput"
S19-1028,P16-2022,0,0.0833388,"filtered such that it may not contain unwanted artifacts. We apply both adversarial techniques to InferSent (Conneau et al., 2017), which serves as our general NLI architecture.2 Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks (Hochreiter & Schmidhuber, 1997). Premises and hypotheses are initially mapped (token-by-token) to Glove (Pennington et al., 2014) representations. We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following Mou et al. (2016), combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise). We use the default training hyper-parameters in the released InferSent codebase.3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set. The default settings also include stopping training either when the learning rate drops below 10−5 or after 20 epochs. In both adversarial settings, the hyper-parameters are swept through {0"
S19-1028,D14-1162,0,0.0955863,"LI. We use the standard SNLI split and report validation and test results. We also test on SNLI-hard, a subset of SNLI that Gururangan et al. (2018) filtered such that it may not contain unwanted artifacts. We apply both adversarial techniques to InferSent (Conneau et al., 2017), which serves as our general NLI architecture.2 Following the standard training details used in InferSent, we encode premises and hypotheses separately using bi-directional long short-term memory (BiLSTM) networks (Hochreiter & Schmidhuber, 1997). Premises and hypotheses are initially mapped (token-by-token) to Glove (Pennington et al., 2014) representations. We use max-pooling over the BiLSTM states to extract premise and hypothesis representations and, following Mou et al. (2016), combine the representations by concatenating their vectors, their difference, and their multiplication (element-wise). We use the default training hyper-parameters in the released InferSent codebase.3 These include setting the initial learning rate to 0.1 and the decay rate to 0.99, using SGD optimization and dividing the learning rate by 5 at every epoch when the accuracy deceases on the validation set. The default settings also include stopping train"
S19-1028,W17-0907,0,0.145469,"Missing"
S19-1028,L18-1239,0,0.442817,"othesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy. 1 A person writing something on a newspaper I A person is driving a fire truck A man is doing tricks on a skateboard I Nobody is doing tricks Table 1: Examples from SNLI’s development set that Poliak et al. (2018)’s hypothesis-only model correctly predicted as contradictions. The first line in each section is a premise and lines with I are corresponding hypotheses. The italicized words are correlated with the “contradiction” label in SNLI Introduction Popular datasets for Natural Language Inference (NLI) - the task of determining whether one sentence (premise) likely entails another (hypothesis) - contain hypothesis-only biases that allow models to perform the task surprisingly well by only considering hypotheses while ignoring the corresponding premises. For instance, such a method correctly predicted"
S19-1028,W18-5448,0,0.0525636,"Missing"
S19-1028,D18-1316,0,\N,Missing
S19-1028,D18-1002,0,\N,Missing
S19-1028,W19-1801,1,\N,Missing
W04-2323,W01-1605,0,0.022112,"nsus approach. Finally, it is worth exploring whether it is a good idea to have multiple annotations for a given corpus in the first place. Some corpora, such as the Penn Treebank, require its annotators to meet whenever there is a conflict so that the conflict can be resolved before the corpus is publicly released. Penn has now begun a Discourse Treebank as well (Creswell et al., 2003). Wiebe et al. (1999) use statistical methods to automatically correct the biases in annotations of speaker subjectivity. The corrections are then used as a basis for further conflict resolution. Carlson et al. (2001) also used conflict resolution when creating their discourse-tagged corpus. One interesting area of research would be to compare how annotators choose to resolve their conflicts compared to the different automatic approaches of finding a gold standard. It is possible that the compromises made by the annotators cannot be captured by any computational method, in which case it may be worth having all conflicts resolved manually. Acknowledgments We would like to thank Jill Nickerson for comments on an earlier draft of this paper. This work is supported in part by the National Science Foundation un"
W04-2323,J86-3001,1,0.264295,"o not exhibit crossing brackets with the appropriate gold standard. For each segmentation, we compute a non-crossing-bracket percentage by dividing the number of non-crossing-brackets by the total number of bracket pairs.   4 Empirical Methodology 4.1 Boston Directions Corpus For our empirical analysis of different gold standard approaches, we used the Boston Directions Corpus (BDC). The BDC corpus contains transcribed monologues by speakers who were instructed to perform a series of direction-giving tasks. The monologues were subsequently annotated by a group of subjects according to the Grosz and Sidner (1986) theory of discourse structure. This theory provides a foundation for hierarchical segmentation of discourses into constituent parts. Some of the subjects were experts in discourse theory and others were naive annotators. In our experiments here, we only consider the annotations from experts. 4.2 Experimental Design Our experiments were run on 12 discourses in the spontaneous speech component of the BDC. The lengths of the discourses ranged from 15 to 150 intonational phrases. Each discourse was segmented by three different annotators, resulting in 36 separate annotations. For each discourse,"
W04-2323,J97-1003,0,0.174758,"Missing"
W04-2323,P96-1038,0,0.300042,"iple judges annotating the same discourses, so as to avoid bias from using a single judge’s annotations as ground truth. Usually, for a particular discourse, these multiple annotations are unified into a single annotation, either manually by the annotators’ discussions or automatically. However, annotation unification approaches have not been formally evaluated, and although manual unification might be the best approach, it can be time-consuming. Indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation (Hearst, 1997; Hirschberg and Nakatani, 1996), because of the difficulties of obtaining consistent hierarchical annotations. In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods (Carlson et al., 2001; Marcu, 2000). There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that “hierarchical segmentation is imp"
W04-2323,P93-1020,0,0.0781918,"Missing"
W04-2323,P99-1032,0,0.0884063,"Missing"
W04-2323,J96-2004,0,\N,Missing
W04-3312,P03-2041,0,0.0806468,"first time. The relation between a TAG derivation tree and its derived tree is not a mere homomorphism. The appropriate morphism generalizing linear complete homomorphisms to allow adjunction can presumably be used to provide a bimorphism characterization of STAG as well, further unifying these strands of research. The bimorphism characterization of STSG has immediate application. First, the symmetry of the tree relations defined by an STSG is a trivial corollary. Second, it has been claimed in passing that synchronous treesubstitution grammars are “equivalent to top-down tree transducers.” (Eisner, 2003). This is clearly contravened by the distinction between B(LC, LC) and B(D, M ). Third, the bimorphism characterization of tree transducers has led to a series of composition closure results. Similar techniques may now be applicable to synchronous formalisms, where no composition results are known. For instance, the argument for the lack of composition closure in B(LCF, LCF ) (Arnold and Dauchet, 1982) may be directly applicable to a similar proof for B(LC, LC), hence for STSG; the conjecture remains for future work. + + Figure 4: Generated STSG for example bimorphism the example. The correspo"
W04-3312,J97-2003,0,0.028885,"ings of speech samples into phoneme strings, then into triphone strings, finally into words strings. (Because of nondeterminism in the process, the nondeterministic string possibilities may be represented as a single lattice. Nonetheless, the underlying abstract operation is one of string transduction.) Morphological processes can similarly be modeled as character string transductions. For this reason, weighted finite-state transducers (WFST), a general formalism for string-tostring transduction, can serve as a kind of universal formalism for representing low-level natural-language processes (Mohri, 1997). Higher-level natural-language processes can also be thought of as transductions, but on more highly structured representations, for instance trees. Semantic interpretation can be viewed as a transduction from a syntactic parse tree to a tree of semantic operations whose simplification to logical form can be viewed as a further transduction. This raises the question as to whether there is a universal formalism for NL tree transductions that can play the same role there that WFST plays for string transduction. In this paper, we investigate the formal properties of synchronous tree-substitution"
W05-0711,W04-1612,0,\N,Missing
W05-0711,W04-1606,0,\N,Missing
W05-0711,N04-4038,0,\N,Missing
W05-0711,W02-0504,0,\N,Missing
W05-0711,W02-0505,0,\N,Missing
W05-0711,P03-1006,0,\N,Missing
W05-0711,2003.mtsummit-semit.12,0,\N,Missing
W07-0402,W06-1506,0,0.699647,"e variable quantified over in the scope of the quantifier; the other adjoins over the scope to provide the quantifier and restriction. Williford (1993) explored the use of multiple adjunction (Schabes and Shieber, 1993) to achieve scope ambiguity. Since the scope components of subject and object noun phrases adjoin at the same location in the semantic tree, they give rise to a systematic ambiguity as to which dominates the other in the derived tree, reflecting the semantic scope ambiguity of the sentence; the derivation tree itself is therefore a scope neutral representation. Previous work by Han (2006a; 2006b) and Nesson and Shieber (2006) describe this approach in detail, showing its applicability to a range of semantic phenomena. A range of research has proceeded in an alternative line of using complex-feature-based TAG — rather than synchronous TAG — for TAG semantics (Kallmeyer and Romero, 2004, and work cited therein). Semantic representations are carried in features associated with nodes. Nonetheless, multicomponent TAG with separate trees for bound position and scope is used here too. However, the two trees are syntactic trees, the quantified NP tree and a vestigial S tree, respecti"
W07-0402,W06-1505,0,0.10647,"e variable quantified over in the scope of the quantifier; the other adjoins over the scope to provide the quantifier and restriction. Williford (1993) explored the use of multiple adjunction (Schabes and Shieber, 1993) to achieve scope ambiguity. Since the scope components of subject and object noun phrases adjoin at the same location in the semantic tree, they give rise to a systematic ambiguity as to which dominates the other in the derived tree, reflecting the semantic scope ambiguity of the sentence; the derivation tree itself is therefore a scope neutral representation. Previous work by Han (2006a; 2006b) and Nesson and Shieber (2006) describe this approach in detail, showing its applicability to a range of semantic phenomena. A range of research has proceeded in an alternative line of using complex-feature-based TAG — rather than synchronous TAG — for TAG semantics (Kallmeyer and Romero, 2004, and work cited therein). Semantic representations are carried in features associated with nodes. Nonetheless, multicomponent TAG with separate trees for bound position and scope is used here too. However, the two trees are syntactic trees, the quantified NP tree and a vestigial S tree, respecti"
W07-0402,W04-3321,0,0.016747,"and object noun phrases adjoin at the same location in the semantic tree, they give rise to a systematic ambiguity as to which dominates the other in the derived tree, reflecting the semantic scope ambiguity of the sentence; the derivation tree itself is therefore a scope neutral representation. Previous work by Han (2006a; 2006b) and Nesson and Shieber (2006) describe this approach in detail, showing its applicability to a range of semantic phenomena. A range of research has proceeded in an alternative line of using complex-feature-based TAG — rather than synchronous TAG — for TAG semantics (Kallmeyer and Romero, 2004, and work cited therein). Semantic representations are carried in features associated with nodes. Nonetheless, multicomponent TAG with separate trees for bound position and scope is used here too. However, the two trees are syntactic trees, the quantified NP tree and a vestigial S tree, respectively. (An example is shown in Figure 6.) In such analyses, the single-node auxiliary S tree is used for the scope part of the syntax in order to get the desired relationship between the quantifier and the quantified expression in features threaded through the derivation tree and hence in the semantics."
W07-0402,W04-3305,0,0.0185639,"place use of whwords as in Sentence 5 while still maintaining the usual semantic analysis: (5) John likes who? who(x, likes(x, john)) 5 Stranded Prepositions Sentence 6 presents a particularly challenging case for TAG semantics. The problem arises because who must contribute its bound variable, x, to the noun phrase “a picture of x”. However, in the standard syntactic analysis who substitutes into the likes tree, and in any reasonable semantic analysis, who takes scope at the root of the likes tree. (6) Who does John like a picture of? who(x, a(y, and(picture(y), of (x, y)), likes( john, y))) Kallmeyer and Scheffler (2004) propose a syntactic analysis in which “a picture of” adjoins into the syntactic tree for “likes”. The syntax for this analysis is shown for comparison in Figure 6. Associated with the syntactic analysis is a semantic analysis, which differs from ours in that all of the semantic computation is accomplished by use of a flexible set of features that are associated with 14 nodes in the syntactic trees. This analysis maintains Frank’s Constraint on Elementary Tree Minimality (CETM) if one analyzes the prepositional phrase as a complement of picture but it does so at the expense of a straightforwar"
W07-0402,C90-3045,1,0.898454,"rees in which the root and a frontier node, called the foot node and distinguished by the diacritic ∗, are labeled with the same nonterminal A. The adjunction operation involves splicing an auxiliary tree in at an internal node in an elementary tree also labeled with nonterminal A. Trees without a foot node, intended for substitution rather than adjunction into other trees, are called initial trees. Examples of the substitution and adjunction operations on sample elementary trees are shown in Figure 1. For further information, refer to Joshi and Schabes (1997). Synchronous TAG (Shieber, 1994; Shieber and Schabes, 1990) extends TAG by taking the elementary structures to be pairs of TAG trees with links between particular nodes in those trees. Derivation proceeds as in TAG except that all operations must be paired. That is, a tree can only be substituted or adjoined at a node if its pair is simultaneously substituted or adjoined at a linked node. We notate the links by using boxed indices i marking linked nodes. (a) N P John john NP VP e e (b) t Adv V P∗ !t, t&quot; t∗ S NP S Mary mary 1 N P↓ 3 V P 2 V t VP Adv V 1 2 !e, t&quot; e↓ 3 !t, t&quot; VP apparently apparently (c) t John apparently likes Mary 4 john apparently NP"
W07-0402,J94-1004,1,\N,Missing
W07-0402,W90-0102,1,\N,Missing
W07-0412,C90-3001,0,0.438002,"dinner, the modifiers recently and by preparing dinner adjoining at the VP and S links, respectively. Expressing this relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the STAG (as described at the end of Section 5). The basic idea of using synchronous TAG for machine translation dates from the original definition (Shieber and Schabes, 1990), and has been pursued by several researchers (Abeille et al., 1990; Dras, 1999; Prigent, 1994; Palmer et al., 1999), but only recently in its probabilistic form (Nesson et al., 2006). The directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted. It leads to the possibility of making direct use of bilingual dictionary material in a statistical machine translation system.5 But even if the formalism is not used in that way, there is import to the fact that its expressivity matches that thought by lexicographers of the last several millennia to be needed for capturing the translation"
W07-0412,J90-2002,0,0.292486,"on, pages 88–95, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics tries hacqua / wateri10 and hdormire / sleepi191 .1 This property doesn’t distinguish among any of the formal means for capturing these direct lexical relationships. Finite-state string transducers naturally capture these simple relationships, but so do more (and less) expressive formalisms. Simple word-by-word replacement is not a viable translation method; this was noted even as early as Weaver’s famous memorandum (Weaver, 1955). Systems based on word-to-word lexicons, such as the IBM systems (Brown et al., 1990; Brown et al., 1993), incorporate further devices that allow reordering of words (a “distortion model”) and ranking of alternatives (a monolingual language model). Together, these allow for the possibility that The Word Principle: Words translate differently when adjacent to other words. This property of the translation relation is patently true. Even a word-to-word system with the ability to reorder words and rank alternatives has obvious limitations, which have motivated the machine translation research community toward progressively more expressive formalisms. Again, we see precedent for t"
W07-0412,J93-2003,0,0.0134607,"ochester, New York, April 2007. 2007 Association for Computational Linguistics tries hacqua / wateri10 and hdormire / sleepi191 .1 This property doesn’t distinguish among any of the formal means for capturing these direct lexical relationships. Finite-state string transducers naturally capture these simple relationships, but so do more (and less) expressive formalisms. Simple word-by-word replacement is not a viable translation method; this was noted even as early as Weaver’s famous memorandum (Weaver, 1955). Systems based on word-to-word lexicons, such as the IBM systems (Brown et al., 1990; Brown et al., 1993), incorporate further devices that allow reordering of words (a “distortion model”) and ranking of alternatives (a monolingual language model). Together, these allow for the possibility that The Word Principle: Words translate differently when adjacent to other words. This property of the translation relation is patently true. Even a word-to-word system with the ability to reorder words and rank alternatives has obvious limitations, which have motivated the machine translation research community toward progressively more expressive formalisms. Again, we see precedent for the move in bilingual"
W07-0412,W00-2008,0,0.0153919,"mphasize that the advantage that we find for STAGs in displaying well the necessary properties for statistical machine translation systems implicit in bilingual dictionaries is not that they are able to code efficiently all generalizations about the translation relation. Indeed, STAG is not able to do so (Shieber, 1994), which has motivated more expressive extenX X∗ X X X∗ X X X X X∗ X X X∗ X∗ X X X X X X X X X wS wT wS wT wS wT X∗ S S X X S S ! S ! S X X X X ! ! ! ! Figure 2: A normal form for synchronous tree-insertion grammar. (Reproduced from Nesson et al. (2006).) sions of the formalism (Chiang et al., 2000). For example, STAG might express the construction relation hattraversare QC di corsa / run across STi and similar relations between Italian verbs of direction with modifiers of motion and English verbs of motion with directional modifiers. However, the generalization that directional verbs with motion-manner adverbials translate as motion-manner verbs with directional adverbials is not expressed or expressible by STAG. Each instance of the generalization must be specified or learned separately.6 Nonetheless, we are content (in the spirit of statistical MT) to have lots of such particular case"
W07-0412,P05-1033,0,0.752199,"is plenty of evidence for the Phrase Principle and the Construction Principle. At the token level, the general interest in socalled syntax-aware statistical MT approaches is itself evidence that researchers believe that the tokens accounting for the performance gap in current systems based on the Word and Phrase Principles transcend those principles in some way, presumably because they manifest the Construction Principle.3 Only time will tell if such syntax-aware systems are able to display performance improvements over their nonstructural alternatives. Successful experiments such as those of Chiang (2005) using synchronous context-free grammar are a good first start.4 2.3 Heritage of the construction principle We have argued that a formalism expressive enough to model the translation relation implicit in bilingual dictionaries must be based on relations over constructions, the primitive relations found in such bilingual dictionaries and founded by the Construction Principle. The fundamentality of this principle is evidenced by the fact that it has informed bilingual dictionaries literally since their inception. The earliest known bilingual dictionaries are those incorporated in the so-called l"
W07-0412,P99-1011,0,0.462385,"recently and by preparing dinner adjoining at the VP and S links, respectively. Expressing this relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the STAG (as described at the end of Section 5). The basic idea of using synchronous TAG for machine translation dates from the original definition (Shieber and Schabes, 1990), and has been pursued by several researchers (Abeille et al., 1990; Dras, 1999; Prigent, 1994; Palmer et al., 1999), but only recently in its probabilistic form (Nesson et al., 2006). The directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted. It leads to the possibility of making direct use of bilingual dictionary material in a statistical machine translation system.5 But even if the formalism is not used in that way, there is import to the fact that its expressivity matches that thought by lexicographers of the last several millennia to be needed for capturing the translation relation; th"
W07-0412,W05-0833,0,0.0153569,"onstellation of properties—expressivity, trainability, and efficiency—that make it a good candidate at a conceptual level for founding a machine translation system. What would such a system look like? It would start with a universal normal form subgrammar serving as the robust “backoff” relation to which additional more articulated bilingual material could be added in the form of additional tree pairs. These tree pairs might be manually generated, automatically reconstructed from repurposed bilingual dictionaries, or automatically induced from aligned bilingual treebanks (Groves et al., 2004; Groves and Way, 2005) or even unannotated bilingual corpora (Chiang, 2005). In fact, since all of these sources of data yield interacting tree pairs, more than one of 94 these techniques might be used. In any case, further training would automatically determine the interactions of these information sources. The conclusions of this paper are admittedly programmatic. But plausible arguments for a program of research may be just the thing for clarifying a research direction and even promoting its pursual. In that sense, this paper can be read as a kind of manifesto for the use of probabilistic synchronous TAG as a su"
W07-0412,C04-1154,0,0.0407391,"rmalism possesses a constellation of properties—expressivity, trainability, and efficiency—that make it a good candidate at a conceptual level for founding a machine translation system. What would such a system look like? It would start with a universal normal form subgrammar serving as the robust “backoff” relation to which additional more articulated bilingual material could be added in the form of additional tree pairs. These tree pairs might be manually generated, automatically reconstructed from repurposed bilingual dictionaries, or automatically induced from aligned bilingual treebanks (Groves et al., 2004; Groves and Way, 2005) or even unannotated bilingual corpora (Chiang, 2005). In fact, since all of these sources of data yield interacting tree pairs, more than one of 94 these techniques might be used. In any case, further training would automatically determine the interactions of these information sources. The conclusions of this paper are admittedly programmatic. But plausible arguments for a program of research may be just the thing for clarifying a research direction and even promoting its pursual. In that sense, this paper can be read as a kind of manifesto for the use of probabilistic"
W07-0412,N03-1017,0,0.00556959,"limitations, which have motivated the machine translation research community toward progressively more expressive formalisms. Again, we see precedent for the move in bilingual dictionaries, which provide phrasal translations in addition to simple word translations: hby and large / nel complessoi86 , hfull moon / luna pienai406 . The insight at work here is The Phrase Principle: Phrases (not words) translate differently when adjacent to other phrases. And again, we see this insight informing statistical machine translation systems, for instance, in the phrase-based approaches of Och (2003) and Koehn et al. (2003). These two principles, while true, do not exhaust the insights implicit in the structure of bilingual dictionaries. A fuller view is accomplished by moving from words and phrases to constructions. 2.1 The construction principle The phenomenon that underlies the use of synchronous grammars for MT is simply this: 1 Throughout, we notate entries in HCICD with the notation hentry form / translation formi page , providing the Italian and English forms, along with the page number of the cited entry. 89 The Construction Principle: Words and phrases translate differently in construction with other wo"
W07-0412,2006.amta-papers.15,1,0.880231,"s relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the STAG (as described at the end of Section 5). The basic idea of using synchronous TAG for machine translation dates from the original definition (Shieber and Schabes, 1990), and has been pursued by several researchers (Abeille et al., 1990; Dras, 1999; Prigent, 1994; Palmer et al., 1999), but only recently in its probabilistic form (Nesson et al., 2006). The directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted. It leads to the possibility of making direct use of bilingual dictionary material in a statistical machine translation system.5 But even if the formalism is not used in that way, there is import to the fact that its expressivity matches that thought by lexicographers of the last several millennia to be needed for capturing the translation relation; this fact indicates at least that STAG’s use as a substrate for MT systems may be a promising research dir"
W07-0412,H05-1101,0,0.0594961,"se, too much might be made of this question of computational complexity. The algorithms used for decoding of statistical MT systems almost universally incorporate heuristics for efficiency reasons, even those that are polynomial. One reviewer notes that “the admittedly perplexing reality is that exponential decoders run much faster than polynomial ones, pre93 Here, the STAG situation is equivocal. Bilingual parsing of a corpus relative to an STAG is a necessary first step in parameter training. The recognition problem for STAG, like that for synchronous context-free grammar (SCFG) is NP-hard (Satta and Peserico, 2005). Under appropriate restrictions of binarizability, SCFG parsing can be done in O(n6 ) time, doubling the exponent of CFG parsing. Similarly, STAG parsing under suitable limitations (Nesson et al. (2005)) can be done in O(n12 ) time doubling the exponent of monolingual TAG parsing. On the positive side, recent work exploring the automatic binarization of synchronous grammars (Zhang et al., 2006) has indicated that non-binarizable constructions seem to be relatively rare in practice. Nonetheless, such a high-degree polynomial makes the complete algorithm impractical. Nesson et al. (2006) use sy"
W07-0412,J95-4002,0,0.0167275,"rizability, SCFG parsing can be done in O(n6 ) time, doubling the exponent of CFG parsing. Similarly, STAG parsing under suitable limitations (Nesson et al. (2005)) can be done in O(n12 ) time doubling the exponent of monolingual TAG parsing. On the positive side, recent work exploring the automatic binarization of synchronous grammars (Zhang et al., 2006) has indicated that non-binarizable constructions seem to be relatively rare in practice. Nonetheless, such a high-degree polynomial makes the complete algorithm impractical. Nesson et al. (2006) use synchronous treeinsertion grammar (STIG) (Schabes and Waters, 1995) rather than STAG for this very reason. STIG retains the ability to express a universal normal form, while allowing O(n6 ) bilingual parsing. (Again, limitations on the formalism are required to achieve this complexity.) Even this complexity may be too high. Methods such as those of Chiang (2005) have been proposed for further reducing the complexity of SCFG parsing; they may be applicable to STIG (and STAG) parsing as well. The STIG formalism can be shown to be expressively equivalent to synchronous tree-substitution grammar (STSG) and even SCFG. Does this vitiate the argument for STIG as a n"
W07-0412,C90-3045,1,0.647787,"odification, as in Eli recently took his father by surprise by preparing dinner, the modifiers recently and by preparing dinner adjoining at the VP and S links, respectively. Expressing this relation in other frameworks involves either limiting its scope (for instance, to particular objects and intervening material), expanding its scope (by separating the translations of the contiguous portions of the constructions), or mimicking the structure of the STAG (as described at the end of Section 5). The basic idea of using synchronous TAG for machine translation dates from the original definition (Shieber and Schabes, 1990), and has been pursued by several researchers (Abeille et al., 1990; Dras, 1999; Prigent, 1994; Palmer et al., 1999), but only recently in its probabilistic form (Nesson et al., 2006). The directness with which the formalism follows from the structure of bilingual dictionaries has not to our knowledge been previously noted. It leads to the possibility of making direct use of bilingual dictionary material in a statistical machine translation system.5 But even if the formalism is not used in that way, there is import to the fact that its expressivity matches that thought by lexicographers of the"
W07-0412,W04-3312,1,0.853931,"s their arguments) essentially implementing a kind of STSG. However, because modifiers can make these trees discontiguous, they augment the model by allowing for free insertion of modifiers in certain locations. One view of this is as an implementation of the principle that motivates adjoining, without using adjoining itself. Thus, systems that are designed to take account of the principles adduced in this paper are likely to be implementing aspects of STAG implicitly, even if not explicitly. Similarly, recent research is beginning to unify synchronous grammar formalisms and tree transducers (Shieber, 2004; Shieber, 2006). There may well be equally direct transducer formalisms that elegantly express construction-based translation relations. This would not be a denial of the present thesis but a happy acknowledgment of it. 6 Conclusion We have argued that probabilistic synchronous TAG or some closely related formalism possesses a constellation of properties—expressivity, trainability, and efficiency—that make it a good candidate at a conceptual level for founding a machine translation system. What would such a system look like? It would start with a universal normal form subgrammar serving as th"
W07-0412,E06-1048,1,0.840285,"ts) essentially implementing a kind of STSG. However, because modifiers can make these trees discontiguous, they augment the model by allowing for free insertion of modifiers in certain locations. One view of this is as an implementation of the principle that motivates adjoining, without using adjoining itself. Thus, systems that are designed to take account of the principles adduced in this paper are likely to be implementing aspects of STAG implicitly, even if not explicitly. Similarly, recent research is beginning to unify synchronous grammar formalisms and tree transducers (Shieber, 2004; Shieber, 2006). There may well be equally direct transducer formalisms that elegantly express construction-based translation relations. This would not be a denial of the present thesis but a happy acknowledgment of it. 6 Conclusion We have argued that probabilistic synchronous TAG or some closely related formalism possesses a constellation of properties—expressivity, trainability, and efficiency—that make it a good candidate at a conceptual level for founding a machine translation system. What would such a system look like? It would start with a universal normal form subgrammar serving as the robust “backof"
W07-0412,N06-1033,0,0.0326759,"ingual parsing of a corpus relative to an STAG is a necessary first step in parameter training. The recognition problem for STAG, like that for synchronous context-free grammar (SCFG) is NP-hard (Satta and Peserico, 2005). Under appropriate restrictions of binarizability, SCFG parsing can be done in O(n6 ) time, doubling the exponent of CFG parsing. Similarly, STAG parsing under suitable limitations (Nesson et al. (2005)) can be done in O(n12 ) time doubling the exponent of monolingual TAG parsing. On the positive side, recent work exploring the automatic binarization of synchronous grammars (Zhang et al., 2006) has indicated that non-binarizable constructions seem to be relatively rare in practice. Nonetheless, such a high-degree polynomial makes the complete algorithm impractical. Nesson et al. (2006) use synchronous treeinsertion grammar (STIG) (Schabes and Waters, 1995) rather than STAG for this very reason. STIG retains the ability to express a universal normal form, while allowing O(n6 ) bilingual parsing. (Again, limitations on the formalism are required to achieve this complexity.) Even this complexity may be too high. Methods such as those of Chiang (2005) have been proposed for further redu"
W07-0412,W06-1628,0,\N,Missing
W07-0412,W90-0102,1,\N,Missing
W08-2310,W06-1506,0,0.0124875,"handled elegantly through synchronization. This research has provided syntax and semantics for such diverse and complex linguistic phenomena as relative clauses1 (Han, 2006; 1 Both published analyses fail to predict all available scope readings for some sentences. This paper presents a relative Stuart Shieber School of Engineering and Applied Sciences Harvard University shieber@seas.harvard.edu Nesson and Shieber, 2006), nested quantifiers (Nesson and Shieber, 2006), wh-questions (Nesson and Shieber, 2006; Nesson and Shieber, 2007), in-situ wh-questions (Nesson and Shieber, 2007), it-clefts (Han and Hedberg, 2006), and topicalization (Nesson and Shieber, 2007). In these analyses the constraints of the tree-local or set-local MCTAG formalisms have played a critical role in permitting the available semantic readings while ruling out the unavailable ones. This research has demonstrated the value of synchronous grammars for characterizing the syntactic-semantic interface by showing how much more could be done using this simple mechanism than previously thought. The analysis of nested quantifiers presented by Nesson and Shieber (2006) exemplifies this. Consider the sentence: (1) Two politicians courted ever"
W08-2310,W06-1505,0,0.0215374,"s analyses. 1 Introduction As first described by Shieber and Schabes (1990), synchronous tree-adjoining grammar (STAG) can be used to provide a semantics for a TAG syntactic analysis by taking the tree pairs to represent a syntactic analysis synchronized with a semantic analysis. Recent work has used the STAG formalism to demonstrate that many of the cases in which syntactic and semantic derivations appeared to be divergent could be handled elegantly through synchronization. This research has provided syntax and semantics for such diverse and complex linguistic phenomena as relative clauses1 (Han, 2006; 1 Both published analyses fail to predict all available scope readings for some sentences. This paper presents a relative Stuart Shieber School of Engineering and Applied Sciences Harvard University shieber@seas.harvard.edu Nesson and Shieber, 2006), nested quantifiers (Nesson and Shieber, 2006), wh-questions (Nesson and Shieber, 2006; Nesson and Shieber, 2007), in-situ wh-questions (Nesson and Shieber, 2007), it-clefts (Han and Hedberg, 2006), and topicalization (Nesson and Shieber, 2007). In these analyses the constraints of the tree-local or set-local MCTAG formalisms have played a critic"
W08-2310,J87-1005,1,0.501821,"potential to handle hard cases 3 But see the study by VanLehn (1978) for a contrary view on which this reading is merely dispreferred. We return to this issue later. such as control verbs, relative clauses, and inverse linking, while maintaining the simplicity of previous STAG syntax-semantics analyses. 2 Difficult Cases for STAG Syntax and Semantics The elegance of the STAG analysis is encouraging. However, certain cases seem to require more flexibility than the previous analysis, couched in tree- and set-local MCTAG, provides. For instance, as mentioned above, some accounts (VanLehn, 1978; Hobbs and Shieber, 1987) indicate that a fifth scope reading is possible in sentences like sentence (1). We illustrate the limitations of STAG with two further examples involving the semantics of control verbs and relative clauses. 2.1 Control Verbs Consider the sentence: (2) Every boy always wants to eat some food. Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Formalisms Tübingen, Germany. June 6-8, 2008. Synchronous Vector TAG for Syntax and Semantics !e, t&quot; N N∗ N ! ! N S/N P↓ 2 and !e, t&quot;∗ !e, t&quot;↓ 1 se N↓ 1 S/N P !e, t&quot; λ z 1 N P↓ 1 V P V !e, t&quot; N who N↓ 1 S !e, t&quot; N P !e,"
W08-2310,P96-1016,0,0.0430717,"Unlike set-local and tree-local MCTAG which are known to be NP-hard to parse (Søgaard et al., 2007), lexicalized V-TAG can be parsed in polynomial time (Rambow, 1994; Kallmeyer, 2007). Although SV-TAG recognition is also NP-hard due to the complexity introduced by synchronization, related work on synchronous unordered vector grammar with dominance links suggests that for a given simple SV-TAG grammar a polynomial time tree-to-forest translation algorithm may exist that permits a parse of the syntax of a sentence to be translated into the forest of corresponding semantic trees (or vice versa) (Rambow and Satta, 1996). As with all synchronous-grammar-based analyses, the derivation tree still provides an underspecified representation for the semantics. 3.1 The Derivation Tree In the STAG model of syntax and semantics the derivation tree is the interface between the two as well as the means for capturing underspecification in the semantics. An SV-TAG permits greater freedom for divergence between syntax and semantics because rather than requiring all trees in a set to be synchronized, in SV-TAG only the foundation trees are synchronized. As a result, underspecification in the SV-TAG model extends beyond mult"
W08-2310,C90-3045,1,0.815366,"be divergent could be handled elegantly through synchronization. This research has provided syntax and semantics for diverse and complex linguistic phenomena. However, certain hard cases push the STAG formalism to its limits, requiring awkward analyses or leaving no clear solution at all. In this paper a new variant of STAG, synchronous vector TAG (SV-TAG), and demonstrate that it has the potential to handle hard cases such as control verbs, relative clauses, and inverse linking, while maintaining the simplicity of previous STAG syntax-semantics analyses. 1 Introduction As first described by Shieber and Schabes (1990), synchronous tree-adjoining grammar (STAG) can be used to provide a semantics for a TAG syntactic analysis by taking the tree pairs to represent a syntactic analysis synchronized with a semantic analysis. Recent work has used the STAG formalism to demonstrate that many of the cases in which syntactic and semantic derivations appeared to be divergent could be handled elegantly through synchronization. This research has provided syntax and semantics for such diverse and complex linguistic phenomena as relative clauses1 (Han, 2006; 1 Both published analyses fail to predict all available scope re"
W08-2310,W07-0402,1,0.801282,"the cases in which syntactic and semantic derivations appeared to be divergent could be handled elegantly through synchronization. This research has provided syntax and semantics for such diverse and complex linguistic phenomena as relative clauses1 (Han, 2006; 1 Both published analyses fail to predict all available scope readings for some sentences. This paper presents a relative Stuart Shieber School of Engineering and Applied Sciences Harvard University shieber@seas.harvard.edu Nesson and Shieber, 2006), nested quantifiers (Nesson and Shieber, 2006), wh-questions (Nesson and Shieber, 2006; Nesson and Shieber, 2007), in-situ wh-questions (Nesson and Shieber, 2007), it-clefts (Han and Hedberg, 2006), and topicalization (Nesson and Shieber, 2007). In these analyses the constraints of the tree-local or set-local MCTAG formalisms have played a critical role in permitting the available semantic readings while ruling out the unavailable ones. This research has demonstrated the value of synchronous grammars for characterizing the syntactic-semantic interface by showing how much more could be done using this simple mechanism than previously thought. The analysis of nested quantifiers presented by Nesson and Shie"
W08-2310,W90-0102,1,\N,Missing
W16-0528,D14-1179,0,0.034134,"Missing"
W16-0528,W11-2838,0,0.0404598,"ets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work t"
W16-0528,W12-2006,0,0.0311,"y (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full ge"
W16-0528,W16-0506,0,0.0258345,"rposes here, we refer to all such edits as “grammatical” errors. 2 The 2016 Shared Task also included a probabilistic esti243 Evaluation is at the sentence level, but the paragraph-level context for each sentence is also provided. The paragraphs, themselves, are shuffled so that full article context is not available. A coarse academic field category is also provided for each paragraph. Our models described below do not make use of the paragraph context nor the field category, and they treat each sentence independently. Further information about the task is available in the Shared Task report (Daudaravicius et al., 2016). 3 Related Work While this is the first year for a shared task focusing on sentence-level binary error identification, previous work and shared tasks have focused on the related tasks of intra-sentence identification and correction of errors. Until recently, standard handannotated grammatical error datasets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, per"
W16-0528,W14-1702,0,0.0194439,"University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014). 4 Models We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of errors, unlike some previous work for t"
W16-0528,D14-1181,1,0.039554,"y separate classifiers for human-differentiated subsets of errors, unlike some previous work for the related task of grammatical error correction. We next introduce two approaches for the task of sentence-level grammatical error identification: A binary classifier and a sequence-to-sequence model that is trained for correction but can also be used for identification as a side-effect. 4.1 Baseline Convolutional Neural Net To establish a baseline, we follow past work that has shown strong performance with convolutional neural nets (CNNs) across various domains for sentence-level classification (Kim, 2014; Zhang and Wallace, 2015). We utilize the one-layer CNN architecture of Kim (2014) with the publicly available5 word vectors trained on the Google News dataset, which contains about 100 billion words (Mikolov et al., 2013). We experiment with keeping the word vectors static (CNN- STATIC) and fine-tuning the vectors (CNN- NONSTATIC). The CNN models only have access to sentence-level labels and are not given correction-level annotations. 4.2 Encoder-Decoder While it may seem more natural to utilize models trained for binary prediction, such as the aforementioned CNN, or for example, the recurre"
W16-0528,D15-1166,0,0.0472903,"Missing"
W16-0528,P15-2097,0,0.0078628,"learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014). 4 Models We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of err"
W16-0528,W13-3601,0,0.0412932,"m for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et al., 2012). The CoNLL-2013 Shared Task (Ng et al., 2013)3 focused on the correction of a limited set of five error types in essays by second-language learners of English at the National University of Singapore. The follow-up CoNLL-2014 Shared Task (Ng et al., 2014)4 focused on the full generation task of correcting all errors in essays by second-language learners. As with machine translation (MT), evaluation of mation track. We leave for future work the adaptation of our approach to that task. 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area,"
W16-0528,W14-1701,0,0.22087,"ntains a “grammatical” error, broadly construed1 ). The dataset consists of sentences taken from academic articles annotated with corrections by professional editors. Annotations are described via insertions and deletions, which are marked with start and end tags. Tokens to be deleted are surrounded with the deletion start tag <del> and the deletion end tag </del> and tokens to be inserted are surrounded with the insertion start tag <ins> and the insertion end tag </ins>. Replacements (as shown in Figure 1) are represented as deletioninsertion pairs. Unlike the related CoNLL-2014 Shared Task (Ng et al., 2014) data, errors are not labeled with fine-grained types (article or determiner error, verb tense error, etc.). More formally, we assume a vocabulary V of natural language word types (some of which have orthographic errors) and a set Q = {<ins>, </ins>, <del>, </del>} of annotation tags. Given a sentence s = [s1 , . . . , sI ], where si ∈ V is the i-th token of the sentence of length I, we seek to predict whether or not the gold, annotated target sentence t = [t1 , . . . , tJ ], where tj ∈ Q ∪ V is the j-th token of the annotated sentence of length J, is identical to s. We are given both s and t"
W16-0528,P11-1094,0,0.031188,"they treat each sentence independently. Further information about the task is available in the Shared Task report (Daudaravicius et al., 2016). 3 Related Work While this is the first year for a shared task focusing on sentence-level binary error identification, previous work and shared tasks have focused on the related tasks of intra-sentence identification and correction of errors. Until recently, standard handannotated grammatical error datasets were not available, complicating comparisons and limiting the choice of methods used. Given the lack of a large hand-annotated corpus at the time, Park and Levy (2011) demonstrated the use of the EM algorithm for parameter learning of a noise model using error data without corrections, performing evaluation on a much smaller set of sentences hand-corrected by Amazon Mechanical Turk workers. More recent work has emerged as a result of a series of shared tasks, starting with the Helping Our Own (HOO) Pilot Shared Task run in 2011, which focused on a diverse set of errors in a small dataset (Dale and Kilgarriff, 2011), and the subsequent HOO 2012 Shared Task, which focused on the automated detection and correction of preposition and determiner errors (Dale et"
W16-0528,W14-1704,0,0.00810069,". 3 http://www.comp.nus.edu.sg/˜nlp/ conll13st.html 4 http://www.comp.nus.edu.sg/˜nlp/ conll14st.html the full generation task is still an open research area, but a subsequent human evaluation ranked the output from the CoNLL-2014 Shared Task systems (Napoles et al., 2015). The system of Felice et al. (2014) ranked highest, utilizing a combination of a rule-based system and phrase-based MT, with re-ranking via a large web-scale language model. Of the non-MT based approaches, the IllinoisColumbia system was a strong performer, combining several classifiers trained for specific types of errors (Rozovskaya et al., 2014). 4 Models We use an end-to-end approach that does not have separate components for candidate generation or reranking that make use of hand-tuned rules or explicit syntax, nor do we employ separate classifiers for human-differentiated subsets of errors, unlike some previous work for the related task of grammatical error correction. We next introduce two approaches for the task of sentence-level grammatical error identification: A binary classifier and a sequence-to-sequence model that is trained for correction but can also be used for identification as a side-effect. 4.1 Baseline Convolutional"
W16-0708,P14-1005,0,0.0437352,"Missing"
W16-0708,D13-1203,0,0.403498,"gest some directions for further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models do not need to contend with the intricacies of the various systems in the pipeline, Accordingly, in this paper we consider systems that attempt to move beyond OntoNotes by making coreference predictions without access to pipelined features, using only a document’s words and sentence"
W16-0708,N06-2015,0,0.0359464,"upstream models, and because we might expect them to generalize better to situations in which upstream features are unavailable or unreliable. Through quantitative and qualitative error analysis we identify what sorts of cases are particularly difficult for such models, and suggest some directions for further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models"
W16-0708,D14-1181,0,0.00466964,"Missing"
W16-0708,J13-4004,0,0.0210064,"ent antecedents. We will moreover require that in making these antecedent predictions no pipelined features are used. In particular, we will assume that “unpipelined” systems have access only to a document’s mention-boundaries, to the sets C(x) for each x ∈ X (when training), to the words in each document, and to the document’s sentence boundaries. Whereas recent coreference systems typically make use of syntactic information, named-entity tags, word-lists containing type information (e.g., number, gender, animacy), and speaker information (Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014; Lee et al., 2013), given the aforementioned restrictions, the only common coreference features that remain legal are word-based features and “distance” features. Distance features are typically defined in terms of the number of words, mentions, or sentences between a mention and a candidate antecedent (Durrett and Klein, 2013), and such features can presumably be defined accurately in many settings without the use of upstream systems. 54 2 Models We will use a very simple mention-ranking style model for our antecedent prediction. Mentionranking models make use of a scoring function s(x, y) that scores the comp"
W16-0708,Q15-1029,0,0.0136016,"eference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models do not need to contend with the intricacies of the various systems in the pipeline, Accordingly, in this paper we consider systems that attempt to move beyond OntoNotes by making coreference predictions without access to pipelined features, using only a document’s words and sentence boundaries. In the hopes of shedding light on whether this is a viable strategy, we consider, as a cas"
W16-0708,W12-4501,0,0.0254827,"eresting because they allow for side-stepping the intricacies of upstream models, and because we might expect them to generalize better to situations in which upstream features are unavailable or unreliable. Through quantitative and qualitative error analysis we identify what sorts of cases are particularly difficult for such models, and suggest some directions for further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors thro"
W16-0708,P15-1137,1,0.780594,"further improvement. 1 Introduction Most recent approaches to identity coreference resolution rely on a set of pipelined features generated by relatively accurate upstream systems. For instance, the CoNLL 2012 coreference datasets (Pradhan et al., 2012), which are based on the OntoNotes corpus (Hovy et al., 2006), make available both gold and predicted parse, part-of-speech, and namedentity information for each sentence in the corpus. While recent systems have managed to improve on the state of the art in coreference resolution by taking advantage of such information (Durrett and Klein, 2013; Wiseman et al., 2015; Bj¨orkelund and Kuhn, 2014; Fernandes et al., 2012; Martschat and Strube, 2015), we might be interested in systems that do not use pipelined features for several reasons: first, pipelined systems are known to accumulate errors throughout the stages of the pipeline. Second, unpipelined models do not need to contend with the intricacies of the various systems in the pipeline, Accordingly, in this paper we consider systems that attempt to move beyond OntoNotes by making coreference predictions without access to pipelined features, using only a document’s words and sentence boundaries. In the ho"
W16-0708,W12-4502,0,\N,Missing
W17-6204,W08-2301,0,0.398809,"habes (1990) and Williford (1993)) and an e-rooted reference tree. Non-quantificational DPs like Noah have a degenerate scope tree t∗ that does not modify the derived tree, so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion, 2008), or used the more constrained STAG plus adjustments, such as using de Bruijn indices in the semantics (Nesson, 2009), creating multiple reflexive trees (Storoshenko et al., 2008), or operating at multiple links in the derivation (Frank, 2008). No STAG approach to our knowledge has captured both reflexives and reciprocals. Our analysis seeks to fill this void by showing that both kinds of anaphors can be captured uniformly in STAG. To achieve this, we simplify and generalize one previous analysis of reflexives in STAG, namely that of Frank (2008), so it can apply to reciprocals and a variety o"
W17-6204,W12-4601,0,0.0340284,"Missing"
W17-6204,W08-2303,0,0.143549,"ng to fundamental syntactic and semantic constraints. We also generalize his analysis to apply to both reflexives and reciprocals. Our full analysis is described in Section 2. We demonstrate the power of this approach in Section 3 using the examples of cataphoric constructions and anaphors as arguments of object control verbs. Analogous derivations capture ditransitive verbs in which the reflexive can be coindexed with the subject or object, though space limitations preclude their inclusion. To handle non-local cases, we avail ourselves of a version of delayed locality, originally proposed by Chiang and Scheffler (2008), and in Section 4 we show how delayed locality accounts for syntactic constructions such as anaphors as arguments of raising verbs and Exceptional Case Marking (ECM) verbs. The analysis also accounts for anaphors in picture-DPs and quantificational picture-DPs, anaphors in adjuncts, and sentences with multiple anaphors. 1.1 Syntactic nodes have an associated feature structure containing finite feature values. The feature structure must unify with the feature structure of any substituting or adjoining node in order for the operation to take place; if any features conflict, the unification fail"
W17-6204,W07-0402,1,0.863349,"joining Grammars and Related Formalisms (TAG+13), pages 31–42, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. tion types from σ to τ , or the abbreviated version στ where no ambiguity results. The featurestructure-based synchronous MCTAG framework we use is exemplified in Figure 1. Elementary tree sets for DPs, as in (a) and (b), contain multiple syntactic trees and semantic trees, two of each, independently motivated for handling quantification and topicalization. The syntax has a TP auxiliary tree (allowing for frontings such as topicalization, following Nesson and Shieber (2007)) in addition to the “in situ” DP tree; the synchronous semantics has a t auxiliary tree (used for quantifier scope, following Shieber and Schabes (1990) and Williford (1993)) and an e-rooted reference tree. Non-quantificational DPs like Noah have a degenerate scope tree t∗ that does not modify the derived tree, so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to an"
W17-6204,W08-2313,0,0.348883,"ena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion, 2008), or used the more constrained STAG plus adjustments, such as using de Bruijn indices in the semantics (Nesson, 2009), creating multiple reflexive trees (Storoshenko et al., 2008), or operating at multiple links in the derivation (Frank, 2008). No STAG approach to our knowledge has captured both reflexives and reciprocals. Our analysis seeks to fill this void by showing that both kinds of anaphors can be captured uniformly in STAG. To achieve this, we simplify and generalize one previous analysis of reflexives in STAG, namely that of Frank (2008), so it can apply to reciprocals and a variety of reflexive cases. We simplify Frank’s analysis, eliminating the c-command and dominance relations used for proper variable binding by appealing to fundamental syntactic and semantic constraints. We also generalize his analysis to apply to bot"
W17-6204,W06-1506,0,0.0387688,"llowing for frontings such as topicalization, following Nesson and Shieber (2007)) in addition to the “in situ” DP tree; the synchronous semantics has a t auxiliary tree (used for quantifier scope, following Shieber and Schabes (1990) and Williford (1993)) and an e-rooted reference tree. Non-quantificational DPs like Noah have a degenerate scope tree t∗ that does not modify the derived tree, so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion, 2008), or used the more constrained STAG plus adjustments, such as using de Bruijn indices in the semantics (Nesson, 2009), creating multiple reflexive trees (Storoshenko et al., 2008), or operating at multiple links in the derivation (Frank, 2008). No STAG approach to our knowledge has captured both reflexives and reciprocals. Our analysis seeks to fill this void by showi"
W17-6204,W06-1509,0,0.414302,"e, following Shieber and Schabes (1990) and Williford (1993)) and an e-rooted reference tree. Non-quantificational DPs like Noah have a degenerate scope tree t∗ that does not modify the derived tree, so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion, 2008), or used the more constrained STAG plus adjustments, such as using de Bruijn indices in the semantics (Nesson, 2009), creating multiple reflexive trees (Storoshenko et al., 2008), or operating at multiple links in the derivation (Frank, 2008). No STAG approach to our knowledge has captured both reflexives and reciprocals. Our analysis seeks to fill this void by showing that both kinds of anaphors can be captured uniformly in STAG. To achieve this, we simplify and generalize one previous analysis of reflexives in STAG, namely that of Frank (2008), so it can apply to recipro"
W17-6204,W06-1505,0,0.0349135,"and topicalization. The syntax has a TP auxiliary tree (allowing for frontings such as topicalization, following Nesson and Shieber (2007)) in addition to the “in situ” DP tree; the synchronous semantics has a t auxiliary tree (used for quantifier scope, following Shieber and Schabes (1990) and Williford (1993)) and an e-rooted reference tree. Non-quantificational DPs like Noah have a degenerate scope tree t∗ that does not modify the derived tree, so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion, 2008), or used the more constrained STAG plus adjustments, such as using de Bruijn indices in the semantics (Nesson, 2009), creating multiple reflexive trees (Storoshenko et al., 2008), or operating at multiple links in the derivation (Frank, 2008). No STAG approach to our knowledge has captured both reflexives and reciprocals."
W17-6204,W10-4402,0,0.0385358,"osing. In this section, we extend the derivation notion to allow for delayed locality, first proposed by Chiang and Scheffler (2008). Delayed locality relaxes the set-locality constraint to allow a delay in composition. Two trees in a multicomponent tree set may compose into (any number of) other trees before eventually composing into the same elementary tree.8 This differs from the more expressive non-local MCTAG in requiring that the members eventually compose into the same elementary tree (Chiang and Scheffler, 2008). Delayed locality has permitted analyses of non-local right-node raising (Han et al., 2010), bound vari4.2 Anaphors with ECM verbs ECM (or “subject-to-object raising”) verbs, as in (10), have two arguments: a subject (Noah and Emma) and a proposition (themselves/each other to be happy). Based on these structural properties, the elementary tree for an ECM verb contains a subject position and adjoins into a predicate to fill its proposition argument, as shown in Figure 7(d). 8 Storoshenko and Han (2013) propose a slightly different definition of a delay than Chiang and Scheffler (2008); we postpone committing to a particular definition to future work, but recognize that overgeneration"
W17-6204,C90-3045,1,0.614879,"types from σ to τ , or the abbreviated version στ where no ambiguity results. The featurestructure-based synchronous MCTAG framework we use is exemplified in Figure 1. Elementary tree sets for DPs, as in (a) and (b), contain multiple syntactic trees and semantic trees, two of each, independently motivated for handling quantification and topicalization. The syntax has a TP auxiliary tree (allowing for frontings such as topicalization, following Nesson and Shieber (2007)) in addition to the “in situ” DP tree; the synchronous semantics has a t auxiliary tree (used for quantifier scope, following Shieber and Schabes (1990) and Williford (1993)) and an e-rooted reference tree. Non-quantificational DPs like Noah have a degenerate scope tree t∗ that does not modify the derived tree, so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion,"
W17-6204,W08-2320,0,0.228894,"so merely serves as a placeholder to maintain structural consistency. phenomena (Nesson and Shieber, 2007), prepositions (Nesson, 2009), it-clefts (Han and Hedberg, 2006), pied-piping in relative clauses (Han, 2006), and clitic climbing (Bleam, 2000). Previous applications of TAG to anaphors have either appealed to extra facilities, such as recursive semantic features (Kallmeyer and Romero, 2007; Ryant and Scheffler, 2006; Champollion, 2008), or used the more constrained STAG plus adjustments, such as using de Bruijn indices in the semantics (Nesson, 2009), creating multiple reflexive trees (Storoshenko et al., 2008), or operating at multiple links in the derivation (Frank, 2008). No STAG approach to our knowledge has captured both reflexives and reciprocals. Our analysis seeks to fill this void by showing that both kinds of anaphors can be captured uniformly in STAG. To achieve this, we simplify and generalize one previous analysis of reflexives in STAG, namely that of Frank (2008), so it can apply to reciprocals and a variety of reflexive cases. We simplify Frank’s analysis, eliminating the c-command and dominance relations used for proper variable binding by appealing to fundamental syntactic and seman"
W17-6204,W10-4418,0,0.0949058,"Missing"
W17-6204,C88-2147,0,0.733124,"nd in Section 4 we show how delayed locality accounts for syntactic constructions such as anaphors as arguments of raising verbs and Exceptional Case Marking (ECM) verbs. The analysis also accounts for anaphors in picture-DPs and quantificational picture-DPs, anaphors in adjuncts, and sentences with multiple anaphors. 1.1 Syntactic nodes have an associated feature structure containing finite feature values. The feature structure must unify with the feature structure of any substituting or adjoining node in order for the operation to take place; if any features conflict, the unification fails (Vijay-Shanker and Joshi, 1988). In particular, we can mark DP substitution nodes in their feature structure with their case requirements (which can be thought of as a manifestation of their being assigned abstract Case (Polinsky and Preminger, 2014)), while lexical item trees rooted at DP that exhibit morphological case will have that case depicted in the root feature structure as well. This built-in feature checking system will play a role in several aspects of our STAG framework, including matching phifeatures (number, person, gender) of anaphors and antecedents, making c-command and dominance constraints unnecessary, an"
W19-0128,W18-3024,0,0.0771767,"evaluation schemes though: They often define a length threshold in the test set and report the performance of the model on this fixed set. We acknowledge three unsettling issues with these formulations. First, the sequences in the training set are usually assumed to be uniformly or geometrically distributed, with little regard to the nature and complexity of the language. This assumption may undermine any conclusions drawn from empirical investigations, especially given that natural language is not uniformly distributed, an aspect that is known to affect learning in modern RNN architectures (Liu et al., 2018). Second, in a test set where the sequences are enumerated by their lengths, if a network makes an error on a sequence of, say, length 7, but correctly recognizes longer sequences of length up to 1000, would we consider the model’s gener277 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 277-286. New York City, New York, January 3-6, 2019 alization as good or bad? In a setting where we monitor only the shortest sequence that was incorrectly predicted by the network, this scheme clearly misses the potential success of the model after witnessing a failure, thereby mi"
W19-0128,P18-2117,0,0.18481,"order to generate the following distributions: U-shaped (↵ = 0.25, = 0.25): The probabilities of having short and long sequences are equally high, but the probability of having an average-length sequence is low. Right-tailed (↵ = 1, = 5): Short sequences are more probable than long sequences. Left-tailed (↵ = 5, = 1): Long sequences are more probable than short sequences. 4.3 Length Windows Most of the previous studies trained networks on sequences of lengths n 2 [1, N ], where typical N values were between 10 and 50 (Bod´en and Wiles, 2000; Gers and Schmidhuber, 2001), and more recently 100 (Weiss et al., 2018). To determine the impact of the choice of training length-window on the stability and inductive capabilities of the LSTM networks, we experimented with three different length-windows for n: [1, 30], [1, 50], and [50, 100]. In the third window setting [50, 100], we further wanted to see whether LSTM are capable of generalizing to short sequences that are contained in the window range [1, 50], as well as to sequences that are longer than the sequences seen in the training set. Figure 3: Generalization graphs showing the average performance of LSTMs trained under different probability distributi"
W19-3905,J76-4004,0,0.437687,"Missing"
W19-3905,W18-5414,0,0.338283,"2n . Inspired by the early model design of NNPDAs, Grefenstette et al. (2015) also proposed memory-augmented recurrent networks (Neural Stacks, Queues, and DeQues), which are RNNs equipped with unbounded differentiable memory modules, to perform sequence-to-sequence transduction tasks that require specific data structures. Deleu and Dureau (2016) investigated the ability of Neural Turing Machines (NTMs; Graves et al. (2014)) to capture long-distance dependencies in the Dyck-1 language. Their empirical findings demonstrated that an NTM can recognize this language by emulating a DPA. Similarly, Sennhauser and Berwick (2018), Bernardy (2018), and Hao et al. (2018) conducted experiments on the Dyck languages to explore whether recurrent networks can learn nested structures. These studies assessed the performance of their recurrent models to predict the next possible parenthesis, assuming that it is a closing parenthesis.2 In fact, Bernardy (2018) used a purpose-designed architecture, called RUSS, which contains recurrent units with stack-like states, to perform the closing-parenthesis-completion task. Though the RUSS model had no trouble generalizing to longer and deeper sequences, as the author mentions, the spec"
W19-3905,D16-1248,0,0.0488552,"order to validate our hypothesis, we visualized the hidden and cell states of some of our LSTM models that achieved full accuracy on the test sets. Figure 2 illustrates that our LSTM is able to recognize the samples in D1 ||6 by emulating a DCA6 . In fact, the discrete even transitions in the cell state dynamics of the model reveal that six out of eight hidden units in the model are acting like separate counters. In some cases, we further discovered that certain units learned to count the length of the input sequences. Such length counting behaviours are also observed in machine translation (Shi et al., 2016; Bau et al., 2019; Dalvi et al., 2019) when the LSTMs are trained on a fixed-length training corpus.8 On the other hand, Figure 3 provides visualizations of the hidden and cell state dynamics of one of our single-layer LSTM models with four hidden units when the model was presented two sequences in the Dyck-2 language. Both sequences have some noticeable patterns and were chosen to explore whether the model behaves differently in repeated (or similar) subsequences. It seems that the LSTM model is trying to employ a complex counting strategy to learn the Dyck-2 language but failing to accompli"
W19-3905,W18-5425,0,0.190351,"can learn nested structures. These studies assessed the performance of their recurrent models to predict the next possible parenthesis, assuming that it is a closing parenthesis.2 In fact, Bernardy (2018) used a purpose-designed architecture, called RUSS, which contains recurrent units with stack-like states, to perform the closing-parenthesis-completion task. Though the RUSS model had no trouble generalizing to longer and deeper sequences, as the author mentions, the specificity of the architecture disqualifies it as a practical model choice for natural language modeling tasks. Additionally, Skachkova et al. (2018) trained recurrent networks to predict the last appropriate closing parenthesis, given a Dyck-2 sequence without its last symbol. They showed that their GRU and LSTM models performed with almost full accuracy on this parenthesis-completion task, but their task does not illustrate that these RNN models can recognize the Dyck language. Most recently, Weiss et al. (2018) and Suzgun et al. (2019) showed that the LSTM networks can develop natural counting mechanisms to recWe start by defining several subclasses of deterministic pushdown automata (DPA). Following Valiant and Paterson (1975), we defi"
W19-3905,W18-5433,0,0.0178383,"refenstette et al. (2015) also proposed memory-augmented recurrent networks (Neural Stacks, Queues, and DeQues), which are RNNs equipped with unbounded differentiable memory modules, to perform sequence-to-sequence transduction tasks that require specific data structures. Deleu and Dureau (2016) investigated the ability of Neural Turing Machines (NTMs; Graves et al. (2014)) to capture long-distance dependencies in the Dyck-1 language. Their empirical findings demonstrated that an NTM can recognize this language by emulating a DPA. Similarly, Sennhauser and Berwick (2018), Bernardy (2018), and Hao et al. (2018) conducted experiments on the Dyck languages to explore whether recurrent networks can learn nested structures. These studies assessed the performance of their recurrent models to predict the next possible parenthesis, assuming that it is a closing parenthesis.2 In fact, Bernardy (2018) used a purpose-designed architecture, called RUSS, which contains recurrent units with stack-like states, to perform the closing-parenthesis-completion task. Though the RUSS model had no trouble generalizing to longer and deeper sequences, as the author mentions, the specificity of the architecture disqualifies"
W19-3905,W19-0128,1,0.84361,"e generalizing to longer and deeper sequences, as the author mentions, the specificity of the architecture disqualifies it as a practical model choice for natural language modeling tasks. Additionally, Skachkova et al. (2018) trained recurrent networks to predict the last appropriate closing parenthesis, given a Dyck-2 sequence without its last symbol. They showed that their GRU and LSTM models performed with almost full accuracy on this parenthesis-completion task, but their task does not illustrate that these RNN models can recognize the Dyck language. Most recently, Weiss et al. (2018) and Suzgun et al. (2019) showed that the LSTM networks can develop natural counting mechanisms to recWe start by defining several subclasses of deterministic pushdown automata (DPA). Following Valiant and Paterson (1975), we define a deterministic one-counter automaton (DCA1 ) to be a DPA with a stack alphabet consisting of only one symbol. Traditionally, this construction allows moves (that is, executing actions on the stack without the observance of any inputs), but we restrict our attention to simple DCA1 s without -moves in the rest of this paper. Similarly, we call a DPA that contains k separate stacks, with e"
W19-3905,P18-2117,0,0.196759,"nt networks— Elman-RNNs (or RNNs, in short), LSTMs, and Gated Recurrent Units (GRUs)—to perform dynamic counting by training them to learn the Dyck-1 language. Our results demonstrate that the LSTMs with only a single hidden unit perform with perfect accuracy on the Dyck-1 learning task, and successfully generalize far beyond the training set. Furthermore, we show that the LSTMs can learn the shuffles of multiple Dyck-1 languages, defined over disjoint parenthesis-pairs, which require the emulation of multiple-counter arbitraryturn machines. Our results corroborate the theoretical findings of Weiss et al. (2018), while extending their empirical observations. On the other hand, when trained to learn the Dyck-2 language, which is a strictly context-free language, all our recurrent models failed to learn the language. In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be smallsized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Sho"
W90-0102,E89-1032,0,0.0231195,"Missing"
W90-0102,P85-1012,0,0.0167701,"so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic information. Semantichead-driven generation (Shieber et al., forthcoming; Calder et al., 1989) uses semantic heads and their complements as a locus of semantic locality. Joshi (1987) points out that tree-adjoining grammars may be an especially appropriate formalism for generation because of their syntactic locality properties, which, intuitively at least, ought to correlate with some notion of semantic locality. The same observation runs as an undercurrent in the work of McDonald and Pustejovsky (1985), who apply TAGs to the task of generaScope of the Paper The portion of the full-blown generation problem that we address here is what might be referred to as the tactical as opposed to the strategic generation problem. T h a t is, we are concerned only with how to compute instances of a well-defined relation between strings and canonical logical forms 1 in the direction from logical forms to strings, a problem that is sometimes referred to as ""reversing"" a grammar. This aspect of the generation problem, which ignores the crucial issues in determining what content to communicate, what predicat"
W90-0102,W89-0235,1,0.535065,"ars, the semantic monotonicity requirement precludes ""lexicaliza13 tion"" of the semantics. It is not possible to require nontrivial semantics to be associated with each lexical item. This fact, and the inefficiencies of generation thatfollow from it, was the initial motivation for the move to semantic-head-driven generation (Shieber et al., forthcoming). The efficiencies that that algorithmgains for augmented-context-free generation inhere in the synchronous TAG generation process if the semantic gramamr is lexicalized. In summary, just as lexicalization of the syntactic grammar aids parsing (Schabes and Joshi, 1989), so lexicalization of the semantic grammar aids generation. The simple generation algorithm that we have just presented seems to require that we completely analyze the logical form before generating the target string, as the process is a cascade of three subprocesses: parsing the logical form to a source derivation, mapping from source to target derivation, and computing the target derivation yield. As is common in such cases, portions of these computations can be interleaved, so that generation of the target string can proceed incrementally while traversing the source logical form. To what e"
W90-0102,C88-2121,1,0.323021,"Missing"
W90-0102,C88-2128,1,0.602073,"ation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation grammars as a computational aid is seen to be an inherent property of synchronous TAGs. Introduction The recent history of grammar reversing can be viewed as an effort to recover some notion of semantic locality on which to base a generation process. For instance, Wedekind (1988) requires a property of a grammar that he refers to as connectedness, which specifies that complements be semantically connected to their head. Shieber (1988) defines a notion of semantic monoLonicity, a kind of compositionality property that guarantees that it can be locally determined whether phrases can contribute to forming an expression with a given meaning. Generation schemes that reorder top-down generation (Dymetman and Isabelle, 1988; Strzalkowski, 1989) so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic information. Semantichead-driven generation (Shieber et al., forthcoming; Calder et al., 1989) uses semantic heads and their complements as a locus of semantic localit"
W90-0102,C88-2150,0,0.0334042,"s. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation grammars as a computational aid is seen to be an inherent property of synchronous TAGs. Introduction The recent history of grammar reversing can be viewed as an effort to recover some notion of semantic locality on which to base a generation process. For instance, Wedekind (1988) requires a property of a grammar that he refers to as connectedness, which specifies that complements be semantically connected to their head. Shieber (1988) defines a notion of semantic monoLonicity, a kind of compositionality property that guarantees that it can be locally determined whether phrases can contribute to forming an expression with a given meaning. Generation schemes that reorder top-down generation (Dymetman and Isabelle, 1988; Strzalkowski, 1989) so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic informati"
W90-0102,H86-1020,0,\N,Missing
W90-0102,C90-3001,1,\N,Missing
W90-0102,E89-1001,1,\N,Missing
W90-0102,P88-1032,1,\N,Missing
W90-0102,J87-1005,1,\N,Missing
W90-0102,P89-1027,0,\N,Missing
W90-0102,P85-1011,0,\N,Missing
