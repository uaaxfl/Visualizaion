2010.amta-papers.15,P98-1013,0,0.078272,"ibit Figure 3: Taxonomy of {appear, occur, emerge, exhibit} By merging ‘appear.2’ and ‘appear.5’, ‘appear.2,5’ becomes the least common hypernym of occur and emerge. However, according to WordNet, there is no link between exhibit and ‘appear.2,5’ so that exhibit is not connected to the rest. Thus, we end up having two semantic classes for this set, ‘appear.2,5’ and ‘exhibit’. By using the taxonomy, we can evaluate how well each set of English verbs corresponds to WordNet semantic relations. There already exist English lexical databases such as WordNet, VerbNet (Kipper et al., 2006), FrameNet (Baker et al., 1998), but this approach suggests an automatic way of deriving new semantic classes and validating and extending preexisting classes, which can be applied to other languages. Verifying that English semantic classes can be derived correctly using a parallel corpus gives us an idea about how well this approach may work on other languages that do not already have similar lexical databases. 5 Experiments We used only the SPM method on the Xinhua English-Chinese parallel corpus to derive semantic classes in English and Chinese, since the SPM method performed better than the GIZA++ mapping method on both"
2010.amta-papers.15,W09-3020,1,0.631565,"idea of semantic similarity based on triangulation between parallel corpora outlined in Resnik (2004) and Madnani et al. (2008a; 2008b), but is implemented here quite differently. It is most similar in execution to the work of (Mareˇcek, 2009b), which improves word alignment by aligning tectogrammatical trees in a parallel English/Czech corpus. The Czech corpus is first lemmatized because of the rich morphology, and then the word alignment is “symmetrized”. However, this approach does not explicitly make use of the predicate-argument structure to confirm the alignments or to suggest new ones. Choi et al. (2009) showed how to enhance Chinese-English word alignments by exploring predicate-argument structure alignment using parallel PropBanks. They used the ‘English Chinese Translation Treebank’ (ECTB) for their experiments, the same corpus we use. The resulting system showed improvement over pure GIZA++ alignment, though it only took advantage of Chinese to English GIZA++ output and required careful tuning of a number of threshold parameters to balance between precision and recall. Fung et al. (2007) demonstrated that there is poor semantic parallelism betwen Chinese-English bilingual sentences. Their"
2010.amta-papers.15,D09-1076,0,0.0307221,"For a set of Chinese verbs aligned to an English verb we manually checked semantic similarity between the Chinese verbs within a set. Our results show that the verb sets we generated have a high correlation with semantic classes. This could potentially lead to an automatic technique for generating semantic classes for verbs. 1 Introduction This paper discusses attempts to use alignments between English and Chinese predicate-argument structures in a parallel PropBanked corpus1 as a basis for determining cross-lingual semantic similarity. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment 1 PropBank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate (Palmer et al., 2005). has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent al"
2010.amta-papers.15,2007.tmi-papers.10,0,0.130691,"licitly make use of the predicate-argument structure to confirm the alignments or to suggest new ones. Choi et al. (2009) showed how to enhance Chinese-English word alignments by exploring predicate-argument structure alignment using parallel PropBanks. They used the ‘English Chinese Translation Treebank’ (ECTB) for their experiments, the same corpus we use. The resulting system showed improvement over pure GIZA++ alignment, though it only took advantage of Chinese to English GIZA++ output and required careful tuning of a number of threshold parameters to balance between precision and recall. Fung et al. (2007) demonstrated that there is poor semantic parallelism betwen Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of employing features from automatic syntactic parses of the Chinese and English sentences, word alignment with a bilingual lexicon, and tuning on an unannotated parallel corpus. Later, Wu and Fung (2009) used parallel PropBanks to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs using the predicate-"
2010.amta-papers.15,W10-1810,1,0.855266,"Missing"
2010.amta-papers.15,kipper-etal-2006-extending,1,0.844183,".5 appear.2 occur.3 emerge.1 exhibit Figure 3: Taxonomy of {appear, occur, emerge, exhibit} By merging ‘appear.2’ and ‘appear.5’, ‘appear.2,5’ becomes the least common hypernym of occur and emerge. However, according to WordNet, there is no link between exhibit and ‘appear.2,5’ so that exhibit is not connected to the rest. Thus, we end up having two semantic classes for this set, ‘appear.2,5’ and ‘exhibit’. By using the taxonomy, we can evaluate how well each set of English verbs corresponds to WordNet semantic relations. There already exist English lexical databases such as WordNet, VerbNet (Kipper et al., 2006), FrameNet (Baker et al., 1998), but this approach suggests an automatic way of deriving new semantic classes and validating and extending preexisting classes, which can be applied to other languages. Verifying that English semantic classes can be derived correctly using a parallel corpus gives us an idea about how well this approach may work on other languages that do not already have similar lexical databases. 5 Experiments We used only the SPM method on the Xinhua English-Chinese parallel corpus to derive semantic classes in English and Chinese, since the SPM method performed better than th"
2010.amta-papers.15,2008.amta-papers.13,0,0.485261,"Missing"
2010.amta-papers.15,J03-1002,0,0.00466096,"Missing"
2010.amta-papers.15,J05-1004,1,0.341062,"ed have a high correlation with semantic classes. This could potentially lead to an automatic technique for generating semantic classes for verbs. 1 Introduction This paper discusses attempts to use alignments between English and Chinese predicate-argument structures in a parallel PropBanked corpus1 as a basis for determining cross-lingual semantic similarity. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment 1 PropBank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate (Palmer et al., 2005). has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent alignment across sentences. We begin by running GIZA++ (Och and Ney, 2003), one of the most popular alignment tools, to obtain automatic word alignments between the parallel Engl"
2010.amta-papers.15,N09-2004,0,0.0897596,"+ alignment, though it only took advantage of Chinese to English GIZA++ output and required careful tuning of a number of threshold parameters to balance between precision and recall. Fung et al. (2007) demonstrated that there is poor semantic parallelism betwen Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of employing features from automatic syntactic parses of the Chinese and English sentences, word alignment with a bilingual lexicon, and tuning on an unannotated parallel corpus. Later, Wu and Fung (2009) used parallel PropBanks to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs using the predicate-argument structure annotated in parallel PropBanks. Although their work is not directly related to ours, it shows how parallel PropBanks can be used to improve MT systems in general. 3 3.1 Symmetric predicate mapping Parallel Corpus We apply our predicate-argument mapping method to the ‘Engish Chinese Translation Treebank’ (ECTB), a parallel English-Chinese corpus. In addition to Treebank syntactic structure, the"
2010.amta-papers.15,2007.mtsummit-papers.71,0,0.176751,"Missing"
2010.amta-papers.15,C98-1013,0,\N,Missing
2010.amta-papers.15,P07-2045,0,\N,Missing
2010.amta-papers.15,J06-1003,0,\N,Missing
2020.aacl-main.38,N19-1423,0,0.022023,"(±0.7) 56.5 (±2.5) 83.1 (±0.9) Ae + Re 71.5 (±1.9) 51.7 (±1.3) 86.7 (±3.4) Be + Ae + Re 74.1 (±0.3) 60.9 (±5.2) 84.0 (±2.4) Table 4: Performance of ensemble models. Berte /RoBERTae /ALBERTe use transcript embeddings from all 3 tasks trained by the BERT/RoBERTa/ALBERT models in Table 3, respectively. Be +Re uses transcript embeddings from both Berte and RoBERTae (so the total of 6 embeddings), Ae +Re uses transcript embeddings from both ALBERTe and RoBERTae (6 embeddings), and Be +Ae +Re uses transcript embeddings from all three models (9 embeddings). Three transformer encoders are used, BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2020), and ALBERT (Lan et al., 2019) for our experiments. Every model is trained 3 times and its average performance with the standard deviation are reported.6 CRec MRec CSbj MSbj CV0 77 53 37 27 CV1 77 53 37 28 CV2 77 53 37 28 CV3 77 53 37 29 CV4 77 53 37 29 ALL 385 265 185 141 Table 5: Statistics of the CV sets for our experiments. Rec/Sbj: # of recordings/subjects, P4C/M: in control/MCI group. CVi : the i’th set. ALL: i=0 CVi . 5.1 Performance of Individual Models Individual models are built by training transcripts from each task separately using MLPi in Section 4. Ta"
2020.aacl-main.38,2021.ccl-1.108,0,0.0880942,"Missing"
2020.aacl-main.38,D18-1304,0,0.0615708,"Missing"
2020.acl-main.505,D18-1241,0,0.0134588,"on Transformer-based contextualized embedding approaches such as BERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and AlBERT (Lan et al., 2019) have re-established the state-of-the-art for practically all question answering (QA) tasks on not only general domain datasets such as SQ UAD (Rajpurkar et al., 2016, 2018), MS M ARCO (Nguyen et al., 2016), T RIVIAQA (Joshi et al., 2017), N EWS QA (Trischler et al., 2017), or NARRATIVE QA (Koisk et al., 2018), but also multiturn question datasets such as SQA (Iyyer et al., 2017), Q UAC (Choi et al., 2018), C O QA (Reddy et al., 2019), or CQA (Talmor and Berant, 2018). However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models (Sun et al., 2019; Yang and Choi, 2019) due to the challenges in representing utterances composed by heterogeneous speakers. Several limitations can be expected for language models trained on general domains to process dialogue. First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tu"
2020.acl-main.505,N19-1423,0,0.233892,"enting the entire μ This sectionSoftmax introduces aoijnovel approach forSoftmax pre- dialogue, ij which creates the input string sequence training (Section 2.1) and fine-tuning (Section 2.2) I = {CLS}⊕U ⊕. . .⊕U . For every w ∈ I, let μ n ij μ w w w s w s c w eij effectively e11 ⋯ e1n ⋯ to e1 transformers ei ei1µ ⋯ eik ⋯ einw1 ei ⋯ emn ⋯ ems em1 learn dialogue contexts. Iij = (I  {wij }) ∪ {µij }, where µij is the masked Our approach has been evaluated with two kinds token substituted in place of wij . Iijµ is then fed Transformer Encoder (TE) Transformer Encoder (TE) of transformers, BERT (Devlin et al., 2019) and into the transformer encoder (TE), which generates RoBERTa (Liu et al., 2019), and shown significant a sequence of embeddings {ec } ⊕ E1 ⊕ . . . ⊕ Em s1 w11 ⋯ w1n ⋯ μij ⋯ sm wm1 ⋯ wmn [CLSi] si wi1 ⋯ μij ⋯ win improvement to a question answering task (QA) on w where Ei = {esi , ew i1 , .., ein } is the embedding list µ multiparty dialogue (Section 3). c s w for Ui , and (e , ei , eij , eij ) are the embeddings of (CLS, si , wij , µij ) respectively. Finally, eµij is fed 2.1 Pre-training Language Models into a softmax layer that generates the output vector Pre-training involves 3 tasks in"
2020.acl-main.505,P17-1167,0,0.0756212,"Missing"
2020.acl-main.505,P17-1147,0,0.0604703,"Missing"
2020.acl-main.505,Q18-1023,0,0.053979,"Missing"
2020.acl-main.505,2021.ccl-1.108,0,0.117394,"Missing"
2020.acl-main.505,P18-2124,0,0.0523925,"Missing"
2020.acl-main.505,D16-1264,0,0.126775,"Missing"
2020.acl-main.505,Q19-1016,0,0.0607388,"ualized embedding approaches such as BERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and AlBERT (Lan et al., 2019) have re-established the state-of-the-art for practically all question answering (QA) tasks on not only general domain datasets such as SQ UAD (Rajpurkar et al., 2016, 2018), MS M ARCO (Nguyen et al., 2016), T RIVIAQA (Joshi et al., 2017), N EWS QA (Trischler et al., 2017), or NARRATIVE QA (Koisk et al., 2018), but also multiturn question datasets such as SQA (Iyyer et al., 2017), Q UAC (Choi et al., 2018), C O QA (Reddy et al., 2019), or CQA (Talmor and Berant, 2018). However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models (Sun et al., 2019; Yang and Choi, 2019) due to the challenges in representing utterances composed by heterogeneous speakers. Several limitations can be expected for language models trained on general domains to process dialogue. First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tuning for the end tasks is oft"
2020.acl-main.505,Q19-1014,0,0.187506,"for practically all question answering (QA) tasks on not only general domain datasets such as SQ UAD (Rajpurkar et al., 2016, 2018), MS M ARCO (Nguyen et al., 2016), T RIVIAQA (Joshi et al., 2017), N EWS QA (Trischler et al., 2017), or NARRATIVE QA (Koisk et al., 2018), but also multiturn question datasets such as SQA (Iyyer et al., 2017), Q UAC (Choi et al., 2018), C O QA (Reddy et al., 2019), or CQA (Talmor and Berant, 2018). However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models (Sun et al., 2019; Yang and Choi, 2019) due to the challenges in representing utterances composed by heterogeneous speakers. Several limitations can be expected for language models trained on general domains to process dialogue. First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tuning for the end tasks is often not sufficient enough to build robust dialogue models. Second, unlike sentences in a wiki or news article written by one author with a coherent topic, utterances in a dialogue are from multiple speakers who may talk"
2020.acl-main.505,N18-1059,0,0.0196758,"such as BERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and AlBERT (Lan et al., 2019) have re-established the state-of-the-art for practically all question answering (QA) tasks on not only general domain datasets such as SQ UAD (Rajpurkar et al., 2016, 2018), MS M ARCO (Nguyen et al., 2016), T RIVIAQA (Joshi et al., 2017), N EWS QA (Trischler et al., 2017), or NARRATIVE QA (Koisk et al., 2018), but also multiturn question datasets such as SQA (Iyyer et al., 2017), Q UAC (Choi et al., 2018), C O QA (Reddy et al., 2019), or CQA (Talmor and Berant, 2018). However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models (Sun et al., 2019; Yang and Choi, 2019) due to the challenges in representing utterances composed by heterogeneous speakers. Several limitations can be expected for language models trained on general domains to process dialogue. First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tuning for the end tasks is often not sufficient enough to build"
2020.acl-main.505,W17-2623,0,0.0617018,"Missing"
2020.acl-main.505,W19-5923,1,0.50633,"ll question answering (QA) tasks on not only general domain datasets such as SQ UAD (Rajpurkar et al., 2016, 2018), MS M ARCO (Nguyen et al., 2016), T RIVIAQA (Joshi et al., 2017), N EWS QA (Trischler et al., 2017), or NARRATIVE QA (Koisk et al., 2018), but also multiturn question datasets such as SQA (Iyyer et al., 2017), Q UAC (Choi et al., 2018), C O QA (Reddy et al., 2019), or CQA (Talmor and Berant, 2018). However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models (Sun et al., 2019; Yang and Choi, 2019) due to the challenges in representing utterances composed by heterogeneous speakers. Several limitations can be expected for language models trained on general domains to process dialogue. First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tuning for the end tasks is often not sufficient enough to build robust dialogue models. Second, unlike sentences in a wiki or news article written by one author with a coherent topic, utterances in a dialogue are from multiple speakers who may talk about different topics"
2020.bionlp-1.10,N19-1408,0,0.0212906,"been sampled for the given episode. where hj is the hidden state at time step j, sj is the jth segment, and θ is the set of parameters. Although segmentation of documents is still necessary, no pseudo labels are needed. We get the segment representation by averaging its token embedding from the last layer of BERT. The final hidden state at each step j is the concatenated hidden states of a single-layer Bi-directional LSTM. After we get the hidden state for each segment, a max-pooling operation is performed on h1:n over the time dimension to obtain a fixed-length vector, similar to Kim (2014); Adhikari et al. (2019). A dense layer is immediately followed. It is particularly important to strengthen regularization on this dataset with small sample size. Dropout (Srivastava et al., 2014) as a way of regularization has been shown effective in deep learning models, and Merity et al. (2018) has successfully applied dropout-like technique in LSTM: the use of DropConnect (Wan et al., 2013) is applied on the four hidden-to-hidden matrices, preventing overfitting from occurring on the recurrent weights. 5 sponsible for the reward, and the REINFORCE algorithm is used to train the policy (Williams, 1992). State At e"
2020.bionlp-1.10,Q17-1010,0,0.0980767,"Missing"
2020.bionlp-1.10,N19-1423,0,0.179947,"gnored in the averaging process. For the unstructured notes, concatenation of Term Frequency-Inverse Document Frequency 96 (TF-IDF) and Latent Dirichlet Allocation (LDA) representation is fed into LR. However, we have found that the representation from LDA only contributes marginally, while LDA takes significantly more inferring time. Thus, we drop LDA and only use TF-IDF as our BoW encoder (Section 4.1). Various deep learning models regarding text classification have been proposed in recent years. Pretrained language models like BERT have shown state-of-the-art performance on many NLP tasks (Devlin et al., 2019). ClinicalBERT is also introduced on the medical domain (Huang et al., 2019). However, deep learning approaches have two drawbacks on this particular dataset. First, deep learning requires large dataset to train, whereas most of our unstructured note types only have fewer than 2,000 samples. Second, these approaches are not designed for long documents, and difficult to keep long-term dependencies over thousands of tokens. Reinforcement learning has been explored to combat data noise by previous work (Zhang et al., 2018; Qin et al., 2018) on the short text setting. A policy network makes decisi"
2020.bionlp-1.10,D14-1181,0,0.0102358,"s a1:T have been sampled for the given episode. where hj is the hidden state at time step j, sj is the jth segment, and θ is the set of parameters. Although segmentation of documents is still necessary, no pseudo labels are needed. We get the segment representation by averaging its token embedding from the last layer of BERT. The final hidden state at each step j is the concatenated hidden states of a single-layer Bi-directional LSTM. After we get the hidden state for each segment, a max-pooling operation is performed on h1:n over the time dimension to obtain a fixed-length vector, similar to Kim (2014); Adhikari et al. (2019). A dense layer is immediately followed. It is particularly important to strengthen regularization on this dataset with small sample size. Dropout (Srivastava et al., 2014) as a way of regularization has been shown effective in deep learning models, and Merity et al. (2018) has successfully applied dropout-like technique in LSTM: the use of DropConnect (Wan et al., 2013) is applied on the four hidden-to-hidden matrices, preventing overfitting from occurring on the recurrent weights. 5 sponsible for the reward, and the REINFORCE algorithm is used to train the policy (Wil"
2020.bionlp-1.10,W18-2502,0,0.0185467,"an probability is that subsequences do i not contain equal information. pnmax represents the best potential, while longer text should give more i i is more easily afimportance to pnmean , because pnmax fected by noise as the text length grows. Although Equation 2 seems intuitive, the use of pseudo labels on subsequences becomes another source of noise, especially when there are thousands of tokens; thus, the performance is uncertain. Section 6.2 provides detailed empirical analysis for this model. For the baseline model, the bag-of-words representation with TF-IDF scores, excluding stopwords (Nothman et al., 2018), is fed into logistic regression (LR). The objective is to minimize the negative log likelihood of the gold label yi : − ClinicalBERT 4.4 Weight-dropped LSTM We split documents of each patient into multiple short segments, and feed the segment representation to long short-term memory network (LSTM) at each time step: Averaged Word Embedding Word embeddings generated by fastText are used to establish another baseline, that utilizes subwords to better represent unseen terms (Bojanowski et al., hj ← LSTM(sj , hj−1 ; θ) 97 (3) Environment Policy Network Action st ⟶ π at State θ at st+1 a1 ↓ a2 ↓"
2020.bionlp-1.10,P18-1199,0,0.0290554,"shown state-of-the-art performance on many NLP tasks (Devlin et al., 2019). ClinicalBERT is also introduced on the medical domain (Huang et al., 2019). However, deep learning approaches have two drawbacks on this particular dataset. First, deep learning requires large dataset to train, whereas most of our unstructured note types only have fewer than 2,000 samples. Second, these approaches are not designed for long documents, and difficult to keep long-term dependencies over thousands of tokens. Reinforcement learning has been explored to combat data noise by previous work (Zhang et al., 2018; Qin et al., 2018) on the short text setting. A policy network makes decision left-to-right over tokens, and is jointly trained with another classifier. However, there is little investigation of using RL on the long text setting, as it still requires an effective encoder to give meaningful representation of long documents. Therefore, in our experiments, the first step is to select the best encoder, and then apply RL on the long document classification. 4 4.1 2017). It is suitable for this task as unseen terms or misspellings frequently appear in these clinical notes. The averaged word embedding is used to repre"
2020.emnlp-main.679,N19-1423,0,0.0199665,"Missing"
2020.emnlp-main.686,P15-1136,0,0.0355406,"on modern coreference resolution models in order to envision the future direction of this research. 67.2 68 65 62 64.21 W-16 65.29 C-16 L-17 L-18 F-19 J-20 Figure 1: Performance of the recent state-of-the-art models on the CoNLL 2012 shared task. 1W-16: Wiseman et al. (2016), C-16: Clark and Manning (2016), L-17: Lee et al. (2017), L-18: Lee et al. (2018), F-19: Fei et al. (2019), K-19: Kantor and Globerson (2019), J-19: Joshi et al. (2019), J-20: Joshi et al. (2020). 2 Related Work Most neural network-based coreference resolution models have adapted antecedent-ranking (Wiseman et al., 2015; Clark and Manning, 2015; Lee et al., 2017, 2018; Joshi et al., 2019, 2020), which relies on the local decisions between each mention and its 1 Source codes and models are available at https://github.com/emorynlp/coref-hoi. 8527 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8527–8533, c November 16–20, 2020. 2020 Association for Computational Linguistics 0.53 0 0.24 0 0 antecedents. To achieve deeper global optimization, Wiseman et al. (2016); Clark and Manning (2016); Yu et al. (2020) built entity representations in the ranking process, whereas Lee et al. (2018); Kanto"
2020.emnlp-main.686,P16-1061,0,0.240954,"rning (Figure 1). Most of these previous models have also adapted higher-order inference (HOI) for the global optimization of coreference links, although HOI clearly has not been the focus of those works, for the fact that gains from HOI have been reported marginal. This has inspired us to analyze the impact of HOI on modern coreference resolution models in order to envision the future direction of this research. 67.2 68 65 62 64.21 W-16 65.29 C-16 L-17 L-18 F-19 J-20 Figure 1: Performance of the recent state-of-the-art models on the CoNLL 2012 shared task. 1W-16: Wiseman et al. (2016), C-16: Clark and Manning (2016), L-17: Lee et al. (2017), L-18: Lee et al. (2018), F-19: Fei et al. (2019), K-19: Kantor and Globerson (2019), J-19: Joshi et al. (2019), J-20: Joshi et al. (2020). 2 Related Work Most neural network-based coreference resolution models have adapted antecedent-ranking (Wiseman et al., 2015; Clark and Manning, 2015; Lee et al., 2017, 2018; Joshi et al., 2019, 2020), which relies on the local decisions between each mention and its 1 Source codes and models are available at https://github.com/emorynlp/coref-hoi. 8527 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proc"
2020.emnlp-main.686,N19-1423,0,0.0353677,", and vice versa. fc is computed by FFNN similar to fa , and φ(Cy ) is the meta-feature such as the cluster size. Algorithm 1 Antecedent Ranking for CM 1: procedure RANKING(g1 , · · · , gN ) 2: Ci=1,··· ,N ← gi 3: R ← ranking_order(g1 , · · · , gN ) 4: for x = R1 · · · RN do 5: for y ∈ Y(x) do . Parallelized 6: fc (gx , Cy ) ← 0 if Cy = gy 7: sx (y) ← fa (gx , gy ) + fc (gx , Cy , φ(Cy )) 8: y 0 ← argmaxy∈Y(x) sx (y) 9: if y 0 6=  then 10: merge Cx and Cy0 11: return s1 , · · · , sN Two simple configurations can be tuned for CM. We can have the sequential left-to-right ranking • BERT: BERT (Devlin et al., 2019) as the encoder • SpanBERT: SpanBERT (Joshi et al., 2020) as the encoder • +AA: SpanBERT with attended antecedent (§3.2) • +EE: SpanBERT with entity equalization (§3.2) • +SC: SpanBERT with span clustering (§3.3) • +CM: SpanBERT with cluster merging (§3.3) Note that BERT and SpanBERT completely rely on only local decisions without any HOI. Particularly, +AA is equivalent to Joshi et al. (2020). 4.1 Results Table 1 shows the best results in comparison to previous state-of-the-art systems. We also report the mean scores and standard deviations from 5 repeated developments, which we could not fin"
2020.emnlp-main.686,P19-1064,0,0.0111051,"atures to enrich the span representation with more “global” information. The updated span representation gx0 can be derived as in Eq. 3, where gx0 is the interpolation between the current and refined representation gx and ax , and Wf is the gate parameter. gx0 is used to perform another round of antecedent-ranking in replacement of gx . gx0 = fx ◦ gx + (1 − fx ) ◦ ax The following two methods share the same updating process for gx0 , but with different ways to obtain the refined span representation ax . Attended Antecedent (AA) takes the antecedent information to enrich gx0 (Lee et al., 2018; Fei et al., 2019; Joshi et al., 2019, 2020). The refined span ax is the attended antecedent representation over the current antecedent distribution P (y), where gy∈Y(x) is the antecedent representation: X ax = P (y) · gy (4) y∈Y(x) Entity Equalization (EE) takes the clustering relaxation as in Eq. 5 to model the entity distribution (Kantor and Globerson, 2019), where Q(x ∈ Ey0 ) is the probability of the span x referring to an entity Ey0 in which the span y 0 is the first mention. P (y) is the current antecedent distribution. Q(x ∈ Ey0 ) = Px−1   k=y0 P (y = k) · Q(k ∈ Ey0 ) P (y = )   0 y0 &lt; x y 0 = x"
2020.emnlp-main.686,2020.tacl-1.5,0,0.278367,"not been the focus of those works, for the fact that gains from HOI have been reported marginal. This has inspired us to analyze the impact of HOI on modern coreference resolution models in order to envision the future direction of this research. 67.2 68 65 62 64.21 W-16 65.29 C-16 L-17 L-18 F-19 J-20 Figure 1: Performance of the recent state-of-the-art models on the CoNLL 2012 shared task. 1W-16: Wiseman et al. (2016), C-16: Clark and Manning (2016), L-17: Lee et al. (2017), L-18: Lee et al. (2018), F-19: Fei et al. (2019), K-19: Kantor and Globerson (2019), J-19: Joshi et al. (2019), J-20: Joshi et al. (2020). 2 Related Work Most neural network-based coreference resolution models have adapted antecedent-ranking (Wiseman et al., 2015; Clark and Manning, 2015; Lee et al., 2017, 2018; Joshi et al., 2019, 2020), which relies on the local decisions between each mention and its 1 Source codes and models are available at https://github.com/emorynlp/coref-hoi. 8527 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8527–8533, c November 16–20, 2020. 2020 Association for Computational Linguistics 0.53 0 0.24 0 0 antecedents. To achieve deeper global optimization,"
2020.emnlp-main.686,D19-1588,0,0.524949,"he span representation with more “global” information. The updated span representation gx0 can be derived as in Eq. 3, where gx0 is the interpolation between the current and refined representation gx and ax , and Wf is the gate parameter. gx0 is used to perform another round of antecedent-ranking in replacement of gx . gx0 = fx ◦ gx + (1 − fx ) ◦ ax The following two methods share the same updating process for gx0 , but with different ways to obtain the refined span representation ax . Attended Antecedent (AA) takes the antecedent information to enrich gx0 (Lee et al., 2018; Fei et al., 2019; Joshi et al., 2019, 2020). The refined span ax is the attended antecedent representation over the current antecedent distribution P (y), where gy∈Y(x) is the antecedent representation: X ax = P (y) · gy (4) y∈Y(x) Entity Equalization (EE) takes the clustering relaxation as in Eq. 5 to model the entity distribution (Kantor and Globerson, 2019), where Q(x ∈ Ey0 ) is the probability of the span x referring to an entity Ey0 in which the span y 0 is the first mention. P (y) is the current antecedent distribution. Q(x ∈ Ey0 ) = Px−1   k=y0 P (y = k) · Q(k ∈ Ey0 ) P (y = )   0 y0 &lt; x y 0 = x (5) y0 &gt; x The refin"
2020.emnlp-main.686,P19-1066,0,0.158473,"x . gx0 = fx ◦ gx + (1 − fx ) ◦ ax The following two methods share the same updating process for gx0 , but with different ways to obtain the refined span representation ax . Attended Antecedent (AA) takes the antecedent information to enrich gx0 (Lee et al., 2018; Fei et al., 2019; Joshi et al., 2019, 2020). The refined span ax is the attended antecedent representation over the current antecedent distribution P (y), where gy∈Y(x) is the antecedent representation: X ax = P (y) · gy (4) y∈Y(x) Entity Equalization (EE) takes the clustering relaxation as in Eq. 5 to model the entity distribution (Kantor and Globerson, 2019), where Q(x ∈ Ey0 ) is the probability of the span x referring to an entity Ey0 in which the span y 0 is the first mention. P (y) is the current antecedent distribution. Q(x ∈ Ey0 ) = Px−1   k=y0 P (y = k) · Q(k ∈ Ey0 ) P (y = )   0 y0 &lt; x y 0 = x (5) y0 &gt; x The refined span ax is the attended entity repre(x) sentation, where ey is the entity representation to which the span y belongs till the span x: sc (x, y) = wc FFNNc (gx , gy , φ(x, y)) e(t) x gx , gy are the span embeddings of x and y, φ(x, y) is the meta-information (e.g., speakers, distance), and wm , wc are the mention and coref"
2020.emnlp-main.686,D17-1018,0,0.303079,"solution models in order to envision the future direction of this research. 67.2 68 65 62 64.21 W-16 65.29 C-16 L-17 L-18 F-19 J-20 Figure 1: Performance of the recent state-of-the-art models on the CoNLL 2012 shared task. 1W-16: Wiseman et al. (2016), C-16: Clark and Manning (2016), L-17: Lee et al. (2017), L-18: Lee et al. (2018), F-19: Fei et al. (2019), K-19: Kantor and Globerson (2019), J-19: Joshi et al. (2019), J-20: Joshi et al. (2020). 2 Related Work Most neural network-based coreference resolution models have adapted antecedent-ranking (Wiseman et al., 2015; Clark and Manning, 2015; Lee et al., 2017, 2018; Joshi et al., 2019, 2020), which relies on the local decisions between each mention and its 1 Source codes and models are available at https://github.com/emorynlp/coref-hoi. 8527 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8527–8533, c November 16–20, 2020. 2020 Association for Computational Linguistics 0.53 0 0.24 0 0 antecedents. To achieve deeper global optimization, Wiseman et al. (2016); Clark and Manning (2016); Yu et al. (2020) built entity representations in the ranking process, whereas Lee et al. (2018); Kantor and Globerson (2"
2020.emnlp-main.686,N18-2108,0,0.701116,"Missing"
2020.emnlp-main.686,P02-1014,0,0.489147,"an refinement, and it constructs the actual clusters and obtains the “true” predicted entities using P (y) instead of modeling the “soft” entity clusters through the relaxation as in EE (Section 3.2). This way, although we lose the differentiable property, the obtaining of true entities with the same empirical inference time as EE has made SC desirable. The entity representation ei for an entity cluster Ci is given by the attended spans in this cluster: order or the easy-first order (L3) whose sequence is ordered by each span’s max antecedent score, building the most confident clusters first (Ng and Cardie, 2002; Clark and Manning, 2016). There can be element-wise mean or max-reduction among the spans in the two merging clusters (L10). Distinguished from Wiseman et al. (2016), clusters in CM are searched and merged in training without the use of oracle clusters, closing the gap between training and test time. 4 αt = wα FFNNα (gt ) exp(αt ) αi,t = P k∈C exp(αk ) X i ei = αi,t · gt Experiments For our experiments, the CoNLL 2012 English shared task dataset is used (Pradhan et al., 2012). Given the end-to-end coreference system in Section 3.1, six models are developed as follows:2 t∈Ci The entity cluste"
2020.emnlp-main.686,N18-1202,0,0.061239,"Missing"
2020.emnlp-main.686,W12-4501,0,0.398426,"t order (L3) whose sequence is ordered by each span’s max antecedent score, building the most confident clusters first (Ng and Cardie, 2002; Clark and Manning, 2016). There can be element-wise mean or max-reduction among the spans in the two merging clusters (L10). Distinguished from Wiseman et al. (2016), clusters in CM are searched and merged in training without the use of oracle clusters, closing the gap between training and test time. 4 αt = wα FFNNα (gt ) exp(αt ) αi,t = P k∈C exp(αk ) X i ei = αi,t · gt Experiments For our experiments, the CoNLL 2012 English shared task dataset is used (Pradhan et al., 2012). Given the end-to-end coreference system in Section 3.1, six models are developed as follows:2 t∈Ci The entity clusters Ci are constructed in the same way as in the final cluster prediction. The refined span ax is then equal to the representation of entity ei to which it belongs (gx ∈ Ci ). Cluster Merging (CM) performs sequential antecedent ranking combining both antecedent and entity information to gradually build up the entity clusters, which is distinguished from span refinement methods that simply re-rank antecedents. Algorithm 1 describes the ranking process for CM. gi is the i’th span,"
2020.emnlp-main.686,P15-1137,0,0.0819631,"lyze the impact of HOI on modern coreference resolution models in order to envision the future direction of this research. 67.2 68 65 62 64.21 W-16 65.29 C-16 L-17 L-18 F-19 J-20 Figure 1: Performance of the recent state-of-the-art models on the CoNLL 2012 shared task. 1W-16: Wiseman et al. (2016), C-16: Clark and Manning (2016), L-17: Lee et al. (2017), L-18: Lee et al. (2018), F-19: Fei et al. (2019), K-19: Kantor and Globerson (2019), J-19: Joshi et al. (2019), J-20: Joshi et al. (2020). 2 Related Work Most neural network-based coreference resolution models have adapted antecedent-ranking (Wiseman et al., 2015; Clark and Manning, 2015; Lee et al., 2017, 2018; Joshi et al., 2019, 2020), which relies on the local decisions between each mention and its 1 Source codes and models are available at https://github.com/emorynlp/coref-hoi. 8527 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8527–8533, c November 16–20, 2020. 2020 Association for Computational Linguistics 0.53 0 0.24 0 0 antecedents. To achieve deeper global optimization, Wiseman et al. (2016); Clark and Manning (2016); Yu et al. (2020) built entity representations in the ranking process, whereas"
2020.emnlp-main.686,N16-1114,0,0.184394,"Missing"
2020.emnlp-main.686,2020.acl-main.622,0,0.803126,"Missing"
2020.emnlp-main.686,2020.lrec-1.2,0,0.434617,"Missing"
2020.figlang-1.38,N19-1423,0,0.03249,"asm example from Reddit. TRN TST AU 4.9 (±3.2) 4.2 (±1.9) AT 140.4 (±112.8) 128.5 (±78.8) (a) Twitter dataset statistics. TRN TST NC 4,400 1,800 AU 3.5 (±0.8) 5.3 (±2.0) AT 45.8 (±17.3) 93.6 (±57.8) (b) Reddit dataset statistics. 4 Approach Two types of transformer-based sarcasm detection models are used for our experiments: a) The target-oriented model takes only the target utterance as input (Section 4.1). b) The context-aware model takes both the target utterance and the context utterances as input (Section 4.2). These two models are coupled with the latest transformer encoders e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2020), and ALBERT (Lan et al., 2019), and compared to evaluate how much impact the context makes to predict whether or not the target utterance involves sarcasm. 4.1 Table 1: Examples of the conversation threads where the target utterances involve sarcasm. Ci : i’th utterance in the context, T: the target utterance. Target-oriented Model Figure 1a shows the overview of the target-oriented model. Let W = {w1 , . . . , wn } be the input target utterance, where wi is the i’th token in W and n is the max-number of tokens in any target utterance. W is first prepended by the s"
2020.figlang-1.38,D17-1050,0,0.174869,"detection, and irony detection (Van Hee et al., 2018). Many computational approaches have been proposed to detect sarcasm in conversations (Ghosh et al., 2015; Joshi et al., 2015, 2016). However, most of the previous studies use the utterances in isolation, which makes it hard even for human to detect sarcasm without the contexts. Thus, it’s essential to interpret the target utterances along with contextual information comprising textual features from the conversation thread, metadata about the conversation from external sources, or visual context (Bamman and Smith, 2015; Ghosh et al., 2017; Ghosh and Veale, 2017; Ghosh et al., 2018). This paper presents a transformer-based sarcasm detection model that takes both the target utterance and its context and predicts if the target utterance involves sarcasm. Our model uses a transformer encoder to coherently generate the embedding representation for the target utterance and the context by performing multi-head attentions (Section 4). This approach is evaluated on two types of datasets collected from Twitter and Reddit (Section 3), and depicts significant improvement over the baseline using only the target utterance as input (Section 5). Our error analysis"
2020.figlang-1.38,W17-5523,0,0.448262,"rofiling, harassment detection, and irony detection (Van Hee et al., 2018). Many computational approaches have been proposed to detect sarcasm in conversations (Ghosh et al., 2015; Joshi et al., 2015, 2016). However, most of the previous studies use the utterances in isolation, which makes it hard even for human to detect sarcasm without the contexts. Thus, it’s essential to interpret the target utterances along with contextual information comprising textual features from the conversation thread, metadata about the conversation from external sources, or visual context (Bamman and Smith, 2015; Ghosh et al., 2017; Ghosh and Veale, 2017; Ghosh et al., 2018). This paper presents a transformer-based sarcasm detection model that takes both the target utterance and its context and predicts if the target utterance involves sarcasm. Our model uses a transformer encoder to coherently generate the embedding representation for the target utterance and the context by performing multi-head attentions (Section 4). This approach is evaluated on two types of datasets collected from Twitter and Reddit (Section 3), and depicts significant improvement over the baseline using only the target utterance as input (Section"
2020.figlang-1.38,J18-4009,0,0.366932,"etection (Van Hee et al., 2018). Many computational approaches have been proposed to detect sarcasm in conversations (Ghosh et al., 2015; Joshi et al., 2015, 2016). However, most of the previous studies use the utterances in isolation, which makes it hard even for human to detect sarcasm without the contexts. Thus, it’s essential to interpret the target utterances along with contextual information comprising textual features from the conversation thread, metadata about the conversation from external sources, or visual context (Bamman and Smith, 2015; Ghosh et al., 2017; Ghosh and Veale, 2017; Ghosh et al., 2018). This paper presents a transformer-based sarcasm detection model that takes both the target utterance and its context and predicts if the target utterance involves sarcasm. Our model uses a transformer encoder to coherently generate the embedding representation for the target utterance and the context by performing multi-head attentions (Section 4). This approach is evaluated on two types of datasets collected from Twitter and Reddit (Section 3), and depicts significant improvement over the baseline using only the target utterance as input (Section 5). Our error analysis illustrates that the"
2020.figlang-1.38,D15-1116,0,0.682358,"used in social media platforms such as Twitter or Reddit to express users’ nuanced intents, the language is often full of spelling errors, acronyms, slangs, emojis, and special characters, which adds another level of difficulty in this task. Despite of its challenges, sarcasm detection has recently gained substantial attention because it can bring the last gist to deep contextual understanding for various applications such as author profiling, harassment detection, and irony detection (Van Hee et al., 2018). Many computational approaches have been proposed to detect sarcasm in conversations (Ghosh et al., 2015; Joshi et al., 2015, 2016). However, most of the previous studies use the utterances in isolation, which makes it hard even for human to detect sarcasm without the contexts. Thus, it’s essential to interpret the target utterances along with contextual information comprising textual features from the conversation thread, metadata about the conversation from external sources, or visual context (Bamman and Smith, 2015; Ghosh et al., 2017; Ghosh and Veale, 2017; Ghosh et al., 2018). This paper presents a transformer-based sarcasm detection model that takes both the target utterance and its contex"
2020.figlang-1.38,P15-2124,0,0.262256,"a platforms such as Twitter or Reddit to express users’ nuanced intents, the language is often full of spelling errors, acronyms, slangs, emojis, and special characters, which adds another level of difficulty in this task. Despite of its challenges, sarcasm detection has recently gained substantial attention because it can bring the last gist to deep contextual understanding for various applications such as author profiling, harassment detection, and irony detection (Van Hee et al., 2018). Many computational approaches have been proposed to detect sarcasm in conversations (Ghosh et al., 2015; Joshi et al., 2015, 2016). However, most of the previous studies use the utterances in isolation, which makes it hard even for human to detect sarcasm without the contexts. Thus, it’s essential to interpret the target utterances along with contextual information comprising textual features from the conversation thread, metadata about the conversation from external sources, or visual context (Bamman and Smith, 2015; Ghosh et al., 2017; Ghosh and Veale, 2017; Ghosh et al., 2018). This paper presents a transformer-based sarcasm detection model that takes both the target utterance and its context and predicts if th"
2020.figlang-1.38,D16-1104,0,0.0174847,"edge rather than its literal sense (Van Hee et al., 2018). Various approaches have been presented for this task. Most earlier works had taken the target utterance without context as input. Both explicit and implicit incongruity features were explored in these works (Joshi et al., 2015). To detect whether certain words in the target utterance involve sarcasm, several approaches based on distributional semantics were proposed (Ghosh et al., 2015). Additionally, word embedding-based features like distance-weighted similarities were also adapted to capture the subtle forms of context incongruity (Joshi et al., 2016). Nonetheless, it is difficult to detect sarcasm by considering only the target utterances in isolation. 276 Proceedings of the Second Workshop on Figurative Language Processing, pages 276–280 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Non-textual features such as the properties of the author, audience and environment were also taken into account (Bamman and Smith, 2015). Both the linguistic and context features were used to distinguish between information-seeking and rhetorical questions in forums and tweets (Oraby et al., 2017). Traditional"
2020.figlang-1.38,W17-5537,0,0.0237139,"incongruity (Joshi et al., 2016). Nonetheless, it is difficult to detect sarcasm by considering only the target utterances in isolation. 276 Proceedings of the Second Workshop on Figurative Language Processing, pages 276–280 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Non-textual features such as the properties of the author, audience and environment were also taken into account (Bamman and Smith, 2015). Both the linguistic and context features were used to distinguish between information-seeking and rhetorical questions in forums and tweets (Oraby et al., 2017). Traditional machine learning methods such as Support Vector Machines were used to model sarcasm detection as a sequential classification task over the target utterance and its surrounding utterances (Wang et al., 2015). Recently, deep learning methods using LSTM were introduced, considering the prior turns (Ghosh et al., 2017) as well as the succeeding turns (Ghosh et al., 2018). Notice the huge variances in the utterance lengths for both the Twitter and the Reddit datasets. For the Reddit dataset, the average lengths of conversations as well as utterances are significantly larger in the tes"
2020.figlang-1.38,S18-1005,0,0.117835,"Missing"
2020.iwpt-1.13,W18-6009,1,0.850714,"tions, we consulted two existing dependency annotation guidelines for Korean: Lee et al. (2019) and Oh (2019). They offer a thorough analysis on applicability of the universal dependency relation labels to Korean, and further identify a list of dependency relations such as iobj, xcomp, expl, and cop (among others) as not suited for capturing characteristics of Korean grammar. Additionally, where applicable, we took into consideration the UD Japanese Treebank (Asahara et al., 2018), since Japanese exhibits many parallel syntactic phenomena as another strictly head-final agglutinative language (Kanayama et al., 2018). Reevaluation of iobj We turned our attention to iobj, the DEPREL label for indirect object. We found PKT-UD v2018’s decision to assign nominals with dative case markings to iobj questionable, for the following reasons. First, unlike English, where word order distinguishes indirect objects from direct objects (e.g. “She gave me:iobj a box:obj”), Korean has no such structural constraint that forms the basis for identifying instances of iobj. The only potential identifier, then, is dative postpositions such as ‘-에게’(to) and ‘한테’(by), which correspond roughly to English preposition ‘to’ as in “S"
2020.iwpt-1.13,L18-1287,0,0.0118941,"mber of tokens in those versions respectively, PC: percentage change. 4.2 DEPREL Revision In re-examining PTK-UD v2018’s dependency relations, we consulted two existing dependency annotation guidelines for Korean: Lee et al. (2019) and Oh (2019). They offer a thorough analysis on applicability of the universal dependency relation labels to Korean, and further identify a list of dependency relations such as iobj, xcomp, expl, and cop (among others) as not suited for capturing characteristics of Korean grammar. Additionally, where applicable, we took into consideration the UD Japanese Treebank (Asahara et al., 2018), since Japanese exhibits many parallel syntactic phenomena as another strictly head-final agglutinative language (Kanayama et al., 2018). Reevaluation of iobj We turned our attention to iobj, the DEPREL label for indirect object. We found PKT-UD v2018’s decision to assign nominals with dative case markings to iobj questionable, for the following reasons. First, unlike English, where word order distinguishes indirect objects from direct objects (e.g. “She gave me:iobj a box:obj”), Korean has no such structural constraint that forms the basis for identifying instances of iobj. The only potentia"
2020.iwpt-1.13,W11-3801,1,0.571467,"dicated by the blue bold font. Figure 1: Example from v2018 and v2020, that translates to “Renault announced last January that it was negotiating an exclusive acquisition with Samsung Motors, and ...”. This example continues in Figure 2. as well but unreleased due to their license issues. Similar to the Kaist UD Treebank, the Penn Korean UD Treebank2 was automatically converted into UD structures from phrase structure trees (Chun et al., 2018). The Sejong UD Treebank was also automatically converted from the Sejong Corpus, a phrase structure Treebank consisting of 60K sentences from 6 genres (Choi and Palmer, 2011). Treebank Sentences Tokens Released Unit Genre GKT 6k 80k O Eojeol Blog, News KTB PUD 27k 1k 350k 16k O O Eojeol Eojeol Litr, Blog, News, News Acdm, Mscr PKT 5k 132k X Eojeol News Sejong 60k 825k X Eojeol Litr, News, Acdm, Mscr Table 1: Korean UD Treebanks. Each abbreviations indicate genres of source texts: webblogs(Blog), newswire(News), literatures(Litr), academic(Acdm), manuscripts(Mscr). In a related effort, the Electronic and Telecommunication Research Institute (ETRI) in Korea conducted a research on standardizing dependency relations and structures (Lim et al., 2015). This effort resu"
2020.iwpt-1.13,L18-1347,1,0.788868,"ntroduced (Section 5) to Related Works Korean UD Corpora According to the UD project website,1 three Korean treebanks are officially registered and released: the Google Korean UD Treebank (McDonald et al., 2013), the Kaist UD Treebank (Choi et al., 1994), and the Parallel Universal Dependencies Treebank (Zeman et al., 2017). These treebanks were created by converting and modifying the previously existing treebanks. The Korean portion of the Google UD Treebank had been re-tokenized into the morpheme level in accordance with other Korean corpora, and systematically corrected for several errors (Chun et al., 2018). The Kaist Korean UD Treebank was derived by automatic conversion using headfinding rules and linguistic heuristics (Chun et al., 2018). The Parallel Universal Dependencies Treebank was designed for the CoNLL 2017 shared task on Multilingual Parsing, consisting of 1K sentences extracted from newswires and Wikipedia articles. The Penn Korean UD Treebank and the Sejong UD Treebank were registered on the UD website 1 https://universaldependencies.org 122 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 122–131 c Virtual Meeting, July 9"
2020.iwpt-1.13,N19-1423,0,0.0136438,"685 589 739 2,199 0 0 5,501 4,114 7,341 9,849 16,891 9 13,794 5,010 0 132,041 PC 653.4 ↑ 56.3 ↓ 5.4 ↑ 0.1 ↓ 0.8 ↓ 15.5 ↓ 100.0 ↓ 79.8 ↓ 39.7 ↓ 25.9 ↓ 28.2 ↓ 100.0 ↓ 0.0 ↓ 98.4 ↓ 0.0 11.6 ↑ 4,005.6 ↑ 100.0 ↑ 100.0 ↓ 100.0 ↓ 1.0 ↓ 2.5 ↑ 4,666.9 ↑ 0.3 ↑ 403.2 ↑ 100.0 ↑ 5.5 ↑ 0.5 ↓ 100.0 ↓ 0.0 Figure 7: Revision of the DEPREL of the separated postposition 을 at ""‘하늘’을 봐 (Look at the ‘sky’)"" in PKT-UD v2020, where case relation for orphaned postposition revised to goeswith. 5 Parsing Approach Our dependency parsing model is based on the biaffine parser using contextualized embeddings such as BERT (Devlin et al., 2019) that has shown the state-of-the-art results on both syntactic and semantic dependency parsing tasks in multiple languages (He and Choi, 2020). This model is simplified from the original biaffine parser introduced by Dozat and Manning (2017) such that trainable token embeddings are removed and lemmas are used instead of word forms. This section proposes an even more simplified model that no longer uses embeddings from POS tags, so it can be easily adapted to languages that do not have dedicated POS taggers, and drops the Bidirectional LSTM encoder while integrating the transformer layers direc"
2020.iwpt-1.13,Y02-1007,1,0.348244,"stly, reducing several critical errors that used to be made by the previous model. 1 1. Issue checking in PKT-UD v2018. 2. Revised annotation guidelines for Korean and the release of the new corpus, PKT-UD v2020. 3. Development of a robust dependency parsing model using the latest transformer encoder. 2 2.1 Introduction In 2018, Chun et al. (2018) published on three dependency treebanks in Korean that followed the latest guidelines from the Universal Dependencies (UD) project, that was UDv2. These treebanks were automatically derived from the existing treebanks, the Penn Korean Treebank (PKT; Han et al. 2001), the Google UD Treebank (McDonald et al., 2013), and the KAIST Treebank (Choi et al., 1994), using head-finding rules and heuristics. This paper first addresses the known issues in the original Penn Korean UD Treebank, henceforth PKT-UD v2018, through a sampling-based analysis (Section 3), and then describes the revised guidelines for both part-of-speech tags and dependency relations to handle those issues (Section 4). Then, a transformer-based dependency parsing approach using biaffine attention is introduced (Section 5) to Related Works Korean UD Corpora According to the UD project website,"
2020.iwpt-1.13,D18-2012,0,0.0192791,"token embeddings are removed and lemmas are used instead of word forms. This section proposes an even more simplified model that no longer uses embeddings from POS tags, so it can be easily adapted to languages that do not have dedicated POS taggers, and drops the Bidirectional LSTM encoder while integrating the transformer layers directly into the biaffine decoder so that it minimizes the redundancy of having multiple encoders for the generation of contextualized embeddings. Given an input sentence, every token wi is first segmented into one or more sub-tokens by the SentencePiece tokenizer (Kudo and Richardson, 2018) and fed into a transformer. The output embedding corresponding to the first sub-token of wi is treated as the embedding representation of wi , say ei , and fed into four types of multilayer perceptron (MLP) layers to extract features for wi being a head (*-h) or a dependent (*-d) for the arc relations (arc-*) and the labels (rel-*) (k and l are the dimensions of the arc and label representations, respectively): h(arc-h) = MLP(arc-h) (ei ) ∈ Rk×1 i h(arc-d) = MLP(arc-d) (ei ) ∈ Rk×1 i Table 3: Universal dependency label comparison between v2018 and v2020 of the Penn Universal Dependency Treeba"
2020.iwpt-1.13,P13-2017,0,0.0806809,"t used to be made by the previous model. 1 1. Issue checking in PKT-UD v2018. 2. Revised annotation guidelines for Korean and the release of the new corpus, PKT-UD v2020. 3. Development of a robust dependency parsing model using the latest transformer encoder. 2 2.1 Introduction In 2018, Chun et al. (2018) published on three dependency treebanks in Korean that followed the latest guidelines from the Universal Dependencies (UD) project, that was UDv2. These treebanks were automatically derived from the existing treebanks, the Penn Korean Treebank (PKT; Han et al. 2001), the Google UD Treebank (McDonald et al., 2013), and the KAIST Treebank (Choi et al., 1994), using head-finding rules and heuristics. This paper first addresses the known issues in the original Penn Korean UD Treebank, henceforth PKT-UD v2018, through a sampling-based analysis (Section 3), and then describes the revised guidelines for both part-of-speech tags and dependency relations to handle those issues (Section 4). Then, a transformer-based dependency parsing approach using biaffine attention is introduced (Section 5) to Related Works Korean UD Corpora According to the UD project website,1 three Korean treebanks are officially register"
2020.iwpt-1.19,P18-2077,0,0.302526,"2017) introduce deep dependency graphs that address several limitations in UD tree structures. Schuster et al. (2017) analyze gapping constructions in the enhanced UD representation. Nivre et al. (2018) evaluate both rule-based and data-driven systems for adding enhanced dependencies to existing treebanks. Apart from syntactic relations, researchers are moving towards semantic dependency parsing (Oepen et al., 2015) for more direct analysis of entities and events. The efforts of treebank construction stimulates the interest of many researchers in improving the performance of semantic parsers (Dozat and Manning, 2018; Du et al., 2015; Almeida and Martins, 2015). This paper presents our parsing approach to the Shared Task on Enhanced Universal Dependencies at IWPT 2020 (Nivre et al., 2016; Bouma et al.).1 Our system is a simplified version of the transformer-based dependency parsers presented by He and Choi (2020), which employs the deep biaffine dependency parsing decoder (Dozat and Manning, 2017) over the transformer encoder, BERT (Devlin et al., 2019). We simplify their network by removing the LSTM and fine-tuning their static transformer encoder. In order to effectively predict the enhanced dependencie"
2020.iwpt-1.19,J93-2004,0,0.0692795,"up layer by layer. Our parser employs pre-trained transformer models in Section 3.2. For the decoder, the deep biaffine attention (Dozat and Manning, 2017) dominates the graph based approach since its establishment. The top ranked graph-based dependency parser at the CoNLL 2017 Shared Task (Dozat et al., 2017) adopts biaffine attention with rich character level features. With a parsing algorithm other than MST, the biaffine parser is successfully extended to semantic dependency parsing (Dozat and Manning, 2018). The current state-of-the-art dependency parsing records on English Penn Treebank (Marcus et al., 1993) and Chinese Treebank (Xue et al., 2005) are maintained by the Head-Driven Phrase Structure parser (Zhou and Zhao, 2019), which jointly learns constituency parsing and dependency parsing with layers including biaffine attention. Apart from parsing, biaffine attention has also been applied to graph related task including relation extraction (Nguyen and Verspoor, 2019) and coreference resolution (Zhang et al., 2018). 6 Conclusion This paper describes our parsing approach to enhanced universal dependencies for the IWPT 2020 shared task. We find that the multilingual BERT encoder is able to parse"
2020.iwpt-1.19,K17-3009,0,0.0881068,"Missing"
2020.semeval-1.299,N19-1423,0,0.598433,"press their views and exchange ideas publicly. However, some people may take advantage of the anonymity in social media platform to express their comments rudely, and attack other people verbally with offensive language. To keep a healthy online environment for the adolescences (Chen et al., 2012) and to filter offensive messages for the users (Razavi et al., 2010), it is necessary and significant for technology companies to develop an efficient and effective computational methods to identify offensive language automatically. Transformer-based contextualized embedding approaches such as BERT (Devlin et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020) or ELECTRA (Clark et al., 2020) have re-established the state-of-the-art for many natural language classification tasks especially the GLUE Dataset (Wang et al., 2018). Their pre-trained models were pre-trained on different large datasets, for example, BERT was pre-trained on the B OOK C ORPUS (Zhu et al., 2015) and English Wikipedia, and RoBERTa was pre-trained on CC-N EWS (Nagel, 2016), O PEN W EB T EXT (Gokaslan and Cohen, 2019), and S TORIES (Trinh and Le, 2018) which enable their models to learn different"
2020.semeval-1.299,W17-3013,0,0.0598522,"Missing"
2020.semeval-1.299,W18-4401,0,0.0524115,"Missing"
2020.semeval-1.299,malmasi-zampieri-2017-detecting,0,0.0241328,"cts of offensive language have been studied, like the type and target of offensive posts (Zampieri et al., 2019), cyberbullying (Dinakar et al., 2011; Huang et al., 2014), aggression (Kumar et al., 2018), toxic comments (Georgakopoulos et This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 2244 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2244–2250 Barcelona, Spain (Online), December 12, 2020. al., 2018) and hate speech (Badjatiya et al., 2017; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018). Many deep learning approaches have been used to address the task. The Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs) and FastText were applied on the hate speech detection task (Badjatiya et al., 2017). Gamback and Sikdar (2017) used four Convolutional Neural Network (CNN) models with random word vectors, word2vec word vectors, character n-gram, and concatenation of word2vec word embeddings and character n-grams as feature embeddings separately to categorize each tweet into four classes: racism, sexism, both (racism and sexism) and"
2020.semeval-1.299,W18-5446,0,0.032504,"ment for the adolescences (Chen et al., 2012) and to filter offensive messages for the users (Razavi et al., 2010), it is necessary and significant for technology companies to develop an efficient and effective computational methods to identify offensive language automatically. Transformer-based contextualized embedding approaches such as BERT (Devlin et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020) or ELECTRA (Clark et al., 2020) have re-established the state-of-the-art for many natural language classification tasks especially the GLUE Dataset (Wang et al., 2018). Their pre-trained models were pre-trained on different large datasets, for example, BERT was pre-trained on the B OOK C ORPUS (Zhu et al., 2015) and English Wikipedia, and RoBERTa was pre-trained on CC-N EWS (Nagel, 2016), O PEN W EB T EXT (Gokaslan and Cohen, 2019), and S TORIES (Trinh and Le, 2018) which enable their models to learn different language features. This paper presents six transformer-based offensive language identification models that learn different features from the target utterance. To combine the distinctive learned language features, we introduce an ensemble strategy whic"
2020.semeval-1.299,N19-1144,0,0.0565349,"mbine the distinctive learned language features, we introduce an ensemble strategy which concatenates the representations of the individual models and feed them into the linear decoder to make binary classification (Section 4.2). It largely improves the performance over the baseline on our dev set (Section 4.4). 2 Related Work Offensive language in Twitter (Wiegand et al., 2018), Facebook (Kumar et al., 2018), and Wikipedia (Georgakopoulos et al., 2018) has been widely studied. In addition, different aspects of offensive language have been studied, like the type and target of offensive posts (Zampieri et al., 2019), cyberbullying (Dinakar et al., 2011; Huang et al., 2014), aggression (Kumar et al., 2018), toxic comments (Georgakopoulos et This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 2244 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2244–2250 Barcelona, Spain (Online), December 12, 2020. al., 2018) and hate speech (Badjatiya et al., 2017; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018). Many deep learning approaches have been used to address"
2020.sigdial-1.29,P19-1005,0,0.141139,"s including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: aut"
2020.sigdial-1.29,W19-2310,0,0.0625606,"mputational Linguistics in these years, it presents a unique opportunity to observe and assess which evaluation metrics have been most widely adopted by the larger community in this period of expeditious development. We provide a detailed survey of both automated and human evaluations in order to present the most accurate depiction of the current evaluation protocols. However, our in-depth analysis is limited to that of the human evaluations due to the abundance of previous work in automated metric analysis. As such, we defer to such work as Liu et al. (2016), Ghandeharioun et al. (2019), and Ghazarian et al. (2019) for more detail on automated metrics. As a part of our analysis, we also present a case study of real human-machine dialogues which explores the significance of different human evaluation metrics in terms of overall user satisfaction through an expert analysis. As a result of our work, the most commonly used evaluation metrics in contemporary literature - both automated and human - are revealed in detail and our findings towards the prevalence, impact, and applicability of human evaluation metrics are illustrated. 2 Method AUT STA INT Evaluation Protocols For a holistic understanding of curre"
2020.sigdial-1.29,N19-1169,0,0.0174673,"ocus on task-oriented dialogue systems, which does not translate well to chat-oriented dialogue systems (Walker et al., 1997; Malchanau et al., 2019). Previous works which have included chat-oriented evaluations have lacked comprehensive coverage over the many varieties of such evaluation procedures that are currently in use. Instead, the emphasis has rested primarily on automated metrics at the expense of detailed analysis of human evaluation (Deriu et al., 2019). At this stage in conversational AI, it is probable that automated and human metrics reveal different aspects of dialogue systems (Hashimoto et al., 2019). It would be remiss to focus on a single evaluation category when assessing the state of the field. For this reason, our work aims to fill in the gaps of previous dialogue system evaluation surveys by identifying and comparing human evaluation protocols for chat-oriented dialogue systems. To this end, we present a comparative analysis of the evaluations used for chat-oriented dialogue systems over the past several years. Since the field of conversational AI has experienced a rapid growth 236 Proceedings of the SIGdial 2020 Conference, pages 236–245 c 1st virtual meeting, 01-03 July 2020. 2020"
2020.sigdial-1.29,D18-1071,0,0.0837019,"evant papers since 2018, primarily from top-tier venues, and synthesized their methods. These papers focus on open domain (or nontask-oriented) dialogue, and employ a variety of approaches including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9,"
2020.sigdial-1.29,N16-1014,0,0.0364372,"entailment scores between response and persona description (Madotto et al., 2019) • Coherence: average word embedding similarity between dialogue context and generated response (Xu et al., 2018) Application-Specific The other observed metrics can be considered application-specific since Entity A/R is used to measure the ability of the system to produce the correct entities in its responses and C is specifically created as a measure of the consistency between the dialogue responses and their respective persona descriptions. • Distinct: a subset of Distinct-1, Distinct2, and Distinct-sentence (Li et al., 2016) • Embedding: a subset of average, extrema, and greedy embedding similarity (Liu et al., 2016) • Entity A/R: Accuracy and recall for including the correct entities in the response (Liu et al., 2018) • Entity Score: average number of entities per response (Young et al., 2018) • Entropy: average character-level entropy over all responses (Mou et al., 2016) 4 Analysis of Human Evaluation While automated evaluation measures dimensions of dialogue objectively, human evaluation captures the subjective assessment from the user’s point of view. Regardless of the exact method chosen, all human evaluati"
2020.sigdial-1.29,P19-1002,0,0.0815389,"AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interac"
2020.sigdial-1.29,W04-1013,0,0.0733541,"dialogues. The sampled dialogues together with the system responses are provided to human evaluators to assess. Because only the last utterance in these excerpts are generated by the dialogue systems, it is difficult to evaluate sequential aspects about dialogue management through static evaluation (e.g., coherence among responses generated by the same system). 2.3 • Inertia: inertia on the clusters of embeddings of responses (Du and Black, 2019) • Perplexity: inverse likelihood of predicting the responses of the test set (Chen et al., 1998) • ROUGE: a subset of ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) The automated metrics in Table 2 fall into the following five categories: Interactive Evaluation Ground Truth Response Similarity Most commonly used automated metrics focus on assessing how well system responses match the ground truth human responses, using word overlap (BLEU, ROUGE) or embedding similarity. Unlike static evaluation, interactive evaluation has the same person play the role of both the user (one who interacts with the system) and the evaluator. In this setup, the evaluator has a conversation with the dialogue system and makes the assessment at the end of the conversation. Even"
2020.sigdial-1.29,D19-1012,0,0.132655,"Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: ap"
2020.sigdial-1.29,D16-1230,0,0.175361,"Missing"
2020.sigdial-1.29,P18-1138,0,0.182103,"18, primarily from top-tier venues, and synthesized their methods. These papers focus on open domain (or nontask-oriented) dialogue, and employ a variety of approaches including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 1"
2020.sigdial-1.29,D18-1075,0,0.161984,"p-tier venues, and synthesized their methods. These papers focus on open domain (or nontask-oriented) dialogue, and employ a variety of approaches including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19"
2020.sigdial-1.29,P19-1542,0,0.185965,"ality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protoc"
2020.sigdial-1.29,D18-1255,0,0.215886,"nthesized their methods. These papers focus on open domain (or nontask-oriented) dialogue, and employ a variety of approaches including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Tabl"
2020.sigdial-1.29,C16-1316,0,0.0114286,"the correct entities in its responses and C is specifically created as a measure of the consistency between the dialogue responses and their respective persona descriptions. • Distinct: a subset of Distinct-1, Distinct2, and Distinct-sentence (Li et al., 2016) • Embedding: a subset of average, extrema, and greedy embedding similarity (Liu et al., 2016) • Entity A/R: Accuracy and recall for including the correct entities in the response (Liu et al., 2018) • Entity Score: average number of entities per response (Young et al., 2018) • Entropy: average character-level entropy over all responses (Mou et al., 2016) 4 Analysis of Human Evaluation While automated evaluation measures dimensions of dialogue objectively, human evaluation captures the subjective assessment from the user’s point of view. Regardless of the exact method chosen, all human evaluations involve gathering external annotators who answer questions regarding the dialogues resulting from a dialogue system. 238 1 BLEU C Coherence Distinct Embedding Entity A/R Entity Score Entropy Inertia Perplexity ROUGE 2 3 3 3 3 4 3 5 3 6 3 7 8 3 3 3 9 3 10 3 11 3 12 3 3 3 3 13 3 14 3 15 3 16 3 17 3 3 3 3 3 3 3 3 3 18 19 20 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3"
2020.sigdial-1.29,P02-1040,0,0.117148,"ext Coherence Embedding similarities between dialogue contexts and system responses have been used to quantitatively assess the relevance between the system responses and the preceding dialogue history (Coherence, Embedding). Analysis of Automated Evaluation Response Diversity Other widespread metrics assess the diversity of the system responses in order to determine the amount of repetition and generic content in the system responses (Distinct, Entropy, Inertia, Entity Score). Table 2 shows the 11 metrics used for automated evaluation in our survey: • BLEU: a subset of BLEU-1 through BLEU-4 (Papineni et al., 2002) Language Model Fitness Generative models are usually evaluated in terms of how well they learn to model the language of the dialogues in their training corpus (Perplexity). • C: sum of entailment scores between response and persona description (Madotto et al., 2019) • Coherence: average word embedding similarity between dialogue context and generated response (Xu et al., 2018) Application-Specific The other observed metrics can be considered application-specific since Entity A/R is used to measure the ability of the system to produce the correct entities in its responses and C is specifically"
2020.sigdial-1.29,D18-1073,0,0.0556503,". These papers focus on open domain (or nontask-oriented) dialogue, and employ a variety of approaches including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three eva"
2020.sigdial-1.29,P19-1372,0,0.142725,"f emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protocols. 2.1 • Incorporati"
2020.sigdial-1.29,P19-1371,0,0.0425518,"nses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protocols. 2.1 • Incorporation of knowledge bases ["
2020.sigdial-1.29,P97-1035,0,0.741206,"Missing"
2020.sigdial-1.29,P19-1369,0,0.172945,"nding on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protocols. 2.1 • Incorporation of knowledge bases [2, 4, 7, 18, 20] 1 ti"
2020.sigdial-1.29,D18-1432,0,0.0336771,"Missing"
2020.sigdial-1.29,P19-1362,0,0.0414728,"sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protocols. 2.1 • Incorporation of knowledge bases [2, 4, 7, 18, 20] 1 tive. Automated evaluation"
2020.sigdial-1.29,P18-1205,0,0.428981,"a variety of approaches including:1 AUT & STA AUT & INT STA & INT • Integration of personality [8, 12] • Handling of emotion-driven responses [10] • Purely depending on neural-based sequenceto-sequence models [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding pro"
2020.sigdial-1.29,D19-1192,0,0.0479202,"els [19] Based on these papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protocols. 2.1 • Incorporation of knowledge bases [2, 4, 7, 18, 20] 1 tive. Automated evaluation is performed systemati"
2020.sigdial-1.29,P19-1366,0,0.0623756,"papers, three main categories are found as evaluation protocols for open-domain dialogue systems: automated, static, and interacThroughout the paper, the following are used to refer to the related work: 1: Li and Sun (2018) 2: Liu et al. (2018) 3: Luo et al. (2018) 4: Moghe et al. (2018) 5: Parthasarathi and Pineau (2018) 6: Xu et al. (2018) 7: Young et al. (2018) 8: Zhang et al. (2018) 9: Du and Black (2019) 10: Li et al. (2019) 11: Lin et al. (2019) 12: Madotto et al. (2019) 13: Qiu et al. (2019) 14: Tian et al. (2019) 15: Wu et al. (2019) 16: Zhang et al. (2019) 17: Zhou et al. (2019) 18: Zhu et al. (2019) 19: Adiwardana et al. (2020) 20: Wang et al. (2020). References [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 13, 14, 15, 16, 17, 20] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 18, 19, 20] [8, 19] [1, 3, 4, 7, 9, 10, 11, 12, 13, 14 15, 16, 17, 20] [] [19] # 17 16 2 14 0 1 Table 1: Distributions of the three evaluation protocols. #: number of papers using the corresponding protocol, AUT/STA/INT: automated/static/interactive evaluation. &: approaches using both protocols. 2.1 • Incorporation of knowledge bases [2, 4, 7, 18, 20] 1 tive. Automated evaluation is performed systematically by a batch scrip"
2020.sigdial-1.32,D19-3032,0,0.0149695,"emerged to expedite the process of dialogue system creation. These frameworks cater to various use cases and levels of developer expertise. Popular commercial-oriented frameworks are primarily intended for non-experts and have workflows supporting rapid prototyping (Bocklisch et al., 2017). They often allow developers to customize natural language understanding (NLU) modules and perform dialogue management using state machines. Some frameworks require more expertise, but offer better developer control, by following the information state formulation of dialogue management (Ultes et al., 2017; Jang et al., 2019; Kiefer et al., 2019). According to this formulation, dialogues are driven by iterative application of logical implication rules (Larsson and Traum, 2000). This design provides support for complex interactions, but sacrifices the intuitiveness and development speed of dialogue management based on state machines. Other frameworks (e.g., ChatScript, botml) rely on custom programming languages to design conversation flow. The custom syntax they specify is based on pattern matching. Although requiring expertise, rapid prototyping in these frameworks is possible with a high degree of developer’s c"
2020.sigdial-1.32,P17-4013,0,0.0414358,"Missing"
2021.cmcl-1.18,N18-1202,0,0.365515,"result for classifying 32 emotions. Our layer analysis can derive an emotion graph to depict hierarchical relations among the emotions. Our emotion representations can be used to generate an emotion wheel directly comparable to the one from Plutchik’s model, and also augment the values of missing emotions in the PAD emotional state model. 1 Introduction Jinho D. Choi Computer Science Emory University Atlanta GA 30322, USA jinho.choi@emory.edu interest of interpretability in deep learning models, several studies have attempted to capture various knowledge encoded in language (Adi et al., 2017; Peters et al., 2018; Hewitt and Manning, 2019), and shown that it is possible to learn computational representations through distributional semantics for abstract concepts. Inspired by these prior studies, we build a deep learning-based framework to generate emotion embeddings from text and assess its ability of enhancing cognitive models of emotions. Our contributions are summarized as follows:1 • To develop a deep probing model that allows us to interpret the process of representation learning on emotion classification (Section 3). • To achieve the state-of-the-art result on the Empathetic Dialogue dataset for"
2021.cmcl-1.18,P19-1534,0,0.0282178,"⋯ wn e11 ⋯ PHℓ1 eℓ1 ⋯ ⋯ e0 ⋯ ⊕ ⋯ w2 E N C O D E R ⋯ w1 PH11 ⊕ PH1k e1k ⋯ PHℓk N O R M L I N E A R o eℓk Figure 1: The overview of our deep learning-based multi-head probing model. NLP researchers have produced several corpora for emotion detection including FriendsED (Zahiri and Choi, 2018), EmoInt (Mohammad et al., 2017), EmoBank (Buechel and Hahn, 2017), and DailyDialogs (Li et al., 2017), all of which are based on coarse-grained emotions with at most 7 categories. For a more comprehensive analysis, we adapt the Empathetic Dialogue dataset based on fine-grained emotions with 32 categories (Rashkin et al., 2019). 3 Multi-head Probing Model 4 4.1 Experiments Contextualized Embedding Encoder For all experiments, BERT (Devlin et al., 2019) is used as the contextualized embedding encoder for our multi-head probing model in Section 3. BERT prepends the special token CLS to the input document W such that W 0 = {CLS} ⊕ W is fed into the ENCODER in Figure 1 instead, which generates the document embedding e0 by applying several layers of multi-head attentions to CLS along with the other tokens in W (Vaswani et al., 2017).2 We present a multi-head probing model allowing us to interpret how emotion embeddings a"
2021.codi-sharedtask.6,P15-1136,0,0.0718197,"Missing"
2021.codi-sharedtask.6,P16-1061,0,0.0498735,"Missing"
2021.codi-sharedtask.6,2020.tacl-1.5,0,0.0363026,"Missing"
2021.codi-sharedtask.6,D19-1588,0,0.180401,"the 1st place on the leaderboard of the anaphora resolution track in the CRAC 2021 shared task, and achieves the best evaluation results on all four datasets. 1 Introduction Coreference resolution of anaphoric identities (a.k.a. anaphora resolution) is a long-studied Natural Language Processing (NLP) task, and is still considered one of the unsolved problems, as it demands deep semantic understanding as well as world knowledge. Although there is a significant performance boost recently by the neural decoders (Lee et al., 2017, 2018) and deep contextualized encoders such as BERT and SpanBERT (Joshi et al., 2019, 2020), the majority of the experiments are based on OntoNotes (Pradhan et al., 2012) from the CoNLL 2012 shared task, which may overestimate the model performance due to two perspectives: the lack of support for harder cases such as singletons and split-antecedents, and the lack of focus on real-world dialogues. In this work, we target on the task of anaphora resolution in the CRAC 2021 shared task (Khosla et al., 2021) that addresses both perspectives, and present an effective coreference resolution system that is adapted from the recent end-to-end coreference model. All datasets in the CRA"
2021.codi-sharedtask.6,2021.codi-sharedtask.1,0,0.370547,"nowledge. Although there is a significant performance boost recently by the neural decoders (Lee et al., 2017, 2018) and deep contextualized encoders such as BERT and SpanBERT (Joshi et al., 2019, 2020), the majority of the experiments are based on OntoNotes (Pradhan et al., 2012) from the CoNLL 2012 shared task, which may overestimate the model performance due to two perspectives: the lack of support for harder cases such as singletons and split-antecedents, and the lack of focus on real-world dialogues. In this work, we target on the task of anaphora resolution in the CRAC 2021 shared task (Khosla et al., 2021) that addresses both perspectives, and present an effective coreference resolution system that is adapted from the recent end-to-end coreference model. All datasets in the CRAC 2021 shared task are in the Universal Anaphora format. For simplicity, we refer to it as the UA format, and refer to 55 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 55–62 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics 2 Related Work Both sm and sa are computed by the FeedForward Neural Network (FFNN), and"
2021.codi-sharedtask.6,W12-4501,0,0.567604,"shared task, and achieves the best evaluation results on all four datasets. 1 Introduction Coreference resolution of anaphoric identities (a.k.a. anaphora resolution) is a long-studied Natural Language Processing (NLP) task, and is still considered one of the unsolved problems, as it demands deep semantic understanding as well as world knowledge. Although there is a significant performance boost recently by the neural decoders (Lee et al., 2017, 2018) and deep contextualized encoders such as BERT and SpanBERT (Joshi et al., 2019, 2020), the majority of the experiments are based on OntoNotes (Pradhan et al., 2012) from the CoNLL 2012 shared task, which may overestimate the model performance due to two perspectives: the lack of support for harder cases such as singletons and split-antecedents, and the lack of focus on real-world dialogues. In this work, we target on the task of anaphora resolution in the CRAC 2021 shared task (Khosla et al., 2021) that addresses both perspectives, and present an effective coreference resolution system that is adapted from the recent end-to-end coreference model. All datasets in the CRAC 2021 shared task are in the Universal Anaphora format. For simplicity, we refer to i"
2021.codi-sharedtask.6,P15-1137,0,0.0274496,"ues. In Section 3.3, we further adapt more speaker encoding to benefit multispeaker dialogues and the personal pronoun issue. Pretrained Transformers encoders have been successfully adopted by recent coreference resolution models and shown significant improvement (Joshi et al., 2019, 2020). We also adopt the Transformers encoder in our approach because of its superior performance. For the neural decoder, there have been two popular directions from recent work. One is mention-ranking-based, where the model predicts only one antecedent for each mention without focusing on the cluster structure (Wiseman et al., 2015; Lee et al., 2017; Wu et al., 2020). The other is cluster-based, where the model maintains the predicted clusters and performs cluster merging (Clark and Manning, 2015, 2016; Xia et al., 2020; Yu et al., 2020). We adopt the mention-ranking framework in our approach because of its simplicity as well as its state-of-the-art decoding performance. 3 3.1 For inference, the selected antecedent is the preceding candidate with the most pairwise score, denoted by argmaxy0 ∈Yi s(xi , y 0 ). For training, the marginal log-likelihood of all gold antecedents Yˆi ⊆ Yi for each xi ∈ X is optimized, denoted"
2021.codi-sharedtask.6,D19-1199,0,0.0541049,"Missing"
2021.codi-sharedtask.6,2020.acl-main.622,0,0.077415,"re speaker encoding to benefit multispeaker dialogues and the personal pronoun issue. Pretrained Transformers encoders have been successfully adopted by recent coreference resolution models and shown significant improvement (Joshi et al., 2019, 2020). We also adopt the Transformers encoder in our approach because of its superior performance. For the neural decoder, there have been two popular directions from recent work. One is mention-ranking-based, where the model predicts only one antecedent for each mention without focusing on the cluster structure (Wiseman et al., 2015; Lee et al., 2017; Wu et al., 2020). The other is cluster-based, where the model maintains the predicted clusters and performs cluster merging (Clark and Manning, 2015, 2016; Xia et al., 2020; Yu et al., 2020). We adopt the mention-ranking framework in our approach because of its simplicity as well as its state-of-the-art decoding performance. 3 3.1 For inference, the selected antecedent is the preceding candidate with the most pairwise score, denoted by argmaxy0 ∈Yi s(xi , y 0 ). For training, the marginal log-likelihood of all gold antecedents Yˆi ⊆ Yi for each xi ∈ X is optimized, denoted by the loss Lc : Approach Mention-Ra"
2021.codi-sharedtask.6,D17-1018,0,0.241893,"performance, with up to 27 F1 improvement over the baseline. Our final system ranks the 1st place on the leaderboard of the anaphora resolution track in the CRAC 2021 shared task, and achieves the best evaluation results on all four datasets. 1 Introduction Coreference resolution of anaphoric identities (a.k.a. anaphora resolution) is a long-studied Natural Language Processing (NLP) task, and is still considered one of the unsolved problems, as it demands deep semantic understanding as well as world knowledge. Although there is a significant performance boost recently by the neural decoders (Lee et al., 2017, 2018) and deep contextualized encoders such as BERT and SpanBERT (Joshi et al., 2019, 2020), the majority of the experiments are based on OntoNotes (Pradhan et al., 2012) from the CoNLL 2012 shared task, which may overestimate the model performance due to two perspectives: the lack of support for harder cases such as singletons and split-antecedents, and the lack of focus on real-world dialogues. In this work, we target on the task of anaphora resolution in the CRAC 2021 shared task (Khosla et al., 2021) that addresses both perspectives, and present an effective coreference resolution system"
2021.codi-sharedtask.6,2020.emnlp-main.695,0,0.0342166,"Missing"
2021.codi-sharedtask.6,N18-2108,0,0.126292,"Missing"
2021.codi-sharedtask.6,2020.emnlp-main.686,1,0.916594,"OD datasets shown in the bottom part. TRN/DEV/TST: the train/dev/test split. #D: total number of documents; #M: total number of mentions; #C: total number of clusters; #S: averaged number of speakers per document (excluding unknown speakers). all non-referring expressions and regard them as non-mentions, as they will not be counted in the final evaluation (Section 5). In addition, our current approach does not consider split-antecedents, which we will leave as future work. 4.3 Implementation Our system is based on the PyTorch implementation of the end-to-end coreference resolution model from Xu and Choi (2020), and we follow the similar hyperparameter settings. Specifically, SpanBERTLarge (Joshi et al., 2020) is used as the Transformers encoder with maximum sequence length of 512. Long documents are split into multiple sequences, and each sequence is encoded by SpanBERTLarge independently, as suggested by (Joshi et al., 2019). During training, we limit the maximum sequences to be 3 due to the GPU memory constraints, and a long document will be truncated into multiple documents if it exceeds the maximum sequences. Preprocessing Hyperparameters For all datasets, nested mentions are always enabled. We"
2021.codi-sharedtask.6,L16-1145,0,0.0249332,"ment (dev) set. In addition, four other corpora from the CRAC 2021 shared task are also used as the development set as well as the final test set, namely AMI, LIGHT, Persuasion for Good (PSUA), Switchboard (SWBD). All above datasets are of the dialogue domain except for RST and GNOME. Table 2 shows the detailed statistics of all UAD datasets. Note that certain datasets do not provide speaker information, therefore their averaged numbers of speakers per document are shown as 0. For non-UA format data (OD), we use two datasets in the CoNLL format: OntoNotes (ON) (Pradhan et al., 2012) and BOLT (Li et al., 2016). OntoNotes consists of documents in six genres, where only two genres “Telephone Conversation” and “Broadcast Conversation” are of the dialogue domain; we use the same provided train/dev/test split for OntoNotes. BOLT has the same annotation scheme as OntoNotes and consists of documents from discussion forums, instant messages and telephone conversations. We perform a random 80/10/10 split for the train/dev/test set of BOLT. Detailed statistics of both datasets are shown in the bottom of Table 2. 4.2 #D #M #C #S TRAINS-93 PEAR RST GNOME 98 20 413 5 12148 3401 62409 5499 4523 1168 38724 2598 0"
2021.codi-sharedtask.6,2020.lrec-1.2,0,0.0968644,"solution models and shown significant improvement (Joshi et al., 2019, 2020). We also adopt the Transformers encoder in our approach because of its superior performance. For the neural decoder, there have been two popular directions from recent work. One is mention-ranking-based, where the model predicts only one antecedent for each mention without focusing on the cluster structure (Wiseman et al., 2015; Lee et al., 2017; Wu et al., 2020). The other is cluster-based, where the model maintains the predicted clusters and performs cluster merging (Clark and Manning, 2015, 2016; Xia et al., 2020; Yu et al., 2020). We adopt the mention-ranking framework in our approach because of its simplicity as well as its state-of-the-art decoding performance. 3 3.1 For inference, the selected antecedent is the preceding candidate with the most pairwise score, denoted by argmaxy0 ∈Yi s(xi , y 0 ). For training, the marginal log-likelihood of all gold antecedents Yˆi ⊆ Yi for each xi ∈ X is optimized, denoted by the loss Lc : Approach Mention-Ranking (MR) Our baseline model MR adopts the mention-ranking strategy, and follows the architecture of the end-toend neural coreference resolution model (Lee et al., 2017, 201"
2021.codi-sharedtask.6,N09-2051,0,0.0869823,"Missing"
2021.codi-sharedtask.6,C18-1003,1,0.854521,"ask as the CoNLL format. The UA format is an extension of the CoNLL format, and further supports bridging references and discourse deixis. For anaphora resolution, the UA format differs from the CoNLL format on three aspects: the support of singletons, split-antecedents, and non-referring expressions (excluded from the current evaluation). Our approach specifically addresses the singleton problem (Section 3.1), which is shown to be a critical component under the UA setting that brings 17-22 F1 improvement on all datasets (Section 5.2). Few recent work has studied the split-antecedent problem (Zhou and Choi, 2018), and we leave the split-antecedents as future work. In addition to singletons, our approach also emphasizes on the speaker encoding (Section 3.3) and knowledge transfer (Section 3.4) to address the dialogue-domain perspective. Especially, we use a simple strategy of speaker-augmented encoding that captures the speaker interaction and dialogueturn information, utilizing the strong Transformers encoder. It has been shown by the previous study that conversational metadata such as speakers can be significant for coreference resolution on dialogue documents (Luo et al., 2009), and we do see 2-3 F1"
2021.crac-1.3,D19-1588,0,0.014756,"ory Language and Information Toolkit (ELIT) (Xu and Choi, 2020), to examine how well the existing coreference resolution model accounts for our annotated corpus, FantasyCoref. We experiment on end-to-end coreference systems with and without higher-order inference (HOI) approaches implemented in ELIT: attended antecedent (AA), entity equalization (EE), span clustering (SC), and cluster merging (CM). The end-to-end coreference system is based on c2f-coref model (Lee et al., 2018) and SpanBERT (Joshi et al., 2020), and we adopt the “independent” splitting variant for long documents introduced by Joshi et al. (2019). The higher-order inference is a widely adapted method for global optimization of coreference links. Both AA and EE refines mention representation by aggregating its antecedents’ information. While AA uses the distribution over antecedents from the span-ranking process as attention mechanism for refinement, EE aggregates all mentions in the cluster thus equalizing all representations of mentions in the same cluster. SC also refines mention representation from spans in cluster where it belongs, but differs from EE that it constructs the actual clusters from true predicted entities. CM ranks an"
2021.crac-1.3,P19-1066,0,0.0271835,"Missing"
2021.crac-1.3,2020.lrec-1.6,0,0.317691,"ty together in a document (Pradhan et al., 2011, 2012). A few coreference datasets have been created for several genres (Hovy et al., 2006; Li et al., 2016; Zhou and Choi, 2018); however, coreference resolution on literary texts has been comparatively underexplored. 24 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 24–35 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics lyze this issue in its guidelines. Moreover, previous studies of coreference resolution on literary texts (Bamman et al., 2020; Roesiger et al., 2018; Yoder et al., 2021) present only limited analyses on dynamic issues that occur in fictional texts (e.g., asymmetry of knowledge, comprehensive physical or status change in entities). The lack of a high-quality dataset in terms of size and consistency is the major hindrance in this task. This inspires us to create a new coreference corpus on literature and evaluate a state-of-the-art coreference system on this genre to confirm the feasibility of this research. Contributions of this work are as follows: AFT) have also been annotated to be used as a separate test set. AFT"
2021.crac-1.3,N18-2108,0,0.0307999,"Missing"
2021.crac-1.3,N19-1220,0,0.0542051,"Missing"
2021.crac-1.3,L16-1145,0,0.0347182,"Missing"
2021.crac-1.3,H05-1004,0,0.0697587,"is occurs when the knowledge is shared differently (a) between the reader and the characters or (b) between characters or when (c) the reader’s knowledge changes throughout the plot (e.g., a plot-twist) (Bamman et al., 2020). Furthermore, we elaborate this issue by categorizing it into deception (lie, disguise), mistaking, secret and a plot-twist. In these cases, we follow the omniscient writer’s point of view. 2.3 Inter-Annotator Agreement To ensure the quality of our annotation, the standard coreference evaluation metrics, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005), are used to estimate interannotator agreement (IAA). The Kappa score (Cohen, 1960) is considered the standard method of Changes in Entities In literary texts, changes often take place in the development of entities (e.g., 26 3.2 evaluating IAA in NLP tasks; however, it evaluates IAA in a pair-wise fashion, while coreference resolution assigns a markable to a coreference chain. Therefore, to avoid the Kappa score overpenalizing the wrongly assigned markable (He, 2007), we used the aforementioned metrics instead. The agreement scores are calculated in four stages. First three stages were on GF"
2021.crac-1.3,W12-4501,0,0.0924717,"Missing"
2021.crac-1.3,W11-1901,0,0.0374796,"hree linguists is formed for this project, who use an open source tool called CorefAnnotator (Reiter, 2018) to create our FantasyCoref corpus. Our annotation guidelines are largely based on the OntoNotes Coreference Guidelines 7.0 (Hovy et al., 2006), while referring to other studies on literary texts (Bamman et al., 2020; Roesiger et al., 2018) to consider the characteristics of the genre. The referents are annotated with the omniscient writer’s point of view. Furthermore, while defining coreference relations, we adopt the entity-cluster view, which is also adopted by the CoNLL shared tasks (Pradhan et al., 2011, 2012). This view indicates that two or more mentions are non-hierarchically grouped into the same entity cluster (Cranenburgh, van, 2019). • We create the corpus called FantasyCoref, comprising of 211 stories in Grimms’ Fairy Tales, two stories from The Arabian Nights, and Alice’s Adventures in Wonderland with coreference annotation that shows high interannotator agreement (Section 3). • We evaluate a state-of-the-art coreference system on FantasyCoref (Section 4), and give error analysis specific to fictional texts, depicting limitations of the current system on this genre. We also show how"
2021.crac-1.3,N06-2015,0,0.212566,"Missing"
2021.crac-1.3,W18-4515,0,0.221178,"ment (Pradhan et al., 2011, 2012). A few coreference datasets have been created for several genres (Hovy et al., 2006; Li et al., 2016; Zhou and Choi, 2018); however, coreference resolution on literary texts has been comparatively underexplored. 24 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 24–35 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics lyze this issue in its guidelines. Moreover, previous studies of coreference resolution on literary texts (Bamman et al., 2020; Roesiger et al., 2018; Yoder et al., 2021) present only limited analyses on dynamic issues that occur in fictional texts (e.g., asymmetry of knowledge, comprehensive physical or status change in entities). The lack of a high-quality dataset in terms of size and consistency is the major hindrance in this task. This inspires us to create a new coreference corpus on literature and evaluate a state-of-the-art coreference system on this genre to confirm the feasibility of this research. Contributions of this work are as follows: AFT) have also been annotated to be used as a separate test set. AFT includes The Story of"
2021.crac-1.3,M95-1005,0,0.76285,"girls = gifts) under a single entity. Asymmetry of Knowledge This occurs when the knowledge is shared differently (a) between the reader and the characters or (b) between characters or when (c) the reader’s knowledge changes throughout the plot (e.g., a plot-twist) (Bamman et al., 2020). Furthermore, we elaborate this issue by categorizing it into deception (lie, disguise), mistaking, secret and a plot-twist. In these cases, we follow the omniscient writer’s point of view. 2.3 Inter-Annotator Agreement To ensure the quality of our annotation, the standard coreference evaluation metrics, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005), are used to estimate interannotator agreement (IAA). The Kappa score (Cohen, 1960) is considered the standard method of Changes in Entities In literary texts, changes often take place in the development of entities (e.g., 26 3.2 evaluating IAA in NLP tasks; however, it evaluates IAA in a pair-wise fashion, while coreference resolution assigns a markable to a coreference chain. Therefore, to avoid the Kappa score overpenalizing the wrongly assigned markable (He, 2007), we used the aforementioned metrics instead. The agreement scores are ca"
2021.crac-1.3,2020.acl-main.622,0,0.0283794,"Missing"
2021.crac-1.3,2020.emnlp-main.686,1,0.847629,"mber of entities in each partition. In addition, the stories were not partitioned in the middle of pair quotation marks to avoid starting or ending the partition in the middle of one’s utterance. The train set is not partitioned, since feeding the model as many sentences as it can handle would be more desirable to learn coreferent links between distant mention pairs. For brevity, the original versions and the partitioned versions of the dev/test set are referred to as follows for the rest of the paper: Model We use the state-of-the-art NLP model, Emory Language and Information Toolkit (ELIT) (Xu and Choi, 2020), to examine how well the existing coreference resolution model accounts for our annotated corpus, FantasyCoref. We experiment on end-to-end coreference systems with and without higher-order inference (HOI) approaches implemented in ELIT: attended antecedent (AA), entity equalization (EE), span clustering (SC), and cluster merging (CM). The end-to-end coreference system is based on c2f-coref model (Lee et al., 2018) and SpanBERT (Joshi et al., 2020), and we adopt the “independent” splitting variant for long documents introduced by Joshi et al. (2019). The higher-order inference is a widely ada"
2021.crac-1.3,2021.nuse-1.2,0,0.0330338,"011, 2012). A few coreference datasets have been created for several genres (Hovy et al., 2006; Li et al., 2016; Zhou and Choi, 2018); however, coreference resolution on literary texts has been comparatively underexplored. 24 Proceedings of the 4th Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2021), pages 24–35 c Punta Cana, Dominican Republic, November 10–11, 2021. 2021 Association for Computational Linguistics lyze this issue in its guidelines. Moreover, previous studies of coreference resolution on literary texts (Bamman et al., 2020; Roesiger et al., 2018; Yoder et al., 2021) present only limited analyses on dynamic issues that occur in fictional texts (e.g., asymmetry of knowledge, comprehensive physical or status change in entities). The lack of a high-quality dataset in terms of size and consistency is the major hindrance in this task. This inspires us to create a new coreference corpus on literature and evaluate a state-of-the-art coreference system on this genre to confirm the feasibility of this research. Contributions of this work are as follows: AFT) have also been annotated to be used as a separate test set. AFT includes The Story of Aladdin and The Story"
2021.crac-1.3,C18-1003,1,0.901209,"Missing"
2021.emnlp-demo.19,W13-2322,0,0.0387792,"el, and in the third sentence, the edge label :NEG indicates that Edmund Pope (who corresponds to the person node) as a conceiver/source has a full negative epistemic stance (Boye, 2012) towards the do event. Uniform Meaning Representation is a graph-based cross-linguistically applicable semantic representation that was recently developed with the goal of supporting interpretable natural language applications that require deep semantic analysis of texts (Van Gysel et al., 2021). UMR has two components: a sentence-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example"
2021.emnlp-demo.19,N13-3004,0,0.0251768,"ure. At the sentence level, UMR-Writer makes clear distinctions between the annotation of lexicalized and abstract concepts, named entity types, attributes, and relations. At the document level, UMR-Writer has separate functionalities for annotating temporal and modal dependencies as well as coreference. UMR-Writer allows the user to easily switch between the sentence-level and documentlevel views with a simple click. Fundamentally, UMR is a representation based on relations between concepts, and there are a number of tools that support annotation for relations. Some examples include Anafora (Chen and Styler, 2013), MAE (Stubbs, 2011; Rim, 2016), WebAnno (Eckart de Castilho et al., 2016), and BRAT (Stenetorp et al., 2012). Anafora is a web-based tool that supports the annotation of relations between text 3.1 Importing Source text into UMR-Writer spans. MAE is a standalone tool that offers flexible for Annotation and versatile schema support for complex relation Annotators can upload their source data in the form sets. WebAnno supports semantic role labelling or event annotations, and it enables the annota- of single files for annotation from the upload page. UMR-Writer can parse and render plain text fo"
2021.emnlp-demo.19,W16-4011,0,0.0390984,"Missing"
2021.emnlp-demo.19,2020.acl-demos.35,0,0.0609907,"Missing"
2021.emnlp-demo.19,H89-1022,0,0.757965,"Missing"
2021.emnlp-demo.19,J05-1004,0,0.758657,"such a tool and how they are addressed. 1 1.1 Introduction UMR Overview semantic concepts (including word senses, entity types etc.) and edges represent relations (participant roles and general semantic relations). The solid lines represent sentence-level relations while the dashed lines represent semantic relations that go beyond sentence boundaries. The direction of the arrows is always from parent to child, at both the sentence- and document level. For instance, at the sentence level, taste-01 is an eventive concept labeled with the first sense of the lemma “taste” as defined in PropBank (Palmer et al., 2005), and a person concept with the name “Edmund Pope” is its ARG0. The concept taste-01 also has an aspect attribute with the value State. The pronoun “he” in the third sentence is decomposed into a person concept with a person attribute 3rd and number attribute Singular, indicating third person singular. At the document level, the person concept mapped from the pronoun “he” refers to the same entity as the person concept in the first sentence, as indicated by the dashed line connecting these nodes. The event taste-01 in the first sentence occurs before document creation time (DCT), as indicated"
2021.emnlp-demo.19,E12-2021,0,0.0671642,"Missing"
2021.emnlp-demo.19,W11-0416,0,0.0419944,"MR-Writer makes clear distinctions between the annotation of lexicalized and abstract concepts, named entity types, attributes, and relations. At the document level, UMR-Writer has separate functionalities for annotating temporal and modal dependencies as well as coreference. UMR-Writer allows the user to easily switch between the sentence-level and documentlevel views with a simple click. Fundamentally, UMR is a representation based on relations between concepts, and there are a number of tools that support annotation for relations. Some examples include Anafora (Chen and Styler, 2013), MAE (Stubbs, 2011; Rim, 2016), WebAnno (Eckart de Castilho et al., 2016), and BRAT (Stenetorp et al., 2012). Anafora is a web-based tool that supports the annotation of relations between text 3.1 Importing Source text into UMR-Writer spans. MAE is a standalone tool that offers flexible for Annotation and versatile schema support for complex relation Annotators can upload their source data in the form sets. WebAnno supports semantic role labelling or event annotations, and it enables the annota- of single files for annotation from the upload page. UMR-Writer can parse and render plain text format tion of semant"
2021.emnlp-demo.19,W19-3321,1,0.900068,"Missing"
2021.emnlp-demo.19,2020.emnlp-main.432,1,0.749736,"-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example in Figwell as coreference. UMR abstracts away from ure 1, UMR is a fairly complex representation that syntactic representations and preserves semantic has many dimensions, and we need to address a relations within and across sentences. Building number of challenges in order to develop a tool a corpus of UMRs could potentially be very use- that makes UMR annotation practical. First of all, ful to NLP practitioners in multiple fields, such as the UMR annotation scheme involves both closed information extracti"
2021.emnlp-demo.19,D18-1371,1,0.82262,"o components: a sentence-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example in Figwell as coreference. UMR abstracts away from ure 1, UMR is a fairly complex representation that syntactic representations and preserves semantic has many dimensions, and we need to address a relations within and across sentences. Building number of challenges in order to develop a tool a corpus of UMRs could potentially be very use- that makes UMR annotation practical. First of all, ful to NLP practitioners in multiple fields, such as the UMR annotation scheme involves both close"
2021.emnlp-demo.19,L18-1490,1,0.778314,"o components: a sentence-level representation that is adapted from Abstract Meaning Representation (AMR) (Banarescu et al., 2013), and a documentlevel representation that captures semantic relations that potentially go beyond sentence boundaries. Like AMR, the UMR sentence-level representation captures the argument structures of predicative events, word senses, as well as semantic types of named entities. It also adds a representation for aspect and quantifier scope, which are not part of 1.2 Challenges in Building a Tool for UMR AMR. At the document level, UMR represents temAnnotation poral (Zhang and Xue, 2018b,a; Yao et al., 2020) and modal dependencies (Vigus et al., 2019) as As should be clear from the UMR example in Figwell as coreference. UMR abstracts away from ure 1, UMR is a fairly complex representation that syntactic representations and preserves semantic has many dimensions, and we need to address a relations within and across sentences. Building number of challenges in order to develop a tool a corpus of UMRs could potentially be very use- that makes UMR annotation practical. First of all, ful to NLP practitioners in multiple fields, such as the UMR annotation scheme involves both close"
2021.emnlp-main.451,W08-1301,0,0.0555708,"Missing"
2021.emnlp-main.451,N19-1423,0,0.19252,"e-tuning. Our probing methods align with these unsupervised probes while focus more on explaining the impact of multi-task learning. 3 Multi-Task Learning Our goal of MTL is to build a joint model sharing the same encoder but using a distinct decoder for each task that outperforms its single-task counterparts while being faster and more memory efficient. Our model adapts hard parameter sharing (Caruana, 1993) such that all decoders take the same hidden states generated by the shared encoder as input and make task-specific predictions in parallel. 3.1 Shared Encoder For main experiments, BERT (Devlin et al., 2019) is used as the shared encoder although our approach can be adapted to any transformer encoders (§A.4). Every token gets split into subtokens by BERT; eventually, the average of the last layer’s hidden states generated for those subtokens is used as the final embedding of that token. Additionally, word dropout is applied for generalization by replacing random subtokens with [MASK] during training. 3.2 Task-Specific Decoders Five tasks are experimented, part-of-speech tagging (POS), named entity recognition (NER), dependency parsing (DEP), constituency parsing (CON), and semantic role labeling"
2021.emnlp-main.451,P18-2058,0,0.0171286,", DEP’th row in NER’th column is the DEP result of the joint model between DEP and NER). See also Table 12 for similar results of other TEs. approach. Also, the final embedding of [CLS] from BERT is used to represent the root node. CON The two-stage CRF decoder is used for CON (Zhang et al., 2020). The unlabeled bracket scorer is optimized using a tree-structure CRF objective on unlabeled constituents. The encoding layer from the original approach is substituted by BERT. Also, [CLS] and [SEP] in BERT are used to represent [BOS] and [EOS], respectively. SRL The end-to-end span ranking decoder (He et al., 2018) is used for SRL. The attention-based span representations are replaced by the averaged embeddings as suggested by Xia et al. (2019). For simplification, a linear layer is used as the ranker instead of the biaffine one since they have shown similar performance in our preliminary experiments. 3.3 Data and Loss Balancing During multi-task training, batches from different tasks are shuffled together and randomly sampled to optimize the shared encoder and the corresponding decoder. Following Wang et al. (2019), a task is sampled based on a probability proportional to its dataset size raised to the"
2021.emnlp-main.451,D19-1275,0,0.0186369,"witt and Manning (2019) successfully discover full dependency parse trees. The encoded dependency structure is also supported by Jawahar et al. (2019) using probes on embeddings. Apart from these parameterized probes, parameter-free approaches (Clark et al., 2019a; Wu et al., 2020) also agree with the existence of rich linguistic knowledge in BERT, which is closely related to our probing methods. What remains unclear is the impact of fineturning on TEs. Using supervised probes, Peters et al. (2019) claim that fine-tuning adapts BERT embeddings to downstream tasks, which is later challenged by Hewitt and Liang (2019) since supervised probe itself can encode knowledge. Then, Zhao and Bethard (2020) propose a methodology to test such encoding of a linguistic phenomenon by comparing the probing performance before and after fine-tuning. Our probing methods align with these unsupervised probes while focus more on explaining the impact of multi-task learning. 3 Multi-Task Learning Our goal of MTL is to build a joint model sharing the same encoder but using a distinct decoder for each task that outperforms its single-task counterparts while being faster and more memory efficient. Our model adapts hard parameter"
2021.emnlp-main.451,N19-1419,0,0.354964,"gh performance “over-parameterized” as downstream tasks may not to predict certain linguistic structures, confirming need all those parameters, prone to cause an over- the existence of stem cells inherently more talented; head in computation. One promising approach to it is consistent with previous work stating that TEs mitigate this overhead is multi-task learning (MTL) carry on a good amount of syntactic and semantic where a TE is shared across multiple tasks; thus, it knowledge (Tenney et al., 2019; Liu et al., 2019a; needs to be run only once to generate final embed- Jawahar et al., 2019; Hewitt and Manning, 2019). dings for all tasks (Clark et al., 2019b). After single-task learning, probing results typically Despite the success in MTL on closely-related improve along with the task performance, illustrattasks such as language understanding (Wang et al., ing that the stem cells are developed into more task2018) or relation extraction (Chen et al., 2020; Lin specific experts. On the contrary, MTL often drops et al., 2020), MTL on core NLP tasks (e.g., tagging, both probing and task performance, supporting our parsing, labeling) whose decoders are very distinct hypothesis that attention heads lose expert"
2021.emnlp-main.451,P19-1356,0,0.19573,"ill give remarkably high performance “over-parameterized” as downstream tasks may not to predict certain linguistic structures, confirming need all those parameters, prone to cause an over- the existence of stem cells inherently more talented; head in computation. One promising approach to it is consistent with previous work stating that TEs mitigate this overhead is multi-task learning (MTL) carry on a good amount of syntactic and semantic where a TE is shared across multiple tasks; thus, it knowledge (Tenney et al., 2019; Liu et al., 2019a; needs to be run only once to generate final embed- Jawahar et al., 2019; Hewitt and Manning, 2019). dings for all tasks (Clark et al., 2019b). After single-task learning, probing results typically Despite the success in MTL on closely-related improve along with the task performance, illustrattasks such as language understanding (Wang et al., ing that the stem cells are developed into more task2018) or relation extraction (Chen et al., 2020; Lin specific experts. On the contrary, MTL often drops et al., 2020), MTL on core NLP tasks (e.g., tagging, both probing and task performance, supporting our parsing, labeling) whose decoders are very distinct hypothesis that"
2021.emnlp-main.451,D19-1279,0,0.0217344,"xperimental results and visualization of other recent TEs including RoBERTa (Liu et al., 2019c), ELECTRA (Clark et al., 2020) and DeBERTa (He et al., 2020) in §A.4 to further demonstrate the generality of our hypothesis. To the best of knowledge, this is the first time that a comprehensive analysis of attention heads is made for MTL on those core tasks by introducing novel parameter-free probing methods.1 2 Related Work A small portion of our work overlaps with multitask learning. MTL with pre-trained transformers specifically in NLP (Wang et al., 2018; Clark et al., 2019b; Liu et al., 2019b; Kondratyuk and Straka, 2019; Chen et al., 2020; Lin et al., 2020) has been widely studied. Most work focus on neural architecture design to encourage beneficial message passing across tasks. Our MTL framework adopts conventional architecture and applies tricks of batch sampling (Wang et al., 2019) and loss balancing. Most of our work falls into the analysis of BERT, especially from a linguistic view. Since BERT was introduced, studies on explaining why BERT works have never stopped. The most related studies are those trying to study the linguistic structures learnt by BERT. Among them, Tenney et al. (2019) and Liu et al"
2021.emnlp-main.451,2020.acl-main.713,0,0.019575,"ecent TEs including RoBERTa (Liu et al., 2019c), ELECTRA (Clark et al., 2020) and DeBERTa (He et al., 2020) in §A.4 to further demonstrate the generality of our hypothesis. To the best of knowledge, this is the first time that a comprehensive analysis of attention heads is made for MTL on those core tasks by introducing novel parameter-free probing methods.1 2 Related Work A small portion of our work overlaps with multitask learning. MTL with pre-trained transformers specifically in NLP (Wang et al., 2018; Clark et al., 2019b; Liu et al., 2019b; Kondratyuk and Straka, 2019; Chen et al., 2020; Lin et al., 2020) has been widely studied. Most work focus on neural architecture design to encourage beneficial message passing across tasks. Our MTL framework adopts conventional architecture and applies tricks of batch sampling (Wang et al., 2019) and loss balancing. Most of our work falls into the analysis of BERT, especially from a linguistic view. Since BERT was introduced, studies on explaining why BERT works have never stopped. The most related studies are those trying to study the linguistic structures learnt by BERT. Among them, Tenney et al. (2019) and Liu et al. (2019a) showed part-of-speech, synta"
2021.emnlp-main.451,W19-4825,0,0.0357151,"Missing"
2021.emnlp-main.451,N19-1112,0,0.341195,"for 2020). However, their architectures can be viewed any task can still give remarkably high performance “over-parameterized” as downstream tasks may not to predict certain linguistic structures, confirming need all those parameters, prone to cause an over- the existence of stem cells inherently more talented; head in computation. One promising approach to it is consistent with previous work stating that TEs mitigate this overhead is multi-task learning (MTL) carry on a good amount of syntactic and semantic where a TE is shared across multiple tasks; thus, it knowledge (Tenney et al., 2019; Liu et al., 2019a; needs to be run only once to generate final embed- Jawahar et al., 2019; Hewitt and Manning, 2019). dings for all tasks (Clark et al., 2019b). After single-task learning, probing results typically Despite the success in MTL on closely-related improve along with the task performance, illustrattasks such as language understanding (Wang et al., ing that the stem cells are developed into more task2018) or relation extraction (Chen et al., 2020; Lin specific experts. On the contrary, MTL often drops et al., 2020), MTL on core NLP tasks (e.g., tagging, both probing and task performance, supportin"
2021.emnlp-main.451,P19-1441,0,0.360602,"for 2020). However, their architectures can be viewed any task can still give remarkably high performance “over-parameterized” as downstream tasks may not to predict certain linguistic structures, confirming need all those parameters, prone to cause an over- the existence of stem cells inherently more talented; head in computation. One promising approach to it is consistent with previous work stating that TEs mitigate this overhead is multi-task learning (MTL) carry on a good amount of syntactic and semantic where a TE is shared across multiple tasks; thus, it knowledge (Tenney et al., 2019; Liu et al., 2019a; needs to be run only once to generate final embed- Jawahar et al., 2019; Hewitt and Manning, 2019). dings for all tasks (Clark et al., 2019b). After single-task learning, probing results typically Despite the success in MTL on closely-related improve along with the task performance, illustrattasks such as language understanding (Wang et al., ing that the stem cells are developed into more task2018) or relation extraction (Chen et al., 2020; Lin specific experts. On the contrary, MTL often drops et al., 2020), MTL on core NLP tasks (e.g., tagging, both probing and task performance, supportin"
2021.emnlp-main.451,2021.ccl-1.108,0,0.0477287,"Missing"
2021.emnlp-main.451,W19-4302,0,0.049652,"Missing"
2021.emnlp-main.451,W13-3516,0,0.0263917,"Missing"
2021.emnlp-main.451,P19-1452,0,0.260477,"heads not fine-tuned for 2020). However, their architectures can be viewed any task can still give remarkably high performance “over-parameterized” as downstream tasks may not to predict certain linguistic structures, confirming need all those parameters, prone to cause an over- the existence of stem cells inherently more talented; head in computation. One promising approach to it is consistent with previous work stating that TEs mitigate this overhead is multi-task learning (MTL) carry on a good amount of syntactic and semantic where a TE is shared across multiple tasks; thus, it knowledge (Tenney et al., 2019; Liu et al., 2019a; needs to be run only once to generate final embed- Jawahar et al., 2019; Hewitt and Manning, 2019). dings for all tasks (Clark et al., 2019b). After single-task learning, probing results typically Despite the success in MTL on closely-related improve along with the task performance, illustrattasks such as language understanding (Wang et al., ing that the stem cells are developed into more task2018) or relation extraction (Chen et al., 2020; Lin specific experts. On the contrary, MTL often drops et al., 2020), MTL on core NLP tasks (e.g., tagging, both probing and task perf"
2021.emnlp-main.451,2020.acl-main.577,0,0.0693599,"we propose the to justify our hypothesis and demonstrate how Stem Cell Hypothesis, likening these talented attenattention heads are transformed across the five tasks during MTL through label analysis. tion heads to stem cells, which cannot be fine-tuned for multiple tasks that are very distinct (Section 4). 1 Introduction To validate this hypothesis, many parameter-free Transformer encoders (TEs) have established re- probes are designed to observe how every attention cent state-of-the-art results on many core NLP tasks head is updated while trained individually or jointly. (He and Choi, 2019; Yu et al., 2020; Zhang et al., Intriguingly, we find that heads not fine-tuned for 2020). However, their architectures can be viewed any task can still give remarkably high performance “over-parameterized” as downstream tasks may not to predict certain linguistic structures, confirming need all those parameters, prone to cause an over- the existence of stem cells inherently more talented; head in computation. One promising approach to it is consistent with previous work stating that TEs mitigate this overhead is multi-task learning (MTL) carry on a good amount of syntactic and semantic where a TE is shared a"
2021.emnlp-main.451,P19-1580,0,0.0276207,"et al., 2018) is chosen for z, which gives the following form of Gα (u) that is differentiable, where (l, r) defines the interval that gα (u) can be stretched into (l < 0, r > 1): a task specific loss or the balanced MTL loss:   −l E u∼U (0,1) [z] = sigmoid α − log r n (1) X E u∼U (0,1) [L0 ] = E [zj ] j=1 4.2 Pruning Strategies Two types of pruning strategies, static and dynamic, are applied for the attention head analysis: Static Pruning We refer to the conventional twostage train-then-prune as static pruning (SP) since it fine-tunes the encoder first then freezes the decoder for pruning (Voita et al., 2019). Dynamic Pruning Since SP requires twice the efforts to obtain a pruned model, we propose a new method that simultaneously fine-tunes and prunes. This strategy is referred to as dynamic pruning (DP) since the decoder dynamically adapts to the encoder that is being pruned during training, as opposed to SP which instead freezes the decoder. DP is found to be more effective in our experiments. All pruning models are trained for 3 runs with different random seeds and the best checkpoints by scores on development sets are kept. Once trained, E u∼U (0,1) [z] ∈ (0, 1) is used as a measure of how muc"
2021.emnlp-main.451,P19-1439,0,0.0381315,"Missing"
2021.emnlp-main.451,2020.acl-main.429,0,0.212315,"oded dependency structure is also supported by Jawahar et al. (2019) using probes on embeddings. Apart from these parameterized probes, parameter-free approaches (Clark et al., 2019a; Wu et al., 2020) also agree with the existence of rich linguistic knowledge in BERT, which is closely related to our probing methods. What remains unclear is the impact of fineturning on TEs. Using supervised probes, Peters et al. (2019) claim that fine-tuning adapts BERT embeddings to downstream tasks, which is later challenged by Hewitt and Liang (2019) since supervised probe itself can encode knowledge. Then, Zhao and Bethard (2020) propose a methodology to test such encoding of a linguistic phenomenon by comparing the probing performance before and after fine-tuning. Our probing methods align with these unsupervised probes while focus more on explaining the impact of multi-task learning. 3 Multi-Task Learning Our goal of MTL is to build a joint model sharing the same encoder but using a distinct decoder for each task that outperforms its single-task counterparts while being faster and more memory efficient. Our model adapts hard parameter sharing (Caruana, 1993) such that all decoders take the same hidden states generat"
2021.emnlp-main.451,W18-5446,0,0.0826048,"Missing"
2021.emnlp-main.451,2020.acl-main.383,0,0.0166497,"troduced, studies on explaining why BERT works have never stopped. The most related studies are those trying to study the linguistic structures learnt by BERT. Among them, Tenney et al. (2019) and Liu et al. (2019a) showed part-of-speech, syntactic chunks and roles can be discovered from BERT embeddings. Using a supervised probe, Hewitt and Manning (2019) successfully discover full dependency parse trees. The encoded dependency structure is also supported by Jawahar et al. (2019) using probes on embeddings. Apart from these parameterized probes, parameter-free approaches (Clark et al., 2019a; Wu et al., 2020) also agree with the existence of rich linguistic knowledge in BERT, which is closely related to our probing methods. What remains unclear is the impact of fineturning on TEs. Using supervised probes, Peters et al. (2019) claim that fine-tuning adapts BERT embeddings to downstream tasks, which is later challenged by Hewitt and Liang (2019) since supervised probe itself can encode knowledge. Then, Zhao and Bethard (2020) propose a methodology to test such encoding of a linguistic phenomenon by comparing the probing performance before and after fine-tuning. Our probing methods align with these u"
2021.emnlp-main.538,P18-1073,0,0.0649422,"Missing"
2021.emnlp-main.538,2020.acl-main.747,0,0.0420112,"ce (NLI) covering 40 lancertain low-resource languages that can achieve up guages in total, which outperforms the baseto 8.6 F1 gain in NER evaluation. Therefore, our lines significantly by 10 F1 on average for SL framework features the joint training strategy, NER and 2.5 accuracy score for NLI. maximizing potentials of different languages ben1 Introduction efiting each other. Second, compared with simply using all unlabeled data as silver labels without Recent multilingual pre-trained language models considering prediction confidence, estimating unsuch as mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020) and mT5 (Xue et al., 2021) certainties becomes critical in the transfer process, have demonstrated remarkable performance on var- as higher quality of silver labels should lead to ious direct zero-shot cross-lingual transfer tasks, better performance. We hence introduce three difwhere the model is finetuned on the source lan- ferent uncertainty estimations in the SL framework. guage, and directly evaluated on multiple target lanSpecifically, we adapt uncertainty estimation guages that are unseen in the task-finetuning stage. techniques based on variational inference and While direct zero-shot"
2021.emnlp-main.538,D18-1269,0,0.062862,"Missing"
2021.emnlp-main.538,N19-1423,0,0.0161335,"ally for ral Language Inference (NLI) covering 40 lancertain low-resource languages that can achieve up guages in total, which outperforms the baseto 8.6 F1 gain in NER evaluation. Therefore, our lines significantly by 10 F1 on average for SL framework features the joint training strategy, NER and 2.5 accuracy score for NLI. maximizing potentials of different languages ben1 Introduction efiting each other. Second, compared with simply using all unlabeled data as silver labels without Recent multilingual pre-trained language models considering prediction confidence, estimating unsuch as mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020) and mT5 (Xue et al., 2021) certainties becomes critical in the transfer process, have demonstrated remarkable performance on var- as higher quality of silver labels should lead to ious direct zero-shot cross-lingual transfer tasks, better performance. We hence introduce three difwhere the model is finetuned on the source lan- ferent uncertainty estimations in the SL framework. guage, and directly evaluated on multiple target lanSpecifically, we adapt uncertainty estimation guages that are unseen in the task-finetuning stage. techniques based on variational infere"
2021.emnlp-main.538,D19-1658,0,0.0492797,"Missing"
2021.emnlp-main.538,2020.emnlp-main.671,1,0.774891,"5 accuracy score for NLI on average). (3) Further analysis is conducted to compare different uncertainties and their characteristics. 2 Related Work We introduce the work of uncertain estimation briefly. As deep learning models are optimized by minimizing the loss without special care on the uncertainty, they are usually poor at quantifying uncertainty and tend to make over-confident predictions, despite producing high accuracies (Lakshminarayanan et al., 2017). Estimating the uncertainty of deep learning models has been recently studied in NLP tasks (Xiao and Wang, 2019a; Zhang et al., 2019; He et al., 2020). There are two main uncertainty types in Bayesian modelling (Kendall and Gal, 2017; Depeweg et al., 2018): epistemic uncertainty that captures the model uncertainty itself, which can be explained away with more data; aleatoric uncertainty that captures the intrinsic data uncertainty regardless of models. Aleatoric uncertainty can further be devided into two sub-types: heteroscedastic uncertainty that depends on input data, and homoscedastic uncertainty that remains constant for all data within a task. In this work, we only focus on aleatoric uncertainty, as it is more closely related to our S"
2021.emnlp-main.538,2020.acl-main.618,0,0.0875524,"Missing"
2021.emnlp-main.538,P17-1178,0,0.0459564,"Missing"
2021.emnlp-main.538,2021.naacl-main.41,0,0.0129526,"low-resource languages that can achieve up guages in total, which outperforms the baseto 8.6 F1 gain in NER evaluation. Therefore, our lines significantly by 10 F1 on average for SL framework features the joint training strategy, NER and 2.5 accuracy score for NLI. maximizing potentials of different languages ben1 Introduction efiting each other. Second, compared with simply using all unlabeled data as silver labels without Recent multilingual pre-trained language models considering prediction confidence, estimating unsuch as mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020) and mT5 (Xue et al., 2021) certainties becomes critical in the transfer process, have demonstrated remarkable performance on var- as higher quality of silver labels should lead to ious direct zero-shot cross-lingual transfer tasks, better performance. We hence introduce three difwhere the model is finetuned on the source lan- ferent uncertainty estimations in the SL framework. guage, and directly evaluated on multiple target lanSpecifically, we adapt uncertainty estimation guages that are unseen in the task-finetuning stage. techniques based on variational inference and While direct zero-shot transfer is a sensible tes"
2021.emnlp-main.538,N19-1316,1,0.822503,"10 F1 for NER and 2.5 accuracy score for NLI on average). (3) Further analysis is conducted to compare different uncertainties and their characteristics. 2 Related Work We introduce the work of uncertain estimation briefly. As deep learning models are optimized by minimizing the loss without special care on the uncertainty, they are usually poor at quantifying uncertainty and tend to make over-confident predictions, despite producing high accuracies (Lakshminarayanan et al., 2017). Estimating the uncertainty of deep learning models has been recently studied in NLP tasks (Xiao and Wang, 2019a; Zhang et al., 2019; He et al., 2020). There are two main uncertainty types in Bayesian modelling (Kendall and Gal, 2017; Depeweg et al., 2018): epistemic uncertainty that captures the model uncertainty itself, which can be explained away with more data; aleatoric uncertainty that captures the intrinsic data uncertainty regardless of models. Aleatoric uncertainty can further be devided into two sub-types: heteroscedastic uncertainty that depends on input data, and homoscedastic uncertainty that remains constant for all data within a task. In this work, we only focus on aleatoric uncertainty, as it is more closel"
2021.iwpt-1.5,P18-1026,0,0.0589261,"rsing which incrementally and directly builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a Levi graph. We generate a Levi graph which is a linearized form originally used in seq2seq models for AMRto-text (Beck et al., 2018; Guo et al., 2019; Ribeiro et al., 2019). Our Levi graph approach differs from seq2seq approaches in its attention based arc prediction, where arc is directly predicted by attention heads instead of brackets in the target sequence. 3 3.1 Text Transformer e0t e1t ⋯ ent Graph Transformer e0w e1w ⋯ enw e0f e1f ⋯ enf e0v e1v ⋯ emv Text Encoder Feature Encoder Graph Encoder w0 w1 ⋯ wn NLP Tools v0 v1 ⋯ vm Figure 1: Overview of our Text-to-Graph Transducer. E t and E v are fed into a Graph Transformer that predicts the target node as well as its relations to all nodes in V . The target node predict"
2021.iwpt-1.5,2020.findings-emnlp.89,0,0.0332376,"by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expanding graph nodes without resorting to any transition s"
2021.iwpt-1.5,D18-1264,0,0.0184885,"abels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expa"
2021.iwpt-1.5,P18-1170,0,0.049966,"Missing"
2021.iwpt-1.5,2021.emnlp-main.714,0,0.0608429,"Missing"
2021.iwpt-1.5,D18-1198,0,0.0188526,"se models assign labels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a seman"
2021.iwpt-1.5,P18-1037,0,0.0178212,"on Abstract Meaning Representation (AMR) has recently gained lots of interests due to its capability in capturing abstract concepts (Banarescu et al., 2013). In the form of directed acyclic graphs (DAGs), an AMR graph consists of nodes as concepts and edges as labeled relations. To build such a graph from plain text, a parser needs to predict concepts and relations in concord. While significant research efforts have been conducted to improve concept and arc predictions, label prediction has been relatively stagnated. Most previous models have adapted the biaffine decoder for label prediction (Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019; Zhou et al., 2020; Lindemann et al., 2020). These models assign labels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang"
2021.iwpt-1.5,Q19-1019,0,0.0179485,"ntally and directly builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a Levi graph. We generate a Levi graph which is a linearized form originally used in seq2seq models for AMRto-text (Beck et al., 2018; Guo et al., 2019; Ribeiro et al., 2019). Our Levi graph approach differs from seq2seq approaches in its attention based arc prediction, where arc is directly predicted by attention heads instead of brackets in the target sequence. 3 3.1 Text Transformer e0t e1t ⋯ ent Graph Transformer e0w e1w ⋯ enw e0f e1f ⋯ enf e0v e1v ⋯ emv Text Encoder Feature Encoder Graph Encoder w0 w1 ⋯ wn NLP Tools v0 v1 ⋯ vm Figure 1: Overview of our Text-to-Graph Transducer. E t and E v are fed into a Graph Transformer that predicts the target node as well as its relations to all nodes in V . The target node predicted by the Graph Tr"
2021.iwpt-1.5,P14-5010,0,0.00261771,"to “translation-based methods” (Koller et al., 2019) possibly due to the prevalence of seq2seq model in Neural Machine Translation, while we believe that translation refers more to the transduction between languages while AMR is neither a language nor an interlingua. 3 In our case, BERT (Devlin et al., 2019) is used as the Text Encoder and ∀i .efi = eiLEMMA ⊕ eiPOS ⊕ eiNER ⊕ eiCHAR is created by the Feature Encoder using predictions (lemmas, partof-speech tags and named-entities) from the NLP Tools and character level features from a Convolutional Neural Network. In this work, we use CoreNLP (Manning et al., 2014) for a fair comparison with existing approaches. g(C|W |L) = softmax(β ⊕ · WC|W |L ) p(xi ) = g(C) · [softmax(β ⊕ · WG )]i X X + g(W ) αj + g(L) αj j∈W (xi ) j∈L(xi ) g(C|W |L) is the gate probability of the target node being in C|W |L, respectively (WC|W |L ∈ Rd×1 ). 4 51 Graph Encoder creates ∀i .evi = transformer(eiNODE ⊕ eiCHAR ). p(xi ) is estimated by measuring the probabilities of xi being the target if xi ∈ C (WG ∈ Rd×|C |), and if xi ∈ W |L where W |L(xi ) = {j : (xi = yj ) ∧ yj ∈ W |L}, respectively. Finally, the output layer onode = [p(xi ) : xi ∈ X] ∈ R1×(|C|+|W |+|L|) gets created"
2021.iwpt-1.5,P19-4002,0,0.0117914,"a Graph Encoder to create E v = {ev0 , ev1 , . . . , evm }. Finally, αi = softmax( ∈ R1×k ∈ R1×d α = [αj1 : j ∈ [1, n]] ∈ R1×n β ⊕ = (β 1 ⊕ . . . ⊕ β h ) · W⊕ ∈ R1×d αj indicates the probability of wj being aligned to the target node, and β ⊕ is the embedding representing the node. Let C be the list of all concepts in training data and L be the list of lemmas for tokens in W such that |W |= |L|. Given X = C _ W _ L, α and β ⊕ are fed into a Node Decoder estimating the score of each xi ∈ X being the target node: 2 Seq2seq-based parsing is sometimes categorized into “translation-based methods” (Koller et al., 2019) possibly due to the prevalence of seq2seq model in Neural Machine Translation, while we believe that translation refers more to the transduction between languages while AMR is neither a language nor an interlingua. 3 In our case, BERT (Devlin et al., 2019) is used as the Text Encoder and ∀i .efi = eiLEMMA ⊕ eiPOS ⊕ eiNER ⊕ eiCHAR is created by the Feature Encoder using predictions (lemmas, partof-speech tags and named-entities) from the NLP Tools and character level features from a Convolutional Neural Network. In this work, we use CoreNLP (Manning et al., 2014) for a fair comparison with exi"
2021.iwpt-1.5,P19-1451,0,0.0181992,"ffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expanding graph nodes wit"
2021.iwpt-1.5,P17-1014,0,0.0216786,"an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a L"
2021.iwpt-1.5,2020.findings-emnlp.288,0,0.049231,"Missing"
2021.iwpt-1.5,P18-1171,0,0.0186839,"using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a Levi graph. We generate a Levi graph which is"
2021.iwpt-1.5,E17-1035,0,0.0131187,"et al., 2020). These models assign labels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and direc"
2021.iwpt-1.5,P19-1450,0,0.0210046,"uistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a Levi graph. We generate a Levi graph which is a linearized form originally used in seq2seq models for AMRto-text (Beck et al., 2018; Guo et al., 2019; Ribeiro et al., 2019). Our Levi graph approach differs from seq2seq approaches in its attention based arc prediction, where arc is directly predicted by attention heads instead of brackets in the target sequence. 3 3.1 Text Transformer e0t e1t ⋯ ent Graph Transformer e0w e1w ⋯ enw e0f e1f ⋯ enf e0v e1v ⋯ emv"
2021.iwpt-1.5,D19-1314,0,0.0194666,"y builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a Levi graph. We generate a Levi graph which is a linearized form originally used in seq2seq models for AMRto-text (Beck et al., 2018; Guo et al., 2019; Ribeiro et al., 2019). Our Levi graph approach differs from seq2seq approaches in its attention based arc prediction, where arc is directly predicted by attention heads instead of brackets in the target sequence. 3 3.1 Text Transformer e0t e1t ⋯ ent Graph Transformer e0w e1w ⋯ enw e0f e1f ⋯ enf e0v e1v ⋯ emv Text Encoder Feature Encoder Graph Encoder w0 w1 ⋯ wn NLP Tools v0 v1 ⋯ vm Figure 1: Overview of our Text-to-Graph Transducer. E t and E v are fed into a Graph Transformer that predicts the target node as well as its relations to all nodes in V . The target node predicted by the Graph Transformer gets appended"
2021.iwpt-1.5,2020.emnlp-main.323,0,0.0139462,"ue to its capability in capturing abstract concepts (Banarescu et al., 2013). In the form of directed acyclic graphs (DAGs), an AMR graph consists of nodes as concepts and edges as labeled relations. To build such a graph from plain text, a parser needs to predict concepts and relations in concord. While significant research efforts have been conducted to improve concept and arc predictions, label prediction has been relatively stagnated. Most previous models have adapted the biaffine decoder for label prediction (Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019; Zhou et al., 2020; Lindemann et al., 2020). These models assign labels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2"
2021.iwpt-1.5,S16-1181,0,0.0259537,"2018; Zhang et al., 2019a; Cai and Lam, 2019; Zhou et al., 2020; Lindemann et al., 2020). These models assign labels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua"
2021.iwpt-1.5,2020.emnlp-main.196,0,0.0248598,"a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Naseem et al., 2019; Fernandez Astudillo et al., 2020; Lee et al., 50 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 50–57 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2020), (ii) seq2seq-based parsing 2 which transduces raw sentences into linearized AMR graphs in text form (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018; Xu et al., 2020; Bevilacqua et al., 2021), (iii) seq2graph-based parsing which incrementally and directly builds a semantic graph via expanding graph nodes without resorting to any transition system (Cai and Lam, 2019; Zhang et al., 2019b; Lyu et al., 2020). (iv) graph algebra parsing which translates an intermediate grammar structure into AMR (Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019, 2020). Our work is most closely related to seq2graph paradigm while we extend the definition of node to accommodate relation labels in a Levi graph. We generate a Levi graph which is a linearized for"
2021.iwpt-1.5,P19-1009,0,0.0351227,"Missing"
2021.iwpt-1.5,D19-1392,0,0.0361223,"Missing"
2021.iwpt-1.5,2020.acl-main.397,0,0.0167293,"lots of interests due to its capability in capturing abstract concepts (Banarescu et al., 2013). In the form of directed acyclic graphs (DAGs), an AMR graph consists of nodes as concepts and edges as labeled relations. To build such a graph from plain text, a parser needs to predict concepts and relations in concord. While significant research efforts have been conducted to improve concept and arc predictions, label prediction has been relatively stagnated. Most previous models have adapted the biaffine decoder for label prediction (Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019; Zhou et al., 2020; Lindemann et al., 2020). These models assign labels from the biaffine decoder to arcs predicted by another decoder, which can be misled by incorrect arc predictions during decoding. 1 2 Resources are publicly available at https://github. com/emorynlp/levi-graph-amr-parser. Related Work Recent AMR parsing approaches can be categorized into four classes: (i) transition-based parsing which casts the parsing process into a sequence of transitions defined on an abstract machine (e.g., a transition system using a buffer and a stack) (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onai"
2021.mrl-1.19,N19-1391,0,0.0222777,"Missing"
2021.mrl-1.19,D18-1269,0,0.0227954,"in Korean through zeroshot learning (Sec. 4). All models are experimented on our dataset and thoroughly compared to evaluate the feasibility of this work (Sec. 5). Our results are promising although depicting few challenges in zero-shot learning for English and Korean (Sec. 6). The contributions of this work can be summarized as follows: Crosslingual representation learning aims to derive embeddings for words (or sentences) from multiple • To create a crosslingual dataset that enables to languages that can be projected into a shared vector develop robust zero-shot NER models in Korean. space (Conneau et al., 2018; Schuster et al., 2019b; Conneau and Lample, 2019). One important appli- • To present a new data selection scheme that can cation of crosslingual embeddings has been found notably improve zero-shot model performance. for transferring models trained on a high-resource language to a low-resource one (Lin et al., 2019; • To provide a comparative analysis among several crosslingual approaches and establish the initial Schuster et al., 2019a; Artetxe and Schwenk, 2019). foundation of this research. The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et"
2021.mrl-1.19,D16-1250,0,0.0302547,"ing Projection 4.2 Embedding Projection Let X, Y ∈ Rn×d be parallel matrices between the source and target languages, where n is the number of parallel terms (words or sentences) in those two languages. Let xi , yi ∈ R1×d be the i’th rows in X and Y , which are the embeddings of the i’th terms in the source and target languages respectively, that refer to the same content. Then, the transformation matrix W ∈ Rd×d can be found by minimizing the distance between XW and Y as follows: argminkXW − Y k s.t. W T W = I W This optimization can be achieved by singular value decomposition as proposed by Artetxe et al. (2016), where U, V ∈ Rd×p , Σ ∈ Rp×p : W = UV T s.t. X T Y = U ΣV T still preserved as in the source language. Therefore, the model is limited to learn sequence information of the target language, which can be an issue for languages with very different word orderings. 4.3 Annotation Projection Let S = {S1 , . . . , Sn } and T = {T1 , . . . , Tn } be lists of sentences in the source and target languages, and (Si , Ti ) be the i’th pair of parallel sentences in those two languages. Let Si = {si1 , . . . , sin } and Ti = {ti1 , . . . , tim } where si and ti are the i’th word in S and T . Then, annotati"
2021.mrl-1.19,N19-1423,0,0.505554,"ls in Korean. space (Conneau et al., 2018; Schuster et al., 2019b; Conneau and Lample, 2019). One important appli- • To present a new data selection scheme that can cation of crosslingual embeddings has been found notably improve zero-shot model performance. for transferring models trained on a high-resource language to a low-resource one (Lin et al., 2019; • To provide a comparative analysis among several crosslingual approaches and establish the initial Schuster et al., 2019a; Artetxe and Schwenk, 2019). foundation of this research. The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et al., 2020) have made it possible to develop robust 2 Related Work crosslingual models through zero-shot learning that requires no labeled training data on the target side For crosslingual representation alignment, Artetxe (Jebbara and Cimiano, 2019; Chidambaram et al., et al. (2016) and Smith et al. (2017) suggested 2019; Chi et al., 2020). However, these approaches orthogonality constraints on the embedding transtend not to work as well for languages whose words formation that led to better quality translation. Alcannot be easily aligned. darmaki and Diab (2019) derived a"
2021.mrl-1.19,Q19-1038,0,0.0821772,"ngual dataset that enables to languages that can be projected into a shared vector develop robust zero-shot NER models in Korean. space (Conneau et al., 2018; Schuster et al., 2019b; Conneau and Lample, 2019). One important appli- • To present a new data selection scheme that can cation of crosslingual embeddings has been found notably improve zero-shot model performance. for transferring models trained on a high-resource language to a low-resource one (Lin et al., 2019; • To provide a comparative analysis among several crosslingual approaches and establish the initial Schuster et al., 2019a; Artetxe and Schwenk, 2019). foundation of this research. The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et al., 2020) have made it possible to develop robust 2 Related Work crosslingual models through zero-shot learning that requires no labeled training data on the target side For crosslingual representation alignment, Artetxe (Jebbara and Cimiano, 2019; Chidambaram et al., et al. (2016) and Smith et al. (2017) suggested 2019; Chi et al., 2020). However, these approaches orthogonality constraints on the embedding transtend not to work as well for languages whose words f"
2021.mrl-1.19,2020.acl-main.493,0,0.0184738,"provide a comparative analysis among several crosslingual approaches and establish the initial Schuster et al., 2019a; Artetxe and Schwenk, 2019). foundation of this research. The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et al., 2020) have made it possible to develop robust 2 Related Work crosslingual models through zero-shot learning that requires no labeled training data on the target side For crosslingual representation alignment, Artetxe (Jebbara and Cimiano, 2019; Chidambaram et al., et al. (2016) and Smith et al. (2017) suggested 2019; Chi et al., 2020). However, these approaches orthogonality constraints on the embedding transtend not to work as well for languages whose words formation that led to better quality translation. Alcannot be easily aligned. darmaki and Diab (2019) derived a context-aware Our team is motivated to create a rich crosslingual crosslingual mapping from a parallel corpus using resource between English and Korean, which are word alignment. Schuster et al. (2019b) aligned 224 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 224–237 November 11, 2021. ©2021 Association for Computational Ling"
2021.mrl-1.19,W19-4330,0,0.0422413,"Missing"
2021.mrl-1.19,N19-1380,0,0.0663422,"shot learning (Sec. 4). All models are experimented on our dataset and thoroughly compared to evaluate the feasibility of this work (Sec. 5). Our results are promising although depicting few challenges in zero-shot learning for English and Korean (Sec. 6). The contributions of this work can be summarized as follows: Crosslingual representation learning aims to derive embeddings for words (or sentences) from multiple • To create a crosslingual dataset that enables to languages that can be projected into a shared vector develop robust zero-shot NER models in Korean. space (Conneau et al., 2018; Schuster et al., 2019b; Conneau and Lample, 2019). One important appli- • To present a new data selection scheme that can cation of crosslingual embeddings has been found notably improve zero-shot model performance. for transferring models trained on a high-resource language to a low-resource one (Lin et al., 2019; • To provide a comparative analysis among several crosslingual approaches and establish the initial Schuster et al., 2019a; Artetxe and Schwenk, 2019). foundation of this research. The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et al., 2020) have made it"
2021.mrl-1.19,N19-1162,0,0.0580668,"shot learning (Sec. 4). All models are experimented on our dataset and thoroughly compared to evaluate the feasibility of this work (Sec. 5). Our results are promising although depicting few challenges in zero-shot learning for English and Korean (Sec. 6). The contributions of this work can be summarized as follows: Crosslingual representation learning aims to derive embeddings for words (or sentences) from multiple • To create a crosslingual dataset that enables to languages that can be projected into a shared vector develop robust zero-shot NER models in Korean. space (Conneau et al., 2018; Schuster et al., 2019b; Conneau and Lample, 2019). One important appli- • To present a new data selection scheme that can cation of crosslingual embeddings has been found notably improve zero-shot model performance. for transferring models trained on a high-resource language to a low-resource one (Lin et al., 2019; • To provide a comparative analysis among several crosslingual approaches and establish the initial Schuster et al., 2019a; Artetxe and Schwenk, 2019). foundation of this research. The latest multilingual transformer encoders such as BERT (Devlin et al., 2019) and XLM (Conneau et al., 2020) have made it"
2021.mrl-1.19,D19-1077,0,0.0588809,"k (Translation Language Modeling). Luo et al. (2021) adds a crossattention module into the Transformer encoder to explicitly build the interdependence between langauges. For cross-lingual NER, Ni et al. (2017) presented weakly supervised crosslingual models using annotation and representation projection. Huang et al. (2019) made an empirical analysis of how sequential order and multilingual embeddings are used in crosslingual NER. Artetxe and Schwenk (2019) presented multilingual transfer models that used few-shot learning adapting supervising BEA, ranking and retraining for massive transfer. Wu and Dredze (2019) and Wu et al. (2020) directly transfers the NER model trained on the source language to the target language using crosslingual representations from multilingual encoders (Direct model transfer). 3 3.1 English-Korean Crosslingual Dataset Data Collection (KEAT)1 containing 1.6M English-Korean parallel sentences from various sources such as news media, government website/journal, law & administration, conversation and etc. For the present study, 800K parallel sentences from the news portion of this corpus are extracted. 3.2 Data Preprocessing Since KEAT is not organized into documents, each sent"
2021.mrl-1.19,J03-1002,0,0.0134028,"tched in Section 3.4 in the case of DEV and TST. In addition, Korean sentences are processed by the Mecab morphological analyzer4 that produces more linguistically sounding tokens than SentencePiece (Kudo and Richardson, 2018) in KoBERT. All named entities from the CRF tagger are then remapped to the tokens produced by the Mecab analyzer using heuristics so they can better reflect the previous morphology work in Korean (Hong, 2009). Words in every parallel sentence pair, tokenized by the ELIT and Mecab analyzers, are aligned by GIZA++, that has been adapted by many prior crosslingual studies (Och and Ney, 2003). Table 2 shows the statistics of pseudo-annotated named entities in our dataset. The detailed descriptions of these tags are provided in Appendix A.1. The overall statistics are comparable between En2 https://github.com/emorynlp/elit glish and Korean, 2.5 and 2.3 entities per sentence, 3 https://github.com/eagle705/ respectively. GPEe , the 3rd most frequent tag in pytorch-bert-crf-ner 4 https://bitbucket.org/eunjeon/mecab-ko/ English, is not supported by the Korean tagger but 226 rather tagged as ORGk or LOCk , explaining why the numbers of these two tags in Korean are much greater than thos"
2021.mrl-1.19,N18-1202,0,0.0185467,"-label annotated with an existing English NER model. During decoding, generally not robust for the case of distant language pairs such as English and Korean. Thus, we propose the model takes Korean sentences represented by the encoded embeddings k∗ and makes the predic- a few constraints to filter out noisy annotation. tions. Entity Matching Let ψ be a boolean. If ψ = F, Given the latest contextualized encoders that genall parallel sentences in (S, T) are used for training. erate different embeddings for the same word type If ψ = T, (Si , Ti ) is selected for training only if all by contexts (Peters et al., 2018; Devlin et al., 2019; named entities in Si are properly labeled in Ti by Liu et al., 2019), the size of X and Y is as large as the above projection approach. the number of all aligned words in the training data. It is worth saying that the transformed embedding Relative Frequency Let e be an entity term such space may be similar to the actual encoded space as “도널드 트럼프 (Donald Trump)” in Figure 1. in the target language; however, the word order is Let Le be a set of entity types pseudo-annotated for 228 all occurrences of e in the target language. Then, the relative frequency P (`|e) for ` ∈ L"
2021.mrl-1.19,W13-3516,0,0.0271643,"(18.7) 1,955 (19.5) PRODUCT 172 (1.6) 176 (1.6) 161 (1.6) QUANTITY 52 (0.5) 51 (0.5) 50 (0.5) TIME 148 (1.3) 133 (1.2) 132 (1.3) WOA 156 (1.4) 155 (1.4) 141 (1.4) (b) Statistics of the evaluation set (TST). Table 3: The statistics of manually annotated named entities on the parallel sentences in the DEV and TST sets. The numbers in the parentheses indicate the percentages of the corresponding tags for each set. EN/KR: # of entities in the English/Korean sentences respectively, E ∩ K: # of entities existing in both English and Korean sentences. ELIT2 using the Flair model trained on OntoNotes (Pradhan et al., 2013). Korean sentences are tagged by a CRF-based model adapting KoBERT (Korean BERT)3 trained on the corpus distributed by Cheon and Kim (2018). Note that the named entity types pseudo-annotated on the Korean sentences don’t match with those of the English sentences for now, which will be matched in Section 3.4 in the case of DEV and TST. In addition, Korean sentences are processed by the Mecab morphological analyzer4 that produces more linguistically sounding tokens than SentencePiece (Kudo and Richardson, 2018) in KoBERT. All named entities from the CRF tagger are then remapped to the tokens pro"
2021.nlp4convai-1.9,2020.sigdial-1.29,1,0.715888,"tion on a development set of the data. For NARA, the decision was based on performance on utterance-level quality prediction instead. Details on the final configurations are provided in Appendix A. 5 15024 38.8 34.98 Table 2: Statistics for the AP19 dataset. 4.2 Model Configurations 4.4 Dialogue Quality Regression Results Table 3 shows the performance of our models on predicting overall dialogue quality. Previous works have shown the difficulty of this task, the noisiness of user ratings, and the low agreement between independent human annotators on the same conversations (Liang et al., 2020; Finch and Choi, 2020). As noted in Section 4.3, we used the development performance for hyperparameter tuning only. Models We trained 3 variants of the ARA model presented in Section 3, including both the non-contextualized base version and two extensions using different contextualization methods: Non-Contextualized This is the base ARA model from Section 3 that does not utilize contextualization of the utterance embeddings. Model ARA ARA-O ARA-A NARA Order Driven Contextualization ARA-O extends the base model by using a bidirectional LSTM layer in order to target the importance of utterance order when determining"
2021.nlp4convai-1.9,L16-1502,0,0.0186949,"s r correlations achieved by each model on the dev/test data. Attention Based Contextualization ARA-A extends the base model by using a self-attention layer to incorporate long-range cross-utterance relationships when determining utterance quality. 5 Evaluation Evaluating our model is challenging because it requires human judgements about the magnitude and We also include an additional baseline model: 96 direction of effect of each utterance on the quality of the conversation it is a part of. Annotating quality on an utterance level is already a difficult task with much subjectivity involved (Higashinaka et al., 2016). Furthermore, asking humans to annotate quality using real numbered values results in arbitrary judgements of magnitude, further complicating a direct evaluation of our task. Nevertheless, a model that appropriately assigns utterance scores in a way that explains each utterance’s relative impact to the conversation quality should agree substantially with judgements of human experts. We address these challenges by presenting two evaluation procedures that approximate the ground truth of our task formulation while framing all human judgements as non-arbitrary decisions with high inter-annotator"
2021.nlp4convai-1.9,D19-1410,0,0.0149709,"s collected between March and July of 2020. Dialogues with less than 5 utterances were excluded because we observed that such dialogues frequently occurred due to unintentional invocation of the Alexa Prize skill. We heldout 4873 conversations as development and test splits each. Table 2 shows statistics of the remaining AP19 conversations (N =38,693) used for training and utterance-level evaluation. Rating Dialogues Proportion Avg. Turns 1 4785 12.4 26.04 2 4534 11.7 30.22 3 5965 15.4 32.80 4 8385 21.7 36.57 4.3 Utterances are embedded using the DistilBERT version of Sentence-BERT (SBERT) by Reimers and Gurevych (2019). SBERT is a sentence encoder utilizing a siamese neural network architecture and BERT-based embeddings that has been shown to outperform other methods of encoding sentences on a variety of downstream NLP tasks. For all ARA-derivative models, hyperparameters were chosen such that they yielded the bestperforming models at predicting conversation quality according to Pearson’s correlation on a development set of the data. For NARA, the decision was based on performance on utterance-level quality prediction instead. Details on the final configurations are provided in Appendix A. 5 15024 38.8 34.9"
2021.nlp4convai-1.9,N18-1163,0,0.0273932,"thousands of system logs and designing statistical analyses, both of which are time consuming and unlikely to provide a holistic view of a system’s shortcomings. Inspired by this problem, the presented work investigates the extent to which it is possible to automatically distinguish turns within chat-oriented dialogues that have a negative effect on overall dialogue quality. The interpretation of dialogue quality is especially difficult in the chat-oriented dialogue 2 Related Work Related work has explored techniques for modelling dialogue quality on both the conversation and utterance level. Sandbank et al. (2018) present 93 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 93–101 November 10, 2021. ©2021 Association for Computational Linguistics 3 an approach for classifying low-quality conversations in commercial conversational assistants. Liang et al. (2020) argued against the feasability of conversation-level quality prediction on a Likertscale and present a pairwise comparison model instead using methods that compensated for the high noise in user scores. Choi et al. (2019) presents methods for both predicting user satisfaction and detecting conversation"
2021.smm4h-1.2,D11-1145,0,0.049345,"nificantly outperforms the transformer-based models pretrained on domain-specific data. 1 Li Xiong Emory University lxiong@emory.edu Introduction Social media has made substantial amount of data available for various applications in the financial, educational, and health domains. Among these, the applications in healthcare have a particular importance. Although previous studies have demonstrated that the self-reported online social data is subject to various biases (Olteanu et al., 2018), this data has enabled many applications in the health domain, including tracking the spread of influenza (Aramaki et al., 2011), detecting the reports of the novel coronavirus (Karisani and Karisani, 2020), and identifying various illness reports (Karisani and Agichtein, 2018). One of the well-studied areas in online public health monitoring is the extraction of adverse drug reactions (ADR) from social media data. ADRs are the unintended effects of drugs for prevention, diagnosis, or treatment. The researchers in Duh et al. (2016) reported that consumers, on average, report the negative effect of drugs on social media 11 months earlier than other platforms. This highlights the importance of this task. Another team of"
2021.smm4h-1.2,I11-1080,0,0.0684809,"Missing"
2021.smm4h-1.2,N19-1423,0,0.118217,"ational Linguistics Goharian (2013), where the authors utilize the related lexicons and extraction patterns to identify ADRs in user reviews. With the surge of neural networks in text processing, subsequently, the traditional models were aggregated with these techniques to achieve better generalization (Tutubalina and Nikolenko, 2017). The recent methods for extracting ADRs entirely rely on neural network models, particularly on multi-layer transformers (Vaswani et al., 2017). In the shared task of SMM4H 2019 (Weissenbacher and Gonzalez-Hernandez, 2019), the top performing run was BERT model (Devlin et al., 2019) pretrained on drug related tweets. Remarkably, one year later in the shared task of SMM4H 2020 (Gonzalez-Hernandez et al., 2020), again a variant of pretrained BERT achieved the best performance (Liu et al., 2019). Here, we propose an algorithm to improve on pretrained BERT in this task. Our model relies on multi-view learning and exploits unlabeled data. To our knowledge, our model is the first approach that improves on the domain-specific pretrained BERT. softmax softmax Document View Drug View BERT [CLS] this seroquel hitting BERT me [SEP] [CLS] this seroquel hitting me [SEP] Figure 1: The"
2021.smm4h-1.2,P84-1044,0,0.178271,"Missing"
C18-1003,W16-3612,1,0.922002,"ntity linking model that jointly identifies both singular and plural mentions (Section 5). All models are evaluated on our dataset (Section 6); the experiments reveal significant improvement from our new models compared to the previous state-of-the-art models dedicated for singular mentions. As far as we can tell, this is the first time that such annotation for plural mentions is provided in a large enough scale that deep learning models can be trained on, at the same time, machine learning models are developed to achieve promising results for the resolution of plural mentions. 2 Related Work Chen and Choi (2016) were the first to introduce the task of character identification and provided a new corpus based on TV show transcripts. Given a dialogue transcribed in text where all mentions are detected, character identification aims to find the entity for each personal mention, who may or may not be active in the dialogue. Unlike most other entity linking tasks focusing on Wikification, this task is challenging because it is dialogue-based where the entities are general characters in the show. This corpus was later expanded by Chen et al. (2017) who added annotation for the ambiguous entity types. In thi"
C18-1003,K17-1023,1,0.839291,"lts for the resolution of plural mentions. 2 Related Work Chen and Choi (2016) were the first to introduce the task of character identification and provided a new corpus based on TV show transcripts. Given a dialogue transcribed in text where all mentions are detected, character identification aims to find the entity for each personal mention, who may or may not be active in the dialogue. Unlike most other entity linking tasks focusing on Wikification, this task is challenging because it is dialogue-based where the entities are general characters in the show. This corpus was later expanded by Chen et al. (2017) who added annotation for the ambiguous entity types. In this work, we expanded the corpus further by doubling the size of the annotation and adding new annotation for plurals. The character identification corpus can be used for both coreference resolution and entity linking tasks. Our approach to coreference resolution was partially motivated by the previous works, Clark and Manning (2016) and Durrett et al. (2013), who tackled the general cases of coreference resolution including plurals; however, since their approaches were based on the annotation provided by CoNLL’12, they did not handle p"
C18-1003,D16-1245,0,0.0860999,"handle the uniqueness of plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks. 1 Introduction Resolution tasks such as coreference resolution and entity linking are challenging because they require a holistic view of a document (or across multiple documents) to find correct entities. Although many models have been proposed for these tasks (Clark and Manning, 2016; Francis-Landau et al., 2016; Wiseman et al., 2016; Gupta et al., 2017; Lee et al., 2017), most of them are focused on singular mentions such that they are insufficient for resolving the other type of mentions, plural, although the amount of plural mentions is not negligible in practice.1 Table 1 illustrates how mentions are annotated for coreference resolution by the CoNLL’12 shared task (Pradhan et al., 2012) and our proposed work. In the CoNLL’12 annotation, the plural mention They8 is grouped with the noun phrase [Mary1 and John2 ]3 ; however, the other plural mention We7 becomes a single"
C18-1003,P13-1012,0,0.0239591,"linking tasks focusing on Wikification, this task is challenging because it is dialogue-based where the entities are general characters in the show. This corpus was later expanded by Chen et al. (2017) who added annotation for the ambiguous entity types. In this work, we expanded the corpus further by doubling the size of the annotation and adding new annotation for plurals. The character identification corpus can be used for both coreference resolution and entity linking tasks. Our approach to coreference resolution was partially motivated by the previous works, Clark and Manning (2016) and Durrett et al. (2013), who tackled the general cases of coreference resolution including plurals; however, since their approaches were based on the annotation provided by CoNLL’12, they did not handle plural mentions to our satisfaction (Table 1). Jain et al. (2004) presented a rule-based system for resolving plural mentions, which was limited to unambiguous plural types. Our work is distinguished because we handle both ambiguous and unambiguous types of plural mentions, which makes it more challenging. Chen et al. (2017) presented an entity linking model that identified the real entity of each singular mention, w"
C18-1003,N16-1150,0,0.0306293,"plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks. 1 Introduction Resolution tasks such as coreference resolution and entity linking are challenging because they require a holistic view of a document (or across multiple documents) to find correct entities. Although many models have been proposed for these tasks (Clark and Manning, 2016; Francis-Landau et al., 2016; Wiseman et al., 2016; Gupta et al., 2017; Lee et al., 2017), most of them are focused on singular mentions such that they are insufficient for resolving the other type of mentions, plural, although the amount of plural mentions is not negligible in practice.1 Table 1 illustrates how mentions are annotated for coreference resolution by the CoNLL’12 shared task (Pradhan et al., 2012) and our proposed work. In the CoNLL’12 annotation, the plural mention They8 is grouped with the noun phrase [Mary1 and John2 ]3 ; however, the other plural mention We7 becomes a singleton because there is no noun"
C18-1003,D17-1284,0,0.0245017,"coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks. 1 Introduction Resolution tasks such as coreference resolution and entity linking are challenging because they require a holistic view of a document (or across multiple documents) to find correct entities. Although many models have been proposed for these tasks (Clark and Manning, 2016; Francis-Landau et al., 2016; Wiseman et al., 2016; Gupta et al., 2017; Lee et al., 2017), most of them are focused on singular mentions such that they are insufficient for resolving the other type of mentions, plural, although the amount of plural mentions is not negligible in practice.1 Table 1 illustrates how mentions are annotated for coreference resolution by the CoNLL’12 shared task (Pradhan et al., 2012) and our proposed work. In the CoNLL’12 annotation, the plural mention They8 is grouped with the noun phrase [Mary1 and John2 ]3 ; however, the other plural mention We7 becomes a singleton because there is no noun phrase representing such an entity. Since"
C18-1003,D17-1018,0,0.0476906,"ion and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks. 1 Introduction Resolution tasks such as coreference resolution and entity linking are challenging because they require a holistic view of a document (or across multiple documents) to find correct entities. Although many models have been proposed for these tasks (Clark and Manning, 2016; Francis-Landau et al., 2016; Wiseman et al., 2016; Gupta et al., 2017; Lee et al., 2017), most of them are focused on singular mentions such that they are insufficient for resolving the other type of mentions, plural, although the amount of plural mentions is not negligible in practice.1 Table 1 illustrates how mentions are annotated for coreference resolution by the CoNLL’12 shared task (Pradhan et al., 2012) and our proposed work. In the CoNLL’12 annotation, the plural mention They8 is grouped with the noun phrase [Mary1 and John2 ]3 ; however, the other plural mention We7 becomes a singleton because there is no noun phrase representing such an entity. Since CoNLL’12 limits eac"
C18-1003,H05-1004,0,0.164133,"our coreference resolution models. B3 (Bagga and Baldwin, 1998) is a mention-based metric that measures precision (P ) and recall (R) as follows (D: a set of documents, N : the total number s/o of mentions in D, Cm : the cluster from the system (s) or the oracle (o) that the mention m belongs to): P = s ∩ Co | 1 X X |Cm m s | N |Cm R= d∈D m∈d s ∩ Co | 1 X X |Cm m o| N |Cm d∈D m∈d 29 ∗ is replaced by the union of In our case, each mention can be assigned to more than one cluster; thus, Cm all clusters that the mention m belongs to, which enables this metric to evaluate plural mentions. CEAFφ4 (Luo, 2005) is an entity-based metric that first creates a similarity matrix M ∈ R|S|×|O |where S and O are the sets of clusters produced by the system and the oracle, respectively. It then measures the similarity between every pair of clusters (C s , C o ) ∈ S × O where s ∈ [1, |S|] and o ∈ [1, |O|] such that: Ms,o = 2 × |C s ∩ C o | |C s |+ |C o | Given this similarity matrix, the Hungarian algorithm is used to find the list H that contains similarity scores from the most similar matching pairs of clusters (C s , C o ) ∈ SP× O such that |H |= min(|S|, |O|). Finally, the overall similarity between S and"
C18-1003,W12-4501,0,0.711461,"g are challenging because they require a holistic view of a document (or across multiple documents) to find correct entities. Although many models have been proposed for these tasks (Clark and Manning, 2016; Francis-Landau et al., 2016; Wiseman et al., 2016; Gupta et al., 2017; Lee et al., 2017), most of them are focused on singular mentions such that they are insufficient for resolving the other type of mentions, plural, although the amount of plural mentions is not negligible in practice.1 Table 1 illustrates how mentions are annotated for coreference resolution by the CoNLL’12 shared task (Pradhan et al., 2012) and our proposed work. In the CoNLL’12 annotation, the plural mention They8 is grouped with the noun phrase [Mary1 and John2 ]3 ; however, the other plural mention We7 becomes a singleton because there is no noun phrase representing such an entity. Since CoNLL’12 limits each plural mention to be linked to a single noun phrase, it loses connections to individual entities that exist within the document but not grouped as a noun phrase. Document [Mary1 and John2 ]3 came to see me4 yesterday. She5 looked happy, and so did he6 . We7 had a great time together. They8 left around noon. CoNLL’12 Our W"
C18-1003,M95-1005,0,0.29658,"Missing"
C18-1003,N16-1114,0,0.0450914,"ents show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks. 1 Introduction Resolution tasks such as coreference resolution and entity linking are challenging because they require a holistic view of a document (or across multiple documents) to find correct entities. Although many models have been proposed for these tasks (Clark and Manning, 2016; Francis-Landau et al., 2016; Wiseman et al., 2016; Gupta et al., 2017; Lee et al., 2017), most of them are focused on singular mentions such that they are insufficient for resolving the other type of mentions, plural, although the amount of plural mentions is not negligible in practice.1 Table 1 illustrates how mentions are annotated for coreference resolution by the CoNLL’12 shared task (Pradhan et al., 2012) and our proposed work. In the CoNLL’12 annotation, the plural mention They8 is grouped with the noun phrase [Mary1 and John2 ]3 ; however, the other plural mention We7 becomes a singleton because there is no noun phrase representing su"
choi-etal-2010-propbank,han-etal-2002-development,1,\N,Missing
choi-etal-2010-propbank,P98-1013,0,\N,Missing
choi-etal-2010-propbank,C98-1013,0,\N,Missing
choi-etal-2010-propbank,J05-1004,1,\N,Missing
choi-etal-2010-propbank,kipper-etal-2006-extending,1,\N,Missing
choi-etal-2010-propbank,palmer-etal-2008-pilot,1,\N,Missing
choi-etal-2010-propbank-instance,J93-2004,0,\N,Missing
choi-etal-2010-propbank-instance,han-etal-2002-development,1,\N,Missing
choi-etal-2010-propbank-instance,J05-1004,1,\N,Missing
choi-etal-2010-propbank-instance,palmer-etal-2008-pilot,1,\N,Missing
K17-1023,P06-1005,0,0.029843,"concatenated with the pairwise features extracted by φp (mi , mj ) to form the mention-pair embedding rp (mi , mj ), defined as follows:   r (m ) rp (mi , mj ) = CONV3 ( s i ) k φp (mi , mj ) rs (mj ) • If ∃1≤j<i . max(σ(mi , mj )) ≥ 0.5, then Cmk ← Cmk ∪ {mi }, where mk = argj max(σ(mi , mj )). Table 3 shows feature templates used for our ACNN model. Sentence and utterance embeddings are the average vectors of all word embeddings in the sentence and utterance, respectively. Speaker embeddings are randomly generated using the Gaussian distribution. Gender and plurality information are from Bergsma and Lin (2006), and word animacy is from Durrett and Klein (2013). Map φ1e (m) The learned mention-pair embedding is put through the hidden layer with the linear rectifier activation function (ReLu) before applying the sigmoid function σ(mi , mj ) to determine the coreferent relation between mentions mi and mj , defined as follows: φ2e (m) φ3e (m) h(x) = ReLU(wh · x + bh ) φ4e (m) σ(mi , mj ) = sigmoid(ws · h(rp (mi , mj )) + bs ) The purpose of the sigmoid function σ(mi , mj ) is twofold. For each mention mi , it performs binary classifications between mi and mj where j ∈ [1, i). If max(σ(mi , mj )) < 0.5,"
K17-1023,W16-3612,1,0.922098,"output of this task will enhance inference on dialogue contexts by providing finer-grained information about individuals. In this paper, we augment and correct the existing corpus for character identification, and propose an end-to-end deep-learning system that combines neural models for coreference resolution and entity linking to tackle the task of character identification. The updated corpus and the source code of our models are published and publicly available.2 This combined system utilizes the strengths from both The dialogues are extracted from TV show transcripts by the previous work (Chen and Choi, 2016). 2 nlp.mathcs.emory.edu/character-mining/ 216 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 216–225, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Speaker Joey Rachel Chandler Ross Utterance Yeah, right! ... You1 serious? Everything you2 need to know is in that first kiss. Yeah. For us3 , it’s like the stand-up comedian4 you5 have to sit through before the main dude6 starts. It’s not that we7 don’t like the comedian8 , it’s that ... that’s not why we9 bought the ticket. {You1 } → Rachel, {us3"
K17-1023,W13-4073,0,0.0226781,"nowledge bases, our task is closely aligned to entity linking. Recent advances in entity linking are also applicable to our task since we see FrancisLandau et al. (2016) use convolutional nets to capture semantic similarity between a mention and an entity by comparing context of the mention with the description of the entity. This work validates our usage of deep learning for character identification. 3 Dialogue tracking has been an expanding task as shown by the Dialogue State Tracking Challenges hosted by Microsoft (Kim et al., 2015). That an ongoing conversation can be dynamically tracked (Henderson et al., 2013) is exciting and applicable to our task because the state of a conversation may yield significant hints for entity linking and coreference resolution. Speaker identification, a task similar to character identification, has already shown some success with partial dialogue tracking by dynamically identifying speakers at each turn in a dialogue using conditional random field models. 3 Corpus The character identification corpus created by Chen and Choi (2016) includes entity annotation of personal mentions specific to the domain of multiparty dialogues. While the corpus covers a large amount of en"
K17-1023,H05-1004,0,0.181493,"Missing"
K17-1023,D16-1245,0,0.0754003,"tion, entities in this task have no precompiled information from a knowledge base, which is another challenging aspect. This task is similar to coreference resolution in the sense that it groups mentions into entities, but distinct because it requires the identification of mention groups as real entities. Furthermore, even if it can be tackled as a coreference resolution task, only a few coreference resolution systems are designed to handle dialogues well (Rocha, 1999; Niraula et al., 2014) although several state-of-the-art systems have been proposed for the general domain (Peng et al., 2015; Clark and Manning, 2016; Wiseman et al., 2016). Due to the nature of multiparty dialogues where speakers take turns to complete a context, character identification becomes a critical step to adapt higher-level NLP tasks (e.g., question answering, summarization) to this domain. This task can also bring another level of sophistication to intelligent personal assistants and intelligent tutoring systems. Perhaps the most challenging aspect comes from colloquial writing that consists of ironies, metaphors, or rhetorical questions. Despite all the challenges, we believe that the output of this task will enhance inference"
K17-1023,D13-1203,0,0.0197484,"d by φp (mi , mj ) to form the mention-pair embedding rp (mi , mj ), defined as follows:   r (m ) rp (mi , mj ) = CONV3 ( s i ) k φp (mi , mj ) rs (mj ) • If ∃1≤j<i . max(σ(mi , mj )) ≥ 0.5, then Cmk ← Cmk ∪ {mi }, where mk = argj max(σ(mi , mj )). Table 3 shows feature templates used for our ACNN model. Sentence and utterance embeddings are the average vectors of all word embeddings in the sentence and utterance, respectively. Speaker embeddings are randomly generated using the Gaussian distribution. Gender and plurality information are from Bergsma and Lin (2006), and word animacy is from Durrett and Klein (2013). Map φ1e (m) The learned mention-pair embedding is put through the hidden layer with the linear rectifier activation function (ReLu) before applying the sigmoid function σ(mi , mj ) to determine the coreferent relation between mentions mi and mj , defined as follows: φ2e (m) φ3e (m) h(x) = ReLU(wh · x + bh ) φ4e (m) σ(mi , mj ) = sigmoid(ws · h(rp (mi , mj )) + bs ) The purpose of the sigmoid function σ(mi , mj ) is twofold. For each mention mi , it performs binary classifications between mi and mj where j ∈ [1, i). If max(σ(mi , mj )) < 0.5, the model considers no coreferent relation between"
K17-1023,niraula-etal-2014-dare,0,0.0217365,"entity linking focuses on Wikification (Mihalcea and Csomai, 2007a; Ratinov et al., 2011a; Guo et al., 2013). Unlike Wikification, entities in this task have no precompiled information from a knowledge base, which is another challenging aspect. This task is similar to coreference resolution in the sense that it groups mentions into entities, but distinct because it requires the identification of mention groups as real entities. Furthermore, even if it can be tackled as a coreference resolution task, only a few coreference resolution systems are designed to handle dialogues well (Rocha, 1999; Niraula et al., 2014) although several state-of-the-art systems have been proposed for the general domain (Peng et al., 2015; Clark and Manning, 2016; Wiseman et al., 2016). Due to the nature of multiparty dialogues where speakers take turns to complete a context, character identification becomes a critical step to adapt higher-level NLP tasks (e.g., question answering, summarization) to this domain. This task can also bring another level of sophistication to intelligent personal assistants and intelligent tutoring systems. Perhaps the most challenging aspect comes from colloquial writing that consists of ironies,"
K17-1023,N16-1150,0,0.0775397,"ution, we take a similar approach to feature engineering by building mention and cluster embeddings with word embeddings (Clark and Manning, 2016) and include additional mention features described by Wiseman et al. (2015). We are motivated to use convolutional networks through the work of Wu and Ma (2017), but we distinguish our approach by using deep convolution to build embeddings for character identification. Entity linking has traditionally relied heavily on knowledge databases, most notably, Wikipedia, for entities (Mihalcea and Csomai, 2007b; Ratinov et al., 2011b; Gattani et al., 2013; Francis-Landau et al., 2016).3 Although we do not make use of knowledge bases, our task is closely aligned to entity linking. Recent advances in entity linking are also applicable to our task since we see FrancisLandau et al. (2016) use convolutional nets to capture semantic similarity between a mention and an entity by comparing context of the mention with the description of the entity. This work validates our usage of deep learning for character identification. 3 Dialogue tracking has been an expanding task as shown by the Dialogue State Tracking Challenges hosted by Microsoft (Kim et al., 2015). That an ongoing conver"
K17-1023,N13-1122,0,0.0185906,"ective is to assign each mention to an entity, who may or may not appear as a speaker in the dialogue. For the example in Table 1, the mention comedian is not one of the speakers in the dialogue; nonetheless, it clearly refers to a real person that may appear in some other dialogues. Identifying such mentions as actual characters requires cross-document entity resolution, which makes this task challenging. 1 Character identification can be viewed as a task of entity linking. Most of the previous work on entity linking focuses on Wikification (Mihalcea and Csomai, 2007a; Ratinov et al., 2011a; Guo et al., 2013). Unlike Wikification, entities in this task have no precompiled information from a knowledge base, which is another challenging aspect. This task is similar to coreference resolution in the sense that it groups mentions into entities, but distinct because it requires the identification of mention groups as real entities. Furthermore, even if it can be tackled as a coreference resolution task, only a few coreference resolution systems are designed to handle dialogues well (Rocha, 1999; Niraula et al., 2014) although several state-of-the-art systems have been proposed for the general domain (Pe"
K17-1023,W12-4501,0,0.039874,"vious work, our annotators are familiar with the TV show, and the task takes about 3 weeks to complete. F1 F2 Σ P 5,101 5,312 10,413 S 2,610 2,432 5,042 C 1,259 1,280 2,388 G 109 42 151 N 152 111 263 O 184 167 351 Σ 9,306 9,304 18,608 Table 2: Counts of disambiguated mentions. P/S: main and secondary character entities. C/G/N/O: Collective/General/Generic/Other. 4 a coreference resolution system. Thus, the end result of this task largely depends on the quality of the coreference resolution model. Several coreference resolution systems have been proposed and shown state-of-the-art performance (Pradhan et al., 2012); however, they are not necessarily designed for the genre of multiparty dialogue, where each document comprises utterances from multiple speakers. This section describes a novel approach to coreference resolution using Convolutional Neural Networks (CNN). Our model takes groups of features incorporating several dialogue aspects, feeds them into deep convolution layers, and dynamically generates mention embeddings and mention-pair embeddings, which are used to create the cluster embeddings that significantly improve the performance of our entity linking model (Section 5). 4.1 Agglomerative CNN"
K17-1023,P11-1138,0,0.174694,"n the dialogue. The objective is to assign each mention to an entity, who may or may not appear as a speaker in the dialogue. For the example in Table 1, the mention comedian is not one of the speakers in the dialogue; nonetheless, it clearly refers to a real person that may appear in some other dialogues. Identifying such mentions as actual characters requires cross-document entity resolution, which makes this task challenging. 1 Character identification can be viewed as a task of entity linking. Most of the previous work on entity linking focuses on Wikification (Mihalcea and Csomai, 2007a; Ratinov et al., 2011a; Guo et al., 2013). Unlike Wikification, entities in this task have no precompiled information from a knowledge base, which is another challenging aspect. This task is similar to coreference resolution in the sense that it groups mentions into entities, but distinct because it requires the identification of mention groups as real entities. Furthermore, even if it can be tackled as a coreference resolution task, only a few coreference resolution systems are designed to handle dialogues well (Rocha, 1999; Niraula et al., 2014) although several state-of-the-art systems have been proposed for th"
K17-1023,W99-0208,0,0.0936753,"vious work on entity linking focuses on Wikification (Mihalcea and Csomai, 2007a; Ratinov et al., 2011a; Guo et al., 2013). Unlike Wikification, entities in this task have no precompiled information from a knowledge base, which is another challenging aspect. This task is similar to coreference resolution in the sense that it groups mentions into entities, but distinct because it requires the identification of mention groups as real entities. Furthermore, even if it can be tackled as a coreference resolution task, only a few coreference resolution systems are designed to handle dialogues well (Rocha, 1999; Niraula et al., 2014) although several state-of-the-art systems have been proposed for the general domain (Peng et al., 2015; Clark and Manning, 2016; Wiseman et al., 2016). Due to the nature of multiparty dialogues where speakers take turns to complete a context, character identification becomes a critical step to adapt higher-level NLP tasks (e.g., question answering, summarization) to this domain. This task can also bring another level of sophistication to intelligent personal assistants and intelligent tutoring systems. Perhaps the most challenging aspect comes from colloquial writing th"
K17-1023,M95-1005,0,0.12608,"Missing"
K17-1023,P15-1137,0,0.0186956,"ion to learn mention, mention-pair, and cluster embeddings, and the results are taken as input to our entity linking model that assigns mentions to their real entities. Entities, including main characters and recurring support characters, are selected from a TV show to mimic a realistic scenario. To the best of our knowledge, this is the first end-toend model that performs character identification on multiparty dialogues. 2 Related Work The latest coreference systems employ advanced context features in tandem with deep networks to achieve state-of-the-art performance (Clark and Manning, 2016; Wiseman et al., 2015). Since our task is similar to coreference resolution, we take a similar approach to feature engineering by building mention and cluster embeddings with word embeddings (Clark and Manning, 2016) and include additional mention features described by Wiseman et al. (2015). We are motivated to use convolutional networks through the work of Wu and Ma (2017), but we distinguish our approach by using deep convolution to build embeddings for character identification. Entity linking has traditionally relied heavily on knowledge databases, most notably, Wikipedia, for entities (Mihalcea and Csomai, 2007"
K17-1023,N16-1114,0,0.0715909,"Missing"
K17-1023,K15-1002,0,\N,Missing
L18-1347,W11-3801,1,0.667691,"s also promoted research on cross-lingual learning that explores the possibility of adapting statistical parsing models from one language to another (McDonald et al., 2013). Several treebanks had been introduced for Korean, all of which comprised annotation of morphemes and phrase structure trees (Choi et al., 1994; Han et al., 2002; Hong, 2009), each following its own set of guidelines. Phrase structure trees in these treebanks had been converted into dependency trees using head-finding rules and linguistically-motivated heuristics, and used to evaluate Korean dependency parsing performance (Choi and Palmer, 2011; Choi, 2013). The previous efforts did not, however, focus on the compatibility among dependency trees converted from different corpora, resulting in the generation of a distinct set of dependency relations for each treebank. This paper presents three dependency treebanks in Korean, derived from existing corpora and pseudo-annotated by the latest UD guidelines, version 2. The motivation behind this study is to make a comprehensive analysis between these corpora and convert phrase structure trees across different treebanks into dependency trees with consistent relations, providing a large corp"
L18-1347,P13-2017,0,0.28444,"Missing"
L18-1347,petrov-etal-2012-universal,0,0.0762141,"he Google UD Korean Treebank is manually assessed and systematically corrected (Section 3.). • Phrase structure trees in both the Penn Korean Treebank and the K AIST Treebank are converted into dependency trees using the UD guidelines (Sections 4. and 5.). • Corpus analytics are provided that include statistics of the new dependency treebanks, and remaining issues with the current annotation (Section 6.). To the best of our knowledge, this is the first time that these Korean corpora are analyzed together and transformed into dependency trees following the latest UD guidelines. 2. Related Work Petrov et al. (2012) introduced the universal part-of-speech tagset and provided a mapping from 25 different treebank tagsets to this universal set. They showed that parsing performance using the universal part-of-speech tagset was comparable to the one using the original tagsets. McDonald et al. (2013) presented the universal dependency annotation and provided pseudo and manually annotated dependency treebanks for 6 languages. They showed promising results for cross-lingual parsing and initiated the effort for developing universally acceptable grammars. The official UD project started with a group of 10 language"
L18-1347,K17-3001,0,0.0644624,"Missing"
N10-2004,J93-2004,0,0.0372778,"the same open-dialog as you saw in Figure. 3. The [New Tasks] shows a list of tasks that have not been adjudicated, and the [My Tasks] shows a list of tasks that have been adjudicated. Gold mode does not allow adjudicators to open tasks that have not been at least single-annotated. 5 5.1 Figure 3: Open-dialog Once you choose a task and click the [Enter] button, Jubilee’s main window will be prompted (Figure 4). There are three views available in the main window: the treebank view, frameset view and argument view. By default, the treebank view shows the first tree (in the Penn Treebank format (Marcus et al., 1993)) in the selected task. The frameset view displays rolesets and allows the annotator to choose the sense of the predicate with respect to the current tree. The argument view contains buttons representing each of the Propbank argument labels. 15 Gold mode Demonstrations Cornerstone We will begin by demonstrating how to view frameset files in both multi-lemma and unilemma mode. In each mode, we will open an existing frameset file, compare its interface with the actual xml file, and show how intuitive it is to interact with the tool. Next, we will demonstrate how to create and edit a new frameset"
N10-2004,palmer-etal-2008-pilot,1,0.824823,"using one tool that simultaneously provides rich syntactic information as well as comprehensive semantic information. Both Cornerstone and Jubilee are developed in Java (Jdk 6.0), so they run on any platform where the Java virtual machine is installed. They are light enough to run as X11 applications. This aspect is important because Propbank data are usually stored in a server, so annotators need to update them remotely (via ssh). One of the biggest advantages of using these tools is that they accommodate several languages; in fact, the tools have been used for Propbank projects in Arabic (M.Diab et al., 2008), Chinese (Xue and Palmer, 2009), English (Palmer et al., 2005) and Hindi, and have been tested in Korean (Han et al., 2002). This demo paper details how to create Propbank framesets in Cornerstone, and how to annotate Propbank instances using Jubilee. There are two modes in which to run Cornerstone: multi-lemma and uni-lemma mode. In multilemma mode, a predicate can have multiple lem13 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 13–16, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics mas, whereas a predicate can have only one lemma in un"
N10-2004,J05-1004,1,0.555357,"absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation. Cornerstone is a user-friendly xml editor, customized to allow frame authors to create and edit frameset files. Both tools have been successfully adapted to many Propbank projects; they run platform independently, are light enough to run as X11 applications and support multiple languages such as Arabic, Chinese, English, Hindi and Korean. 1 Introduction Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles (Palmer et al., 2005). Propbank annotation also requires the choice of a sense id for each predicate. Thus, for each predicate in the Propbank, there exists a corresponding frameset file encompassing one or more senses of the predicate. All frameset files are written in xml, which is somewhat difficult to read and edit. Although there already exist many xml editors, most of them require some degree of knowledge of xml, and none of them are specifically customized for frameset files. This motivated the development of our own frameset editor, Cornerstone. Jubilee is a Propbank instance editor. For each verb predicat"
N10-2004,han-etal-2002-development,1,\N,Missing
N15-2019,D14-1159,0,0.127376,"ation. Thanks to years of research on statistical parsing, several tools are 1 2 Jinho D. Choi Mathematics and Computer Science Emory University Atlanta, GA 30322, USA jinho.choi@emory.edu https://answers.yahoo.com https://www.quora.com Robustness of handling several types of questions is one of the key aspects about a question-answering system; yet most of previous work had focused on answering simple factoid questions (Yao et al., 2013; Yih et al., 2013). Recently, researchers started focusing on solving complex questions involving arithmetics or biological processes (Hosseini et al., 2014; Berant et al., 2014). A complex question can be described as a question requiring the collection and synthesis of information from multiple sentences (Chali and Joty, 2008). The more complex the questions become, the harder it is to build a structural model that is general enough to capture information for all different types of questions. This paper suggests an architectural approach of representing entity relations as well as its application to complex question-answering. First, we present a systematic way of building a graph by merging four kinds of information: syntactic dependencies, semantic role labels, na"
N15-2019,D08-1032,0,0.0161569,"a, GA 30322, USA jinho.choi@emory.edu https://answers.yahoo.com https://www.quora.com Robustness of handling several types of questions is one of the key aspects about a question-answering system; yet most of previous work had focused on answering simple factoid questions (Yao et al., 2013; Yih et al., 2013). Recently, researchers started focusing on solving complex questions involving arithmetics or biological processes (Hosseini et al., 2014; Berant et al., 2014). A complex question can be described as a question requiring the collection and synthesis of information from multiple sentences (Chali and Joty, 2008). The more complex the questions become, the harder it is to build a structural model that is general enough to capture information for all different types of questions. This paper suggests an architectural approach of representing entity relations as well as its application to complex question-answering. First, we present a systematic way of building a graph by merging four kinds of information: syntactic dependencies, semantic role labels, named entities, and coreference links, generated by existing tools (Section 3). We then demonstrate, how our graph can be coupled with statistical learnin"
N15-2019,P09-1068,0,0.015752,"ever, it is distinguished in a way that our constructed graph is not designed to handle just arithmetic questions, but complex questions in general. Our work is also related to research of aligning text into a set of entities and instances describing states of the world. Snyder and Barzilay (2007) presented an approach for solving text-to-database alignment as a structured multi-label classification. Vogel and Jurafsky (2010) presented a learning system that followed navigational paths based on natural language by utilizing apprenticeship from directions on the map paired with human language. Chambers and Jurafsky (2009) presented an unsupervised learning system for narrative schemas based on coreferent arguments in chains of verbs. Pourdamghani et al. (2014) 141 and Pan et al. (2015) presented Abstract Meaning Representation (AMR) consisting of multi-layered relations for English sentences. Our semantics-based graph shares a similar idea with AMR; however, our graph is constructed from existing structures such as dependency trees and semantic roles, whereas AMR requires its won annotation, which could be manual intensive work for building statical parsing models. 3 3.1 Semantics-based Knowledge Approach Moti"
N15-2019,P13-1104,1,0.825083,"served today? Tim’s cat had kittens. He gave 3 to Jessica and 6 to Sara. He now has 9 kittens. How many kittens did he have to start with? Equation x=9+6 x=3+6+9 Table 2: Sample of arithmetic questions. 5 5.1 Experiments Data For our experiments, we use the arithmetic dataset provided by the Allen Institute.3 The dataset consists of 395 arithmetic questions together with their 3 allenai.org/content/data/ arithmeticquestions.pdf equations and answers. We parsed all data using the dependency parser, the semantic role labeler, the named entity tagger, and the coreference resolution in ClearNLP (Choi and McCallum, 2013; Choi, 2012).4 We then split the dataset into 3-folds for cross validation in a way that the polarity distributions are similar across different sets (Table 3). 5.2 Features The following features are used for our experiments: • Semantic role labels; especially numbered arguments as in PropBank (Palmer et al., 2005). • Sequence of verbs and arguments whose semantic roles are recognized as ‘themes’. • Frequency of verbs and theme arguments in the current context. • Similarity between verbs and theme arguments across sentences. • Distance of the verb to the final question. Given our graph, it w"
N15-2019,N10-1145,0,0.0288987,"quires understanding of the entire 140 Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 140–146, c Denver, Colorado, June 1, 2015. 2015 Association for Computational Linguistics context (Section 4). Our experiments show that it is possible to retrieve document-level entity relations through our graph, providing enough information to handle such complex questions (Section 5). 2 Related Work Punyakanok et al. (2004) presented a system using edit distance between question and potential answer pairs, measured by the number of required transformations of their dependency trees. Heilman and Smith (2010) presented a more sophisticated system finding the most efficient tree transformation using a greedy search. Cui et al. (2005) proposed a system utilizing fuzzy relation matching guided by statistical models. Yao et al. (2013) described an approach taking both an edit distance and sequence tagging for selecting finer-grained answers within answer candidates. All the work above leverages dependency tree matching similar to ours; however, our approach performs matching through semantic relations as well as coreference links, and also is designed for handling complex questions whereas the others"
N15-2019,D14-1058,0,0.133263,"ts of meaning representation. Thanks to years of research on statistical parsing, several tools are 1 2 Jinho D. Choi Mathematics and Computer Science Emory University Atlanta, GA 30322, USA jinho.choi@emory.edu https://answers.yahoo.com https://www.quora.com Robustness of handling several types of questions is one of the key aspects about a question-answering system; yet most of previous work had focused on answering simple factoid questions (Yao et al., 2013; Yih et al., 2013). Recently, researchers started focusing on solving complex questions involving arithmetics or biological processes (Hosseini et al., 2014; Berant et al., 2014). A complex question can be described as a question requiring the collection and synthesis of information from multiple sentences (Chali and Joty, 2008). The more complex the questions become, the harder it is to build a structural model that is general enough to capture information for all different types of questions. This paper suggests an architectural approach of representing entity relations as well as its application to complex question-answering. First, we present a systematic way of building a graph by merging four kinds of information: syntactic dependencies, se"
N15-2019,P14-1026,0,0.151596,"m finding the most efficient tree transformation using a greedy search. Cui et al. (2005) proposed a system utilizing fuzzy relation matching guided by statistical models. Yao et al. (2013) described an approach taking both an edit distance and sequence tagging for selecting finer-grained answers within answer candidates. All the work above leverages dependency tree matching similar to ours; however, our approach performs matching through semantic relations as well as coreference links, and also is designed for handling complex questions whereas the others mainly focused on factoid questions. Kushman et al. (2014) described an approach for predicting sentence-to-equation alignments for solving arithmetic questions. Hosseini et al. (2014) presented a system predicting verb categories, and constructing equations from the context using these categories. Berant et al. (2014) proposed an approach that extracted structures from biological processes, and mapped each question to a query form. Our work is related to the first two work; however, it is distinguished in a way that our constructed graph is not designed to handle just arithmetic questions, but complex questions in general. Our work is also related t"
N15-2019,J05-1004,0,0.246815,"lution. In Figure 1, although John, He, and his are recognized as individual instances, they are grouped into one entity because they all refer to John. Maintaining these relations is crucial for answering complex questions. Instance An instance is the atomic-level object in our graph that usually represents a word-token, but can also represent compound words (e.g., New York), multiword expressions, etc. The instance is linked to other instances as a predicate, an argument, or an attribute. Predicate & Argument An instance is a predicate of another instance if it forms any argument structure (Palmer et al., 2005). Currently, our graph takes non-auxiliary verbs and a few eventive nouns as predicates provided by a semantic role labeler. An instance is an argument of another if it is required to complete the meaning 142 of the other instance. In Figure 1, John and car are arguments of bought because they are necessary to give an understanding of bought. We plan to improve these relations through semantic parsing in the future. The predicate and argument relations represent both semantic and syntactic relations between instances in the document. Semantic role labels in (Palmer et al., 2005) and dependency"
N15-2019,N15-1119,0,0.0666352,"esearch of aligning text into a set of entities and instances describing states of the world. Snyder and Barzilay (2007) presented an approach for solving text-to-database alignment as a structured multi-label classification. Vogel and Jurafsky (2010) presented a learning system that followed navigational paths based on natural language by utilizing apprenticeship from directions on the map paired with human language. Chambers and Jurafsky (2009) presented an unsupervised learning system for narrative schemas based on coreferent arguments in chains of verbs. Pourdamghani et al. (2014) 141 and Pan et al. (2015) presented Abstract Meaning Representation (AMR) consisting of multi-layered relations for English sentences. Our semantics-based graph shares a similar idea with AMR; however, our graph is constructed from existing structures such as dependency trees and semantic roles, whereas AMR requires its won annotation, which could be manual intensive work for building statical parsing models. 3 3.1 Semantics-based Knowledge Approach Motivation Our motivation arises from both the complexity and the variety of questions and their relevant contexts. The complexity concerns with exploiting syntactic depen"
N15-2019,D14-1048,0,0.0319768,"eral. Our work is also related to research of aligning text into a set of entities and instances describing states of the world. Snyder and Barzilay (2007) presented an approach for solving text-to-database alignment as a structured multi-label classification. Vogel and Jurafsky (2010) presented a learning system that followed navigational paths based on natural language by utilizing apprenticeship from directions on the map paired with human language. Chambers and Jurafsky (2009) presented an unsupervised learning system for narrative schemas based on coreferent arguments in chains of verbs. Pourdamghani et al. (2014) 141 and Pan et al. (2015) presented Abstract Meaning Representation (AMR) consisting of multi-layered relations for English sentences. Our semantics-based graph shares a similar idea with AMR; however, our graph is constructed from existing structures such as dependency trees and semantic roles, whereas AMR requires its won annotation, which could be manual intensive work for building statical parsing models. 3 3.1 Semantics-based Knowledge Approach Motivation Our motivation arises from both the complexity and the variety of questions and their relevant contexts. The complexity concerns with"
N15-2019,P10-1083,0,0.0250933,"categories. Berant et al. (2014) proposed an approach that extracted structures from biological processes, and mapped each question to a query form. Our work is related to the first two work; however, it is distinguished in a way that our constructed graph is not designed to handle just arithmetic questions, but complex questions in general. Our work is also related to research of aligning text into a set of entities and instances describing states of the world. Snyder and Barzilay (2007) presented an approach for solving text-to-database alignment as a structured multi-label classification. Vogel and Jurafsky (2010) presented a learning system that followed navigational paths based on natural language by utilizing apprenticeship from directions on the map paired with human language. Chambers and Jurafsky (2009) presented an unsupervised learning system for narrative schemas based on coreferent arguments in chains of verbs. Pourdamghani et al. (2014) 141 and Pan et al. (2015) presented Abstract Meaning Representation (AMR) consisting of multi-layered relations for English sentences. Our semantics-based graph shares a similar idea with AMR; however, our graph is constructed from existing structures such as"
N15-2019,N13-1106,0,0.0217845,"Missing"
N15-2019,P13-1171,0,0.0145753,"tion-answering has been been well-explored, several challenges still remain. One of such challenges concerns about architectural aspects of meaning representation. Thanks to years of research on statistical parsing, several tools are 1 2 Jinho D. Choi Mathematics and Computer Science Emory University Atlanta, GA 30322, USA jinho.choi@emory.edu https://answers.yahoo.com https://www.quora.com Robustness of handling several types of questions is one of the key aspects about a question-answering system; yet most of previous work had focused on answering simple factoid questions (Yao et al., 2013; Yih et al., 2013). Recently, researchers started focusing on solving complex questions involving arithmetics or biological processes (Hosseini et al., 2014; Berant et al., 2014). A complex question can be described as a question requiring the collection and synthesis of information from multiple sentences (Chali and Joty, 2008). The more complex the questions become, the harder it is to build a structural model that is general enough to capture information for all different types of questions. This paper suggests an architectural approach of representing entity relations as well as its application to complex q"
N15-2023,levy-andrew-2006-tregex,0,0.0158052,"in clause past. The future orientation provided by would is future with respect to the past reference time. However, the would in the second sentence is not a will of a past reference time, but picks out a “less-vivid” future relative to the present reference time (Iatridou, 2000). 5 5.1 Classification Syntactic structural rules We used the constituency grammar rules generated by Wolff and Copley. Rules were generated on the basis of linguistic theory, and then later refined on the basis of analyses of the false positives and misses. The rules were instantiated in the Tregex pattern language (Levy and Andrew, 2006), which could then be used to find matching structures in the parsed sentences. There were 39 future-related rules, 16 past-related rules, and 3 present-related rules. The rules varied from the purely syntactic to the lexical, with a number of rules containing of mix of both. Syntactic information helped to disambiguate the senses of the modal verbs. Fourteen of the future-related rules emphasized the modal verbs. Rules are released online at https://github.com/clir/time-percep tion. 5.2 Adaptive sub-gradient descent To build statistical models, we used a stochastic adaptive subgradient algori"
N15-2023,W09-3212,0,0.185419,"ree 164,772 196,049 Token 11,910 15,228 Table 1: Total number of sentences, subtrees and tokens We used the Stanford factored parser (Klein and Manning, 2002) to parse sentences into constituency grammar tree representations. Tokens were generated by a uni-bi-trigram mixed model. Subtree structures were generated using the Varro algorithm (Martens, 2010) with threshold k = 1 to include lexicons. For the future corpus, 2,529,040 subtrees were processed while for the non-future corpus 2,792,875 were processed. A subset of the subtrees were selected as words for the LDA analysis, as described in Martens (2009). 4 Examples While there are many cases of grammatical future marking (i.e., will, be going to) and lexical future meaning (e.g., plan, want, need, tomorrow, goal, ambition), many of the ways people use to refer to the future do not fall into one of these two types of linguistic categories. For example, as we have seen, it’s possible to have future reference without an obvious grammatical or lexical way of referring to the future. One way of doing this is with so-called futurate sentences (Copley, 2009; Kaufmann, 2005), such as Mary is going to Paris, which can refer to a contextually-provided"
N15-2023,C10-2093,0,0.0855253,"t not frequently seen in Natural Language Processing. Frequent Subtree Mining is a common data mining topic. Related algorithms such as TreeMiner, FreeQT have been developed to find most frequent structure in a given tree bank (Chi et al., 2005). Similar approaches have been explored in Moschitti (2006)’s work on using subtrees as features for Support Vector Machine. We did not use his approach because were were not interested in the similarity between tree structures, but rather in the linguistic regularities implicit in the text. For this reason, we chose to use Varro algorithm developed by Martens (2010), to exhaustively generate subtrees. 3 happiness, life and financial satisfaction. The task resulted in a total of 2007 sentences. Table 1 describes the distribution of our data. The sentences were rated by three human raters. For each sentence, raters indicated whether the expression referred to the future and their level of confidence of their decision. Data We used data collected through Amazon Mechanical Turk (MTurk). Participants were asked to write down their mind wanderings as follows: Please think back to the last time you were thinking about something other than what you were currentl"
N15-2023,E06-1015,0,0.0291601,"een developed to enable people to classify pre-labeled documents. The approach in this paper is single-label text classification using A DAG RAD (Duchi et al., 2011a). Later on, we explored Latent Dirichlet Modeling (Blei et al., 2003) on the basis of induced subtrees, which are commonly used in data mining, but not frequently seen in Natural Language Processing. Frequent Subtree Mining is a common data mining topic. Related algorithms such as TreeMiner, FreeQT have been developed to find most frequent structure in a given tree bank (Chi et al., 2005). Similar approaches have been explored in Moschitti (2006)’s work on using subtrees as features for Support Vector Machine. We did not use his approach because were were not interested in the similarity between tree structures, but rather in the linguistic regularities implicit in the text. For this reason, we chose to use Varro algorithm developed by Martens (2010), to exhaustively generate subtrees. 3 happiness, life and financial satisfaction. The task resulted in a total of 2007 sentences. Table 1 describes the distribution of our data. The sentences were rated by three human raters. For each sentence, raters indicated whether the expression refe"
N16-1031,D08-1031,0,0.0121939,"timizing combinations between existing features. Discovering novel features may require linguistic background as well as good understanding in machine learning such that it is often difficult to do. Optimizing feature combinations can be also difficult but usually requires less domain knowledge and more importantly, it can be as effective as discovering new features. It has been shown for many tasks that approaches using simple machine learning with extensive feature engineering outperform ones using more advanced machine learning with less intensive feature engineering (Xue and Palmer, 2004; Bengtson and Roth, 2008; Ratinov and Roth, 2009; Zhang and Nivre, 2011). Recently, people have tried to automate the second part of feature engineering, the optimization of feature combinations, through leading-edge models such as neural networks (Collobert et al., 2011). Coupled with embedding approaches (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks (Socher et al., 2013; Devlin et al., 2014; Yu e"
N16-1031,J92-4003,0,0.374956,"features. Digits inside the curly brackets imply the context windows with respect to the word wi to be tagged. For example, f{0,±1} represents the word-forms of wi , wi−1 , and wi+1 . No joint features (e.g., f0 f1 ) are included in this template; they should be automatically induced by dynamic feature induction. Orthographic (Gim´enez and M`arquez, 2004) and word shape (Finkel et al., 2005) features are adapted from the previous work. The positional features indicate whether wi is the first or the last word in the sentence. Word clusters are trained on the same datasets in Section 4.3 using Brown et al. (1992). f:{0,±1,±2} , fu:{0,±1,±2} , s:{0,±1} , c:{0,±1} , π2:{0} , π3:{0} , σ1:{0} , σ2:{0} , σ3:{0} , σ4:{0} , p:{0,−1,−2,−3} , a:{0,1,2,3} , O:{0} , P:{0} Table 2: Feature template for part-of-speech tagging. f : wordform, fu : uncapitalized word-form, s: word shape, c: word cluster, πk : k’th prefix, σk : k’th suffix, p: part-of-speech tag, a: ambiguity class, O: orthographic feature set, P: positional feature set. 4.5 Development The regularization parameter λ (Section 3.2) and the modulo divisor δ (Section 3.4) are tuned during development through grid search on λ ∈ [1E-9, 1E-6] and δ ∈ [1.5M,"
N16-1031,P13-1104,1,0.361499,"Missing"
N16-1031,P12-2071,1,0.895131,"Missing"
N16-1031,W02-1001,0,0.0458851,"adapted to any NLP task using sparse features. Dynamic feature induction allows humans to focus on the first part of feature engineering, the discovery of novel features, while machines handle the second part. Our approach was experimented with two core NLP tasks, part-of-speech tagging (Section 4) and named entity recognition (Section 5) and showed the state-of-the-art results for both tasks. 2 2.1 Background Nonlinearity in NLP Linear classification algorithms such as Perceptron, Winnow, or Support Vector Machines with a linear kernel have performed exceptionally well for various NLP tasks (Collins, 2002; Zhang and Johnson, 2003; Pradhan et al., 2005). This is not because our feature space is linearly separable by nature, but sparse fea271 Proceedings of NAACL-HLT 2016, pages 271–281, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Overview of dynamic feature induction. tures introduced to NLP yield very high dimensional vector space such that it is rather forced to be linearly separable. For example, NLP features for a word wi typically involve the word forms of wi−1 and wi (e.g., fi−1 , fi ). If the feature space is not linearly separable"
N16-1031,P14-1129,0,0.0212205,"Missing"
N16-1031,W08-0804,0,0.0501567,"Missing"
N16-1031,gimenez-marquez-2004-svmtool,0,0.046245,"Missing"
N16-1031,P08-2060,0,0.00934475,"Low dimensional features are well explored for most NLP tasks; it is the high dimensional features that are quite sensitive to specific tasks. Finding high dimensional features can be a manual intensive work and this is what dynamic feature induction intends to take over. 2.2 Related Work Kudo and Matsumoto (2003) introduced the polynomial kernel expansion that explicitly enumerated the feature combinations. Our approach is distinguished because they used a frequency-based PrefixSpan algorithm (Pei et al., 2001) whereas we used the online learning weights for finding the feature combinations. Goldberg and Elhadad (2008) suggested an efficient algorithm for computing polynomial kernel SVMs by combining inverted indexing and kernel expansion. Their work is focused more on improving support vector machines whereas our work is generalized to any linear classification algorithm. 1 The joint features tend to yield a much higher dimensional feature space than the primitive features. 272 Okanohara and Tsujii (2009) introduced an approach for generating feature combinations using `1 regularization and grafting (Perkins et al., 2003). Although we share similar ideas, their grafting algorithm starts with an empty featu"
N16-1031,C12-1059,0,0.0208724,"Missing"
N16-1031,P03-1004,0,0.0457198,"nt features we introduce, the higher chance we get for the feature space being linearly separable although these joint features can be very overfitted. Let us define low dimensional features as the primitive features such as fi−1 or fi , and high dimensional features as the joint features such as fi fi+1 . 1 Low dimensional features are well explored for most NLP tasks; it is the high dimensional features that are quite sensitive to specific tasks. Finding high dimensional features can be a manual intensive work and this is what dynamic feature induction intends to take over. 2.2 Related Work Kudo and Matsumoto (2003) introduced the polynomial kernel expansion that explicitly enumerated the feature combinations. Our approach is distinguished because they used a frequency-based PrefixSpan algorithm (Pei et al., 2001) whereas we used the online learning weights for finding the feature combinations. Goldberg and Elhadad (2008) suggested an efficient algorithm for computing polynomial kernel SVMs by combining inverted indexing and kernel expansion. Their work is focused more on improving support vector machines whereas our work is generalized to any linear classification algorithm. 1 The joint features tend to"
N16-1031,P09-1116,0,0.0168463,"Missing"
N16-1031,J93-2004,0,0.0586421,"Missing"
N16-1031,D15-1151,0,0.0125782,"ot find a critical role in the classification. 4.6 Evaluation 5 Named Entity Recognition 5.1 Corpus The English corpus from the CoNLL’03 shared task is used (Tjong Kim Sang and De Meulder, 2003) for named entity recognition experiments. Set TRN DEV TST Articles 946 216 231 Sentences 14,987 3,466 3,684 Words 203,621 51,362 46,435 Table 5: Distributions of the English corpus from the CoNLL’03 Table 4 shows the accuracies achieved by the models from Section 4.5 and the previous state-of-the-art approaches on the evaluation set. Approach Manning (2011) Manning (2011) Shen et al. (2007) Sun (2014) Moore (2015) Spoustov´a et al. (2009) Søgaard (2011) Tsuboi (2014) This work: M0 This work: M1 This work: M2 This work: M3 This work: M4 M2 showed a slightly lower accuracy on OOV than M1 even with the additional word cluster features. On the other hand, M2 did show a slightly higher accuracy on ALL, indicating that the model was probably too overfitted to the in-vocabulary words.8 However, M4 was still able to achieve improvements over M2 on both ALL and OOV, implying that dynamic feature induction facilitated the classifier to be trained more robustly. ALL 97.29 97.32 97.33 97.36 97.36 97.44 97.50 97.51"
N16-1031,N09-2025,0,0.0300171,"re combinations. Our approach is distinguished because they used a frequency-based PrefixSpan algorithm (Pei et al., 2001) whereas we used the online learning weights for finding the feature combinations. Goldberg and Elhadad (2008) suggested an efficient algorithm for computing polynomial kernel SVMs by combining inverted indexing and kernel expansion. Their work is focused more on improving support vector machines whereas our work is generalized to any linear classification algorithm. 1 The joint features tend to yield a much higher dimensional feature space than the primitive features. 272 Okanohara and Tsujii (2009) introduced an approach for generating feature combinations using `1 regularization and grafting (Perkins et al., 2003). Although we share similar ideas, their grafting algorithm starts with an empty feature set whereas ours starts with low dimensional features, and their correlation parameters αi,y are pre-computed whereas ours are dynamically determined. Strubell et al. (2015) suggested an algorithm that dynamically selected strong features during decoding. Our work is distinguished because we do not run multiple training phases as they do for figuring our strong features. 3 Dynamic Feature"
N16-1031,W14-1609,0,0.0326177,"Missing"
N16-1031,D14-1162,0,0.107861,"Missing"
N16-1031,W09-1119,0,0.0626257,"ween existing features. Discovering novel features may require linguistic background as well as good understanding in machine learning such that it is often difficult to do. Optimizing feature combinations can be also difficult but usually requires less domain knowledge and more importantly, it can be as effective as discovering new features. It has been shown for many tasks that approaches using simple machine learning with extensive feature engineering outperform ones using more advanced machine learning with less intensive feature engineering (Xue and Palmer, 2004; Bengtson and Roth, 2008; Ratinov and Roth, 2009; Zhang and Nivre, 2011). Recently, people have tried to automate the second part of feature engineering, the optimization of feature combinations, through leading-edge models such as neural networks (Collobert et al., 2011). Coupled with embedding approaches (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks (Socher et al., 2013; Devlin et al., 2014; Yu et al., 2014). However, n"
N16-1031,P07-1096,0,0.0154719,"plit for part-of-speech tagging experiments. Set TRN DEV TST Sections 0-18 19-21 22-24 Sentences 38,219 5,527 5,462 ALL 912,344 131,768 129,654 OOV 0 4,467 3,649 Table 1: Distributions of the Wall Street Journal corpus. TRN: training, DEV: development, TST: evaluation, ALL: all words, OOV: out-of-vocabulary words. 4.2 Tagging and Learning Algorithms A one-pass, left-to-right tagging algorithm is used for our experiments. Such a simple algorithm is chosen because we want to see the performance gain purely from our approach, not by a more sophisticated tagging algorithm (Toutanova et al., 2003; Shen et al., 2007), which may improve the performance further. For learning, the final algorithm from Section 3 is used. Additionally, mini-batch is applied, where each batch consists of training instances from k-number of sentences, causing the sizes of these batches different. We found that grouping instances with respect to the sentence boundary was more effective than batching them across arbitrary sentences. For all our experiments, the learning rate η = 0.02 and the mini-batch boundary k = 5 were used without tuning. 4.3 Ambiguity Classes The ambiguity class of a word is the concatenation of all possible"
N16-1031,D13-1170,0,0.00450501,"(Xue and Palmer, 2004; Bengtson and Roth, 2008; Ratinov and Roth, 2009; Zhang and Nivre, 2011). Recently, people have tried to automate the second part of feature engineering, the optimization of feature combinations, through leading-edge models such as neural networks (Collobert et al., 2011). Coupled with embedding approaches (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks (Socher et al., 2013; Devlin et al., 2014; Yu et al., 2014). However, neural networks are not as good at optimizing combinations between sparse features, which are still the most dominating factors in natural language processing. This paper introduces a new technique called dynamic feature induction that automates the optimization of feature combinations (Section 3), and can be easily adapted to any NLP task using sparse features. Dynamic feature induction allows humans to focus on the first part of feature engineering, the discovery of novel features, while machines handle the second part. Our approach was exper"
N16-1031,P11-2009,0,0.0253505,"Missing"
N16-1031,E09-1087,0,0.0314792,"Missing"
N16-1031,P15-1015,0,0.0127968,"improving support vector machines whereas our work is generalized to any linear classification algorithm. 1 The joint features tend to yield a much higher dimensional feature space than the primitive features. 272 Okanohara and Tsujii (2009) introduced an approach for generating feature combinations using `1 regularization and grafting (Perkins et al., 2003). Although we share similar ideas, their grafting algorithm starts with an empty feature set whereas ours starts with low dimensional features, and their correlation parameters αi,y are pre-computed whereas ours are dynamically determined. Strubell et al. (2015) suggested an algorithm that dynamically selected strong features during decoding. Our work is distinguished because we do not run multiple training phases as they do for figuring our strong features. 3 Dynamic Feature Induction The intuition behind dynamic feature induction is to keep populating high dimensional features by joining low dimensional features together until the feature space becomes ‘more’ linearly separable.2 Figure 1 shows how features are induced during training: 1. Given a training instance (x1 , y1 ), where x1 is a feature set consisting of 5 features and y1 is the gold lab"
N16-1031,P08-1076,0,0.0158099,"Missing"
N16-1031,P13-2004,0,0.0507114,"Missing"
N16-1031,W03-0419,0,0.0565422,"Missing"
N16-1031,N03-1033,0,0.0538967,"993) with the standard split for part-of-speech tagging experiments. Set TRN DEV TST Sections 0-18 19-21 22-24 Sentences 38,219 5,527 5,462 ALL 912,344 131,768 129,654 OOV 0 4,467 3,649 Table 1: Distributions of the Wall Street Journal corpus. TRN: training, DEV: development, TST: evaluation, ALL: all words, OOV: out-of-vocabulary words. 4.2 Tagging and Learning Algorithms A one-pass, left-to-right tagging algorithm is used for our experiments. Such a simple algorithm is chosen because we want to see the performance gain purely from our approach, not by a more sophisticated tagging algorithm (Toutanova et al., 2003; Shen et al., 2007), which may improve the performance further. For learning, the final algorithm from Section 3 is used. Additionally, mini-batch is applied, where each batch consists of training instances from k-number of sentences, causing the sizes of these batches different. We found that grouping instances with respect to the sentence boundary was more effective than batching them across arbitrary sentences. For all our experiments, the learning rate η = 0.02 and the mini-batch boundary k = 5 were used without tuning. 4.3 Ambiguity Classes The ambiguity class of a word is the concatenat"
N16-1031,D14-1101,0,0.0398926,"Missing"
N16-1031,P10-1040,0,0.0531724,"Missing"
N16-1031,W04-3212,0,0.0128124,"and the process of optimizing combinations between existing features. Discovering novel features may require linguistic background as well as good understanding in machine learning such that it is often difficult to do. Optimizing feature combinations can be also difficult but usually requires less domain knowledge and more importantly, it can be as effective as discovering new features. It has been shown for many tasks that approaches using simple machine learning with extensive feature engineering outperform ones using more advanced machine learning with less intensive feature engineering (Xue and Palmer, 2004; Bengtson and Roth, 2008; Ratinov and Roth, 2009; Zhang and Nivre, 2011). Recently, people have tried to automate the second part of feature engineering, the optimization of feature combinations, through leading-edge models such as neural networks (Collobert et al., 2011). Coupled with embedding approaches (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks (Socher et al., 2013;"
N16-1031,W03-0434,0,0.0179641,"NLP task using sparse features. Dynamic feature induction allows humans to focus on the first part of feature engineering, the discovery of novel features, while machines handle the second part. Our approach was experimented with two core NLP tasks, part-of-speech tagging (Section 4) and named entity recognition (Section 5) and showed the state-of-the-art results for both tasks. 2 2.1 Background Nonlinearity in NLP Linear classification algorithms such as Perceptron, Winnow, or Support Vector Machines with a linear kernel have performed exceptionally well for various NLP tasks (Collins, 2002; Zhang and Johnson, 2003; Pradhan et al., 2005). This is not because our feature space is linearly separable by nature, but sparse fea271 Proceedings of NAACL-HLT 2016, pages 271–281, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Figure 1: Overview of dynamic feature induction. tures introduced to NLP yield very high dimensional vector space such that it is rather forced to be linearly separable. For example, NLP features for a word wi typically involve the word forms of wi−1 and wi (e.g., fi−1 , fi ). If the feature space is not linearly separable with these features, a co"
N16-1031,P11-2033,0,0.0115215,"Discovering novel features may require linguistic background as well as good understanding in machine learning such that it is often difficult to do. Optimizing feature combinations can be also difficult but usually requires less domain knowledge and more importantly, it can be as effective as discovering new features. It has been shown for many tasks that approaches using simple machine learning with extensive feature engineering outperform ones using more advanced machine learning with less intensive feature engineering (Xue and Palmer, 2004; Bengtson and Roth, 2008; Ratinov and Roth, 2009; Zhang and Nivre, 2011). Recently, people have tried to automate the second part of feature engineering, the optimization of feature combinations, through leading-edge models such as neural networks (Collobert et al., 2011). Coupled with embedding approaches (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks (Socher et al., 2013; Devlin et al., 2014; Yu et al., 2014). However, neural networks are not a"
N18-1185,P16-1223,0,0.252374,"Sentence Sentence Passage Generation Scene Passage Figure 1: The overview of passage generation. Each episode is split into scenes, and each summary is segmented to sentences. Elasticsearch passes the scene-sentence pairs to crowd workers who are asked to check the relevancy, replace all pronouns with the corresponding names, and generate new passages for the scenes (Section 3.1). 2 Related Work 3 Hermann et al. (2015) introduced the CNN/Daily Mail dataset where documents and passages were news articles and their summaries respectively, and evaluated neural models with three types of readers. Chen et al. (2016) proposed the entity centric model and the bidirectional LSTM model using attention, and conducted a thorough analysis on this dataset. Trischler et al. (2016) presented the EpiReader that combined a reasoner with an extractor for encoding documents and passages using both CNN and RNN. Dhingra et al. (2017) proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages. At last, Cui et al. (2017) introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention. Hill et al. ("
N18-1185,W16-3612,1,0.809795,"iends from the fan sites suggested by Jurczyk and Choi (2017), generate passages for each dialog using the plot summaries and crowdsourced descriptions (Section 3.1), then annotate mentions of all characters in both the dialogs and the passages for passage completion (Section 3.2). 3.1 Passage Generation An episode consists of multiple scenes, which may or may not be coherent. In our corpus, each scene is considered a separate dialog. The lengths of the scenes vary from 1 to 256 utterances; we select only scenes whose lengths are between 5 and 25 utterances as suggested by the previous works (Chen and Choi, 2016; Jurczyk and Choi, 2017), which notably improves the readability for crowd workers, resulting higher quality annotation. The plot summaries collected from the fan sites are associated with episodes, not scenes. To break down the episode-level summaries into scene-level, they are segmented into sentences by the tokenizer in NLP4J.2 Each sentence in the plot summaries 1 2 2040 nlp.mathcs.emory.edu/character-mining https://github.com/emorynlp/nlp4j (a) A dialog from Friends: Season 8, Episode 12, Scene 2. ID 1 2 3 4 5 6 7 8 9 10 11 12 Speaker @ent03 @ent01 @ent03 @ent02 @ent03 @ent01 @ent02 @ent"
N18-1185,K17-1023,1,0.79123,"uAD (Rajpurkar et al., 2016). Unlike the other corpora where documents and passages are written in a similar writing style, they are multiparty dialogs and plot summaries in our corpus, which have very different writing styles. This raises another level of difficulty to match contexts between documents and queries for the task of passage completion. Corpus The Character Mining project provides transcripts of the TV show Friends for ten seasons in the JSON format.1 Each season contains ≈24 episodes, each episode is split into ≈13 scenes, where each scene comprises a sequence of ≈21 utterances. Chen et al. (2017) annotated the first two seasons of the show for an entity linking task, where personal mentions (e.g., she, mom, Rachel) were identified by their corresponding characters. Jurczyk and Choi (2017) collected plot summaries of all episodes for the first eight seasons to evaluate a document retrieval task that returned a ranked list of relevant documents given any sentence in the plot summaries. For the creation of our corpus, we collect more plot summaries for the last two seasons of Friends from the fan sites suggested by Jurczyk and Choi (2017), generate passages for each dialog using the plot"
N18-1185,P16-1046,0,0.0351155,"racter entities. Type # of dialogs # of passages # of queries Avg. # of utterances per dialog Avg. # of tokens per dialog/passage Avg. # of mentions per dialog/passage Avg. # of entities per dialog/passage Max # of mentions per dialog/passage Max # of entities per dialog/passage Approach This section presents our deep learning architecture that integrates rich feature extraction from convolutional neural networks (CNN) into robust sequence modeling in recurrent neural networks (RNN) (Section 4.1). The combination of CNN and RNN has been adapted by several NLP tasks such as text summarization (Cheng and Lapata, 2016), essay scoring (Dong et al., 2017), sentiment analysis (Wang et al., 2016), or even reading comprehension (Dhingra et al., 2017). Unlike previous works that feed a sequence of sentences encoded by CNN to RNN, a sequence of utterances is encoded by CNN in our model, where each utterance is spoken by a distinct speaker and contains one or more sentences that are coherent in topics. Our best model is optimized by both the utterance (Section 4.2) and the dialog (Section 4.3) level attentions, showing significant improvement over the pure CNN+RNN model. 4.1 CNN + LSTM Each utterance comes with a s"
N18-1185,N16-1031,1,0.909896,"m the fan sites suggested by Jurczyk and Choi (2017), generate passages for each dialog using the plot summaries and crowdsourced descriptions (Section 3.1), then annotate mentions of all characters in both the dialogs and the passages for passage completion (Section 3.2). 3.1 Passage Generation An episode consists of multiple scenes, which may or may not be coherent. In our corpus, each scene is considered a separate dialog. The lengths of the scenes vary from 1 to 256 utterances; we select only scenes whose lengths are between 5 and 25 utterances as suggested by the previous works (Chen and Choi, 2016; Jurczyk and Choi, 2017), which notably improves the readability for crowd workers, resulting higher quality annotation. The plot summaries collected from the fan sites are associated with episodes, not scenes. To break down the episode-level summaries into scene-level, they are segmented into sentences by the tokenizer in NLP4J.2 Each sentence in the plot summaries 1 2 2040 nlp.mathcs.emory.edu/character-mining https://github.com/emorynlp/nlp4j (a) A dialog from Friends: Season 8, Episode 12, Scene 2. ID 1 2 3 4 5 6 7 8 9 10 11 12 Speaker @ent03 @ent01 @ent03 @ent02 @ent03 @ent01 @ent02 @ent"
N18-1185,P17-1055,0,0.149787,"ere documents and passages were news articles and their summaries respectively, and evaluated neural models with three types of readers. Chen et al. (2016) proposed the entity centric model and the bidirectional LSTM model using attention, and conducted a thorough analysis on this dataset. Trischler et al. (2016) presented the EpiReader that combined a reasoner with an extractor for encoding documents and passages using both CNN and RNN. Dhingra et al. (2017) proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages. At last, Cui et al. (2017) introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention. Hill et al. (2016) released the Children Book Test dataset where documents were children’s book stories and passages were excerpts from those stories. Paperno et al. (2016) introduced the LAMBADA dataset comprising novels from the Book corpus. Onishi et al. (2016) introduced the Who-did-What dataset consisting of articles from the LDC English Gigaword newswire corpus. All corpora described above provide queries, that are passages where certain words are masked by blanks,"
N18-1185,P17-1168,0,0.173066,"e corresponding names, and generate new passages for the scenes (Section 3.1). 2 Related Work 3 Hermann et al. (2015) introduced the CNN/Daily Mail dataset where documents and passages were news articles and their summaries respectively, and evaluated neural models with three types of readers. Chen et al. (2016) proposed the entity centric model and the bidirectional LSTM model using attention, and conducted a thorough analysis on this dataset. Trischler et al. (2016) presented the EpiReader that combined a reasoner with an extractor for encoding documents and passages using both CNN and RNN. Dhingra et al. (2017) proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages. At last, Cui et al. (2017) introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention. Hill et al. (2016) released the Children Book Test dataset where documents were children’s book stories and passages were excerpts from those stories. Paperno et al. (2016) introduced the LAMBADA dataset comprising novels from the Book corpus. Onishi et al. (2016) introduced the Who-did-What dataset consisting of articl"
N18-1185,K17-1017,0,0.0232679,"passages # of queries Avg. # of utterances per dialog Avg. # of tokens per dialog/passage Avg. # of mentions per dialog/passage Avg. # of entities per dialog/passage Max # of mentions per dialog/passage Max # of entities per dialog/passage Approach This section presents our deep learning architecture that integrates rich feature extraction from convolutional neural networks (CNN) into robust sequence modeling in recurrent neural networks (RNN) (Section 4.1). The combination of CNN and RNN has been adapted by several NLP tasks such as text summarization (Cheng and Lapata, 2016), essay scoring (Dong et al., 2017), sentiment analysis (Wang et al., 2016), or even reading comprehension (Dhingra et al., 2017). Unlike previous works that feed a sequence of sentences encoded by CNN to RNN, a sequence of utterances is encoded by CNN in our model, where each utterance is spoken by a distinct speaker and contains one or more sentences that are coherent in topics. Our best model is optimized by both the utterance (Section 4.2) and the dialog (Section 4.3) level attentions, showing significant improvement over the pure CNN+RNN model. 4.1 CNN + LSTM Each utterance comes with a speaker label encoded by the entity"
N18-1185,P17-1147,0,0.0418962,"ook stories and passages were excerpts from those stories. Paperno et al. (2016) introduced the LAMBADA dataset comprising novels from the Book corpus. Onishi et al. (2016) introduced the Who-did-What dataset consisting of articles from the LDC English Gigaword newswire corpus. All corpora described above provide queries, that are passages where certain words are masked by blanks, for the evaluation of passage completion. More datasets are available for another type of a reading comprehension task, that is multiple choice question answering, such as MCTest (Richardson et al., 2013), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016). Unlike the other corpora where documents and passages are written in a similar writing style, they are multiparty dialogs and plot summaries in our corpus, which have very different writing styles. This raises another level of difficulty to match contexts between documents and queries for the task of passage completion. Corpus The Character Mining project provides transcripts of the TV show Friends for ten seasons in the JSON format.1 Each season contains ≈24 episodes, each episode is split into ≈13 scenes, where each scene compris"
N18-1185,W17-5407,1,0.806388,"have very different writing styles. This raises another level of difficulty to match contexts between documents and queries for the task of passage completion. Corpus The Character Mining project provides transcripts of the TV show Friends for ten seasons in the JSON format.1 Each season contains ≈24 episodes, each episode is split into ≈13 scenes, where each scene comprises a sequence of ≈21 utterances. Chen et al. (2017) annotated the first two seasons of the show for an entity linking task, where personal mentions (e.g., she, mom, Rachel) were identified by their corresponding characters. Jurczyk and Choi (2017) collected plot summaries of all episodes for the first eight seasons to evaluate a document retrieval task that returned a ranked list of relevant documents given any sentence in the plot summaries. For the creation of our corpus, we collect more plot summaries for the last two seasons of Friends from the fan sites suggested by Jurczyk and Choi (2017), generate passages for each dialog using the plot summaries and crowdsourced descriptions (Section 3.1), then annotate mentions of all characters in both the dialogs and the passages for passage completion (Section 3.2). 3.1 Passage Generation A"
N18-1185,D16-1241,0,0.057448,"encoding documents and passages using both CNN and RNN. Dhingra et al. (2017) proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages. At last, Cui et al. (2017) introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention. Hill et al. (2016) released the Children Book Test dataset where documents were children’s book stories and passages were excerpts from those stories. Paperno et al. (2016) introduced the LAMBADA dataset comprising novels from the Book corpus. Onishi et al. (2016) introduced the Who-did-What dataset consisting of articles from the LDC English Gigaword newswire corpus. All corpora described above provide queries, that are passages where certain words are masked by blanks, for the evaluation of passage completion. More datasets are available for another type of a reading comprehension task, that is multiple choice question answering, such as MCTest (Richardson et al., 2013), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016). Unlike the other corpora where documents and passages are written in a similar writing sty"
N18-1185,D16-1013,0,0.0137687,"gmented to sentences. Elasticsearch passes the scene-sentence pairs to crowd workers who are asked to check the relevancy, replace all pronouns with the corresponding names, and generate new passages for the scenes (Section 3.1). 2 Related Work 3 Hermann et al. (2015) introduced the CNN/Daily Mail dataset where documents and passages were news articles and their summaries respectively, and evaluated neural models with three types of readers. Chen et al. (2016) proposed the entity centric model and the bidirectional LSTM model using attention, and conducted a thorough analysis on this dataset. Trischler et al. (2016) presented the EpiReader that combined a reasoner with an extractor for encoding documents and passages using both CNN and RNN. Dhingra et al. (2017) proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages. At last, Cui et al. (2017) introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention. Hill et al. (2016) released the Children Book Test dataset where documents were children’s book stories and passages were excerpts from those stories. Paperno et al. (2016)"
N18-1185,P16-2037,0,0.016235,"es per dialog Avg. # of tokens per dialog/passage Avg. # of mentions per dialog/passage Avg. # of entities per dialog/passage Max # of mentions per dialog/passage Max # of entities per dialog/passage Approach This section presents our deep learning architecture that integrates rich feature extraction from convolutional neural networks (CNN) into robust sequence modeling in recurrent neural networks (RNN) (Section 4.1). The combination of CNN and RNN has been adapted by several NLP tasks such as text summarization (Cheng and Lapata, 2016), essay scoring (Dong et al., 2017), sentiment analysis (Wang et al., 2016), or even reading comprehension (Dhingra et al., 2017). Unlike previous works that feed a sequence of sentences encoded by CNN to RNN, a sequence of utterances is encoded by CNN in our model, where each utterance is spoken by a distinct speaker and contains one or more sentences that are coherent in topics. Our best model is optimized by both the utterance (Section 4.2) and the dialog (Section 4.3) level attentions, showing significant improvement over the pure CNN+RNN model. 4.1 CNN + LSTM Each utterance comes with a speaker label encoded by the entity ID in our corpus (Table 1). This entity"
N18-1185,Q16-1019,0,0.0601491,"Missing"
N18-1185,P16-1144,0,0.0265903,"ischler et al. (2016) presented the EpiReader that combined a reasoner with an extractor for encoding documents and passages using both CNN and RNN. Dhingra et al. (2017) proposed the gated-attention reader that incorporated attention on multiplicative interactions between documents and passages. At last, Cui et al. (2017) introduced the attention-overattention reader that placed document-to-passage attention over passage-to-document attention. Hill et al. (2016) released the Children Book Test dataset where documents were children’s book stories and passages were excerpts from those stories. Paperno et al. (2016) introduced the LAMBADA dataset comprising novels from the Book corpus. Onishi et al. (2016) introduced the Who-did-What dataset consisting of articles from the LDC English Gigaword newswire corpus. All corpora described above provide queries, that are passages where certain words are masked by blanks, for the evaluation of passage completion. More datasets are available for another type of a reading comprehension task, that is multiple choice question answering, such as MCTest (Richardson et al., 2013), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016)"
N18-1185,D14-1162,0,0.0797687,"Missing"
N18-1185,D16-1264,0,0.0748838,". Paperno et al. (2016) introduced the LAMBADA dataset comprising novels from the Book corpus. Onishi et al. (2016) introduced the Who-did-What dataset consisting of articles from the LDC English Gigaword newswire corpus. All corpora described above provide queries, that are passages where certain words are masked by blanks, for the evaluation of passage completion. More datasets are available for another type of a reading comprehension task, that is multiple choice question answering, such as MCTest (Richardson et al., 2013), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016). Unlike the other corpora where documents and passages are written in a similar writing style, they are multiparty dialogs and plot summaries in our corpus, which have very different writing styles. This raises another level of difficulty to match contexts between documents and queries for the task of passage completion. Corpus The Character Mining project provides transcripts of the TV show Friends for ten seasons in the JSON format.1 Each season contains ≈24 episodes, each episode is split into ≈13 scenes, where each scene comprises a sequence of ≈21 utterances. Chen et al. (2017) annotated"
P11-2121,W09-1210,0,0.00760678,"nglish L AS UAS 88.54 90.57 88.62 90.66 89.15∗ 91.18∗ 88.79 (3) 89.88 (1) - Czech L AS UAS 78.12 83.29 78.30 83.47 80.24∗ 85.24∗ 80.38 (1) 80.11 (2) - Table 3: Accuracy comparisons between different parsing approaches (L AS/UAS: labeled/unlabeled attachment score). ∗ indicates a statistically significant improvement. (#) indicates an overall rank of the system in CoNLL’09. Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Parsing speed (in ms) Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open source project, called ClearParser (code. google.com/p/clearparser). Note that features used in MaltParser have not"
P11-2121,C10-1011,0,0.0114405,"earning models are excluded because they are independent from the parsing algorithms. The average parsing speeds are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre, CN, and Our+, respectively. Our approach shows linear growth all along, even for the sentence groups where some approaches start showing curves. Nivre 22 18 14 CN 10 6 Our+ 2 0 10 20 30 40 50 60 70 Sentence length Figure 1: Average parsing speeds with respect to sentence groups in Table 4. 3 Later, ‘Merlo’ and ‘Bohnet” introduced more advanced systems, showing some improvements over their previous approaches (Titov et al., 2009; Bohnet, 2010). 691 < 10 1,415 < 20 2,289 < 30 1,714 < 40 815 < 50 285 < 60 72 < 70 18 Table 4: # of sentences in each group, extracted from both English/Czech evaluation sets. ‘< n’ implies a group containing sentences whose lengths are less than n. We also measured average parsing speeds for ‘Our’, which showed a very similar growth to ‘Our+’. The average parsing speed of ‘Our’ was 2.20 ms; it performed slightly faster than ‘Our+’ because it skipped more nodes by performing more non-deterministic SHIFT’s, which may or may not have been correct decisions for the corresponding parsing states. It is worth me"
P11-2121,D07-1101,0,0.007977,"non-projective parsing by selecting LEFT-POP or LEFT-ARC , respectively. Our experiments show that this additional transition improves both parsing accuracy and speed. The advantage derives from improving the efficiency of the choice mechanism; it is now simply a transition choice and requires no additional processing. 3 Bootstrapping automatic parses tially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using goldTransition-based parsing has"
P11-2121,cer-etal-2010-parsing,0,0.0356787,"Missing"
P11-2121,W10-3110,0,0.00549945,"g as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because"
P11-2121,W09-1205,0,0.00769796,"is even more significant in a language like Czech for which parsers generally perform more poorly. CN Our Our+ Merlo Bohnet English L AS UAS 88.54 90.57 88.62 90.66 89.15∗ 91.18∗ 88.79 (3) 89.88 (1) - Czech L AS UAS 78.12 83.29 78.30 83.47 80.24∗ 85.24∗ 80.38 (1) 80.11 (2) - Table 3: Accuracy comparisons between different parsing approaches (L AS/UAS: labeled/unlabeled attachment score). ∗ indicates a statistically significant improvement. (#) indicates an overall rank of the system in CoNLL’09. Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Parsing speed (in ms) Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open"
P11-2121,P10-1001,0,0.016745,"arsing by selecting LEFT-POP or LEFT-ARC , respectively. Our experiments show that this additional transition improves both parsing accuracy and speed. The advantage derives from improving the efficiency of the choice mechanism; it is now simply a transition choice and requires no additional processing. 3 Bootstrapping automatic parses tially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using goldTransition-based parsing has the advantage of using p"
P11-2121,H05-1066,0,0.152438,"Missing"
P11-2121,P08-1108,0,0.0813248,"n discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 Martha Palme"
P11-2121,P05-1013,0,0.0808087,"that this implementation reduces the parsing complexity from O(n2 ) to linear time in practice (a worst-case complexity is O(n2 )). We suggest another transition-based parsing approach that reduces the search space even more. The idea is to merge transitions in Choi-Nicolov’s non-projective algorithm with transitions in Nivre’s projective algorithm (Nivre, 2003). Nivre’s projective algorithm has a worst-case complexity of O(n), which is faster than any non-projective parsing algorithm. Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. Ideally, it is better to perform projective parsing for most cases and perform non-projective parsing only when it is needed. In this algorithm, we add another transition to Choi-Nicolov’s approach, LEFT-POP , similar to the L EFT-A RC transition in Nivre’s projective algorithm. By adding this transition, an oracle can now choose either projective or non-projective parsing depending on parsing states.1 1 We also tried adding the R IGHT-A RC transition from Nivre’s projective algorithm, which did not improve parsing performa"
P11-2121,W03-3017,0,0.604553,"t improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 Martha Palmer Department of Linguistics University of Colorado at Boulder mpalmer@colorado.edu approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n)"
P11-2121,J08-4003,0,0.587147,"a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 Martha Palmer Department of Linguistics University of Colorado at Boulder mpalmer@colorado.edu approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2 ) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use p"
P11-2121,P09-1040,0,0.47951,"ally performs noticeably faster than the graph-based 687 Martha Palmer Department of Linguistics University of Colorado at Boulder mpalmer@colorado.edu approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2 ) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use parse history as features to make the next prediction. This parse information helps to improve parsing accuracy without hurting parsing complexity (Nivre, 2006). Most current transition-based approaches use gold-standard parses as features during training; however, this is not necessarily w"
P11-2121,P08-1066,0,0.0108105,"n-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn"
P11-2121,D08-1059,0,0.134199,"nsitive. Furthermore, S EARN interpolates the current policy with the previous policy whereas we do not perform such interpolation. During decoding, S EARN generates a sequence of decisions and makes a final prediction. In our case, the decisions are predicted dependency relations and the final prediction is a dependency tree. S EARN has been successfully adapted to several NLP tasks such as named entity recognition, syntactic chunking, and POS tagging. To the best of our knowledge, this is the first time that this idea has been applied to transition-based parsing and shown promising results. Zhang and Clark (2008) suggested a transitionbased projective parsing algorithm that keeps B different sequences of parsing states and chooses the 690 one with the best score. They use beam search and show a worst-case parsing complexity of O(n) given a fixed beam size. Similarly to ours, their learning mechanism using the structured perceptron algorithm involves training on automatically derived parsing states that closely resemble potential states encountered during decoding. 5 Experiments 5.1 Corpora and learning algorithm All models are trained and tested on English and Czech data using automatic lemmas, POS ta"
P11-2121,W09-1201,0,\N,Missing
P12-2071,P07-1033,0,0.185376,"Missing"
P12-2071,gimenez-marquez-2004-svmtool,0,0.533414,"Missing"
P12-2071,P07-1096,0,0.0307625,"Missing"
P12-2071,P11-2009,0,0.235986,"Missing"
P12-2071,E09-1087,0,0.0578867,"Missing"
P12-2071,N03-1033,0,0.436616,"p: POS, a: ambiguity class, c∗ : character sequence in wi (e.g., c:2 : the 1st and 2nd characters of wi , cn−1: : the n-1’th and n’th characters of wi ). See Gim´enez and M`arquez (2004) for more details. 2.4 Machine learning Liblinear L2-regularization, L1-loss support vector classification is used for our experiments (Hsieh et al., 2008). From several rounds of cross-validation, learning parameters of (c = 0.2, e = 0.1, B = 0.4) and (c = 0.1, e = 0.1, B = 0.9) are found for the generalized and domain-specific models, respectively (c: cost, e: termination criterion, B: bias). 3 Related work Toutanova et al. (2003) introduced a POS tagging algorithm using bidirectional dependency networks, and showed the best contemporary results. Gim´enez and M`arquez (2004) used one-pass, left-to-right and right-to-left combined tagging algorithm and achieved near state-of-the-art results. Shen et al. 365 (2007) presented a tagging approach using guided learning for bidirectional sequence classification and showed current state-of-the-art results.3 Our individual models (generalized and domainspecific) are similar to Gim´enez and M`arquez (2004) in that we use a subset of their features and take onepass, left-to-right"
P13-1104,D12-1133,0,0.528635,"ter w4 and w6 are compared, R IGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in σ (state 10). After w6 and w7 are compared, w6 is popped out of σ (state 12) because it is not needed for later parsing states. 3 3.1 Selectional branching Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is the maximum number of transitions performed t"
P13-1104,W06-2920,0,0.427147,"Missing"
P13-1104,W08-2102,0,0.0500522,"Missing"
P13-1104,cer-etal-2010-parsing,0,0.0588043,"Missing"
P13-1104,P11-2121,1,0.8603,"ies as low as O(n) and O(n2 ) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohne"
P13-1104,P12-2071,1,0.352871,"ith our new hybrid parsing algorithm, A DAG RAD, rich non-local features, and bootstrapping, our parser gives higher parsing accuracy than most other transition-based dependency parsers in multiple languages and shows faster parsing speed. It is interesting to see that our greedy parser outperformed most other greedy dependency parsers. This is because our parser used both bootstrapping and Zhang and Nivre (2011)’s non-local features, which had not been used by other greedy parsers. In the future, we will experiment with more advanced dependency representations (de Marneffe and Manning, 2008; Choi and Palmer, 2012b) to show robustness of our approach. Furthermore, we will evaluate individual methods of our approach separately to show impact of each method on parsing performance. We also plan to implement the typical beam search approach to make a direct comparison to our selectional branching.8 Acknowledgments Special thanks are due to Luke Vilnis of the University of Massachusetts Amherst for insights on 8 Our parser is publicly available under an open source project, ClearNLP (clearnlp.googlecode.com). the A DAG RAD derivation. We gratefully acknowledge a grant from the Defense Advanced Research Proj"
P13-1104,W02-1001,0,0.240565,"Missing"
P13-1104,W08-1301,0,0.0264151,"Missing"
P13-1104,D12-1019,0,0.00958663,"ined with b = 80 and m = 0.88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44"
P13-1104,D12-1029,0,0.0246076,"Missing"
P13-1104,N10-1115,0,0.0497612,"Missing"
P13-1104,C12-1059,0,0.0658165,"evelopment set (lines 2-4). Then, the next model Mr is trained on all data but this time, Mr−1 is used to generate multiple transition sequences (line 7-8). Among all transition sequences generated by Mr−1 , training instances from only T1 and Tg are used to train Mr , where T1 is the one-best sequence and Tg is a sequence giving the most accurate parse output compared to the gold-standard tree. The score of Mr is measured (line 9), and repeat the procedure if Sr−1 &lt; Sr ; otherwise, return the previous model Mr−1 . 5 Adaptive subgradient algorithm Alternatively, the dynamic oracle approach of Goldberg and Nivre (2012) can be used to generate multiple transition sequences, which is expected to show similar results. y = y0 otherwise The algorithm takes three hyper-parameters; T is the number of iterations, α is the learning rate, and ρ is the ridge (T &gt; 0, α &gt; 0, ρ ≥ 0). G is our running estimate of a diagonal covariance matrix for the gradients (per-coordinate learning rates). For each instance, scores for all labels are measured by the logistic regression function f (x, y) in Section 3.3. These scores are subtracted from an output of the indicator function I(y, y 0 ), which forces our model to keep learnin"
P13-1104,P06-2041,0,0.0609051,"Missing"
P13-1104,P08-1068,0,0.0798175,"Missing"
P13-1104,J93-2004,0,0.0428576,"Missing"
P13-1104,D10-1004,0,0.036365,"Missing"
P13-1104,E06-1011,0,0.0953577,"Missing"
P13-1104,P05-1012,0,0.423524,"Missing"
P13-1104,W06-2932,0,0.010059,"012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44 83.17 75.2 75.94 85.53 75.66 83.29 77.46 84.65 Swedish LAS UAS 84.58 89.50 82.55 88.93 83.55 89.30 84.66 89.80 86.32 91.12 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. 4.5 Non-pro"
P13-1104,P08-1108,0,0.0104809,"cy parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44 83.17 75.2 75.94 85.53 75.66 83.29 77.46 84.65 Swedish LAS UAS 84.58 89.50 82.55 88.93 83.55 89.30 84.66 89.80 86.32 91.12 86.80 91.36 Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation. 4.5 Non-projective parsing experiments Table 5 shows comparison between state-of-the-a"
P13-1104,P10-1110,0,0.705806,"state 5) so it can be compared to other tokens in β (state 10). After w4 and w6 are compared, R IGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in σ (state 10). After w6 and w7 are compared, w6 is popped out of σ (state 12) because it is not needed for later parsing states. 3 3.1 Selectional branching Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e t"
P13-1104,P05-1013,0,0.0815568,"on Transition-based dependency parsing has gained considerable interest because it runs fast and performs accurately. Transition-based parsing gives complexities as low as O(n) and O(n2 ) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-a"
P13-1104,N12-1015,0,0.027483,"Missing"
P13-1104,W06-2933,0,0.052667,"Missing"
P13-1104,P10-1001,0,0.18735,"s for IO, feature extraction and bootstrapping. pendency parsers using beam search. The second block shows results from other kinds of parsing approaches (e.g., graph-based parsing, ensemble parsing, linear programming, dual decomposition). The third block shows results from parsers using external data. The last block shows results from our approach. The Time column show how many seconds per sentence each parser takes.7 Approach Zhang and Clark (2008) Huang and Sagae (2010) Zhang and Nivre (2011) Bohnet and Nivre (2012) McDonald et al. (2005) Mcdonald and Pereira (2006) Sagae and Lavie (2006) Koo and Collins (2010) Zhang and McDonald (2012) Martins et al. (2010) Rush et al. (2010) Koo et al. (2008) Carreras et al. (2008) Bohnet and Nivre (2012) Suzuki et al. (2009) bt = 80, bd = 80, m = 0.88 bt = 80, bd = 64, m = 0.88 bt = 80, bd = 32, m = 0.88 bt = 80, bd = 16, m = 0.88 bt = 80, bd = 8, m = 0.88 bt = 80, bd = 4, m = 0.88 bt = 80, bd = 2, m = 0.88 bt = 80, bd = 1, m = 0.88 bt = 1, bd = 1, m = 0.88 1,200,000 Transitions 1,000,000 800,000 600,000 400,000 200,000 0 0 10 20 30 40 50 60 70 80 Beam size = 1, 2, 4, 8, 16, 32, 64, 80 Figure 5: The total number of transitions performed during decoding with respe"
P13-1104,W03-3017,0,0.196237,"sent a new transition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. We then introduce selectional branching that uses confidence estimates to decide when to employ a beam. With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed. 2 Transition-based dependency parsing We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre’s arceager and list-based algorithms (Nivre, 2003; Nivre, 2008). Nivre’s arc-eager is a projective parsing algorithm showing a complexity of O(n). Nivre’s list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2 ). Table 1 shows transitions in our algorithm. The top 4 and 1052 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Transition Current state L EFTl -R EDUCE l ( [σ|i], δ, [j|β], A ) ⇒ ( σ, δ, [j|β], A ∪ {i ← j} ) l ( [σ|i], δ, [j|β], A ) ⇒ ( [σ|i|δ|j], [ ], β, A ∪ {i"
P13-1104,W04-0308,0,0.162124,"hoi and Palmer (2011) who integrated our L EFT-R EDUCE transition into Nivre’s list-based algorithm. Our algorithm is distinguished from theirs because ours gives different parsing complexities of O(n) and O(n2 ) for projective and non-projective parsing, respectively, whereas their algorithm gives O(n2 ) 1059 for both cases; this is possible because of the new integration of the R IGHT-S HIFT and N O -R EDUCE transitions. There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a S WAP transition into Nivre’s arc-standard algorithm (Nivre, 2004) and Fernández-González and Gómez-Rodríguez (2012) integrated a buffer transition into Nivre’s arc-eager algorithm to handle non-projectivity. Our selectional branching method is most relevant to Zhang and Clark (2008) who introduced a transition-based dependency parsing model that uses beam search. Huang and Sagae (2010) later applied dynamic programming to this approach and showed improved efficiency. Zhang and Nivre (2011) added non-local features to this approach and showed improved parsing accuracy. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS ta"
P13-1104,P11-2033,0,0.696046,"ompared to other tokens in β (state 10). After w4 and w6 are compared, R IGHT-PASS is performed (state 9) because there is a dependency between w6 and w2 in σ (state 10). After w6 and w7 are compared, w6 is popped out of σ (state 12) because it is not needed for later parsing states. 3 3.1 Selectional branching Motivation For transition-based parsing, state-of-the-art accuracies have been achieved by parsers optimized on multiple transition sequences using beam search, which can be done very efficiently when it is coupled with dynamic programming (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Huang et al., 2012; Bohnet and Nivre, 2012). Despite all the benefits, there is one downside of this approach; it generates a fixed number of transition sequences no matter how confident the onebest sequence is.3 If every prediction leading to the one-best sequence is confident, it may not be necessary to explore more sequences to get the best output. Thus, it is preferred if the beam size is not fixed but proportional to the number of low confidence predictions made for the one-best sequence. The selectional branching method presented here performs at most d · t − e transitions, where t is"
P13-1104,J08-4003,0,0.693883,"ansition-based parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. We then introduce selectional branching that uses confidence estimates to decide when to employ a beam. With our new approach, we achieve a higher parsing accuracy than the current state-of-the-art transition-based parser that uses beam search and a much faster speed. 2 Transition-based dependency parsing We introduce a transition-based dependency parsing algorithm that is a hybrid between Nivre’s arceager and list-based algorithms (Nivre, 2003; Nivre, 2008). Nivre’s arc-eager is a projective parsing algorithm showing a complexity of O(n). Nivre’s list-based algorithm is a non-projective parsing algorithm showing a complexity of O(n2 ). Table 1 shows transitions in our algorithm. The top 4 and 1052 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052–1062, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Transition Current state L EFTl -R EDUCE l ( [σ|i], δ, [j|β], A ) ⇒ ( σ, δ, [j|β], A ∪ {i ← j} ) l ( [σ|i], δ, [j|β], A ) ⇒ ( [σ|i|δ|j], [ ], β, A ∪ {i → j} ) ( [σ|i"
P13-1104,P09-1040,0,0.302801,"ves complexities as low as O(n) and O(n2 ) for projective and non-projective parsing, respectively (Nivre, 2008).1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller (Nivre and Nilsson, 2005) so a parser can assume projectivity for most cases while recognizing ones for which projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhan"
P13-1104,N12-1054,0,0.0163317,".88, which is the best setting found during development. Our parser shows higher accuracy than Zhang and Nivre (2011), which is the current state-of-the-art transition-based parser that uses beam search. Bohnet and Nivre (2012)’s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Our parser gives a comparative accuracy to Koo and Collins (2010) that is a 3rdorder graph-based parsing approach. In terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7 Dhillon et al. (2012) and Rush and Petrov (2012) also have shown good results on this data but they are excluded from our comparison because they use different kinds of constituent-to-dependency conversion methods. 1058 Approach Nivre et al. (2006) McDonald et al. (2006) Nivre (2009) F.-González and G.-Rodríguez (2012) Nivre and McDonald (2008) Martins et al. (2010) bt = 80, bd = 1, m = 0.88 bt = 80, bd = 80, m = 0.88 Danish LAS UAS 84.77 89.80 84.79 90.58 84.2 85.17 90.10 86.67 91.50 86.75 91.04 87.27 91.36 Dutch LAS UAS 78.59 81.35 79.19 83.57 81.63 84.91 80.75 83.59 82.45 85.33 Slovene LAS UAS 70.30 78.72 73.44 83.17 75.2 75.94 85.53 75."
P13-1104,D10-1001,0,0.051107,"Missing"
P13-1104,N06-2033,0,0.100696,"Missing"
P13-1104,D09-1058,0,0.0337446,"Missing"
P13-1104,W03-3023,0,0.51149,"Missing"
P13-1104,D08-1059,0,0.706933,"ch projectivity should not be assumed (Nivre, 2009; Choi and Palmer, 2011). 1 We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing. Andrew McCallum Department of Computer Science University of Massachusetts Amherst Amherst, MA, 01003, USA mccallum@cs.umass.edu Greedy transition-based dependency parsing has been widely deployed because of its speed (Cer et al., 2010); however, state-of-the-art accuracies have been achieved by globally optimized parsers using beam search (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results. In our experiments, a greedy parser performs as accurately as a parser that uses beam search for about"
P13-1104,D12-1030,0,0.030318,"Missing"
P15-1038,D14-1082,0,0.0182931,"http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced the best speed performance. Given this trend, we also include how those three parsers perform at beam 1 in our analyses. We compared ten state of the art parsers representi"
P15-1038,P13-1104,1,0.926373,"Missing"
P15-1038,P12-2071,1,0.342324,"Missing"
P15-1038,W08-1301,0,0.108481,"Missing"
P15-1038,Q13-1033,0,0.0475998,"Missing"
P15-1038,P11-2125,0,0.09249,"Missing"
P15-1038,W09-1201,0,0.0163968,"Missing"
P15-1038,D11-1037,0,0.019356,": newswire, PT : pivot text, TC : telephone conversation, WB : web text, ALL : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4 , Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter,"
P15-1038,W13-3518,0,0.028072,"2012). 3 conll.cemantix.org/2012/download/ids/ 4 http://nlp.cs.lth.se/software http://nlp.stanford.edu/software 6 http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced the best speed performance. Given this trend, we also in"
P15-1038,W14-6110,0,0.0237582,"Missing"
P15-1038,N12-1015,0,0.0184437,"Missing"
P15-1038,W07-2416,0,0.0132148,"Missing"
P15-1038,D12-1096,0,0.0247064,"Missing"
P15-1038,D10-1069,0,0.0128203,"L : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4 , Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter, the ClearNLP converter produces a similar set of dependency labels but ge"
P15-1038,D13-1116,0,0.029659,"Missing"
P15-1038,W13-3516,0,0.0362406,"Missing"
P15-1038,P14-1130,0,0.0156994,"cy parsing was performed by (Kummerfeld and others, 2012). 3 conll.cemantix.org/2012/download/ids/ 4 http://nlp.cs.lth.se/software http://nlp.stanford.edu/software 6 http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced"
P15-1038,N06-2033,0,0.0191273,"Missing"
P15-1038,J93-2004,0,0.0541048,"rted in these shared tasks are: labeled attachment score (LAS) – the percentage of predicted dependencies where the arc and the label are assigned correctly; unlabeled attachment score (UAS) – where the arc is assigned correctly; label accuracy score (LS) – where the label is assigned correctly; and exact match (EM) – the percentage of sentences whose predicted trees are entirely correct. Although shared tasks have been tremendously useful for advancing the state of the art in dependency parsing, most English evaluation has employed a single-genre corpus, the WSJ portion of the Penn Treebank (Marcus et al., 1993), so it is not immediately clear how these results genIntroduction Dependency parsing is a valuable form of syntactic processing for NLP applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages. Thanks to over a decade of research on statistical dependency parsing, many dependency parsers are now publicly available. In this paper, we report on a comparative analysis of leading statistical dependency parsers using a multi-genre corpus. Our purpose is not to introduce a new parsing algorithm but to assess the performance of exist"
P15-1038,P13-2109,0,0.137596,"Missing"
P15-1038,W13-4917,1,0.529379,"Missing"
P15-1038,D07-1013,0,0.0163942,"6 11,467 10,976 8,969 1,634 1,366 WB 284,975 36,351 38,490 12,452 1,797 1,787 ALL 2,084,081 291,640 216,357 105,179 15,161 11,697 Table 1: Distribution of data used for our experiments. The first three/last three rows show the number of tokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine, NW : newswire, PT : pivot text, TC : telephone conversation, WB : web text, ALL : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only"
P15-1038,E12-2021,0,0.0425666,"Missing"
P15-1038,J11-1007,0,0.0218317,"Missing"
P15-1038,W08-2121,0,0.021027,"Missing"
P15-1038,W10-1905,0,0.0467351,"Missing"
P15-1038,D11-1036,0,0.0159019,"ate non-projective dependencies. Development data ClearNLP, LTDP, SNN and Yara make use of the development data (for parameter tuning). Mate and Turbo self-tune parameter settings using the training data. The others were trained using their default/“standard” parameter settings. 5 D EPENDA BLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, D EPENDA BLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of D EPENDA BLE: Beam search ClearNLP, LTDP, Redshift and Yara have the option of different beam settings. The higher the beam size, the more accurate th"
P15-1038,nilsson-nivre-2008-malteval,0,0.0972031,"elopment data ClearNLP, LTDP, SNN and Yara make use of the development data (for parameter tuning). Mate and Turbo self-tune parameter settings using the training data. The others were trained using their default/“standard” parameter settings. 5 D EPENDA BLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, D EPENDA BLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of D EPENDA BLE: Beam search ClearNLP, LTDP, Redshift and Yara have the option of different beam settings. The higher the beam size, the more accurate the parser usually becomes, but typicall"
P15-1038,C10-1094,0,0.0281924,"Missing"
P15-1038,S14-2008,0,\N,Missing
P15-1038,W06-2920,0,\N,Missing
P15-1038,D07-1096,0,\N,Missing
P16-3020,P08-1002,0,0.369484,"ion; one of the least developed tasks left in natural language processing. Coreference resolution can be processed in two steps, mention detection and antecedent resolution. For mention detection, the classification of the pronoun it as either referential or non-referential is of critical importance because the identification of non-referential instances of it is essential to remove from the total list of possible mentions (Branco et al., 2005; Wiseman et al., 2015). Although previous work has demonstrated a lot of promise for classifying all instances of it (Boyd et al., 2005; M¨uller, 2006; Bergsma et al., 2008; Li et al., 2009), it is still a difficult task, especially when performed on social networks data containing grammatical errors, ambiguity, and colloquial language. In specific, we found that the incorrect classification of non-referential it was one of the major reasons for the failure of a question answering system handling social networks data. In this paper, we first introduce our new corpus, QA-It, sampled from the Yahoo! Answers corpus and manually anJinho D. Choi Math & Computer Science Emory University Atlanta, GA 30322, USA jinho.choi@emory.edu notated with 4 categories of it, refer"
P16-3020,W05-0406,0,0.232904,"is to resolve coreference resolution; one of the least developed tasks left in natural language processing. Coreference resolution can be processed in two steps, mention detection and antecedent resolution. For mention detection, the classification of the pronoun it as either referential or non-referential is of critical importance because the identification of non-referential instances of it is essential to remove from the total list of possible mentions (Branco et al., 2005; Wiseman et al., 2015). Although previous work has demonstrated a lot of promise for classifying all instances of it (Boyd et al., 2005; M¨uller, 2006; Bergsma et al., 2008; Li et al., 2009), it is still a difficult task, especially when performed on social networks data containing grammatical errors, ambiguity, and colloquial language. In specific, we found that the incorrect classification of non-referential it was one of the major reasons for the failure of a question answering system handling social networks data. In this paper, we first introduce our new corpus, QA-It, sampled from the Yahoo! Answers corpus and manually anJinho D. Choi Math & Computer Science Emory University Atlanta, GA 30322, USA jinho.choi@emory.edu n"
P16-3020,J92-4003,0,0.131058,"Missing"
P16-3020,P13-1104,1,0.854022,"Missing"
P16-3020,P12-2071,1,0.861393,"Missing"
P16-3020,E06-1007,0,0.485218,"Missing"
P16-3020,P15-1137,0,0.0329483,"uch a corpus is created for question answering. 1 Introduction One important factor in processing document-level text is to resolve coreference resolution; one of the least developed tasks left in natural language processing. Coreference resolution can be processed in two steps, mention detection and antecedent resolution. For mention detection, the classification of the pronoun it as either referential or non-referential is of critical importance because the identification of non-referential instances of it is essential to remove from the total list of possible mentions (Branco et al., 2005; Wiseman et al., 2015). Although previous work has demonstrated a lot of promise for classifying all instances of it (Boyd et al., 2005; M¨uller, 2006; Bergsma et al., 2008; Li et al., 2009), it is still a difficult task, especially when performed on social networks data containing grammatical errors, ambiguity, and colloquial language. In specific, we found that the incorrect classification of non-referential it was one of the major reasons for the failure of a question answering system handling social networks data. In this paper, we first introduce our new corpus, QA-It, sampled from the Yahoo! Answers corpus an"
P17-3009,N16-1062,0,0.0200732,"le is independent from the others, dialogues within a typical TV show are highly structured (Knyazeva et al., 2015). Every utterance is highly related to its prior and subsequent utterances, and it is important to take sequential information into account in predicting the speakers. However, contextual information is completely ignored by the basic CNN model. Each batch of input to the model consists of discrete utterances from different episodes and seasons, as shown in Figure 2. To remedy the loss of contextual information, the CNN model is modified in a manner similar to the one proposed by Lee and Dernoncourt (2016). After the global max pooling layer, each utterance vector is concatenated with both the previous two utterances and the subsequent utterance in the same scene. Then, the vector is fed into the fully con2 snap.stanford.edu/data/web-Amazon. html 51 Dataset M P R1 R2 J C O Total Training Development Evaluation 5,017 898 810 4,349 799 769 5,308 1,092 1,082 5,527 821 983 4,738 930 909 5,268 846 673 7,006 931 999 37,213 6,317 6,225 Training Development Evaluation 919 151 116 840 148 116 934 151 137 931 131 132 943 148 129 994 139 119 1,199 159 156 6,760 1,027 905 Table 2: Dataset distribution by s"
P17-3009,W15-4640,0,0.0404173,"Missing"
P17-3009,W16-3612,1,0.894727,"Missing"
P17-3009,D14-1181,0,0.00466804,"conclude that a simple RNN model is unable to perform speaker identification based on textual data. Variations on the hyperparameters, including the dimension of the RNN, the dimension of word embeddings, and dropout rate, produced no appreciable improvements. 4.3 Pooling Prediction U-#1 U-#2 U-#3 U-#4 Figure 2: The baseline CNN model. Convolution Pooling Prediction Scene Convolutional Neural Network Widely utilized for computer vision, Convolutional Neural Network (CNN) models have recently been applied to natural language processing and showed great results for many tasks (Yih et al., 2014; Kim, 2014; Shen et al., 2014). Speaker identification can be conceptualized as a variant of document classification. Therefore, we elected to use the traditional CNN for our task. The model is a minor modification to the proposal of Kim (2014), which consists of a 1-dimensional convolution layer with different filter sizes, a global max pooling layer, and a fully connected layer. Each utterance is treated as one sample and classified independently. One of the challenges is the large number of misspellings and colloquialisms in the dataset as a result of the mistakes in the human transcription process a"
P17-3009,tiedemann-2012-parallel,0,0.0604456,"Missing"
P17-3009,P14-2105,0,0.0123082,"fter training. We conclude that a simple RNN model is unable to perform speaker identification based on textual data. Variations on the hyperparameters, including the dimension of the RNN, the dimension of word embeddings, and dropout rate, produced no appreciable improvements. 4.3 Pooling Prediction U-#1 U-#2 U-#3 U-#4 Figure 2: The baseline CNN model. Convolution Pooling Prediction Scene Convolutional Neural Network Widely utilized for computer vision, Convolutional Neural Network (CNN) models have recently been applied to natural language processing and showed great results for many tasks (Yih et al., 2014; Kim, 2014; Shen et al., 2014). Speaker identification can be conceptualized as a variant of document classification. Therefore, we elected to use the traditional CNN for our task. The model is a minor modification to the proposal of Kim (2014), which consists of a 1-dimensional convolution layer with different filter sizes, a global max pooling layer, and a fully connected layer. Each utterance is treated as one sample and classified independently. One of the challenges is the large number of misspellings and colloquialisms in the dataset as a result of the mistakes in the human transcriptio"
S18-1007,N13-1122,0,0.0390186,"Missing"
S18-1007,N16-1031,1,0.899944,"ing task that aims for holistic understanding in multiparty dialogue. Most of previous works on entity linking have focused on Wikification, which links named entity mentions to their relevant Wikipedia articles (Mihalcea and Csomai, 2007; Ratinov et al., 2011; Guo et al., 2013). Unlike Wikification where most entities come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom3 Corpus The character identification corpus was first created by collecting transcripts from the popular TV show, Friends (Chen and Choi, 2016). These transcripts were voluntarily provided by fans who made them publicly available.4 Dialogues in this corpus mimic daily conversations that are more natural and various in topics than other dialogue corpora (Janin et al., 2003; Danescu-Niculescu-Mizil and Lee, 2011; Hu et al., 2013; Kim et al., 2015; Lowe et al., 2015). Although they are scripted, the interpretation of these dialogues is no easier than unscripted 4 58 http://www.livesinabox.com/friends/ scripts.shtml Season 1 Season 2 Total Episodes 24 23 47 Scenes 229 219 448 Speakers 105 101 171 Utterances 4,725 4,501 9,226 Sentences 8,"
S18-1007,P13-1104,1,0.781272,"character, that are Ross, Chandler, Joey, Rachel, Monica, and Pheobe, a secondary character (other frequently recurring characters across the show), or one of the following ambiguous types suggested by Chen et al. (2017): • Generic: indicates actual characters in the show whose identities are unknown (e.g., That waitress is really cute, I am going to ask her out). Generic entities are annotated with their group names and optional numberings (e.g., Man 1, Woman 1). Mention Annotation For mention annotation, a heuristic-based mention detector was developed, which utilized dependency relations (Choi and McCallum, 2013), named entity tags (Choi, 2016), and personal noun gazetteers, then automatically detected mentions for the entire corpus. In this heuristic, a noun phrase was considered a personal mention if it was either: • Collective: indicates the plural use of the pronoun you, which cannot be deterministically distinguished from the singular use. 1. A PERSON named entity, or • General: indicates mentions used in reference to a general case rather than an specific entity (e.g., The ideal guy you look for doesn’t exist). 2. A pronoun or a possessive pronoun excluding the pronouns it and they, or • Other:"
S18-1007,W11-0609,0,0.0367788,"nov et al., 2011; Guo et al., 2013). Unlike Wikification where most entities come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom3 Corpus The character identification corpus was first created by collecting transcripts from the popular TV show, Friends (Chen and Choi, 2016). These transcripts were voluntarily provided by fans who made them publicly available.4 Dialogues in this corpus mimic daily conversations that are more natural and various in topics than other dialogue corpora (Janin et al., 2003; Danescu-Niculescu-Mizil and Lee, 2011; Hu et al., 2013; Kim et al., 2015; Lowe et al., 2015). Although they are scripted, the interpretation of these dialogues is no easier than unscripted 4 58 http://www.livesinabox.com/friends/ scripts.shtml Season 1 Season 2 Total Episodes 24 23 47 Scenes 229 219 448 Speakers 105 101 171 Utterances 4,725 4,501 9,226 Sentences 8,680 7,380 16,060 Tokens 66,355 65,675 132,030 Table 1: Distributions from the subset of the character identification corpus used for this shared task. dialogues; they not only involve as much disfluency and context switching as real dialogues do, but also include more h"
S18-1007,D11-1141,0,0.0662343,"nd publicly release a larger and cleaner dataset, hoping to support researchers for more enhanced modeling. 1 Introduction Most of the earlier works in natural language processing (NLP) had focused on formal writing such as newswires, whereas many recent works have targeted at colloquial writing such as text messages or social media. Since the evolution of Web 2.0, the amount of user-generated contents involving colloquial writing has exceeded the one with formal writing. NLP tasks are relatively well-explored at this point for certain types of colloquial writing i.e., microblogs and reviews (Ritter et al., 2011; Kong et al., 2014; Ranganath et al., 2016; Shin et al., 2017). However, the genre of multiparty dialogue is still under-explored, even though digital contents in dialogue forms keep increasing at a faster rate than any other types of writing.1 This inspires us 1 2 https://competitions.codalab.org/ competitions/17310 3 https://github.com/emorynlp/ semeval-2018-task4 https://medium.com/hijiffy/10-graphs-that-show-theimmense-power-of-messaging-apps-4a41385b24d6 57 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 57–64 New Orleans, Louisiana, June 5–6,"
S18-1007,D13-1036,0,0.0253418,"like Wikification where most entities come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom3 Corpus The character identification corpus was first created by collecting transcripts from the popular TV show, Friends (Chen and Choi, 2016). These transcripts were voluntarily provided by fans who made them publicly available.4 Dialogues in this corpus mimic daily conversations that are more natural and various in topics than other dialogue corpora (Janin et al., 2003; Danescu-Niculescu-Mizil and Lee, 2011; Hu et al., 2013; Kim et al., 2015; Lowe et al., 2015). Although they are scripted, the interpretation of these dialogues is no easier than unscripted 4 58 http://www.livesinabox.com/friends/ scripts.shtml Season 1 Season 2 Total Episodes 24 23 47 Scenes 229 219 448 Speakers 105 101 171 Utterances 4,725 4,501 9,226 Sentences 8,680 7,380 16,060 Tokens 66,355 65,675 132,030 Table 1: Distributions from the subset of the character identification corpus used for this shared task. dialogues; they not only involve as much disfluency and context switching as real dialogues do, but also include more humor, sarcasm, or"
S18-1007,W17-5220,1,0.831554,"ort researchers for more enhanced modeling. 1 Introduction Most of the earlier works in natural language processing (NLP) had focused on formal writing such as newswires, whereas many recent works have targeted at colloquial writing such as text messages or social media. Since the evolution of Web 2.0, the amount of user-generated contents involving colloquial writing has exceeded the one with formal writing. NLP tasks are relatively well-explored at this point for certain types of colloquial writing i.e., microblogs and reviews (Ritter et al., 2011; Kong et al., 2014; Ranganath et al., 2016; Shin et al., 2017). However, the genre of multiparty dialogue is still under-explored, even though digital contents in dialogue forms keep increasing at a faster rate than any other types of writing.1 This inspires us 1 2 https://competitions.codalab.org/ competitions/17310 3 https://github.com/emorynlp/ semeval-2018-task4 https://medium.com/hijiffy/10-graphs-that-show-theimmense-power-of-messaging-apps-4a41385b24d6 57 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 57–64 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Jack Judy"
S18-1007,D14-1108,0,0.0235114,"larger and cleaner dataset, hoping to support researchers for more enhanced modeling. 1 Introduction Most of the earlier works in natural language processing (NLP) had focused on formal writing such as newswires, whereas many recent works have targeted at colloquial writing such as text messages or social media. Since the evolution of Web 2.0, the amount of user-generated contents involving colloquial writing has exceeded the one with formal writing. NLP tasks are relatively well-explored at this point for certain types of colloquial writing i.e., microblogs and reviews (Ritter et al., 2011; Kong et al., 2014; Ranganath et al., 2016; Shin et al., 2017). However, the genre of multiparty dialogue is still under-explored, even though digital contents in dialogue forms keep increasing at a faster rate than any other types of writing.1 This inspires us 1 2 https://competitions.codalab.org/ competitions/17310 3 https://github.com/emorynlp/ semeval-2018-task4 https://medium.com/hijiffy/10-graphs-that-show-theimmense-power-of-messaging-apps-4a41385b24d6 57 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 57–64 New Orleans, Louisiana, June 5–6, 2018. ©2018 Associa"
S18-1007,W15-4640,0,0.0333773,"es come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom3 Corpus The character identification corpus was first created by collecting transcripts from the popular TV show, Friends (Chen and Choi, 2016). These transcripts were voluntarily provided by fans who made them publicly available.4 Dialogues in this corpus mimic daily conversations that are more natural and various in topics than other dialogue corpora (Janin et al., 2003; Danescu-Niculescu-Mizil and Lee, 2011; Hu et al., 2013; Kim et al., 2015; Lowe et al., 2015). Although they are scripted, the interpretation of these dialogues is no easier than unscripted 4 58 http://www.livesinabox.com/friends/ scripts.shtml Season 1 Season 2 Total Episodes 24 23 47 Scenes 229 219 448 Speakers 105 101 171 Utterances 4,725 4,501 9,226 Sentences 8,680 7,380 16,060 Tokens 66,355 65,675 132,030 Table 1: Distributions from the subset of the character identification corpus used for this shared task. dialogues; they not only involve as much disfluency and context switching as real dialogues do, but also include more humor, sarcasm, or metaphor. Thus, models evaluated on t"
S18-1007,W12-4501,0,0.102463,"Missing"
S18-1007,P11-1138,0,0.0263325,"Joey are the active speakers of the dialogue, whereas Jack and Judy are not although they are passively mentioned as mom and dad in this context. Linking such mentions to their global entities demands inferred knowledge about the kinship from other dialogues, challenging crossdocument resolution. Thus, character identification can be viewed as an entity linking task that aims for holistic understanding in multiparty dialogue. Most of previous works on entity linking have focused on Wikification, which links named entity mentions to their relevant Wikipedia articles (Mihalcea and Csomai, 2007; Ratinov et al., 2011; Guo et al., 2013). Unlike Wikification where most entities come with structured information from knowledge bases (e.g., Infobox, Freebase, DBPedia), entities in character identification have no such precom3 Corpus The character identification corpus was first created by collecting transcripts from the popular TV show, Friends (Chen and Choi, 2016). These transcripts were voluntarily provided by fans who made them publicly available.4 Dialogues in this corpus mimic daily conversations that are more natural and various in topics than other dialogue corpora (Janin et al., 2003; Danescu-Niculesc"
vaidya-etal-2012-empty,bhatia-etal-2010-empty,1,\N,Missing
vaidya-etal-2012-empty,W11-0403,1,\N,Missing
vaidya-etal-2012-empty,W10-1836,1,\N,Missing
vaidya-etal-2012-empty,W09-3036,1,\N,Missing
vaidya-etal-2012-empty,J08-2004,0,\N,Missing
vaidya-etal-2012-empty,J05-1004,1,\N,Missing
vaidya-etal-2012-empty,I08-2099,0,\N,Missing
vaidya-etal-2012-empty,choi-etal-2010-propbank-instance,1,\N,Missing
W09-3020,J07-3002,0,0.027874,"d into two parts: the Xinhua Chinese newswire with literal English translations (4,363 parallel sentences) and the Sinorama Chinese news magazine with non-literal English translations (12,600 parallel sentences). We experimented with the two parts separately to see how literal and non-literal translations affect word-alignments. Introduction Since verbs tend to be the roots of dependency relations in a sentence (Palmer et al., 2005), when it comes down to translations, finding correct mappings between verbs in a source and a target language is very important. Many machine translation systems (Fraser and Marcu, 2007) use wordalignment tools such as GIZA++ (Och and Ney, 2003) to retrieve word mappings between a source and a target language. Although GIZA++ gives well-structured alignments, it has limitations in several ways. First, it is hard to verify if alignments generated by GIZA++ are correct. Second, GIZA++ may not find alignments for low-frequent words. Third, GIZA++ does not account for any semantic information. In this paper, we suggest a couple of ways to enhance word-alignments for predicating expressions such as verbs1 . We restricted the source and the target language to Chinese and English, r"
W09-3020,J03-1002,0,0.00432598,"sh translations (4,363 parallel sentences) and the Sinorama Chinese news magazine with non-literal English translations (12,600 parallel sentences). We experimented with the two parts separately to see how literal and non-literal translations affect word-alignments. Introduction Since verbs tend to be the roots of dependency relations in a sentence (Palmer et al., 2005), when it comes down to translations, finding correct mappings between verbs in a source and a target language is very important. Many machine translation systems (Fraser and Marcu, 2007) use wordalignment tools such as GIZA++ (Och and Ney, 2003) to retrieve word mappings between a source and a target language. Although GIZA++ gives well-structured alignments, it has limitations in several ways. First, it is hard to verify if alignments generated by GIZA++ are correct. Second, GIZA++ may not find alignments for low-frequent words. Third, GIZA++ does not account for any semantic information. In this paper, we suggest a couple of ways to enhance word-alignments for predicating expressions such as verbs1 . We restricted the source and the target language to Chinese and English, respectively. The goal is to use the linguistic annotation a"
W09-3020,J05-1004,1,0.35791,"Missing"
W09-3020,W04-2705,0,\N,Missing
W10-1811,W04-3228,0,0.0233037,"nformation got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. Figure 1 shows a dependency tree that has been converted from the corresponding phrase structure tree. S VP NP1 Introduction Dependency structure has recently gained wide interest because it is simple yet provides useful information for many NLP tasks such as sentiment analysis (Kessler and Nicolov, 2009) or machine translation (Gildea, 2004). Although dependency structure is a kind of syntactic structure, it is quite different from phrase structure: phrase structure gives phrase information by grouping constituents whereas dependency structure gives dependency relations between pairs of words. Many dependency relations (e.g., subject, object) have high correlations with semantic roles (e.g., agent, patient), which makes dependency structure suitDT NNS VBP The results appear PP1 IN NP in NP NN POS today ’s ROOT NMOD root The NN news PMOD NMOD SBJ results LOC appear NMOD in today &apos;s news Figure 1: Phrase vs. dependency structure 91"
W10-1811,W07-2416,0,0.046988,"nn Treebank, but used a different dependency conversion tool, Penn2Malt.1 Our work is distinguished from theirs because we keep the tree structure but use heuristics to find the boundaries. Johansson (2008) also tried to find semantic boundaries for evaluation of his semantic role labeling system using dependency structure. He used heuristics that apply to general cases whereas we add more detailed heuristics for specific cases. 3 We used the same tool as the one used for the CoNLL’09 shared task to automatically convert the phrase structure trees in the Penn Treebank to the dependency trees (Johansson and Nugues, 2007). The script gives several options for the conversion; we mostly used the default values except for the following options:2 • splitSlash=false: do not split slashes. This option is taken so the dependency trees preserve the same number of word-tokens as the original phrase structure trees. • noSecEdges=true: ignore secondary edges if present. This option is taken so all siblings of verb predicates in phrase structure become children of the verbs in dependency structure regardless of empty categories. Figure 3 shows the converted dependency tree, which is produced when the secondary edge (*ICH*"
W10-1811,J93-2004,0,0.0378088,"Missing"
W10-1811,J05-1004,1,0.392094,"hermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation. 1 In 2009, the Conference on Computational Natural Language Learning (CoNLL) opened a shared task: the participants were supposed to take dependency trees as input and produce semantic role labels as output (Hajiˇc et al., 2009). The dependency trees were automatically converted from the Penn Treebank (Marcus et al., 1993), which consists of phrase structure trees, using some heuristics (cf. Section 3). The semantic roles were extracted from the Propbank (Palmer et al., 2005). Since Propbank arguments were originally annotated at the phrase level using the Penn Treebank and the phrase information got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. Figure 1 shows a dependency tree that has been converted from the corresponding phrase structure tree. S VP NP1 Introduction Dependency structure has recently gained wide interest because it is simple yet provides usef"
W10-1811,N04-1030,0,0.0400337,"ns. First, it is often not clear how punctuation 96 NMOD P NMOD a APPO fellow OPRD DEP TMP PMOD Accuracy = named John , who stayed for years 1 X · c(gold(arg), sys(arg)) T ∀arg Figure 20: Past-participle example 2 P recision = ∀arg needs to be annotated in either Treebank or Propbank; because of that, annotation for punctuation is not entirely consistent, which makes it hard to evaluate. Second, although punctuation gives useful information for obtaining semantic boundaries, it is not crucial for semantic roles. In fact, some of the state-of-art semantic role labeling systems, such as ASSERT (Pradhan et al., 2004), give an option for omitting punctuation from the output. For these reasons, our final model ignores punctuation for semantic boundaries. 6 Recall = 1 X |gold(arg) ∩ sys(arg)| · T |gold(arg)| ∀arg F1 = 2 · P recision · Recall P recision + Recall Table 1 shows the results from the models using the measurements. As expected, each model shows improvement over the previous one in terms of accuracy and F1-score. The F1-score of Model VI shows improvement that is statistically significant compared to Model I using t-test (t = 149.00, p < 0.0001). The result from the final model is encouraging becau"
W10-1811,W09-1201,0,\N,Missing
W10-1811,W09-3020,1,\N,Missing
W11-0403,I08-2099,0,0.0893979,"ation as well as lexical semantic information in the form of PropBank. The corpus also produces phrase structure representations in addition to de1 The term ’semantic argument’ is used to indicate all numbered arguments as well as modifiers in PropBank. 21 Proceedings of the Fifth Law Workshop (LAW V), pages 21–29, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics pendency structure. The Hindi Dependency Treebank has created an annotation scheme for Hindi by adapting labels from Panini’s Sanskrit grammar (also known as CPG: Computational Paninian Grammar; see Begum et al. (2008)). Previous work has demonstrated that the English PropBank tagset is quite similar to English dependency trees annotated with the Paninian labels (Vaidya et al., 2009). PropBank has also been mapped to other dependency schemes such as Functional Generative Description (Cinkova, 2006). 2.2 Hindi Dependency Treebank The Hindi Dependency Treebank (HDT) includes morphological, part-of-speech and chunking information as well as dependency relations. These are represented in the Shakti Standard Format (SSF; see Bharati et al. (2007)). The dependency labels depict relations between chunks, which are"
W11-0403,bhatia-etal-2010-empty,1,0.819722,"Missing"
W11-0403,W09-3036,1,0.903849,"(from now on, PropBank) is a corpus in which the arguments of each verb predicate are annotated with their semantic roles (Palmer et al., 2005). PropBank annotation has been carried out in several languages; most of them are annotated on top of Penn Treebank style phrase structure (Xue and Palmer, 2003; Palmer et al., 2008). However, a different grammatical analysis has been used for the Hindi PropBank annotation, dependency structure, which may be particularly suited for the analysis of flexible word order languages such as Hindi. As a syntactic corpus, we use the Hindi Dependency Treebank (Bhatt et al., 2009). Using dependency structure has some advantages. First, semantic arguments1 can be marked explicitly on the syntactic trees, so annotations of the predicate argument structure can be more consistent with the dependency structure. Second, the Hindi Dependency Treebank provides a rich set of dependency relations that capture the syntactic-semantic information. This facilitates mappings between syntactic dependents and semantic arguments. A successful mapping would reduce the annotation effort, improve the inter-annotator agreement, and guide a full fledged semantic role labeling task. In this p"
W11-0403,choi-etal-2010-propbank,1,0.941772,"indi PropBank (HPB) contains the labeling of semantic roles, which are defined on a verb-by-verb basis. The description at the verb-specific level is fine-grained; e.g., ‘hitter’ and ‘hittee’. These verbspecific roles are then grouped into broader categories using numbered arguments (ARG#). Each verb can also have modifiers not specific to the verb (ARGM*). The annotation process takes place in two stages: the creation of frameset files for individual verb types, and the annotation of predicate argu22 ment structures for each verb instance. As annotation tools, we use Cornerstone and Jubilee (Choi et al., 2010a; Choi et al., 2010b). The annotation is done on the HDT; following the dependency annotation, PropBank annotates each verb’s syntactic dependents as their semantic arguments at the chunk level. Chunked trees are conveniently displayed for annotators in Jubilee. PropBank annotations generated in Jubilee can also be easily projected onto the SSF format of the original dependency trees. The HPB currently consists of 24 labels including both numbered arguments and modifiers (Table 1). In certain respects, the HPB labels make some distinctions that are not made in some other language such as Engl"
W11-0403,choi-etal-2010-propbank-instance,1,0.840294,"Missing"
W11-0403,cinkova-2006-propbank,0,0.0177999,"shop (LAW V), pages 21–29, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics pendency structure. The Hindi Dependency Treebank has created an annotation scheme for Hindi by adapting labels from Panini’s Sanskrit grammar (also known as CPG: Computational Paninian Grammar; see Begum et al. (2008)). Previous work has demonstrated that the English PropBank tagset is quite similar to English dependency trees annotated with the Paninian labels (Vaidya et al., 2009). PropBank has also been mapped to other dependency schemes such as Functional Generative Description (Cinkova, 2006). 2.2 Hindi Dependency Treebank The Hindi Dependency Treebank (HDT) includes morphological, part-of-speech and chunking information as well as dependency relations. These are represented in the Shakti Standard Format (SSF; see Bharati et al. (2007)). The dependency labels depict relations between chunks, which are “minimal phrases consisting of correlated, inseparable entities” (Bharati et al., 2006), so they are not necessarily individual words. The annotation of chunks also assumes that intra-chunk dependencies can be extracted automatically (Husain et al., 2010). The dependency tagset consi"
W11-0403,hajicova-kucerova-2002-argument,0,0.0884332,"Missing"
W11-0403,W10-1810,1,0.788338,"is that neither the nominal nor the light verb seem to project arguments of their own. Important progress has been made in this work k7p pof Table 3: Mappings to the HPB modifiers. 3.3 इस_काम_& Simple and complex predicates this_work_LOC HPB distinguishes annotations between simple and complex predicates. Simple predicates consist of only a single verb whereas complex predicates consist of a light verb and a pre-verbal element. The complex predicates are identified with a special label ARGM-PRX (ARGument-PRedicating eXpresstion), which is being used for all light verb annotations in PropBank (Hwang et al., 2010). Figure 2 shows an example of the predicating noun mention annotated as ARGM-PRX, used with come. The predicating noun also has its own argument, matter of, indicated with the HDT label r6-k1. The HDT has two labels, r6-k1 and r6-k2, for the arguments of the predicating noun. Hence, the argument span for complex predicates includes not only direct dependents of the verb but also dependents of the noun. During the hearing on Wednesday, the matter was mentioned k7t k7t r6-k1 स""नवाई_&apos;_दौरान hearing_of_during ब""धवार_को Wed._of माम0_का matter_of pof िज3_भी mention_to आया come ARGM-PRX ARG1 ARGM-TM"
W11-0403,H94-1020,0,0.282794,"_का matter_of pof िज3_भी mention_to आया come ARGM-PRX ARG1 ARGM-TMP ARGM-TMP Figure 2: Complex predicate example. The ARGM-PRX label usually overlaps with the HDT label pof, indicating a ‘part of units’ as pre24 मह+वप.ण_1ग / 4त important_progress &apos;ई_) be_PRES ARG1 ARGM-LOC Figure 3: HDT vs. HPB on complex predicates. 4 Automatic mapping of HDT to HPB Mapping between syntactic and semantic structures has been attempted in other languages. The Penn English and Chinese Treebanks consist of several semantic roles (e.g., locative, temporal) annotated on top of Penn Treebank style phrase structure (Marcus et al., 1994; Xue and Palmer, 2009). The Chinese PropBank specifies mappings between syntactic and semantic arguments in frameset files (e.g., SBJ → ARG0) that can be used for automatic mapping (Xue and Palmer, 2003). However, these Chinese mappings are limited to certain types of syntactic arguments (mostly subjects and objects). Moreover, semantic annotations on the Treebanks are done independently from PropBank annotations, which causes disagreement between the two structures. Dependency structure transparently encodes relations between predicates and their arguments, which facilitates mappings between"
W11-0403,J05-1004,1,0.513651,"rically-derived rules: predicate ID, predicate’s voice type, and argument’s dependency label. The predicate ID is either the lemma or the roleset ID of the predicate. Predicate lemmas are already provided in HDT. When we use predicate lemmas, we assume no manual annotation of PropBank. Thus, rules generated from predicate lemmas can be applied to any future data without modification. When we use roleset ID’s, we assume that sense annotations are already done. PropBank includes annotations of coarse verb senses, called roleset ID’s, that differentiate each verb predicate with different senses (Palmer et al., 2005). A verb predicate can form several argument structures with respect to different senses. Using roleset ID’s, we generate more fine-grained rules that are specific to those senses. The predicate’s voice type is either ‘active’ or ‘passive’, also provided in HDT. There are not many instances of passive construction in our current data, which makes it difficult to generate rules general enough for future data. However, even with the lack of training instances, we find some advantage of using the voice feature in our experiments. Finally, the argument’s dependency label is the dependency label of"
W11-0403,palmer-etal-2008-pilot,1,0.730572,"Missing"
W11-0403,W03-1707,1,0.733316,"as pre24 मह+वप.ण_1ग / 4त important_progress &apos;ई_) be_PRES ARG1 ARGM-LOC Figure 3: HDT vs. HPB on complex predicates. 4 Automatic mapping of HDT to HPB Mapping between syntactic and semantic structures has been attempted in other languages. The Penn English and Chinese Treebanks consist of several semantic roles (e.g., locative, temporal) annotated on top of Penn Treebank style phrase structure (Marcus et al., 1994; Xue and Palmer, 2009). The Chinese PropBank specifies mappings between syntactic and semantic arguments in frameset files (e.g., SBJ → ARG0) that can be used for automatic mapping (Xue and Palmer, 2003). However, these Chinese mappings are limited to certain types of syntactic arguments (mostly subjects and objects). Moreover, semantic annotations on the Treebanks are done independently from PropBank annotations, which causes disagreement between the two structures. Dependency structure transparently encodes relations between predicates and their arguments, which facilitates mappings between syntactic and semantic arguments. Hajiˇcov´a and Kuˇcerov´a (2002) tried to project PropBank semantic roles onto the Prague Dependency Treebank, and showed that the projection is not trivial. The same ma"
W11-0403,N06-5006,0,\N,Missing
W11-0906,W10-1811,1,0.895778,"Missing"
W11-0906,P11-2121,1,0.849858,"do not include predicate sense classification as a part of our task, which is rather a task of word sense disambiguation than semantic role labeling. For in-domain and out-of-domain evaluations, W SJ section 23 and the Brown corpus are used, also distributed by CoNLL’09. To retrieve automatically generated dependency trees as input to our semantic role labeler, we train our open source dependency parser, called ClearParser3 , on the training set and run the parser on the evaluation sets. ClearParser uses a transition-based dependency parsing algorithm that gives near state-of-the-art results (Choi and Palmer, 2011), and mirrors our SRL algorithm. 5.3 Accuracy comparisons Baseline +Dynamic +Cluster 5.2 Statistical models We use Liblinear L2-L1 S VM for learning; a linear classification algorithm using L2 regularization and L1 loss function. This algorithm is designed to handle large scale data: it assumes the data to be linearly separable so does not use any kind of kernel space (Hsieh et al., 2008). As a result, it significantly reduces training time compared to typical S VM, yet performs accurately. For our experiments, we use the following learning parameters: c = 0.1 (cost), e = 0.2 (termination crit"
W11-0906,J02-3001,0,0.648942,"hansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002). This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (Hsieh et al., 2008). Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing). This motivates the development of"
W11-0906,W09-1201,0,0.0783207,"Missing"
W11-0906,W08-2122,0,0.0519197,"of relation between any word pair, semantic role labeling restricts its search only to top-down relations between predicate and argument pairs. Second, dependency parsing requires one head for each word, so the final output is a tree, whereas semantic role labeling allows multiple predicates for each argument. Thus, not all dependency parsing algorithms, such as a maximum spanning tree algorithm (Mcdonald and Pereira, 2006), can be naively applied to semantic role labeling. Some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results (Henderson et al., 2008; Titov et al., 2009). However, these algorithms are originally designed for dependency parsing, so are not necessarily customized for semantic role label38 ing. Here, we present a novel transition-based algorithm dedicated to semantic role labeling. The key difference between this algorithm and most other transition-based algorithms is in its directionality. Given an identified predicate, this algorithm tries to find top-down relations between the predicate and the words on both left and right-hand sides, whereas other transition-based algorithms would consider words on either the left or the"
W11-0906,P10-1110,0,0.066037,"Missing"
W11-0906,D08-1008,0,0.238536,"atistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 1 Introduction Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). Lately, dependency-based SRL has shown advantages over constituent-based SRL (Johansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002"
W11-0906,C10-1081,0,0.0127869,"e that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 1 Introduction Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). Lately, dependency-based SRL has shown advantages over constituent-based SRL (Johansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semanti"
W11-0906,E06-1011,0,0.0314723,"of dependency parsing in the sense that both try to find relations between word pairs. However, they are distinguished in two major ways. First, unlike dependency parsing that tries to find some kind of relation between any word pair, semantic role labeling restricts its search only to top-down relations between predicate and argument pairs. Second, dependency parsing requires one head for each word, so the final output is a tree, whereas semantic role labeling allows multiple predicates for each argument. Thus, not all dependency parsing algorithms, such as a maximum spanning tree algorithm (Mcdonald and Pereira, 2006), can be naively applied to semantic role labeling. Some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results (Henderson et al., 2008; Titov et al., 2009). However, these algorithms are originally designed for dependency parsing, so are not necessarily customized for semantic role label38 ing. Here, we present a novel transition-based algorithm dedicated to semantic role labeling. The key difference between this algorithm and most other transition-based algorithms is in its directionality. Given an identified predicate, this algorith"
W11-0906,J08-4003,0,0.0300517,"e takes less time than training them as a joint-inference task. However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (Hsieh et al., 2008). Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing). This motivates the development of a new semantic role labeling algorithm that treats these two steps as a joint inference task. Our algorithm is inspired by shift-reduce parsing (Nivre, 2008). The algorithm uses several transitions to identify predicates and their arguments with semantic roles. One big advantage of the transitionbased approach is that it can use previously identified arguments as features to predict the next argument. We apply this technique to our approach and achieve comparable results to another state-of-theart system evaluated on the same data sets. 37 Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 37–45, c Portland, Oregon, USA, June 23, 2011. 2011 Association for Computational Linguistics NO-PRED SHIFT NO-ARC← NO-A"
W11-0906,J05-1004,1,0.336784,"Missing"
W11-0906,J08-2006,0,0.0229608,"rate classifiers for identification and classification, which did not lead to better performance in our case. Table 2 shows parsing states generated by our algorithm. Our experiments show that this algorithm gives comparable results against another state-ofthe-art system. 3 want buy A0 A1 1 1 1 1 ... john:A0 to:A1 car:A1 ... 0s 0s 1 1 1 0 0 1 0s 0s Figure 2: Projecting the predicate argument structure of each verb into vector space. Predicate argument clustering Some studies showed that verb clustering information could improve performance in semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2008). This is because semantic role labelers usually perform worse on verbs not seen during training, for which the clustering information can provide useful features. Most previous studies used either bag-ofwords or syntactic structure to cluster verbs; however, this may or may not capture the nature of predicate argument structure, which is more semantically oriented. Thus, it is preferable to cluster verbs by their predicate argument structures to get optimized features for semantic role labeling. In this section, we present a self-learning clustering technique that effectively improves labelin"
W11-0906,scheible-2010-evaluation,0,0.0620201,"Missing"
W11-0906,D07-1002,0,0.076197,"ing clustering technique that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 1 Introduction Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). Lately, dependency-based SRL has shown advantages over constituent-based SRL (Johansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or d"
W11-0906,W04-3212,1,0.934216,"tuent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002). This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (Hsieh et al., 2008). Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing). This motivates the development of a new semantic role labeling algorithm that treats these two steps as a joint inference task. Our algorith"
W11-0906,D08-1059,0,0.0715103,"Missing"
W11-0906,D09-1004,0,0.0145657,"ied as an argument of wj with a label L. These transitions can be performed in any order as long as their preconditions are satisfied. For our experiments, we use the following generalized sequence: ← [ (NO-PRED)∗ ⇒ (LEFT-ARC← L |N O -A RC )∗ ⇒ → (RIGHT-ARC→ L |N O -A RC )∗ ⇒ S HIFT ]∗ By adding the NO-ARC transitions, we successfully merge these two steps together without decrease in labeling accuracy.1 Since each word can be a predicate candidate and each predicate considers all other words as argument candidates, a worst-case complexity of the algorithm is O(n2 ). To reduce the complexity, Zhao et al. (2009) reformulated a pruning algorithm introduced by Xue and Palmer (2004) for dependency structure by considering only direct dependents of a predicate and its ancestors as argument candidates. This pruning algorithm can be easily applied to our algorithm: the oracle can prefilter such dependents and uses the information to perform NO-ARC transitions without consulting statistical models. 1 Notice that this algorithm does not take separate steps for argument identification and classification. 39 We also experimented with the traditional approach of building separate classifiers for identification"
W11-0906,N07-1070,0,\N,Missing
W11-0906,W05-0620,0,\N,Missing
W11-3801,P11-2121,1,0.853282,"형태소 분석기: http://www.sejong.or.kr/ 6 The reason we use outputs from two different systems is to compare the impact of fine vs. coarsegrained morphologies on dependency parsing in Korean. I MA gives not only richer POS tags but also more fine-grained (segmented) morphemes than Mach. We hypothesize that a richer morphology does not necessarily provide better features for dependency parsing. We evaluate our hypothesis by comparing parsing models trained on morphemes and POS tags generated by these two systems. 5 Dependency parsing 5.1 Parsing algorithm To build statistical parsing models, we use Choi and Palmer (2011)’s transition-based dependency parsing approach, which has shown state-of-the-art performance in English and Czech. The key idea of this approach is to combine transitions from projective and non-projective dependency parsing algorithms so it can perform projective and non-projective parsing accordingly. As a result, it shows an expected linear time parsing speed for generating both projective and non-projective dependency trees. Our algorithm uses three lists: λ1 , λ2 , and β. λ1,2 contain tokens that have been processed and β contains tokens that have not been processed by the algorithm. For"
W11-3801,W10-1406,0,0.105991,"nd their heads from the rightmost children, which aligns with the general concept of Korean being a head-final language. Note that these headrules do not involve the POS tags in Table 1; those POS tags are used only for morphemes within tokens (and each token is annotated with a phrase-level tag). It is possible to extend the headrules to token-level and find the head morpheme of each token; however, finding dependencies between different morphemes within a token is not especially interesting although 4 there are some approaches that have treated each morpheme as an individual token to parse (Chung et al., 2010).5 S Q NP VP VNP AP DP IP X|L|R r l r r r r r r r VP;VNP;S;NP|AP;Q;* S|VP|VNP|NP;Q;* NP;S;VP;VNP;AP;* VP;VNP;NP;S;IP;* VNP;NP;S;* AP;VP;NP;S;* DP;VP;* IP;VNP;* * Table 3: Head-percolation rules for the Sejong Treebank. l/r implies looking for the leftmost/rightmost constituent. * implies any phrase-level tag. |implies a logical OR and ; is a delimiter between tags. Each rule gives higher precedence to the left (e.g., S takes the highest precedence in VP). Once we have the headrules, it is pretty easy to generate dependency trees from constituent trees. For each phrase (or clause) in a constitu"
W11-3801,han-etal-2000-handling,1,0.661818,"ded chunking for parsing and conditional random fields for learning. Our work is distinguished from theirs in mainly two ways. First, we add labels to dependency edges during the conversion, so parsing performance can be evaluated on both labels and edges. Second, we selectively choose morphemes useful for dependency parsing, which prevents generating very sparse features. The morpheme selection is done automatically by applying our linguistically motivated rules (cf. Section 5.3). 3 The system is not publicly available but can be requested from the Sejong project (http://www.sejong.or.kr). 3 Han et al. (2000) presented an approach for handling structural divergence and recovering dropped arguments in a Korean-to-English machine translation system. In their approach, they used a Korean dependency parser for lexico-structural processing. 3 Constituent-to-dependency conversion 3.1 Constituent trees in the Sejong Treebank The Sejong Treebank contains constituent trees similar to ones in the Penn Treebank.4 Figure 2 shows a constituent tree and morphological analysis for a sentence, She still loved him, in Korean. S NP-SBJ VP AP 그녀는 She 여전히 still VP NP-OBJ VP 그를 him 사랑했다 loved 그녀는 → 그녀(she)/NP+는/JX 여전히"
W11-3801,W07-2416,0,0.211438,"However, there is a Treebank, called the Sejong Treebank1 , containing a large number of constituent trees in Korean (about 60K sentences), 1 http://www.sejong.or.kr/eindex.php 1 Proceedings of the 2nd Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 1–11, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical features for dependency parsing under an assumption that each token consists of a single POS tag. This assumption is only partially true in Korean; a token can consist of a sequence of morph"
W11-3801,N10-1095,0,0.0500215,"Missing"
W11-3801,J93-2004,0,0.037206,"ependency parsing. To perform statistical dependency parsing, we need sufficiently large training data. There is not much training data available for dependency structure in Korean. However, there is a Treebank, called the Sejong Treebank1 , containing a large number of constituent trees in Korean (about 60K sentences), 1 http://www.sejong.or.kr/eindex.php 1 Proceedings of the 2nd Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 1–11, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical fe"
W11-3801,de-marneffe-etal-2006-generating,0,0.0911649,"Missing"
W11-3801,P05-1012,0,0.031882,"MRL 2011), pages 1–11, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical features for dependency parsing under an assumption that each token consists of a single POS tag. This assumption is only partially true in Korean; a token can consist of a sequence of morphemes with different POS tags.2 말한다 talk (verb) ➔ 말/NNG talk (noun) 한/XSV 다/EF do ending_marker Figure 1: Morphological analysis of a verb talk in Korean. P OS tags are described in Table 1. grained morphology does not necessarily mean a better morphology for parsing"
W11-3801,P08-1108,0,0.0644132,"Missing"
W11-3801,P05-1013,0,0.0469165,"010)). A dependency tree generated by this procedure is guaranteed to be well-formed (unique root, single head, connected, and acyclic); however, it does not include labels yet. Section 3.3 shows how to add dependency labels to these trees. In addition, Section 3.4 describes heuristics to resolve some of the special cases (e.g., coordinations, nested function tags). It is worth mentioning that constituent trees in the Sejong Treebank do not include any empty categories. This implies that dependency trees generated by these headrules consist of only projective dependencies (non-crossing edges; Nivre and Nilsson (2005)). On the other hand, the Penn Korean Treebank contains empty categories representing longdistance dependencies. It will be interesting to see if we can train empty category insertion and resolution models on the Penn Korean Treebank, run the mod5 Chung et al. (2010) also showed that recovering certain kinds of null elements improves PCFG parsing, which can be applied to dependency parsing as well. els on the Sejong Treebank, and use the automatically inserted and linked empty categories to generate non-projective dependencies. 3.3 Dependency labels Two types of dependency labels are derived f"
W11-3801,J08-4003,0,0.019912,"c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical features for dependency parsing under an assumption that each token consists of a single POS tag. This assumption is only partially true in Korean; a token can consist of a sequence of morphemes with different POS tags.2 말한다 talk (verb) ➔ 말/NNG talk (noun) 한/XSV 다/EF do ending_marker Figure 1: Morphological analysis of a verb talk in Korean. P OS tags are described in Table 1. grained morphology does not necessarily mean a better morphology for parsing. For instance"
W11-3801,C02-1092,0,0.0791834,"dividual morphemes where each morpheme has its own POS tag. It is not clear which combination of these morphemes yields the best representation of the token for dependency parsing. Moreover, deriving joined features from multiple tokens (e.g., a joined feature of POS tags between two tokens) can be problematic; considering all combinations of morphemes within multiple tokens can be cumbersome and generate very sparse features. Obviously, having a good morphological analysis is very important for parsing. There are many automatic morphological analyzers available in Korean (Kang and Woo, 2001; Shim and Yang, 2002). Some of them use different kinds of morphologies better suited for their purposes. It is useful to have a fine-grained morphology; however, a more fine2 English words can consist of multiple morphemes as well (e.g., buying → buy/verb + ing/progressive suffix), but such morphology is usually not used in parsing. 2 Related work Marneffe et al. (2006) introduced a system for extracting typed dependencies from the Penn Treebank style constituent trees, known as the Stanford dependencies. Johansson and Nugues (2007) presented the LTH constituent-to-dependency converter that had been used to prepa"
W11-3801,W08-2121,0,0.0300793,"Missing"
W11-3801,W09-1201,0,\N,Missing
W13-3517,N10-1061,0,0.151151,"es iteratively applied to identify the chains, such as Haghighi and Klein (2009), Raghunathan et al. (2010), Stoyanov et al. (2010). Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in"
W13-3517,P98-1013,0,0.00899288,"irs. Coreference resolution forms an important component for natural language processing and information extraction pipelines due to its utility in relation extraction, cross-document coreference, text summarization, and question answering. The task of coreference is challenging for automated systems as the local information contained in the document is often not enough to accurately disambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of men"
W13-3517,D08-1031,0,0.287666,"stem (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, head string match, head substring match, head word pai"
W13-3517,D07-1074,0,0.0791348,"rs represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links ins"
W13-3517,N07-1011,1,0.351235,"nts in the development set. The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP2 tagger to obtain the POS tags. The hyperparameters of our system, such as regularization, initial number of candidates, and the number of compar2 http://opennlp.apache.org/ isons during training (k in Section 2.3) are tuned on the development data when trained on the train set. The models we use to evaluate on the test data set are trained on the training and development sets, following the standard evaluation for coreference first used by Culotta et al. (2007). To provide the initial ranked list of entity candidates from Wikipedia, we query the KB Bridge system (Dalton and Dietz, 2013) with the proper name mentions. KB Bridge is an information-retrievalbased entity linking system that connects the query mentions to Wikipedia entities using a sequential dependence model. This system has been shown to match or outperform the top performing systems in the 2012 TAC KBP entity linking task. 4.2 Methods Our experiments investigate a number of baselines that are similar or identical to existing approaches. Wikipedia Linking: As a simple baseline, we direc"
W13-3517,doddington-etal-2004-automatic,0,0.0111976,"ashington State does appear in the candidate entities of the first two mentions, albeit with a lower rank. In our approach, clustering the first two mentions causes the shared candidate Washington State to move to the top of the list. The coreference system is now able to easily identify that the “Washington State” mention is compatible with the Washington State entity formed by the previous two mentions, providing evidence that the final mention should be clustered with either of them in subsequent comparisons. 4 Experiments 4.1 Setup We evaluate our system on the ACE 2004 annotated dataset (Doddington et al., 2004). Following the setup in Bengston and Roth (2008), we split the corpus into training, development, and test sets, resulting in 268 documents in the train set, 107 documents in the test set, and 68 documents in the development set. The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP2 tagger to obtain the POS tags. The hyperparameters of our system, such as regularization, initial number of candidates, and the number of compar2 http://opennlp.apache.org/ isons during training (k in Section 2.3) are tuned on the developmen"
W13-3517,D09-1120,0,0.222949,"phrase mentions during training. We obtain B3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on this metric, suggesting that dynamic alignment is beneficial even when the features have not been engineered for events or for different genres. 6 Related Work Within-document coreference has been wellstudied for a number of years. A variety of approaches incorporate linguistic knowledge as rules iteratively applied to identify the chains, such as Haghighi and Klein (2009), Raghunathan et al. (2010), Stoyanov et al. (2010). Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improve"
W13-3517,P11-1055,0,0.051117,"and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO and FrameNet) into a cl"
W13-3517,W11-1902,0,0.0754511,"Missing"
W13-3517,P02-1014,0,0.297829,"outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, he"
W13-3517,P10-1142,0,0.041068,"Missing"
W13-3517,N06-1025,0,0.429605,"ften not enough to accurately disambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multipl"
W13-3517,W11-1901,0,0.0115112,"ur baseline is shown in Figure 3. Our static linking matches the performance of Ratinov and Roth (2012) on the non-transcripts. Further, the improvement of static linking on the transcripts over the baseline is lower than that on the non-transcript data, suggesting that noisy mentions and text result in poor quality alignment. Dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than OntoNotes We also run our systems on the OntoNotes dataset, which was used for evaluation in CoNLL 2011 Shared Task (Pradhan et al., 2011). The dataset consists of 2083 documents from a much larger variety of genres, such as conversations, magazines, web text, etc. Further, the dataset also consists of mentions that refer to events, most of which do not appear as Wikipedia pages. Since only the nonsingleton mentions are annotated in the training set, we also include additional noun phrase mentions during training. We obtain B3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on t"
W13-3517,D10-1048,0,0.807132,"approach stays fairly even as X is varied. Even though the experiments suggest that the larger documents are tougher to coreference,3 dynamic linking provides higher improvements when the documents contain a larger number of mentions. 5.2 Performance on Transcripts The quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document. In particular, 158 3 i.e., the absolute values are lower for these splits. The baseline system obtains 83.08, 79.29, 79.64, and 79.77 respectively for X = 10, 33, 40, 50. Method Culotta et al. (2007) Raghunathan et al. (2010) Stoyanov and Eisner (2012) Wiki-linking Bengston and Roth (2008) Baseline Static Linking Dynamic Linking Pairwise P/R F1 71.6 46.2 56.1 64.15 14.99 66.56 47.07 82.53 40.80 72.20 47.40 24.30 55.14 54.61 57.23 MUC P/R 80.4 71.8 74.41 82.7 82.84 88.39 85.07 28.39 69.9 72.02 66.93 72.02 F1 75.8 80.1 41.10 75.8 77.05 76.18 78.01 CEAF P/R 58.54 58.4 75.58 75.40 75.33 75.35 76.55 76.37 F1 58.47 75.49 75.44 76.46 B3 P/R 86.7 73.2 86.3 75.4 92.89 88.3 87.02 93.10 89.37 57.21 74.5 75.97 72.72 76.12 F1 79.3 80.4 81.8 70.81 80.8 81.12 81.66 82.21 Table 2: Evaluation on the ACE test data, with the system"
W13-3517,P11-1082,0,0.568436,"ctive recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge base, improving recall at the cost of lower precision; the attributes of all the linked entities are aggregated as features. Although this approach is more robust to noise in the documents, the features of a mention merge the different aspects of the entities, for example a “Michael Jordan” mention will contain features for both the scientist and basketball personas. Instead of fixing the alignment of the mentions to the knowledge base, our proposed approach maintains a ranked list of candidate entities for each mention. To expand the se"
W13-3517,D12-1113,0,0.675058,"isambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge"
W13-3517,P11-1138,0,0.0541057,"a. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in Ratinov and Roth (2012). Dalton and Dietz (2013) introduce an approximation to the above approach, but incorporate retrieval-based supervised reranking that provides multiple candidates and scores; this approach performed competitively on previous TAC-KBP entity linking benchmarks (Dietz and Dalton, 2012). Alignment to an external Conclusions A number of possible avenues for future study are apparent. First, our alignment to a knowledgebase can benefit from more document-aware"
W13-3517,D10-1099,1,0.101154,"Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO"
W13-3517,J01-4004,0,0.231353,"ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features men"
W13-3517,spitkovsky-chang-2012-cross,0,0.0297459,"he large set of surface string variations and constant reranking of the entity candidates during inference allows our approach to correct mistakes in alignment and makes external information applicable to a wider variety of mentions. Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system"
W13-3517,C12-1154,0,0.168314,"Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyano"
W13-3517,P10-2029,0,0.230993,", 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, head string match, head substring match, head word pair, mention substring mat"
W13-3517,C98-1013,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W16-3612,W15-4640,0,0.118764,"can either be pre-determined, inferred, or dynamically introduced ; however, a mention is usually linked to one pre-determined entity for entity linking. The context can be drawn from any kind of document where characters are present (e.g., dialogues, narratives, novels). This paper focuses on context extracted from multiparty conversation, especially from transcripts of TV shows. Entities, mainly the characters in the shows or the speakers in conversations, are predetermined due to the nature of the dialogue data. Instead of grabbing transcripts from the existing corpora (Janin et al., 2003; Lowe et al., 2015), TV shows are selected because they represent everyday conversation well, nonetheless they can very well be domain-specific depending on the plots and settings. Their contents and exchanges between characters are written for ease of comprehension. Prior knowledge regarding characters is usually not required and can be learned as show proceeds. Moreover, TV shows cover a variety of topics and are carried on over a long period of time by specific groups of people. The knowledge base can be either pre-populated or populated from the context. For the example in Figure 1, all the speakers can be i"
W16-3612,N16-1031,1,0.803525,"nown since you indicates a general human being. CoNLL’12 Shared Task Our corpus is reformatted to adapt the CoNLL’12 shared task on coreference resolution for the compatibility with the existing systems (Pradhan et al., 2012). Each statement is parsed into a constituent tree using the Berkeley Parser (Petrov et al., 2006), and tagged with named entities using the NLP4J Monica: (to Rachel) You1 do this, and you2 do that. You3 still end up with nothing. The case of you also results in another ambiguity when it is used as a filler: Ross: (to Chandler and Joey) You1 know, life is hard. 94 tagger (Choi, 2016). The CoNLL format allows speaker information for each statement, which is used by both systems we experiment with. The converted format preserves all necessary annotation for our task. 4.2 Coreference Evaluation Metrics All systems are evaluated with the official CoNLL scorer on three metrics concerning coreference resolution: MUC, B3 , and CEAFe . MUC Stanford Multi-Sieve System MUC (Vilain et al., 1995) concerns the number of pairwise links needed to be inserted or removed to map system responses to gold keys. The number of links the system and gold shared and minimum numbers of links neede"
W16-3612,H05-1004,0,0.14505,"operates between pairs of clusters unlike the previous model. Iteratively, it builds up entity-specific mention clusters using agglomerative clustering and imitation learning. This approach is particularly in alignment with our task, which finds groups of mentions referring to a centralized character. Furthermore, it allows new models to be trained with our corpus. This would give insight on whether our task can be learned by machines and whether a generalized model can be trained to distinguish speakers in all context. P (mi ) = |Smi ∩ Gmi | |Smi ∩ Gmi | , R(mi ) = |Smi | |Gmi | CEAFe CEAFe (Luo, 2005) metric further points out the drawback of B3 , in which entities can be used more than once during evaluation. As result, both multiple coreference chains of the same entity and chains with mentions of multiple entities are not penalized. To cope with this problem, CEAF evaluates only on the best one-to-one mapping between the system’s and gold’s entities. Given a system entity Si and gold entity Gj . An entity-based similarity metric φ(Si , Gj ) gives the count of common mentions that refer to both Si and Gj . The alignment with the best total similarity is denoted as Φ(g ∗ ). Thus precision"
W16-3612,P15-1136,0,0.0861299,"Missing"
W16-3612,niraula-etal-2014-dare,0,0.0919257,"Missing"
W16-3612,N15-1086,0,0.0911639,"Missing"
W16-3612,K15-1002,0,0.0554091,"Missing"
W16-3612,P06-1055,0,0.0070353,"/yourself might refer to. Such confusion often occurs during a multiparty conversation when one party attempts to give a general example using personal mentions that refer to no one in specific. For the following example, annotators label the you’s as Rachel although they should be labeled as unknown since you indicates a general human being. CoNLL’12 Shared Task Our corpus is reformatted to adapt the CoNLL’12 shared task on coreference resolution for the compatibility with the existing systems (Pradhan et al., 2012). Each statement is parsed into a constituent tree using the Berkeley Parser (Petrov et al., 2006), and tagged with named entities using the NLP4J Monica: (to Rachel) You1 do this, and you2 do that. You3 still end up with nothing. The case of you also results in another ambiguity when it is used as a filler: Ross: (to Chandler and Joey) You1 know, life is hard. 94 tagger (Choi, 2016). The CoNLL format allows speaker information for each statement, which is used by both systems we experiment with. The converted format preserves all necessary annotation for our task. 4.2 Coreference Evaluation Metrics All systems are evaluated with the official CoNLL scorer on three metrics concerning corefe"
W16-3612,W12-4501,0,0.0672299,"ctively. One common disagreement in annotation is caused by the ambiguity of speakers that you/your/yourself might refer to. Such confusion often occurs during a multiparty conversation when one party attempts to give a general example using personal mentions that refer to no one in specific. For the following example, annotators label the you’s as Rachel although they should be labeled as unknown since you indicates a general human being. CoNLL’12 Shared Task Our corpus is reformatted to adapt the CoNLL’12 shared task on coreference resolution for the compatibility with the existing systems (Pradhan et al., 2012). Each statement is parsed into a constituent tree using the Berkeley Parser (Petrov et al., 2006), and tagged with named entities using the NLP4J Monica: (to Rachel) You1 do this, and you2 do that. You3 still end up with nothing. The case of you also results in another ambiguity when it is used as a filler: Ross: (to Chandler and Joey) You1 know, life is hard. 94 tagger (Choi, 2016). The CoNLL format allows speaker information for each statement, which is used by both systems we experiment with. The converted format preserves all necessary annotation for our task. 4.2 Coreference Evaluation M"
W16-3612,W15-4615,0,0.0441265,"Missing"
W16-3612,P11-1138,0,0.0323352,"Missing"
W16-3612,J13-4004,0,0.039834,"rning coreference resolution: MUC, B3 , and CEAFe . MUC Stanford Multi-Sieve System MUC (Vilain et al., 1995) concerns the number of pairwise links needed to be inserted or removed to map system responses to gold keys. The number of links the system and gold shared and minimum numbers of links needed to describe coreference chains of the system and gold are computed. Precision is calculated by dividing the former with the latter that describes the system chains, and recall is calculated by dividing the former with the later that describes the gold chains. The Stanford multi-pass sieve system (Lee et al., 2013) is used to provide a baseline of how a coreference resolution system performs on our task. The system is composed of multiple sieves of linguistic rules that are in the orders of high-to-low precision and low-to-high recall. Information regarding mentions, such as plurality, gender, and parse tree, is extracted during mention detection and used as global features. Pairwise links between mentions are formed based on defined linguistic rules at each sieve in order to construct coreference chains and mention clusters. Although no machine learning is involved, the system offers efficiency in deco"
W16-3612,D13-1020,0,0.0233745,"cing. Our corpus comprises 543 scenes from two TV shows, and shows the inter-annotator agreement of κ = 79.96. For statistical modeling, this task is reformulated as coreference resolution, and experimented with a state-of-the-art system on our corpus. Our best model gives a purity score of 69.21 on average, which is promising given the challenging nature of this task and our corpus. 1 Introduction • Introducing a subtask of entity linking, called character identification (Section 2). Machine comprehension has recently become one of the main targeted challenges in natural language processing (Richardson et al., 2013; Hermann et al., 2015; Hixon et al., 2015). The latest approaches to machine comprehension show lots of promises; however, most of these approaches face difficulties in understanding information scattered across different parts of documents. Reading comprehension in dialogues is particularly hard because speakers take turns to form a conversation such that it often requires connecting mentions from multiple utterances together to derive meaningful inferences. Coreference resolution is a common choice for making connections between these mentions. However, most of the state-of-the-art corefere"
W16-3612,W99-0208,0,0.736887,"Missing"
W16-3612,M95-1005,0,0.774882,"Missing"
W16-3612,P15-1137,0,0.0508749,"Missing"
W17-4416,P13-1104,1,0.819465,"Missing"
W17-4416,P99-1057,0,0.314645,"Missing"
W17-5220,S16-1173,0,0.0278407,"Missing"
W17-5220,P11-2008,0,0.0282482,"Missing"
W17-5220,P14-1062,0,0.009271,"This success has engendered an upsurge in deep neural network research for sentiment analysis. Various modified models have been proposed in the literature. One of the famous deep learning methods that models a document is the generalized phrase proposed by Yin and Sch¨utze (2014), which represents a sentence using element-wise addition, multiplication, or recursive auto-encoder. Endeavors to capture n-gram information bore fruits with CNN, max pooling, and softmax (Collobert et al., 2011; Kim, 2014), which is regarded as the standard methods of the document classification problem these days. Kalchbrenner et al. (2014a) extended this standard CNN model with dynamic k-max pooling, which served as an input layer to another stacked convolution layer. Multichannel CNN methods (Kim, 2014; Yin and Sch¨utze, 2015) are another branch of CNN, where assorted embeddings are considered together when convolving the input. Unlike Kim (2014)’s model that relies on a single type of embedding with different mutability characteristics of the weights of embedding layer, Yin and Sch¨utze (2015) incorporates diverse sort of embedding types using multichannel CNN. Two notable pioneers in using lexicon for sentiment analysis are"
W17-5220,C04-1200,0,0.494774,"then, many kinds of feature learning methods had been introduced to increase the performance (Pang and 149 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–158 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Lee, 2008; Liu, 2012; Gimpel et al., 2011; Feldman, 2013; Mohammad et al., 2013b). Aside from pure machine learning approaches, lexicon based approaches had been another trend, which relied on the manual or algorithmic creation of word sentiment scores (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Taboada et al., 2011). Since the emergence of the Convolutional Neural Networks (CNN; Collobert et al. (2011)), conventional methods have become gradually obsolete because of the outstanding performance from the CNN variants. CNN based models are distinguished from earlier methods because they do not rely on laborious feature engineering. The first success of CNN in sentiment analysis was triggered by document classification research (Kim, 2014), where CNN showed state-of-the-art results in numerous document classification datasets. This success has engendered an upsurge i"
W17-5220,D14-1181,0,0.0461143,"Missing"
W17-5220,D15-1166,0,0.0413811,"e Mohammad et al. (2013a); Kalchbrenner et al. (2014b) generated scores with other manually generated sentiment lexicon scores to achieved the state-of-the-art result in SemEval2013 Twitter sentiment analysis task. In general domain, Hu and Liu (2004) generated a user review lexicon that showed promising result in capturing sentiment in customer product reviews. Attention based methods have been successful in many application domains, such as image classification (Stollenga et al., 2014), image caption generation (Xu et al., 2015), machine translation (Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015), and question answering (Shih et al., 2016; Chen et al., 2015; Yang et al., 2016). However, in the field of sentiment analysis, the attention is applied to only aspect-based sentiment classification (Yanase et al., 2016). To the best knowledge of ours, there is no attention-based model for a general sentiment analysis task. 3 Approach The models proposed here are based on a convolutional architecture and use naive concatenation (Section 3.2.1), multichannel (Section 3.2.2), separate convolution (Section 3.2.3), and embedding attention (Section 3.3) for the integration of lexicon embeddings to"
W17-5220,S13-2053,0,0.463839,"ysis. 2 Related Work The first attempt of sentiment analysis on text was initiated by Pang et al. (2002) who pioneered this field by using bag-of-word features. This work mostly hinged on feature engineering; since then, many kinds of feature learning methods had been introduced to increase the performance (Pang and 149 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–158 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Lee, 2008; Liu, 2012; Gimpel et al., 2011; Feldman, 2013; Mohammad et al., 2013b). Aside from pure machine learning approaches, lexicon based approaches had been another trend, which relied on the manual or algorithmic creation of word sentiment scores (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Taboada et al., 2011). Since the emergence of the Convolutional Neural Networks (CNN; Collobert et al. (2011)), conventional methods have become gradually obsolete because of the outstanding performance from the CNN variants. CNN based models are distinguished from earlier methods because they do not rely on laborious feature engineering. The first success of CNN in"
W17-5220,W04-3253,0,0.030231,"t systems. Our analysis shows that lexicon embeddings allow building high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis. 1 • Can lexicons be still useful for sentiment analysis when coupled with word embeddings? • If yes, what is the most effective way of incorporating lexicons with word embeddings? Introduction Sentiment analysis is a task of identifying sentiment polarities expressed in documents, typically positive, neutral, or negative. Although the task of sentiment analysis has been well-explored (Mullen and Collier, 2004; Pang and Lee, 2005; Wilson et al., 2005), it is still very challenging due to the complexity of extracting human emotion from raw text. The recent advance of deep learning has definitely elevated the performance of this task (Socher et al., 2013; Kim, 2014; Yin and Sch¨utze, 2015) although there is much more room to improve. In the traditional setting where statistical models are based on sparse features, lexicons consisting of words and their sentiment scores are shown to be highly effective for sentiment analysis because To answer these questions, we first construct lexicon embeddings that"
W17-5220,S16-1001,0,0.00795275,"mber of filters whose length is 1. 3. Execute max pooling for each row of the attention matrix sa , which generates the attention vector va ∈ Rn (Figure 2(a)). 4. Transpose the document matrix s such that sT ∈ Rd×n , and multiply it with the attention vector va ∈ Rn , which generates the embedding attention vector ve ∈ Rd (Figure 2(b)). generated from both word and lexicon embedding spaces for all of the three lexicon integration methods in Section 3.2. 4 Experiments 4.1 Corpora 4.1.1 SemEval-2016 Task 4 All models are evaluated on the micro-blog dataset distributed by the SemEval’16 Task 4a (Nakov et al., 2016). The dataset is gleaned from tweets with annotation of three sentiment classes: positive, neutral, and negative. The available dataset contains only tweet IDs and their sentiment polarities; the actual tweet texts are not included in this dataset due to the copyright restrictions. Although the download script provided by SemEval’16 gives a way of accessing the actual texts on the web, a portion of tweets is no longer accessible. To compensate this loss, the dataset also includes tweet instances from the SemEval’13 task. TRN DEV TST + 6,480 786 7,059 0 6,577 548 10,342 2,328 254 3,231 All 15,3"
W17-5220,P05-1015,0,0.19878,"ows that lexicon embeddings allow building high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis. 1 • Can lexicons be still useful for sentiment analysis when coupled with word embeddings? • If yes, what is the most effective way of incorporating lexicons with word embeddings? Introduction Sentiment analysis is a task of identifying sentiment polarities expressed in documents, typically positive, neutral, or negative. Although the task of sentiment analysis has been well-explored (Mullen and Collier, 2004; Pang and Lee, 2005; Wilson et al., 2005), it is still very challenging due to the complexity of extracting human emotion from raw text. The recent advance of deep learning has definitely elevated the performance of this task (Socher et al., 2013; Kim, 2014; Yin and Sch¨utze, 2015) although there is much more room to improve. In the traditional setting where statistical models are based on sparse features, lexicons consisting of words and their sentiment scores are shown to be highly effective for sentiment analysis because To answer these questions, we first construct lexicon embeddings that are specifically de"
W17-5220,W02-1011,0,0.0199906,"3.2). We then incorporate an efficient attention mechanism to our CNN models, which provides a global view of the document by emphasizing (or de-emphasizing) important words and lexicons (Section 3.3). Our models using lexicon embeddings are evaluated on two well-known datasets, the SemEval’16 dataset and the Stanford Sentiment Treebank, and show state-of-the-art results on both datasets (Section 4). To the best of our knowledge, this is the first time that lexicon embeddings are introduced for sentiment analysis. 2 Related Work The first attempt of sentiment analysis on text was initiated by Pang et al. (2002) who pioneered this field by using bag-of-word features. This work mostly hinged on feature engineering; since then, many kinds of feature learning methods had been introduced to increase the performance (Pang and 149 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–158 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Lee, 2008; Liu, 2012; Gimpel et al., 2011; Feldman, 2013; Mohammad et al., 2013b). Aside from pure machine learning approaches, lexicon based approaches had been"
W17-5220,S16-1030,0,0.0328209,"Missing"
W17-5220,D12-1110,0,0.0062622,"sections propose three methods for lexicon integration to the baseline CNN model (Section 3.1), which depict different characteristics depending on the peculiarities of each domain. Section 3.2 describes how lexicon embeddings can be incorporated into the CNN model in Section 3.1. Each CNN model uses several filters with different lengths; given the filter length l, the convolution considers l-gram features. However, these l-gram features account only for local views, not the global view of the document, which is necessary for several transitional cases such as negation in sentiment analysis (Socher et al., 2012). To ameliorate this issue, we introduce the embedding attention vector (EAV), which transforms the document matrix in each embedding space into a vector. For example, the EAV in the word embedding space is calculated as a weighted sum of each column in the document matrix s ∈ Rn×d , which yields a vector v ∈ Rd . For each document, two EAVs can be derived, one 3.2.1 Naive Concatenation The simplest way of blending a lexicon embedding into its corresponding word embedding is to append 151 Embedding Attention from the document matrix consisting of word embeddings and the other from the one cons"
W17-5220,D13-1170,0,0.0498559,"timent analysis when coupled with word embeddings? • If yes, what is the most effective way of incorporating lexicons with word embeddings? Introduction Sentiment analysis is a task of identifying sentiment polarities expressed in documents, typically positive, neutral, or negative. Although the task of sentiment analysis has been well-explored (Mullen and Collier, 2004; Pang and Lee, 2005; Wilson et al., 2005), it is still very challenging due to the complexity of extracting human emotion from raw text. The recent advance of deep learning has definitely elevated the performance of this task (Socher et al., 2013; Kim, 2014; Yin and Sch¨utze, 2015) although there is much more room to improve. In the traditional setting where statistical models are based on sparse features, lexicons consisting of words and their sentiment scores are shown to be highly effective for sentiment analysis because To answer these questions, we first construct lexicon embeddings that are specifically designed for sentiment analysis and integrate them into the existing Convolutional Neural Network (CNN) model similar to Kim (2014). Three ways of lexicon integration to the CNN model are proposed, which show distinctive characte"
W17-5220,J11-2001,0,0.143539,"ethods had been introduced to increase the performance (Pang and 149 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–158 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Lee, 2008; Liu, 2012; Gimpel et al., 2011; Feldman, 2013; Mohammad et al., 2013b). Aside from pure machine learning approaches, lexicon based approaches had been another trend, which relied on the manual or algorithmic creation of word sentiment scores (Hu and Liu, 2004; Kim and Hovy, 2004; Ding et al., 2008; Taboada et al., 2011). Since the emergence of the Convolutional Neural Networks (CNN; Collobert et al. (2011)), conventional methods have become gradually obsolete because of the outstanding performance from the CNN variants. CNN based models are distinguished from earlier methods because they do not rely on laborious feature engineering. The first success of CNN in sentiment analysis was triggered by document classification research (Kim, 2014), where CNN showed state-of-the-art results in numerous document classification datasets. This success has engendered an upsurge in deep neural network research for sentime"
W17-5220,H05-1044,0,0.197231,"eddings allow building high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis. 1 • Can lexicons be still useful for sentiment analysis when coupled with word embeddings? • If yes, what is the most effective way of incorporating lexicons with word embeddings? Introduction Sentiment analysis is a task of identifying sentiment polarities expressed in documents, typically positive, neutral, or negative. Although the task of sentiment analysis has been well-explored (Mullen and Collier, 2004; Pang and Lee, 2005; Wilson et al., 2005), it is still very challenging due to the complexity of extracting human emotion from raw text. The recent advance of deep learning has definitely elevated the performance of this task (Socher et al., 2013; Kim, 2014; Yin and Sch¨utze, 2015) although there is much more room to improve. In the traditional setting where statistical models are based on sparse features, lexicons consisting of words and their sentiment scores are shown to be highly effective for sentiment analysis because To answer these questions, we first construct lexicon embeddings that are specifically designed for sentiment a"
W17-5220,S16-1046,0,0.015084,"neral domain, Hu and Liu (2004) generated a user review lexicon that showed promising result in capturing sentiment in customer product reviews. Attention based methods have been successful in many application domains, such as image classification (Stollenga et al., 2014), image caption generation (Xu et al., 2015), machine translation (Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015), and question answering (Shih et al., 2016; Chen et al., 2015; Yang et al., 2016). However, in the field of sentiment analysis, the attention is applied to only aspect-based sentiment classification (Yanase et al., 2016). To the best knowledge of ours, there is no attention-based model for a general sentiment analysis task. 3 Approach The models proposed here are based on a convolutional architecture and use naive concatenation (Section 3.2.1), multichannel (Section 3.2.2), separate convolution (Section 3.2.3), and embedding attention (Section 3.3) for the integration of lexicon embeddings to CNN. 3.1 Baseline Our baseline approach is a one-layer CNN model using pre-trained word embeddings, which is a reimplementation of the CNN model introduced by Kim (2014). Let s ∈ Rn×d be a matrix representing the input d"
W17-5220,P14-3006,0,0.0442325,"Missing"
W17-5220,K15-1021,0,0.0157621,"Missing"
W17-5407,N15-1086,0,0.0397933,"Missing"
W17-5407,P16-1091,0,0.0433219,"Missing"
W17-5407,D14-1181,0,0.00438469,"Missing"
W17-5407,N15-1046,0,0.0637144,"Missing"
W17-5407,W16-3612,1,0.789545,"ristics are applied to extract meaningful contextual words by traversing the subtree of the argument. Our heuristics are designed for the type of dependency trees generated by NLP4J, but similar rules can be generalized to other types of dependency trees. Relations from dialogues are attached with the speaker names to compensate the lack of entity information. The Character Mining project provides transcripts of the TV show, Friends; transcripts from 8 seasons of the show are publicly available in the JSON format,1 where the first 2 seasons are annotated for the character identification task (Chen and Choi, 2016). Each season consists of episodes, each episode contains scenes, each scene includes utterances, where each utterance comes with the speaker information. For each episode, the episode summary and plot are first collected from fan sites,2 then sentence segmented by NLP4J,3 the same tool used for the provided transcripts. Generally, summaries give broad descriptions of the episodes, whereas plots describe facts within individual scenes. Finally, we create a dataset by treating each sentence as a query and its relevant episode as the target document. Table 2 shows the distributions of this datas"
W17-5407,W14-4318,0,0.0666382,"Missing"
W17-5407,W03-2113,0,0.0205087,"Missing"
W17-5407,W12-1642,0,0.0204295,"nd target documents. Due to the spike of applications that are required to maintain the conversation, dialog data has re2 Related work Information extraction for dialogue data has already been widely explored. Yoshino et al. (2011) presented a spoken dialogue system that extracts predicate-argument structures and uses them to extract facts from news documents. Flycht-Eriksson and J¨onsson (2003) developed a dialogue interaction process of accessing textual data from a bird encyclopedia. An unsupervised technique for meeting summarization using decision-related utterances has been presented by Wang and Cardie (2012). Gorinski and Lapata (2015) studied movie script summarization. All the aforementioned work uses the syntactic and semantic relation extraction and thus is similar to ours; however, it is distinguished in a way that it lacks a cross-genre aspect. 48 Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 48–53 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Joey Ross Rachel Chandler Joey Dialogue One woman? That’s like saying there’s only one flavor of ice cream for you. Lemme tell you something, Ross. There’s lots"
W17-5407,N15-1113,0,0.0295319,"to the spike of applications that are required to maintain the conversation, dialog data has re2 Related work Information extraction for dialogue data has already been widely explored. Yoshino et al. (2011) presented a spoken dialogue system that extracts predicate-argument structures and uses them to extract facts from news documents. Flycht-Eriksson and J¨onsson (2003) developed a dialogue interaction process of accessing textual data from a bird encyclopedia. An unsupervised technique for meeting summarization using decision-related utterances has been presented by Wang and Cardie (2012). Gorinski and Lapata (2015) studied movie script summarization. All the aforementioned work uses the syntactic and semantic relation extraction and thus is similar to ours; however, it is distinguished in a way that it lacks a cross-genre aspect. 48 Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 48–53 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Joey Ross Rachel Chandler Joey Dialogue One woman? That’s like saying there’s only one flavor of ice cream for you. Lemme tell you something, Ross. There’s lots of flavors out there. You k"
W17-5407,P16-1194,0,0.0241052,"Missing"
W17-5407,P16-1051,0,0.0269556,"Missing"
W17-5407,W11-2008,0,0.0135024,", proper ranking of the retrieved documents can significantly improve the overall user satisfation by putting more relevant documents at the top (Bali´nski and Daniłowicz, 2005; Yang et al., 2006; Zhou and Wade, 2009). Many previous works provide strong baselines for unstructured text retrieval and ranking problems; however, these systems usually assume a homogeneous domain for queries and target documents. Due to the spike of applications that are required to maintain the conversation, dialog data has re2 Related work Information extraction for dialogue data has already been widely explored. Yoshino et al. (2011) presented a spoken dialogue system that extracts predicate-argument structures and uses them to extract facts from news documents. Flycht-Eriksson and J¨onsson (2003) developed a dialogue interaction process of accessing textual data from a bird encyclopedia. An unsupervised technique for meeting summarization using decision-related utterances has been presented by Wang and Cardie (2012). Gorinski and Lapata (2015) studied movie script summarization. All the aforementioned work uses the syntactic and semantic relation extraction and thus is similar to ours; however, it is distinguished in a w"
W17-5407,D09-1163,0,0.0269311,"ing. 1 Introduction Document retrieval has been a central task in natural language processing and information retrieval. The goal is to match a query against a set of documents. Over the last decade, advanced techniques have emerged and provided powerful systems that can accurately retrieve relevant documents (Blair and Maron, 1985; Callan, 1994; Cao et al., 2006). While the retrieval part is crucial, proper ranking of the retrieved documents can significantly improve the overall user satisfation by putting more relevant documents at the top (Bali´nski and Daniłowicz, 2005; Yang et al., 2006; Zhou and Wade, 2009). Many previous works provide strong baselines for unstructured text retrieval and ranking problems; however, these systems usually assume a homogeneous domain for queries and target documents. Due to the spike of applications that are required to maintain the conversation, dialog data has re2 Related work Information extraction for dialogue data has already been widely explored. Yoshino et al. (2011) presented a spoken dialogue system that extracts predicate-argument structures and uses them to extract facts from news documents. Flycht-Eriksson and J¨onsson (2003) developed a dialogue interac"
W18-6009,L18-1287,1,0.271859,"fundamental issues due to hypotactic attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic t"
W18-6009,W17-6508,0,0.118231,"Missing"
W18-6009,W11-3801,1,0.783345,"the complexities outlined in the previous section, the UD Japanese and UD Korean teams had to work within the bounds of the principles laid out by the Universal Dependencies version 2. Therefore, in the official version 2.2 release used for the CoNLL 2018 shared task (Zeman et al., 2018), UD Japanese and UD Korean adopted two separate strategies in order to ensure compliance, 79 the verb ‘run’ with the (nsubj) label. 4.2 UD Korean Unlike the Japanese UD, the Korean UD effort has made a conscious decision to use right-headedness for conjunction following the coordination guidelines proposed by Choi and Palmer (2011). Thus, the coordinate structures in all three of the Korean UD corpora (Chun et al., 2018) were developed with the rightmost conjunct as the head of the phrase, with each conjunct pointing to its right sibling as its head. For the latest available UD_Korean-GSD, however, the dependencies were converted to leftheaded structures post-development in an effort to fully comply with the UD guidelines despite the problems left-headed structures pose for the language as described in Section 3. The other two Korean UD corpora, namely the Kaist and the Korean Penn Treebank, reflect right-headed coordin"
W18-6009,L18-1347,1,0.849927,"attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept"
W18-6009,de-marneffe-etal-2014-universal,0,0.118166,"Missing"
W18-6009,W15-2113,0,0.550814,"Missing"
W18-6009,W14-4202,1,0.883365,"Missing"
W18-6009,C18-1324,0,0.0259098,"on scheme the conjunctive particle かわいい ⽝ と 猫 が ⾛る “와” (wa) is kept suffixized in the left nominal conkawaii inu -to neko -ga hashiru junct eojeol, thus the conjunction relation cc is not ADJ NOUN CCONJ NOUN ADP VERB overtly marked. ‘cute’ ‘dog’ ‘and’ ‘cat’ -NOM ‘run’ A common problem with adjectival modification in UD shown in Figures 4 and 5 is that there Figure 4: Left-headed representation of a nominal cois no way to distinguish between modification ordination in Japanese “⽝と猫” (‘dog and cat’), in a of the full coordination vs. of the first conjunct sentence “かわいい⽝と猫が⾛る” (‘A cute dog and (Przepiórkowski and Patejuk, 2018) . For examcat run’). root ple, there is no way to specify the scope of the adjective ‘cute’: the two readings (1) only a dog is nsubj cute and (2) both animals are cute. acl conj 3.2 Verbal coordination 예쁜 개와 고양이가 달린다 yeyppun kay+wa koyangi+ka tali+nta ADJ NOUN NOUN VERB ‘cute’ ‘dog+and’ ‘cat-NOM’ ‘run’ Further critical issues are attested in the verbal coordinate structures. Figure 6 shows the left-headed verbal coordination “⾷べて⾛る” (‘eat and run’) in a noun phrase “⾷べて⾛る⼈” (‘a person who eats and runs’), where verb “⾷べ” (‘eat’) is the child of “⼈” (‘person’). Despite this dependency relatio"
W18-6009,L16-1376,0,0.0419402,"Missing"
W18-6009,L16-1680,0,0.017083,"on strictly reduces the dependency distance in head-final languages. These results support the advantages of the rightheaded strategy in Japanese coordinate structures. nsubj 오바마 대통령이 말한다 obama taythonglyeng+i malha+nta PROPN NOUN VERB ‘Obama’ ‘president-NOM’ ‘say’ (a) Korean right-headed flat structure. root nsubj flat 오바마 대통령이 말한다 obama taythonglyeng+i malha+nta PROPN NOUN VERB ‘Obama’ ‘president-NOM’ ‘say’ (b) (a) converted to left-headed structure as reflected in the UD_Korean-GSD. Figure 12: The use of flat in Korean UD v2.2. guages? To answer this question, we trained and tested UDPipe (Straka et al., 2016) on multiple versions of UD Japanese and Korean corpora. 5.1 Japanese As described in Section 4.1, the current UD Japanese-GSD corpus does not use conj tags. The corpus was converted into another version with coordinations without changing the dependency structures (right-headed coordination), that is, some of nmod and advcl labels are converted into conj label when the original manual annotation used conj regarding them as nominal or verbal coordinations. Also CCONJ tag and cc label are assigned to the coordinative case markers. The corpus was further converted into left-headed coordination,"
W18-6009,L16-1261,1,0.697171,"structure poses some fundamental issues due to hypotactic attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline"
W19-3309,W13-2322,0,0.0382621,"on the literals (e.g., the Circus Procession in Figure 2, McLoughlin Brothers, 1888, N.Y.) to see if subjects indeed recognize them correctly. Any object that is either ambiguous or not predefined is annotated as an unknown entity. It is worth mentioning that unlike mention annotation for coreference resolution in OntoNotes (Pradhan et al., 2012) where whole noun phrases are annotated as mentions, function words such as articles or determiners and modifiers such as adverbs or adjectives are not considered part of mentions in our annotation, which is similar to abstract meaning representation (Banarescu et al., 2013). Such abstraction is more suitable for spoken data where the usage of these function words and modifiers is not so consistent. Once predicates are identified, arguments are annotated with the following thematic roles (in the examples, predicates are in italic, arguments are in brackets, and thematic roles are in subscripts): 3.2 If an argument is a preposition phrase, the thematic role is annotated on the preposition object such that in the example above, only the head noun [hat] is annotated as dir instead of the entire preposition phrase “out of the hat”.4 As shown in the prp example, a pre"
W19-3309,W12-4501,0,0.0179229,"group entity is defined because subjects regularly describe them together as one unit. Picture often refers to the types of the picture that subjects view it as (e.g., poster in Figure 2). Special kinds of entities, Title and Copyright, are also defined that are annotated on the literals (e.g., the Circus Procession in Figure 2, McLoughlin Brothers, 1888, N.Y.) to see if subjects indeed recognize them correctly. Any object that is either ambiguous or not predefined is annotated as an unknown entity. It is worth mentioning that unlike mention annotation for coreference resolution in OntoNotes (Pradhan et al., 2012) where whole noun phrases are annotated as mentions, function words such as articles or determiners and modifiers such as adverbs or adjectives are not considered part of mentions in our annotation, which is similar to abstract meaning representation (Banarescu et al., 2013). Such abstraction is more suitable for spoken data where the usage of these function words and modifiers is not so consistent. Once predicates are identified, arguments are annotated with the following thematic roles (in the examples, predicates are in italic, arguments are in brackets, and thematic roles are in subscripts"
W19-3309,N18-2110,0,0.029905,"Missing"
W19-3309,E12-2021,0,0.112576,"Missing"
