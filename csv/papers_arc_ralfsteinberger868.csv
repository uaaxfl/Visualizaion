W19-3714,{JRC} {TMA}-{CC}: {S}lavic Named Entity Recognition and Linking. Participation in the {BSNLP}-2019 shared task,2019,0,0,4,1,6082,guillaume jacquet,Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing,0,"We report on the participation of the JRC Text Mining and Analysis Competence Centre (TMA-CC) in the BSNLP-2019 Shared Task, which focuses on named-entity recognition, lemmatisation and cross-lingual linking. We propose a hybrid system combining a rule-based approach and light ML techniques. We use multilingual lexical resources such as JRC-NAMES and BABELNET together with a named entity guesser to recognise names. In a second step, we combine known names with wild cards to increase recognition recall by also capturing inflection variants. In a third step, we increase precision by filtering these name candidates with automatically learnt inflection patterns derived from name occurrences in large news article collections. Our major requirement is to achieve high precision. We achieved an average of 65{\%} F-measure with 93{\%} precision on the four languages."
W17-1702,Multi-word Entity Classification in a Highly Multilingual Environment,2017,15,2,3,0,31705,sophie chesney,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"This paper describes an approach for the classification of millions of existing multi-word entities (MWEntities), such as organisation or event names, into thirteen category types, based only on the tokens they contain. In order to classify our very large in-house collection of multilingual MWEntities into an application-oriented set of entity categories, we trained and tested distantly-supervised classifiers in 43 languages based on MWEntities extracted from BabelNet. The best-performing classifier was the multi-class SVM using a TF.IDF-weighted data representation. Interestingly, one unique classifier trained on a mix of all languages consistently performed better than classifiers trained for individual languages, reaching an averaged F1-value of 88.8{\%}. In this paper, we present the training and test data, including a human evaluation of its accuracy, describe the methods used to train the classifiers, and discuss the results."
steinberger-etal-2017-large,Large-scale news entity sentiment analysis,2017,0,0,1,1,24362,ralf steinberger,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"We work on detecting positive or negative sentiment towards named entities in very large volumes of news articles. The aim is to monitor changes over time, as well as to work towards media bias detection by com-paring differences across news sources and countries. With view to applying the same method to dozens of languages, we use lin-guistically light-weight methods: searching for positive and negative terms in bags of words around entity mentions (also consid-ering negation). Evaluation results are good and better than a third-party baseline sys-tem, but precision is not sufficiently high to display the results publicly in our multilin-gual news analysis system Europe Media Monitor (EMM). In this paper, we focus on describing our effort to improve the English language results by avoiding the biggest sources of errors. We also present new work on using a syntactic parser to identify safe opinion recognition rules, such as predica-tive structures in which sentiment words di-rectly refer to an entity. The precision of this method is good, but recall is very low."
L16-1084,Cross-lingual Linking of Multi-word Entities and their corresponding Acronyms,2016,24,1,3,1,6082,guillaume jacquet,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper reports on an approach and experiments to automatically build a cross-lingual multi-word entity resource. Starting from a collection of millions of acronym/expansion pairs for 22 languages where expansion variants were grouped into monolingual clusters, we experiment with several aggregation strategies to link these clusters across languages. Aggregation strategies make use of string similarity distances and translation probabilities and they are based on vector space and graph representations. The accuracy of the approach is evaluated against Wikipedia{'}s redirection and cross-lingual linking tables. The resulting multi-word entity resource contains 64,000 multi-word entities with unique identifiers and their 600,000 multilingual lexical variants. We intend to make this new resource publicly available."
W14-1309,Experiments to Improve Named Entity Recognition on {T}urkish Tweets,2014,23,25,2,0,38764,dilek kuccuk,Proceedings of the 5th Workshop on Language Analysis for Social Media ({LASM}),0,"Social media texts are significant information sources for several application areas including trend analysis, event monitoring, and opinion mining. Unfortunately, existing solutions for tasks such as named entity recognition that perform well on formal texts usually perform poorly when applied to social media texts. In this paper, we report on experiments that have the purpose of improving named entity recognition on Turkish tweets, using two different annotated data sets. In these experiments, starting with a baseline named entity recognition system, we adapt its recognition rules and resources to better fit Twitter language by relaxing its capitalization constraint and by diacritics-based expansion of its lexical resources, and we employ a simplistic normalization scheme on tweets to observe the effects of these on the overall named entity recognition performance on Turkish tweets. The evaluation results of the system with these different settings are provided with discussions of these results."
kucuk-etal-2014-named,Named Entity Recognition on {T}urkish Tweets,2014,19,19,3,0,38764,dilek kuccuk,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Various recent studies show that the performance of named entity recognition (NER) systems developed for well-formed text types drops significantly when applied to tweets. The only existing study for the highly inflected agglutinative language Turkish reports a drop in F-Measure from 91{\%} to 19{\%} when ported from news articles to tweets. In this study, we present a new named entity-annotated tweet corpus and a detailed analysis of the various tweet-specific linguistic phenomena. We perform comparative NER experiments with a rule-based multilingual NER system adapted to Turkish on three corpora: a news corpus, our new tweet corpus, and another tweet corpus. Based on the analysis and the experimentation results, we suggest system features required to improve NER results for social media like Twitter."
pajzs-etal-2014-media,Media monitoring and information extraction for the highly inflected agglutinative language {H}ungarian,2014,23,0,2,0,39645,julia pajzs,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Europe Media Monitor (EMM) is a fully-automatic system that analyses written online news by gathering articles in over 70 languages and by applying text analysis software for currently 21 languages, without using linguistic tools such as parsers, part-of-speech taggers or morphological analysers. In this paper, we describe the effort of adding to EMM Hungarian text mining tools for news gathering; document categorisation; named entity recognition and classification for persons, organisations and locations; name lemmatisation; quotation recognition; and cross-lingual linking of related news clusters. The major challenge of dealing with the Hungarian language is its high degree of inflection and agglutination. We present several experiments where we apply linguistically light-weight methods to deal with inflection and we propose a method to overcome the challenges. We also present detailed frequency lists of Hungarian person and location name suffixes, as found in real-life news texts. This empirical data can be used to draw further conclusions and to improve existing Named Entity Recognition software. Within EMM, the solutions described here will also be applied to other morphologically complex languages such as those of the Slavic language family. The media monitoring and analysis system EMM is freely accessible online via the web page http://emm.newsbrief.eu/overview.html."
jacquet-etal-2014-clustering,Clustering of Multi-Word Named Entity variants: Multilingual Evaluation,2014,15,4,3,1,6082,guillaume jacquet,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Multi-word entities, such as organisation names, are frequently written in many different ways. We have previously automatically identified over one million acronym pairs in 22 languages, consisting of their short form (e.g. EC) and their corresponding long forms (e.g. European Commission, European Union Commission). In order to automatically group such long form variants as belonging to the same entity, we cluster them, using bottom-up hierarchical clustering and pair-wise string similarity metrics. In this paper, we address the issue of how to evaluate the named entity variant clusters automatically, with minimal human annotation effort. We present experiments that make use of Wikipedia redirection tables and we show that this method produces good results."
hajlaoui-etal-2014-dcep,{DCEP} -Digital Corpus of the {E}uropean Parliament,2014,16,9,4,0,18371,najeh hajlaoui,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We are presenting a new highly multilingual document-aligned parallel corpus called DCEP - Digital Corpus of the European Parliament. It consists of various document types covering a wide range of subject domains. With a total of 1.37 billion words in 23 languages (253 language pairs), gathered in the course of ten years, this is the largest single release of documents by a European Union institution. DCEP contains most of the content of the European Parliament{'}s official Website. It includes different document types produced between 2001 and 2012, excluding only the documents already exist in the Europarl corpus to avoid overlapping. We are presenting the typical acquisition steps of the DCEP corpus: data access, document alignment, sentence splitting, normalisation and tokenisation, and sentence alignment efforts. The sentence-level alignment is still in progress but based on some first experiments; we showed that DCEP is very useful for NLP applications, in particular for Statistical Machine Translation."
balahur-etal-2014-resource,Resource Creation and Evaluation for Multilingual Sentiment Analysis in Social Media Texts,2014,16,13,3,0.220271,428,alexandra balahur,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents an evaluation of the use of machine translation to obtain and employ data for training multilingual sentiment classifiers. We show that the use of machine translated data obtained similar results as the use of native-speaker translations of the same data. Additionally, our evaluations pinpoint to the fact that the use of multilingual data, including that obtained through machine translation, leads to improved results in sentiment classification. Finally, we show that the performance of the sentiment classifiers built on machine translated data can be improved using original data from the target language and that even a small amount of such texts can lead to significant growth in the classification performance."
R13-1031,Acronym recognition and processing in 22 languages,2013,15,3,3,1,16860,maud ehrmann,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We are presenting work on recognising acronyms of the form Long-Form (Short-Form) such as xe2x80x9cInternational Monetary Fund (IMF)xe2x80x9d in millions of news articles in twenty-two languages, as part of our more general effort to recognise entities and their variants in news text and to use them for the automatic analysis of the news, including the linking of related news across languages. We show how the acronym recognition patterns, initially developed for medical terms, needed to be adapted to the more general news domain and we present evaluation results. We describe our effort to automatically merge the numerous long-form variants referring to the same short-form, while keeping non-related long-forms separate. Finally, we provide extensive statistics on the frequency and the distribution of shortform/long-form pairs across languages."
steinberger-etal-2012-dgt,{DGT}-{TM}: A freely available Translation Memory in 22 languages,2012,13,47,1,1,24362,ralf steinberger,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The European Commission's (EC) Directorate General for Translation, together with the EC's Joint Research Centre, is making available a large translation memory (TM; i.e. sentences and their professionally produced translations) covering twenty-two official European Union (EU) languages and their 231 language pairs. Such a resource is typically used by translation professionals in combination with TM software to improve speed and consistency of their translations. However, this resource has also many uses for translation studies and for language technology applications, including Statistical Machine Translation (SMT), terminology extraction, Named Entity Recognition (NER), multilingual classification and clustering, and many more. In this reference paper for DGT-TM, we introduce this new resource, provide statistics regarding its size, and explain how it was produced and how to use it."
steinberger-etal-2012-jrc,{JRC} Eurovoc Indexer {JEX} - A freely available multi-label categorisation tool,2012,13,22,1,1,24362,ralf steinberger,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700 hierarchically organised subject domains used by European Institutions and many authorities in Member States of the European Union (EU) for the classification and retrieval of official documents. JEX is JRC-developed multi-label classification software that learns from manually labelled data to automatically assign EuroVoc descriptors to new documents in a profile-based category-ranking task. The JEX release consists of trained classifiers for 22 official EU languages, of parallel training data in the same languages, of an interface that allows viewing and amending the assignment results, and of a module that allows users to re-train the tool on their own document collections. JEX allows advanced users to change the document representation so as to possibly improve the categorisation result through linguistic pre-processing. JEX can be used as a tool for interactive EuroVoc descriptor assignment to increase speed and consistency of the human categorisation process, or it can be used fully automatically. The output of JEX is a language-independent EuroVoc feature vector lending itself also as input to various other Language Technology tasks, including cross-lingual clustering and classification, cross-lingual plagiarism detection, sentence selection and ranking, and more."
E12-2006,{ONTS}: {``}Optima{''} News Translation System,2012,20,13,6,0.487805,5084,marco turchi,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from Arabic, Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and Turkish into English. The Moses-based system was optimised for the news domain and differs from other available systems in four ways: (1) News items are automatically categorised on the source side, before translation; (2) Named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language, making use of a separate entity repository; (3) News titles are translated with a separate translation system which is optimised for the specific style of news titles; (4) The system was optimised for speed in order to cope with the large volume of daily news articles."
W11-4009,INVITED TALK 2: Bringing Multilingual Information Extraction to the User,2011,0,0,1,1,24362,ralf steinberger,Proceedings of the {RANLP} 2011 Workshop on Information Extraction and Knowledge Acquisition,0,None
W11-1704,Creating Sentiment Dictionaries via Triangulation,2011,18,79,7,1,12063,josef steinberger,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"The paper presents a semi-automatic approach to creating sentiment dictionaries in many languages. We first produced high-level gold-standard sentiment dictionaries for two languages and then translated them automatically into third languages. Those words that can be found in both target language word lists are likely to be useful because their word senses are likely to be similar to that of the two source languages. These dictionaries can be further corrected, extended and improved. In this paper, we present results that verify our triangulation hypothesis, by evaluating triangulated lists and comparing them to non-triangulated machine-translated word lists."
R11-1015,"{JRC}-{NAMES}: A Freely Available, Highly Multilingual Named Entity Resource",2011,8,32,1,1,24362,ralf steinberger,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"This paper describes a new, freely available, highly multilingual named entity resource for person and organisation names that has been compiled over seven years of large-scale multilingual news analysis combined with Wikipedia mining, resulting in 205,000 person and organisation names plus about the same number of spelling variants written in over 20 different scripts and in many more languages. This resource, produced as part of the Europe Media Monitor activity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number of purposes. These include improving name search in databases or on the internet, seeding machine learning systems to learn named entity recognition rules, improve machine translation results, and more. We describe here how this resource was created; we give statistics on its current size; we address the issue of morphological inflection; and we give details regarding its functionality. Updates to this resource will be made available daily."
R11-1017,Building a Multilingual Named Entity-Annotated Corpus Using Annotation Projection,2011,20,31,3,1,16860,maud ehrmann,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"As developers of a highly multilingual named entity recognition (NER) system, we face an evaluation resource bottleneck problem: we need evaluation data in many languages, the annotation should not be too time-consuming, and the evaluation results across languages should be comparable. We solve the problem by automatically annotating the English version of a multi-parallel corpus and by projecting the annotations into all the other language versions. For the translation of English entities, we use a phrase-based statistical machine translation system as well as a lookup of known names from a multilingual name database. For the projection, we incrementally apply different methods: perfect string matching, perfect consonant signature matching and edit distance similarity. The resulting annotated parallel corpus will be made available for reuse."
R11-1035,Highly Multilingual Coreference Resolution Exploiting a Mature Entity Repository,2011,14,3,8,1,12063,josef steinberger,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper we present an approach to large-scale coreference resolution for an ample set of human languages, with a particular emphasis on time performance and precision. One of the distinctive features of our approach is the use of a mature multilingual named entity repository (persons and organizations) gradually compiled over the past few years. Our experiments show promising results xe2x80x90 an overall precision of 94% tested on seven different languages. We also present an extrinsic evaluation on seven languages in the context of summarization where we gauge the contribution of the coreference resolver towards the end summarization performance."
R11-1113,Multilingual Entity-Centered Sentiment Analysis Evaluated by Parallel Corpora,2011,15,28,4,1,12063,josef steinberger,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We propose the creation and use of a multilingual parallel news corpus annotated with opinion towards entities, produced by projecting sentiment annotation from one language to several others. The objective is to save annotation time for development and evaluation purposes, and to guarantee comparability of opinion mining evaluation results across languages. By creating this resource, we answered the question whether sentiment is consistently translated across languages so that projection can actually be an option. We describe our approach to multilingual sentiment analysis and show its performance in 7 languages of the parallel corpus."
P10-2070,Wrapping up a Summary: From Representation to Generation,2010,17,7,4,1,12063,josef steinberger,Proceedings of the {ACL} 2010 Conference Short Papers,0,The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.
zaghouani-etal-2010-adapting,Adapting a resource-light highly multilingual Named Entity Recognition system to {A}rabic,2010,16,21,4,0,579,wajdi zaghouani,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a fully functional Arabic information extraction (IE) system that is used to analyze large volumes of news texts every day to extract the named entity (NE) types person, organization, location, date and number, as well as quotations (direct reported speech) by and about people. The Named Entity Recognition (NER) system was not developed for Arabic, but - instead - a highly multilingual, almost language-independent NER system was adapted to also cover Arabic. The Semitic language Arabic substantially differs from the Indo-European and Finno-Ugric languages currently covered. This paper thus describes what Arabic language-specific resources had to be developed and what changes needed to be made to the otherwise language-independent rule set in order to be applicable to the Arabic language. The achieved evaluation results are generally satisfactory, but could be improved for certain entity types. The results of the IE tools can be seen on the Arabic pages of the freely accessible Europe Media Monitor (EMM) application NewsExplorer, which can be found at http://press.jrc.it/overview.html."
balahur-etal-2010-sentiment,Sentiment Analysis in the News,2010,14,119,2,0.392157,428,alexandra balahur,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles â author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which we apply these concepts. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance."
Y09-2019,Summarizing Opinions in Blog Threads,2009,21,16,4,0.392157,428,alexandra balahur,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"In this paper we present an approach to summarizing positive and negative opinions in blog threads. We first run a sentiment analysis system and consequently pass its output through a standard LSA-based text summarization system. Further on, we evaluate our approach and present the results obtained, which we believe are promising in the context of multi-document text summarization. Finally, we discuss the main issues in applying standard text summarization techniques to the slightly different task of summarizing opinions in blog threads."
W09-4602,Invited talk: Linking News Content Across Languages,2009,6,0,1,1,24362,ralf steinberger,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"Organisations and individuals that need to monitor what the media say about certain issues face an extreme information overload, especially if they are interested in the news written in more than one language. News aggregators sometimes pre-filter potentially user-relevant articles or automatically group related articles into clusters. However, the enormous amount of available online information calls for further automatic information processing to enable users to sieve through even larger amounts of textual data in less time and to navigate and explore the document collections efficiently."
2009.mtsummit-papers.7,462 Machine Translation Systems for {E}urope,2009,9,65,3,0,4417,philipp koehn,Proceedings of Machine Translation Summit XII: Papers,0,"We built 462 machine translation systems for all language pairs of the Acquis Communautaire corpus. We report and analyse the performance of these system, and compare them against pivot translation and a number of system combination methods (multi-pivot, multisource) that are possible due to the available systems."
W08-1408,Story tracking: linking similar news over time and across languages,2008,10,26,2,1,33358,bruno pouliquen,Coling 2008: Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization,0,"The Europe Media Monitor system (EMM) gathers and aggregates an average of 50,000 newspaper articles per day in over 40 languages. To manage the information overflow, it was decided to group similar articles per day and per language into clusters and to link daily clusters over time into stories. A story automatically comes into existence when related groups of articles occur within a 7-day window. While cross-lingual links across 19 languages for individual news clusters have been displayed since 2004 as part of a freely accessible online application (http://press.jrc.it/NewsExplorer), the newest development is work on linking entire stories across languages. The evaluation of the monolingual aggregation of historical clusters into stories and of the linking of stories across languages yielded mostly satisfying results."
C08-3001,Online-Monitoring of Security-Related Events,2008,8,7,4,0.882353,28187,martin atkinson,Coling 2008: Companion volume: Demonstrations,0,"This paper presents a fully operational real-time event extraction system which is capable of accurately and efficiently extracting violent and natural disaster events from vast amount of online news articles per day in different languages. Due to the requirement that the system must be multilingual and easily extendable, it is based on a shallow linguistic analysis. The event extraction results can be viewed on a publicly accessible website."
steinberger-etal-2006-jrc,The {JRC}-{A}cquis: A Multilingual Aligned Parallel Corpus with 20+ Languages,2006,12,341,1,1,24362,ralf steinberger,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EU languages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction)."
pouliquen-etal-2006-geocoding,"Geocoding Multilingual Texts: Recognition, Disambiguation and Visualisation",2006,6,54,3,1,33358,bruno pouliquen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We are presenting a method to recognise geographical references in free text. Our tool must work on various languages with a minimum of language-dependent resources, except a gazetteer. The main difficulty is to disambiguate these place names by distinguishing places from persons and by selecting the most likely place out of a list of homographic place names world-wide. The system uses a number of language-independent clues and heuristics to disambiguate place name homographs. The final aim is to index texts with the countries and cities they mention and to automatically visualise this information on geographical maps using various tools."
C04-1138,Multilingual and cross-lingual news topic tracking,2004,9,52,2,1,33358,bruno pouliquen,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We are presenting a working system for automated news analysis that ingests an average total of 7600 news articles per day in five languages. For each language, the system detects the major news stories of the day using a group-average unsupervised agglomerative clustering process. It also tracks, for each cluster, related groups of articles published over the previous seven days, using a cosine of weighted terms. The system furthermore tracks related news across languages, in all language pairs involved. The cross-lingual news cluster similarity is based on a linear combination of three types of input: (a) cognates, (b) automatically detected to geographical place names and (c) the results of a mapping process onto a multilingual classification system. A manual evaluation showed that the system produces good results."
1997.tmi-1.24,Automatic selection and ranking of translation candidates,1997,-1,-1,2,0,49115,antonio sanfilippo,Proceedings of the 7th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
C94-1008,Treating {`}Free Word Order{'} in Machine Translation,1994,8,9,1,1,24362,ralf steinberger,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In free word order languages, every sentence is embedded in its specific context. The order of constituents is determined by the categories theme, rheme and contraslive focus. This paper shows how to recognise and to translate these categories automatically on a sentential basis, so that sentence embedding can be achieved without having to refer to the context. Traditionally neglected modifier classes are fully covered by the proposed method."
