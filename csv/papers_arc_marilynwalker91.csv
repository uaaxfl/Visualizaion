2021.inlg-1.45,Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text {NLG},2021,-1,-1,2,1,5999,juraj juraska,Proceedings of the 14th International Conference on Natural Language Generation,0,"Ever since neural models were adopted in data-to-text language generation, they have invariably been reliant on extrinsic components to improve their semantic accuracy, because the models normally do not exhibit the ability to generate text that reliably mentions all of the information provided in the input. In this paper, we propose a novel decoding method that extracts interpretable information from encoder-decoder models{'} cross-attention, and uses it to infer which attributes are mentioned in the generated text, which is subsequently used to rescore beam hypotheses. Using this decoding method with T5 and BART, we show on three datasets its ability to dramatically reduce semantic errors in the generated outputs, while maintaining their state-of-the-art quality."
2021.emnlp-demo.15,Athena 2.0: Contextualized Dialogue Management for an {A}lexa {P}rize {S}ocial{B}ot,2021,-1,-1,1,1,6000,marilyn walker,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena{'}s success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel conversations with every interaction. Here we describe Athena{'}s system design and performance in the Alexa Prize during the 20/21 competition. A live demo of Athena as well as video recordings will provoke discussion on the state of the art in conversational AI."
2020.sigdial-1.3,Learning from Mistakes: Combining Ontologies via Self-Training for Dialogue Generation,2020,-1,-1,5,1,10319,lena reed,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Natural language generators (NLGs) for task-oriented dialogue typically take a meaning representation (MR) as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies. Here we explore, for the first time, whether it is possible to train an NLG for a new larger ontology using existing training sets for the restaurant domain, where each set is based on a different ontology. We create a new, larger combined ontology, and then train an NLG to produce utterances covering it. For example, if one dataset has attributes for family friendly and rating information, and the other has attributes for decor and service, our aim is an NLG for the combined ontology that can produce utterances that realize values for family friendly, rating, decor and service. Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly challenging. We then develop a novel self-training method that identifies (errorful) model outputs, automatically constructs a corrected MR input to form a new (MR, utterance) training pair, and then repeatedly adds these new instances back into the training data. We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4{\%} improvement over the baseline model. We also report a human qualitative evaluation of the final model showing that it achieves high naturalness, semantic coherence and grammaticality."
2020.acl-main.224,Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation,2020,-1,-1,2,0,6808,chao zhao,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text."
W19-8623,{V}i{GGO}: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation,2019,22,1,3,1,5999,juraj juraska,Proceedings of the 12th International Conference on Natural Language Generation,0,"The uptake of deep learning in natural language generation (NLG) led to the release of both small and relatively large parallel corpora for training neural models. The existing data-to-text datasets are, however, aimed at task-oriented dialogue systems, and often thus limited in diversity and versatility. They are typically crowdsourced, with much of the noise left in them. Moreover, current neural NLG models do not take full advantage of large training data, and due to their strong generalizing properties produce sentences that look template-like regardless. We therefore present a new corpus of 7K samples, which (1) is clean despite being crowdsourced, (2) has utterances of 9 generalizable and conversational dialogue act types, making it more suitable for open-domain dialogue systems, and (3) explores the domain of video games, which is new to dialogue systems despite having excellent potential for supporting rich conversations."
W19-8101,Maximizing Stylistic Control and Semantic Accuracy in {NLG}: Personality Variation and Discourse Contrast,2019,32,0,4,1,10318,vrindavan harrison,Proceedings of the 1st Workshop on Discourse Structure in Neural NLG,0,"Neural generation methods for task-oriented dialogue typically generate from a meaning representation that is populated using a database of domain information, such as a table of data describing a restaurant. While earlier work focused solely on the semantic fidelity of outputs, recent work has started to explore methods for controlling the style of the generated text while simultaneously achieving semantic accuracy. Here we experiment with two stylistic benchmark tasks, generating language that exhibits variation in personality, and generating discourse contrast. We report a huge performance improvement in both stylistic control and semantic accuracy over the state of the art on both of these benchmarks. We test several different models and show that putting stylistic conditioning in the decoder and eliminating the semantic re-ranker used in earlier models results in more than 15 points higher BLEU for Personality, with a reduction of semantic error to near zero. We also report an improvement from .75 to .81 in controlling contrast and a reduction in semantic error from 16{\%} to 2{\%}."
P19-1065,Implicit Discourse Relation Identification for Open-domain Dialogues,2019,30,0,5,0,4838,mingyu ma,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. Previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. In this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. Moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model."
P19-1596,Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural {NLG},2019,40,2,4,1,2978,shereen oraby,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years. While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. We present YelpNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains. The experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics."
W18-6535,Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?,2018,44,1,3,1,10319,lena reed,Proceedings of the 11th International Conference on Natural Language Generation,0,"Responses in task-oriented dialogue systems often realize multiple propositions whose ultimate form depends on the use of sentence planning and discourse structuring operations. For example a recommendation may consist of an explicitly evaluative utterance e.g. \textit{Chanpen Thai is the best option}, along with content related by the justification discourse relation, e.g. \textit{It has great food and service}, that combines multiple propositions into a single phrase. While neural generation methods integrate sentence planning and surface realization in one end-to-end learning framework, previous work has not shown that neural generators can: (1) perform common sentence planning and discourse structuring operations; (2) make decisions as to whether to realize content in a single sentence or over multiple sentences; (3) generalize sentence planning and discourse relation operations beyond what was seen in training. We systematically create large training corpora that exhibit particular sentence planning operations and then test neural models to see what they learn. We compare models without explicit latent variables for sentence planning with ones that provide explicit supervision during training. We show that only the models with additional supervision can reproduce sentence planning and discourse operations and generalize to situations unseen in training."
W18-6536,"Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features",2018,27,6,2,1,10318,vrindavan harrison,Proceedings of the 11th International Conference on Natural Language Generation,0,"Question Generation is the task of automatically creating questions from textual input. In this work we present a new Attentional Encoder{--}Decoder Recurrent Neural Network model for automatic question generation. Our model incorporates linguistic features and an additional sentence embedding to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to named entity recognition, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our model achieves state of the art results of 19.98 Bleu{\_}4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added features improve the quality of the generated questions."
W18-6554,Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs,2018,0,3,2,1,5999,juraj juraska,Proceedings of the 11th International Conference on Natural Language Generation,0,"One of the biggest challenges of end-to-end language generation from meaning representations in dialogue systems is making the outputs more natural and varied. Here we take a large corpus of 50K crowd-sourced utterances in the restaurant domain and develop text analysis methods that systematically characterize types of sentences in the training data. We then automatically label the training data to allow us to conduct two kinds of experiments with a neural generator. First, we test the effect of training the system with different stylistic partitions and quantify the effect of smaller, but more stylistically controlled training data. Second, we propose a method of labeling the style variants during training, and show that we can modify the style of the generated utterances using our stylistic labels. We contrast and compare these methods that can be used with any existing large corpus, showing how they vary in terms of semantic quality and stylistic control."
W18-5003,Modeling Linguistic and Personality Adaptation for Natural Language Generation,2018,0,0,3,1,28033,zhichao hu,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"Previous work has shown that conversants adapt to many aspects of their partners{'} language. Other work has shown that while every person is unique, they often share general patterns of behavior. Theories of personality aim to explain these shared patterns, and studies have shown that many linguistic cues are correlated with personality traits. We propose an adaptation measure for adaptive natural language generation for dialogs that integrates the predictions of both personality theories and adaptation theories, that can be applied as a dialog unfolds, on a turn by turn basis. We show that our measure meets criteria for validity, and that adaptation varies according to corpora and task, speaker, and the set of features used to model it. We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation."
W18-5019,Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators,2018,41,6,6,1,2978,shereen oraby,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated semantics. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on style has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, Personage, to synthesize a new corpus of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three models. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals: this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large."
N18-1014,A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation,2018,24,3,4,1,5999,juraj juraska,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model. Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets."
L18-1628,Exploring Conversational Language Generation for Rich Content about Hotels,2018,23,0,1,1,6000,marilyn walker,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Dialogue systems for hotel and tourist information have typically simplified the richness of the domain, focusing system utterances on only a few selected attributes such as price, location and type of rooms. However, much more content is typically available for hotels, often as many as 50 distinct instantiated attributes for an individual entity. New methods are needed to use this content to generate natural dialogues for hotel information, and in general for any domain with such rich complex content. We describe three experiments aimed at collecting data that can inform an NLG for hotels dialogues, and show, not surprisingly, that the sentences in the original written hotel descriptions provided on webpages for each hotel are stylistically not a very good match for conversational interaction. We quantify the stylistic features that characterize the differences between the original textual data and the collected dialogic data. We plan to use these in stylistic models for generation, and for scoring retrieved utterances for use in hotel dialogues"
L18-1707,{S}lug{NERDS}: A Named Entity Recognition Tool for Open Domain Dialogue Systems,2018,17,1,5,1,10320,kevin bowden,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In dialogue systems, the tasks of named entity recognition (NER) and named entity linking (NEL) are vital preprocessing steps for understanding user intent, especially in open domain interaction where we cannot rely on domain-specific inference. UCSC's effort as one of the funded teams in the 2017 Amazon Alexa Prize Contest has yielded Slugbot, an open domain social bot, aimed at casual conversation. We discovered several challenges specifically associated with both NER and NEL when building Slugbot, such as that the NE labels are too coarse-grained or the entity types are not linked to a useful ontology. Moreover, we have discovered that traditional approaches do not perform well in our context: even systems designed to operate on tweets or other social media data do not work well in dialogue systems. In this paper, we introduce Slugbot's Named Entity Recognition for dialogue Systems (SlugNERDS), a NER and NEL tool which is optimized to address these issues. We describe two new resources that we are building as part of this work: SlugEntityDB and SchemaActuator. We believe these resources will be useful for the research community."
W17-5537,Are you serious?: Rhetorical Questions and Sarcasm in Social Media Dialog,2017,27,3,5,1,2978,shereen oraby,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"Effective models of social dialog must understand a broad range of rhetorical and figurative devices. Rhetorical questions (RQs) are a type of figurative language whose aim is to achieve a pragmatic goal, such as structuring an argument, being persuasive, emphasizing a point, or being ironic. While there are computational models for other forms of figurative language, rhetorical questions have received little attention to date. We expand a small dataset from previous work, presenting a corpus of 10,270 RQs from debate forums and Twitter that represent different discourse functions. We show that we can clearly distinguish between RQs and sincere questions (0.76 F1). We then show that RQs can be used both sarcastically and non-sarcastically, observing that non-sarcastic (other) uses of RQs are frequently argumentative in forums, and persuasive in tweets. We present experiments to distinguish between these uses of RQs using SVM and LSTM models that represent linguistic features and post-level context, achieving results as high as 0.76 F1 for {``}sarcastic{''} and 0.77 F1 for {``}other{''} in forums, and 0.83 F1 for both {``}sarcastic{''} and {``}other{''} in tweets. We supplement our quantitative experiments with an in-depth characterization of the linguistic variation in RQs."
W17-5540,Inferring Narrative Causality between Event Pairs in Films,2017,21,1,2,1,28033,zhichao hu,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"To understand narrative, humans draw inferences about the underlying relations between narrative events. Cognitive theories of narrative understanding define these inferences as four different types of causality, that include pairs of events A, B where A physically causes B (X drop, X break), to pairs of events where A causes emotional state B (Y saw X, Y felt fear). Previous work on learning narrative relations from text has either focused on {``}strict{''} physical causality, or has been vague about what relation is being learned. This paper learns pairs of causal events from a corpus of film scene descriptions which are action rich and tend to be told in chronological order. We show that event pairs induced using our methods are of high quality and are judged to have a stronger causal relation than event pairs from Rel-Grams."
W17-5543,Modelling Protagonist Goals and Desires in First-Person Narrative,2017,0,3,5,1,22205,elahe rahimtoroghi,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus."
W17-5211,Linguistic Reflexes of Well-Being and Happiness in Echo,2017,32,0,2,1,22133,jiaqi wu,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Different theories posit different sources for feelings of well-being and happiness. Appraisal theory grounds our emotional responses in our goals and desires and their fulfillment, or lack of fulfillment. Self-Determination theory posits that the basis for well-being rests on our assessments of our competence, autonomy and social connection. And surveys that measure happiness empirically note that people require their basic needs to be met for food and shelter, but beyond that tend to be happiest when socializing, eating or having sex. We analyze a corpus of private micro-blogs from a well-being application called Echo, where users label each written post about daily events with a happiness score between 1 and 9. Our goal is to ground the linguistic descriptions of events that users experience in theories of well-being and happiness, and then examine the extent to which different theoretical accounts can explain the variance in the happiness scores. We show that recurrent event types, such as obligation and incompetence, which affect people{'}s feelings of well-being are not captured in current lexical or semantic resources."
W17-4904,Harvesting Creative Templates for Generating Stylistically Varied Restaurant Reviews,2017,27,1,3,1,2978,shereen oraby,Proceedings of the Workshop on Stylistic Variation,0,"Many of the creative and figurative elements that make language exciting are lost in translation in current natural language generation engines. In this paper, we explore a method to harvest templates from positive and negative reviews in the restaurant domain, with the goal of vastly expanding the types of stylistic variation available to the natural language generator. We learn hyperbolic adjective patterns that are representative of the strongly-valenced expressive language commonly used in either positive or negative reviews. We then identify and delexicalize entities, and use heuristics to extract generation templates from review sentences. We evaluate the learned templates against more traditional review templates, using subjective measures of convincingness, interestingness, and naturalness. Our results show that the learned templates score highly on these measures. Finally, we analyze the linguistic categories that characterize the learned positive and negative templates. We plan to use the learned templates to improve the conversational style of dialogue systems in the restaurant domain."
W17-4911,Stylistic Variation in Television Dialogue for Natural Language Generation,2017,-1,-1,2,0,31606,grace lin,Proceedings of the Workshop on Stylistic Variation,0,"Conversation is a critical component of storytelling, where key information is often revealed by what/how a character says it. We focus on the issue of character voice and build stylistic models with linguistic features related to natural language generation decisions. Using a dialogue corpus of the television series, The Big Bang Theory, we apply content analysis to extract relevant linguistic features to build character-based stylistic models, and we test the model-fit through an user perceptual experiment with Amazon{'}s Mechanical Turk. The results are encouraging in that human subjects tend to perceive the generated utterances as being more similar to the character they are modeled on, than to another random character."
W17-2708,Inference of Fine-Grained Event Causality from Blogs and Films,2017,17,0,3,1,28033,zhichao hu,Proceedings of the Events and Stories in the News Workshop,0,"Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80{\%} of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire."
P17-2022,Learning Lexico-Functional Patterns for First-Person Affect,2017,18,0,5,1,10319,lena reed,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Informal first-person narratives are a unique resource for computational models of everyday events and people{'}s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate{'}s arguments. We present a method to learn proxies for these functions from first-person narratives. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F."
E17-1070,Argument Strength is in the Eye of the Beholder: Audience Effects in Persuasion,2017,54,14,3,1,16783,stephanie lukin,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within natural language processing. We show that belief change is affected by personality factors, with conscientious, open and agreeable people being more convinced by emotional arguments."
W16-3604,Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue,2016,22,22,6,1,2978,shereen oraby,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-3636,Measuring the Similarity of Sentential Arguments in Dialogue,2016,34,10,3,1,678,amita misra,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"When people converse about social or political topics, similar arguments are often paraphrased by different speakers, across many different conversations. Debate websites produce curated summaries of arguments on such topics; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions, or labels capturing the essence of one particular aspect of an argument, e.g. Morality or Second Amendment. We call these frequently paraphrased propositions ARGUMENT FACETS. Like these curated sites, our goal is to induce and identify argument facets across multiple conversations, and produce summaries. However, we aim to do this automatically. We frame the problem as consisting of two steps: we first extract sentences that express an argument from raw social media dialogs, and then rank the extracted arguments in terms of their similarity to one another. Sets of similar arguments are used to represent argument facets. We show here that we can predict ARGUMENT FACET SIMILARITY with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics, easily beating several reasonable baselines."
W16-3644,Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events,2016,0,7,3,1,22205,elahe rahimtoroghi,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
S16-1068,{NLDS}-{UCSC} at {S}em{E}val-2016 Task 6: A Semi-Supervised Approach to Detecting Stance in Tweets,2016,32,4,5,1,678,amita misra,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Stance classification aims to identify, for a particular issue under discussion, whether the speaker or author of a conversational turn has Pro (Favor) or Con (Against) stance on the issue. Detecting stance in tweets is a new task proposed for SemEval-2016 Task6, involving predicting stance for a dataset of tweets on the topics of abortion, atheism, climate change, feminism and Hillary Clinton. Given the small size of the dataset, our team created our own topic-specific training corpus by developing a set of high precision hashtags for each topic that were used to query the twitter API, with the aim of developing a large training corpus without additional human labeling of tweets for stance. The hashtags selected for each topic were predicted to be stance-bearing on their own. Experimental results demonstrate good performance for our features for opinion-target pairs based on generalizing dependency features using sentiment lexicons."
N16-1146,Automatically Inferring Implicit Properties in Similes,2016,17,4,3,0.9965,29454,ashequl qadir,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Author(s): Qadir, A; Riloff, E; Walker, MA | Abstract: xc2xa92016 Association for Computational Linguistics. A simile is a figure of speech comparing two fundamentally different things. Sometimes, a simile will explain the basis of a comparison by explicitly mentioning a shared property. For example, my room is as cold as Antarctica gives cold as the property shared by the room and Antarctica. But most similes do not give an explicit property (e.g., my room feels like Antarctica) leaving the reader to infer that the room is cold. We tackle the problem of automatically inferring implicit properties evoked by similes. Our approach involves three steps: (1) generating candidate properties from different sources, (2) evaluating properties based on the influence of multiple simile components, and (3) aggregated ranking of the properties. We also present an analysis showing that the difficulty of inferring an implicit property for a simile correlates with its interpretive diversity."
L16-1163,{P}ersona{B}ank: A Corpus of Personal Narratives and Their Story Intention Graphs,2016,0,10,4,1,16783,stephanie lukin,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a new corpus, PersonaBank, consisting of 108 personal stories from weblogs that have been annotated with their Story Intention Graphs, a deep representation of the content of a story. We describe the topics of the stories and the basis of the Story Intention Graph representation, as well as the process of annotating the stories to produce the Story Intention Graphs and the challenges of adapting the tool to this new personal narrative domain. We also discuss how the corpus can be used in applications that retell the story using different styles of tellings, co-tellings, or as a content planner."
L16-1504,Coordinating Communication in the Wild: The Artwalk Dialogue Corpus of Pedestrian Navigation and Mobile Referential Communication,2016,35,2,3,0,35225,kris liu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Artwalk Corpus is a collection of 48 mobile phone conversations between 24 pairs of friends and 24 pairs of strangers performing a novel, naturalistically-situated referential communication task. This task produced dialogues which, on average, are just under 40 minutes. The task requires the identification of public art while walking around and navigating pedestrian routes in the downtown area of Santa Cruz, California. The task involves a Director on the UCSC campus with access to maps providing verbal instructions to a Follower executing the task. The task provides a setting for real-world situated dialogic language and is designed to: (1) elicit entrainment and coordination of referring expressions between the dialogue participants, (2) examine the effect of friendship on dialogue strategies, and (3) examine how the need to complete the task while negotiating myriad, unanticipated events in the real world â such as avoiding cars and other pedestrians â affects linguistic coordination and other dialogue behaviors. Previous work on entrainment and coordinating communication has primarily focused on similar tasks in laboratory settings where there are no interruptions and no need to navigate from one point to another in a complex space. The corpus provides a general resource for studies on how coordinated task-oriented dialogue changes when we move outside the laboratory and into the world. It can also be used for studies of entrainment in dialogue, and the form and style of pedestrian instruction dialogues, as well as the effect of friendship on dialogic behaviors."
L16-1550,A Corpus of Gesture-Annotated Dialogues for Monologue-to-Dialogue Generation from Personal Narratives,2016,21,2,7,1,28033,zhichao hu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Story-telling is a fundamental and prevalent aspect of human social behavior. In the wild, stories are told conversationally in social settings, often as a dialogue and with accompanying gestures and other nonverbal behavior. This paper presents a new corpus, the Story Dialogue with Gestures (SDG) corpus, consisting of 50 personal narratives regenerated as dialogues, complete with annotations of gesture placement and accompanying gesture forms. The corpus includes dialogues generated by human annotators, gesture annotations on the human generated dialogues, videos of story dialogues generated from this representation, video clips of each gesture used in the gesture annotations, and annotations of the original personal narratives with a deep representation of story called a Story Intention Graph. Our long term goal is the automatic generation of story co-tellings as animated dialogues from the Story Intention Graph. We expect this corpus to be a useful resource for researchers interested in natural language generation, intelligent virtual agents, generation of nonverbal behavior, and story and narrative representations."
L16-1552,A Verbal and Gestural Corpus of Story Retellings to an Expressive Embodied Virtual Character,2016,0,1,4,0,35290,jackson tolins,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a corpus of 44 human-agent verbal and gestural story retellings designed to explore whether humans would gesturally entrain to an embodied intelligent virtual agent. We used a novel data collection method where an agent presented story components in installments, which the human would then retell to the agent. At the end of the installments, the human would then retell the embodied animated agent the story as a whole. This method was designed to allow us to observe whether changes in the agent{'}s gestural behavior would result in human gestural changes. The agent modified its gestures over the course of the story, by starting out the first installment with gestural behaviors designed to manifest extraversion, and slowly modifying gestures to express introversion over time, or the reverse. The corpus contains the verbal and gestural transcripts of the human story retellings. The gestures were coded for type, handedness, temporal structure, spatial extent, and the degree to which the participants{'} gestures match those produced by the agent. The corpus illustrates the variation in expressive behaviors produced by users interacting with embodied virtual characters, and the degree to which their gestures were influenced by the agent{'}s dynamic changes in personality-based expressive style."
L16-1553,A Multimodal Motion-Captured Corpus of Matched and Mismatched Extravert-Introvert Conversational Pairs,2016,0,2,5,0,35290,jackson tolins,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a new corpus, the Personality Dyads Corpus, consisting of multimodal data for three conversations between three personality-matched, two-person dyads (a total of 9 separate dialogues). Participants were selected from a larger sample to be 0.8 of a standard deviation above or below the mean on the Big-Five Personality extraversion scale, to produce an Extravert-Extravert dyad, an Introvert-Introvert dyad, and an Extravert-Introvert dyad. Each pair carried out conversations for three different tasks. The conversations were recorded using optical motion capture for the body and data gloves for the hands. Dyads{'} speech was transcribed and the gestural and postural behavior was annotated with ANVIL. The released corpus includes personality profiles, ANVIL files containing speech transcriptions and the gestural annotations, and BVH files containing body and hand motion in 3D."
L16-1704,{I}nternet Argument Corpus 2.0: An {SQL} schema for Dialogic Social Media and the Corpora to go with it,2016,29,21,4,1,35414,rob abbott,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Large scale corpora have benefited many areas of research in natural language processing, but until recently, resources for dialogue have lagged behind. Now, with the emergence of large scale social media websites incorporating a threaded dialogue structure, content feedback, and self-annotation (such as stance labeling), there are valuable new corpora available to researchers. In previous work, we released the INTERNET ARGUMENT CORPUS, one of the first larger scale resources available for opinion sharing dialogue. We now release the INTERNET ARGUMENT CORPUS 2.0 (IAC 2.0) in the hope that others will find it as useful as we have. The IAC 2.0 provides more data than IAC 1.0 and organizes it using an extensible, repurposable SQL schema. The database structure in conjunction with the associated code facilitates querying from and combining multiple dialogically structured data sources. The IAC 2.0 schema provides support for forum posts, quotations, markup (bold, italic, etc), and various annotations, including Stanford CoreNLP annotations. We demonstrate the generalizablity of the schema by providing code to import the ConVote corpus."
W15-4627,Generating Sentence Planning Variations for Story Telling,2015,40,12,3,1,16783,stephanie lukin,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems. Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted. This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content. We propose that a solution to this problem lies in new methods for developing language generation resources. We describe the ES-TRANSLATOR, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs. We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings."
W15-4631,Argument Mining: Extracting Arguments from Online Dialogue,2015,18,39,3,1,36620,reid swanson,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Online forums are now one of the primary venues for public dialogue on current social and political issues. The related corpora are often huge, covering any topic imaginable. Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic. We frame this goal as consisting of two tasks: argument extraction and argument facet similarity. We focus here on the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics. A secondary goal is to develop regressors that are topic independent: we report results of cross-domain training and domain-adaptation with RRSE values for several topics as low as .72, when trained on topic independent features."
W15-0515,And That{'}s A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue,2015,53,27,5,1,2978,shereen oraby,Proceedings of the 2nd Workshop on Argumentation Mining,0,"We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unannotated forum posts. This process automatically produces a large set of patterns representing linguistic expressions that are highly correlated with factual and emotional language. Finally, we analyze the most discriminating patterns to better understand the defining characteristics of factual and emotional arguments."
P15-1012,Joint Models of Disagreement and Stance in Online Debate,2015,25,53,5,0,4221,dhanya sridhar,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Online debate forums present a valuable opportunity for the understanding and modeling of dialogue. To understand these debates, a key challenge is inferring the stances of the participants, all of which are interrelated and dependent. While collectively modeling usersxe2x80x99 stances has been shown to be effective (Walker et al., 2012c; Hasan and Ng, 2013), there are many modeling decisions whose ramifications are not well understood. To investigate these choices and their effects, we introduce a scalable unified probabilistic modeling framework for stance classification models that 1) are collective, 2) reason about disagreement, and 3) can model stance at either the author level or at the post level. We comprehensively evaluate the possible modeling choices on eight topics across two online debate corpora, finding accuracy improvements of up to 11.5 percentage points over a local classifier. Our results highlight the importance of making the correct modeling choices for online dialogues, and having a unified probabilistic modeling framework that makes this possible."
N15-1046,Using Summarization to Discover Argument Facets in Online Idealogical Dialog,2015,39,21,4,1,678,amita misra,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"More and more of the information available on the web is dialogic, and a significant portion of it takes place in online forum conversations about current social and political topics. We aim to develop tools to summarize what these conversations are about. What are the CENTRAL PROPOSITIONS associated with different stances on an issue; what are the abstract objects under discussion that are central to a speakerxe2x80x99s argument? How can we recognize that two CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize that the CENTRAL PROPOSITIONS are exactly those arguments that people find most salient, and use human summarization as a probe for discovering them. We describe our corpus of human summaries of opinionated dialogs, then show how we can identify similar repeated arguments, and group them into FACETS across many discussions of a topic. We define a new task, ARGUMENT FACET SIMILARITY (AFS), and show that we can predict AFS with a .54 correlation score, versus an ngram system baseline of .39 and a semantic textual similarity system baseline of .45."
D15-1019,Learning to Recognize Affective Polarity in Similes,2015,30,5,3,0.9965,29454,ashequl qadir,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"A simile is a comparison between two essentially unlike things, such as xe2x80x9cJane swims like a dolphinxe2x80x9d. Similes often express a positive or negative sentiment toward something, but recognizing the polarity of a simile can depend heavily on world knowledge. For example, xe2x80x9cmemory like an elephantxe2x80x9d is positive, but xe2x80x9cmemory like a sievexe2x80x9d is negative. Our research explores methods to recognize the polarity of similes on Twitter. We train classifiers using lexical, semantic, and sentiment features, and experiment with both manually and automatically generated training data. Our approach yields good performance at identifying positive and negative similes, and substantially outperforms existing sentiment resources."
W14-4323,Identifying Narrative Clause Types in Personal Stories,2014,40,9,4,1,36620,reid swanson,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"This paper describes work on automatically identifying categories of narrative clauses in personal stories written by ordinary people about their daily lives and experiences. We base our approach on Labov & Waletzkyxe2x80x99s theory of oral narrative which categorizes narrative clauses into subtypes, such as ORIENTATION, ACTION and EVALUATION. We describe an experiment where we annotate 50 personal narratives from weblogs and experiment with methods for achieving higher annotation reliability. We use the resulting annotated corpus to train a classifier to automatically identify narrative categories, achieving a best average F-score of .658, which rises to an F-score of .767 on the cases with the highest annotator agreement. We believe the identified narrative structure will enable new types of computational analysis of narrative discourse."
W14-2715,Collective Stance Classification of Posts in Online Debate Forums,2014,-1,-1,3,0,4221,dhanya sridhar,Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media,0,None
swanson-etal-2014-getting,Getting Reliable Annotations for Sarcasm in Online Dialogues,2014,16,10,5,1,36620,reid swanson,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The language used in online forums differs in many ways from that of traditional language resources such as news. One difference is the use and frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the aim is to develop a theory of sarcasm in dialogue, or engineer automatic methods for reliably detecting sarcasm, a major challenge is simply the difficulty of getting enough reliably labelled examples. In this paper we describe our work on methods for achieving highly reliable sarcasm annotations from untrained annotators on Mechanical Turk. We explore the use of a number of common statistical reliability measures, such as Kappa, Karger{'}s, Majority Class, and EM. We show that more sophisticated measures do not appear to yield better results for our data than simple measures such as assuming that the correct label is the one that a majority of Turkers apply."
W13-4006,Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue,2013,47,21,2,1,678,amita misra,Proceedings of the {SIGDIAL} 2013 Conference,0,"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research communityxe2x80x99s ability to develop better theories, as well as good off-the-shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic-independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%."
W13-1104,Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue,2013,23,35,2,1,16783,stephanie lukin,Proceedings of the Workshop on Language Analysis in Social Media,0,"More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall. The best performing classifier for the first phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap over the first level with generalized syntactic patterns."
D13-1036,Unsupervised Induction of Contingent Event Pairs from Film Scenes,2013,27,7,5,1,28033,zhichao hu,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search."
N12-1072,Stance Classification using Dialogic Properties of Persuasion,2012,18,71,1,1,6000,marilyn walker,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Public debate functions as a forum for both expressing and forming opinions, an important aspect of public life. We present results for automatically classifying posts in online debate as to the position, or stance that the speaker takes on an issue, such as Pro or Con. We show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parent-post context alone."
walker-etal-2012-corpus,A Corpus for Research on Deliberation and Debate,2012,19,131,1,1,6000,marilyn walker,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Deliberative, argumentative discourse is an important component of opinion formation, belief revision, and knowledge discovery; it is a cornerstone of modern civil society. Argumentation is productively studied in branches ranging from theoretical artificial intelligence to political rhetoric, but empirical analysis has suffered from a lack of freely available, unscripted argumentative dialogs. This paper presents the Internet Argument Corpus (IAC), a set of 390,704 posts in 11,800 discussions extracted from the online debate site 4forums.com. A 2866 thread/130,206 post extract of the corpus has been manually sided for topic of discussion, and subsets of this topic-labeled extract have been annotated for several dialogic and argumentative markers: degrees of agreement with a previous post, cordiality, audience-direction, combativeness, assertiveness, emotionality of argumentation, and sarcasm. As an application of this resource, the paper closes with a discussion of the relationship between discourse marker pragmatics, agreement, emotionality, and sarcasm in the IAC corpus."
walker-etal-2012-annotated,An Annotated Corpus of Film Dialogue for Learning and Characterizing Character Style,2012,28,17,1,1,6000,marilyn walker,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Interactive story systems often involve dialogue with virtual dramatic characters. However, to date most character dialogue is written by hand. One way to ease the authoring process is to (semi-)automatically generate dialogue based on film characters. We extract features from dialogue of film characters in leading roles. Then we use these character-based features to drive our language generator to produce interesting utterances. This paper describes a corpus of film dialogue that we have collected from the IMSDb archive and annotated for linguistic structures and character archetypes. We extract different sets of features using external sources such as LIWC and SentiWordNet as well as using our own written scripts. The automation of feature extraction also eases the process of acquiring additional film scripts. We briefly show how film characters can be represented by models learned from the corpus, how the models can be distinguished based on different categories such as gender and film genre, and how they can be applied to a language generator to generate utterances that can be perceived as being similar to the intended character model."
W11-1701,Cats Rule and Dogs Drool!: Classifying Stance in Online Debate,2011,25,126,2,0.496454,31520,pranav anand,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,"A growing body of work has highlighted the challenges of identifying the stance a speaker holds towards a particular topic, a task that involves identifying a holistic subjective disposition. We examine stance classification on a corpus of 4873 posts across 14 topics on ConvinceMe.net, ranging from the playful to the ideological. We show that ideological debates feature a greater share of rebuttal posts, and that rebuttal posts are significantly harder to classify for stance, for both humans and trained classifiers. We also demonstrate that the number of subjective expressions varies across debates, a fact correlated with the performance of systems sensitive to sentiment-bearing terms. We present results for identifing rebuttals with 63% accuracy, and for identifying stance on a per topic basis that range from 54% to 69%, as compared to unigram baselines that vary between 49% and 60%. Our results suggest that methods that take into account the dialogic context of such posts might be fruitful."
W11-0702,How can you say such things?!?: Recognizing Disagreement in Informal Political Argument,2011,-1,-1,2,1,35414,rob abbott,Proceedings of the Workshop on Language in Social Media ({LSM} 2011),0,None
J11-3002,Controlling User Perceptions of Linguistic Style: Trainable Generation of Personality Traits,2011,76,73,2,1,39969,franccois mairesse,Computational Linguistics,0,"Recent work in natural language generation has begun to take linguistic variation into account, developing algorithms that are capable of modifying the system's linguistic style based either on the user's linguistic style or other factors, such as personality or politeness. While stylistic control has traditionally relied on handcrafted rules, statistical methods are likely to be needed for generation systems to scale to the production of the large range of variation observed in human dialogues. Previous work on statistical natural language generation (SNLG) has shown that the grammaticality and naturalness of generated utterances can be optimized from data; however these data-driven methods have not been shown to produce stylistic variation that is perceived by humans in the way that the system intended. This paper describes Personage, a highly parameterizable language generator whose parameters are based on psychological findings about the linguistic reflexes of personality. We present a novel SNLG method which uses parameter estimation models trained on personality-annotated data to predict the generation decisions required to convey any combination of scalar values along the five main dimensions of personality. A human evaluation shows that parameter estimation models produce recognizable stylistic variation along multiple dimensions, on a continuous scale, and without the computational cost incurred by overgeneration techniques."
W10-4303,Dynamic Adaptation in Dialog Systems,2010,0,0,1,1,6000,marilyn walker,Proceedings of the {SIGDIAL} 2010 Conference,0,"A hallmark of human robust intelligence is the ability to flexibly and dynamically adapt behavior to the current situation. For dialog behavior, this entails adaptation to features of both the dialog partner (e.g., relationship, age, personality) and the dialog situation (e.g., task context, asynchronous interaction, limited modalities such as voice-only communication). We don't completely understand how humans do this, nor do we have the ability to produce such dynamically adaptable behavior in human computer dialog interaction. In this talk I will discuss our recent work on dynamic adaptation to the user, and present some experimental results showing that it is possible to automatically generate both verbal and nonverbal system behaviors that are perceived by the user as reliably expressing particular system personalities. I will describe two of my current projects at UCSC that are integrating these capabilities into mobile dialogue systems: SpyFeet, a role playing augmented reality game for encouraging girls to exercise, and Skipper, a dialogue system that gives pedestrians directions in both urban and campus environments."
P08-1020,Trainable Generation of Big-Five Personality Styles through Data-Driven Parameter Estimation,2008,24,37,2,1,39969,franccois mairesse,Proceedings of ACL-08: HLT,1,"Previous work on statistical language generation has primarily focused on grammaticality and naturalness, scoring generation possibilities according to a language model or user feedback. More recent work has investigated data-driven techniques for controlling linguistic style without overgeneration, by reproducing variation dimensions extracted from corpora. Another line of work has produced handcrafted rule-based systems to control specific stylistic dimensions, such as politeness and personality. This paper describes a novel approach that automatically learns to produce recognisable variation along a meaningful stylistic dimensionxe2x80x94 personalityxe2x80x94without the computational cost incurred by overgeneration techniques. We present the first evaluation of a data-driven generation method that projects multiple personality traits simultaneously and on a continuous scale. We compare our performance to a rule-based generator in the same domain."
P08-1055,Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation,2008,25,31,2,0.753307,45985,joseph polifroni,Proceedings of ACL-08: HLT,1,"Despite its long history, and a great deal of research producing many useful algorithms and observations, research in cooperative response generation has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We hypothesize that a particular type of cooperative response, intensional summaries, are eective for when users are unfamiliar with the domain. We evaluate this hypothesis with two experiments with cruiser, a DS for in-car or mobile users to access restaurant information. First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser. Then, we experiment with four algorithms for constructing intensional summaries in cruiser, and show that two summary types are equally eective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model."
I08-2143,{POLL}y: A Conversational System that uses a Shared Representation to Generate Action and Social Language,2008,28,6,2,0,48655,swati gupta,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"We present a demo of our conversational system POLLy (POliteness in Language Learning) which uses a common planning representation to generate actions to be performed by embodied agents in a virtual environment and to generate spoken utterances for dialogues about the steps involved in completing the task. In order to generate socially appropriate dialogue, Brown and Levinsonxe2x80x99s theory of politeness is used to constrain the dialogue generation process."
W07-2308,Generating Politeness in Task Based Interaction: An Evaluation of the Effect of Linguistic Form and Culture,2007,16,9,2,0,48655,swati gupta,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"Politeness is an integral part of human language variation, e.g. consider the difference in the pragmatic effect of realizing the same communicative goal with either Get me a glass of water mate! or I wonder if I could possibly have some water please? This paper presents POLLy (Politeness for Language Learning), a system which combines a natural language generator with an AI Planner to model Brown and Levinson's theory of politeness (BL (2) our indirect strategies which should be the politest forms, are seen as the rudest; and (3) English and Indian native speakers of English have different perceptions of the level of politeness needed to mitigate particular face threats."
P07-1063,{PERSONAGE}: Personality Generation for Dialogue,2007,27,93,2,1,39969,franccois mairesse,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Over the last fifty years, the xe2x80x9cBig Fivexe2x80x9d model of personality traits has become a standard in psychology, and research has systematically documented correlations between a wide range of linguistic variables and the Big Five traits. A distinct line of research has explored methods for automatically generating language that varies along personality dimensions. We present PERSONAGE (PERSONAlity GEnerator), the first highly parametrizable language generator for extraversion, an important aspect of personality. We evaluate two personality generation methods: (1) direct generation with particular parameter settings suggested by the psychology literature; and (2) overgeneration and selection using statistical models trained from judgexe2x80x99s ratings. Results show that both methods reliably generate utterances that vary along the extraversion dimension, according to human judges."
P06-1034,Learning to Generate Naturalistic Utterances Using Reviews in Spoken Dialogue Systems,2006,53,24,3,0,1442,ryuichiro higashinaka,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts. Dictionary creation is a costly process; it is currently done by hand for each dialogue domain. We propose a novel unsupervised method for learning such mappings from user reviews in the target domain, and test it on restaurant reviews. We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision. Experimental analyses show that the mappings learned cover most of the domain ontology, and provide good linguistic variation. A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline."
N06-2022,Automatic Recognition of Personality in Conversation,2006,18,49,2,1,39969,franccois mairesse,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"The identification of personality by automatic analysis of conversation has many applications in natural language processing, from leader identification in meetings to partner matching on dating websites. We automatically train models of the main five personality dimensions, on a corpus of conversation extracts and personality ratings. Results show that the models perform better than the baseline, and their analysis confirms previous findings linking language and personality, while revealing many new linguistic and prosodic markers."
polifroni-walker-2006-learning,Learning Database Content for Spoken Dialogue System Design,2006,15,18,2,0.753307,45985,joseph polifroni,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Spoken dialogue systems are common interfaces to backend data in information retrieval domains. As more data is made available on the Web and IE technology matures, dialogue systems, whether they be speech- or text-based, will be more in demand to provide user-friendly access to this data. However, dialogue systems must become both easier to configure, as well as more informative than the traditional form-based systems that are currently available. We present techniques in this paper to address the issue of automating both content selection for use in summary responses and in system initiative queries."
barker-etal-2006-simulating,Simulating Cub Reporter Dialogues: The collection of naturalistic human-human dialogues for information access to text archives,2006,9,1,5,0,33318,emma barker,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes a dialogue data collection experiment and resulting corpus for dialogues between a senior mobile journalist and a junior cub reporter back at the office. The purpose of the dialogue is for the mobile journalist to collect background information in preparation for an interview or on-the-site coverage of a breaking story. The cub reporter has access to text archives that contain such background information. A unique aspect of these dialogues is that they capture information-seeking behavior for an open-ended task against a large unstructured data source. Initial analyses of the corpus show that the experimental design leads to real-time, mixedinitiative, highly interactive dialogues with many interesting properties."
P04-1011,Trainable Sentence Planning for Complex Information Presentations in Spoken Dialog Systems,2004,16,113,3,0,16906,amanda stent,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, flexible and general, yet produce high quality output in particular domains. A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain. This paper presents a trainable sentence planner for the MATCH dialog system. We show that trainable sentence planning can produce output comparable to that of MATCH's template-based generator even for quite complex information presentations."
walker-2004-talk,Can We Talk? Prospects for Automatically Training Spoken Dialogue Systems,2004,8,0,1,1,6000,marilyn walker,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"There is a strong relationship between evaluation and methods for automatically training language processing systems, where generally the same resource and metrics are used both to train system components and to evaluate them. To date, in dialogue systems research, this general methodology is not typically applied to the dialogue manager and spoken language generator. I will argue that any metric that can be used to evaluate system performance should also be usable as a feedback function for automatically training the system. My argument is motivated with examples of the application of reinforcement learning to dialogue manager optimization, and the use of boosting to train the spoken language generator."
W02-0221,Training a Dialogue Act Tagger for Human-human and Human-computer Travel dialogues,2002,18,18,2,0.961538,4772,rashmi prasad,Proceedings of the Third {SIG}dial Workshop on Discourse and Dialogue,0,"While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the human-computer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available."
P02-1048,{MATCH}: An Architecture for Multimodal Dialogue Systems,2002,12,219,6,0,38458,michael johnston,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences, the task at hand, and the physical and social environment. We describe a multimodal application architecture which combines finite-state multimodal language processing, a speech-act based multimodal dialogue manager, dynamic multimodal output generation, and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output. Our testbed application MATCH (Multimodal Access To City Help) provides a mobile multimodal speech-pen interface to restaurant and sub-way information for New York City."
P02-1049,What{'}s the Trouble: Automatically Identifying Problematic Dialogues in {DARPA} Communicator Dialogue Systems,2002,10,17,3,0,1049,helen hastie,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Spoken dialogue systems promise efficient and natural access to information services from any phone. Recently, spoken dialogue systems for widely used applications such as email, travel information, and customer care have moved from research labs into commercial use. These applications can receive millions of calls a month. This huge amount of spoken dialogue data has led to a need for fully automatic methods for selecting a subset of caller dialogues that are most likely to be useful for further system improvement, to be stored, transcribed and further analyzed. This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain. We show that using fully automatic features we can identify classes of problematic dialogues with accuracies from 67% to 89%."
hastie-etal-2002-automatic,Automatic Evaluation: Using a {DATE} Dialogue Act Tagger for User Satisfaction and Task Completion Prediction,2002,14,13,3,0,1049,helen hastie,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The objective of the DARPA Communicator project is to support rapid, cost-effective development of multi-modal speech-enabled dialogue systems with advanced conversational capabilities. During the course of the Communicator program, we have been involved in developing methods for measuring progress towards the program goals and assessing advances in the component technologies required to achieve such goals. Our goal has been to develop a lightweight evaluation paradigm for heterogeneous systems. In this paper, we utilize the Communicator evaluation corpus from 2001 and build on previous work applying the PARADISE evaluation framework to establish a baseline for fully automatic system evaluation. We train a regression tree to predict User Satisfaction using a random 80 of the dialogues for training. The metrics (features) we use for prediction are a fully automatic Task Success Measure, Efficiency Measures, and System Dialogue Act Behaviors extracted from the dialogue logfiles using the DATE (Dialogue Act Tagging for Evaluation) tagging scheme. The learned tree with the DATE metrics has a correlation of 0.614 ( of 0.376) with the actual user satisfaction values for the held out test set, while the learned tree without the DATE metrics has a correlation of 0.595 ( of 0.35)."
whittaker-etal-2002-fish,Fish or Fowl:A {W}izard of {O}z Evaluation of Dialogue Strategies in the Restaurant Domain,2002,7,36,2,0,31546,steve whittaker,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Recent work on evaluation of spoken dialogue systems suggests that the information presentation phase of complex dialogues is often the primary contributor to dialogue duration. This indicates that better algorithms are needed for the presentation of complex information in speech. Currently however we lack data about the tasks and dialogue strategies on which to base such algorithms. In this paper, we describe a Wizard of Oz tool and a study which applies user models based on multi-attribute decision theory to the problem of generating tailored and concise system responses for a spoken dialogue system. The resulting Wizard corpus will be distributed by the LDC as part of our work on the ISLE project."
rambow-etal-2002-dependency,A Dependency Treebank for {E}nglish,2002,10,32,5,0,1354,owen rambow,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents the syntactic annotation level of a project aimed at providing a small dialog corpus with multiple levels of annotation. The syntactic annotation is based on dependency syntax. We outline the reasons for choosing dependency, and show the syntactic annotation for some constructions. We finish by describing the current state of the project."
C02-1138,Towards Automatic Generation of Natural Language Generation Systems,2002,17,27,4,0,15412,john chen,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were an automatic method for creating natural language generation components that can produce quality output efficiently. We conduct experiments that show that this goal appears to be realizable. In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FER-GUS, a stochastic surface, realizer. We show how these stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system."
P01-1056,Evaluating a Trainable Sentence Planner for a Spoken Dialogue System,2001,15,18,3,0.15377,1354,owen rambow,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches. In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments. In order to perform an exhaustive comparison, we also evaluate a hand-crafted template-based generation component, two rule-based sentence planners, and two baseline sentence planners. We show that the trainable sentence planner performs better than the rule-based systems and the baselines, and as well as the hand-crafted system."
P01-1066,Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems,2001,15,160,1,1,6000,marilyn walker,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes the application of the PARADISE evaluation framework to the corpus of 662 human-computer dialogues collected in the June 2000 Darpa Communicator data collection. We describe results based on the standard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme. We show that performance models derived via using the standard metrics can account for 37% of the variance in user satisfaction, and that the addition of DATE metrics improved the models by an absolute 5%."
N01-1003,{SP}o{T}: A Trainable Sentence Planner,2001,13,101,1,1,6000,marilyn walker,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences. In this paper, we present SPoT, a sentence planner, and a new methodology for automatically training SPoT on the basis of feedback provided by human judges. We reconceptualize the task into two distinct phases. First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input. Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan. The SPR uses ranking rules automatically learned from training data. We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan."
H01-1004,Amount of Information Presented in a Complex List: Effects on User Performance,2001,3,0,2,0,53923,dawn dutton,Proceedings of the First International Conference on Human Language Technology Research,0,"AT&T Communicator is a state-of-the-art speech-enabled telephony-based application that allows the end-user to, among other things, select and reserve airline itineraries. This experiment explores how the amount and structure of information presented in complex lists influences the user experience and the ability of subjects to successfully complete a selection task. Presenting all the relevant information needed for a decision at once was the factor that most positively influenced successful task completion and the user experience. Subjects preferred hearing all of the relevant information about each flight, without initiating additional dialog with the system. Additionally, successful task completion rates improved when all of the flights were presented at once, without any intervening questions from the system."
H01-1015,{DATE}: A Dialogue Act Tagging Scheme for Evaluation of Spoken Dialogue Systems,2001,27,50,1,1,6000,marilyn walker,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes a dialogue act tagging scheme developed for the purpose of providing finer-grained quantitative dialogue metrics for comparing and evaluating DARPA COMMUNICATOR spoken dialogue systems. We show that these dialogue act metrics can be used to quantify the amount of effort spent in a dialogue maintaining the channel of communication or, establishing the frame for communication, as opposed to actually carrying out the travel planning task that the system is designed to support. We show that the use of these metrics results in a 7% improvement in the fit in models of user satisfaction. We suggest that dialogue act metrics can ultimately support more focused qualitative analysis of the role of various dialogue strategy parameters, e.g. initiative, across dialogue systems, thus clarifying what development paths might be feasible for enhancing user satisfaction in future versions of these systems."
H01-1055,Natural Language Generation in Dialog Systems,2001,18,35,3,0.15377,1354,owen rambow,Proceedings of the First International Conference on Human Language Technology Research,0,"Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach. However, the improved speech recognition has brought to light a new problem: as dialog systems understand more of what the user tells them, they need to be more sophisticated at responding to the user. The issue of system response to users has been extensively studied by the natural language generation community, though rarely in the context of dialog systems. We show how research in generation can be adapted to dialog systems, and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques."
W00-0304,{NJF}un- A Reinforcement Learning Spoken Dialogue System,2000,5,27,4,0.563003,6782,diane litman,ANLP-NAACL 2000 Workshop: Conversational Systems,0,"This paper describes NJFun, a real-time spoken dialogue system that provides users with information about things to do in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users."
P00-1024,Learning Attribute Selections for Non-Pronominal Expressions,2000,21,23,2,0,36604,pamela jordan,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"A fundamental function of any task-oriented dialogue system is the ability to generate nominal expressions that describe objects in the task domain. In this paper, we report results from using machine learning to train and test a nominal-expression generator on a set of 393 nominal descriptions from the COCONUT corpus of task-oriented design dialogues. Results show that we can achieve a 50% match to human performance as opposed to a 16% baseline for just guessing the most frequent type of nominal expression in the COCONUT corpus. To our surprise our results indicate that many of the central features of previously proposed selection models did not improve the performance of the learned nominal-expression generator."
walker-etal-2000-evaluation,Evaluation for Darpa Communicator Spoken Dialogue Systems,2000,17,30,1,1,6000,marilyn walker,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The overall objective of the DARPA COMMUNICATOR project is to support rapid, cost-effective development of multi-modal speechenabled dialogue systems with advanced conversational capabilities, such as plan optimization, explanation and negotiation. In order to make this a reality, we need to find methods for evaluating the contribution of various techniques to the usersxe2x80x99 willingness and ability to use the system. This paper reports on the approach to spoken dialogue system evaluation that we are applying in the COMMUNICATOR program. We describe our overall approach, the experimental design, the logfile standard, and the metrics applied in the experimental evaluation planned for June of 2000."
walker-etal-2000-developing,Developing and Testing General Models of Spoken Dialogue System Peformance,2000,26,15,1,1,6000,marilyn walker,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The design of methods for performance evaluation is a major open research issue in the area of spoken language dialogue systems. This paper presents the PARADISE methodology for developing predictive models of spoken dialogue performance, and shows how to evaluate the predictive power and generalizability of such models. To illustrate the methodology, we develop a number of models for predicting system usability (as measured by user satisfaction), based on the application of PARADISE to experimental data from two different spoken dialogue systems. We compare both linear and tree-based models. We then measure the extent to which the models generalize across different systems, different experimental conditions, and different user populations, by testing models trained on a subset of the corpus against a test set of dialogues. The results show that the models generalize well across the two systems, and are thus a first approximation towards a general performance model of system usability."
C00-1073,Automatic Optimization of Dialogue Management,2000,11,70,4,0.563003,6782,diane litman,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,Designing the dialogue strategy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We then show that our approach measurably improves performance in an experimental system.
A00-2028,Learning to Predict Problematic Situations in a Spoken Dialogue System: Experiments with {H}ow {M}ay {I} {H}elp {Y}ou?,2000,13,86,1,1,6000,marilyn walker,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline."
P99-1040,Automatic Detection of Poor Speech Recognition at the Dialogue Level,1999,18,65,2,0.776028,6782,diane litman,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm."
P98-2129,Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent,1998,21,50,3,0.776028,6782,diane litman,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT's cooperative rather than literal strategy contributes to greater performance."
P98-2219,Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email,1998,18,101,1,1,6000,marilyn walker,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent's choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders."
C98-2124,Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent,1998,21,50,3,0.776028,6782,diane litman,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT's cooperative rather than literal strategy contributes to greater performance."
C98-2214,Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email,1998,18,101,1,1,6000,marilyn walker,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent's choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders."
W97-0601,Evaluating Interactive Dialogue Systems: Extending Component Evaluation to Integrated System Evaluation,1997,28,6,1,1,6000,marilyn walker,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,This paper discusses the range of ways in which spoken dialogue system components have been evaluated and discusses approaches to evaluation that attempt to integrate component evaluation into an overall view of system performance. We will argue that the PARADISE (PARAdigm for DIalogue System Evaluation) framework has several advantages over other proposals.
P97-1035,{PARADISE}: A Framework for Evaluating Spoken Dialogue Agents,1997,32,406,1,1,6000,marilyn walker,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity."
J97-4008,Book Reviews: Using Language,1997,-1,-1,1,1,6000,marilyn walker,Computational Linguistics,0,None
J97-1001,Empirical Studies in Discourse,1997,80,27,1,1,6000,marilyn walker,Computational Linguistics,0,"Computational theories of discourse are concerned with the context-based interpretation or generation of discourse phenomena in text and dialogue. In the past, research in this area focused on specifying the mechanisms underlying particular discourse phenomena; the models proposed were often motivated by a few constructed examples. While this approach led to many theoretical advances, models developed in this manner are difficult to evaluate because it is hard to tell whether they generalize beyond the particular examples used to motivate them. Recently however the field has turned to issues of robustness and the coverage of theories of particular phenomena with respect to specific types of data. This new empirical focus is supported by several recent advances: an increasing theoretical consensus on discourse models; a large amount of online dialogue and textual corpora available; and improvements in component technologies and tools for building and testing discourse and dialogue testbeds. This means that it is now possible to determine how representative particular discourse phenomena are, how frequently they occur, whether they are related to other phenomena, what percentage of the cases a particular model covers, the inherent difficulty of the problem, and how well an algorithm for processing or generating the phenomena should perform to be considered a good model. This issue brings together a collection of papers illustrating recent approaches to empirical research in discourse generation and interpretation. Section 2 gives a general overview of empirical studies in discourse and describes an empirical research strategy that leads from empirical findings to general theories. Section 3 discusses how each article exemplifies the empirical research strategy and how empirical methods have been employed in each research project."
J96-2005,Limited Attention and Discourse Structure,1996,24,70,1,1,6000,marilyn walker,Computational Linguistics,0,"Cet article examine le role de l'attention limitee dans un modele informatique de traitement du discours. Le modele de cache est propose comme une implementation informatique de la memoire de travail humaine ; les operations sur l'etat d'attention sont formulees comme des operations sur un cache. Puisqu'un cache peut etre utilise pour traiter les references et les operations d'un programme structure hierarchiquement, alors un cache peut etre utilise pour modeliser un etat d'attention quand les intentions du discours sont structures hierarchiquement"
W94-0320,The Role of Cognitive Modeling in Communicative Intentions,1994,17,0,2,0.15377,1354,owen rambow,Proceedings of the Seventh International Workshop on Natural Language Generation,0,"A discourse planner for (task-oriented) dialogue must be able to make choices about whether relevant, but optional information (for example, the satellites in an RST-based planner) should be communicated. We claim that effective text planners must explicitly model aspects of the Hearer's cognitive state, such as what the hearer is attending to and what inferences the hearer can draw, in order to make these choices. We argue that a mere representation of the Hearer's knowledge is inadequate. We support this claim by (1) an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task)."
J94-2003,{J}apanese Discourse and the Process of Centering,1994,54,163,1,1,6000,marilyn walker,Computational Linguistics,0,"This paper has three aims: (1) to generalize a compulational account of the discourse process called CENTERING, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process; we demonstrate this argument with respect to the interpretation of ZEROS, unexpressed arguments of the verb, in Japanese. The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical TOPIC, the postposition wa, as well as those for grammatical functions such as SUBJECT, ga, OBJECT, o and OBJECT2, ni. In addition, we investigate the role of speaker's EMPATHY, which is the viewpoint from which an event is described. This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as kureta, kita. Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of ZEROS, but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity. We propose a discourse rule of ZERO TOPIC ASSIGNMENT, and show that CENTERING provides constraints on when a ZERO can be interpreted as the ZERO TOPIC."
C94-2196,Discourse and Deliberation: Testing a Collaborative Strategy,1994,23,15,1,1,6000,marilyn walker,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A discourse strategy is a strategy for communicating with another agent. Designing effective dialogue systems requires designing agents that can choose among discourse strategies. We claim that the design of effective strategies must take cognitive factors into account, propose a new method for testing thehypothesized factors, and present experimental results on an effective strategy for supporting deliberation. The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics."
W93-0238,Information and Deliberation in Discourse,1993,-1,-1,1,1,6000,marilyn walker,Intentionality and Structure in Discourse Relations,0,None
C92-2122,A Case Study of Natural Language Customisation: The Practical Effects of World Knowledge,1992,6,2,1,1,6000,marilyn walker,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,This paper proposes a methodology for the customisation of natural language interfaces to information retrieval applications. We report a field study in which we tested this methodology by customising a commercially available natural language system to a large database of sales and marketing information. We note that it was difficult to tailor the common sense reasoning capabilities of the particular system we used to our application. This study validates aspects of the suggested methodology as well as providing insights that should inform the design of natural language systems for this class of applications.
C92-1054,Redundancy in Collaborative Dialogue,1992,9,78,1,1,6000,marilyn walker,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
P90-1010,Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation,1990,27,187,1,1,6000,marilyn walker,28th Annual Meeting of the Association for Computational Linguistics,1,"Conversation between two people is usually of MIXED-INITIATIVE, with CONTROL over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles."
P89-1031,Evaluating Discourse Processing Algorithms,1989,30,85,1,1,6000,marilyn walker,27th Annual Meeting of the Association for Computational Linguistics,1,"In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualititive evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining."
