2020.clssts-1.7,Reformulating Information Retrieval from Speech and Text as a Detection Problem,2020,-1,-1,5,0,21849,damianos karakos,Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020),0,"In the IARPA MATERIAL program, information retrieval (IR) is treated as a hard detection problem; the system has to output a single global ranking over all queries, and apply a hard threshold on this global list to come up with all the hypothesized relevant documents. This means that how queries are ranked relative to each other can have a dramatic impact on performance. In this paper, we study such a performance measure, the Average Query Weighted Value (AQWV), which is a combination of miss and false alarm rates. AQWV requires that the same detection threshold is applied to all queries. Hence, detection scores of different queries should be comparable, and, to do that, a score normalization technique (commonly used in keyword spotting from speech) should be used. We describe unsupervised methods for score normalization, which are borrowed from the speech field and adapted accordingly for IR, and demonstrate that they greatly improve AQWV on the task of cross-language information retrieval (CLIR), on three low-resource languages used in MATERIAL. We also present a novel supervised score normalization approach which gives additional gains."
2020.clssts-1.8,The 2019 {BBN} Cross-lingual Information Retrieval System,2020,-1,-1,12,0,21853,le zhang,Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020),0,"In this paper, we describe a cross-lingual information retrieval (CLIR) system that, given a query in English, and a set of audio and text documents in a foreign language, can return a scored list of relevant documents, and present findings in a summary form in English. Foreign audio documents are first transcribed by a state-of-the-art pretrained multilingual speech recognition model that is finetuned to the target language. For text documents, we use multiple multilingual neural machine translation (MT) models to achieve good translation results, especially for low/medium resource languages. The processed documents and queries are then scored using a probabilistic CLIR model that makes use of the probability of translation from GIZA translation tables and scores from a Neural Network Lexical Translation Model (NNLTM). Additionally, advanced score normalization, combination, and thresholding schemes are employed to maximize the Average Query Weighted Value (AQWV) scores. The CLIR output, together with multiple translation renderings, are selected and translated into English snippets via a summarization model. Our turnkey system is language agnostic and can be quickly trained for a new low-resource language in few days."
2020.clssts-1.9,What Set of Documents to Present to an Analyst?,2020,-1,-1,2,0,21851,richard schwartz,Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020),0,"We describe the human triage scenario envisioned in the Cross-Lingual Information Retrieval (CLIR) problem of the [REDUCT] Program. The overall goal is to maximize the quality of the set of documents that is given to a bilingual analyst, as measured by the AQWV score. The initial set of source documents that are retrieved by the CLIR system is summarized in English and presented to human judges who attempt to remove the irrelevant documents (false alarms); the resulting documents are then presented to the analyst. First, we describe the AQWV performance measure and show that, in our experience, if the acceptance threshold of the CLIR component has been optimized to maximize AQWV, the loss in AQWV due to false alarms is relatively constant across many conditions, which also limits the possible gain that can be achieved by any post filter (such as human judgments) that removes false alarms. Second, we analyze the likely benefits for the triage operation as a function of the initial CLIR AQWV score and the ability of the human judges to remove false alarms without removing relevant documents. Third, we demonstrate that we can increase the benefit for human judgments by combining the human judgment scores with the original document scores returned by the automatic CLIR system."
P15-1004,Statistical Machine Translation Features with Multitask Tensor Networks,2015,34,4,7,0,14459,hendra setiawan,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various nonlocal translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is 2.7 and 1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features."
P14-1129,Fast and Robust Neural Network Joint Models for Statistical Machine Translation,2014,29,332,6,0.666667,9604,jacob devlin,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements."
2014.iwslt-papers.11,Anticipatory translation model adaptation for bilingual conversations,2014,-1,-1,5,0,37937,sanjika hewavitharana,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"Conversational spoken language translation (CSLT) systems facilitate bilingual conversations in which the two participants speak different languages. Bilingual conversations provide additional contextual information that can be used to improve the underlying machine translation system. In this paper, we describe a novel translation model adaptation method that anticipates a participant{'}s response in the target language, based on his counterpart{'}s prior turn in the source language. Our proposed strategy uses the source language utterance to perform cross-language retrieval on a large corpus of bilingual conversations in order to obtain a set of potentially relevant target responses. The responses retrieved are used to bias translation choices towards anticipated responses. On an Iraqi-to-English CSLT task, our method achieves a significant improvement over the baseline system in terms of BLEU, TER and METEOR metrics."
N13-1069,Systematic Comparison of Professional and Crowdsourced Reference Translations for Machine Translation,2013,11,7,5,1,21850,rabih zbib,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazonxe2x80x99s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation."
N12-1006,Machine Translation of {A}rabic Dialects,2012,20,90,7,1,21850,rabih zbib,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Arabic Dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine-English and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon's Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus."
W10-1763,Decision Trees for Lexical Smoothing in Statistical Machine Translation,2010,23,8,4,1,21850,rabih zbib,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We present a method for incorporating arbitrary context-informed word attributes into statistical machine translation by clustering attribute-qualified source words, and smoothing their word translation probabilities using binary decision trees. We describe two ways in which the decision trees are used in machine translation: by using the attribute-qualified source word clusters directly, or by using attribute-dependent lexical translation probabilities that are obtained from the trees, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-of-speech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system."
2006.amta-papers.25,A Study of Translation Edit Rate with Targeted Human Annotation,2006,-1,-1,5,0,44205,matthew snover,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU{---}even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as{---}or better than{---}a second human judgment does."
H94-1065,Adaptation to New Microphones Using Tied-Mixture Normalization,1994,7,3,3,0,56385,anastasios anastasakos,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"In this paper, we present several approaches designed to increase the robustness of BYBLOS, the BBN continuous speech recognition system. We address the problem of increased degradation in performance when there is mismatch in the characteristics of the training and the test microphones. We introduce a new supervised adaptation algorithm that computes a transformation from the training microphone codebook to that of a new microphone, given some information about the new microphone. Results are reported for the development and evaluation test sets of the 1993 ARPA CSR Spoke 6 WSJ task, which consist of speech recorded with two alternate microphones, a stand-mount and a telephone microphone. The proposed algorithm improves the performance of the system when tested with the stand-mount microphone by reducing the difference in error rate between the high quality training microphone and the alternate stand-mount microphone recordings by a factor of 2. Several results are presented for the telephone speech leading to important conclusions: a) the performance on telephone speech is dramatically improved by simply retraining the system on the high-quality training data after they have been bandlimited in the telephone bandwith; and b) additional training data recorded with the high quality microphone give further substantial improvement in performance."
H94-1086,On-Line Cursive Handwriting Recognition Using Hidden {M}arkov Models and Statistical Grammars,1994,11,22,1,1,21852,john makhoul,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"The BYBLOS continuous speech recognition system is applied to on-line cursive handwriting recognition. By exploiting similarities between on-line cursive handwriting and continuous speech recognition, we can use the same base system adapted to handwriting feature vectors instead of speech. The use of hidden Markov models obviates the need for segmentation of the handwritten script sentences before recognition. To test our system, we collected handwritten sentences using text from the ARPA Airline Travel Information Service (ATIS) and the ARPA Wall Street Journal (WSJ) corpora. In an initial experiment on the ATIS data, a word error rate of 1.1% was achieved with a 3050-word lexicon, 52-character set, collected from one writer. In a subsequent writer-dependent test on the WSJ data, error rates ranging between 2%-5% were obtained with a 25,595-word lexicon, 86-character set, collected from six different writers. Details of the recognition system, the data collection process, and analysis of the experiments are presented."
H94-1088,Robust Continuous Speech Recognition,1994,0,7,1,1,21852,john makhoul,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"PROBLEM TO BE SOLVED: To provide a continuous speech recognition device permitting to limit hypotheses about a word by a narrow beam width and perform a continuous speech recognition of natural utterance at a low calculation cost. SOLUTION: A ward collating part 4 detects hypotheses about a word of an uttered speech sentence and calculates an acoustic likelihood for outputting it, based on featured parameters of a speech signal of the uttered speech sentence inputted, for example, by using one-pass Viterbi decoding method. A likelihood correction part 7, relatively to the hypotheses about the word from the word collating part 4, delays the acoustic likelihood at the time directional center part of each speech element of the word so that the likelihood is shifted to a time delayed later than the center part and corrects the acoustic likelihood of the hypothesis about the word. A word hypothesis limiting part 6, based on the word hypothesis with all likelihood including the acoustic likelihood from the likelihood correction part 7, limits the word hypotheses so as to make one word hypothesis represent, which has a highest likelihood among the all likelihood calculated from the starting time of the utterance until the stop time of the word for each leading phoneme environment of the word."
H94-1090,"Usable, Real-Time, Interactive Spoken Language Systems",1994,-1,-1,1,1,21852,john makhoul,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,None
H93-1015,Comparative Experiments on Large Vocabulary Speech Recognition,1993,11,22,4,0.384615,21851,richard schwartz,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"This paper describes several key experiments in large vocabulary speech recognition. We demonstrate that, counter to our intuitions, given a fixed amount of training speech, the number of training speakers has little effect on the accuracy. We show how much speech is needed for speaker-independent (SI) recognition in order to achieve the same performance as speaker-dependent (SD) recognition. We demonstrate that, though the N-Best Paradigm works quite well up to vocabularies of 5,000 words, it begins to break down with 20,000 words and long sentences. We compare the performance of two feature preprocessing algorithms for microphone independence and we describe a new microphone adaptation algorithm based on selection among several codebook transformations."
H93-1079,Robust Continuous Speech Recognition,1993,0,0,1,1,21852,john makhoul,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"PURPOSE:To handle speech recognition and language processing untidily even when the state of the head of speaking is unstable by using a bilateral purging table in the recognition of a continuous speech to predict input speech data, verifying the prediction by the word spotting function of a speech recognition part, and further performing island drive type processing. CONSTITUTION:This continuous speech recognition device is equipped with the speech recognition part which performs word spotting by using the input speech and a predictive purger part 505 which uses the action of the bilateral purging table 506 for word prediction to perform the island drive type processing; and the presence of a word predicted by the predictive purger part 505 is verified by driving the speech recognition part 502."
H93-1081,"Usable, Real-Time, Interactive Spoken Language Systems",1993,0,0,1,1,21852,john makhoul,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"The primary objective of this project is develop a robust, high-performance, domain-independent spoken language system. The system, termed HARC (Hear And Respond to Continuous speech), is composed of the BYBLOS speech recognition and the DELPHI natural language understanding system. The goal is to develop systems that exhibit the following advances: high-accuracy speech understanding with vocabulary of up to 10,000 words; a highly interactive user interface capable of mixed-initiative dialogue and other types of system feedback; transparent adaptability to new users; easy portability to new applications; a system implementable in real-time on cost-effective COTS (commercial, off-the-shelf) hardware."
H92-1012,Session 3: Spoken Language Systems {III},1992,-1,-1,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
H92-1014,{BBN} {BYBLOS} and {HARC} {F}ebruary 1992 {ATIS} Benchmark Results,1992,9,13,7,0.964912,48824,francis kubala,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"We present results from the February '92 evaluation on the ATIS travel planning domain for HARC, the BBN spoken language system (SLS). In addition, we discuss in detail the individual performance of BYBLOS, the speech recognition (SPREC) component.In the official scoring, conducted by NIST, BBN's HARC system produced a weighted SLS score of 43.7 on all 687 evaluable utterances in the test set. This was the lowest error achieved by any of the 7 systems evaluated.For the SPREC evaluation BBN's BYBLOS system achieved a word error rate of 6.2% on the same 687 utterances and 9.4% on the entire test set of 971 utterances. These results were significantly better than any other speech system evaluated."
H92-1049,{BBN} Real-Time Speech Recognition Demonstrations,1992,2,0,5,0.952381,57008,steve austin,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"Typically, real-time speech recognition -- if achieved at all -- is accomplished either by greatly simplifying the processing to be done, or by the use of special-purpose hardware. Each of these approaches has obvious problems. The former results in a substantial loss in accuracy, while the latter often results in obsolete hardware being developed at great expense and delay."
H92-1096,Development of a Spoken Language System,1992,0,0,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The primary objective of this project is to develop a robust, high-performance spoken language system. We have achieved this by integrating the BYBLOS speech recognition system with the DELPHI natural language processing system to produce the HARC (Hear And Respond to Continuous speech) system."
H92-1097,Robust Continuous Speech Recognition,1992,0,7,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The primary objective of this basic research program is to develop robust methods and models for speaker-independent acoustic recognition of spontaneously-produced, continuous speech. The work has focussed on developing accurate and detailed models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition. Important goals of this work are to achieve the highest possible word recognition accuracy in continuous speech and to develop methods for the rapid adaptation of phonetic models to the voice of a new speaker."
H91-1080,Research in Continuous Speech Recognition,1991,7,0,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"A primary application of the study of hesitation phenomena lies in improving the performance of automatic recognizers, given an input of spontaneous speech (e.g., verbal conversions with computer databases). Speech researchers have often expressed interest in exploiting the intonation of spoken utterances in the recognition process, but have been deterred by the complex nature of how intonation (including pauses) relates to the text of an utterance. Even straightforward phenomena such as unfilled pauses (i.e., silence periods-which are generally easy to identify, if long enough) are not reliable indicators to the syntactic or semantic sentence structure of an utterance"
H91-1081,Spoken Language Systems,1991,-1,-1,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,None
H90-1022,Developing an Evaluation Methodology for Spoken Language Systems,1990,3,22,3,0.486372,56331,madeleine bates,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"There has been a long-standing methodology for evaluating work in speech recognition (SR), but until recently no community-wide methodology existed for either natural language (NL) researchers or speech understanding (SU) researchers for evaluating the systems they developed.Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology is being used in practice for the first time.This paper gives an overview of the process that was followed in creating a meaningful evaluation mechanism, describes the current mechanism, and presents some directions for future development."
H90-1079,Research in Continuous Speech Recognition,1990,-1,-1,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,None
H90-1080,Spoken Language Systems,1990,0,0,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The papers in this session addressed issues in combining speech recognition with natural language systems. The first three papers concern the use of grammars. Speech recognizers and Natural Language parsers make different requirements of language knowledge. Recognizers need efficient methods for constraining the search space, while parsers need detailed analytical knowledge. One solution to the problem of integrating speech recognizers with NL processors is to use different language constraints in the two modules. This in effect means using different grammars for recognizing and parsing. The recognizer may use no grammar or simple, efficient grammars, while the parser uses a more complete representation of the language. This means that the recognizer can overgenerate, or produce strings not acceptable to the parser. In this case, a recognition error can lead to a failure to parse the utterance. One solution to this problem is to use an N-Best recognizer. Such a recognizer produces the N (where N is preset) best scoring hypotheses for an utterance. These hypotheses are passed to the parser which can then pick the overall best one."
H89-2033,Improved {HMM} Models for High Performance Speech Recognition,1989,9,7,7,0,57008,steve austin,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"In this paper we report on the various techniques that we implemented in order to improve the basic speech recognition performance of the BYBLOS system. Some of these methods are new, while others are not. We present methods that improved performance as well as those that did not. The methods include Linear Discriminant Analysis, Supervised Vector Quantization, Shared Mixture VQ. Deleted Estimation of Context Weights, MMI Estimation Using N-Best Alternatives, Cross-Word Triphone Models. While we have not yet combined all of the methods in one system, the overall word recognition error rate on the May 1988 test set using the Word-Pair grammar has decreased from 3.4% to 1.7%."
H89-2035,Automatic Detection Of New Words In A Large Vocabulary Continuous Speech Recognition System,1989,1,3,3,0,57760,ayman asadi,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"In practical large vocabulary speech recognition systems, it is nearly impossible for a speaker to remember which words are in the vocabulary. The probability of the speaker using words outside the vocabulary can be quite high. For the case when a speaker uses a new word, current systems will always' recognize other words within the vocabulary in place of the new word, and the speaker wouldn't know what the problem is.In this paper, we describe a preliminary investigation of techniques that automatically detect when the speaker has used a word that is not in the vocabulary. We developed a technique that uses a general model for the acoustics of any word to recognize the existence of new words. Using this general word model, we measure the correct detection of new words versus the false alarm rate.Experiments were run using the DARPA 1000-word Resource Management Database for continuous speech recognition. The recognition system used is the BBN BYBLOS continuous speech recognition system (Chow et al., 1987). The preliminary results indicate a detection rate of 74% with a false alarm rate of 3.4%."
H89-2058,Research in Continuous Speech Recognition,1989,0,0,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,The primary goal of this basic research is to develop improved methods and models for acoustic recognition of continuous speech. The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition. Important goals of this work are to achieve the highest possible word recognition accuracy in continuous speech and to develop methods for the rapid adaptation of phonetic models to the voice of a new speaker.
H89-2059,Spoken Language Systems,1989,0,0,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"The papers in this session were concerned with higher-level processing in speech recognition systems and, in some cases, the interface between the speech-recognition and natural-language components of a spoken language system. The session consisted of talks from four DARPA sites, Dragon Systems, SRI International, BBN Systems and Technologies, and MIT Lincoln Laboratory. These talks were followed by a period of free discussion."
H89-2077,White Paper on Spoken Language Systems,1989,0,4,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"Spoken language is the most natural and common form of human-human communication, whether face to face, over the telephone, or through various communication media such as radio and television. In contrast, human-machine interaction is currently achieved largely through keyboard strokes, pointing, or other mechanical means, using highly stylized languages. Communication, whether human-human or human-machine, suffers greatly when the two communicating agents do not speak the same language. The ultimate goal of work on spoken language systems is to overcome this language barrier by building systems that provide the necessary interpretive function between various languages, thus establishing spoken language as a versatile and natural communication medium between humans and machines and among humans speaking different languages."
H89-1006,Research in Continuous Speech Recognition,1989,0,0,1,1,21852,john makhoul,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,The primary goal of this basic research is to develop improved methods and models for acoustic recognition of continuous speech. The work has focussed on developing accurate and detailed mathematical models of phonemes and their coarticulation for the purpose of large-vocabulary continuous speech recognition. Important goals of this work are to achieve the highest possible word recognition accuracy in continuous speech and to develop methods for the rapid adaptation of phonetic models to the voice of a new speaker.
H89-1010,The {BBN} {BYBLOS} Continuous Speech Recognition System,1989,12,23,8,0,21851,richard schwartz,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"In this paper we describe the algorithms used in the BBN BYBLOS Continuous Speech Recognition system. The BYBLOS system uses context-dependent hidden Markov models of phonemes to provide a robust model of phonetic coarticulation. We provide an update of the ongoing research aimed at improving the recognition accuracy. In the first experiment we confirm the large improvement in accuracy that can be derived by using spectral derivative parameters in the recognition. In particular, the word error rate is reduced by a factor of two. Currently the system achieves a word error rate of 2.9% when tested on the speaker-dependent part of the standard 1000-Word DARPA Resource Management Database using the Word-Pair grammar supplied with the database. When no grammar was used, the error rate is 15.3%. Finally, we present a method for smoothing the discrete densities on the states of the HMM, which is intended to alleviate the problem of insufficient training for detailed phonetic models."
H89-1011,Speaker Adaptation from Limited Training in the {BBN} {BYBLOS} Speech Recognition System,1989,6,3,3,0,48824,francis kubala,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"The BBN BYBLOS continuous speech recognition system has been used to develop a method of speaker adaptation from limited training. The key step in the method is the estimation of a probabilistic spectral mapping between a prototype speaker, for whom there exists a well-trained speaker-dependent hidden Markov model (HMM), and a target speaker for whom there is only a small amount of training speech available. The mapping defines a set of transformation matrices which are used to modify the parameters of the prototype model. The resulting transformed model is then used as an approximation to a well-trained model for the target speaker. We review the techniques employed to accomplish this transformation and present experimental results conducted on the DARPA Resource Management database."
