alfonseca-manandhar-2002-improving,C92-2070,0,\N,Missing
alfonseca-manandhar-2002-improving,C92-2082,0,\N,Missing
alfonseca-manandhar-2002-improving,A00-1031,0,\N,Missing
alfonseca-manandhar-2002-proposal,J93-2004,0,\N,Missing
alfonseca-manandhar-2002-proposal,J96-3009,0,\N,Missing
alfonseca-manandhar-2002-proposal,C92-2082,0,\N,Missing
alfonseca-manandhar-2002-proposal,A00-1031,0,\N,Missing
C10-1142,A00-2018,0,0.022523,"Missing"
C10-1142,D08-1094,0,0.22877,"Missing"
C10-1142,P07-1028,0,0.0113016,"onal meaning of the word sequence eats mice is governed by the verb eats. Following this model, the distributional vector (s) can be written as: (s) ≈ (h, s) (7) where w ~ is the distributional vector of the word w, Rw is the set of the vectors representing the direct −1 is selectional preferences of the word w, and Rw the set of the vectors representing the indirect selectional preferences of the word w. Given a set of −1 syntactic relations R, the set Rw and Rw contain respectively a selectional preference vector Rw (r) and Rw (r)−1 for each r ∈ R. Selectional preferences are computed as in Erk (2007). If x is the semantic head of sequence s, then the model can be written as: r (s) = (x, x ← − y) = ~x Ry (r) (8) Otherwise, if y is the semantic head: r (s) = (y, x ← − y) = ~y Rx−1 (r) (9) is in both cases realised using BAM or BMM. We will call these models: basic additive model with selectional preferences (BAM-SP) and basic multiplicative model with selectional preferences (BMM-SP). Both Mitchell and Lapata (2008) and Erk and Pad´o (2008) experimented with few empirically estimated parameters. Thus, the general additive CDS model has not been adequately explored. Estimating Additive Compo"
C10-1142,W09-0209,1,0.774592,"l., 1990). Moore-Penrose pseudoinverse (Penrose, 1955) is computed in the following way. Let the original matrix Q have n rows and m columns and be of rank r. The SVD decomposition of the original matrix Q is Q = U ΣV T where Σ is a square diagonal matrix of dimension r. Then, the pseudoinverse matrix that minimises the equation 18 is: Q+ = V Σ+ U T (20) where the diagonal matrix Σ+ is the r × r transposed matrix of Σ having as diagonal elements the reciprocals of the singular values δ11 , δ12 , ..., δ1r of Σ. Using SVD to compute the pseudo-inverse matrix allows for different approximations (Fallucchi and Zanzotto, 2009). The algorithm for computing the singular value decomposition is iterative (Golub and Kahan, 1965). Firstly derived dimensions have higher singular value. Then, dimension k is more informative than dimension k 0 > k. We can consider different values for k to obtain different SVD for the approximations Q+ k of the original matrix Q+ in equation 20), i.e.: 1266 + T Q+ k = Vn×k Σk×k Uk×m (21) where Q+ k is a matrix n by m obtained considering the first k singular values. 4 Building positive and negative examples As explained in the previous section, estimating CDS models, needs a set of triples"
C10-1142,P09-2017,1,0.85318,"Missing"
C10-1142,J03-4004,0,0.00938077,"model for compositional distributional semantics. 1 Given the successful application to words, distributional semantics has been extended to word sequences. This has happened in two ways: (1) via the reformulation of DH for specific word sequences (Lin and Pantel, 2001); and (2) via the definition of compositional distributional semantics (CDS) models (Mitchell and Lapata, 2008; Jones and Mewhort, 2007). These are two different ways of addressing the problem. Introduction Lexical distributional semantics has been largely used to model word meaning in many fields as computational linguistics (McCarthy and Carroll, 2003; Manning et al., 2008), linguistics (Harris, 1964), corpus linguistics (Firth, 1957), and cognitive research (Miller and Charles, 1991). The fundamental hypothesis is the distributional hypothesis (DH): “similar words share similar contexts” (Harris, 1964). Recently, this hypothesis has been operationally defined in many ways in the fields of Lin and Pantel (2001) propose the pattern distributional hypothesis that extends the distributional hypothesis for specific patterns, i.e. word sequences representing partial verb phrases. Distributional meaning for these patterns is derived directly by"
C10-1142,P08-1028,0,0.797891,"xtracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics. 1 Given the successful application to words, distributional semantics has been extended to word sequences. This has happened in two ways: (1) via the reformulation of DH for specific word sequences (Lin and Pantel, 2001); and (2) via the definition of compositional distributional semantics (CDS) models (Mitchell and Lapata, 2008; Jones and Mewhort, 2007). These are two different ways of addressing the problem. Introduction Lexical distributional semantics has been largely used to model word meaning in many fields as computational linguistics (McCarthy and Carroll, 2003; Manning et al., 2008), linguistics (Harris, 1964), corpus linguistics (Firth, 1957), and cognitive research (Miller and Charles, 1991). The fundamental hypothesis is the distributional hypothesis (DH): “similar words share similar contexts” (Harris, 1964). Recently, this hypothesis has been operationally defined in many ways in the fields of Lin and P"
C10-1142,J07-2002,0,0.0675455,"Missing"
C10-1142,W01-0513,0,0.0186172,"~xi , ~yi ). Examples are positive in the sense that ~zi = (xy) for an ideal CDS. There are no available sets to contain such triples, with the exception of the set used in Mitchell and Lapata (2008) which is designed only for testing purposes. It contains similar and dissimilar pairs of sequences (s1 ,s2 ) where each sequence is a verb-noun pair (vi ,ni ). From the positive part of this set, we can only derive quadruples where (v1 n1 ) ≈ (v2 n2 ) but we cannot derive the ideal resulting vector of the composition (vi ni ). Sets used to test multiword expression (MWE) detection models (e.g., (Schone and Jurafsky, 2001; Nicholson and Baldwin, 2008; Kim and Baldwin, 2008; Cook et al., 2008; Villavicencio, 2003; Korkontzelos and Manandhar, 2009)) are again not useful as containing only valid MWE that cannot be used to determine the set of training triples needed here. As a result, we need a novel idea to build sets of triples to train CDS models. We can leverage on knowledge stored in dictionaries. In the rest of the section, we describe how we build the positive example set E and a control negative example set N E. Elements of the two sets are pairs (t,s) where t is a target word s is a sequence of words. t"
C10-1142,W03-1808,0,0.0193835,"ailable sets to contain such triples, with the exception of the set used in Mitchell and Lapata (2008) which is designed only for testing purposes. It contains similar and dissimilar pairs of sequences (s1 ,s2 ) where each sequence is a verb-noun pair (vi ,ni ). From the positive part of this set, we can only derive quadruples where (v1 n1 ) ≈ (v2 n2 ) but we cannot derive the ideal resulting vector of the composition (vi ni ). Sets used to test multiword expression (MWE) detection models (e.g., (Schone and Jurafsky, 2001; Nicholson and Baldwin, 2008; Kim and Baldwin, 2008; Cook et al., 2008; Villavicencio, 2003; Korkontzelos and Manandhar, 2009)) are again not useful as containing only valid MWE that cannot be used to determine the set of training triples needed here. As a result, we need a novel idea to build sets of triples to train CDS models. We can leverage on knowledge stored in dictionaries. In the rest of the section, we describe how we build the positive example set E and a control negative example set N E. Elements of the two sets are pairs (t,s) where t is a target word s is a sequence of words. t is the word that represent the distributional meaning of s in the case of E. Contrarily, t i"
C10-2055,P08-1088,0,0.518138,"omain terms that co-occur in English and Spanish. The source word is powers and the target word is poderes. The word delegation and delegacion are the highly associated words with the source word and the target word respectively. Their in-domain terms, as shown in the middle, can be used to map the source word in context of word delegation to its corresponding target word in context of delegacion. and English monolingual corpora. They expanded the lexicon with the standard context-based approach and achieved about 25.0 percent accuracy (Koehn and Knight, 2002). Similar techniques were used in Haghighi et al. (2008) who employ dimension reduction in the extraction method. They recorded 58.0 percent as their best F1 score for the context vector approach on non-parallel comparable corpora containing Wikipedia articles. However, their method scores less on comparable corpora containing distinct sentences derived from the Europarl English-Spanish corpus. 3 Learning in-domain terms In the standard context vector approach, we associate each source word and target word with their context vectors. The source and target context vectors are then compared using the initial seed dictionary and a similarity measure."
C10-2055,W09-1702,1,0.747264,"qg , the Euclidean distance is given by: v u g uX dist(P, Q) = t (pi , qi )2 i=1 1 We used the following formula to estimate the number of bins: g = 1 + 3.3 ∗ log ( |ID(WS , WR ) |) In the next section, we describe the setup including the data, the lexicon and the evaluation used in our experiments. 5 Experimental setup 5.1 Data For comparable text, we derive English and Spanish distinct sentences from the Europarl parallel corpora. We split the corpora into three parts according to year. We used about 500k sentences for each language in the experiments. This approach is further explained in Ismail and Manandhar (2009) and is similar to Koehn and Knight (2001) and Haghighi et al. (2008). 5.2 Pre-processing For corpus pre-processing, we use sentence boundary detection and tokenization on the raw text before we clean the tags and filter stop words. We sort and rank words in the text according to their frequencies. For each of these words, we compute their context term log-likelihood values. 5.3 Lexicon In the experiment, a bilingual lexicon is required for evaluation. We extract our evaluation lexicon from the Word Reference2 free online dictionary. This extracted bilingual lexicon has low coverage. 485 2 htt"
C10-2055,W95-0114,0,0.666098,"od is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words. We provide a method for identifying such terms. Our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary. In addition, we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary. 1 Introduction In bilingual lexicon extraction, the context-based approach introduced by Rapp (1995) is widely used (Fung, 1995; Diab and Finch, 2000; among others). The focus has been on learning from comparable corpora since the late 1990s (Rapp, 1999; Koehn and Knight, 2002; among others). However, so far, the accuracy of bilingual lexicon extraction using comparable corpora is quite poor especially when orthographic features are not used. Moreover, when orthographic features are not used, a large initial seed dictionary is essential in order to acquire higher accuracy lexicon (Koehn and Knight, 2002). This means that cur2 Related work Most of the early work in bilingual lexicon extraction employ an initial seed di"
C10-2055,W01-0504,0,0.0134922,"g uX dist(P, Q) = t (pi , qi )2 i=1 1 We used the following formula to estimate the number of bins: g = 1 + 3.3 ∗ log ( |ID(WS , WR ) |) In the next section, we describe the setup including the data, the lexicon and the evaluation used in our experiments. 5 Experimental setup 5.1 Data For comparable text, we derive English and Spanish distinct sentences from the Europarl parallel corpora. We split the corpora into three parts according to year. We used about 500k sentences for each language in the experiments. This approach is further explained in Ismail and Manandhar (2009) and is similar to Koehn and Knight (2001) and Haghighi et al. (2008). 5.2 Pre-processing For corpus pre-processing, we use sentence boundary detection and tokenization on the raw text before we clean the tags and filter stop words. We sort and rank words in the text according to their frequencies. For each of these words, we compute their context term log-likelihood values. 5.3 Lexicon In the experiment, a bilingual lexicon is required for evaluation. We extract our evaluation lexicon from the Word Reference2 free online dictionary. This extracted bilingual lexicon has low coverage. 485 2 http://wordreference.com 5.4 Evaluation In th"
C10-2055,W02-0902,0,0.258353,"d for identifying such terms. Our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary. In addition, we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary. 1 Introduction In bilingual lexicon extraction, the context-based approach introduced by Rapp (1995) is widely used (Fung, 1995; Diab and Finch, 2000; among others). The focus has been on learning from comparable corpora since the late 1990s (Rapp, 1999; Koehn and Knight, 2002; among others). However, so far, the accuracy of bilingual lexicon extraction using comparable corpora is quite poor especially when orthographic features are not used. Moreover, when orthographic features are not used, a large initial seed dictionary is essential in order to acquire higher accuracy lexicon (Koehn and Knight, 2002). This means that cur2 Related work Most of the early work in bilingual lexicon extraction employ an initial seed dictionary. A large bilingual lexicon with 10k to 20k entries is necessary (Fung, 1995; Rapp, 1999). Koehn and Knight (2002) introduce techniques for co"
C10-2055,P95-1050,0,0.30421,"bilingual lexicons. Our method is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words. We provide a method for identifying such terms. Our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary. In addition, we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary. 1 Introduction In bilingual lexicon extraction, the context-based approach introduced by Rapp (1995) is widely used (Fung, 1995; Diab and Finch, 2000; among others). The focus has been on learning from comparable corpora since the late 1990s (Rapp, 1999; Koehn and Knight, 2002; among others). However, so far, the accuracy of bilingual lexicon extraction using comparable corpora is quite poor especially when orthographic features are not used. Moreover, when orthographic features are not used, a large initial seed dictionary is essential in order to acquire higher accuracy lexicon (Koehn and Knight, 2002). This means that cur2 Related work Most of the early work in bilingual lexicon extractio"
C10-2055,P99-1067,0,0.926315,"vide a method for identifying such terms. Our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary. In addition, we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary. 1 Introduction In bilingual lexicon extraction, the context-based approach introduced by Rapp (1995) is widely used (Fung, 1995; Diab and Finch, 2000; among others). The focus has been on learning from comparable corpora since the late 1990s (Rapp, 1999; Koehn and Knight, 2002; among others). However, so far, the accuracy of bilingual lexicon extraction using comparable corpora is quite poor especially when orthographic features are not used. Moreover, when orthographic features are not used, a large initial seed dictionary is essential in order to acquire higher accuracy lexicon (Koehn and Knight, 2002). This means that cur2 Related work Most of the early work in bilingual lexicon extraction employ an initial seed dictionary. A large bilingual lexicon with 10k to 20k entries is necessary (Fung, 1995; Rapp, 1999). Koehn and Knight (2002) int"
C10-2055,J93-1003,0,0.0968356,"ble 1. C[i,j] community powers 124 ¬ community ¬ powers 11779 460218 11903 462049 C(community) C(¬ community) 1831 1955 C(powers) 471997 C(¬ powers) Here C[i, j] denotes the count of the number of sentences in which i co-occurs with j. Total corpus size: N = 473952 in the above Table 1: Contingency table for observed values of target word powers and context word community. The LL value of a target word a and context word b is given by: Identifying highly associated words LL(a, b) = To identify the context terms CT (WS ) of a source word WS , as in (Rapp, 1999), we use loglikelihood ratio (LL) Dunning (1993). We choose all words with LL &gt; t1 where t1 is a threshold. The highly associated words then are the top k highest ranked context terms. In our experiments, we only choose the top 100 highest ranked context terms as our highly associated terms. In order to compute the log-likelihood ratio of target word a to co-occur with context word b, we X i∈{a,¬a},j∈{b,¬b} 3.1.1 2C(i, j) log C(i, j)N C(i) C(j) Identifying in-domain terms In our work, to find the translation equivalent of a source word WS , we do not use the context terms CT (WS ). Instead, we use the in-domain terms IDT (WS , WR ). For eac"
C16-1337,S13-2050,0,0.380725,"Missing"
C16-1337,E09-1013,0,0.216092,"choice of informative representations is a very important modelling aspect. Broad context related to topic or domain can restrict the possible senses that are applicable to an ambiguous word, but in order to make fine grained distinctions, context on the phrasal or syntactic level is usually needed. Ideally, a WSI system should incorporate different types of contexts to increase the confidence of its decisions. Combining the information present in different context representations can pose many difficulties in an unsupervised setting. Previous work has combined lexical with syntactic context (Brody and Lapata, 2009; Lau et al., 2012), and topical with local lexical context (Wang et al., 2015). Another challenge for WSI systems is the need to apply clustering methods in high dimensional spaces of sparse features. Probabilistic latent variable models have been very successful in WSI by inducing latent representations of features that help improve generalization. While the latent variable approach has been very successful for word features, it has not provided considerable advantages when used with syntactic features (Brody and Lapata, 2009; Lau et al., 2012). A possible reason for this is that syntactic f"
C16-1337,C14-1035,0,0.165152,"e appropriate number of senses from the data. They showed that HDP-LDA performs significantly better than LDA even when the number of senses is set to the same value. They also experimented with combined syntactic and word features. Syntactic features did not provide any advantage to either of these two LDA type models, but this could be attributed to sparsity. In addition, none of these models associate different context types with different stages of the generating process. LDA type models assume that the topic distribution of context words correspond to different senses of the target word. Chang et al. (2014) proposed a model similar to LDA, but specifically tailored to WSI by estimating different latent variable distributions for context words and senses. In their setting, the latent topics for context words provide a method to overcome the sparsity problem related with the high dimensionality of the discrete feature space. The sense-topic model (Wang et al., 2015) is another type of structured latent variable model related to our work. It makes a distinction between local and global context as we do, and jointly infers latent representations for both. The authors also make use of word embeddings"
C16-1337,D14-1082,0,0.0478237,"voured in model selection by BIC. In our experiments, we set |zg |= |zl |= |zs |= K and train models with K in the range of [2, 50]. We observe that given enough training instances, ICL picks models with a large numbers of components corresponding to more fine-grained senses. This is reasonable since with more data the model becomes more confident into making such fine-grained distinctions, and is also likely to encounter unusual word usages. 4 Evaluation We evaluate our model in two SemEval WSI datasets. For both datasets we parse the data using the Stanford Neural Network dependency parser (Chen and Manning, 2014) using Universal Dependencies (De Marneffe et al., 2014), which is the same format used by the dependency based embeddings. We train a different model for every word type. 4.1 SemEval-2010 Task 14: Word Sense Induction and Disambiguation The SemEval-2010 WSI dataset consists of 50 verbs and 50 nouns. The task organizers provide a fixed training set with 879,807 instances of the target words. The distribution of instances for each word is highly imbalanced. The test set consists of 8,915 instances. Two types of evaluation are performed: supervised and unsupervised. The supervised evaluation is"
C16-1337,J81-4005,0,0.760154,"Missing"
C16-1337,de-marneffe-etal-2014-universal,0,0.0555649,"Missing"
C16-1337,J13-3008,0,0.0255906,"Missing"
C16-1337,P15-1010,0,0.0282441,"iscrete feature space. The sense-topic model (Wang et al., 2015) is another type of structured latent variable model related to our work. It makes a distinction between local and global context as we do, and jointly infers latent representations for both. The authors also make use of word embeddings as a method of feature weighting and for extracting additional context for ambiguous word instances. Contrary to our work, their features are discrete and syntax is ignored. While several word embeddings models apply WSI in their training stage to create sense embeddings (Neelakantan et al., 2015; Iacobacci et al., 2015), there has been limited application of word embeddings as input representations to WSI models. The model of Huang et al. (2016) uses a recursive autoencoder to compose word embeddings to a context representation according to the structure provided by a syntactic parser. The final context representation captures both semantic and syntactic information and is used as the input to a rival penalization competitive learning clustering algorithm. While this approach uses both continuous word embeddings and syntactic information, it is fundamentally different from our work since their framework is n"
C16-1337,S13-2049,0,0.132111,"cipled way and also allows the application of model selection criteria to automatically determine the optimal number of senses. We address the issue of high dimensional feature spaces when dealing with syntactic features, by using word and dependency feature embeddings. In particular, we use the same skip-gram based model to create representations of all three context types. We evaluate our model in two competitive benchmarks: SemEval-2010 Task 14: Word Sense Induction and Disambiguation (Manandhar et al., 2010), and SemEval-2013 Task 11: Word Sense Induction for graded and non-graded senses (Jurgens and Klapaftis, 2013). The two tasks provide different WSI evaluation frameworks and metrics. The proposed model achieves the state-of-the-art results in both datasets. Code is available at https://cs.york.ac.uk/nlp/extvec 2 Related Work Among the most successful WSI systems are probabilistic latent variable models. Brody and Lapata (2009) extend the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) to combine evidence from different types of contexts. A limitation of this model is that the number of senses needs to be determined manually. Lau et al. (2012) propose using the non-parametric extension of L"
C16-1337,D10-1073,1,0.836817,"n competitive learning clustering algorithm. While this approach uses both continuous word embeddings and syntactic information, it is fundamentally different from our work since their framework is not probabilistic. This makes difficult to incorporate additional contextual information like global context, and to define structural dependencies between context types. 3578 A large variety of other clustering methods have been applied to WSI. A notable class of approaches is those using co-occurrence graphs and applying graph-based algorithms to identify hubs which are indicative of word senses (Klapaftis and Manandhar, 2010; Di Marco and Navigli, 2013). 3 Model Description Target Word: operate Global Context: ... while profits are volatile , many industries with volatile profits ranging from oil exploration to computer software operate without substantial government regulation . moreover , free markets generally work well for industries with large fluctuations , because ... Local (win5) Context: from oil exploration to computer software without substantial government regulation Syntactic Dependencies: advcl volatile punct , nsubj industries nmod:without regulation punct . Table 1: Global, local and syntactic con"
C16-1337,N16-1175,1,0.908069,"6, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3577–3587, Osaka, Japan, December 11-17 2016. model (Mikolov et al., 2013) is a very popular technique for learning embeddings that scales to huge corpora and can capture important semantic and syntactic properties of words. Skip-gram embeddings exhibit compositional properties under addition, making them useful for constructing representations of phrases and larger units of text. Recently, the skip-gram model has been extended to learn embeddings of dependency context features (Levy and Goldberg, 2014; Komninos and Manandhar, 2016) that capture additional syntactic information. While word embeddings have been successfully used in many supervised NLP problems to overcome the problem of sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011), their application in WSI has been so far very limited. In this paper, we propose a WSI model to address both the issue of multiple context representations and feature sparsity. Our model is a structured generative model that jointly models topical, phrasal and syntactic context in a hierarchical way. The probabilistic framework allows us to integrate differe"
C16-1337,S10-1079,1,0.888877,"Missing"
C16-1337,E12-1060,0,0.120055,"presentations is a very important modelling aspect. Broad context related to topic or domain can restrict the possible senses that are applicable to an ambiguous word, but in order to make fine grained distinctions, context on the phrasal or syntactic level is usually needed. Ideally, a WSI system should incorporate different types of contexts to increase the confidence of its decisions. Combining the information present in different context representations can pose many difficulties in an unsupervised setting. Previous work has combined lexical with syntactic context (Brody and Lapata, 2009; Lau et al., 2012), and topical with local lexical context (Wang et al., 2015). Another challenge for WSI systems is the need to apply clustering methods in high dimensional spaces of sparse features. Probabilistic latent variable models have been very successful in WSI by inducing latent representations of features that help improve generalization. While the latent variable approach has been very successful for word features, it has not provided considerable advantages when used with syntactic features (Brody and Lapata, 2009; Lau et al., 2012). A possible reason for this is that syntactic features, such as de"
C16-1337,S13-2039,0,0.0407289,"Missing"
C16-1337,P14-2050,0,0.0379795,"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3577–3587, Osaka, Japan, December 11-17 2016. model (Mikolov et al., 2013) is a very popular technique for learning embeddings that scales to huge corpora and can capture important semantic and syntactic properties of words. Skip-gram embeddings exhibit compositional properties under addition, making them useful for constructing representations of phrases and larger units of text. Recently, the skip-gram model has been extended to learn embeddings of dependency context features (Levy and Goldberg, 2014; Komninos and Manandhar, 2016) that capture additional syntactic information. While word embeddings have been successfully used in many supervised NLP problems to overcome the problem of sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011), their application in WSI has been so far very limited. In this paper, we propose a WSI model to address both the issue of multiple context representations and feature sparsity. Our model is a structured generative model that jointly models topical, phrasal and syntactic context in a hierarchical way. The probabilistic framework"
C16-1337,J14-3007,0,0.0381335,"Missing"
C16-1337,S10-1011,1,0.878339,"rarchical way. The probabilistic framework allows us to integrate different types of information in a principled way and also allows the application of model selection criteria to automatically determine the optimal number of senses. We address the issue of high dimensional feature spaces when dealing with syntactic features, by using word and dependency feature embeddings. In particular, we use the same skip-gram based model to create representations of all three context types. We evaluate our model in two competitive benchmarks: SemEval-2010 Task 14: Word Sense Induction and Disambiguation (Manandhar et al., 2010), and SemEval-2013 Task 11: Word Sense Induction for graded and non-graded senses (Jurgens and Klapaftis, 2013). The two tasks provide different WSI evaluation frameworks and metrics. The proposed model achieves the state-of-the-art results in both datasets. Code is available at https://cs.york.ac.uk/nlp/extvec 2 Related Work Among the most successful WSI systems are probabilistic latent variable models. Brody and Lapata (2009) extend the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) to combine evidence from different types of contexts. A limitation of this model is that the numb"
C16-1337,P10-1040,0,0.0542468,"scales to huge corpora and can capture important semantic and syntactic properties of words. Skip-gram embeddings exhibit compositional properties under addition, making them useful for constructing representations of phrases and larger units of text. Recently, the skip-gram model has been extended to learn embeddings of dependency context features (Levy and Goldberg, 2014; Komninos and Manandhar, 2016) that capture additional syntactic information. While word embeddings have been successfully used in many supervised NLP problems to overcome the problem of sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011), their application in WSI has been so far very limited. In this paper, we propose a WSI model to address both the issue of multiple context representations and feature sparsity. Our model is a structured generative model that jointly models topical, phrasal and syntactic context in a hierarchical way. The probabilistic framework allows us to integrate different types of information in a principled way and also allows the application of model selection criteria to automatically determine the optimal number of senses. We address the issue of high dimensional feature spa"
C16-1337,Q15-1005,0,0.487243,"ontext related to topic or domain can restrict the possible senses that are applicable to an ambiguous word, but in order to make fine grained distinctions, context on the phrasal or syntactic level is usually needed. Ideally, a WSI system should incorporate different types of contexts to increase the confidence of its decisions. Combining the information present in different context representations can pose many difficulties in an unsupervised setting. Previous work has combined lexical with syntactic context (Brody and Lapata, 2009; Lau et al., 2012), and topical with local lexical context (Wang et al., 2015). Another challenge for WSI systems is the need to apply clustering methods in high dimensional spaces of sparse features. Probabilistic latent variable models have been very successful in WSI by inducing latent representations of features that help improve generalization. While the latent variable approach has been very successful for word features, it has not provided considerable advantages when used with syntactic features (Brody and Lapata, 2009; Lau et al., 2012). A possible reason for this is that syntactic features, such as dependency contexts, exhibit much more sparsity than words. A"
C16-1337,D14-1113,0,\N,Missing
D10-1073,S07-1002,0,0.641824,"get word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free1 graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008). The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. All the described methods for sense induction ap1 One needs to specify only the number of iterations. The number of clusters is generated automatically. Figure 2: Graph of words for the target word paper. Numbers inside vertices correspond to their degree. a context is defined as a paragraph2 containing the target word. The aim of this stage is to capture nouns contextually related to tw. Initially, the target word is removed from bc and part-of-speech tagging is applied to each context. Following the work in (V´eronis, 2004; Agirre et al., 2006) only nouns are kept"
D10-1073,W06-1669,0,0.0327756,"Missing"
D10-1073,W06-3812,0,0.255065,"he target word. Additionally, they extract second-order co-occurrences. Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times. This co-occurrence frequency is also used to weight the edges. The resulting graph is then pruned by removing the target word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free1 graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008). The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. All the described methods for sense induction ap1 One needs to specify only the number of iterations. The number of clusters is generated automatically. Figure 2: Graph of words for the target word paper. Numb"
D10-1073,E09-1013,0,0.413277,"Missing"
D10-1073,E03-1020,0,0.0534513,"Missing"
D10-1073,J93-1003,0,0.402265,"ph of words for the target word paper. Numbers inside vertices correspond to their degree. a context is defined as a paragraph2 containing the target word. The aim of this stage is to capture nouns contextually related to tw. Initially, the target word is removed from bc and part-of-speech tagging is applied to each context. Following the work in (V´eronis, 2004; Agirre et al., 2006) only nouns are kept and lemmatised. In the next step, the distribution of each noun in the base corpus is compared to the distribution of the same noun in a reference corpus3 using the log-likelihood ratio (G2 ) (Dunning, 1993). Nouns with a G2 below a pre-specified threshold (parameter p1 ) are removed from each paragraph of the base corpus. The upper left part of Figure 3 shows the words kept as a result of this stage. 3.2 Graph creation Graph vertices: To create the graph of vertices, we represent each context ci as a vertex in a graph G. Graph edges: Edges between the vertices of the graph are drawn based on their similarity, defined in Equation 1, where simcl (ci , cj ) is the collocational weight of contexts ci , cj and simwd (ci , cj ) is their bag-of-words weight. If the edge weight W (ci , cj ) is above a p"
D10-1073,S01-1001,0,0.0204297,"Missing"
D10-1073,N10-1010,1,0.738944,"aim of our evaluation is to assess whether the hierarchical structure inferred by HRGs is more informative than the hierarchical structure inferred by traditional Hierarchical Clustering (HAC). Hence, our third baseline, takes as input a similarity matrix of the graph vertices and performs bottom-up clustering with average-linkage, which has already been used in WSI in (Pantel and Lin, 4 The number of iterations for CW was set to 200. 751 2003) and was shown to have superior or similar performance to single-linkage and complete-linkage in the related problem of learning a taxonomy of senses (Klapaftis and Manandhar, 2010). To calculate the similarity matrix of vertices we follow a process similar to the one used in Section 4.2 for calculating the probability of an internal node. The similarity between two vertices is calculated according to the degree of connectedness among their direct neighbours. Specifically, we would like to assign high similarity to pairs of vertices, whose neighbours are close to forming a clique. Given two vertices (contexts) ci and cj , let N (ci , cj ) be the set of their neighbours and K(ci , cj ) be the set of edges between the vertices in N (ci , cj ). The maximum number of edges t"
D10-1073,P04-3020,0,0.0375937,"lustering yielding improvements over state-of-the-art WSD systems based on sense induction. 1 Introduction A number of NLP problems can be cast into a graphbased framework, in which entities are represented as vertices in a graph and relations between them are depicted by weighted or unweighted edges. For instance, in unsupervised WSD a number of methods (Widdows and Dorow, 2002; V´eronis, 2004; Agirre et al., 2006) have constructed word co-occurrence graphs for a target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are repRecent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves. This hierarchical structure provides additional information as opposed to flat clustering by explicitly including organisation at all scales of a graph (Clauset et al., 2008). In this paper, we present an unsupervised method for inferring the hierarchical structure (binary tree) of a gr"
D10-1073,S07-1037,0,0.268185,"Missing"
D10-1073,N03-4011,0,0.0751358,"Missing"
D10-1073,N06-4007,0,0.170417,"Missing"
D10-1073,S07-1087,0,0.116267,"Missing"
D10-1073,W09-3204,0,0.0149154,"Missing"
D10-1073,W04-0811,0,0.0539038,"Missing"
D10-1073,C04-1146,0,0.0270284,"Missing"
D10-1073,C02-1114,0,0.062925,"he senses of a polysemous word. The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs significantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction. 1 Introduction A number of NLP problems can be cast into a graphbased framework, in which entities are represented as vertices in a graph and relations between them are depicted by weighted or unweighted edges. For instance, in unsupervised WSD a number of methods (Widdows and Dorow, 2002; V´eronis, 2004; Agirre et al., 2006) have constructed word co-occurrence graphs for a target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are repRecent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves. This hierarchical structure provides additional infor"
E06-2029,N04-1025,0,0.081441,"Missing"
E06-2029,O97-1002,0,\N,Missing
E12-1067,W06-3209,0,0.189722,"0,ed,ing,ly, s} Di Dj {walk, talk}{0,ed,ing,s} {walk}{0,ing} walk {talk}{ed,s} walking talked X1 {quick}{0,ly} talks quick quickly Figure 1: A sample tree structure. method is similar to the Dirichlet Process (DP) based model of Goldwater et al. (2006). From this perspective, our method can be understood as adding a hierarchical structure learning layer on top of the DP based learning method proposed in Goldwater et al. (2006). Dreyer and Eisner (2011) propose an infinite Diriclet mixture model for capturing paradigms. However, they do not address learning of hierarchy. The method proposed in Chan (2006) also learns within a hierarchical structure where Latent Dirichlet Allocation (LDA) is used to find stem-suffix matrices. However, their work is supervised, as true morphological analyses of words are provided to the system. In contrast, our proposed method is fully unsupervised. 3 Probabilistic Hierarchical Model The hierarchical clustering proposed in this work is different from existing hierarchical clustering algorithms in two aspects: • It is not single-pass as the hierarchical structure changes. • It is probabilistic and is not dependent on a distance metric. 3.1 Mathematical Definition"
E12-1067,W02-0603,0,0.743135,"Missing"
E12-1067,D11-1057,0,0.464347,"ckling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair Suresh Manandhar Department of Computer Science University of York Heslington, York, YO10 5GH, UK suresh@cs.york.ac.uk (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem ∈ StemList and Suffix ∈ SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the structure does not change further. The paper is structured as follows: secti"
E12-1067,J01-2001,0,0.456273,"ically complex languages. Applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair Suresh Manandhar Department of Computer Science University of York Heslington, York, YO10 5GH, UK suresh@cs.york.ac.uk (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem ∈ StemList and Suffix ∈ SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical s"
E12-1067,W02-0602,0,0.695532,"Missing"
E12-1067,W10-2211,0,\N,Missing
E95-1023,E95-1025,0,0.017896,"ond what is available within current typed feature formalisms. Our approach is in the spirit of Reape's approach but improves upon it. Furthermore, a sound, complete and terminating consistency checking procedure is described. Our constraint solving rules axe d e t e r m i n i s t i c and i n c r e m e n t a l . Hence these do not introduce costly choice-points. These constraint solving rules can be employed for building an efficient implementation. This is an i m p o r t a n t requirement for practical systems. Indeed we have successfully extended the P r o F I T t y p e d feature formalism (Erbach, 1995) with the constructs described in this paper. 2 O u t l i n e of an a l t e r n a t i v e approach For space reasons, our t r e a t m e n t is necessarily somewhat superficial since we do not take into account other interacting p h e n o m e n a such as f r o n t i n g or extraposition. The definition in (11) does not make specific assumption about whether a context-free backbone is employed or not. However, if a C F G backbone is employed then we assume t h a t the value of the subcat attribute is treated as an u n o r d e r e d sequence (i.e. a set) as defined in (11). (12) NPdom / To motiva"
E95-1023,P94-1035,1,0.609374,"s to employ a specification such as the one given in (11) which is a partial specification of the lexical entry for the verb sah. The specification can be thought of as a formal specification of the intuitive description given in (12). (11) V [3 p h o n : &lt; sah &gt; 13 f i e l d : F i e l d [3 s y n : ( cat : v [3 subcat : { N P [3 dora : N P d o m , V i [3 dora : V i d o m } [3 dora :D N P d o m [3 dora :D V i d o m ) [3 V i d o m &lt; dora {V} [3 N P d o m &lt; do,n {Vi} [3 Vi&lt;V 166 V~ The essential idea is to use set-valued descriptions to model word-order domains. In paxticulax subset constraints (Manandhar, 1994) are employed to construct larger domains from smaller ones. Thus in example (11) the domain of the verb is constructed by including the domains of the subcategorised arguments (enforced by the constraints dora :D N P d o m f 3 d o m :D V i D o m ) . Note t h a t in this example the verb itself is not p a r t of its own domain. The binary constraint Vi &lt; V enforces precedence ordering between the s i g n s V i and V. The constraint V ~ d o m &lt; do,~ {V} ensures t h a t every element of the set V i D o m precedes the sign V. In other words, the set V i D o m is in the d o m a i n precedence rela"
E95-1023,C94-1008,0,0.0205858,"der Strafle ihn er laufen sah. According to Uszkoreit (Uszkoreit, 1985), ordering of arguments in the middle field is governed by the following set of LP constraints given in (14) which axe to be interpreted disjunctively. (14) PPRN : + &lt; PPRN : T R : agent &lt; T R : t h e m e T R : agent &lt; T R : goal T R : goal &lt; T R : t h e m e FOCUS:&lt; FOCUS:+ The LP constraint in (14) states t h a t for every pair of constituents in the middle field at least one of the conditions should apply otherwise the sentence is considered ungrammatical. A related but more elaborate LP rule mechanism is considered in (Steinberger, 1994). To approximate this complex LP constraint employing the kind of logical machinery described in this paper, we can use a description such as the one given in (15). The definition given in (15) extends the description given in (11). Note that it is not necessary to know whether the P P i n d e r S t r a f l e is focussed to rule out (17) since the fact that the pronoun i h n is f o c u s : - is enough to trigger the inconsistency. 3 (15) s y n : d o m : M F f3 3x3y if x E MF A y E MF A x &lt; y then if x=pprn: +Ay=pprn:-then T else i f x ---- t r : a g e n t A y = t r : t h e m e then T else i f"
I11-1024,W03-1812,0,0.900245,"their multiplicative counterparts. 1 Introduction Compositionality is a language phenomenon where the meaning of an expression can be expressed in terms of the meaning of its constituents. Multiword expressions (Sag et al., 2002, MWEs) are known to display a continuum of compositionality (McCarthy et al., 2003) where some of them are compositional e.g. “swimming pool”, some are non-compositional e.g. “cloud nine”, and some in between e.g. “zebra crossing”. The past decade has seen interest in developing computational methods for compositionality in MWEs (Lin, 1999; Schone and Jurafsky, 2001; Baldwin et al., 2003; Bannard et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Sporleder and Li, 2 Compositionality in Compound Nouns In this section, we describe the experimental setup for the collecting compositionality judgments of English compound nouns. All the existing datasets focused either on verb-particle, verbnoun or adjective-noun phrases. Instead, we focus on compound nouns for which resources are rel210 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 A"
I11-1024,D07-1039,1,0.914925,"Missing"
I11-1024,W03-1809,0,0.0884781,"(Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of the whole (section 3). We propose various constituent based models (section 4.3) which are intuitive and related to existing models of compositionality detection (section 4.1) and we evaluate these models in comparison to composition"
I11-1024,P08-1028,0,0.812426,"the second in determining compositionality. Results (both ρ and R2 ) clearly show that a relation exists between the constituent literality scores and the phrase compositionality. Existing compositionality approaches on noun-noun compounds such as (Baldwin et al., 2003; Korkontzelos and Manandhar, 2009) use the semantics of only one of the constituent words (generally the head word) Overall, this study suggests that it is possible to estimate the phrase level compositionality scores given the constituent word level literality scores. This motivates us to present constituent 214 vector models (Mitchell and Lapata, 2008; Widdows, 2008) which make use of the semantics of the constituents in a different manner. These models are described in section 4.4 and are evaluated in comparison with the constituent-based models. The vector space model used in all our experiments is described as follows. based models (section 4.3) for compositionality score estimation of a compound. We begin the next section on computational models with a discussion of related work. 4 4.1 Computational Models Related work 4.2 Most methods in compositionality detection can be classified into two types - those which make use of lexical fixe"
I11-1024,W11-1304,0,0.221301,"thy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of the whole (section 3). We propos"
I11-1024,W07-1106,0,0.0288709,"e lexical fixedness in which the component words have high statistical association. Some of the methods which exploit this feature are (Lin, 1999; Pedersen, 2011). This property does not hold always because institutionalized MWEs (Sag et al., 2002) are known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the variations of any compound noun are highly limited. Other methods like (Baldwin et al., 2003; Sporleder and Li, 2009) are based on semantic similarities between the constituents and the MWE. Baldwin et al. (2003) use only the information of the semantic similarity between one of the constituents and the compound to determine the compositionality. Sporleder and Li (2009) determine the compositionality of verbal phrases in a given context (token-based disam"
I11-1024,W11-1306,0,0.0320891,"ty score estimation of a compound. We begin the next section on computational models with a discussion of related work. 4 4.1 Computational Models Related work 4.2 Most methods in compositionality detection can be classified into two types - those which make use of lexical fixedness and syntactic properties of the MWEs, and those which make use of the semantic similarities between the constituents and the MWE. Non compositional MWEs are known to have lexical fixedness in which the component words have high statistical association. Some of the methods which exploit this feature are (Lin, 1999; Pedersen, 2011). This property does not hold always because institutionalized MWEs (Sag et al., 2002) are known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the vari"
I11-1024,W11-1310,1,0.854619,"Missing"
I11-1024,J09-1005,0,0.18527,"and reduce the impact of ambiguity. The second is that distributional models are greatly influenced by frequency and since we aim to work with distributional models for compositionality detection we base our findings on the most frequent sense of the compound noun. In this work we consider the compositionality of the noun-noun compound type without token based disambiguation which we leave for future work. atively scarce. In this paper, we only deal with compound nouns made up of two words separated by space. 2.1 Annotation setup In the literature (Nunberg et al., 1994; Baldwin et al., 2003; Fazly et al., 2009), compositionality is discussed in many terms including simple decomposable, semantically analyzable, idiosyncratically decomposable and non-decomposable. For practical NLP purposes, Bannard et al. (2003) adopt a straightforward definition of a compound being compositional if “the overall semantics of the multi-word expression (here compound) can be composed from the simplex semantics of its parts, as described (explicitly or implicitly) in a finite lexicon”. We adopt this definition and pose compositionality as a literality issue. A compound is compositional if its meaning can be understood f"
I11-1024,W01-0513,0,0.0462878,"Missing"
I11-1024,W11-0115,0,0.0607213,"2005; Biemann and Giesbrecht, 2011): higher correlation scores indicate better compositionality predictions. s3 = f (s1, s2) Composition function based models In these models (Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Giesbrecht, 2009) of compositionality detection, firstly a vector for the compound is composed from its constituents using a compositionality function ⊕. Then the similarity between the composed vector and true cooccurrence vector of the compound is measured to determine the compositionality: the higher the similarity, the higher the compositionality of the compound. Guevara (2011) observed that additive models performed well for building composition vectors of phrases from their parts whereas Mitchell and Lapata (2008) found in favor of multiplicative models. We experiment using both the compositionality functions simple addition5 and simple multiplication, which are the most widely used composition functions, known for their simplicity and good performance. Vector v1 ⊕ v2 for a compound w3 is composed from its constituent word vectors v1 and v2 using the vector addition av1 + bv2 and simple multiplication v1v2 where the ith element of v1 ⊕ v2 is defined as (av1 + bv2)"
I11-1024,D08-1027,0,0.0267317,"Missing"
I11-1024,W06-1203,0,0.263496,"york.ac.uk diana@dianamccarthy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of t"
I11-1024,E09-1086,0,0.00987205,"known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the variations of any compound noun are highly limited. Other methods like (Baldwin et al., 2003; Sporleder and Li, 2009) are based on semantic similarities between the constituents and the MWE. Baldwin et al. (2003) use only the information of the semantic similarity between one of the constituents and the compound to determine the compositionality. Sporleder and Li (2009) determine the compositionality of verbal phrases in a given context (token-based disambiguation) based on the lexical chain similarities of the constituents and the context of the MWE. Bannard et al. (2003) and McCarthy et al. (2003) study the compositionality in verb particles and they found that methods based on the similarity between simpl"
I11-1024,P09-2017,1,0.915764,"Missing"
I11-1024,H05-1113,0,0.257826,"Missing"
I11-1024,P99-1041,0,0.0571438,"s, additive models perform better than their multiplicative counterparts. 1 Introduction Compositionality is a language phenomenon where the meaning of an expression can be expressed in terms of the meaning of its constituents. Multiword expressions (Sag et al., 2002, MWEs) are known to display a continuum of compositionality (McCarthy et al., 2003) where some of them are compositional e.g. “swimming pool”, some are non-compositional e.g. “cloud nine”, and some in between e.g. “zebra crossing”. The past decade has seen interest in developing computational methods for compositionality in MWEs (Lin, 1999; Schone and Jurafsky, 2001; Baldwin et al., 2003; Bannard et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Sporleder and Li, 2 Compositionality in Compound Nouns In this section, we describe the experimental setup for the collecting compositionality judgments of English compound nouns. All the existing datasets focused either on verb-particle, verbnoun or adjective-noun phrases. Instead, we focus on compound nouns for which resources are rel210 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, c Ch"
I11-1024,W03-1810,1,0.568929,", UK Suresh Manandhar University of York, UK siva@cs.york.ac.uk diana@dianamccarthy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the"
I11-1079,J93-1003,0,0.133967,"ty. The selected clusters are then combined using a composition function, to produce a single vector representing the semantics of the target compound noun N . 3.1.1 Figure 2: Running example of WSI The aim of this stage is to capture words contextually related to tw. In the first step, the target word is removed from bc and part-of-speech tagging is applied to each context. Only nouns and verbs are kept and lemmatised. In the next step, the distribution of each word in the base corpus is compared to the distribution of the same noun in a reference corpus using the log-likelihood ratio (G2 ) (Dunning, 1993). Words that have a G2 below a pre-specified threshold (parameter p1 ) are removed from each context of the base corpus. The result of this stage is shown in the upper left part of Figure 2. Graph creation & clustering: Each context ci ∈ bc is represented as a vertex in a graph G. Edges between the vertices of the graph are drawn based on their similarity, defined in Equation 2, where smcl (ci , cj ) is the collocational weight of contexts ci , cj and smwd (ci , cj ) is their bag-of-words weight. If the edge weight W (ci , cj ) is above a prespecified threshold (parameter p3 ), then an edge is"
I11-1079,D08-1094,0,0.15084,"Missing"
I11-1079,P10-2017,0,0.0445281,"Missing"
I11-1079,D10-1073,1,0.813692,"Missing"
I11-1079,N10-1010,1,0.912273,"y is a problem in vector space models. Our approach differs to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been pro"
I11-1079,P09-2017,1,0.880724,"Missing"
I11-1079,S07-1002,0,0.00838561,"the set of vertices which share a direct connection with vertex i. During the update step for a vertex i: each class Ck receives a score equal to the sum of the weights of edges (i, j), where j has been assigned class Ck . The maximum score determines the strongest class. In case of multiple strongest classes, one is chosen randomly. Classes are updated immediately, which means that a node can inherit classes from its LN that were introduced in the same iteration. Experimental setting The parameters of the WSI method were fine-tuned on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task, i.e. supervised (WSD) evaluation. We tried various parameter combinations shown in Table 1. Specifically, we selected the parameter combination p1 =15, p2 =10, p3 = 0.05 that maximized the performance in this evaluation. We use ukWaC (Ferraresi et al., 2008) corpus to retrieve all the instances of the target words. 3.2 Dynamic Prototype Based Sense Selection Kilgarriff (1997) argues that representing a word with a fixed set of senses is not a good way of modelling word senses. Instead word senses should be defined according to a given context."
I11-1079,P08-1028,0,0.849714,"ounds and evaluate on a compositionality-based similarity task. Our results show that: (1) selecting relevant senses of the constituent words leads to a better semantic composition of the compound, and (2) dynamic prototypes perform better than static prototypes. 1 animal house h 30 hunting h 90 buy 60 15 vector dimensions apartment price 90 55 12 20 rent 45 33 kill 10 i 90 i Figure 1: A hypothetical vector space model. Compositional Distributional Semantic methods formalise the meaning of a phrase by applying a vector composition function on the vectors associated with its constituent words (Mitchell and Lapata, 2008; Widdows, 2008). For example, the result of vector addition to compose the semantics of house hunting from the vectors house and hunting is the vector h120, 75, 102, 75, 78, 100i. As can be observed the resulting vector does not reflect the correct meaning of the compound house hunting due to the presence of irrelevant co-occurrences such as animal or kill. These cooccurrences are relevant to one sense of hunting, i.e. (the activity of hunting animals), but not to the sense of hunting meant in house hunting, i.e. the activity of looking thoroughly. Given that hunting has been associated with"
I11-1079,D10-1012,0,0.0071372,"e compound noun. The results are encouraging showing that polysemy is a problem in vector space models. Our approach differs to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of composit"
I11-1079,J07-2002,0,0.0285319,"of hunting, i.e. (the activity of hunting animals), but not to the sense of hunting meant in house hunting, i.e. the activity of looking thoroughly. Given that hunting has been associated with a single prototype (vector) by conflating all of its senses, the application of a composition function ⊕ is likely to include irrelevant co-occurrences in house ⊕ hunting. A potential solution to this problem would involve the following steps: Introduction Vector Space Models of lexical semantics have become a standard framework for representing a word’s meaning. Typically these methods (Sch¨utze, 1998; Pado and Lapata, 2007; Erk and Pad´o, 2008) utilize a bag-of-words model or 705 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 705–713, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP following Mitchell and Lapata (2008) are defined as follows: 1. build separate prototype vectors for each of the senses of house and hunting 2. select the relevant prototype vectors of house and hunting and then perform the semantic composition. In this paper we present two methods (section 3) for carrying out the above steps on noun-noun compounds. The first one (section 3.1) ap"
I11-1079,N10-1013,0,0.284791,"fers to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been proposed in order to estimate the semantics of compound"
I11-1079,P07-2011,0,0.0890041,"Missing"
I11-1079,J98-1004,0,0.706399,"Missing"
I11-1079,W11-1301,0,0.0217646,"xt section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been proposed in order to estimate the semantics of compound words from the semantics of the constituent words. Mitchell and Lapata (2008) discussed and evaluated various composition functions for phrases consisting of two words. Among these, the simple additive (ADD) and simple multiplicative (MULT) functions are easy to implement and competitive with respect to existing sophisticated methods (Widdows, 2008; Vecchi et al., 2011). Let us assume a target compound noun N that consists of two nouns n and n0 . Bold letters represent their corresponding distributional vectors obtained from corpora. ⊕(N) denotes the vector of N obtained by applying the composition function ⊕ on n and n0 . Real number vi denote the ith cooccurrence in v. The functions ADD and MULT 3 Sense Prototype Vectors for Semantic Composition In this section we describe two approaches for building sense specific prototype vectors of constituent words in a noun-noun compound. The first approach performs WSI to build static multi prototype vectors. The ot"
I11-1079,W06-3812,0,\N,Missing
I11-1079,W11-0115,0,\N,Missing
I13-1152,P10-1132,0,0.0195779,"that wi = si + mi , where si denotes the stem. Introduction The morphology of a word is an important indicator that determines its PoS tag, meanwhile the PoS tag of a word helps in identifying the correct morphological segmentation of the word. This relationship between morphology and syntax has been beneficial in both morphology learning with the exploitation of the syntactic features and in PoS tagging with the adoption of morphological features. There has been a number of research that have performed PoS tagging by making use of morphological information (Clark (2003), Hasan and Ng (2009), Abend et al. (2010), Christodoulopoulos et al. (2011), etc.). There has been also a number of other research that have performed morphological segmentation by adopting syntactic information (Hu et al. (2005), Can and Manandhar (2009), Lee et al. (2011), etc.). However, there is a small number of research that combines two tasks in a single framework. Sirts and Alum¨ae (2012) share a similar goal The generative story is summarised as follows: p(ci , ci 1,i+1 , wi , s, m) = p(ci )p(ci 1,i+1 |ci ) p(wi |ci )p(m|ci )p(s) 2.1 PoS Tagging The model adopts a finite mixture model for PoS tagging (see Figure 1). Each mix"
I13-1152,D10-1056,0,0.013338,"esults with a comparison to other systems in Table 1 by using a dataset of 250K words. We use a small portion of Penn WSJ treebank for the comparison. The dataset involves 250K words where the number of word types is 20957. The other systems are also tested on a small portion of WSJ involving 16850 word types, which is reported in Christodoulopoulos et al. (2011). Our system outperforms Christodoulopoulos et al. (2011) with the many-to-one evaluation, whereas Christodoulopoulos et al. (2011) perform better than our system based on V-measure evaluation. It should be noted that Clark (2003) and Christodoulopoulos et al. (2010) are both typebased. 1 Although, Figure 3 shows that results for 36k words are better than results for 48k words, this could be due to the particular choice of training sets we used. en n other Found es s ing d ed e NULL NULL Figure 4: Confusion matrix shows the correlation between found morphs and true morphs. The shades reflect the number of matchings. 4.2 Morphological Segmentation Results We performed the evaluation of morphological segmentation on verbs. We adopted some heuristics that strip off common verb endings such as -ed, -d, -ing, -s, -es from verbs in order to build the gold stand"
I13-1152,D11-1059,0,0.0773546,"here si denotes the stem. Introduction The morphology of a word is an important indicator that determines its PoS tag, meanwhile the PoS tag of a word helps in identifying the correct morphological segmentation of the word. This relationship between morphology and syntax has been beneficial in both morphology learning with the exploitation of the syntactic features and in PoS tagging with the adoption of morphological features. There has been a number of research that have performed PoS tagging by making use of morphological information (Clark (2003), Hasan and Ng (2009), Abend et al. (2010), Christodoulopoulos et al. (2011), etc.). There has been also a number of other research that have performed morphological segmentation by adopting syntactic information (Hu et al. (2005), Can and Manandhar (2009), Lee et al. (2011), etc.). However, there is a small number of research that combines two tasks in a single framework. Sirts and Alum¨ae (2012) share a similar goal The generative story is summarised as follows: p(ci , ci 1,i+1 , wi , s, m) = p(ci )p(ci 1,i+1 |ci ) p(wi |ci )p(m|ci )p(s) 2.1 PoS Tagging The model adopts a finite mixture model for PoS tagging (see Figure 1). Each mixture component represents a PoS ta"
I13-1152,W00-0717,0,0.0724759,"f the stem type si ali ready generated, T s is the number of all stems in i the model, and M s is the number of stem types generated excluding si . Similarly, the conditional probability of a suffix is computed as follows: i wi is the number of word types that are tagged with ci . p(ci ) is computed as follows: wi p(ci |c nc , ⇡) = N i wi wi +⇡ (9) + K⇡ i i where N denotes the number of word tokens in the model excluding wi , K is the number of class indicators (i.e. number of PoS tags). In order to mitigate the sparsity within the context probabilities, we use the approximation introduced by Clark (2000): wi p(< wi 1 , wi+1 where, p(< ci that: p(< ci 1 , ci+1 > |ci ) = p(< ci p(wi 1 , ci+1 ,c (10) > |ci ) 1 )p(wi+1 |ci+1 ) > |ci ) is computed such > |cx , cy , cz , ci , ) = <c 1 , ci+1 1 |ci nc i 1 ,ci ,ci+1 + (11) kci + L > <c ,c > Here, cx is ci i 1 i+1 , cy is ci 1 i 2 i , cz is <c ,c > ci+1 i i+2 , kci is the number of contexts in ci , and L denotes the possible number of different contexts in the model (i.e. K ⇤ K). 3.2 Inferring Morphology Two latent variables are inferred for morphology: stems and suffixes. The sampling probability for morphology is defined as follows: p(wi = si + mi |"
I13-1152,E03-1009,0,0.24132,"suffix mi conditioned on ci , such that wi = si + mi , where si denotes the stem. Introduction The morphology of a word is an important indicator that determines its PoS tag, meanwhile the PoS tag of a word helps in identifying the correct morphological segmentation of the word. This relationship between morphology and syntax has been beneficial in both morphology learning with the exploitation of the syntactic features and in PoS tagging with the adoption of morphological features. There has been a number of research that have performed PoS tagging by making use of morphological information (Clark (2003), Hasan and Ng (2009), Abend et al. (2010), Christodoulopoulos et al. (2011), etc.). There has been also a number of other research that have performed morphological segmentation by adopting syntactic information (Hu et al. (2005), Can and Manandhar (2009), Lee et al. (2011), etc.). However, there is a small number of research that combines two tasks in a single framework. Sirts and Alum¨ae (2012) share a similar goal The generative story is summarised as follows: p(ci , ci 1,i+1 , wi , s, m) = p(ci )p(ci 1,i+1 |ci ) p(wi |ci )p(m|ci )p(s) 2.1 PoS Tagging The model adopts a finite mixture mode"
I13-1152,W05-0503,0,0.0270699,"g the correct morphological segmentation of the word. This relationship between morphology and syntax has been beneficial in both morphology learning with the exploitation of the syntactic features and in PoS tagging with the adoption of morphological features. There has been a number of research that have performed PoS tagging by making use of morphological information (Clark (2003), Hasan and Ng (2009), Abend et al. (2010), Christodoulopoulos et al. (2011), etc.). There has been also a number of other research that have performed morphological segmentation by adopting syntactic information (Hu et al. (2005), Can and Manandhar (2009), Lee et al. (2011), etc.). However, there is a small number of research that combines two tasks in a single framework. Sirts and Alum¨ae (2012) share a similar goal The generative story is summarised as follows: p(ci , ci 1,i+1 , wi , s, m) = p(ci )p(ci 1,i+1 |ci ) p(wi |ci )p(m|ci )p(s) 2.1 PoS Tagging The model adopts a finite mixture model for PoS tagging (see Figure 1). Each mixture component represents a PoS tag that shares a set of features with other members in the same component. Each mixture component ci consists of 1. a distribution over contexts and 2. a d"
I13-1152,W11-0301,0,0.026895,"the word. This relationship between morphology and syntax has been beneficial in both morphology learning with the exploitation of the syntactic features and in PoS tagging with the adoption of morphological features. There has been a number of research that have performed PoS tagging by making use of morphological information (Clark (2003), Hasan and Ng (2009), Abend et al. (2010), Christodoulopoulos et al. (2011), etc.). There has been also a number of other research that have performed morphological segmentation by adopting syntactic information (Hu et al. (2005), Can and Manandhar (2009), Lee et al. (2011), etc.). However, there is a small number of research that combines two tasks in a single framework. Sirts and Alum¨ae (2012) share a similar goal The generative story is summarised as follows: p(ci , ci 1,i+1 , wi , s, m) = p(ci )p(ci 1,i+1 |ci ) p(wi |ci )p(m|ci )p(s) 2.1 PoS Tagging The model adopts a finite mixture model for PoS tagging (see Figure 1). Each mixture component represents a PoS tag that shares a set of features with other members in the same component. Each mixture component ci consists of 1. a distribution over contexts and 2. a distribution over words. Each context is a PoS"
I13-1152,J93-2004,0,0.041594,"n ci , Tcmi is the number of all i suffixes assigned with PoS tag ci , and M m is the number of suffix types already generated excluding mi . In the algorithm, initially each word is assigned a PoS tag and split randomly. The algorithm goes through each word by sampling a PoS tag, a stem, and a suffix. All constituents of the respective word (tag, stem, suffix, context, contexts of adjacent words) are removed from the model beforehand. This process is repeated for a number of iterations until a convergence is ensured. 4 Experiments & Evaluation We used small portions of the Penn WSJ treebank (Marcus et al., 1993) for the experiments. We manually set the hyperparameters and concentration parameters for each experiment: ⇡ = 10 6 , = 10 6 ,  = 10 6 , s = 10 6 , m = 10 6 . These values were set empirically through several experiments. We also inserted a special character at the end of each sentence and assigned it a distinct PoS tag. No other words could be assigned this tag. 4.1 PoS Tagging Results In our experiments we fixed the number of PoS tags to 45, which is the number of PoS tags in 1089 V-measure Many-to-one Christ.11 48.6 57.8 Joint 41.11 59.67 Clark2 63.8 68.8 Christ.2 (Best Pub.)3 67.7 72.0 1"
I13-1152,D07-1043,0,0.0172409,"Missing"
I13-1152,N12-1045,0,0.0391326,"Missing"
I13-1152,W02-0603,0,0.345312,"ts we used. en n other Found es s ing d ed e NULL NULL Figure 4: Confusion matrix shows the correlation between found morphs and true morphs. The shades reflect the number of matchings. 4.2 Morphological Segmentation Results We performed the evaluation of morphological segmentation on verbs. We adopted some heuristics that strip off common verb endings such as -ed, -d, -ing, -s, -es from verbs in order to build the gold standard. Irregular verbs are introduced exceptionally and left as they are. The results obtained from the 96K setting were used for the evaluation. We ran Morfessor Baseline (Creutz and Lagus, 2002; Creutz and Lagus, 2005; Creutz and Lagus, 2007) on the verbs in the same dataset. Table 2 gives the scores where missing types refers to the case that gold standard suggests a suffix but no suffix is identified in the results, extra suffixes means that gold standard does not identify any suffixes but the results contain suffixes, wrong suffixes implies that both gold standard and results identify suffixes but they are not the same, and correct types means that both gold standard and results contain suffixes and they match. Our model identifies 12257 suffix types, whereas Morfessor Baseline i"
I13-1152,J01-2001,0,0.247356,"Missing"
I13-1152,E09-1042,0,0.0172051,"itioned on ci , such that wi = si + mi , where si denotes the stem. Introduction The morphology of a word is an important indicator that determines its PoS tag, meanwhile the PoS tag of a word helps in identifying the correct morphological segmentation of the word. This relationship between morphology and syntax has been beneficial in both morphology learning with the exploitation of the syntactic features and in PoS tagging with the adoption of morphological features. There has been a number of research that have performed PoS tagging by making use of morphological information (Clark (2003), Hasan and Ng (2009), Abend et al. (2010), Christodoulopoulos et al. (2011), etc.). There has been also a number of other research that have performed morphological segmentation by adopting syntactic information (Hu et al. (2005), Can and Manandhar (2009), Lee et al. (2011), etc.). However, there is a small number of research that combines two tasks in a single framework. Sirts and Alum¨ae (2012) share a similar goal The generative story is summarised as follows: p(ci , ci 1,i+1 , wi , s, m) = p(ci )p(ci 1,i+1 |ci ) p(wi |ci )p(m|ci )p(s) 2.1 PoS Tagging The model adopts a finite mixture model for PoS tagging (se"
J18-2005,E12-1067,1,0.332305,"tructure of a group of morphologically related words. Following Goldsmith (2001) and Monson (2008), we use the term paradigm as consisting of a set of stems and a set of suffixes where each combination of a stem and a suffix leads to a valid word form, for example, {walk,talk,order,yawn}{s,ed,ing} generating the surface forms walk+ed, walk+s, walk+ing, talk+ed, talk+s, talk+ing, order+s, order+ed, order+ing, yawn+ed, yawn+s, yawn+ing. A sample paradigm is given in Figure 1. Recently, we introduced a probabilistic hierarchical clustering model for learning hierarchical morphological paradigms (Can and Manandhar 2012). Each node in the hierarchical tree corresponds to a morphological paradigm and each leaf node consists of a word. A single tree is learned, where different branches on the hierarchical tree Submission received: 29 June 2016; revised version received: 30 July 2017; accepted for publication: 1 March 2018. doi:10.1162/COLI a 00318 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics {walk, talk, order, lean} Volume 44, Number 2 {ed, ing, s, 0} walked walki"
J18-2005,W06-3209,0,0.0884747,"same section explains the inference and the morphological segmentation. Section 4 presents the experimental setting and the obtained evaluation scores from each experiment, and Section 5 concludes and addresses the potential future work following the model presented in this article. 2. Related Work There have been many unsupervised approaches to morphology learning that focus solely on segmentation (Creutz and Lagus 2005a, 2007; Snyder and Barzilay 2008; Poon, Cherry, and Toutanova 2009; Narasimhan, Barzilay, and Jaakkola 2015). Others, such as Monson et al. (2008), Can and Manandhar (2010), Chan (2006), and Dreyer and Eisner (2011), learn morphological paradigms that permit additional generalization. A popular paradigmatic model is Linguistica (Goldsmith 2001), which uses the Minimum Description Length principle to minimize the description length of a corpus based on paradigm-like structures called signatures. A signature consists of a list of suffixes that are seen with a particular stem—for example, order-{ed, ing, s} denotes a signature for the stem order. Snover, Jarosz, and Brent (2002) propose a generative probabilistic model that defines a probability distribution over different segm"
J18-2005,W02-0603,0,0.163419,"n morpheme are sampled. For precision, word pairs are sampled from the system results and checked against the gold standard segmentations. For recall, word pairs are sampled from the gold standard and checked against the system results. For each matching morpheme, 1 point is given. Precision and recall are calculated by normalizing the total obtained scores. We compare our results with other unsupervised systems from Morpho Challenge 2010 (Kurimo et al. 2010b) for English, German, and Turkish. More specifically, we compare our model with all competing unsupervised systems: Morfessor Baseline (Creutz and Lagus 2002), Morfessor CATMAP (Creutz and Lagus 2005a), Base Inference (Lignos 2010), Iterative Compounding (Lignos 2010), Aggressive Compounding (Lignos 2010), and Nicolas, Farr´e, and Molinero (2010). Additionally, we compare our system with the Morpho Chain model of Narasimhan, Barzilay, and Jaakkola (2015) by re-training their model on exactly the same training sets as ours. All evaluation was carried out by the Morpho Challenge organizers based on the hidden gold data sets. 365 Computational Linguistics Volume 44, Number 2 –6.00E+06 Marginal likelihood –7.00E+06 –8.00E+06 –9.00E+06 –1.00E+07 –1.10E+"
J18-2005,D11-1057,0,0.0532805,"Missing"
J18-2005,J01-2001,0,0.696228,"nsupervised morphological segmentation systems. Although we apply this model for morphological segmentation, the model itself can also be used for hierarchical clustering of other types of data. 1. Introduction Unsupervised learning of morphology has been an important task because of the benefits it provides to many other natural language processing applications such as machine translation, information retrieval, question answering, and so forth. Morphological paradigms provide a natural way to capture the internal morphological structure of a group of morphologically related words. Following Goldsmith (2001) and Monson (2008), we use the term paradigm as consisting of a set of stems and a set of suffixes where each combination of a stem and a suffix leads to a valid word form, for example, {walk,talk,order,yawn}{s,ed,ing} generating the surface forms walk+ed, walk+s, walk+ing, talk+ed, talk+s, talk+ing, order+s, order+ed, order+ing, yawn+ed, yawn+s, yawn+ing. A sample paradigm is given in Figure 1. Recently, we introduced a probabilistic hierarchical clustering model for learning hierarchical morphological paradigms (Can and Manandhar 2012). Each node in the hierarchical tree corresponds to a mor"
J18-2005,W10-2211,0,0.115948,"our experiments. The hyperparameters are set manually as a result of several experiments. These are the optimum values obtained from a number of experiments.2 Precision, recall, and F-score values against training set sizes are given in Figures 11 and 12 for English and Turkish, respectively. 4.1 Morpho Challenge Evaluation Although we experimented with different sizes of training sets, we used a randomly chosen 600K words from the English and 200K words from the Turkish and German data sets for evaluation purposes. Evaluation is performed according to the method proposed in Morpho Challenge (Kurimo et al. 2010a), which in turn is based on evaluation used by Creutz and Lagus (2007). The gold standard evaluation data set utilized within the Morpho Challenge is a hidden set that is not available publicly. This makes the Morpho Challenge evaluation different from other evaluations that provide test data. In this evaluation, word pairs 2 The source code of the model is accessible at: https://github.com/burcu-can/TreeStructuredDP. 364 unconvert+ed circl+ed telecommut+ing adım+ın ekipman+ını ski+station adım+ını uncloth+ed radar+station compris+ing radar+systeme temas+ını doer+ing şan+ını nam+ını radar+wa"
J18-2005,Q17-1025,0,0.0361341,"Missing"
J18-2005,Q15-1012,0,0.03368,"Missing"
J18-2005,N09-1024,0,0.0436923,"Missing"
J18-2005,W02-0602,0,0.181003,"Missing"
J18-2005,P08-1084,0,0.0374613,". Section 3 describes the probabilistic hierarchical clustering model with its mathematical model definition and how it is applied for morphological segmentation; the same section explains the inference and the morphological segmentation. Section 4 presents the experimental setting and the obtained evaluation scores from each experiment, and Section 5 concludes and addresses the potential future work following the model presented in this article. 2. Related Work There have been many unsupervised approaches to morphology learning that focus solely on segmentation (Creutz and Lagus 2005a, 2007; Snyder and Barzilay 2008; Poon, Cherry, and Toutanova 2009; Narasimhan, Barzilay, and Jaakkola 2015). Others, such as Monson et al. (2008), Can and Manandhar (2010), Chan (2006), and Dreyer and Eisner (2011), learn morphological paradigms that permit additional generalization. A popular paradigmatic model is Linguistica (Goldsmith 2001), which uses the Minimum Description Length principle to minimize the description length of a corpus based on paradigm-like structures called signatures. A signature consists of a list of suffixes that are seen with a particular stem—for example, order-{ed, ing, s} denotes a signature"
N03-1007,W99-0501,0,0.0136273,"Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Moldovan and Rus 2001) or by some form of pattern matching (e.g. Soubbotin 2002; Harabagiu et al. 1999). Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view). While a nu"
N03-1007,O97-1002,0,0.0142297,"rds, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question Answering task, giving an answer A a score dependent on the number of matching terms in A and the question. Our sentence similarity measure followed on these ideas, adding to the use of WordNet relations, part-ofspeech information, compound noun and word frequency information. In particular, sentence similarity was conside"
N03-1007,P99-1020,0,0.0601738,"Missing"
N03-1007,P01-1052,0,0.0181228,"02 for an overview of current systems). In order to achieve this (see for example Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Moldovan and Rus 2001) or by some form of pattern matching (e.g. Soubbotin 2002; Harabagiu et al. 1999). Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not"
N10-1010,S07-1002,0,0.143335,"ction we propose in this paper aims to overcome these limitations. 3 Method Given a set of words W , a WSI method is applied to each wi ∈ W (Section 3.1). The outcome of the first stage is a set of senses, S, where each sw i ∈ S denotes the i-th sense of word w ∈ W . This set 84 Figure 2: WSI for network & LAN of senses is the input to hierarchical clustering that produces a hierarchy of senses (Section 3.2). 3.1 Word sense induction WSI is the task of identifying the senses of a target word in a given text. Recent WSI methods were evaluated under the framework of SemEval2007 WSI task (SWSI) (Agirre and Soroa, 2007). The evaluation framework defines two types of assessment, i.e. evaluation in: (1) a clustering and (2) a WSD setting. Based on this evaluation, we selected the method of Klapaftis & Manandhar (2008) (henceforth referred to as KM) that achieves high Fscore in both evaluation schemes as compared to the systems participating in SWSI. We briefly describe KM mentioning its parameters used in our evaluation (Section 4). Figures 2 (a) and 2 (b) describe the different steps for inducing the senses of the target words network and LAN. Corpus preprocessing: The input to KM is a base corpus bc, in whic"
N10-1010,P81-1030,0,0.649985,"ying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). For instance, the pattern N P0 such as N P1 ,. . . ,N Pn suggests that N P0 is a hypernym of N Pi . For example, given the phrase Fruits, such as oranges, apples,..., the above pattern would suggest that fruit is a hypernym of orange and apple. These patternbased approaches operate at the word level by learning lexical relations between words rather than between senses of words. In the same spirit, other work attempted to exploit the regularities of dictionary entries to identify hyponymy relations (Amsler, 1981). For example in WordNet, WAN is defined as a computer network that spans . . . . Hence, one can easily induce that WAN is a hyponym of computer network by assuming that the first noun phrase in the definition is a hypernym of the target word. These approaches learn lexical relations at the sense level since dictionaries separate the senses of a word. However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). 83 Another limitation is that taxonomies are built according to the"
N10-1010,W06-3812,0,0.0867264,"s between vertices are present, if two collocations co-occur in one or more paragraphs. Figure 2 (a) shows that this process has generated 24 collocations for the target word network. On the top right of the figure we also observe the collocations associated with each paragraph. In the next step, a smoothing technique is applied to discover new edges between vertices. The weight applied to each edge connecting vertices vi and vj (collocations cab , cde ) is the maximum of their conditional probabilities (max(p(cab |cde ), p(cde |cab ))). Finally, the graph is clustered using Chinese whispers (Biemann, 2006). The final output is a set of senses, each one represented by a set of contextually related collocations. In Figure 2, we generated two senses for network and one sense for LAN. 3.2 Hierarchical clustering of senses Given the set of senses S, our task at this point is to hierarchically classify the senses using HAC. Consider for example the words network and LAN, and 2 The British National Corpus, 2001, Distributed by Oxford University Computing Services. 85 Senses computer network meshwork LAN computer network 1 0.0 0.66 meshwork LAN 0.0 1 0.14 0.66 0.14 1 Table 1: Similarity matrix for HAC."
N10-1010,P99-1016,0,0.0350662,"However this would be true if and only if the glosses of the dictionaries were sense-annotated, which is not the case for the majority of electronic dictionaries (Cimiano, 2006). 83 Another limitation is that taxonomies are built according to the sense distinctions present in dictionaries and not according to the actual use of words in the corpus. The majority of taxonomy learning approaches are based on the distributional hypothesis (Harris, 1968). Typically, distributional similarity methods (Cimiano et al., 2004; Cimiano et al., 2005; Faure and N´edellec, 1998; Reinberger and Spyns, 2004; Caraballo, 1999) utilise syntactic dependencies such as subject/verb, object/verb relations, conjunctive and appositive constructions and others. These dependencies are used to extract the features that serve as the dimensions of the vector space. Each target noun is then represented as a vector of extracted features where the frequency of co-occurrence of the target noun with each feature is used to calculate the weight of that feature. The constructed vectors are the input to hierarchical clustering or formal concept analysis (Ganter and Wille, 1999) to produce a taxonomy. These approaches assume that a tar"
N10-1010,C92-2082,0,0.0296622,"set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC) (King, 1967; Sneath and Sokal, 1973). We evaluate our method on two WordNetderived sub-taxonomies and show that our method leads to the development of concept hierarchies that capture a higher number of correct taxonomic relations in comparison to those generated by current distributional similarity approaches. 2 Related work Initial research on taxonomy learning focused on identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). For instance, the pattern N P0 such as N P1 ,. . . ,N Pn suggests that N P0 is a hypernym of N Pi . For example, given the phrase Fruits, such as oranges, apples,..., the above pattern would suggest that fruit is a hypernym of orange and apple. These patternbased approaches operate at the word level by learning lexical relations between words rather than between senses of words. In the same spirit, other work attempted to exploit the regularities of dictionary entries to identify hyponymy relations (Amsler, 1981). For example in WordNet, WAN is defined as a computer network that spans . . ."
N10-1010,C02-1167,0,0.0157136,"ion by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make 82 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics use of automatically acquired taxonomies (Cimiano, 2006), while question answering systems have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words (see Section 2). In this work, we show that Word Sense Induction (WSI) can be effectively employed to address this limitation of existing methods. We present a novel method that employs WSI to generate the different senses of a set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC) (King, 1967; Sneath and Sokal, 1973). We evaluate"
N10-1010,W04-0844,0,0.11065,"each concept belongs to its own group. Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms (King, 1967; Sneath and Sokal, 1973). The knowledge encoded in taxonomies can be utilised in a range of NLP applications. For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information (Cimiano, 2006). WordNet’s (Fellbaum, 1998) taxonomic relations have also been used in Word Sense Disambiguation (WSD) (Navigli and Velardi, 2004b). In named entity recognition, methods relying on gazetteers could make 82 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82–90, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics use of automatically acquired taxonomies (Cimiano, 2006), while question answering systems have also benefited (Moldovan and Novischi, 2002). Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words"
N10-1010,J04-2002,0,\N,Missing
N10-1089,P08-1037,0,0.033169,"Missing"
N10-1089,W03-1812,0,0.310967,"Missing"
N10-1089,W06-1201,0,0.450836,"Missing"
N10-1089,W00-1320,0,0.0339789,"Missing"
N10-1089,P94-1016,0,0.132822,"Missing"
N10-1089,J93-1003,0,0.201923,"Missing"
N10-1089,E06-1043,0,0.103329,"Missing"
N10-1089,W07-1102,0,0.123166,"Missing"
N10-1089,W07-1204,0,0.0263207,"Missing"
N10-1089,1997.iwpt-1.15,0,0.0138632,"Missing"
N10-1089,W06-1203,0,0.0756186,"Missing"
N10-1089,P09-2017,1,0.913903,"e consist the complex change class PL2P+MwA. 640 4 Target multiword expressions and corpora collection We created our set of target multiword expressions using WordNet 3.0 (Miller, 1995). Out of its 52, 217 multiword expressions we randomly chose 120. Keeping the ones that consist of two tokens resulted in the 118 expressions of Table 3. Manually inspecting these multiword expressions proved that they are all compound nominals, proper names or adjective-noun constructions. Each multiword expression was manually tagged as compositional or non-compositional, following the procedure described in Korkontzelos and Manandhar (2009). Table 3 shows the chosen multiword expressions together with information about their compositionality and the parts of speech of their components. Compositional Multiword expressions (Noun - Noun sequences) action officer (3119) key word (3131) pack rat (3443) picture palace (2231) prison guard (4801) tea table (62) bile duct (21649) king snake (2002) palm reading (4428) pill pusher (924) rat race (2556) telephone service (9771) agony aunt (751) checker board (1280) fish finger (1423) laser beam (16716) memory device (4230) sausage pizza (598) water snake (2649) air conditioner (24202) corn"
N10-1089,P99-1041,0,0.17788,"Missing"
N10-1089,W99-0621,0,0.0972318,"Missing"
N10-1089,W07-1104,0,0.114836,"Missing"
N10-1089,H05-1113,0,0.0270948,"Missing"
N10-1089,I05-1007,0,0.0728376,"Missing"
N16-1175,D15-1177,0,0.0547836,"Missing"
N16-1175,de-marneffe-etal-2014-universal,0,0.0257062,"Missing"
N16-1175,D14-1163,0,0.0974963,"Missing"
N16-1175,W09-2415,0,0.0348391,"Missing"
N16-1175,J15-4004,0,0.0809117,"Missing"
N16-1175,D14-1181,0,0.0282083,"Missing"
N16-1175,P14-2050,0,0.0413531,"Missing"
N16-1175,Q15-1016,0,0.0311416,"Missing"
N16-1175,C02-1150,0,0.0343612,"Missing"
N16-1175,D15-1278,0,0.0219529,"Missing"
N16-1175,N15-1142,0,0.0514563,"Missing"
N16-1175,D15-1279,0,0.0891682,"Missing"
N16-1175,D12-1110,0,0.0226065,"Missing"
N16-1175,D13-1170,0,0.0347261,"Missing"
N16-1175,P15-1150,0,0.0782183,"Missing"
N16-1175,P10-1040,0,0.029487,"Missing"
N16-1175,J10-4006,0,\N,Missing
N16-1175,D14-1082,0,\N,Missing
N16-1175,P15-1094,0,\N,Missing
P07-1098,P02-1034,0,0.930157,"various lexical, syntactic and semantic features. The retrieval and answer extraction phases consist in retrieving relevant documents (Collins-Thompson et al., 2004) and selecting candidate answer passages 776 from them. A further answer re-ranking phase is optionally applied. Here, too, the syntactic structure of a sentence appears to provide more useful information than a bag of words (Chen et al., 2006), although the correct way to exploit it is still an open problem. An effective way to integrate syntactic structures in machine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We ar"
P07-1098,kingsbury-palmer-2002-treebank,0,0.643429,"achine learning algorithms is the use of tree kernel (TK) functions (Collins and Duffy, 2002), which have been successfully applied to question classification (Zhang and Lee, 2003; Moschitti, 2006) and other tasks, e.g. relation extraction (Zelenko et al., 2003; Moschitti, 2006). In more complex tasks such as computing the relatedness between questions and answers in answer re-ranking, to our knowledge no study uses kernel functions to encode syntactic information. Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project (Kingsbury and Palmer, 2002) (www.cis.upenn.edu/∼ace) is a promising research direction. We argue that semantic structures can be used to characterize the relation between a question and a candidate answer. In this paper, we extensively study new structural representations, encoding parse trees, bag-of-words, POS tags and predicate argument structures (PASs) for question classification and answer re-ranking. We define new tree representations for both simple and nested PASs, i.e. PASs whose arguments are other predicates (Section 2). Moreover, we define new kernel functions to exploit PASs, which we automatically derive"
P07-1098,W05-0630,1,0.767863,"Missing"
P07-1098,P06-1136,0,\N,Missing
P09-2017,W06-1669,0,0.0604826,"Missing"
P09-2017,J93-1003,0,0.0827214,"th Agirre et al. (2006), only nouns are kept and lemmatized, since they are more discriminative than other PoS. B. Sense Induction methods can be broadly divided into vector-space models and graph based models. Sense induction methods are evaluated under the SemEval-2007 framework (Agirre and Soroa, 2007). We employ the collocational graphbased sense induction of Klapaftis and Manandhar (2008) in this work (henceforth referred to as KM). The method consists of 3 stages: Corpus preprocessing aims to capture nouns that are contextually related to the target MWE/head. Log-likelihood ratio (G2 ) (Dunning, 1993) with respect to a large reference corpus, Web 1T 5-gram Corpus (Brants and Franz, 2006), is used to capture the contextually relevant nouns. P1 is the G2 threshold below which nouns are removed from corpora. Graph creation. A collocation is defined as a pair of nouns cooccuring within a snippet. Each 3 Test set of MWEs To the best of our knowledge there are no noun compound datasets accompanied with compositionality judgements available. Thus, we developed an algorithm to aid human annotation. For each of the 52, 217 MWEs of WordNet 3.0 (Miller, 1995) we collected: 1 Thus, for “red carpet”, c"
P09-2017,W06-1203,0,0.0604324,"tment of Computer Science The University of York Heslington, York, YO10 5NG, UK suresh@cs.york.ac.uk Ioannis Korkontzelos Department of Computer Science The University of York Heslington, York, YO10 5NG, UK johnkork@cs.york.ac.uk Abstract context space and only uses the major senses, filtering out minor senses. In our approach the only language dependent components are a PoS tagger and a parser. There are several studies relevant to detecting compositionality of noun-noun MWEs (Baldwin et al., 2003) verb-particle constructions (Bannard et al., 2003; McCarthy et al., 2003) and verb-noun pairs (Katz and Giesbrecht, 2006). Datasets with human compositionality judgements are available for these MWE categories (Cook et al., 2008). Here, we focus on compound nominals, proper names and adjective-noun constructions. Our contributions are three-fold: firstly, we experimentally show that sense induction can assist in identifying compositional MWEs. Secondly, we show that unsupervised parameter tuning (Korkontzelos et al., 2009) results in accuracy that is comparable to the best manually selected combination of parameters. Thirdly, we propose a semi-supervised approach for extracting noncompositional MWEs from WordNet"
P09-2017,W09-1705,1,0.778951,"Missing"
P09-2017,P99-1004,0,0.0226094,"uster now represents a sense of the target word. KM produces larger number of clusters (uses) than expected. To reduce it we exploit the one sense per collocation property (Yarowsky, 1995). Given a cluster li , we compute the set Si of snippets that contain at least one collocation of li . Any clusters la and lb are merged if Sa ⊆ Sb . C. Comparing the induced senses. We used two techniques to measure the distributional similarity of major uses of the MWE and its semantic head, both based on Jaccard coefficient (J). “Major use” denotes the cluster of collocations which tags the most snippets. Lee (1999) shows that J performs better than other symmetric similarity measures such as cosine, Jensen-Shannon diver|A∩B| gence, etc. The first is Jc = J(A, B) = |A∪B| , where A, B are sets of collocations. The second, Jsn , is based on the snippets that are tagged by the induced uses. Let Ki be the set of snippets in which at least one collocation of the use i occurs. Jsn = J(Kj , Kk ), where j, k are the major uses of the MWE and its semantic head, respectively. D. Determining compositionality. Given the major uses of a MWE and its semantic head, the MWE is considered as compositional, when the corre"
P09-2017,W03-1810,0,0.0629097,"ulti-Word Expressions Suresh Manandhar Department of Computer Science The University of York Heslington, York, YO10 5NG, UK suresh@cs.york.ac.uk Ioannis Korkontzelos Department of Computer Science The University of York Heslington, York, YO10 5NG, UK johnkork@cs.york.ac.uk Abstract context space and only uses the major senses, filtering out minor senses. In our approach the only language dependent components are a PoS tagger and a parser. There are several studies relevant to detecting compositionality of noun-noun MWEs (Baldwin et al., 2003) verb-particle constructions (Bannard et al., 2003; McCarthy et al., 2003) and verb-noun pairs (Katz and Giesbrecht, 2006). Datasets with human compositionality judgements are available for these MWE categories (Cook et al., 2008). Here, we focus on compound nominals, proper names and adjective-noun constructions. Our contributions are three-fold: firstly, we experimentally show that sense induction can assist in identifying compositional MWEs. Secondly, we show that unsupervised parameter tuning (Korkontzelos et al., 2009) results in accuracy that is comparable to the best manually selected combination of parameters. Thirdly, we propose a semi-supervised approach f"
P09-2017,S07-1002,0,\N,Missing
P09-2017,W06-3812,0,\N,Missing
P09-2017,W06-1201,0,\N,Missing
P09-2017,P95-1026,0,\N,Missing
P09-2017,W03-1812,0,\N,Missing
P09-2017,W03-1809,0,\N,Missing
P09-2024,N04-4014,0,0.0350702,"Missing"
P09-2024,P06-4007,0,0.0163881,"sity of York, YO10 5DD, UK sgli@cs.york.ac.uk Suresh Manandhar Department of Computer Science University of York, YO10 5DD, UK suresh@cs.york.ac.uk Abstract the syntactic ambiguity present in the question. Phrases that are not modified by other phrases are considered to be highly ambiguous while phrases that are modified are considered less ambiguous. Small et al. (2004) utilizes clarification dialogue to reduce the misunderstanding of the questions between the HITIQA system and the user. The topics for such clarification questions are based on manually constructed topic frames. Similarly in (Hickl et al., 2006), suggestions are made to users in the form of predictive question and answer pairs (known as QUABs) which are either generated automatically from the set of documents returned for a query (using techniques first described in (Harabagiu et al., 2005), or are selected from a large database of questions-answer pairs created offline (prior to a dialogue) by human annotators. In Curtis et al. (2005), query expansion of the question based on Cyc Knowledge is used to generate topics for clarification questions. In Duan et al. (2008), the tree-cutting model is used to select topics from a set of rele"
P09-2024,P05-1026,0,0.029094,"hrases are considered to be highly ambiguous while phrases that are modified are considered less ambiguous. Small et al. (2004) utilizes clarification dialogue to reduce the misunderstanding of the questions between the HITIQA system and the user. The topics for such clarification questions are based on manually constructed topic frames. Similarly in (Hickl et al., 2006), suggestions are made to users in the form of predictive question and answer pairs (known as QUABs) which are either generated automatically from the set of documents returned for a query (using techniques first described in (Harabagiu et al., 2005), or are selected from a large database of questions-answer pairs created offline (prior to a dialogue) by human annotators. In Curtis et al. (2005), query expansion of the question based on Cyc Knowledge is used to generate topics for clarification questions. In Duan et al. (2008), the tree-cutting model is used to select topics from a set of relevant questions from Yahoo Answers. None of the above methods consider the contexts of the list of answers in the documents returned by QA systems. The topic of a good information-seeking question should not only be relevant to the original question b"
P09-2024,P08-1003,0,0.0655562,"Missing"
P09-2024,P08-1019,0,0.0435389,"Missing"
P11-1143,J93-2003,0,0.0258529,"1,...,S is a parallel corpus. In a sentence pair (f, e), source language String, f = f1 f2 ...fJ has J words, and e = e1 e2 ...eI has I words. And alignment a = a1 a2 ...aJ represents the mapping information from source language words to target words. Statistical machine translation models estimate P r(f|e), the translation probability from source language string e to target language string f (Och et al., 2003): P r(f , a|e). For different alignment models different approaches were proposed to estimate the corresponding alignments and parameters. The details can be found in (Och et al., 2003; Brown et al., 1993). 4.2 Information Need Prediction After estimating the statistical translation probabilities, we treat the information need prediction as the process of ranking words by p(w|Q), the probability of generating word w from question Q: X P (w|Q) = λ Ptr (w|t)P (t|Q) + (1 − λ)P (w|C) t∈Q The word-to-word translation probability Ptr (w|t) is the probability of word w is translated from a word t in question Q using the translation model. The above formula uses linear interpolation smoothing of the document model with the background language model P (t|C). λ is the smoothing parameter. P (t|Q) and P ("
P11-1143,D10-1010,0,0.0352777,"Missing"
P11-1143,W10-1201,0,0.0129985,"listic topic models. Instead, the texts are compared using some “thirdparty” topics that relate to them. A passage D in the retrieved documents (document collection) is represented as a mixture of fixed topics, with topic z get(D) ting weight θz in passage D and each topic is a distribution over a finite vocabulary of words, with (z) word w having a probability φw in topic z. Gibbs Sampling can be used to estimate the corresponding (D) expected posterior probabilities P (z|D) = θˆz and (z) P (w|z) = φˆw (Griffiths and Steyvers, 2004). In this paper we use two LDA based similarity measures in (Celikyilmaz et al., 2010) to measure the similarity between short information need texts. The first LDA similarity method uses KL divergence to measure the similarity between two documents under each given topic: (z=k) (z=k) , Dj )= (z=k) (z=k) (z=k) − KL(Dj (z=k) k k Di (z=k) + Dj 2 (z=k) (z=k) Di + Dj 2 ) ) (z=k) W (Di , Dj ) calculates the similarity between two documents under topic z = k using KL (z=k) divergence measure. Di is the probability distribution of words in document Di given a fixed topic z. The second LDA similarity measure from (Griffiths and Steyvers, 2004) treats each document as a probability dist"
P11-1143,P08-1019,0,0.229241,"were further used as training data to estimate probabilities for a translation-based question retrieval model. Wang et al. (2009) proposed a tree kernel framework to find similar questions in the CQA archive based on syntactic tree structures. Wang et al. (2010) mined lexical and syntactic features to detect question sentences in CQA data. 1426 2.2 Question Recommendation Wu et al. (2008) presented an incremental automatic question recommendation framework based on probabilistic latent semantic analysis. Question recommendation in their work considered both the users’ interests and feedback. Duan et al. (2008) made use of a tree-cut model to represent questions as graphs of topic terms. Questions were recommended based on this topic graph. The recommended questions can provide different aspects around the topic of the query question. The above question search and recommendation research provide different ways to retrieve questions from large archives of question answering data. However, none of them considers the similarity or diversity between questions by exploring their information needs. 3 Short Text Similarity Measures In question retrieval systems accurate similarity measures between document"
P11-1143,O97-1002,0,0.0247309,"aluation on a paraphrase recognition task showed that knowledgebased measures outperform the simpler lexical level approach. We follow the definition in (Mihalcea et al., 2006) to derive a text-to-text similarity metric mcs for two given texts Di and Dj : P w∈Di mcs(Di , Dj ) = P + w∈Dj simLDA1 (Di , Dj ) = K 1 X W (Di(z=k) ,Dj(z=k) ) 10 K k=1 maxSim(w, Dj ) ∗ idf (w) P w∈Di idf (w) W (Di maxSim(w, Di ) ∗ idf (w) P w∈Dj idf (w) − KL(Di For each word w in Di , maxSim(w, Dj ) computes the maximum semantic similarity between w and any word in Dj . In this paper we choose lin (Lin, 1998) and jcn (Jiang and Conrath, 1997) to compute the word-to-word semantic similarity. We only choose nouns and verbs for calculating mcs. Additionally, when w is a noun we restrict the words in document Di (and Dj ) to just nouns. Similarly, when w is a verb, we restrict the words in document Di (and Dj ) to just verbs. 3.3 topics discovered by Latent Dirichlet Allocation (LDA) methods. In contrast to the TFIDF method which measures “common words”, short texts are not compared to each other directly in probabilistic topic models. Instead, the texts are compared using some “thirdparty” topics that relate to them. A passage D in t"
P11-1143,W10-0404,0,0.0202237,"Missing"
P11-1143,C00-2163,0,0.0173561,"Missing"
P11-1143,J03-1002,0,0.0251515,"Missing"
P11-1143,voorhees-tice-2000-trec,0,0.104575,"Missing"
P11-1143,C10-1130,0,0.0401336,"Missing"
P17-2051,Q14-1030,0,0.0178983,"ent and compose diverse feature types from partially aligned and noisy resources. We perform experiments on Freebase utilizing additional entity type information and syntactic textual relations. Our evaluation suggests that the proposed models can better incorporate side information than previously proposed combinations of bilinear models with convolutional neural networks, showing large improvements when scoring the plausibility of unobserved facts with associated textual mentions. 1 Introduction Knowledge Bases (KB) are an important resource for many applications such as question answering (Reddy et al., 2014), relation extraction (Mintz et al., 2009) and named entity recognition (Ling and Weld, 2012). While large collaborative KBs like Freebase (Bollacker et al., 2008) and DBpedia (Auer et al., 2007) contain facts about million of entities, they are mostly incomplete and contain errors. A large amount of research has been dedicated to automatically extend knowledge bases, a task called Entity Linking or Knowledge Base Completion (KBC). Proposed approaches to KBC either reason about the internal structure of the KB, or utilize external data sources that indicate relations between the entities in th"
P17-2051,N13-1008,0,0.350946,"o very large datasets. Utilizing textual data or other external resources for KBC is a challenging task but has the potential of constantly updating KBs as new information becomes available. A line of work uses the KB as a means to obtain distant supervision to train relation extraction systems that classify textual mentions into one of the KBs relations (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012). State-of-the-art approaches for KBC with external textual data are obtained by latent feature models that jointly embed the KB symbols and text relations into the same space (Riedel et al., 2013; Toutanova et al., 2015). The benefit of such models over relation extraction systems is that they can combine both the internal structure of the KB and textual information to reason about the plausibility of unobserved facts. A commonly used approach for augmenting a KBC given an aligned text corpus is by adopting a Universal Schema (Riedel et al., 2013), where extracted textual relations between entities are directly added to the knowledge graph and treated the same as KB relations. This allows application of any latent variable model defined over triples to jointly embed the KB and text re"
P17-2051,D12-1042,0,0.0262313,"ations. Most approaches define a scoring function as a linear or bilinear operator. Latent feature models have shown good performance when considering the internal structure of KBs and are scalable to very large datasets. Utilizing textual data or other external resources for KBC is a challenging task but has the potential of constantly updating KBs as new information becomes available. A line of work uses the KB as a means to obtain distant supervision to train relation extraction systems that classify textual mentions into one of the KBs relations (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012). State-of-the-art approaches for KBC with external textual data are obtained by latent feature models that jointly embed the KB symbols and text relations into the same space (Riedel et al., 2013; Toutanova et al., 2015). The benefit of such models over relation extraction systems is that they can combine both the internal structure of the KB and textual information to reason about the plausibility of unobserved facts. A commonly used approach for augmenting a KBC given an aligned text corpus is by adopting a Universal Schema (Riedel et al., 2013), where extracted textual relations between en"
P17-2051,P11-1055,0,0.113723,"atent feature representations. Most approaches define a scoring function as a linear or bilinear operator. Latent feature models have shown good performance when considering the internal structure of KBs and are scalable to very large datasets. Utilizing textual data or other external resources for KBC is a challenging task but has the potential of constantly updating KBs as new information becomes available. A line of work uses the KB as a means to obtain distant supervision to train relation extraction systems that classify textual mentions into one of the KBs relations (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012). State-of-the-art approaches for KBC with external textual data are obtained by latent feature models that jointly embed the KB symbols and text relations into the same space (Riedel et al., 2013; Toutanova et al., 2015). The benefit of such models over relation extraction systems is that they can combine both the internal structure of the KB and textual information to reason about the plausibility of unobserved facts. A commonly used approach for augmenting a KBC given an aligned text corpus is by adopting a Universal Schema (Riedel et al., 2013), where extracted text"
P17-2051,D15-1174,0,0.217015,". Utilizing textual data or other external resources for KBC is a challenging task but has the potential of constantly updating KBs as new information becomes available. A line of work uses the KB as a means to obtain distant supervision to train relation extraction systems that classify textual mentions into one of the KBs relations (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012). State-of-the-art approaches for KBC with external textual data are obtained by latent feature models that jointly embed the KB symbols and text relations into the same space (Riedel et al., 2013; Toutanova et al., 2015). The benefit of such models over relation extraction systems is that they can combine both the internal structure of the KB and textual information to reason about the plausibility of unobserved facts. A commonly used approach for augmenting a KBC given an aligned text corpus is by adopting a Universal Schema (Riedel et al., 2013), where extracted textual relations between entities are directly added to the knowledge graph and treated the same as KB relations. This allows application of any latent variable model defined over triples to jointly embed the KB and text relations to the same space"
P17-2051,N16-1175,1,0.855776,"Missing"
P17-2051,P09-1113,0,0.415697,"a function of the latent feature representations. Most approaches define a scoring function as a linear or bilinear operator. Latent feature models have shown good performance when considering the internal structure of KBs and are scalable to very large datasets. Utilizing textual data or other external resources for KBC is a challenging task but has the potential of constantly updating KBs as new information becomes available. A line of work uses the KB as a means to obtain distant supervision to train relation extraction systems that classify textual mentions into one of the KBs relations (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012). State-of-the-art approaches for KBC with external textual data are obtained by latent feature models that jointly embed the KB symbols and text relations into the same space (Riedel et al., 2013; Toutanova et al., 2015). The benefit of such models over relation extraction systems is that they can combine both the internal structure of the KB and textual information to reason about the plausibility of unobserved facts. A commonly used approach for augmenting a KBC given an aligned text corpus is by adopting a Universal Schema (Riedel et al., 2013"
P94-1003,P90-1021,0,0.246776,"e case. Our solution to this problem is to dispense with the MSCD operation and to use generalization instead. However, we do propose that generalization should take inputs whose parallelism dependent anaphors have already been resolved. 1 In the case of the combination of (5a) and (5d), this will give 1As described in the next section, we use priority union to resolve these anaphors in both lists and contrasts. The use of generalization as a step towards checking that there is sufficient common ground is subsequent to the use of priority ration as the resolution mechanism. ~See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). 19 default structure, hence our preference to refer to it by the n a m e priority union. Below we demonstrate the results of priority union for the examples in ( l a ) - ( l c ) . Note t h a t the target is the strict structure and the source is the defeasible one. (11) Hannah likes beetles. So does Thomas. Source: Target: (12) 5a 5b (15) Jessy likes her Priority[ AGENT th°mas ] Union: PATIENT beetle like Hannah likes beetles. She alsolikes caterpillars. Source: Target: Priority Union: (13) The situations where the credulous version of the oper"
P94-1003,P93-1009,0,0.0722164,"eetle) = like(hannah, beetle) b. P(like) = like(hannah, beetle) Source: Target: Equation: Sol.1 ($1): Sol.2 (S2): Apply SI: Apply $2: like(jessy, brother-of (jessy) ) P( hannah ) P(jessy) = like(jessy, brother-of (jessy) ) P = ~x.like(x, brother-of(jessy)) e = Ax.like(x, brother-of(x)) like(hannah, brother-of (jessy) ) like(hannah, brother-of(hannah)) Parallelism In the DSP approach to vP-ellipsis and in our approach too, the emphasis has been on semantic parallelism. It has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for example, Kehler 1993 and Asher 1993). Kehler (1993) provides a useful discussion of the issue and argues convincingly that whether syntactic parallelism is required depends on the coherence relation involved. As the examples in (20) and (21) demonstrate, semantic parallelism is sufficient to establish a relation like contrast but it is not sufficient for building a coherent list. DSP claim that a significant attribute of their account is that they can provide the two readings in strict/sloppy ambiguities without having to postulate ambiguity in the source. They claim this as a virtue which is matched by few other"
P94-1003,P84-1085,0,0.0111088,"parallelism-dependent anaphors when they occur in list structures and must therefore be resolved to the corresponding fully referential subject/object in the first member of the list. Abstract 1 Marc (1) Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDM) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (DCUS). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anap"
P94-1003,C88-2120,0,0.0211539,"anaphors when they occur in list structures and must therefore be resolved to the corresponding fully referential subject/object in the first member of the list. Abstract 1 Marc (1) Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDM) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (DCUS). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anaphors whose interpretati"
P94-1035,J91-2001,0,0.0478051,"Missing"
P94-1035,P86-1038,0,0.0462053,"Missing"
P94-1035,J92-2002,0,0.0617597,"Missing"
P94-1035,E93-1003,0,\N,Missing
P99-1038,P92-1005,0,0.0323442,"on of determining whether a set of constraints represents an available reading of an ambiguous sentence or not. We show that a constraint language based upon Park's linguistic theory (Willis and Manandhar, 1999) solves this problem in polynomial time, and contrast this with recent work based on dominance constraints which shows that using the more permissive theory of availability to solve the same problems leads to NP-hardness. 2 Underspecification A recent area of interest has been with underspecified representations of an ambiguous sentence's meaning, for example, Quasi-Logical Form (QLF) (Alshawi and Crouch, 1992) and Underspecified Discourse Representation Theory (UDRT) (Reyle, 1995). We shall characterise the desirable properties of an underspecified meaning representation as: 1. the meaning of a sentence should be represented in a way that is not committed to any one of the possible (intended) meanings of the sentence, and 2. it should be possible to incrementally introduce partial information about the meaning, if such information is available, and without the need to undo work that has already been done. A principal aim of systems providing an underspecified representation of quantifier scope is t"
P99-1038,P98-1058,0,0.214343,"ver, representations which simply allow partial scopes to be stated without further analysis do not adequately capture the behaviour of quantitiers in a sentence. Consider the sentence Every representative of a company saw most samples, represented in the style of QLF: _:see(<+i every x _:rep.of(x, <+j exists y co(y)>)>, <+k most z sample(z)>) Our aim is to present a system in which there is a straightforward computational test of whether a well-formed reading of a sentence exists in which a partial scoping is satisfied, without requiring recourse to the final logical form. The language CLLS (Egg et al., 1998) has recently been developed which correctly generates the well-formed readings by using dominance constraints over trees. Readings of a sentence can be represented using a tree, where dominance represents outscoping, and quantifiers are represented using binary trees whose daughters correspond to the quantifiers' restriction and scope. So for the current example, Every representative of a company saw most samples, the reading: every(x, a(y, co(y), rep.o f ( x, y ) ), most(z, sample(z), see(x, z) ) ) can be represented by the tree in figure 1, where the restrictions of a and most have been omi"
P99-1038,J87-1005,0,0.686587,"by every man. One aspect of the problem is the generation of all available readings in a suitable representation language. Cooper (1983) described a system of &quot;storing&quot; the quantifiers as A-expressions during the parsing process and retrieving them at the sentence level; different orders of quantifier retrieval generate different readings of the sentence. However, Cooper's method generates logical forms in which variables are not correctly 293 bound by their quantifiers, and so do not correspond to a correct sentence meaning. This problem is rectified by nested storage (Keller, 1986) and the Hobbs and Shieber (1987) algorithm. However, the linguistic assumptions underlying these approaches have recently been questioned. Park (1995) has argued that the availability of readings is determined not by the well-formedness of sentences in the meaning language, but by the function-argument relationships within the sentence. His theory proposes that only a subset of the well-formed sentences generated by nested storage are available to a speaker of English. Although the theories have different generative power, it is difficult to find linguistic data that convincingly proves either theory correct. In the absence"
P99-1038,P95-1028,0,0.447194,"(1983) described a system of &quot;storing&quot; the quantifiers as A-expressions during the parsing process and retrieving them at the sentence level; different orders of quantifier retrieval generate different readings of the sentence. However, Cooper's method generates logical forms in which variables are not correctly 293 bound by their quantifiers, and so do not correspond to a correct sentence meaning. This problem is rectified by nested storage (Keller, 1986) and the Hobbs and Shieber (1987) algorithm. However, the linguistic assumptions underlying these approaches have recently been questioned. Park (1995) has argued that the availability of readings is determined not by the well-formedness of sentences in the meaning language, but by the function-argument relationships within the sentence. His theory proposes that only a subset of the well-formed sentences generated by nested storage are available to a speaker of English. Although the theories have different generative power, it is difficult to find linguistic data that convincingly proves either theory correct. In the absence of persuasive linguistic data, it is reasonable to ask whether other grounds exist for choosing to work with either of"
P99-1038,E95-1001,0,0.327759,"ambiguous sentence or not. We show that a constraint language based upon Park's linguistic theory (Willis and Manandhar, 1999) solves this problem in polynomial time, and contrast this with recent work based on dominance constraints which shows that using the more permissive theory of availability to solve the same problems leads to NP-hardness. 2 Underspecification A recent area of interest has been with underspecified representations of an ambiguous sentence's meaning, for example, Quasi-Logical Form (QLF) (Alshawi and Crouch, 1992) and Underspecified Discourse Representation Theory (UDRT) (Reyle, 1995). We shall characterise the desirable properties of an underspecified meaning representation as: 1. the meaning of a sentence should be represented in a way that is not committed to any one of the possible (intended) meanings of the sentence, and 2. it should be possible to incrementally introduce partial information about the meaning, if such information is available, and without the need to undo work that has already been done. A principal aim of systems providing an underspecified representation of quantifier scope is the ability to represent partial scopings. T h a t is, it should be possi"
P99-1038,C98-1056,0,\N,Missing
R13-1017,W02-0603,0,0.0524709,"n which ground truth morphemes are provided. Bernhard (Bernhard, 2008) suggests another morpheme labelling algorithm which labels morphemes as a stem, suffix, base, or prefix. Therefore, the proposed labelling method does not consider any allomorphs or homophonous morphemes. The paper is organised as follows: section 2 gives the intuition behind this work, section 3 describes our clustering algorithm, section 4 presents our experiment results, and finally section 5 and section 6 conclude the paper with a discussion on the obtained results. Introduction Most morphological segmentation systems (Creutz and Lagus (2002; Creutz and Lagus (2004; Goldsmith (2001)) perform only the segmentation of words and do not label morphs according to how they function in a word. As a rule, some morphemes function as inflective, whereas some morphemes function as derivative. However, we do not aim to distinguish inflection or derivation within a word, but we aim to distinguish between various types of morphs which are either inflective or derivative, e.g. allomorphs, homophonous morphemes. Labelling morphs not only helps with analysing the segmentation of a word, but can also help other natural language problems, i.e. part"
R13-1017,W04-0106,0,0.0262644,"phemes are provided. Bernhard (Bernhard, 2008) suggests another morpheme labelling algorithm which labels morphemes as a stem, suffix, base, or prefix. Therefore, the proposed labelling method does not consider any allomorphs or homophonous morphemes. The paper is organised as follows: section 2 gives the intuition behind this work, section 3 describes our clustering algorithm, section 4 presents our experiment results, and finally section 5 and section 6 conclude the paper with a discussion on the obtained results. Introduction Most morphological segmentation systems (Creutz and Lagus (2002; Creutz and Lagus (2004; Goldsmith (2001)) perform only the segmentation of words and do not label morphs according to how they function in a word. As a rule, some morphemes function as inflective, whereas some morphemes function as derivative. However, we do not aim to distinguish inflection or derivation within a word, but we aim to distinguish between various types of morphs which are either inflective or derivative, e.g. allomorphs, homophonous morphemes. Labelling morphs not only helps with analysing the segmentation of a word, but can also help other natural language problems, i.e. part-of-speech tagging. 129"
R13-1017,J01-2001,0,0.0779451,"nhard (Bernhard, 2008) suggests another morpheme labelling algorithm which labels morphemes as a stem, suffix, base, or prefix. Therefore, the proposed labelling method does not consider any allomorphs or homophonous morphemes. The paper is organised as follows: section 2 gives the intuition behind this work, section 3 describes our clustering algorithm, section 4 presents our experiment results, and finally section 5 and section 6 conclude the paper with a discussion on the obtained results. Introduction Most morphological segmentation systems (Creutz and Lagus (2002; Creutz and Lagus (2004; Goldsmith (2001)) perform only the segmentation of words and do not label morphs according to how they function in a word. As a rule, some morphemes function as inflective, whereas some morphemes function as derivative. However, we do not aim to distinguish inflection or derivation within a word, but we aim to distinguish between various types of morphs which are either inflective or derivative, e.g. allomorphs, homophonous morphemes. Labelling morphs not only helps with analysing the segmentation of a word, but can also help other natural language problems, i.e. part-of-speech tagging. 129 Proceedings of Rec"
S07-1092,W06-1669,0,0.2177,"Missing"
S07-1092,W04-0807,0,0.0365407,"nf idence(r0 ) = f req(a, b, c) f req(a, b) (2) Since there is a three-way relationship among a, b and c, we have two more rules r1 = {a, c} => {b} and r2 = {b, c} => {a}. Hence, the weighting of f is the average of the 3 calculated confidences. We apply a filtering heuristic (parameter p4 ) to remove hyperedges with low weights from the hypergraph. At the end of this stage, the constructed hypergraph is reduced, so that our hypergraph model agrees with the one described in subsection 2.1.1. 2.1.3 Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al., 2004) (S3LS), suggested that our hypergraphs are small-world networks, since they exhibited a high clustering coefficient and a small average path length. Furthermore, the frequency of vertices with a given degree plotted against the degree showed that our hypergraphs satisfy a power-law distribution P (d) = c ∗ d−α , where d is the vertex degree, P (d) is the frequency of vertices with degree d. Figure 2 shows the log-log plot for the noun difference of S3LS. lemmatised. Next, each induced cluster cj is assigned a score equal to the sum of weights of its hyperedges found in pi . 3 3.1 Evaluation P"
S07-1092,W04-2406,0,0.168835,"general definitions, they suffer from the lack of explicit semantic and topical relations or interconnections, and they often do not reflect the exact content of the context, in which the target word appears (Veronis, 2004). To overcome this limitation, unsupervised WSD has moved towards inducing the senses of a target word directly from a corpus, and then disambiguating each instance of it. Most of the work in WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word cooccurrences) (Schutze, 1998; Purandare and Pedersen, 2004). These vectors are clustered and the resulting clusters represent the induced senses. However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect lowfrequency senses of a target word. Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word. HyperLex (Veronis, 2004) and the adaptation of PageRank (Brin and Page, 1998) in (Agirre et al., 2006) have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervised WSD systems. Graph-based appr"
S07-1092,J98-1004,0,0.211939,"often contain general definitions, they suffer from the lack of explicit semantic and topical relations or interconnections, and they often do not reflect the exact content of the context, in which the target word appears (Veronis, 2004). To overcome this limitation, unsupervised WSD has moved towards inducing the senses of a target word directly from a corpus, and then disambiguating each instance of it. Most of the work in WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word cooccurrences) (Schutze, 1998; Purandare and Pedersen, 2004). These vectors are clustered and the resulting clusters represent the induced senses. However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect lowfrequency senses of a target word. Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word. HyperLex (Veronis, 2004) and the adaptation of PageRank (Brin and Page, 1998) in (Agirre et al., 2006) have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervise"
S07-1092,S07-1002,0,\N,Missing
S10-1011,S07-1002,0,0.178345,"ystem. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar & Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Query to the target word sense for which the query was created. The relations considered were WordNet’s hypernyms, hyponyms, synonyms, meronyms and holonyms. Each query was manually checked by one of the organisers to remove ambiguous words. The following example shows the query created for the first1 and second2 WordNet sense of the target noun failure. The created queries were issued to Yahoo! search API3 and for each query a maximum of 10"
S10-1011,D09-1056,0,0.357532,"Missing"
S10-1011,N06-2015,0,0.0362233,"hed the POS of the target word in our dataset. Training dataset 2.2 The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: Testing dataset The testing dataset consisted of instances of the same target words from the training dataset. This dataset is part of OntoNotes (Hovy et al., 2006). We used the sense-tagged dataset in which sentences containing target word instances are tagged with OntoNotes (Hovy et al., 2006) senses. The texts come from various news sources including CNN, ABC and others. <Target Word> AND <Relative Set> The <Target Word> consisted of the target word stem. The <Relative Set> consisted of a disjunctive set of word lemmas that were related 1 An act that fails An event that does not accomplish its intended purpose 3 http://developer.yahoo.com/search/ [Access:10/04/2010] 2 64 C1 C2 C3 C4 G1 10 20 1 5 G2 10 50 10 0 G3 15 0 60 0 When H(S|K) is 0, the solutio"
S10-1011,W09-2419,1,0.616074,"Missing"
S10-1011,D07-1043,0,0.711201,"ils failure AND (loss OR nonconformity OR test OR surrender OR ”force play” OR ...) failure AND (ruination OR flop OR bust OR stall OR ruin OR walloping OR ...) Table 2: Training set creation: example queries for target word failure for sense induction. Treating the testing data as new unseen instances ensures a realistic evaluation that allows to evaluate the clustering models of each participating system. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar & Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Query to the target word sense for which the quer"
S10-1011,H05-1059,0,0.00864447,"onsidered were WordNet’s hypernyms, hyponyms, synonyms, meronyms and holonyms. Each query was manually checked by one of the organisers to remove ambiguous words. The following example shows the query created for the first1 and second2 WordNet sense of the target noun failure. The created queries were issued to Yahoo! search API3 and for each query a maximum of 1000 pages were downloaded. For each page we extracted fragments of text that occurred in <p> </p> html tags and contained the target word stem. In the final stage, each extracted fragment of text was POS-tagged using the Genia tagger (Tsuruoka and Tsujii, 2005) and was only retained, if the POS of the target word in the extracted text matched the POS of the target word in our dataset. Training dataset 2.2 The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: Testing dataset The testing dataset consisted of instances of the"
S10-1079,S07-1002,0,0.124165,"r, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words (Katz and Giesbrecht, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). Typically, graph-based methods 2 Word Sense Induction In this section we present our word sense induction and disambiguation algorithms. Figure 355 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 355–358, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 1 shows an example showing how the sense induction algorithm works: The left side of part I shows the context nouns of four snippets containing the target noun “chip”. The most relevant of these nouns are represented as single word vertices (part II). Note that “customer”"
S10-1079,W06-1669,0,0.144961,"Missing"
S10-1079,W06-3812,0,0.145534,"Missing"
S10-1079,J93-1003,0,0.0585637,"and (c) clustering. In a number of different stages, the system uses a reference corpus to count occurrences of word or word pairs. It is chosen to be large because frequencies of words in a large corpus are more significant statistically. Ideally we would use the web or another large repository, but for the purposes of the SemEval-2010 task we used the union of all snippets of all target words. 2.1 Figure 1: An example showing how the proposed word sense induction system works. Nouns that occur infrequently in the reference corpus are removed (parameter P1 ). Then, loglikelihood ratio (LL) (Dunning, 1993) is employed to compare the distribution of each noun to its distribution in reference corpus. The null hypothesis is that the two distributions are similar. If this is true, LL is small value and the corresponding noun is removed (parameter P2 ). We also filter out nouns that are more indicative in the reference corpus than in the target word corpus; i.e. the nouns whose relative frequency in the former is larger than in the latter. At the end of this stage, each snippet is a list of lemmatised nouns contextually related to the target word. 2.2 Constructing the Graph All nouns appearing in th"
S10-1079,W06-1203,0,0.0151364,"enses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006), was long ago abandoned. The reason is that sense lists, such as WordNet (Miller, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words (Katz and Giesbrecht, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). Typically, graph-based methods 2 Word Sense Induction In this section we present our word sense induction and disambiguation algorithms. Figure 355 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 355–358, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 1 shows an example showing how the sense induction algorithm works: The left side of"
S10-1079,P95-1026,0,\N,Missing
S14-2004,E12-2021,0,0.136575,"Missing"
S14-2004,N10-1122,0,0.0570403,"still has the CD slot”) for different aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks. The restaurant reviews dataset of Ganu et al. (2009) uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral). Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, “The restaurant was expensive, but the menu was great.” would be assigned the aspect PRICE"
S14-2004,P08-1036,0,0.0512405,"Informatics Athens University of Economics and Business ion@aueb.gr Abstract Suresh Manandhar Dept. of Computer Science, University of York suresh@cs.york.ac.uk laptop”), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc. Subsequently, a review may convey opposing sentiments (e.g., “Its performance is ideal, I wish I could say the same about the price”) or objective information (e.g., “This one still has the CD slot”) for different aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt dif"
S14-2004,P02-1053,0,0.0623976,"k provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams. 1 John Pavlopoulos Dept. of Informatics, Athens University of Economics and Business annis@aueb.gr Introduction With the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span (Pang et al., 2002; Turney, 2002). However, the need for a more fine-grained approach, such as aspect-based (or ‘feature-based’) sentiment analysis (ABSA), soon became apparent (Liu, 2012). For example, laptop reviews not only express the overall sentiment about a specific model (e.g., “This is a great This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland,"
S14-2004,C10-2088,0,0.00353612,"nt aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different tasks. The restaurant reviews dataset of Ganu et al. (2009) uses six coarse-grained aspects (e.g., FOOD, PRICE, SERVICE) and four overall sentence polarity labels (positive, negative, conflict, neutral). Each sentence is assigned one or more aspects together with a polarity label for each aspect; for example, “The restaurant was expensive, but the menu was great.” would be assigned the aspect PRICE with negative polarity and FOO"
S14-2004,W02-1011,0,0.0452772,"ach aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams. 1 John Pavlopoulos Dept. of Informatics, Athens University of Economics and Business annis@aueb.gr Introduction With the proliferation of user-generated content on the web, interest in mining sentiment and opinions in text has grown rapidly, both in academia and business. Early work in sentiment analysis mainly aimed to detect the overall polarity (e.g., positive or negative) of a given text or text span (Pang et al., 2002; Turney, 2002). However, the need for a more fine-grained approach, such as aspect-based (or ‘feature-based’) sentiment analysis (ABSA), soon became apparent (Liu, 2012). For example, laptop reviews not only express the overall sentiment about a specific model (e.g., “This is a great This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, D"
S14-2004,W14-1306,1,0.655294,"ils: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August 23-24, 2014. ity (positive, negative, conflict, or neutral) of each aspect category discussed in each sentence. Subtasks SB1 and SB2 are useful in cases where no predefined inventory of aspect categories is available. In these cases, frequently discussed aspect terms of the entity can be identified together with their overall sentiment polarities. We hope to include an additional aspect term aggregation subtask in future (Pavlopoulos and Androutsopoulos, 2014b) to cluster near-synonymous (e.g., ‘money’, ‘price’, ‘cost’) or related aspect terms (e.g., ‘design’, ‘color’, ‘feeling’) together with their averaged sentiment scores as shown in Fig. 1. vided. No predefined inventory of aspects is provided, unlike the dataset of Ganu et al. The SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see Section 2). Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice. 2 Task Description For the first two subtasks (SB1, SB2), datasets on both domains (r"
S14-2004,E14-1009,1,0.616343,"ils: http://creativecommons.org/licenses/by/4.0/ 27 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August 23-24, 2014. ity (positive, negative, conflict, or neutral) of each aspect category discussed in each sentence. Subtasks SB1 and SB2 are useful in cases where no predefined inventory of aspect categories is available. In these cases, frequently discussed aspect terms of the entity can be identified together with their overall sentiment polarities. We hope to include an additional aspect term aggregation subtask in future (Pavlopoulos and Androutsopoulos, 2014b) to cluster near-synonymous (e.g., ‘money’, ‘price’, ‘cost’) or related aspect terms (e.g., ‘design’, ‘color’, ‘feeling’) together with their averaged sentiment scores as shown in Fig. 1. vided. No predefined inventory of aspects is provided, unlike the dataset of Ganu et al. The SemEval-2014 ABSA Task is based on laptop and restaurant reviews and consists of four subtasks (see Section 2). Participants were free to participate in a subset of subtasks and the domains (laptops or restaurants) of their choice. 2 Task Description For the first two subtasks (SB1, SB2), datasets on both domains (r"
S14-2004,piperidis-2012-meta,0,0.00755085,"al Train Test 90 31 10 1 20 3 23 8 357 51 500 94 Total Train Test 1232 418 321 83 597 172 431 118 1132 234 3713 1025 Table 3: Aspect categories distribution per sentiment class. general views about a restaurant, without explicitly referring to its atmosphere or environment. 3.3 the F1 measure, defined as usually: F1 = Format and Availability of the Datasets The datasets of the ABSA task were provided in an XML format (see Fig. 3). They are available with a non commercial, no redistribution license through META-SHARE, a repository devoted to the sharing and dissemination of language resources (Piperidis, 2012).5 4 2·P ·R P +R (1) where precision (P ) and recall (R) are defined as: P = |S ∩ G| |S ∩ G| ,R = |S| |G| (2) Here S is the set of aspect term or aspect category annotations (in SB1 and SB3, respectively) that a system returned for all the test sentences (of a domain), and G is the set of the gold (correct) aspect term or aspect category annotations. To evaluate aspect term polarity (SB2) and aspect category polarity (SB4) detection in Phase B, we calculated the accuracy of each system, defined as the number of correctly predicted aspect term or aspect category polarity labels, respectively, d"
S14-2004,H05-1043,0,0.147955,"nd Business ion@aueb.gr Abstract Suresh Manandhar Dept. of Computer Science, University of York suresh@cs.york.ac.uk laptop”), but also sentiments relating to its specific aspects, such as the hardware, software, price, etc. Subsequently, a review may convey opposing sentiments (e.g., “Its performance is ideal, I wish I could say the same about the price”) or objective information (e.g., “This one still has the CD slot”) for different aspects of an entity. ABSA is critical in mining and summarizing opinions from on-line reviews (Gamon et al., 2005; Titov and McDonald, 2008; Hu and Liu, 2004a; Popescu and Etzioni, 2005). In this setting, ABSA aims to identify the aspects of the entities being reviewed and to determine the sentiment the reviewers express for each aspect. Within the last decade, several ABSA systems of this kind have been developed for movie reviews (Thet et al., 2010), customer reviews of electronic products like digital cameras (Hu and Liu, 2004a) or netbook computers (Brody and Elhadad, 2010), services (Long et al., 2010), and restaurants (Ganu et al., 2009; Brody and Elhadad, 2010). Previous publicly available ABSA benchmark datasets adopt different annotation schemes within different task"
S14-2004,H05-2017,0,\N,Missing
S14-2007,S14-2108,0,0.0812075,"Missing"
S14-2007,S14-2134,0,0.0735188,"ion of development data helps, there are still systems that perform in the lower percentile who have used both training and development data for training, indicating that both the features and the machine learning classifier contribute to the models. A novel aspect of the SemEval-2014 shared task that differentiates it from the ShARE/CLEF task—other than the fact that it used more data and a new test set—is the fact that SemEval-2014 allowed the use of a much larger set of unlabeled MIMIC notes to inform the models. Surprisingly, only two of the systems (ULisboa (Leal et al., 2014) and UniPi (Attardi et al., 2014)) used the unlabeled MIMIC corpus to generalize the lexical features. Another team—UTH CCB(Zhang et al., 2014)—used off-the-shelf Brown clusters10 as opposed to training them on the unlabeled MIMIC II data. For Task B, the accuracy of a system using the strict metric was positively correlated with its recall on the disorder mentions that were input to it (i.e., recall for Task A), and did not get penalized for lower precision. Therefore one could essentially gain higher accuracy in Task B by tuning a system to provide the highest mention recall in Task A potentially at the cost of precision an"
S14-2007,S14-2147,0,0.0395526,"0.4 40.2 36.4 35.8 33.3 33.2 31.9 25.3 24.8 24.4 16.5 12.5 87.3 88.0 88.3 90.9 91.2 90.8 86.2 78.3 87.8 87.4 79.7 85.5 87.0 82.5 84.9 74.8 68.3 86.1 86.3 55.8 77.1 69.9 69.3 79.7 61.5 61.2 60.6 59.5 83.4 69.6 69.1 69.6 47.9 47.7 47.3 92.8 53.4 T+D T+D T+D T+D T+D T T+D T+D T T T T+D T+D T T+D T+D T+D T+D T T+D T T+D T+D T T T T T+D T+D T+D T+D T+D T+D T+D T+D P P the best accuracy for Task B and vice-versa for run 0 with run 1 in between the two. In order to fairly compare the performance between two systems one would have to provide perfect mentions as input to Task B. One of the systems—UWM Ghiasvand and Kate (2014)—did run some ablation experiments using gold standard mentions as input to Task B and obtained a best performance of 89.5F1 -score (Table 5 of Ghiasvand and Kate (2014)) as opposed to 62.3 F1 -score (Table 7) in the more realistic setting which is a huge difference. In the upcoming SemEval-2014 where this same evaluation is going to carried out under Task 14, we plan to perform supplementary evaluation where gold disorder mentions would be input to the system while attempting Task B. An interesting outcome of planning a follow-on evaluation to the ShARe/CLEF eHealth 2013 task was that we coul"
S14-2007,S14-2143,0,0.0959678,"Missing"
S14-2007,S14-2127,0,0.109755,"Missing"
S14-2007,S14-2019,0,0.0811758,"and 7. We have inserted the best performing system score from the ShARe/CLEF eHealth 2013 task in these tables. For Task A, referring to Tables 4 and 5, there is a boost of 3.7 absolute percent points for the F1 -score over the same task (Task 1a) in the ShARe/CLEF eHealth 2013. For Task B, referring to Tables 6 and 7, there is a boost of 13.7 percent points for the F1 -score over the same task (Task 1b) in the ShARe/CLEF eHealth 2013 evaluation. The participants used various approaches for tackling the tasks, ranging from purely rule-based/unsupervised (RelAgent (Ramanan and Nathan, 2014), (Matos et al., 2014), KUL11 ) to a hybrid of rules and machine learning classifiers. The top performing systems typically used the latter. Various versions of the IOB formulation were used for tagging the disorder mentions. None of the standard variations on the IOB formulation were explicitly designed or used to handle discontiguous mentions. Some systems used novel variations on this approach. Probably the simplest variation was applied by the UWM team (Ghiasvand and Kate, 2014). In this formulation the following labeled sequence “the/O left/B atrium/I is/O moderately/O Table 6: Performance on test data for par"
S14-2007,D09-1020,0,0.0182625,"entity recognition and word sense disambiguation. Neither of these problems are new to NLP. Research in general-domain NLP goes back to about two decades. For an overview of the development in the field through roughly 2009, we refer the refer to Nadeau and Sekine (2007). NLP has also penetrated the field of bimedical informatics and has been particularly focused on biomedical literature for over the past decade. Advances in that sub-field has also been documented in surveys such as one by Leaman and Gonzalez (2008). Word sense disambiguation also has a long history in the general NLP domain (Navigli, 2009). In spite of word sense annotations in the biomedical literature, recent work by Savova et al. (2008) highlights the importance of annotating them in clinical notes. This is true for many other clinical and linguistic phenomena as the various characteristics of the clinical narrative present a unique challenge to NLP. Recently various initiatives have led to annotated corpora for clinical NLP research. Probably the first comprehensive annotation performed on a clinical corpora was by Roberts et al. (2009), but unfortunately that corpus is not publicly available owing to privacy regulations. T"
S14-2007,S14-2007,1,0.107262,"ct = Number of correctly normalized disorder mentions; and Tg = Total number of disorder mentions in the gold standard. For Task B, the systems were only evaluated on annotations they identified in Task A. Relaxed accuracy only measured the ability to normalize correct spans. Therefore, it was possible to obtain very high values for this measure by simply dropping any mention with a low confidence span. 5 Participants A total of 21 participants from across the world participated in Task A and out of them 18 also participated in Task B. Unfortunately, although interested, the ThinkMiners team (Parikh et al., 2014) could not participate in Task B owing to some UMLS licensing issues. The participating organizations along with the contact user’s User ID and their chosen Team ID are mentioned in Table 3. Eight teams submitted three runs, six submitted two runs and seven submitted just one run. Out of these, only 13 submitted system description papers. We based our analysis on those system descriptions. 6 System Results Tables 4 and 6 show the performance of the systems on Tasks A and B. None of the systems used any additional annotated data so we did not have to compare them separately. Both tables mention"
S14-2007,S14-2083,0,0.0707782,"ts are presented in Tables 5 and 7. We have inserted the best performing system score from the ShARe/CLEF eHealth 2013 task in these tables. For Task A, referring to Tables 4 and 5, there is a boost of 3.7 absolute percent points for the F1 -score over the same task (Task 1a) in the ShARe/CLEF eHealth 2013. For Task B, referring to Tables 6 and 7, there is a boost of 13.7 percent points for the F1 -score over the same task (Task 1b) in the ShARe/CLEF eHealth 2013 evaluation. The participants used various approaches for tackling the tasks, ranging from purely rule-based/unsupervised (RelAgent (Ramanan and Nathan, 2014), (Matos et al., 2014), KUL11 ) to a hybrid of rules and machine learning classifiers. The top performing systems typically used the latter. Various versions of the IOB formulation were used for tagging the disorder mentions. None of the standard variations on the IOB formulation were explicitly designed or used to handle discontiguous mentions. Some systems used novel variations on this approach. Probably the simplest variation was applied by the UWM team (Ghiasvand and Kate, 2014). In this formulation the following labeled sequence “the/O left/B atrium/I is/O moderately/O Table 6: Performanc"
S14-2007,S15-2052,0,\N,Missing
S14-2007,S14-2142,0,\N,Missing
S15-2051,S15-2071,0,0.0854138,"predicted spans that overlap with a gold-standard span, then only one of them is chosen to be true positive (the longest ones), and the other predicted spans are considered false positives. 5 (6) where for each true-positive span there is a goldstandard value gsi,k and a predicted value psi,k for slot sk . 307 5.1 Results Task 1 16 teams participated in Task 1. Strict and relaxed precision, recall, and F metrics are reported in Figure 1. We relied on the strict F to rank different submissions. The best system from team ezDI reported 75.7 strict F, also reporting the highest relaxed F (78.7) (Pathak et al., 2015). For disorder span recognition, most teams used a CRF-based approach. Features explored included traditional NER features: lexical (bag of words and bigrams, orthographic features), syntactic features derived from either part-of-speech and phrase chunking information or dependency parsing, and domain features (note type and section headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similar"
S15-2051,S14-2007,1,0.756531,"Missing"
S15-2082,N10-1122,0,0.210315,"al-2015 Task 12: Aspect Based Sentiment Analysis Maria Pontiki*, Dimitrios Galanis*, Haris Papageorgiou*, Suresh Manandhar±, Ion Androutsopoulos◊* *Institute for Language and Speech Processing, Athena R.C., Athens, Greece ± Dept. of Computer Science, University of York, UK ◊ Dept. of Informatics, Athens University of Economics and Business, Greece {mpontiki, galanisd, xaris} @ilsp.gr suresh@cs.york.ac.uk ion@aueb.gr and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Abstract SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence- or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams. 1 Introduction and Related Wor"
S15-2082,piperidis-2012-meta,0,0.00812028,"Missing"
S15-2082,S14-2004,1,0.440354,"ers has also been addressed in the context of the Multilingual Opinion Analysis Task (Seki et al., 2007; Seki et al., 2008; Seki et al., 2010) and the Sentiment Slot Filling2 Task of the Knowledge Base Population Track (Mitchell, 2013). However, these tasks deal with the identification of opinion targets in general, not in the context of ABSA. SemEval-2014 Task 4 (SE-ABSA14) provided datasets annotated with aspect terms (e.g., “hard disk”, “pizza”) and their polarity for laptop and restaurant reviews, as well as coarser aspect categories (e.g., PRICE) and their polarity only for restaurants3 (Pontiki et al., 2014). The task attracted 165 submissions from 32 teams that experimented with a variety of features (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has bee"
S15-2082,W14-2605,0,0.0327647,"s (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has been adopted for the creation of aspect-level sentiment datasets in other languages, like Czech (Steinberger et al., 2014). SemEval-2015 Task 12 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks (aspect category extraction, aspect term extraction, polarity classification) into a principled unified framework (described in Section 2). In addition, SE-ABSA15 included an aspect level polarity classification subtask for the hotels domain in which no training data were provided (out-of-domain ABSA). The annotation schema and the provided datasets are described in Section 3. The evaluation measures and the baseline methods are described in Section 4, while the evaluation scores and the 2 http://www.nist.gov"
S15-2082,E12-2021,0,0.0529217,"Missing"
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
W00-1602,2000.iwpt-1.15,0,0.0157855,"rsing efficiency. A formal overview is given of precompilation and parsing. Modifications to ALE rules permit a closure over the rules from the lexicon, and analysis leading to a fast treatment of semantic structure. The closure algorithm, and retrieval of full semantic structure are described. 1 Relationship to Work Elsewhere In precompilation, Torisawa et. al. (2000) repeatedly applied rules leading to maximal projections from lexical heads, allocating categories to mothers and non-head daughters. Our approach allocates categories in a closure over the rules starting with the lexicon, as in Kiefer and Krieger (2000). We differ from both these in that the CFG grammar equates to the TFS grammar, accepting exactly the same strings: a CFG parse tree translates into a TFS tree with no loss of nodes. We precompiled 550 CF categories and 18,000 rules in 1 hour on a 280MHz. Pentium II. This compares to 5,500 categories and 2,200,000 rules from 11,000 lexemes in 45 hours by Kiefer and Krieger on a 300MHz. Sun Ultrasparc 2, where sets of TFSs in the closure are replaced by common most-general unifiers. The common unifier technique was used to add a CFG back-bone to a unification grammar by Carroll (1993). An np ha"
W00-1602,P85-1018,0,0.077407,"iscrimination tree which is traversed in correspondance with TFSl , from which types are ignored in a path subtree reached by where omit path : v , TFSl applies and considered where re − introduce path : v , TFSl applies. Where corresponding types from TFSl and the tree are unequal, a new branch is grown, terminating with a new Tl : this mechanism ensures each terminal is marked. Because coindexing by syntactic paths in a rule (Section 5) unifies index, this is re-introduced in an np and in a category that unifies with an np to form an np. The function in (3) is an example 14 of a restrictor, Shieber (1985), although here we shall show that it does not lead to any approximation in parsing. Precompilation generates multiple instantiations of each rule (1), paired with equivalent CFG rules, stored in a tuple: T0 → T1 Tn , rule − name (4) principle. The CFG grammar generated by this method accepts the same strings as the HPSG grammar and does not just approximate it. The semantic component sem0 represents a TFS, identical to sem j in that sub-constituent known as the semantic head, according to the semantics principle of HPSG, except that some paths are co-indexed with paths in the semantic compone"
W01-0720,P96-1024,0,0.0674953,"Missing"
W01-0720,C94-1024,0,0.0151689,"987) using a similar (if rather more empiricist) setting also uses syntactic analysis and compression to build grammars. However, this syntactic analysis would appear to be very expensive and the system has not been applied to large scale problems. The compression metric is applied with respect to the compression of the corpus, rather than the compression of syntactic information extracted from the corpus, as in C LL. It seems unlikely that this simple induction algorithm would generate linguistically plausible grammars when presented with complex naturally occurring data. Joshi and Srinivas (Joshi and Srinivas, 1994) have developed a method called supertagging that similarly attaches complex syntactic tags (supertags) to words. The most effective learning model appears to have been a combination of symbolic and stochastic techniques, like the approach presented here. However, a full lexicon is supplied to the learner, so that the problem is reduced to one of disambiguating between the possible supertags. The learning appears to be supervised and occurs over parts-of-speech rather than over the actual words. However, some notion of label accuracy is supplied and this can be compared with the accuracy of ou"
W01-0720,J93-2004,0,0.0275749,"tural language is often attempted without using the knowledge available from other research areas such as psychology and linguistics. This can lead to systems that solve problems that are neither theoretically or practically useful. In this paper we present a system C LL which aims to learn natural language syntax in a way that is both computationally effective and psychologically plausible. This theoretically plausible system can also perform the practically useful task of unsupervised learning of syntax. C LL has then been applied to a corpus of declarative sentences from the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994) on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems, which are significantly more supervised and are applied to somewhat simpler problems. 1 Introduction Computational learning of natural language can be considered from two common perspectives. Firstly, there is the psychological perspective, which leads to the investigation of learning problems similar to those faced by people and the building of systems that seek to model human language learning faculties. Secondly, there is the computational perspective,"
W01-0720,H94-1020,0,0.448031,"en attempted without using the knowledge available from other research areas such as psychology and linguistics. This can lead to systems that solve problems that are neither theoretically or practically useful. In this paper we present a system C LL which aims to learn natural language syntax in a way that is both computationally effective and psychologically plausible. This theoretically plausible system can also perform the practically useful task of unsupervised learning of syntax. C LL has then been applied to a corpus of declarative sentences from the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994) on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems, which are significantly more supervised and are applied to somewhat simpler problems. 1 Introduction Computational learning of natural language can be considered from two common perspectives. Firstly, there is the psychological perspective, which leads to the investigation of learning problems similar to those faced by people and the building of systems that seek to model human language learning faculties. Secondly, there is the computational perspective, which seeks to build"
W01-0720,W97-1010,0,0.0194126,"supertagging that similarly attaches complex syntactic tags (supertags) to words. The most effective learning model appears to have been a combination of symbolic and stochastic techniques, like the approach presented here. However, a full lexicon is supplied to the learner, so that the problem is reduced to one of disambiguating between the possible supertags. The learning appears to be supervised and occurs over parts-of-speech rather than over the actual words. However, some notion of label accuracy is supplied and this can be compared with the accuracy of our system. Osborne and Briscoe (Osborne and Briscoe, 1997) present a fairly supervised system for learning unusual stochastic CGs (the atomic categories a far more varied than standard CG) again using part-of-speech strings rather than words. While the problem solved is much simpler, this system provides a suitable comparison for learning appropriate lexicons for parsing. Neither Joshi and Srinivas (Joshi and Srinivas, 1994) nor Osborne and Briscoe (Osborne and Briscoe, 1997) can be considered psychologically plausible, but they are computationally effective and they do provide results for comparison. Two other approaches to learning CGs are presente"
W01-0720,W99-0909,1,0.659154,"Missing"
W01-0720,W01-0904,1,0.820375,"Missing"
W01-0720,J03-4003,0,\N,Missing
W01-0904,T75-2001,0,0.225269,"pp category is included in the CG with this approach, but not with our approach, as it is a convenient shorthand for the prepositional phrase category. The second approach is a multiple-pass datadriven system. Rules for translating the trees are applied in order of complexity starting with simple part-of-speech translation and finishing with a category generation stage. s
p BA s 4.1 Top-Down Category Generation The algorithm has two stages. Figure 1: An Example Parse in Basic CG The CG formalism described above has been shown to be weakly equivalent to context-free phrase structure grammars (Bar-Hillel et al., 1964). While such expressive power covers a large amount of natural language structure, it has been suggested that a more flexible and expressive formalism may capture natural language more accurately (Wood, 1993; Steedman, 1993). In future we may consider applying the principle developed here to perform translations to these more complex formalisms, although many of the changes will not actually change the lexical entries, just the way they can be combined. 4 Alternative Approaches This section presents the two approaches to translation that are being compared. Firstly, there is Mark constituents"
W01-0904,P98-1115,0,0.0132428,"he Charniak approach, which is to extract subtree-based grammars e.g. the DataOriented Parsing (DOP) approach (Bod, 1995), or extracting Lexicalised Tree Adjoining Grammars (LTAGs), or more generally Lexicalised Tree Grammars (LTGs) (Neumann, 1998; Xia, 1999; Chen and Vijay-Shanker, 2000). Each approach involves a process that splits up the annotated trees in the treebank into a set of subtrees that define the grammar. These approaches still continue to work with the syntactic data in the same form as it is found in the corpora. A slightly different approach has been followed by Krotov et al (Krotov et al., 1998), where they extract the grammar from the Penn Treebank like Charniak, but then compact it. This provides a smaller grammar of similar quality to a grammar that has not been compacted, when a linguistically motivated compaction is used. However, the formalism remains unchanged. Similarly, Johnson (Johnson, 1998) modifies the labelling of the Penn Treebank, but remains within a CFG framework. Hockenmaier et al (Hockenmaier et al., 2000), although to some extent following the approach of Xia (Xia, 1999) where LTAGs are extracted, have pursued an alternative by extracting Combinatory Categorial G"
W01-0904,J93-2004,0,0.0367435,"ve a correct standard against which to compare the results of systems attempting to solve the task. Similarly, it is crucial in a language learning context, where what is learned can be used to annotate examples e.g. syntax learning, lexical learning. In this case the learned artefact is used to annotate the examples, which can then be compared against the correctly annotated version. Hence, correctly annotated corpora are vital for the evaluation of a very large number of NLP tasks. Unfortunately, there are often no suitably annotated corpora for a given task. For example, the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994; Bies et al., 1994) provides a large corpus of syntactically annotated examples mostly from the Wall Street Journal. It is an excellent resource for tasks dealing with the syntax of written English. However, if the annotation formalism (a phrase-structure grammar with some simple features) does not match that of one’s NLP system, it is of very little use. For example, suppose a parser using Categorial Grammar (Wood, 1993; Steedman, 1993) is developed and applied to the examples in the corpus. While the bracketing of the examples will bear a strong relationship to the brac"
W01-0904,H94-1020,0,0.272639,"against which to compare the results of systems attempting to solve the task. Similarly, it is crucial in a language learning context, where what is learned can be used to annotate examples e.g. syntax learning, lexical learning. In this case the learned artefact is used to annotate the examples, which can then be compared against the correctly annotated version. Hence, correctly annotated corpora are vital for the evaluation of a very large number of NLP tasks. Unfortunately, there are often no suitably annotated corpora for a given task. For example, the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994; Bies et al., 1994) provides a large corpus of syntactically annotated examples mostly from the Wall Street Journal. It is an excellent resource for tasks dealing with the syntax of written English. However, if the annotation formalism (a phrase-structure grammar with some simple features) does not match that of one’s NLP system, it is of very little use. For example, suppose a parser using Categorial Grammar (Wood, 1993; Steedman, 1993) is developed and applied to the examples in the corpus. While the bracketing of the examples will bear a strong relationship to the bracketing of the treeban"
W01-0904,W98-0131,0,0.0214562,"liest example is the approach of Charniak (Charniak, 1996), who simply extracted a context-free grammar by reading off the production rules implied by the trees in the Penn Treebank. While not translating the formalism of the treebank, this has led to work extracting grammars of different formalisms. The majority of work is based on the most obvious extension of the Charniak approach, which is to extract subtree-based grammars e.g. the DataOriented Parsing (DOP) approach (Bod, 1995), or extracting Lexicalised Tree Adjoining Grammars (LTAGs), or more generally Lexicalised Tree Grammars (LTGs) (Neumann, 1998; Xia, 1999; Chen and Vijay-Shanker, 2000). Each approach involves a process that splits up the annotated trees in the treebank into a set of subtrees that define the grammar. These approaches still continue to work with the syntactic data in the same form as it is found in the corpora. A slightly different approach has been followed by Krotov et al (Krotov et al., 1998), where they extract the grammar from the Penn Treebank like Charniak, but then compact it. This provides a smaller grammar of similar quality to a grammar that has not been compacted, when a linguistically motivated compaction"
W01-0904,W99-0909,1,0.850552,"Missing"
W01-0904,W01-0720,1,0.811424,"eebank from the standard phrase structure annotation to a Categorial Grammar (CG) annotation and in the process induces large scale CG lexicons. It is a data-driven multi-pass system that uses both predefined rules and machine learning techniques to translate the trees and in the process induce a large scale CG lexicon. The system was designed to produce the lexical annotations for the sentences without null elements (i.e. without movement) from the Penn Treebank, so that these could be used to evaluate the results produced by an unsupervised CG lexicon learner (Watkinson and Manandhar, 2000; Watkinson and Manandhar, 2001). The system has four major features. Firstly, there is significant control over how the treebank is annotated. This is vital if the results are to be used for evaluation. Secondly, the system prevents propagation of translation errors throughout the trees by being data-driven. Thirdly, the system deals elegantly with erroneous annotation, even providing a degree of self-correction. Finally, the approach is general enough to apply to other similar problems. The system is compared with a top-down alternative based on the algorithm of Hockenmaier et al (Hockenmaier et al., 2000), which is curren"
W01-0904,2000.iwpt-1.9,0,0.012737,"ach of Charniak (Charniak, 1996), who simply extracted a context-free grammar by reading off the production rules implied by the trees in the Penn Treebank. While not translating the formalism of the treebank, this has led to work extracting grammars of different formalisms. The majority of work is based on the most obvious extension of the Charniak approach, which is to extract subtree-based grammars e.g. the DataOriented Parsing (DOP) approach (Bod, 1995), or extracting Lexicalised Tree Adjoining Grammars (LTAGs), or more generally Lexicalised Tree Grammars (LTGs) (Neumann, 1998; Xia, 1999; Chen and Vijay-Shanker, 2000). Each approach involves a process that splits up the annotated trees in the treebank into a set of subtrees that define the grammar. These approaches still continue to work with the syntactic data in the same form as it is found in the corpora. A slightly different approach has been followed by Krotov et al (Krotov et al., 1998), where they extract the grammar from the Penn Treebank like Charniak, but then compact it. This provides a smaller grammar of similar quality to a grammar that has not been compacted, when a linguistically motivated compaction is used. However, the formalism remains u"
W01-0904,J98-4004,0,\N,Missing
W01-0904,J03-4003,0,\N,Missing
W01-0904,C98-1111,0,\N,Missing
W06-1809,E99-1042,0,0.0331548,"more relief to the post-retrieval phase and to the role of the UM. Having removed the need for fine-grained answer spotting, the emphasis is shifted towards finding closely connected sentences that are highly KRAQ06 relevant to answer the query. Readability Within computational linguistics, several applications have been designed to address the needs of users with low reading skills. The computational approach to textual adaptation is commonly based on natural language generation: the process “translate” a difficult text into a syntactically and lexically simpler version. In the case of PSET (Carroll et al., 1999) for instance, a tagger, a morphological analyzer and generator and a parser are used to reformulate newspaper text for users affected by aphasia. Another interesting research is Inui et al.’s lexical and syntactical paraphrasing system for deaf students (Inui et al., 2003). In this system, the judgment of experts (teachers) is used to learn selection rules for paraphrases acquired using various methods (statistical, manual, etc.). In the SKILLSUM project (Williams and Reiter, 2005), used to generate literacy test reports, a set of choices regarding output (cue phrases, ordering and punctuatio"
W06-1809,N04-1025,0,0.027238,"ses. Two attributes are used to classify a phrase p as a keyphrase or a non-keyphrase: its TF × IDF score within the set of retrieved documents and the index of p’s first appearance in the document. Kea outputs a ranked list of phrases, among which we select the top three as keyphrases for each of our documents. 4 http://www.google.com KRAQ06 3.2.3 Estimation of reading levels In order to adjust search result presentation to the user’s reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004). Whereas other popular approaches such as Flesch-Kincaid (Kincaid et al., 1975) are based on sentence length, the language modelling approach accounts especially for lexical information. The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). Moreover, it is more applicable than length-based approach for Web documents, where sentences are typically short regardless of the complexity of the text. The language modelling approach proceeds in two phases: in the training pha"
W06-1809,W03-1602,0,0.0150841,"l linguistics, several applications have been designed to address the needs of users with low reading skills. The computational approach to textual adaptation is commonly based on natural language generation: the process “translate” a difficult text into a syntactically and lexically simpler version. In the case of PSET (Carroll et al., 1999) for instance, a tagger, a morphological analyzer and generator and a parser are used to reformulate newspaper text for users affected by aphasia. Another interesting research is Inui et al.’s lexical and syntactical paraphrasing system for deaf students (Inui et al., 2003). In this system, the judgment of experts (teachers) is used to learn selection rules for paraphrases acquired using various methods (statistical, manual, etc.). In the SKILLSUM project (Williams and Reiter, 2005), used to generate literacy test reports, a set of choices regarding output (cue phrases, ordering and punctuation) are taken by a micro-planner based on a set of rules. Our approach is conceptually different from the above: exploiting the wealth of information available in the context of a Web-based QA system, we can afford to choose among the documents available on a given subject t"
W06-1809,O97-1002,0,0.0325462,"tem returned the following passages: —U Mgood : “Sistine Chapel (sis-teen). A chapel adjoining Saint Peter’s Basilica, noted for the frescoes of biblical 3.3.2 Semantic similarity Within each of the documents retained, we seek for the sentences which are semantically most relevant to the query. Given a sentence p and the query q, we represent them as two sets of words P = {pw1 , . . . , pwm } and Q = {qw1 , . . . , qwn }. The semantic distance from p to q is then: P distq (p) = 1≤i≤m minj [d(pwi , qwj )] where d(pwi , qwj ) represents the Jiang-Conrath word-level distance between pwi and qwj (Jiang and Conrath, 1997), based on WordNet 2.0. The intuition is that for each question word, we find the word in the candidate answer sentence which minimizes the word-level distance and then we compute the sum of such minima. 3.3.3 Passage and cluster ranking For a given document, we can thus isolate a sentence s minimizing the distance to the query. The passage P , i.e. a window of up to 5 sentences centered on s, will be a candidate result. We assign to such passage a score equal to the similarity of s to the query; in turn, the score of P is used as the score of the document containing it. We also define a ranki"
W06-1809,W05-1616,0,0.0240058,"on: the process “translate” a difficult text into a syntactically and lexically simpler version. In the case of PSET (Carroll et al., 1999) for instance, a tagger, a morphological analyzer and generator and a parser are used to reformulate newspaper text for users affected by aphasia. Another interesting research is Inui et al.’s lexical and syntactical paraphrasing system for deaf students (Inui et al., 2003). In this system, the judgment of experts (teachers) is used to learn selection rules for paraphrases acquired using various methods (statistical, manual, etc.). In the SKILLSUM project (Williams and Reiter, 2005), used to generate literacy test reports, a set of choices regarding output (cue phrases, ordering and punctuation) are taken by a micro-planner based on a set of rules. Our approach is conceptually different from the above: exploiting the wealth of information available in the context of a Web-based QA system, we can afford to choose among the documents available on a given subject those which best suit our readability requirements. This is possible thanks to the versatility of language modelling, which allows us to tailor the readability estimation of documents to any kind of user profile in"
W08-0123,W00-1407,0,0.0282339,"to the user input, generating one or more utterances for a given plan step, allowing for reactions to user’s counter arguments as well as backchannel and chitchat phases without cluttering the plan. Experimental results show that this layered ap139 proach allows the user to feel more comfortable in the dialogue while preserving the dialogue consistency provided by the planner. Eventually, this translates into a more persuasive dialogue (see Section 6). 2 Related Work Persuasion through dialogue is a novel field of Human Computer Interaction. Reiter, Robertson, and Osman (2003),Reed (1998) and Carenini and Moore (2000) apply persuasive communication principles to natural language generation, but only focus on monologue. The 3-tier planner for tutoring dialogue by Zinn, Moore, and Core (2002) provides a dialogue management technique close to our approach: a top-tier generates a dialogue plan, the middle-tier generates refinements to the plan and the bottom-tier generates utterances. Mazzotta, de Rosis, and Carofiglio (2007) also propose a planning framework for user-adapted persuasion where the plan operators are mapped to natural language (or ECA) generation. However, these planning approaches do not includ"
W08-0123,A00-1008,0,0.0282528,"ing topic in the field of human computer dialogue. In this paper we describe a novel approach to dialogue management that has been developed to achieve persuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue. 1 Introduction Human computer dialogue is a wide research area in Artificial Intelligence. Computer dialogue is now used at production stage for applications such as tutorial dialogue – that helps teaching students (Freedman, 2000) – task-oriented dialogue – that achieves a particular, limited task, such as booking a trip (Allen et al., 2000) – and chatbot dialogue (Levy et al., 1997) – that is used within entertainment and help systems. None of these approaches use persuasion as a mechanism to achieve dialogue goals. However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents wit"
W08-0123,P04-1085,0,0.0491128,"Missing"
W08-0123,N03-2012,0,0.0666778,"Missing"
W09-1702,C94-1014,0,0.152531,"Missing"
W09-1702,W04-3208,0,0.0160347,"gure 2: Bilingual word pairs are found within context of cognate word civil Figure 3: Utilizing technique with spelling similarity 1996 - 1999, year 2000 - 2003 and year 2004 - 2006. • We only take the first part, about 400k sentences of Europarl Spanish (year 1996 - 1999) and 2nd part, also about 400k from Europarl English (year 2000 - 2003). We refer the particular part taken from the source language corpus as S and the other part of the target language corpus as T. This approach is quite common in order to obtain non-parallel but comparable (or same domain) corpus. Examples can be found in Fung and Cheung (2004), followed by Haghighi et al. (2008). For corpus pre-processing, we only use sentence boundary detection and tokenization on raw text. We decided that large quantities of raw text requiring minimum processing could also be considered as minimal since they are inexpensive and not limited. These should contribute to low or medium density languages for which annotated resources are limited. We also clean all tags and filter out stop words from the corpus. 4.2 Figure 4: Utilizing technique with context similarity Ws or Wt respectively. For example, word participation and education occurring in the"
W09-1702,P98-1069,0,0.0688498,"nt approach. Instead, we use list of contextually relevant terms that co-occur with cognate pairs. For approaches based on contextual features or context similarity, we assume that for a word that occurs in a certain context, its translation equivalent also occurs in equivalent contexts. Contextual features are the frequency counts of context words occurring in the surrounding of target word W. A context vector for each W is then constructed, with only context words found in the seed lexicon. The context vectors are then translated into the target language before their similarity is measured. Fung and Yee (1998) point out that not only the number of common words in context gives some similarity clue to a word and its translation, but the actual ranking of the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors. This idea is similar to Rapp (1999) who proposed to transform all co-occurrence vectors using log likelihood ratio instead of just using the frequency counts of the co-occurrences. These values are used to define whether the context words are highly assoc"
W09-1702,W97-0119,0,0.0997625,"Missing"
W09-1702,P08-1088,0,0.212814,"collected word pairs. Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 10–17, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Yarowski, 2001). However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. Haghighi et al. (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. Instead, they propose assigning a feature to each substring of length of three or less for each word. 3 The Utilizing Technique Most works in bilingual lexicon extraction use lists of high frequency words that are obtained from source and target language corpus to be their source and target word lists respectively. In our work, we aim to extract a high precision bilingual lexicon using different approach. Instead, we use list of contextually relevant terms that co-occur with cognate pairs."
W09-1702,W02-0902,0,0.709507,"t 50.0 percent recall. Precision of 79.0 percent at 50.0 percent recall is recorded when using this technique with context similarity approach. Furthermore, by using a string edit-distance vs. precision curve, we also reveal that the latter model is able to capture words efficiently compared to a baseline model. Section 2 is dedicated to mention some of the related works. In Section 3, the technique that we used is explained. Section 4 describes our experimental setup followed by the evaluation results in Section 5. Discussion and conclusion are in Section 6 and 7 respectively. 2 Related Work Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. In reporting our work, we treat both identical word pairs and similar spelling word pairs as cognate pairs. Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. They propose to restrict the word length, at least of length 6, to increase the accuracy of the collected word pairs. Koehn and Knight (2002) mentio"
W09-1702,P95-1050,0,0.291661,"Missing"
W09-1702,P99-1067,0,0.747366,"t word W. A context vector for each W is then constructed, with only context words found in the seed lexicon. The context vectors are then translated into the target language before their similarity is measured. Fung and Yee (1998) point out that not only the number of common words in context gives some similarity clue to a word and its translation, but the actual ranking of the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors. This idea is similar to Rapp (1999) who proposed to transform all co-occurrence vectors using log likelihood ratio instead of just using the frequency counts of the co-occurrences. These values are used to define whether the context words are highly associated with the W or not. Earlier work relies on a large bilingual dictionary as their seed lexicon (Rapp, 1999; Fung and Yee, 1998; among others). Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. Haghighi et al. (2008), amongst a few others, prop"
W09-1702,C98-1066,0,\N,Missing
W09-1702,2005.mtsummit-papers.11,0,\N,Missing
W09-1702,W01-0504,0,\N,Missing
W09-1705,S07-1002,0,0.516692,"and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of the target one. Recently, Klapaftis and"
W09-1705,S07-1075,0,0.336873,"and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of the target one. Recently, Klapaftis and"
W09-1705,W06-3812,0,0.110344,"n cnn nbc and cnn tv. Marginal frequencies of collocations are updated and the overall result is consequently a smoothing of relative frequencies. The weight applied to each edge connecting vertices i and j (collocations ci and cj ) is the maximum f req of their conditional probabilities: p(i|j) = f reqijj , where f reqi is the number of paragraphs collocation ci occurs. p(j|i) is defined similarly. Inducing senses and tagging In this final stage, the collocational graph is clustered to produced the senses (clusters) of the target word. The clustering method employed is Chinese Whispers (CW) (Biemann, 2006). CW is linear to the number of graph edges, while it offers the advantage that it does not require any input parameters, producing the clusters of a graph automatically. Figure 1: An example undirected weighted graph. Initially, CW assigns all vertices to different classes. Each vertex i is processed for a number of iterations and inherits the strongest class in its local neighbourhood (LN) in an update step. LN is defined as the set of vertices which share an edge with i. In each iteration for vertex i: each class, cl, receives a score equal to the sum of the weights of edges (i, j), where j"
W09-1705,E03-1020,0,0.0247544,"uffer from the lack of explicit semantic and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of t"
W09-1705,J93-1003,0,0.261466,"rc are PoS-tagged. In the next step, only nouns are kept in the paragraphs of bc, since they are characterised by higher discriminative ability than verbs, adverbs or adjectives which may appear in a variety of different contexts. At the end of this pre-processing step, each paragraph of bc and rc is a list of lemmatized nouns (Klapaftis and Manandhar, 2008). In the next step, the paragraphs of bc are filtered by removing common nouns which are noisy; contextually not related to tw. Given a contextual word cw that occurs in the paragraphs of bc, a log-likelihood ratio (G2 ) test is employed (Dunning, 1993), which checks if the distribution of cw in bc is similar to the distribution of cw in rc; p(cw|bc) = p(cw|rc) (null hypothesis). If this is true, G2 has a small value. If this value is less than a pre-specified threshold (parameter p1 ) the noun is removed from bc. 1 The British National Corpus (BNC) (2001, version 2). Distributed by Oxford University Computing Services. Target: cnn nbc nbc tv cnn tv cnn radio news newscast radio television cnn headline nbc politics breaking news Target: nbc news nbc tv soap opera nbc show news newscast nbc newshour cnn headline radio tv breaking news tex. Tw"
W09-1705,N06-2015,0,0.0175727,"of the aforementioned GCM allows the estimation of a different parameter setting for each target word. Table 3 shows the parameters of the collocational graph-based WSI system (Klapaftis and Manandhar, 2008). These parameters affect how the collocational graph is constructed, and in effect the quality of the induced clusters. 4 4.1 Evaluation Experimental setting The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). The corpus consists of text of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006). The evaluation focuses on all 35 nouns of SWSI. SWSI task employs two evaluation schemes. In unsupervised evaluation, the results are treated as clusters of contexts and gold standard (GS) senses as classes. In a perfect clustering solution, each induced cluster contains the same contexts as one of the classes (Homogeneity), and each class contains the same contexts as one of the clusters (Completeness). F-Score is used to assess the overall quality of clustering. Entropy and purity are also used, complementarily. F-Score is a better measure than entropy or purity, since F-Score measures bot"
W09-1705,N07-1032,0,0.0174634,"arameters either empirically or by employing supervised techniques. The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. This issue imposes limits on the unsupervised nature of these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI system on different datasets, one cannot be sure that the same set of parameters is appropriate for all datasets (Karakos et al., 2007). In most cases, a new parameter tuning might be necessary. Unsupervised estimation of free parameters may enhance the unsupervised nature of systems, making them applicable to any dataset, even if there are no tagged data available. In this paper, we focus on estimating the free parameters of the collocational graph-based WSI method (Klapaftis and Manandhar, 2008) using eight graph connectivity measures (GCM). Given a parameter setting and the associated induced clustering solution, each induced cluster corresponds to a subgraph of the original unclustered graph. A graph connectivity measure"
W09-1705,P95-1026,0,0.62514,"on for Computational Linguistics posed the use of a graph-based model for WSI, in which each vertex of the graph corresponds to a collocation (word-pair) that co-occurs with the target word, while edges are drawn based on the cooccurrence frequency of their associated collocations. Clustering of this collocational graph would produce clusters, which consist of a set of collocations. The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches, since collocations provide strong and consistent clues to the senses of a target word (Yarowsky, 1995). The collocational graph-based approach as well as the majority of state-of-the-art WSI systems estimate their parameters either empirically or by employing supervised techniques. The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. This issue imposes limits on the unsupervised nature of these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI s"
W09-1705,W07-0201,0,0.0315635,"per collocation property (Yarowsky, 1995), which means that WSD based on collocations is probably finer than WSD based on simple words, since ambiguity is reduced (Klapaftis and Manandhar, 2008). 3 Unsupervised parameter tuning In this section we investigate unsupervised ways to address the issue of choosing parameter values. To this end, we employ a variety of GCM, which measure the relative importance of each vertex and assess the overall connectivity of the corresponding graph. These measures are average degree, cluster coefficient, graph entropy and edge density (Navigli and Lapata, 2007; Zesch and Gurevych, 2007). GCM quantify the degree of connectivity of the produced clusters (subgraphs), which represent the 39 senses (uses) of the target word for a given clustering solution (parameter setting). Higher values of GCM indicate subgraphs (clusters) of higher connectivity. Given a parameter setting, the induced clustering solution and a graph connectivity measure GCMi , each induced cluster is assigned the resulting score of applying GCMi on the corresponding subgraph of the initial unclustered graph. Each clustering solution is assigned the average of the scores of its clusters (table 6), and the highe"
W09-2419,S07-1002,0,0.567407,"g word senses as a fixed-list of dictionary definitions. These limitations of hand-crafted lexicons include the use of general sense definitions, the lack of explicit semantic and topical relations between concepts (Agirre et al., 2001), and the inability to reflect the exact content of the context in which a target word appears (V´eronis, 2004). Given the significance of WSI, the objective assessment and comparison of WSI methods is crucial. The first effort to evaluate WSI methods under a common framework (evaluation schemes & 117 dataset) was undertaken in the SemEval-2007 WSI task (SWSI) (Agirre and Soroa, 2007), where two separate evaluation schemes were employed. The first one, unsupervised evaluation, treats the WSI results as clusters of target word contexts and Gold Standard (GS) senses as classes. The traditional clustering measure of F-Score (Zhao et al., 2005) is used to assess the performance of WSI systems. The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A s"
W09-2419,N06-2015,0,0.0379745,"he F-Scores of each GS sense (Formula 1), where q is the number of GS senses and N is the total number of target word in118 gs2 100 500 100 gs3 100 100 500 Table 1: Clusters & GS senses matrix. stances. If the clustering is identical to the original classes in the datasets, F-Score will be equal to one. In the example of Table 1, F-Score is equal to 0.714. SemEval-2007 WSI evaluation setting The SemEval-2007 WSI task (Agirre and Soroa, 2007) evaluates WSI systems on 35 nouns and 65 verbs. The corpus consists of texts of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006). For each target word tw, the task consists of firstly identifying the senses of tw (e.g. as clusters of target word instances, cooccurring words, etc.), and secondly tagging the instances of the target word using the automatically induced clusters. In the next sections, we describe and review the two evaluation schemes. 2.1 cl1 cl2 cl3 gs1 500 100 100 F − Score = q X |gsi | i=1 N F (gsi ) (1) As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness (Rosenberg and Hirschberg, 2007). Homogeneity refers t"
W09-2419,D07-1043,0,0.476914,"ults as clusters of target word contexts and Gold Standard (GS) senses as classes. The traditional clustering measure of F-Score (Zhao et al., 2005) is used to assess the performance of WSI systems. The second evaluation scheme, supervised evaluation, uses the training part of the dataset in order to map the automatically induced clusters to GS senses. In the next step, the testing corpus is used to measure the performance of systems in a Word Sense Disambiguation (WSD) setting. A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class (Rosenberg and Hirschberg, 2007). Moreover, F-Score might also fail to evaluate clusters which are not matched to any GS class due to their small size. These two limitations define the matching problem of F-Score (Rosenberg and Hirschberg, 2007) which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. The supervised evaluation scheme employs a method in order to map the automatically induced clusters to GS senses. As a result, this process might change the distribution of clusters by mapping more than one clusters to the same GS sense. The outcom"
W11-1310,W03-1812,0,0.465554,"im(Vw1 ⊕w2 , Vw1 w2 ) &gt; γ, the compound is classified as compositional, where γ is a threshold for deciding compositionality. Global values of a and b were chosen by optimizing the performance on the development set. It was found that no single threshold value γ held for all compounds. Changing the threshold alters performance arbitrarily. This might be due to the polysemous nature of the constituent words which makes the composed vector Vw1 ⊕w2 filled with noisy contexts and thus making the judgement unpredictable. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003). They also observe similar behaviour of the threshold γ. We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling. The above models use a simple addition based compositionality function. Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors. In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication. Bannard et al. (2003); McCar"
W11-1310,W03-1809,0,0.532911,"Missing"
W11-1310,W11-1304,0,0.520526,"s.york.ac.uk diana@dianamccarthy.co.uk Suresh Manandhar University of York, UK Spandana Gella University of York, UK suresh@cs.york.ac.uk spandana@cs.york.ac.uk Abstract In this paper, we highlight the problems of polysemy in word space models of compositionality detection. Most models represent each word as a single prototype-based vector without addressing polysemy. We propose an exemplar-based model which is designed to handle polysemy. This model is tested for compositionality detection and it is found to outperform existing prototype-based models. We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations. 1 Introduction In the field of computational semantics, to represent the meaning of a compound word, two mechanisms are commonly used. One is based on the distributional hypothesis (Harris, 1954) and the other is on the principle of semantic compositionality (Partee, 1995, p. 313). The distributional hypothesis (DH) states that words that occur in similar contexts tend to have similar meanings. Using this hypothesis, distributional models like the Word-space model (WSM, Sahl"
W11-1310,P10-2017,0,0.0416216,"Missing"
W11-1310,W06-1203,0,0.555699,"s as a PSC-based vector. So a PSC-based is composed of component DH-based vectors. Both of these two mechanisms are capable of determining the meaning vector of a compound word. For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same. However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001). The DH-based and PSC -based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds. Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances. Such a representation is called the prototype-based modelling (Murphy, 2002). These prototype-based vectors do not 54 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60, c Portland, Oregon, 24 June 2011. 2011 Association for"
W11-1310,W03-1810,1,0.953329,"Missing"
W11-1310,P08-1028,0,0.553899,"attern using corpus query language. Let w1 w2 be a compound word with constituent words w1 and w2 . Ew denotes the set of exemplars of w. Vw is the prototype vector of the word w, which is built by merging all the exemplars in Ew 1 Sketch Engine http://www.sketchengine.co.uk 55 For the purposes of producing a PSC-based vector for a compound, a vector of a constituent word is built using only the exemplars which do not contain the compound. Note that the vectors are sensitive to a compound’s word-order since the exemplars of w1 w2 are not the same as w2 w1 . We use other WSM settings following Mitchell and Lapata (2008). The dimensions of the WSM are the top 2000 content words in the given corpus (along with their coarse-grained part-of-speech information). Cosine similarity (sim) is used to measure the similarity between two vectors. Values at the specific positions in the vector representing context words are set to the ratio of the probability of the context word given the target word to the overall probability of the context word. The context window of a target word’s exemplar is the whole sentence of the target word excluding the target word. Our language of interest is English. We use the ukWaC corpus"
W11-1310,N10-1013,0,0.0369443,"ionality behaviour of phrases. We therefore also use evidence from the similarities between each constituent word and the compound. 4 Our Approach: Exemplar-based Model Our approach works as follows. Firstly, given a compound w1 w2 , we build its DH-based prototype vector Vw1 w2 from all its exemplars Ew1 w2 . Secondly, we remove irrelevant exemplars in Ew1 and Ew2 of constituent words and build the refined prototype vectors Vw1r and Vw2r of the constituent words w1 and w2 respectively. These refined vectors are used to compose the PSC-based vectors 2 of the compound. Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. This work does not relate to compositionality but to measuring semantic similarity of single words. As such, their clusters are not influenced by other words whereas in our approach for detecting compositionality, the other constituent word plays a major role. We use the compositionality functions, simple addition and simple multiplication to build Vw1r +w2r and Vw1r ×w2r respectively. Based on the similarities sim(Vw1 w2 , Vw1r ), sim(Vw1 w2 , Vw2r ), sim(Vw1 w2 , Vw1r +w2r ) and sim(Vw1 w2 , Vw1r ×w2r ), we"
W11-1310,W11-0115,0,0.0621038,"he constituent words which makes the composed vector Vw1 ⊕w2 filled with noisy contexts and thus making the judgement unpredictable. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003). They also observe similar behaviour of the threshold γ. We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling. The above models use a simple addition based compositionality function. Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors. In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication. Bannard et al. (2003); McCarthy et al. (2003) observed that methods based on distributional similarities between a phrase and its constituent words help when determining the compositionality behaviour of phrases. We therefore also use evidence from the similarities between each constituent word and the compound. 4 Our Approach: Exemplar-based Model Our approach works as follows. Firstly, given a compound w1"
W11-1310,P07-2011,0,0.060728,"Missing"
W11-1310,W01-0513,0,0.791299,"a PSC-based is composed of component DH-based vectors. Both of these two mechanisms are capable of determining the meaning vector of a compound word. For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same. However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001). The DH-based and PSC -based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds. Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances. Such a representation is called the prototype-based modelling (Murphy, 2002). These prototype-based vectors do not 54 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics di"
W99-0909,P83-1019,0,0.260424,"Missing"
W99-0909,J93-2004,0,0.0335506,"Missing"
