2020.aacl-main.36,E06-1040,0,0.0581807,"a lower self-BLEU score means that the samples have better diversity. In our experiments, we feed the first ten subwords of every sample from test set to the model, and compare the model-generated sequences to the reference samples in the validation set. We use 10,000 samples to compute corpus-BLEU or selfBLEU, i.e., |Sgen |= |Sref |= 10, 000. Automatic evaluation enables us to do a finegrained sweep of the hyperparameters for each sampling algorithm, and compare them in the qualitydiversity trade-off. However, observations from automatic evaluation could be misaligned with human evaluation (Belz and Reiter, 2006). Therefore, we confirm our key observations with human evaluation. 4.1.2 Human Evaluation Quality We ask a pool of 602 crowdworkers on Amazon Mechanical Turk to evaluate various sampling configurations in the quality aspect. Each worker is presented a set of ten samples along with the prompts (prefixes). They are then asked to rate how likely the sentence would appear in a news article between 0 and 5 (Invalid, Confusing, Unspecific, Average, Expected, and Very Expected respectively). We focus on the Gigaword dataset for human evaluation since news articles are ubiquitous and do not often req"
2020.aacl-main.36,P18-1082,0,0.175591,"translation, summarization, image captioning, and other subfields. However, in the task of open-ended language generation (which is the focus of this work), a significant degree of diversity is required. For example, conditioned on the prompt “The news says that ...”, the LM is expected to be able to generate a wide range of interesting continuations. While the deterministic behavior of decoding algorithms could give high-quality samples, they suffer from a serious lack of diversity. This need for diversity gives rise to a wide adoption of various sampling algorithms. Notably, topk sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and tempered sampling (Caccia et al., 2020) have been used in open-ended 334 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 334–346 c December 4 - 7, 2020. 2020 Association for Computational Linguistics generation (Radford et al., 2018; Caccia et al., 2020), story generation (Fan et al., 2018), and dialogue response generation (Zhang et al., 2020b). However, the sampling algorithm and the hyperparameter are"
2020.aacl-main.36,2020.acl-main.164,0,0.34324,"Missing"
2020.aacl-main.36,P19-1365,0,0.123812,"py algorithms, tend to be less semantically and syntatically coherent. In particular, the target entropy sampling algorithm, which obtains the lowest quality score measured by corpusBLEU, lacks basic language structure. In comparison to target entropy, noised top-k is syntatically coherent, but exhibits logical and factual inconsistencies. These observations aligns with the results we get from automatic evaluation. 6 Related Works Despite the popularity of sampling algorithms in natural language generation, a rigorous comparison or scrutiny of existing algorithms is lacking in the literature. Ippolito et al. (2019) provides a comparison between sampling and decoding algorithms. Holtzman et al. (2020) proposes nucleus sampling, and compare it with top-k sampling (Fan et al., 2018). However, only a few hyperparameter configurations are tested. In Hashimoto et al. (2019) and Caccia et al. (2020), temperature sampling is used and the hyperparameter T is tuned to trade-off between diversity and quality, but it lacks comparisons with other sampling algorithms. Welleck et al. (2020) studies the consistency of existing sampling and decoding algorithms, without comparing the generation performance. In this work"
2020.aacl-main.36,D17-1230,0,0.0190267,"formance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfies these properties performs on par with the existing sampling algorithms.1 1 Figure 1: Human evaluation (y-axis: quality, x-axis: diversity, both are the bigger the better) shows that the generation performance of existing sampling algorithms are on par with each other. Introduction A language model (LM) is a central module for natural language generation (NLG) tasks (Young et al., 2018) such as machine translation (Wu et al., 2018), dialogue response generation (Li et al., 2017), image captioning (Lin et al.), and related tasks. Given a trained LM, finding the best way to generate a sample from it has been an important challenge for NLG applications. ∗ Equal contribution. Our data and code are available https://github.com/moinnadeem/ characterizing-sampling-algorithms. 1 at Decoding, i.e., finding the most probable output sequence from a trained model, is a natural principle for generation. The beam-search decoding algorithm approximately finds the most likely sequence by performing breadth-first search over a restricted search space. It has achieved success in machi"
2020.aacl-main.36,W12-3018,0,0.0159211,"Missing"
2020.aacl-main.36,P02-1040,0,0.106858,"trics, with one pair using automatic evaluation and the other using human evaluation. In the next two sections, we describe the metrics we use, and refer readers to Caccia et al. (2020) for more intuition behind the Q-D trade-off. 4.1.1 Automatic Evaluation For automatic metrics, we adopt the corpus-BLEU (Yu et al., 2016) metric to measure quality and the self-BLEU (Zhu et al., 2018) metric to measure diversity. We formulate them below. Given a batch of generated sentences Sgen and a batch of sentences from ground-truth data as references Sref , corpus-BLEU returns the average 337 BLEU score (Papineni et al., 2002) of every model generated sentence against the reference set: corpus-BLEU(Sgen , Sref ) = of the rating. Therefore, we estimate the human judgement score for a sample as the average rating of the 20 crowdworkers (out of 25) who took the most time to rate the samples. We provide further details about our setup in Appendix C and D. X 1 BLEU(W, Sref ). |Sgen |W ∈S gen (11) Diversity It is difficult for human annotators to estimate diversity of text (Hashimoto et al., 2019). Therefore, we use the n-gram entropy metric (Zhang et al., 2018; He and Glass, 2019) . Given Sgen which contains a large num"
2020.aacl-main.36,P16-1162,0,0.00548779,"we rearrange the elements such that if i &lt; j → pi &gt;= pj .2 We list the transformations and their intuition below: Definition 2.1. (Top-k) In top-k sampling, we only sample from the top K tokens: pˆi = Autoregressive Language Modeling The task of autoregressive language modeling is to learn the probability distribution of the (l + 1)-th word Wl+1 in a sentence W conditioned on the word history W1:l := (W1 , . . . , Wl ) and context C. Here, we use Wi ∈ V to denote a discrete random variable distributed across a fixed vocabulary V . In this work, the vocabulary is constructed on subword level (Sennrich et al., 2016). Given a training set D, maximum likelihood es1 |D| pi · 1{i ≤ K} , PK j=1 pj (2) Definition 2.2. (Nucleus) With a hyperparameter P (0 &lt; P ≤ 1), in nucleus sampling, we sample from the top-P mass of p: p0 pˆi = P|V i| 0 j=1 pj , Pi−1 where p0i = pi · 1{ j=1 pj &lt; P }. 335 2 The token indexes are also permutated accordingly. (3) Definition 2.3. (Tempered) In tempered sampling, the log probabilities are scaled by T1 : exp(log(pi )/T ) . pˆi = P|V | j=1 exp(log(pj )/T ) Property 3. (Slope Preservation): The “slope” of the distribution is preserved. Formally, ∀ˆ pi &gt; pˆj &gt; pˆk &gt; 0 (i.e., they are"
2020.aacl-main.36,2020.emnlp-main.448,1,0.7212,"lgorithms in natural language generation, a rigorous comparison or scrutiny of existing algorithms is lacking in the literature. Ippolito et al. (2019) provides a comparison between sampling and decoding algorithms. Holtzman et al. (2020) proposes nucleus sampling, and compare it with top-k sampling (Fan et al., 2018). However, only a few hyperparameter configurations are tested. In Hashimoto et al. (2019) and Caccia et al. (2020), temperature sampling is used and the hyperparameter T is tuned to trade-off between diversity and quality, but it lacks comparisons with other sampling algorithms. Welleck et al. (2020) studies the consistency of existing sampling and decoding algorithms, without comparing the generation performance. In this work we mainly use the quality-diversity trade-off (Caccia et al., 2020) to conduct a comparison of different sampling algorithms. Parallel to our work, Zhang et al. (2020a) also uses the qualitydiversity trade-off to compare top-k, nucleus, and tempered sampling. Their observation is similar to ours: The performance of the existing algorithms are close with no significant gap. More importantly, the underlying reasons for the success of various sampling algorithms remain"
2020.aacl-main.36,2020.acl-demos.30,0,0.0885999,"option of various sampling algorithms. Notably, topk sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), and tempered sampling (Caccia et al., 2020) have been used in open-ended 334 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 334–346 c December 4 - 7, 2020. 2020 Association for Computational Linguistics generation (Radford et al., 2018; Caccia et al., 2020), story generation (Fan et al., 2018), and dialogue response generation (Zhang et al., 2020b). However, the sampling algorithm and the hyperparameter are usually chosen via heuristics, and a comprehensive comparison between existing sampling algorithm is lacking in the literature. More importantly, the underlying reasons behind the success of the existing sampling algorithms still remains poorly understood. In this work, we begin by using the qualitydiversity (Q-D) trade-off (Caccia et al., 2020) to compare the three existing sampling algorithms. For automatic metrics, we use the BLEU score for quality and n-gram entropy for diversity. We also correlate these automatic metrics with"
2020.acl-main.185,D14-1181,0,0.00496475,"Missing"
2020.acl-main.185,N16-1014,0,0.371007,"ion End-to-end dialogue response generation can be formulated as a sequence-to-sequence (seq2seq) task: given a dialogue context, the model is asked to generate a high-quality response. In recent years, deep learning models, especially seq2seq language generation models (Sutskever et al., 2014; Cho et al., 2014), have brought significant progress to the field of dialogue response generation. However, recent research has revealed undesirable behaviors of seq2seq models that are side effects of standard maximum likelihood estimation (MLE) training, such as the generic (boring) response problem (Li et al., 2016), vulnerability to adversarial attacks (Cheng et al., 2018; Belinkov and Bisk, 2017), and the malicious (egregious) response problem (He and Glass, 2019). In this work, we propose and explore the negative training framework to correct unwanted behaviors of a dialogue response generator. During negative training, we first find or identify input-output pairs for a trained seq2seq model that exhibit some undesirable generation behavior, treat them as “bad examples,” and use them to feed negative training signals to the model. Correspondingly, we regard the training data as “good examples” and sta"
2020.acl-main.185,D17-1230,0,0.31323,"egative training (it appears in every negative example). So, we apply the same frequent word avoiding (FWA) technique used in Section 4.2, except that here only the negative gradient for &lt;EOS&gt; is scaled by 0.110 . In addition to the baseline model, we compare our proposed negative training framework against a 10 We find that scal by zero will result in extremely short responses. D = min max{E(x,y)∼Pdata log D(x, y)+ G D (6) Ex∼Pdata ,y∼G(·|x) log(1 − D(x, y))} where the generator G refers to the seq2seq model Pθ . The GAN framework is very attractive for tackling the generic response problem (Li et al., 2017; Zhang et al., 2018), because the discriminator can act as a critic to judge whether a response sample is boring. We describe the training details and hyper-parameter setting for the GAN approach in Appendix E. We also provide an comparison to the MMI decoding (Li et al., 2016), which is a very popular work in this field. We implement MMI-antiLM for our models. The experimental results are shown in Table 4. The experiment with best diversity result and nondegenerate sample quality are shown in bold. We first observe a large gap on the diversity measures between the baseline models and the tes"
2020.acl-main.185,D16-1230,0,0.0771002,"Missing"
2020.acl-main.185,W15-4640,0,0.0834069,"Missing"
2020.acl-main.185,D15-1166,0,0.334942,"and the frequent response problem (to be described in Section 3.2 and 3.3) in open-domain dialogue response generation. In our experiments, we show that negative training can significantly reduce the hit rate for malicious responses, or discourage frequent responses and greatly improve response diversity. 2 Model Formulation In this work we adopt recurrent neural network (RNN) based encoder-decoder seq2seq models (Sutskever et al., 2014; Cho et al., 2014; Mikolov et al., 2010), which are widely used in NLP applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. We use x = {x1 , x2 , ..., xn } to denote onehot vector representations of the input sequence, which serves as context or history information (e.g. the previous utterance), y = {y1 , y2 , ..., ym }1 to denote scalar indices of the corresponding reference target sequence, and V as the vocabulary. We use θ to represent the parameters for the seq2seq 1 The last word ym is a &lt;EOS&gt; token which indicates the end of a sentence. 2044 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2044–2058 c July 5 - 10, 2020. 2020 Association for Computational Li"
2020.acl-main.213,W14-4012,0,0.0323379,"Missing"
2020.acl-main.213,L18-1001,0,0.0575128,"Missing"
2020.acl-main.213,H92-1073,0,0.307784,"with our approach contain richer phonetic content than the original representations, and achieve better performance on speech recognition and speech translation. 4.4 References Speech recognition and translation The above phonetic classification experiments are meant for analyzing the phonetic properties of a representation. Finally, we apply the representations learned by Lm to automatic speech recognition (ASR) and speech translation (ST) and show their superiority over those learned by Lf . We follow the exact setup in Chung and Glass (2020). For ASR, we use the Wall Street Journal corpus (Paul and Baker, 1992), use si284 for training, and report the word error rate (WER) on dev93. For ST, we use the LibriSpeech En-Fr corpus (Kocabiyikoglu et al., 2018), which aims to translate an English speech to a French text, and report the BLEU score (Papineni et al., 2002). For both tasks, the downstream model is an end-to-end, sequenceto-sequence RNN with attention (Chorowski et al., 2015). We compare different input features to the same model. Results, shown in Table 2, demonstrate that the improvement in predictive coding brought by Lr not only provides representations that contain richer phonetic content,"
2020.acl-main.213,P02-1040,0,0.116802,"Missing"
2020.acl-main.308,K19-1096,1,0.888577,"unlike the articles, the number of media profiles is too small to fine-tune BERT, and (ii) most Twitter descriptions have sentence-like structure and length. If a medium has no Twitter account, we used a vector of zeros. 3.2 Who Read it We argue that the audience of a news medium can be indicative of the political orientation of that medium. We thus propose a number of features to model this, which we describe below. 3.2.1 Twitter Followers Bio Previous research has used the followers’ networks and the retweeting behavior in order to infer the political bias of news media (Wong et al., 2013; Atanasov et al., 2019; Darwish et al., 2020). Here, we analyze the self-description (bio) of Twitter users that follow the target news medium. The assumption is that (i) followers would likely agree with the news medium’s bias, and (ii) they might express their own bias in their self-description. 3367 3.3 What Was Written About the Target Medium We retrieved the public profiles of 5,000 followers for each target news medium with a Twitter account, and we excluded those with non-English bios since our dataset is mostly about US media. Then, we encoded each follower’s bio using SBERT (Reimers and Gurevych, 2019). As"
2020.acl-main.308,D18-1389,1,0.0716118,"ns for future work. 2 Related Work While leveraging social information and temporal structure to predict the factuality of reporting of a news medium is not new (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016), modeling this at the medium level is a mostly unexplored problem. A popular approach to predict the factuality of a medium is to check the general stance of that medium concerning already fact-checked claims (Mukherjee and Weikum, 2015; Popat et al., 2017, 2018). Therefore, stance detection became an essential component in fact-checking systems (Baly et al., 2018b). In political science, media profiling is essential for understanding media choice (Iyengar and Hahn, 2009), voting behavior (DellaVigna and Kaplan, 2007), and polarization (Graber and Dunaway, 2017). The outlet-level bias is measured as a similarity of the language used in news media to political speeches of congressional Republicans or Democrats, also used to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018). Political bias was tradi"
2020.acl-main.308,N19-1216,1,0.833934,"y introducing new sources, mostly related to the social media context, thus achieving sizable improvements on the same dataset. 3365 Figure 1: The architecture of our system for predicting the political bias and the factuality of reporting of news media. The features inside {curly brackets} are calculated at a finer level of granularity and are then aggregated at the medium level. The upper gray box shows the resources used to generate features, e.g., the OpenSmile toolkit is used to extract low-level descriptors (LLD) from YouTube videos; see Section 3 for further details. In follow-up work (Baly et al., 2019), we showed that jointly predicting the political bias and the factuality is beneficial, compared to predicting each of them independently. We used the same sources of information as in (Baly et al., 2018a), but the results were slightly lower. While here we focus on analyzing political bias and factuality separately, future work may analyze how the newly proposed features and sources affect the joint prediction. 3 System and Features In this section, we present our system. For each target medium, it extracts a variety of features to model (i) what was written by the medium, (ii) the audience"
2020.acl-main.308,N18-2004,1,0.915404,"Missing"
2020.acl-main.308,D19-1565,1,0.908899,"Missing"
2020.acl-main.308,N19-1423,0,0.00673533,"veraged the NELA features for the individual articles in order to obtain a NELA representation for a news medium. Using arithmetic averaging is a good idea as it captures the general trend of articles in a medium, while limiting the impact of outliers. For instance, if a medium is known to align with left-wing ideology, this should not change if it published a few articles that align with right-wing ideology. We use this method to aggregate all features that we collected at a level of granularity that is finer than the medium-level. 3366 Embedding Features: We encoded each article using BERT (Devlin et al., 2019) by feeding the first 510 WordPieces2 from the article3 and then averaging the word representations extracted from the second-to-last layer.4 In order to obtain representations that are relevant to our tasks, we finetuned BERT by training a softmax layer on top of the [CLS] output vector to predict the label (bias or factuality) of news articles that are scrapped from an external list of media to avoid overfitting. The articles’ labels are assumed to be the same as those of the media in which they are published (a form of distant supervision). This is common practice in tasks such as “fake new"
2020.acl-main.308,D18-1388,0,0.0668289,"to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018). Political bias was traditionally used as a feature for fact verification (Horne et al., 2018b). In terms of modeling, Horne et al. (2018a) focused on predicting whether an article is biased or not. Political bias prediction was explored by Potthast et al. (2018) and Saleh et al. (2019), where news articles were modeled as left vs. right, or as hyperpartisan vs. mainstream. Similarly, Kulkarni et al. (2018) explored the left vs. right bias at the article level, modeling both textual and URL contents of articles. In our earlier research (Baly et al., 2018a), we analyzed both the political bias and the factuality of news media. We extracted features from several sources of information, including articles published by each medium, what is said about it on Wikipedia, metadata from its Twitter profile, in addition to some web features (URL structure and traffic information). The experiments on the Media Bias/Fact Check (MBFC) dataset showed that combining features from these different sources of info"
2020.acl-main.308,N18-1070,1,0.908968,"Missing"
2020.acl-main.308,D19-1410,0,0.020435,"l., 2013; Atanasov et al., 2019; Darwish et al., 2020). Here, we analyze the self-description (bio) of Twitter users that follow the target news medium. The assumption is that (i) followers would likely agree with the news medium’s bias, and (ii) they might express their own bias in their self-description. 3367 3.3 What Was Written About the Target Medium We retrieved the public profiles of 5,000 followers for each target news medium with a Twitter account, and we excluded those with non-English bios since our dataset is mostly about US media. Then, we encoded each follower’s bio using SBERT (Reimers and Gurevych, 2019). As we had plenty of followers’ bios, this time fine-tuning BERT would have been feasible. However, we were afraid to use distant supervision for labeling as we did with the articles since people sometimes follow media with different political ideologies. Thus, we opted for SBERT, and we averaged the SBERT representations across the bios in order to obtain a medium-level representation. Wikipedia contents describing news media were useful for predicting the political bias and the factuality of these media (Baly et al., 2018a). We automatically retrieved the Wikipedia page for each medium, and"
2020.acl-main.308,S19-2182,1,0.898507,"Missing"
2020.acl-main.308,2020.acl-main.332,1,0.876076,"Missing"
2020.acl-main.308,2020.acl-main.50,1,0.52838,"of the Facebook audience, and information from the profiles of the media followers on Twitter. We further modeled different modalities: text, metadata, and speech signal. The evaluation results have shown that while what was written matters most, the social media context is also important as it is complementary, and putting them all together yields sizable improvements over the state of the art. In future work, we plan to perform user profiling with respect to polarizing topics such as gun control (Darwish et al., 2020), which can then be propagated from users to media (Atanasov et al., 2019; Stefanov et al., 2020). We further want to model the network structure, e.g., using graph embeddings (Darwish et al., 2020). Another research direction is to profile media based on their stance with respect to previously fact-checked claims (Mohtarami et al., 2018; Shaar et al., 2020), or by the proportion and type of propaganda techniques they use (Da San Martino et al., 2019, 2020). Finally, we plan to experiment with other languages. Acknowledgments This research is part of the Tanbih project9 , which aims to limit the effect of “fake news,” propaganda and media bias by making users aware of what they are readin"
2020.acl-main.308,P18-1022,0,0.0863147,"evel bias is measured as a similarity of the language used in news media to political speeches of congressional Republicans or Democrats, also used to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018). Political bias was traditionally used as a feature for fact verification (Horne et al., 2018b). In terms of modeling, Horne et al. (2018a) focused on predicting whether an article is biased or not. Political bias prediction was explored by Potthast et al. (2018) and Saleh et al. (2019), where news articles were modeled as left vs. right, or as hyperpartisan vs. mainstream. Similarly, Kulkarni et al. (2018) explored the left vs. right bias at the article level, modeling both textual and URL contents of articles. In our earlier research (Baly et al., 2018a), we analyzed both the political bias and the factuality of news media. We extracted features from several sources of information, including articles published by each medium, what is said about it on Wikipedia, metadata from its Twitter profile, in addition to some web features (URL structure and tr"
2020.acl-main.422,W19-4814,0,0.0311576,"a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dalvi et al., 2019; Baan et al., 2019). Our study contributes to this line of work by designing measures of localization and distributivity of information in a collection of models. Such measures may facilitate incorporating neuron interactions in new training objectives (Li et al., 2020). 3 Similarity Measures We present five groups of similarity measures, each capturing a different similarity notion. Consider a collection of M models {f (m) }M m=1 , yielding (m) word representations hl and potentially atten(m) tion weights αl at each layer l. Let k index neu(m) (m) (m) rons hl [k] or attention heads αl [k]. hl [k], (m) αl [k] ar"
2020.acl-main.422,D19-1445,0,0.0290966,"sk finetuning setup. In contrast, in XLNet, fine-tuning on any task leads to top layers being very different from all layers of models fine-tuned on other tasks. This suggests that XLNet representations become very task-specific, and thus multi-task fine-tuning may be less effective with XLNet than with BERT. Observing the attnsim similarity based on Jensen–Shannon divergence for base and fine-tuned models (Figure 6), we again see that top layers have lower similarities, implying that they undergo greater changed during fine-tuning. Other attentionbased measures behaved similarly (not shown). Kovaleva et al. (2019) made a similar observation by comparing the cosine similarity of attention matrices in BERT, although they did not perform crosstask comparisons. In fact, the diagonals within each block indicate that bottom layers remain similar to one another even when fine-tuning on different tasks, while top layers diverge after finetuning. The vertical bands at layers 0 mean that many higher layers have a head that is very similar to a head from the first layer, that is, a form of redundancy, which can explain why many heads can be pruned (Michel et al., 2019; Voita et al., 2019b; Kovaleva et al., 2019)."
2020.acl-main.422,N19-1002,0,0.0433694,"rt-ofspeech tagger (Saphra and Lopez, 2019). Our work adopts a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dalvi et al., 2019; Baan et al., 2019). Our study contributes to this line of work by designing measures of localization and distributivity of information in a collection of models. Such measures may facilitate incorporating neuron interactions in new training objectives (Li et al., 2020). 3 Similarity Measures We present five groups of similarity measures, each capturing a different similarity notion. Consider a collection of M models {f (m) }M m=1 , yielding (m) word representations hl and potentially atten(m) tion weights αl at each layer l. Let k index neu(m) (m) (m) ro"
2020.acl-main.422,N19-1112,1,0.849968,"s ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have led to impressive improvements in a variety of tasks. With this progress in breaking the state of the art, interest in the community has expanded to analyzing such models in an effort to illuminate their inner workings. A number of studies have analyzed the internal representations in such models and attempted to assess what linguistic properties they capture. A prominent methodology for this is to train supervised classifiers based on the models’ learned representations, and predict various linguistic properties. For instance, Liu et al. (2019a) train such classifiers on 16 linguistic tasks, including part-of-speech tagging, chunking, named ∗ Equal contribution The code is available at https://github.com/ johnmwu/contextual-corr-analysis. 1 entity recognition, and others. Such an approach may reveal how well representations from different models, and model layers, capture different properties. This approach, known as analysis by probing classifiers, has been used in numerous other studies (Belinkov and Glass, 2019). While the above approach yields compelling insights, its applicability is constrained by the availability of linguist"
2020.acl-main.422,2021.ccl-1.108,0,0.092394,"Missing"
2020.acl-main.422,J93-2004,0,0.0700399,"er-equivalent variant (Peters et al., 2018b). GPT variants We use both the original OpenAI Transformer (GPT; Radford et al. 2018) and its successor GPT2 (Radford et al., 2019), in the small and medium model sizes. These are all unidirectional Transformer LMs. BERT We use BERT-base/large (12/24 layers; Devlin et al. 2019): Transformer LMs trained with a masked LM objective function.6 XLNet We use XLNet-base/large (12/24 layers; Yang et al. 2019). Both are Transformer LM with a permutation-based objective function. Data For analyzing the models, we run them on the Penn Treebank development set (Marcus et al., 1993), following the setup taken by Liu et al. (2019a) in their probing classifier experiments.7 We collect representations and attention weights from each layer in each model for computing the similarity measures. We obtain representations for models used in Liu et al. (2019a) from their implementation and use the transformers library (Wolf et al., 2019) to extract other representations. We aggregate sub-word representations by taking the representation of the last sub-word, following Liu et al. (2019a), and sub-word attentions by summing up at6 BERT is also trained with a next sentence prediction"
2020.acl-main.422,D18-1179,0,0.154589,"the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.1 1 Introduction Contextual word representations such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have led to impressive improvements in a variety of tasks. With this progress in breaking the state of the art, interest in the community has expanded to analyzing such models in an effort to illuminate their inner workings. A number of studies have analyzed the internal representations in such models and attempted to assess what linguistic properties they capture. A prominent methodology for this is to train supervised classifiers based on the models’ learned representations, and predict various linguistic properties. For instance, Liu et al. (2019a) train su"
2020.acl-main.422,D16-1079,0,0.0415815,"Missing"
2020.acl-main.422,D16-1264,0,0.0565186,"d Smith, 2019; Brunner et al., 2020). However, characterizing the effect of such concerns on our attention-based similarity measures is beyond the current scope. 6 Similarity of Fine-tuned Models How does fine-tuning on downstream tasks affect model similarity? In this section, we compare pretrained models and their fine-tuned versions. We use four of the GLUE tasks (Wang et al., 2019): MNLI A multi-genre natural language inference dataset (Williams et al., 2018), where the task is to predict whether a premise entails a hypothesis. QNLI A conversion of the Stanford question answering dataset (Rajpurkar et al., 2016), where the task is to determine whether a sentence contains the answer to a question. QQP A collection of question pairs from the Quora website, where the task is to determine whether two questions are semantically equivalent. SST-2 A binary sentiment analysis task using the Stanford sentiment treebank (Socher et al., 2013). 6.1 Results Top layers are more affected by fine-tuning Figure 5 shows representation-level ckasim similarity heatmaps of pre-trained (not fine-tuned) and fine-tuned versions of BERT and XLNet. The most striking pattern is that the top layers are more affected by fine-tun"
2020.acl-main.422,N19-1329,0,0.296007,"ies between model representations. Bau et al. (2019) used this approach to analyze the role of individual neurons in neural machine translation. They found that individual neurons are important and interpretable. However, their work was limited to a certain kind of architecture (specifically, a recurrent one). In contrast, we compare models of various architectures and objective functions. Other work used similarity measures to study learning dynamics in language models by comparing checkpoints of recurrent language models (Morcos et al., 2018), or a language model and a part-ofspeech tagger (Saphra and Lopez, 2019). Our work adopts a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dal"
2020.acl-main.422,P19-1282,0,0.0196114,"RT-base and middle layers of BERT-large. This parallels the findings from comparing representations of XLNet and BERT, which we conjecture is the result of the permutation-based objective in XLNet. In general, we find the attention-based similarities to be mostly in line with the neuron- and representation-level similarities. Nevertheless, they appear to be harder to interpret, as fine-grained patterns are less noticeable. One might mention in this context concerns regarding the reliability of attention weights for interpreting the importance of input words in a model (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2020). However, characterizing the effect of such concerns on our attention-based similarity measures is beyond the current scope. 6 Similarity of Fine-tuned Models How does fine-tuning on downstream tasks affect model similarity? In this section, we compare pretrained models and their fine-tuned versions. We use four of the GLUE tasks (Wang et al., 2019): MNLI A multi-genre natural language inference dataset (Williams et al., 2018), where the task is to predict whether a premise entails a hypothesis. QNLI A conversion of the Stanford question answering dataset (Rajpurkar et"
2020.acl-main.422,D16-1248,0,0.0236354,"al., 2018), or a language model and a part-ofspeech tagger (Saphra and Lopez, 2019). Our work adopts a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dalvi et al., 2019; Baan et al., 2019). Our study contributes to this line of work by designing measures of localization and distributivity of information in a collection of models. Such measures may facilitate incorporating neuron interactions in new training objectives (Li et al., 2020). 3 Similarity Measures We present five groups of similarity measures, each capturing a different similarity notion. Consider a collection of M models {f (m) }M m=1 , yielding (m) word representations hl and potentially atten(m) tion weights αl at"
2020.acl-main.422,silveira-etal-2014-gold,0,0.0507562,"Missing"
2020.acl-main.422,D13-1170,0,0.0108017,"Missing"
2020.acl-main.422,P19-1452,0,0.0346209,"layers. In effect, we take the column-wise mean of each heatmap. We do this separately for svsim as the distributed measure and neuronsim as the localized measure, and we subtract the svsim means from the neuronsim means. This results in a measure of localization per layer. Figure 3 shows the results. In all models, the localization score mostly increases with layers, indicating that information tends to become more localized at higher layers.12 This pattern is quite consistent, but may be surprising given prior observations on lower layers capturing phenomena that operate at a local context (Tenney et al., 2019), which presumably require fewer neurons. However, this pattern is in line with observations made by Ethayarajh (2019), who reported that upper layers of pre-trained models produce more context-specific representations. There appears to be a correspondence between our localization score and Ethayarajh’s context-specificity score, which is based on the cosine similarity of representations of the same word in different contexts. Thus, more localized representations are also more context-specific. A direct comparison between context-specificity and localization may be fruitful avenue for future w"
2020.acl-main.422,D19-1448,0,0.380748,"also experimented with the RBF variant, which is computationally demanding. We found similar patterns in preliminary experiments, so we focus on the linear variant. has been used to analyze neural network representations (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Chrupała, 2019), or other variants of CCA, such as deep CCA (Andrew et al., 2013). We leave the explorations of such measures to future work. 3.4 Attention-level similarity Previous work analyzing network similarity has mostly focused on representation-based similarities (Morcos et al., 2018; Saphra and Lopez, 2019; Voita et al., 2019a). Here we consider similarity based on attention weights in Transformer models. Analogous to a neuron-level similarity measure, an attention-level similarity measure finds the most “correlated” other attention head. We consider three methods to correlate heads, based on the norm of (m) (m0 ) two attention matrices αl [k], αl0 [k 0 ], their Pearson correlation, and their Jensen–Shannon divergence.5 We then average over heads k in layer l, as before. These measures are similar to neuronsim in that they emphasize localization of information—if two layers have pairs of heads that are very simila"
2020.acl-main.422,P19-1580,0,0.383097,"also experimented with the RBF variant, which is computationally demanding. We found similar patterns in preliminary experiments, so we focus on the linear variant. has been used to analyze neural network representations (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Chrupała, 2019), or other variants of CCA, such as deep CCA (Andrew et al., 2013). We leave the explorations of such measures to future work. 3.4 Attention-level similarity Previous work analyzing network similarity has mostly focused on representation-based similarities (Morcos et al., 2018; Saphra and Lopez, 2019; Voita et al., 2019a). Here we consider similarity based on attention weights in Transformer models. Analogous to a neuron-level similarity measure, an attention-level similarity measure finds the most “correlated” other attention head. We consider three methods to correlate heads, based on the norm of (m) (m0 ) two attention matrices αl [k], αl0 [k 0 ], their Pearson correlation, and their Jensen–Shannon divergence.5 We then average over heads k in layer l, as before. These measures are similar to neuronsim in that they emphasize localization of information—if two layers have pairs of heads that are very simila"
2020.acl-main.422,W16-2524,0,\N,Missing
2020.acl-main.422,P17-1080,1,\N,Missing
2020.acl-main.422,D17-1169,0,\N,Missing
2020.acl-main.422,D18-1119,0,\N,Missing
2020.acl-main.422,Q19-1004,1,\N,Missing
2020.acl-main.422,N19-1357,0,\N,Missing
2020.acl-main.422,P19-1283,0,\N,Missing
2020.acl-main.422,N19-1423,0,\N,Missing
2020.acl-main.422,P19-1647,0,\N,Missing
2020.acl-main.422,N18-1101,0,\N,Missing
2020.acl-main.422,D19-1424,0,\N,Missing
2020.cl-1.1,D18-1313,0,0.0552683,"This implies that a higher BLEU score does not necessarily entail better morphological representations. In other words, a better translation model learns more informative representations, but only when it is actually learning to translate rather than merely memorizing the data as in the autoencoder case. We found these results to be consistent in other language pairs, that is, by changing the source from Arabic to German and Czech and also using character models instead of words (see Section A.2 in the Appendix for more details); however, more through study is required along this direction as Bisazza and Tump (2018) performed a similar experiment on a fine-grained tag level and found contrastive results. 7. Syntax Results To evaluate the NMT representations from a syntactic perspective, we consider two tasks. First, we made use of CCG supertagging, which is assumed to capture syntax at the word level. Second, we used dependency relations between any two words in the sentence for which a dependency edge exists, to investigate how words compose. Specifically, we ask the following questions: (i) Do NMT models acquire structural information while they are being trained on flat sequences of bilingual sentence"
2020.cl-1.1,C16-1333,0,0.0606149,"Missing"
2020.cl-1.1,W16-2308,0,0.232083,"hoices and performance. In current practice, their development is often limited to a trial-and-error process, without gaining a real understanding of what the system has learned. We aim to increase model transparency by analyzing the representations learned by NMT models at different levels of granularity in light of various linguistic phenomena—at morphological, syntactic, and semantic levels—that are considered important for the task of machine translation and for learning complex natural language processing (NLP) problems. We thus strive for post-hoc decomposability, in the sense of Lipton (2016). That is, we analyze models after they have been trained, to uncover what linguistic phenomena are captured within the underlying representations. More specifically, we aim to address the following questions in this article: • What linguistic information is captured in deep learning models? – – – Do the NMT representations capture word morphology? Do the NMT models, being trained on flat sequences of words, still acquire structural information? Do the NMT models learn informative semantic representations? • Is the language information well distributed across the network or are designated part"
2020.cl-1.1,W17-4705,0,0.0190207,"line of work visualizes hidden unit activations in recurrent neural networks (RNNs) that are trained for a given task (Elman 1991; Karpathy, Johnson, and Li 2015; K´ad´ar, Chrupała, and Alishahi 2017). Although such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. Other work aims to evaluate systems on specific linguistic phenomena represented in so-called challenge sets. Prominent examples include older work on MT evaluation (King and Falkedal 1990), as well as more recent evaluations via contrastive translation pairs (Burlot and Yvon 2017; Rios Gonzales, Mascarell, and 1 The learned parameters are implicitly shared by all the language pairs being modeled. 4 Belinkov, Durrani et al. Linguistic Representations in NMT Sennrich 2017; Sennrich 2017; Bawden et al. 2018). The latter line of work constructs minimal pairs of translations that differ by a known linguistic property, and evaluates whether the MT system assigns a higher score to the correct translation. The challenge set evaluation may produce informative results on the quality of the overall model for some linguistic property, but it does not directly assess the learned r"
2020.cl-1.1,P18-1008,0,0.025914,"on recurrent LSTM encoderdecoder models with attention. Although this is the first successful NMT architecture, and still a dominant one, it is certainly not the only one. Other sucessful architectures include fully convolutional (Gehring et al. 2017) and fully attentional, transformer encoder-decoder models (Vaswani et al. 2017). There are also non-autoregressive models, which are promising in terms of efficiency (Gu et al. 2018). At present, NMT systems based on transformer components appear to be the most successful. Combinations of transformer and recurrent components may also be helpful (Chen et al. 2018). The generalization of the particular results in this work to other architectures is a question of study. Recent efforts to analyze transformer-based NMT models include attempts to extract syntactic trees from self-attention weights (Mareˇcek and Rosa 2018; Raganato and Tiedemann 2018) and evaluating representations from the transformer encoder (Raganato and Tiedemann 2018). The latter found that lower layers tend to focus on POS and shallow syntax, whereas higher layers are more focused on semantic tagging. These results are in line with our findings. However, more work is needed to understa"
2020.cl-1.1,P05-1033,0,0.303982,"or Professor admit@@ s to shoot@@ ing his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsu"
2020.cl-1.1,N09-1025,0,0.0771531,"Missing"
2020.cl-1.1,P16-1160,0,0.073408,"Missing"
2020.cl-1.1,C04-1041,0,0.0678557,"l. 2016). Third, as dependencies are bi-lexical relations between words, it is straightforward to obtain representations for them from an NMT model. This makes them amenable to the general methodology followed in this paper. Figure 1a shows an example sentence with syntactic dependencies. 3.3 Semantics The holy grail in MT has long been to achieve an interlingua-based translation model, where the goal is to capture the meaning of the source sentence and generate a target sentence with the same meaning. It has been believed since the inception of MT 3 Refer to Steedman and Baldridge (2011) and Clark and Curran (2004) for more information on CCG supertagging. 8 Belinkov, Durrani et al. Linguistic Representations in NMT Figure 1 Example sentence with syntactic and semantic relations. (a) Syntactic relations according to the Universal Dependencies formalism. Here “Obama” and “ Netanyahu” are the subject and object of “receives”, respectively, obl refers to an oblique relation of the locative modifier, nmod denotes the genitive relation, the prepositions “in” and “of” are treated as case-marking elements, and “the” is a determiner. See https://universaldependencies.org/guidelines.html for detailed definitions"
2020.cl-1.1,P18-1198,0,0.0310848,"oduce informative results on the quality of the overall model for some linguistic property, but it does not directly assess the learned representations. A different approach tries to provide a quantitative analysis by correlating parts of the neural network with linguistic properties, for example, by training a classifier to predict a feature of interest (Adi et al. 2017; Hupkes, Veldhoen, and Zuidema 2017; Conneau ¨ et al. 2018). Such an analysis has been conducted on word embeddings (Kohn 2015; Qian, Qiu, and Huang 2016b), sentence embeddings (Adi et al. 2017; Ganesh, Gupta, and Varma 2017; Conneau et al. 2018), and RNN states (Qian, Qiu, and Huang 2016a; Wu and King 2016; Wang, Chung, and Lee 2017). The language properties mainly analyzed are morphological (Qian, Qiu, and Huang 2016b; Vylomova et al. 2016; Belinkov et al. 2017a; Dalvi et al. 2017), semantic (Qian, Qiu, and Huang 2016b; Belinkov et al. 2017b), ¨ and syntactic (Tran, Bisazza, and Monz 2018; Kohn 2015; Conneau et al. 2018). Recent studies carried a more fine-grained neuron-level analysis for NMT and LM (Bau et al. 2019a; Dalvi et al. 2019a; Lakretz et al. 2019). In contrast to all of this work, we focus on the representations learned"
2020.cl-1.1,P16-2058,0,0.310821,"and Glass (2019) for a recent survey on the topic. 2.2 Subword Units One of the major challenges in training NMT systems is handling less frequent and out-of-vocabulary words. To address this issue, researchers have resorted to using subword units for training the neural network models. Luong and Manning (2016) trained a hybrid system that integrates character-level representation within a wordbased framework. Ling et al. (2015) used a bidirectional long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to compose word embeddings from the character embeddings. Costa-juss`a and Fonollosa (2016) and Renduchintala et al. (2018) combined convolutional and highway layers to replace the standard lookupbased word representations in NMT systems with character-aware representations.2 Sennrich, Haddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 C"
2020.cl-1.1,I17-1015,1,0.836995,"eural network with linguistic properties, for example, by training a classifier to predict a feature of interest (Adi et al. 2017; Hupkes, Veldhoen, and Zuidema 2017; Conneau ¨ et al. 2018). Such an analysis has been conducted on word embeddings (Kohn 2015; Qian, Qiu, and Huang 2016b), sentence embeddings (Adi et al. 2017; Ganesh, Gupta, and Varma 2017; Conneau et al. 2018), and RNN states (Qian, Qiu, and Huang 2016a; Wu and King 2016; Wang, Chung, and Lee 2017). The language properties mainly analyzed are morphological (Qian, Qiu, and Huang 2016b; Vylomova et al. 2016; Belinkov et al. 2017a; Dalvi et al. 2017), semantic (Qian, Qiu, and Huang 2016b; Belinkov et al. 2017b), ¨ and syntactic (Tran, Bisazza, and Monz 2018; Kohn 2015; Conneau et al. 2018). Recent studies carried a more fine-grained neuron-level analysis for NMT and LM (Bau et al. 2019a; Dalvi et al. 2019a; Lakretz et al. 2019). In contrast to all of this work, we focus on the representations learned in neural machine translation in light of various linguistic properties (morphological, syntactic, and semantic) and phenomena such as handling low frequency words. Our work is most similar to Shi, Padhi, and Knight (2016) and Vylomova et al."
2020.cl-1.1,N19-1423,0,0.0237899,"he NMT encoder or decoder. We have already mentioned one work exploiting this idea, known as CoVE (McCann et al. 2017), which used NMT representations as features in other models to perform various NLP tasks. Other prominent contextualizers include ELMo (Peters et al. 2018a), which trains two separate, forward and backward LSTM language models (with a character CNN building block) and concatenates their representations across several layers; GPT (Radford et al. 2018) and GPT-2 (Radford et al. 2019), which use transformer language models based on self-attention (Vaswani et al. 2017); and BERT (Devlin et al. 2019), which uses a bidirectional transformer model trained on masked language modeling (filling the blanks). All these generate representations that feed into task-specific classifiers, potentially with fine-tuning the contextualizer weights.21 21 See Peters, Ruder, and Smith (2019) for an evaluation of when it is worthwhile to fine-tune. 38 Belinkov, Durrani et al. Linguistic Representations in NMT How do NMT representations compare with CWRs trained from raw text? Directly answering this question is beyond the scope of this work, and is also tricky to perform for two reasons. First, CWRs like EL"
2020.cl-1.1,P10-1048,1,0.738996,"representations.2 Sennrich, Haddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (201"
2020.cl-1.1,E14-4029,1,0.825501,"a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (2017) performed a comparative evaluation of character- and BPE-based systems on carefully"
2020.cl-1.1,P11-1105,1,0.842798,"Missing"
2020.cl-1.1,P16-1078,0,0.0616503,"Missing"
2020.cl-1.1,P06-1121,0,0.0689132,"mit@@ s to shoot@@ ing his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg"
2020.cl-1.1,D08-1089,0,0.0710678,"rtificial intelligence, including machine translation (MT). Compared with their traditional counterparts, these models are trained in an end-to-end fashion, providing a simple yet elegant mechanism. This simplicity, however, comes at the price of opaqueness. Unlike traditional systems that contain specialized modules carrying specific sub-tasks, neural MT (NMT) systems train one large network, optimized toward the overall task. For example, non-neural statistical MT systems have sub-components to handle fluency (Heafield 2011), lexical generation (Koehn, Och, and Marcu 2003), word reordering (Galley and Manning 2008; Durrani, Schmid, and Fraser 2011), rich morphology (Koehn and Hoang 2007), and a smorgasbord of features (Chiang, Knight, and Wang 2009) for modeling different phenomena. Neural MT systems, on the other hand, contain a single model based on an encoder-decoder mechanism (Sutskever, Vinyals, and Le 2014) with attention (Bahdanau, Cho, and Bengio 2014). Despite its simplicity, neural MT surpassed non-neural statistical MT within a few years of its emergence. Human evaluation and error analysis revealed that the improvements were obtained through more fluent outputs (Toral and S´anchez-Cartagena"
2020.cl-1.1,W11-2123,0,0.0433646,"ep neural networks have quickly become the predominant approach to most tasks in artificial intelligence, including machine translation (MT). Compared with their traditional counterparts, these models are trained in an end-to-end fashion, providing a simple yet elegant mechanism. This simplicity, however, comes at the price of opaqueness. Unlike traditional systems that contain specialized modules carrying specific sub-tasks, neural MT (NMT) systems train one large network, optimized toward the overall task. For example, non-neural statistical MT systems have sub-components to handle fluency (Heafield 2011), lexical generation (Koehn, Och, and Marcu 2003), word reordering (Galley and Manning 2008; Durrani, Schmid, and Fraser 2011), rich morphology (Koehn and Hoang 2007), and a smorgasbord of features (Chiang, Knight, and Wang 2009) for modeling different phenomena. Neural MT systems, on the other hand, contain a single model based on an encoder-decoder mechanism (Sutskever, Vinyals, and Le 2014) with attention (Bahdanau, Cho, and Bengio 2014). Despite its simplicity, neural MT surpassed non-neural statistical MT within a few years of its emergence. Human evaluation and error analysis revealed th"
2020.cl-1.1,P06-1064,0,0.0106613,"ifferent NLP tasks. Table 4 details the number of tags (or labels) in each task across different languages. 6. Morphology Results In this section, we investigate what kind of morphological information is captured within NMT models, using the tasks of POS and morphological tagging. To probe this, we annotated a subset of the training data (see Table 3) using POS or morphological 11 http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger.html. 12 There are no available CCG banks for the other languages we experiment with, except for a German CCG bank, which is not publicly available (Hockenmaier 2006). 13 The main differences between PSD and the original tectogrammatical annotation are the omission of elided elements, such that all nodes are surface tokens; the inclusion of functional and punctuation tokens; ignoring most cases of function word attachments to content words; ignoring coreference links; and ignoring grammatemes (tectogrammatical correlates of morphological categories). As a side effect, these simplifications make it straightforward to generate representations for surface tokens participating in dependency relations under the PSD formalism. See http://sdp.delph-in.net for mor"
2020.cl-1.1,J07-3004,0,0.0101183,"3 – – – – – – – – 14,006 5,640 14,006 5,640 – – – – – – – – German, Spanish, and Czech) we used RDRPOST (Nguyen et al. 2014), a state-of-the-art morphological tagger. For experiments using gold tags, we used the Arabic Treebank for Arabic (with the versions and splits described in the MADAMIRA manual) and the Tiger corpus for German.11 For semantic tagging, we used the semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al. 2017). For syntactic relation labeling we used the Universal Dependencies data set (Nivre et al. 2017). For CCG supertagging we used the English CCGBank (Hockenmaier and Steedman 2007).12 For semantic dependency labeling we used PSD, which is a reduction of the tectogrammatical analysis layer of the Prague Czech–English Dependency Treebank, and is made available as part of the Semantic Dependency Parsing data set (Oepen et al. 2014, 2015). Most of the PSD dependency labels mark semantic roles of arguments, which are called functors in the Prague dependency treebank.13 PSD annotations are available in English and Czech. Table 3 provides the amount of data used to train the MT classifiers for different NLP tasks. Table 4 details the number of tags (or labels) in each task acr"
2020.cl-1.1,E17-2059,0,0.0241302,"bit grammatical relations such as subject/object/predicate or gender agreement by only changing the word form, whereas others achieve the same through word order or addition of particles. Morphology (aka word structure), poses an exigent problem in machine translation and is at the heart of dealing with the challenge of data-sparsity. Although English is limited in morphology, other languages such as Czech, Arabic, and Russian have highly inflected morphology. This entails that for each lemma many possible word variants could exist, thus causing an out-of-vocabulary word problem. For example, Huck et al. (2017) found only one morphological variant of the Czech word “ˇce¨ s˘ ka” (plural of English “kneecap”) in a corpus of 50K parallel sentences. It required 50M sentences, a size of parallel corpus Table 1 Example sentence with different word-level annotations. The CCG supertags are taken from Nadejde et al. (2017). POS and semantic tags are our own annotation, as well as the German translation and its morphological tags. Words Obama receives Netanyahu in the capital of USA POS SEM CCG NP PER NP VBZ ENS ((S[dcl]NP) /PP)/NP NP PER NP IN REL PP/NP DT DEF NP/N NN REL N IN REL (NPNP) /NP NP GEO NP Word"
2020.cl-1.1,Q17-1024,0,0.0766313,"Missing"
2020.cl-1.1,C12-1083,0,0.0202664,"definitions. (b) Semantic relations according to the PSD formalism. Here ACT-arg and PAT-arg refer respectively to the originator and affected arguments of “receives”, LOC in the location, and APP is the thing that “capital” belongs to. For detailed definitions, see Cinkov´a et al. (2004). that without acquiring such meaning representations it will be impossible to generate human-like translations (Weaver 1955). Traditional statistical MT systems are weak at capturing meaning representations (e.g., “who does what to whom—namely, what are the agent, the action, and the patient in the sentence [Jones et al. 2012]). Although neural MT systems are also trained only on parallel data, without providing any direct supervision of word meaning, they are a continuous space model, and are believed to capture word meaning. Johnson et al. (2017), for example, found preliminary evidence that the shared architecture in their multilingual NMT systems learns a universal interlingua. There have also been some recent efforts to incorporate such information in NMT systems, either explicitly (Rios Gonzales, Mascarell, and Sennrich 2017) or implicitly (Liu, Lu, and Neubig 2018). Tagging task. In this article, we study h"
2020.cl-1.1,C90-2037,0,0.773283,"Missing"
2020.cl-1.1,D07-1091,0,0.249735,"Missing"
2020.cl-1.1,N03-1017,0,0.176326,"Missing"
2020.cl-1.1,2006.iwslt-evaluation.11,0,0.156188,"Missing"
2020.cl-1.1,N19-1002,0,0.0298792,"16b), sentence embeddings (Adi et al. 2017; Ganesh, Gupta, and Varma 2017; Conneau et al. 2018), and RNN states (Qian, Qiu, and Huang 2016a; Wu and King 2016; Wang, Chung, and Lee 2017). The language properties mainly analyzed are morphological (Qian, Qiu, and Huang 2016b; Vylomova et al. 2016; Belinkov et al. 2017a; Dalvi et al. 2017), semantic (Qian, Qiu, and Huang 2016b; Belinkov et al. 2017b), ¨ and syntactic (Tran, Bisazza, and Monz 2018; Kohn 2015; Conneau et al. 2018). Recent studies carried a more fine-grained neuron-level analysis for NMT and LM (Bau et al. 2019a; Dalvi et al. 2019a; Lakretz et al. 2019). In contrast to all of this work, we focus on the representations learned in neural machine translation in light of various linguistic properties (morphological, syntactic, and semantic) and phenomena such as handling low frequency words. Our work is most similar to Shi, Padhi, and Knight (2016) and Vylomova et al. (2016). The former used hidden vectors from a neural MT encoder to predict syntactic properties on the English source side, whereas we study multiple language properties in different languages. Vylomova et al. (2016) analyzed different representations for morphologically rich langu"
2020.cl-1.1,Q17-1026,0,0.0448226,"Missing"
2020.cl-1.1,N13-1060,0,0.0603544,"Missing"
2020.cl-1.1,N18-1121,0,0.0457537,"Missing"
2020.cl-1.1,N19-1112,1,0.910624,"Missing"
2020.cl-1.1,P16-1100,0,0.0292588,"for morphologically rich languages in MT, but they did not directly measure the quality of the learned representations. Surveying the work on analyzing neural networks in NLP is beyond the scope of the present paper. We have highlighted here several of the more relevant studies and refer to Belinkov and Glass (2019) for a recent survey on the topic. 2.2 Subword Units One of the major challenges in training NMT systems is handling less frequent and out-of-vocabulary words. To address this issue, researchers have resorted to using subword units for training the neural network models. Luong and Manning (2016) trained a hybrid system that integrates character-level representation within a wordbased framework. Ling et al. (2015) used a bidirectional long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to compose word embeddings from the character embeddings. Costa-juss`a and Fonollosa (2016) and Renduchintala et al. (2018) combined convolutional and highway layers to replace the standard lookupbased word representations in NMT systems with character-aware representations.2 Sennrich, Haddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment wor"
2020.cl-1.1,D10-1015,0,0.0774087,"Missing"
2020.cl-1.1,W18-5444,0,0.0348371,"Missing"
2020.cl-1.1,W06-2932,0,0.0276513,"Missing"
2020.cl-1.1,J11-1007,0,0.0157602,"), French (fr), German (de), Czech (cs), Arabic (ar), Russian (ru), and Hebrew (he). We trained NMT systems using data made available by the two popular machine translation campaigns, namely, WMT (Bojar et al. 2017) and IWSLT (Cettolo et al. 2016). The MT models were trained using a concatenation of NEWS, TED, and Europarl training data (≈ 2.5M sentence pairs). The multilingual systems were trained by simply concatenating data from different 5 It is also not unrealistic, as dependency parsers often work in two stages, first predicting an unlabeled dependency tree, and then labeling its edges (McDonald and Nivre 2011; McDonald, Lerman, and Pereira 2006). More complicated formulations can be conceived, from predicting the existence of dependencies independently to solving the full parsing task, but dependency labeling is a simple basic task to begin with. 6 Although we studied representations from a charCNN (Kim et al. 2015) in Belinkov et al. (2017a), the extracted features were still based on word representations produced by the charCNN. As a result, in that work we could not analyze and compare subword and character-based models that do not assume a segmentation into words. 7 One could envision more sop"
2020.cl-1.1,P12-2059,0,0.0181017,"ddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (2017) performed a comparative"
2020.cl-1.1,P14-2024,0,0.0151066,"ting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni and Goldberg 2017; Chen et al. 2017; Wu et al. 2017), bu"
2020.cl-1.1,E14-2005,0,0.0130295,"and gold annotations for syntactic and semantics tasks. POS tags Morph tags CCG tags Syntactic dependency Semantic tags Semantic dependency Train Test Train Test Train Test Train Test Train Test Train Test de en cs ru fr es 14,498 8,172 14,498 8,172 – – 14,118 1,776 1,490 373 – – 14,498 8,172 14,498 8,172 41,586 2,407 12,467 4,049 14,084 12,168 12,000 9,692 14,498 8,172 14,498 8,172 – – 14,553 1,894 – – 11,999 10,010 11,824 5,999 11,824 5,999 – – 3,848 1,180 – – – – 11,495 3,003 11,495 3,003 – – – – – – – – 14,006 5,640 14,006 5,640 – – – – – – – – German, Spanish, and Czech) we used RDRPOST (Nguyen et al. 2014), a state-of-the-art morphological tagger. For experiments using gold tags, we used the Arabic Treebank for Arabic (with the versions and splits described in the MADAMIRA manual) and the Tiger corpus for German.11 For semantic tagging, we used the semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al. 2017). For syntactic relation labeling we used the Universal Dependencies data set (Nivre et al. 2017). For CCG supertagging we used the English CCGBank (Hockenmaier and Steedman 2007).12 For semantic dependency labeling we used PSD, which is a reduction of the tectogrammatical"
2020.cl-1.1,S15-2153,0,0.10891,"Missing"
2020.cl-1.1,S14-2008,0,0.146289,"can be noticed by the different graph structure (compare Figure 1b to Figure 1a). Predicate–argument relations have also been used in many (non-neural) MT systems (Komachi, Matsumoto, and Nagata 2006; Wu et al. 2011; Xiong, Zhang, and Li 2012; Li, Resnik, and Daum´e III 2013). Figure 1b shows an example sentence annotated with Prague Semantic Dependencies (PSD), a reduction of the tectogrammatical annotation in the Prague Czech–English Dependency Treebank (Cinkov´a et al. 2004; Cinkov´a et al. 2009), which was made available as part of the Semantic Dependency Parsing shared tasks in SemEval (Oepen et al. 2014, 2015). 4. Methodology We follow a 3-step process for studying linguistic information learned by the trained neural MT systems. The steps include: (i) training a neural MT system; (ii) using the trained model to generate feature representations for words in a language of interest; and (iii) training a classifier using generated features to make predictions for the different linguistic tagging tasks. The quality of the trained classifier on the given task serves as a proxy to the quality of the generated representations. It thus provides a quantitative measure of how well the original MT syste"
2020.cl-1.1,P02-1040,0,0.109365,"s from the forward and backward layers are concatenated. For the average method, all of the hidden states corresponding to subwords or characters of a given word are averaged together for each layer. For the last method, only the hidden state of the final subword or character is considered. language pairs (a total of ≈10M sentence pairs) and training a shared encoder-decoder pipeline. We used German, French, Spanish, and Czech to/from English to train multilingual systems. Language codes were added as prefixes before each sentence. We used official TED test sets to report translation quality (Papineni et al. 2002). We also used the fully aligned United Nations corpus (Ziemski, Junczys-Dowmunt, and Pouliquen 2016) for training the models in some of our experiments. It includes six languages: Arabic, Chinese, English, French, Spanish, and Russian. This data set has the benefit of multiple alignment of the several languages, which allows for comparable cross-linguistic analysis, for example, studying the effect of only changing the target language. We used the first 2 million sentences of the training set, using the official training/development/test split. 5.2 Neural MT Systems 5.2.1 Preprocessing. We us"
2020.cl-1.1,pasha-etal-2014-madamira,0,0.0332275,"Missing"
2020.cl-1.1,W19-4302,0,0.0794676,"Missing"
2020.cl-1.1,N18-1202,0,0.378949,"not all information may be extracted by a simple classifier), as the task-specific encoder-decoder performs better than a classifier trained on its representations. 10.2 Contextualized Word Representations The representations generated by NMT models may be thought of as contextualized word representations (CWRs), as they capture context via the NMT encoder or decoder. We have already mentioned one work exploiting this idea, known as CoVE (McCann et al. 2017), which used NMT representations as features in other models to perform various NLP tasks. Other prominent contextualizers include ELMo (Peters et al. 2018a), which trains two separate, forward and backward LSTM language models (with a character CNN building block) and concatenates their representations across several layers; GPT (Radford et al. 2018) and GPT-2 (Radford et al. 2019), which use transformer language models based on self-attention (Vaswani et al. 2017); and BERT (Devlin et al. 2019), which uses a bidirectional transformer model trained on masked language modeling (filling the blanks). All these generate representations that feed into task-specific classifiers, potentially with fine-tuning the contextualizer weights.21 21 See Peters"
2020.cl-1.1,D18-1179,0,0.0981855,"not all information may be extracted by a simple classifier), as the task-specific encoder-decoder performs better than a classifier trained on its representations. 10.2 Contextualized Word Representations The representations generated by NMT models may be thought of as contextualized word representations (CWRs), as they capture context via the NMT encoder or decoder. We have already mentioned one work exploiting this idea, known as CoVE (McCann et al. 2017), which used NMT representations as features in other models to perform various NLP tasks. Other prominent contextualizers include ELMo (Peters et al. 2018a), which trains two separate, forward and backward LSTM language models (with a character CNN building block) and concatenates their representations across several layers; GPT (Radford et al. 2018) and GPT-2 (Radford et al. 2019), which use transformer language models based on self-attention (Vaswani et al. 2017); and BERT (Devlin et al. 2019), which uses a bidirectional transformer model trained on masked language modeling (filling the blanks). All these generate representations that feed into task-specific classifiers, potentially with fine-tuning the contextualizer weights.21 21 See Peters"
2020.cl-1.1,W17-4737,0,0.1378,"t al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (2017) performed a comparative evaluation of character- and BPE-based systems on carefully crafted synthetic tests and found that character-based models are effective in handling unknown words, but perform worse in cap"
2020.cl-1.1,D16-1079,0,0.12019,"Missing"
2020.cl-1.1,P16-1140,0,0.0902583,"Missing"
2020.cl-1.1,W18-5431,0,0.0485817,"Missing"
2020.cl-1.1,J82-2005,0,0.696897,"Missing"
2020.cl-1.1,W17-4702,0,0.0581143,"Missing"
2020.cl-1.1,C94-1027,0,0.359968,"k. 5.4 Supervised Data and Annotations We make use of gold-standard annotations wherever available, but in some cases we have to rely on using automatic taggers to obtain the annotations. In particular, to analyze the representations on the decoder side, we require parallel sentences.10 It is difficult to obtain gold-standard data with parallel sentences, so we rely on automatic annotation tools. An advantage of using automatic annotations, though, is that we can reduce the effect of domain mismatch and high out-of-vocabulary (OOV) rate in analyzing these representations. We used Tree-Tagger (Schmid 1994) for annotating Russian and the MADAMIRA tagger (Pasha et al. 2014) for annotating Arabic. For the remaining languages (French, 9 The sentence length was varied across different configurations, to keep the training data sizes the same for all systems. 10 We need source sentences to generate encoder states, which in turn are required for obtaining the decoder states that we want to analyze. 14 Belinkov, Durrani et al. Linguistic Representations in NMT Table 3 Train and test data (number of sentences) used to train MT classifiers to predict different tasks. We used automated tools to annotate da"
2020.cl-1.1,E17-2060,0,0.270394,". Although such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. Other work aims to evaluate systems on specific linguistic phenomena represented in so-called challenge sets. Prominent examples include older work on MT evaluation (King and Falkedal 1990), as well as more recent evaluations via contrastive translation pairs (Burlot and Yvon 2017; Rios Gonzales, Mascarell, and 1 The learned parameters are implicitly shared by all the language pairs being modeled. 4 Belinkov, Durrani et al. Linguistic Representations in NMT Sennrich 2017; Sennrich 2017; Bawden et al. 2018). The latter line of work constructs minimal pairs of translations that differ by a known linguistic property, and evaluates whether the MT system assigns a higher score to the correct translation. The challenge set evaluation may produce informative results on the quality of the overall model for some linguistic property, but it does not directly assess the learned representations. A different approach tries to provide a quantitative analysis by correlating parts of the neural network with linguistic properties, for example, by training a classifier to pred"
2020.cl-1.1,W17-4739,0,0.137065,"lly rich languages into constituents in a preprocessing step, using word segmentation in Arabic (Pasha et al. 2014; Abdelali et al. 2016) or compound splitting in German (Koehn and Knight 2003). Previous work also explored generative morphological models, known as Factored Translation Models, that explicitly integrate additional linguistic markup at the word level to learn morphology (Koehn and Hoang 2007). In NMT training, using subword units such as byte-pair encoding (Sennrich, Haddow, and Birch 2016) has become a de facto standard in training competition grade systems (Pinnis et al. 2017; Sennrich et al. 2017). A few have tried morpheme-based segmentation (Bradbury and Socher 2016), and several even used character-based systems (Chung, Cho, and Bengio 2016; Lee, Cho, and Hofmann 2017) to achieve similar performance as the BPE-segmented systems. Table 2 shows an example of each representation unit. BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol. In essence, frequent character n-gram sequences merge to form one symbol. The number of merge operations is controlled by a hyper-parameter OP"
2020.cl-1.1,P16-1162,0,0.213462,"Missing"
2020.cl-1.1,J10-4005,0,0.059411,"Missing"
2020.cl-1.1,D16-1159,0,0.108794,"Missing"
2020.cl-1.1,E14-2006,0,0.0330156,"Missing"
2020.cl-1.1,P16-2049,0,0.0243138,"al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni and Goldberg 2017; Chen et al. 2017; Wu et al. 2017), but sequence-to-sequence NMT models without explicit syntax are the state of the art at the moment (Pinnis et al. 2017; Sennrich et al. 2017). Tagging tasks. In this paper, we analyze whether NMT models trained on flat sequences acquire structural syntactic information. To answer this, we use two tagging tasks. First, we use CCG supertagging, which captures global syntactic information locally at the word level by assigning a label to each word annotating its syntactic role in the sentence. The process is almost equivalent to pars"
2020.cl-1.1,E17-1100,0,0.0444399,"Missing"
2020.cl-1.1,D18-1503,0,0.0645852,"Missing"
2020.cl-1.1,P17-1065,0,0.0226344,"ubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni and Goldberg 2017; Chen et al. 2017; Wu et al. 2017), but sequence-to-sequence NMT models without explicit syntax are the state of the art at the moment (Pinnis et al. 2017; Sennrich et al. 2017). Tagging tasks. In this paper, we analyze whether NMT models trained on flat sequences acquire structural syntactic information. To answer this, we use two tagging tasks. First, we use CCG supertagging, which captures global syntactic information locally at the word level by assigning a label to each word annotating its syntactic role in the sentence. The process is almost equivalent to parsing (Bangalore and Joshi 1999). For example, the syntactic tag"
2020.cl-1.1,I11-1004,0,0.0211866,"ted with SEM tags. The semantic tag ENS describes a present-simple event category. The second semantic task is semantic dependency labeling, the task of assigning a type to each arc in a semantic dependency graph. Such dependencies are also known as predicate–argument relations, and may be seen as a first step toward semantic structure. They capture different aspects from syntactic relations, as can be noticed by the different graph structure (compare Figure 1b to Figure 1a). Predicate–argument relations have also been used in many (non-neural) MT systems (Komachi, Matsumoto, and Nagata 2006; Wu et al. 2011; Xiong, Zhang, and Li 2012; Li, Resnik, and Daum´e III 2013). Figure 1b shows an example sentence annotated with Prague Semantic Dependencies (PSD), a reduction of the tectogrammatical annotation in the Prague Czech–English Dependency Treebank (Cinkov´a et al. 2004; Cinkov´a et al. 2009), which was made available as part of the Semantic Dependency Parsing shared tasks in SemEval (Oepen et al. 2014, 2015). 4. Methodology We follow a 3-step process for studying linguistic information learned by the trained neural MT systems. The steps include: (i) training a neural MT system; (ii) using the tra"
2020.cl-1.1,P12-1095,0,0.0631512,"Missing"
2020.cl-1.1,P15-2041,0,0.0418831,"Missing"
2020.cl-1.1,P02-1039,0,0.235127,"s gir@@ l@@ friend Morfessor Professor admit@@ s to shoot@@ ing his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Has"
2020.cl-1.1,W18-5448,0,0.177549,"corpus by training English-to-{French, Arabic, Spanish, Russian, and English} bilingual models. Comparing successive layers (for example, comparing layer 2 versus layer 3), in the majority of the cases, the higher layer performed statistically significantly better than the lower one (ρ &lt; 0.01), according to the approximate randomization test (Pado´ 2006).17 Similar to the results on morphological tagging, a combination of all layers achieved the best results. See the Combination bar in Figure 9a. This implies that although syntax is 16 In their study of NMT and language model representations, Zhang and Bowman (2018) noticed that POS is better represented at layer 1 whereas CCG supertags are sometimes, but not always, better represented at layer 2 (out of 2-layer encoders). 17 See Section 11 in the supplementary information for the detailed results. 24 Belinkov, Durrani et al. Linguistic Representations in NMT mainly learned at higher layers, syntactic information is at least partly distributed across the network. One possible concern with these results is that they may be appearing because of the stacked RNN layers, and not necessarily due to the translation task. In the extreme case, perhaps even a rand"
2020.cl-1.1,2007.mtsummit-papers.71,0,0.109127,"his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni"
2020.cl-1.1,Q16-1027,0,0.0288016,"the BPE-based units. Comparing encoder representations with decoder representations, it is interesting to see that in several cases the decoder-side representations performed better than the encoder-side representations, even though they are trained using a unidirectional LSTM only. Because we did not see any notable trends in differences between encoder and decoder side representations, we only present the encoder-side results in the rest of the paper. 19 Computational Linguistics Volume 46, Number 1 6.3 Effect of Network Depth Modern NMT systems use very deep architectures (Wu et al. 2016b; Zhou et al. 2016). We are interested in understanding what kind of information different layers capture. Given a trained NMT model with multiple layers, we extract feature representations from the different layers in the encoder. We trained 4-layered models (using (NEW+TED+Europarl data). Figure 6 shows morphological tagging results using representations from different encoder and decoder layers across five language pairs. The general trend shows that representations from the first layer are better than those from the higher layers, for the purpose of capturing morphology. We found this observation to be true"
2020.cl-1.1,L16-1561,0,0.0590262,"Missing"
2020.cl-1.1,E03-1076,0,\N,Missing
2020.cl-1.1,P07-2045,0,\N,Missing
2020.cl-1.1,D15-1246,0,\N,Missing
2020.cl-1.1,J17-4003,0,\N,Missing
2020.cl-1.1,N16-3003,1,\N,Missing
2020.cl-1.1,W16-2301,0,\N,Missing
2020.cl-1.1,E17-2039,0,\N,Missing
2020.cl-1.1,P17-1080,1,\N,Missing
2020.cl-1.1,W17-4707,0,\N,Missing
2020.cl-1.1,D17-1304,0,\N,Missing
2020.cl-1.1,W17-4717,0,\N,Missing
2020.cl-1.1,Q19-1004,1,\N,Missing
2020.cl-1.1,N18-1118,0,\N,Missing
2020.clinicalnlp-1.16,N18-1070,1,0.879471,"Missing"
2020.clinicalnlp-1.16,D16-1147,0,0.0267844,"xternal knowledge graph. We also propose two metrics: symptom hit rate and unrelate rate to evaluate the performance of the system. We make following contributions in this paper, Related Work Knowledge and Graph Processing Many tasks require processing knowledge in different formats. Sukhbaatar et al. (2015) proposed memory networks (MemNNs) for question answering. The context of the question, or knowledge, is stored in an external memory bank and the model reads information from the memory with an attention mechanism. The MemNN model is also applied in question answering in the movie domain (Miller et al., 2016), video question answering (Luo et al., 2019), and stance detection (Mohtarami et al., 2018). The neural Turing machine (Graves et al., 2014) and the neural computer (Graves et al., 2016) also applied external memory banks, and enable the models to write into and read from the external memory cells dynamically. In many tasks, knowledge can be organized as graphs. Recent studies have proposed different neural models for processing graph-structured data. The graph neural networks (GNNs) (Scarselli et al., 2008) uses neural networks to perform message propagation on graphs. The graph convolutiona"
2020.clinicalnlp-1.16,W00-0303,0,0.3018,"amples of dialog between different systems and a user. Conversation I is generated by an automatic diagnosis system, and conversation II is generated by an automatic symptom detection system. The explicit symptom is highlighted in blue, the implicit symptoms are highlighted in red, and unrelated symptoms are marked in green. Task-Oriented Dialog Systems Task-oriented dialog systems aim at completing a specific task by interacting with users through natural language, and the main challenge is learning a dialog policy manager (Papineni et al., 2001). Typical applications include flight booking (Seneff and Polifroni, 2000), movie recommendation (Dodge et al., 2015; Fazel-Zarandi et al., 2017), restaurant reservation (Bordes et al., 2016), and vision grounding (Chattopadhyay et al., 2017). Recently, such systems have been applied in automatic diagnosis (Wei et al., 2018; Xu et al., 2019; Luo et al., 2020). The authors of De Vries et al. (2017) proposed the GuessWhat game, which requires computers to guess a visual object given a natural language description by asking a series of questions. The GuessWhat game is similar with our task in the medical domain. 2.2 information is enough for a doctor to make diagnosis."
2020.emnlp-main.404,D18-1389,1,0.897223,"s. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregator applications, such as Google News, it could enable balanced search, similarly to what is found on AllSides.1 For journalists, it could enable news exploration from a left/center/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classified as left-leaning, the system should be able to pinpoint specific articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social media users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant supervision, assuming that all articles from a given medium"
2020.emnlp-main.404,2020.acl-main.308,1,0.770374,"y to what is found on AllSides.1 For journalists, it could enable news exploration from a left/center/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classified as left-leaning, the system should be able to pinpoint specific articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social media users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant supervision, assuming that all articles from a given medium should share its overall bias, which is not always the case. Here, we revisit this assumption. 1 http://allsides.com/ 4982 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4982–4991, c November 16–20, 2020. 2020 Association for Computational Linguistics Our contributions can be summarized as follows: • We create a new dataset fo"
2020.emnlp-main.404,N19-1216,1,0.845565,"the same media, and thus models could easily learn to predict the article’s source rather than its bias. In their models, they used both the text and the URL contents of the articles. Overall, political bias has been studied at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; SaezTrumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a followup work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A finer-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classifier, where the labels were projected from the document level. 3 Furthermore, AllSides uses the annotated articles to enable its Balanced Search, which shows news coverage on a given topic from media with different political bias. In ot"
2020.emnlp-main.404,2020.semeval-1.186,1,0.822563,"Missing"
2020.emnlp-main.404,D19-1565,1,0.885835,"ree and the direction of the media bias, and on the voters’ reliance on such media (DellaVigna and Kaplan, 2007; Iyengar and Hahn, 2009; Saez-Trumper et al., 2013; Graber and Dunaway, 2017). Thus, making the general public aware, e.g., by tracking and exposing bias in the news is important for a healthy public debate given the important role media play in a democratic society. Media bias can come in many different forms, e.g., by omission, by over-reporting on a topic, by cherry-picking the facts, or by using propaganda techniques such as appealing to emotions, prejudices, fears, etc. (Da San Martino et al., 2019, 2020a,b) Bias can occur with respect to a specific topic, e.g., COVID-19, immigration, climate change, gun control, etc. (Darwish et al., 2020; Stefanov et al., 2020) It could also be more systematic, as part of a political ideology, which in the Western political system is typically defined as left vs. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregat"
2020.emnlp-main.404,N19-1423,0,0.0111726,"assifier to predict one of C classes (in our case, C = 3: left, center, and right). In our experiments, we use two deep learning architectures: (i) Long Short-Term Memory networks (LSTMs), which are Recurrent Neural Networks (RNNs), which use gating mechanisms to selectively pass information across time and to model long-term dependencies (Hochreiter and Schmidhuber, 1997), and (ii) Bidirectional Encoder Representations from Transformers (BERT), with a complex architecture yielding high-quality contextualized embeddings, which have been successful in several Natural Language Processing tasks (Devlin et al., 2019). 4.2 Removing Media Bias Ultimately, our goal is to develop a model that can predict the political ideology of a news article. Our dataset, along with some others, has a special property that might stand in the way of achieving this goal. Most articles published by a given source have the same ideological leaning. This might confuse the model and cause it to erroneously associate the output classes with features that characterize entire media outlets (such as detecting specific writing patterns, or stylistic markers in text). Consequently, the model would fail when applied to articles that we"
2020.emnlp-main.404,S19-2146,0,0.0167275,"ating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that were labeled using distant supervision, which illustrates the dangers of relying on distant supervision. Barr´on-Cedeno et al. (2019) extensively discussed the limitations of distant supervision in a text classification task about article-level propaganda detection, in a setup that is similar to what we deal with in this paper: the learning systems may learn to model the source of the article instead of solving the task they are"
2020.emnlp-main.404,S19-2145,0,0.0605912,"s very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that were labeled using distant supervi"
2020.emnlp-main.404,D18-1388,0,0.385691,"ter/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classified as left-leaning, the system should be able to pinpoint specific articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social media users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant supervision, assuming that all articles from a given medium should share its overall bias, which is not always the case. Here, we revisit this assumption. 1 http://allsides.com/ 4982 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4982–4991, c November 16–20, 2020. 2020 Association for Computational Linguistics Our contributions can be summarized as follows: • We create a new dataset for predicting the political ideology of news articles. The dataset is annotated at the article level an"
2020.emnlp-main.404,W06-2915,0,0.355075,"l. They analyzed a number of representations and machine learning models, showing which ones tend to overfit more, but, unlike our work here, they fell short of recommending a practical solution. Budak et al. (2016) measured the bias at the article level using crowd-sourcing. This is risky as public awareness of media bias is limited (Elejalde et al., 2018). Moreover, the annotation setup does not scale. Finally, their dataset is not freely available, and their approach of randomly crawling articles does not ensure that topics and events are covered from different political perspectives. 4983 Lin et al. (2006) built a dataset annotated with the ideology of 594 articles related to the IsraeliPalestinian conflict published on bitterlemons. org. The articles were written by two editors and 200 guests, which minimizes the risk of modeling the author style. However, the dataset is too small to train modern deep learning approaches. Kulkarni et al. (2018) built a dataset using distant supervision and labels from AllSides. Distant supervision is fine for the purpose of training, but they also used it for testing, which can be problematic. Moreover, their training and test sets contain articles from the sa"
2020.emnlp-main.404,D17-1317,0,0.0255972,"al annotation at the article level is very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that"
2020.emnlp-main.404,D19-1410,0,0.0146421,"cal leaning. To a lesser extent, the content of a Wikipedia page describing a medium can also help unravel its political leaning. Therefore, we concatenated these representations to the encoded articles, at the output of the encoder and right before the SOFTMAX layer, so that both the article encoder and the classification layer that is based on the article and the external media representations are trained jointly and end-to-end. Similarly to (Baly et al., 2020), we retrieved the profiles of up to a 1,000 Twitter followers for each medium, we encoded their bios using the Sentence-BERT model (Reimers and Gurevych, 2019), and we then averaged these encodings to obtain a single representation for that medium. As for the Wikipedia representation, we automatically retrieved the content of the page describing each medium, whenever applicable. Then, we used the pre-trained base BERT model to encode this content by averaging the word representations extracted from BERT’s second-to-last layer, which is common practice, since the last layer may be biased towards the pre-training objectives of BERT. 5 Experiments and Results We evaluated both the LSTM and the BERT models, assessing the impact of (i) de-biasing and (ii"
2020.emnlp-main.404,S19-2182,1,0.867215,"Missing"
2020.emnlp-main.404,D13-1010,0,0.0267926,"es. Distant supervision is fine for the purpose of training, but they also used it for testing, which can be problematic. Moreover, their training and test sets contain articles from the same media, and thus models could easily learn to predict the article’s source rather than its bias. In their models, they used both the text and the URL contents of the articles. Overall, political bias has been studied at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; SaezTrumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a followup work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A finer-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classifier, where the labels were projected from the docum"
2020.emnlp-main.404,P04-1035,0,0.0574369,"at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; SaezTrumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a followup work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A finer-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classifier, where the labels were projected from the document level. 3 Furthermore, AllSides uses the annotated articles to enable its Balanced Search, which shows news coverage on a given topic from media with different political bias. In other words, for each trending event or topic (e.g., impeachment or coronavirus pandemic), the platform pushes news articles from all sides of the political spectrum, as shown in Figure 1. We took advantage of this and downloaded"
2020.emnlp-main.404,2020.acl-main.50,1,0.631944,"aber and Dunaway, 2017). Thus, making the general public aware, e.g., by tracking and exposing bias in the news is important for a healthy public debate given the important role media play in a democratic society. Media bias can come in many different forms, e.g., by omission, by over-reporting on a topic, by cherry-picking the facts, or by using propaganda techniques such as appealing to emotions, prejudices, fears, etc. (Da San Martino et al., 2019, 2020a,b) Bias can occur with respect to a specific topic, e.g., COVID-19, immigration, climate change, gun control, etc. (Darwish et al., 2020; Stefanov et al., 2020) It could also be more systematic, as part of a political ideology, which in the Western political system is typically defined as left vs. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregator applications, such as Google News, it could enable balanced search, similarly to what is found on AllSides.1 For journalists, it could enable news exploration from a"
2020.emnlp-main.404,D14-1162,0,0.0834781,"ing the seeds of the random weights initialization. For LSTM, we varied the length of the input (128–1,024 tokens), the number of layers (1–3), the size of the LSTM cell (200–400), the dropout rate (0–0.8), the learning rate (1e−3 to 1e−5), the gradient clipping value (0–5), and the batch size (8–256). The best results were obtained with a 512-token input, a 2-layer LSTM of size 256, a dropout rate of 0.7, a learning rate of 1e−3, gradient clipping at 0.5, and a batch size of 32. This model has around 1.1M trainable parameters, and was trained with 300-dimensional GloVe input word embeddings (Pennington et al., 2014). 4987 For BERT, we varied the length of the input, the learning rate, and the gradient clipping value. The best results were obtained using a 512-token input, a learning rate of 2e−5, and gradient clipping at 1. This model has 110M trainable parameters. We trained our models on 4 Titan X Pascal GPUs, and the runtime for each epoch was 25 seconds for the LSTM-based models and 22 minutes for the BERT-based models. For each experiment, the model was trained only once with fixed seeds used to initialize the models’ weights. For the Adversarial Adaptation (AA), we have an additional hyper-paramete"
2020.emnlp-main.404,D19-3038,1,0.890046,"Missing"
2020.emnlp-main.404,P18-1022,0,0.126909,"lsides.com/ 4 http://mediabiasfactcheck.com 3 As manual annotation at the article level is very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually an"
2021.acl-long.411,K17-1037,0,0.0230617,"notation is not a prerequisite for speech processing systems. This line of reasoning motivates research that aims to discover meaningful linguistic abstractions (phones, words, etc.) directly from the speech signal, with the intention that they could reduce the reliance of spoken language systems on text transcripts. A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as how word-like and subword-like linguistic units can be made to emerge within these models (Harwath and Glass, 2017; Harwath et al., 2019; Drexler and Glass, 2017; Alishahi et al., 2017; Harwath et al., 2019; Harwath and Glass, 2019; Havard et al., 2019b; Harwath et al., 2020). So far, these efforts have predominantly focused on inference, where the goal is to learn a mapping from speech waveforms to a semantic embedding space. Generation of speech con"
2021.acl-long.411,K19-1006,0,0.02537,"s to discover meaningful linguistic abstractions (phones, words, etc.) directly from the speech signal, with the intention that they could reduce the reliance of spoken language systems on text transcripts. A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives (Synnaeve et al., 2014; Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017; Havard et al., 2019a; Merkx et al., 2019; Chrupała et al., 2017; Alishahi et al., 2017; Scharenborg et al., 2018; Hsu and Glass, 2018a; Kamper et al., 2018; Surís et al., 2019; Ilharco et al., 2019; Eloff et al., 2019), as well as how word-like and subword-like linguistic units can be made to emerge within these models (Harwath and Glass, 2017; Harwath et al., 2019; Drexler and Glass, 2017; Alishahi et al., 2017; Harwath et al., 2019; Harwath and Glass, 2019; Havard et al., 2019b; Harwath et al., 2020). So far, these efforts have predominantly focused on inference, where the goal is to learn a mapping from speech waveforms to a semantic embedding space. Generation of speech conditioned on a point in a semantic space has been less explored, and is what we focus on in this work. We hypoth"
2021.acl-long.411,W19-8609,0,0.0279954,"Missing"
2021.acl-long.411,W04-1013,0,0.0873841,"(Table 2). To evaluate the I2S system, we can use any method that measures the semantic information contained in the generated speech. We consider two sets of end-to-end metrics: word-based and retrieval-based, and one set of proxy unit-based metrics. Word-based metrics transcribe a generated spoken caption into text (manually or with an ASR system) and then measure word-based captioning metrics against a set of reference captions, such as BLEU-4 (Papineni et al., 2002) (adjusted ngram precision), METEOR (Denkowski and Lavie, 2014) (uni-gram F-score considering word-to-word alignment), ROUGE (Lin, 2004) (n-gram recall), CIDEr (Vedantam et al., 2015) (TF-IDF weighted n-gram cosine similarity), and SPICE (Anderson et al., 2016) (F-score of semantic propositions in scene graphs). This enables comparison between image-to-speech systems with a text “upperbound”, but is not applicable to unwritten languages. Retrieval-based metrics include image-to-speech and speech-to-image retrieval (Harwath et al., 2020), which require a separately trained crossmodal retrieval model for evaluation. Such metrics are text-free, but they cannot measure other aspects of language generation such as syntactic correct"
2021.acl-long.411,P02-1040,0,0.113256,"sequences inferred from real speech and soliciting human judgements in the form of Mean Opinion Score (MOS) and Side-By-Side (SXS) preference tests (Table 2). To evaluate the I2S system, we can use any method that measures the semantic information contained in the generated speech. We consider two sets of end-to-end metrics: word-based and retrieval-based, and one set of proxy unit-based metrics. Word-based metrics transcribe a generated spoken caption into text (manually or with an ASR system) and then measure word-based captioning metrics against a set of reference captions, such as BLEU-4 (Papineni et al., 2002) (adjusted ngram precision), METEOR (Denkowski and Lavie, 2014) (uni-gram F-score considering word-to-word alignment), ROUGE (Lin, 2004) (n-gram recall), CIDEr (Vedantam et al., 2015) (TF-IDF weighted n-gram cosine similarity), and SPICE (Anderson et al., 2016) (F-score of semantic propositions in scene graphs). This enables comparison between image-to-speech systems with a text “upperbound”, but is not applicable to unwritten languages. Retrieval-based metrics include image-to-speech and speech-to-image retrieval (Harwath et al., 2020), which require a separately trained crossmodal retrieval"
2021.acl-long.411,W10-0721,0,0.124419,"Missing"
2021.eacl-main.95,N16-1014,0,0.284479,"neural language generation (NLG). In particular, we focus on the opendomain dialogue response task, for the following reasons: (1) There is high similarity between the target dialogue response task (conditional NLG) and the pretraining language modeling (LM) objective, so we expect that language generation skills learnt during pretraining can be well transferred to the down-stream target task. (2) The sequence-tosequence (seq2seq) nature of the model allows us to characterize the model’s generation behavior in various ways (e.g., context sensitivity). End-to-end dialogue response generation (Li et al., 2016) can be formulated as a sequence-tosequence (seq2seq) task: Given a dialogue context (previous utterances), the model is asked to generate a high-quality response. In this work we adopt the encoder-decoder model architecture (Sutskever et al., 2014; Cho et al., 2014), which is widely used in NLG applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. In particular, we use the transformer model (Vaswani et al., 2017), which has currently become the most popular encoderdecoder model architecture (Young et al., 2017). We use the same config"
2021.eacl-main.95,I17-1099,0,0.0177841,"large-scale CCNEWS data (Bakhtin et al., 2019) which is a de-duplicated subset of the English portion of the CommonCrawl news dataset1 . The dataset contains news articles published worldwide between September 2016 and February 2019. It has in total around 1 billion sentences or 27 billion words. To be able to complete experiments in a reasonable amount of time, we use the first 10 percent of the CCNEWS data for pretraining, which contains 100 million sentences and 2.7 billion words. For finetuning, three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of bo"
2021.eacl-main.95,P18-1138,0,0.0213915,"ledge-grounded datasets such as Topical-chat (Gopalakrishnan et al., 2019). 7 Related Works Behavior of pretrained NLG Models Recently, multiple works (Radford et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Talmor et al., 2019; Trinh and Le, 2019) have reported that pre-trained language models (LM) have implicitly stored large amounts of “world knowledge” in its parameters, and are able to answer common-sense questions. However, whether the world knowledge is well preserved after finetuning on target task dataset is not discussed. 1128 On the other hand, knowledge-grounded NLG model (Liu et al., 2018; Guu et al., 2020; Zhou et al., 2018) has been an important and exciting research topic. These studies usually involve additional retrieval modules or external knowledge bases to provide the model with relevant information. In contrast to these works, we study whether the model can conduct knowledgeable dialogues by itself. Forgetting As discussed in Section 3.2, in contrast to the “catastrophic forgetting” problem in sequential learning (Atkinson et al., 2018; Robins, 1995), the performance drop on pretraining data is not necessarily bad for the NLP pretrain-finetune framework, and its impli"
2021.eacl-main.95,2021.ccl-1.108,0,0.063944,"Missing"
2021.eacl-main.95,D15-1166,0,0.0459636,") The sequence-tosequence (seq2seq) nature of the model allows us to characterize the model’s generation behavior in various ways (e.g., context sensitivity). End-to-end dialogue response generation (Li et al., 2016) can be formulated as a sequence-tosequence (seq2seq) task: Given a dialogue context (previous utterances), the model is asked to generate a high-quality response. In this work we adopt the encoder-decoder model architecture (Sutskever et al., 2014; Cho et al., 2014), which is widely used in NLG applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. In particular, we use the transformer model (Vaswani et al., 2017), which has currently become the most popular encoderdecoder model architecture (Young et al., 2017). We use the same configuration as (Vaswani et al., 2017), which has 6 encoder/decoder layers, 16 attention heads, with an embedding dimension of 1024 and a feed-forward dimension of 4096. During standard finetuning, the Adam optimizer (Kingma and Ba, 2014) is used to minimize the negative log-likelihood (NLL) of the reference target sentence y given the input context x in the data distribution (denoted as Pdata ): Lfinetun"
2021.eacl-main.95,N19-4009,1,0.840787,"three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of both MASS and NS, we use a mini-batch size of 2048, with the learning rate (LR) set to 0.0001. Following (Vaswani et al., 2017), the “inverse square root” LR scheduler with a warm-up stage is used. Pretraining is conducted on 32 GPUs and half-precision (float16) speed-up is used. For both MASS and NS, we stop the pretraining after the CCNEWS data is swept 20 times. For all our experiments, a dropout rate of 0.1 is applied to the transformer model. We follow Song et al. (2019) for the recommended hyper-pa"
2021.eacl-main.95,N18-1202,0,0.214203,"from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named “mix-review”. We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications. 1 Figure 1: During finetuning, the model’s performance on the pretraining data drastically degrades. Introduction Large-scale unsupervised pretraining (Peters et al., 2018; Devlin et al., 2018; Song et al., 2019; Yang et al., 2019; Liu et al., 2019) has recently been shown to greatly boost the performance of natural language processing (NLP) models. On a high level, the pretrain-finetune framework can be viewed as a simple two-stage procedure: (1) Use large-scale unsupervised text data to pretrain the model; (2) Use target task data to finetune the model. Recently, multiple works (Radford et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Talmor et al., 2019) have reported that pretrained language models (LM) have implicitly stored large amounts of “world"
2021.eacl-main.95,P19-1004,0,0.0212955,"e dialogue responses more informative and engaging (e.g., the model can learn about the “Avengers” movie, and use it as a topic). To quantify how knowledgeable the finetuned model is, we prepare a set of knowledge terms such as iphone, pokemon, etc., and the corresponding reference description. We then query the model about these knowledge terms, and compare its output against the reference. We also conduct multi-turn human evaluation in the setting of knowledgeable conversations. More details will be given in Section 5.1. The other ability is the utilization of contextual input: as shown by (Sankar et al., 2019), the current open-domain dialogue models (without pretraining) are insensitive to contextual input, which gives rise to the generic response problem (Li et al., 2016). In our preliminary experiments with NS pretraining, we find that similarly to the GPT model (Radford et al., 2019) the pretrained model has the ability to generate closely related responses given the previous sentences as input. Ideally during finetuning, the model can transfer this skill to the target dialogue task. To quantify the model’s sensitivity to context, following (Sankar et al., 2019), we add noise to the input, and"
2021.eacl-main.95,P16-1162,0,0.0261507,"tences or 27 billion words. To be able to complete experiments in a reasonable amount of time, we use the first 10 percent of the CCNEWS data for pretraining, which contains 100 million sentences and 2.7 billion words. For finetuning, three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of both MASS and NS, we use a mini-batch size of 2048, with the learning rate (LR) set to 0.0001. Following (Vaswani et al., 2017), the “inverse square root” LR scheduler with a warm-up stage is used. Pretraining is conducted on 32 GPUs and half-precision (float16) speed-up is used"
2021.emnlp-main.415,P18-1082,0,0.030209,"e set the sure bias problem because the prefixes from the prefix distribution to be the ground-truth data distrimodel will be of higher quality (thus narrowing the bution instead of the model’s own, the discrepancy discrepancy). (2) The sampling algorithms requires between training and generation in the prefix will tuning of hyper-parameters, which will complicate the comparison. With these being said, in our ex- be removed, and hence the model’s generation quality should be much better. We illustrate this idea in periments (including human evaluation) we have also tested with top-k sampling (Fan et al., 2018), Figure 1. In the extreme case of shuffled or random prefixes, due to the claim from exposure bias about with consistent observations. In addition to popular metrics in natural lan- the incremental distortion, we expect the model to generate also badly distorted sequences. guage generation (NLG) such as BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, The samples with different types of prefixes are 2014), our quantification approaches also rely on shown in Table 1. To make the generation from the measurements of the divergence between two data and model prefix more comparable, we"
2021.emnlp-main.415,2021.acl-long.223,0,0.041782,"; Yu et al., 2016; Zhu et al., imum likelihood estimation (MLE) has been the 2018; Lu et al., 2018; Lin et al., 2017; Guo et al., most widely used objective for LM training. How- 2017; Rajeswar et al., 2017; Wiseman and Rush, ever, there is a popular belief in the natural lan- 2016; Nie et al., 2019; Shi et al., 2018; de Masguage processing (NLP) community that standard son d’Autume et al., 2019; Rennie et al., 2016; 5087 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5087–5102 c November 7–11, 2021. 2021 Association for Computational Linguistics Feng et al., 2021) have been proposed as alternatives to MLE training for text generation. Many of these works utilize techniques from generative adversarial networks (GANs) (Goodfellow et al., 2014) or reinforcement learning (RL) (Sutton and Barto, 1998). In this work, we refer to these algorithms as non-MLE methods or text GANs. With the huge research efforts devoted to alleviate exposure bias, interestingly, the existence or significance of exposure bias is much less studied. Interestingly, despite the criticism, MLE (teacher forcing) has remained to be the dominant objective for LM training (Radford et al.,"
2021.emnlp-main.415,D17-1210,0,0.0453165,"Missing"
2021.emnlp-main.415,2020.acl-main.185,1,0.775845,"Finally, the results from this work should not discourage researchers from exploring non-MLE training algorithms for LM (including text GANs). As shown by recent studies, there exists important problems other than exposure bias for the current NLG models, such as the likelihood trap (Holtzman et al., 2020), factuality (Massarelli et al., 2019; He et al., 2021), or robustness (Cheng et al., 2018; Kassner and Schütze, 2019), etc. Therefore, it is completely possible that a non-MLE training objective can lead to better generation performance (Lu et al., 2018; Huszár, 2015; Welleck et al., 2020; He and Glass, 2020; Gu et al., 2017). 9 Conclusion Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 1171–1179, Cambridge, MA, USA. MIT Press. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Zieg"
2021.emnlp-main.415,2021.eacl-main.95,1,0.722702,"x E, we provide a preliminary study of the self-recovery ability in machine translation). Our results do not rule out the possibility that exposure bias could be more serious in other NLG applications of different nature, which we leave for future work. Finally, the results from this work should not discourage researchers from exploring non-MLE training algorithms for LM (including text GANs). As shown by recent studies, there exists important problems other than exposure bias for the current NLG models, such as the likelihood trap (Holtzman et al., 2020), factuality (Massarelli et al., 2019; He et al., 2021), or robustness (Cheng et al., 2018; Kassner and Schütze, 2019), etc. Therefore, it is completely possible that a non-MLE training objective can lead to better generation performance (Lu et al., 2018; Huszár, 2015; Welleck et al., 2020; He and Glass, 2020; Gu et al., 2017). 9 Conclusion Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pages 1171–1179, Cambridge, MA, USA. MIT Press. Tom Bro"
2021.emnlp-main.415,P18-1027,0,0.0633459,"Missing"
2021.emnlp-main.415,D17-1230,0,0.0359043,"among researchers) that during 1 Introduction generation the errors could accumulate along the Language model (LM) has been a central mod- generated sequence, and the distribution generated by the model would be incrementally distorted. ule for natural language generation (NLG) tasks The forced exposure to ground-truth data during (Young et al., 2017) such as open-ended language generation (Radford et al., 2018; Brown et al., training is also referred to as teacher forcing. 2020), machine translation (Wu et al., 2017), diaTo avoid teacher forcing, many training algologue response generation (Li et al., 2017), image rithms (Bengio et al., 2015; Lamb et al., 2016; captioning (Lin et al., 2014), etc. For decades, max- Ranzato et al., 2016; Yu et al., 2016; Zhu et al., imum likelihood estimation (MLE) has been the 2018; Lu et al., 2018; Lin et al., 2017; Guo et al., most widely used objective for LM training. How- 2017; Rajeswar et al., 2017; Wiseman and Rush, ever, there is a popular belief in the natural lan- 2016; Nie et al., 2019; Shi et al., 2018; de Masguage processing (NLP) community that standard son d’Autume et al., 2019; Rennie et al., 2016; 5087 Proceedings of the 2021 Conference on Empiri"
2021.emnlp-main.415,2020.acl-main.704,0,0.0247755,"weaknesses: First, it doesn’t reflect how the generation is consistent with the given prefix W1:l , because it only focuses on the marginal distribution of Wl+1:l+lgen . Second, even in the PM |D case, exposure bias still affects the generation Wl+1:l+lgen .2 To cover these shortcomings, in the next section we propose another quantification method named EB-C, which focuses on the model’s word-level conditional generation distribution of Wl+1 given prefix W1:l . Finally, the standard NLG metrics (such as BLEU) have recently been criticized that they may correlate poorly with human judgements (Sellam et al., 2020). Therefore, in Section 6.3 we conduct a human evaluation for the completeness of our evaluation. In this section, we propose the EB-M metric. Since 5.2 Definition of EB-C the key idea is to compare the generation quality We propose EB-C as a conditional counterpart of with different types of prefixes, denoting the prefix EB-M. Again, let PH ∈ {PM , PD } denote the distribution as PH ∈ {PM , PD } (model or data preprefix distribution. With a given prefix length l, fixes), we first formalize the following 3-step genwe first define the conditional generation deviation eration process: (1) Sample"
2021.emnlp-main.415,2020.acl-main.326,0,0.0282857,"with various metrics to quantify the impact from exposure bias. On the contrary to our expectation, our measurements consistently show that removing the prefix discrepancy only brings limited gain, and the incremental distortion is not observed. Moreover, our analysis reveals an interesting self-recovery ability of the LM, which we hypothesize to be countering the harmful effects from exposure bias. 2 Related Works comparing the model’s performance on seen (training) data and unseen (test) data. However, this methodology is more about the generalization gap, instead of exposure bias. Finally, Wang and Sennrich (2020) discusses the potential link between exposure bias and hallucination in the context of machine translation. In a relevant direction to answer the second question, several recent works attempt to evaluate whether the non-MLE training methods can really give superior NLG performance than standard MLE training for open-ended text generation. Caccia et al. (2018) tunes a “temperature” parameter in the softmax output, and evaluates models over the whole quality-diversity spectrum. Semeniuta et al. (2018) proposes to use “Reverse Language Model score” or “Frechet InferSent Distance” to evaluate the"
2021.woah-1.12,D16-1120,0,0.0718591,"Missing"
2021.woah-1.12,S19-2082,0,0.0489843,"Missing"
2021.woah-1.12,D19-1418,0,0.128911,"TLD models; when the models are trained on the biased datasets, these biases are inherited by the models and further exacerbated during the learning process (Zhou et al., 2021). The biases in TLD systems can make the opinions from the members of minority groups more likely to be removed by the online platform, which may significantly hinder their experience as well as exacerbate the discrimination against them in real life. So far, many debiasing methods have been developed to mitigate biases in learned models, such as data re-balancing (Dixon et al., 2018), residual fitting (He et al., 2019; Clark et al., 2019), adversarial training (Xia et al., 2020) and data filtering approach (Bras et al., 2020; Zhou et al., 2021). While most of these works are successful on other natural language processing (NLP) tasks, their performance on debasing the TLD tasks are unsatisfactory (Zhou et al., 2021). A possible reason is that the toxicity of language is more subjective and nuanced than general NLP tasks that often have unequivocally correct labels (Zhou et al., 2021). As current debiasing techniques reduce the biased behaviors of models by correcting the training data or measuring the difficulty of modeling th"
2021.woah-1.12,D19-6115,0,0.0239427,"datasets for the TLD models; when the models are trained on the biased datasets, these biases are inherited by the models and further exacerbated during the learning process (Zhou et al., 2021). The biases in TLD systems can make the opinions from the members of minority groups more likely to be removed by the online platform, which may significantly hinder their experience as well as exacerbate the discrimination against them in real life. So far, many debiasing methods have been developed to mitigate biases in learned models, such as data re-balancing (Dixon et al., 2018), residual fitting (He et al., 2019; Clark et al., 2019), adversarial training (Xia et al., 2020) and data filtering approach (Bras et al., 2020; Zhou et al., 2021). While most of these works are successful on other natural language processing (NLP) tasks, their performance on debasing the TLD tasks are unsatisfactory (Zhou et al., 2021). A possible reason is that the toxicity of language is more subjective and nuanced than general NLP tasks that often have unequivocally correct labels (Zhou et al., 2021). As current debiasing techniques reduce the biased behaviors of models by correcting the training data or measuring the diff"
2021.woah-1.12,2021.ccl-1.108,0,0.0774712,"Missing"
2021.woah-1.12,D18-1302,0,0.0248916,"erficial correlation at the level of syntax and semantics, and makes the toxicity detector learn to use generalizable features for prediction, thus effectively reducing the impact of dataset biases and yielding a fair TLD model. 2 Previous works Debiasing the TLD Task Researchers have proposed a range of debiasing methods for the TLD task. Some of them try to mitigate the biases by processing the training dataset. For example, Dixon et al. (2018) add additional non-toxic examples containing the identity terms highly correlated to toxicity to balance their distribution in the training dataset. Park et al. (2018) use the combination of debiased word2vec and gender swap data augmentation to reduce the gender bias in TLD task. Badjatiya et al. (2019) apply the strategy of replacing the bias sensitive words (BSW) in training data based on multiple knowledge generalization. Some researchers pay more attention to modifying the models and learning less biased features. Xia et al. (2020) use adversarial training to reduce the tendency of the TLD system to misclassify the AAE texts as toxic speech. Mozafari et al. (2020) propose a novel re-weighting mechanism to alleviate the racial bias in English tweets. Va"
2021.woah-1.12,P19-1163,0,0.0535228,"Missing"
2021.woah-1.12,2020.acl-main.770,0,0.0148829,"t the model from picking up the spurious correlation between the certain trigger-words and toxicity labels. Debiasing Other NLP Task There are many methods proposed to mitigate the biases in NLP tasks other than TLD. Clark et al. (2019) train a robust classifier in an ensemble with a bias-only model to learn the more generalizable patterns in training dataset, which are difficult to be learned by the naive bias-only model. Bras et al. (2020) develop AFLITE, an iterative greedy algorithm that can adversarially filter the biases from the training dataset, as well as the framework to support it. Utama et al. (2020) introduce a novel approach of regularizing the confidence of models on the biased examples, which successfully makes the models perform well on both in-distribution and out-of-distribution data. 3 Invariant Rationalization 3.1 Basic Formulation for Rationalization We propose TLD debiasing based on I NV R AT in this paper. The goal of rationalization is to find a subset of inputs that 1) suffices to yield the same outcome 2) is human interpretable. Normally, we would prefer to find rationale in unsupervised ways because the lack of such annotations in the data. A typical formulation to find ra"
2021.woah-1.12,2020.socialnlp-1.2,0,0.0807152,"TM) (Bojkovsk`y and Pikuliak, 2019), logistic regression (Davidson et al., 2017) and fine-tuning BERT (d’Sa et al., 2020). However, the existing TLD systems exhibit some problematic and discriminatory behaviors (Zhou ∗ * Work is not related to employment at Amazon. The source code is available at https://github. com/voidism/invrat_debias. 1 et al., 2021). Experiments show that the tweets containing certain surface markers, such as identity terms and expressions in African American English (AAE), are more likely to be classified as hate speech by the current TLD systems (Davidson et al., 2017; Xia et al., 2020), although some of them are not actually hateful. Such an issue is predominantly attributed to the biases in training datasets for the TLD models; when the models are trained on the biased datasets, these biases are inherited by the models and further exacerbated during the learning process (Zhou et al., 2021). The biases in TLD systems can make the opinions from the members of minority groups more likely to be removed by the online platform, which may significantly hinder their experience as well as exacerbate the discrimination against them in real life. So far, many debiasing methods have b"
C16-1163,C16-1237,1,0.746099,"Missing"
C16-1163,P15-2114,0,0.0146485,". Table 1: A re-ranking example: we report the Google rank (G), the gold standard relevance (GS) and our rank (R) for each question. et al., 2016), which exploits tree kernel function itself to auto-filter the non relevant subtrees. The main difference with the approach we present in the current paper is the use of neural networks for learning attention weights and thus modeling sentence or word pruning. Neural Approaches Recent work has shown the effectiveness of neural models for answer selection (Severyn and Moschitti, 2015; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. For instance, dos Santos et al. (2015) used CNN and bag-of-words (BOW) representations of original and related questions in order to compute cosine similarity scores. Recently, Bahdanau et al. (2014) presented a neural attention model for machine translation and showed that the attention mechanism is helpful for addressing long sentences. We use an LSTM model (Hochreiter and Schmidhuber, 1997) with an attention mechanism for capturing long dependencies in questions for the question similarity task. The major difference with previous work is that we exploit the"
C16-1163,P08-1019,0,0.0283351,"s our learning-to-rank approach. Section 4 describes the application of LSTMs in TK-based ranking models. Section 5 describes our text selection strategies. Section 6 discusses our experiments and the obtained results. Finally, Section 7 concludes the paper. 2 Related Work Question ranking in cQA has been central in the research community practically since the begining of cQA system design. Beside “standard” similarity measures, different characterizations and models have been explored. For instance, Cao et al. (2008) proposed a question recommendation system based on the questions’ topic and Duan et al. (2008) added the question’s focus into the formula. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topic distribution to retrieve similar historical questions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a col"
C16-1163,P15-1097,1,0.787031,"ranslation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text selection for improving cQA or QA systems is (Barr´on-Cede˜no 1735 Original Question qo : What are the tourist places in Qatar? I’m likely to travel in th"
C16-1163,S16-1172,1,0.882623,"uestions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text select"
C16-1163,S16-1126,0,0.0424839,"s is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, with the use of the KeLP platform (Filice et al., 2015a). The latter two models were applied on the SemEval 2016 Task 3 challenge on cQA (Nakov et al., 2016), which proposed a task on question ranking (together with one on answer ranking). The best-performing system in this task was the one from Franco-Salvador et al. (2016), which used SVMrank (Joachims, 2006) on a manifold of features, including distributed representations and semantic resources. To our knowledge, the only work exploring text selection for improving cQA or QA systems is (Barr´on-Cede˜no 1735 Original Question qo : What are the tourist places in Qatar? I’m likely to travel in the month of June. Just wanna know some good places to visit. G GS R Retrieved Questions 1 -1 8 The Qatar banana island will be transfered by the end of 2013 to 5 stars resort called Anantara. Has anyone seen this island? Where is it? Is it near to Corniche? 2 +1 2 Is there"
C16-1163,W01-0515,0,0.030947,"gure 1: Representation of two questions as syntactic trees. Related nodes are enriched with REL links. 3.4 Feature Vectors We combine the kernel above with an RBF kernel applied to feature vectors composed of similarity features. These are computed between the original and the related question and the Google rank. Such text similarity features (sim) are 20 similarities sim(qo , qs ) using word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also add a structural similarity obtained by comparing the syntactic trees of the questions of an example pair using the partial tree kernel, i.e., T K(t(qo , qs ), t(qs , qo )). Note that the operands of the kernel function are members of the same pair. The ranking-based feature (rank) is computed using the ranking generated by the baseline Google search engine system. Each candidate question is located in one position in the range [1, . . . , 10]. We exploit this information as the inverse of the position. 4 Long Short-Term Memory Networks for TK-based Reranking A"
C16-1163,S15-2047,1,0.587169,"vance (GS) and our rank (R) for each question. et al., 2016), which exploits tree kernel function itself to auto-filter the non relevant subtrees. The main difference with the approach we present in the current paper is the use of neural networks for learning attention weights and thus modeling sentence or word pruning. Neural Approaches Recent work has shown the effectiveness of neural models for answer selection (Severyn and Moschitti, 2015; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. For instance, dos Santos et al. (2015) used CNN and bag-of-words (BOW) representations of original and related questions in order to compute cosine similarity scores. Recently, Bahdanau et al. (2014) presented a neural attention model for machine translation and showed that the attention mechanism is helpful for addressing long sentences. We use an LSTM model (Hochreiter and Schmidhuber, 1997) with an attention mechanism for capturing long dependencies in questions for the question similarity task. The major difference with previous work is that we exploit the weights learned by the attention model for selecting important text seg"
C16-1163,N16-1152,1,0.849591,"in Section 2, several neural approaches have been successfully applied to QA tasks. Unfortunately, question retrieval in cQA is heavily affected by a large amount of noise and a rather different domain, which make it difficult to effectively use out-of-domain embeddings to pre-train neural networks. This probably prevented the participants to SemEval tasks from achieving satisfactory results with such models (Nakov et al., 2016). In this work, we also tried to exploit neural models using their top-level representations for the (qo , qs ) pair and fed them into the TK classifier as proposed by Tymoshenko et al. (2016), but this simple combination proved to be ineffective as well. In contrast, neural embeddings and weights can be useful for selecting better representations for TK models. In the reminder of this section, we present LSTM networks for question retrieval and our approach for incorporating them into TK-based rerankers. We approach question ranking as a classification task: given a pair (qo , qs ), we need to classify qs as relevant or irrelevant. In order to evaluate the neural classifiers on our ranking task, we can rank candidates, qs , according to their posterior probability. Among the diffe"
C16-1163,P06-1051,1,0.693051,"ining examples, αi are weights, yi are the example labels, φ(qoi , qsi ) is the representation of pairs of the original and candidate questions. This leads to the following scoring function: r(qo , qs ) = n X αi yi φ(qo , qs ) · φ(qoi , qsi ) = i=1 n X  αi yi K hqo , qs i, hqoi , qsi i , i=1 where the kernel, K(·, ·), intends to capture the similarity between pairs of objects constituted by the original and retrieved questions. The definition of effective Ks for QA and other relational learning tasks, e.g., textual entailment and paraphrasing, has been studied in a large body of work, e.g., (Zanzotto and Moschitti, 2006; Filice et al., 2015b). Given the high similarity between question ranking in cQA and passage ranking in QA, we opted for the state-of-the-art model proposed by Severyn and Moschitti (2012). It should be noted that we apply TK models to pairs of questions rather than questions with their passages. Figure 1 displays an example of the structure we used for representing the original question, qo and the seventh candidate question, qs , in Table 1. The graph is composed by two macro-trees, one for each question, which in turn are constituted by the syntactic trees of the sentences composing the t"
C16-1163,P11-1066,0,0.015238,"have been explored. For instance, Cao et al. (2008) proposed a question recommendation system based on the questions’ topic and Duan et al. (2008) added the question’s focus into the formula. A different approach using topic modeling for question retrieval was introduced by Ji et al. (2012) and Zhang et al. (2014). Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topic distribution to retrieve similar historical questions. Various methods rely on machine-translation models. For instance, Jeon et al. (2005) and Zhou et al. (2011) used monolingual phrase-based translation models to compare the questions. Jeon et al. (2005) built their translator from a collection of previously identified similar questions whereas Zhou et al. (2011) used question–answer pairs. Other approaches are based on syntactic representations. This is the case of Wang et al. (2009), who consider the number of common substructures of parse trees to estimate the similarity between two questions. Both Barr´on-Cede˜no et al. (2016) and Filice et al. (2016) use parse trees as well. The difference is that they use them directly within a tree kernel, wit"
C16-1163,S16-1138,1,\N,Missing
D08-1087,P00-1073,0,0.0348299,"ave previously assigned topic labels to each word by applying HMM-LDA (Griffiths et al., 2005) to the training documents. Using an ad hoc method to reduce the effective counts of n-grams ending on topic words, we achieved better perplexity and WER than standard trigram LMs. Intuitively, deemphasizing such n-grams will lower the transition probability to out-of-domain topic words from the training data. In this work, we further explore this intuition with a principled feature-based model, integrated with LM smoothing and estimation to allow simultaneous optimization of all model parameters. As Gao and Lee (2000) observed, even purported matched training data may exhibit topic, style, or temporal biases not present in the test set. To address the mismatch, they partition the training documents by their metadata attributes and compute a measure of the likelihood that an n-gram will appear in a new partitioned segment. By pruning n-grams with generality probability below a given threshold, the resulting model achieves lower perplexity than a count-cutoff model of equal size. Complementary to our work, this technique also utilizes segmentation and metadata information. However, our model enables the simu"
D08-1087,W04-2902,1,0.893448,"Missing"
D08-1087,H05-2015,1,0.873851,"Missing"
D08-1087,W06-1644,1,0.845526,"= 2/3. However, as the size of each partition increases, this ratio increases to 1, since most n-grams have a non-zero probability of appearing in each partition. Thus, an alternative is to compute the normalized entropy of the n-gramPdistribution across the S partitions, S −1 or h = log s=1 p(s) log p(s), where p(s) is the S fraction of an n-gram appearing in partition s. For example, the normalized entropy of the unigram C is −1 2 2 4 4 h(C) = log 3 [ 6 log 6 + 6 log 6 + 0] = .58. N -grams clustered in fewer partitions have lower entropy than ones that are more evenly spread out. Following (Hsu and Glass, 2006), we also consider features derived from the HMM-LDA word topic labels.1 Specifically, we compute the empirical probability t that the target word of the n-gram 1 HMM-LDA is performed using 20 states and 50 topics with a 3rd-order HMM. Hyperparameters are sampled with a lognormal Metropolis proposal. The model with the highest likelihood from among 10,000 iterations of Gibbs sampling is used. 831 k means the sun this is a a lot of big o of emf Feature Random log(c) f doc f course f speaker hdoc hcourse hspeaker tdoc tcourse tspeaker i think To address the issue of sparsity in the document topi"
D08-1087,P06-1004,0,0.0815409,"Missing"
D13-1019,P12-1005,1,0.493049,"(Killer et al., 2003), in which each grapheme is listed as a question, to train a triphone grapheme model. Note that to enforce better initial alignments between the graphemes and the speech data, we use a pre-trained acoustic model to identify the non-speech segments at the beginning and the end of each utterance before starting training the monophone grapheme model. Our model jointly discovers the phonetic inventory and the L2S mapping rules from a set of transcribed data. An alternative of our approach is to learn the two latent structures sequentially. We follow the training procedure of Lee and Glass (2012) to learn a set of acoustic models from the speech data and use these acoustic models to generate a phone transcription for each utterance. The phone transcriptions along with the corresponding word transcriptions are fed as inputs to the L2S model proposed in Bisani and Ney (2008). A stochastic lexicon can be learned by applying the L2S model 189 Monophone 17.0 13.8 32.7 31.4 Table 2: Word error rates generated by the four monophone recognizers described in Sec. 5.2 and Sec. 5.3 on the weather query corpus. and the discovered acoustic models to PMM. This two-stage approach for training a spee"
D13-1019,P08-2042,0,0.0607242,"Missing"
D13-1019,H94-1062,0,0.273472,"Missing"
D15-1274,W14-3608,0,0.0691264,"presents a problem for many language processing tasks, including acoustic modeling for speech recognition, language modeling, text-to-speech, and morphological analysis. Automatic methods for diacritization aim to restore diacritics in a non-diacritized text. While earlier work used rule-based methods, more recent studies attempted to learn a diacritization model from diacritized text. A variety of methods have been used, including hidden Markov models, finite-state transducers, and maximum entropy – see the review in (Zitouni and Sarikaya, 2009) – and more recently, deep neural networks (Al Sallab et al., 2014). In addition to learning from diacritized text, these methods typically rely on external resources such as part-of-speech taggers and morphological analyzers like the MADA tool (Habash and Rambow, 2007). However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains. 1 Arabic transliteration follows the Buckwalter scheme: http://www.qamus.org/transliteration.htm. 2281 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281–2285, c Lisbon, Portugal, 17-21 September 2015. 2015 Associatio"
D15-1274,N07-2014,0,0.0836748,"tization aim to restore diacritics in a non-diacritized text. While earlier work used rule-based methods, more recent studies attempted to learn a diacritization model from diacritized text. A variety of methods have been used, including hidden Markov models, finite-state transducers, and maximum entropy – see the review in (Zitouni and Sarikaya, 2009) – and more recently, deep neural networks (Al Sallab et al., 2014). In addition to learning from diacritized text, these methods typically rely on external resources such as part-of-speech taggers and morphological analyzers like the MADA tool (Habash and Rambow, 2007). However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains. 1 Arabic transliteration follows the Buckwalter scheme: http://www.qamus.org/transliteration.htm. 2281 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281–2285, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Diacritic X X X X X X X X Transliteration a Transcription /a/ u /u/ i /i/ F /an/ N /un K /in/ ~ Gemination o No vowel l1,...,lT Output layer Softmax h1,...,hT Hidden"
D15-1274,2006.bcs-1.4,0,0.293541,"art methods that have access to additional resources. 1 Introduction Hebrew, Arabic, and other languages based on the Arabic script usually represent only consonants in writing and do not mark vowels. In such writing systems, diacritics are used for marking short vowels, gemination, and other phonetic units. In practice, diacritics are usually restricted to specific settings such as language teaching or to religious texts. Faced with a non-diacritized word, readers infer missing diacritics based on their prior knowledge and the context of the word in order to resolve ambiguities. For example, Maamouri et al. (2006) mention several types of ambiguity for the Arabic string ÕÎ « Elm, both within and across part-of-speech tags, and at a grammatical Gloss he knew it was known he taught knowledge (def.nom) ... knowledge (indef.gen) flag (def.nom) ... flag (indef.gen) Table 1: Possible diacritized forms for ÕÎ« Elm. level. In practice, a morphological analyzer like MADA (Habash et al., 2009) produces at least 13 different diacritized forms for this word, a subset of which is shown in Table 1.1 The ambiguity in Arabic orthography presents a problem for many language processing tasks, including acoustic modeling"
D15-1274,P06-1073,0,\N,Missing
D18-1389,N18-2004,1,0.801772,"16). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2"
D18-1389,C18-1285,0,0.10102,"Missing"
D18-1389,S17-2006,0,0.117489,"Missing"
D18-1389,C18-1284,0,0.0612131,"increasingly important. For example, the ACM Transactions on Information Systems journal dedicated, in 2016, a special issue on Trust and Veracity of Information in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been"
D18-1389,C18-1158,0,0.0461899,"8; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018; Zubiaga et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018). 2.3 Source Reliability Estimation Unlike stance detection, the problem of source reliability remains largely under-explored. In the case of social media, it concerns modeling the user2 who posted a particular message/tweet, while in the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is our focus in this paper. In previous work, the source"
D18-1389,karadzhov-etal-2017-built,1,0.905758,"ormation in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017;"
D18-1389,karadzhov-etal-2017-fully,1,0.861555,"ormation in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017;"
D18-1389,C18-1288,0,0.0222688,"nt. For example, the ACM Transactions on Information Systems journal dedicated, in 2016, a special issue on Trust and Veracity of Information in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in i"
D18-1389,P17-1066,0,0.0303452,"ing are becoming increasingly important. For example, the ACM Transactions on Information Systems journal dedicated, in 2016, a special issue on Trust and Veracity of Information in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance"
D18-1389,K15-1032,1,0.839773,"ing manual gold annotations specific for the task. Note that estimating the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three small datasets ranging from a couple of hundred to a few thousand articles from a couple of dozen sources, comparing (i) real news vs. (ii) “fake news” vs. (iii) sat"
D18-1389,R15-1058,1,0.866485,"ing manual gold annotations specific for the task. Note that estimating the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three small datasets ranging from a couple of hundred to a few thousand articles from a couple of dozen sources, comparing (i) real news vs. (ii) “fake news” vs. (iii) sat"
D18-1389,P16-2065,1,0.838705,"ting the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three small datasets ranging from a couple of hundred to a few thousand articles from a couple of dozen sources, comparing (i) real news vs. (ii) “fake news” vs. (iii) satire, and found that the latter two have a lot in common across a numbe"
D18-1389,N13-1090,0,0.0157775,"as of a target medium. For example, the absence of a Wikipedia page may indicate that a website is not credible. Also, the content of the page might explicitly mention that a certain website is satirical, left-wing, or has some property related to our task. 3531 • Counts: Statistics about the number of friends, statuses, and favorites. Established media might have higher values for these. Accordingly, we extract the following features: • Has Page: indicates whether the target medium has a Wikipedia page; • Description: A vector representation generated by averaging the Google News embeddings (Mikolov et al., 2013) of all words of the profile description paragraph. These short descriptions might contain an open declaration of partisanship, i.e., left or right political ideology (bias). This could also help predict factuality as extreme partisanship often implies low factuality. In contrast, “fake news” media might just leave this description empty, while high-quality media would want to give some information about who they are. • Vector representation for each of the following segments of the Wikipedia page, whenever applicable: Content, Infobox, Summary, Categories, and Table of Contents. We generate t"
D18-1389,N18-1070,1,0.825569,"b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018; Zubiaga et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018). 2.3 Source Reliability Estimation Unlike stance detection, the problem of source reliability remains largely under-explored. In the case of social media, it concerns modeling the user2 who posted a particular message/tweet, while in the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is our focus in this paper. I"
D18-1389,C18-1287,0,0.234004,"orthy source is one that contains very few false facts. In this paper, we follow a different approach by studying the source reliability as a task in its own right, using manual gold annotations specific for the task. Note that estimating the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three sm"
D18-1389,P13-1162,0,0.639986,"given target medium should be critical for assessing the factuality of its reporting, as well as of its potential bias. Towards this goal, we borrow a set of 141 features that were previously proposed for detecting “fake news” articles (Horne et al., 2018b), as we have described above. These features are used to analyze the following article characteristics: • Structure: POS tags, linguistic features based on the use of specific words (function words, pronouns, etc.), and features for clickbait title classification from (Chakraborty et al., 2016); • Sentiment: sentiment scores using lexicons (Recasens et al., 2013; Mitchell et al., 2013) and full systems (Hutto and Gilbert, 2014); Media Bias Detection As we mentioned above, bias was used as a feature for “fake news” detection (Horne et al., 2018b). It has also been the target of classification, e.g., Horne et al. (2018a) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Finally, Rashkin et al. (2017) studied propaganda, which can be seen as extreme bias. See also a recent positi"
D18-1389,S17-2088,1,0.866619,"Missing"
D18-1389,W17-4214,0,0.0904481,"aradzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018; Zubiaga et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018). 2.3 Source Reliability Estimation Unlike stance detection, the problem of source reliability remains largely under-explored. In the case of social media, it concerns modeling the user2 who posted a particular message/tweet, while in the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is ou"
D18-1389,C18-1283,0,0.0694362,"dual (groups of) features. There have also been some related shared tasks such as the SemEval-2017 task 8 on Rumor Detection (Derczynski et al., 2017), the CLEF-2018 lab on Automatic Identification and Verification of Claims in Political Debates (Atanasova et al., 2018; Barrón-Cedeño et al., 2018; Nakov et al., 2018), and the FEVER-2018 task on Fact Extraction and VERification (Thorne et al., 2018). The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey was run by Thorne and Vlachos (2018), which took a fact-checking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), covering truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the process of proliferation of true and false news online. In particular, they analyzed 126K stories tweeted by 3M people more than 4.5M times, and confirmed that “fake news” spread much wider than true news. Veracity of information has been st"
D18-1389,N18-1074,0,0.0869766,"Missing"
D18-1389,P18-1022,0,0.205979,"structural, sentiment, engagement, topicdependent, complexity, bias, and morality. We use this set of features when analyzing news articles. In yet another follow-up work, Horne et al. (2018a) trained a classifier to predict whether a given news article is coming from a reliable or from an unreliable (“fake news” or conspiracy)3 source. Note that they assumed that all news from a given website would share the same reliability class. Such an assumption is fine for training (distant supervision), but we find it problematic for testing, where we believe manual documents-level labels are needed. Potthast et al. (2018) used 1,627 articles from nine sources, whose factuality has been manually verified by professional journalists from BuzzFeed. They applied stylometric analysis, which was originally designed for authorship verification, to predict factuality (fake vs. real). Rashkin et al. (2017) focused on the language used by “fake news” and compared the prevalence of several features in articles coming from trusted sources vs. hoaxes vs. satire vs. propaganda. However, their linguistic analysis and their automatic classification were at the article level and they only covered eight news media sources. 3 We"
D18-1389,D17-1317,0,0.236805,"r from an unreliable (“fake news” or conspiracy)3 source. Note that they assumed that all news from a given website would share the same reliability class. Such an assumption is fine for training (distant supervision), but we find it problematic for testing, where we believe manual documents-level labels are needed. Potthast et al. (2018) used 1,627 articles from nine sources, whose factuality has been manually verified by professional journalists from BuzzFeed. They applied stylometric analysis, which was originally designed for authorship verification, to predict factuality (fake vs. real). Rashkin et al. (2017) focused on the language used by “fake news” and compared the prevalence of several features in articles coming from trusted sources vs. hoaxes vs. satire vs. propaganda. However, their linguistic analysis and their automatic classification were at the article level and they only covered eight news media sources. 3 We show in parentheses, the labels from opensources.co that are used to define a category. 3530 Unlike the above work, (i) we perform classification at the news medium level rather than focusing on an individual article. Thus, (ii) we use reliable manually-annotated labels as oppose"
D19-1452,N18-2004,1,0.754681,"Missing"
D19-1452,E17-1024,0,0.0584382,"Missing"
D19-1452,N19-1423,0,0.0364481,"t language). In particular, we used the Fake News Challenge dataset (Hanselowski et al., 2018) as source data and an Arabic benchmark dataset (Baly et al., 2018) as target data. The evaluation results have shown 2.7 and 4.0 absolute improvement in terms of macro-F1 and weighted accuracy for stance detection over the current state-of-the-art monolingual baseline, and 11.4, 14.9, 16.1, 12.9, and 13.1 points of absolute improvement in terms of precision at ranks 1–5 for extracting evidence snippets respectively. Furthermore, a key finding in our investigation is that, in contrast to other tasks (Devlin et al., 2019; Peters et al., 2018), pre-training with large amounts of source data is less effective for cross-lingual stance detection. We show that this is because pre-training can considerably bias the model toward the source language. 2 Method Assume that we are given a training dataset for a source language, Ds , which contains a set of  s s s  N triplets as follows: Ds = (ci , di ), yi i=1 , where N is the number of source samples, (csi , dsi ) is a pair of claim csi and document dsi , and yis ∈ Y , Y = {agree, disagree, discuss, unrelated}, is the corresponding label indicating the stance of the"
D19-1452,C18-1288,0,0.085602,"Missing"
D19-1452,C18-1284,0,0.164896,"Missing"
D19-1452,D19-6603,1,0.828223,"n. Stance Detection. This is an important component for automatic fact-checking systems and veracity inference (Nadeem et al., 2019; Zhang et al., 2019; Atanasova et al., 2019). There have been some nuances in the way researchers have defined the stance detection task. Mohammad et al. (2016) and Zarrella and Marsh (2016) worked on stances regarding target propositions, e.g., entities, concepts or events, as in-favor, against, or neither. Most commonly, stance detection has been defined with respect to a claim as agree, disagree, discuss or unrelated (Hanselowski et al., 2018; Xu et al., 2018; Fang et al., 2019). Previous work mostly developed the models with rich hand-crafted features such as words, word embeddings, and sentiment lexicons (Riedel et al., 2017; Baird et al., 2017; Hanselowski et al., 2018). More recently, Mohtarami et al. (2018) presented a mono-lingual and feature-light memory network for stance detection. In this paper, we built on this work to extend previous efforts in stance detection to a cross-lingual setting, achieving the state-of-the-art result on the target language. 6 Conclusion and Future Work We proposed an effective language adaptation approach to align class labels in"
D19-1452,C18-1158,0,0.261469,"ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4442–4452, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics • Our model is able to extract accurate text snippets as evidence to explain its predictions in the target language (results are in Section 4.2). • To the best of our knowledge, this is the first work on cross-lingual stance detection. We conducted our experiments on English (as source language) and Arabic (as target language). In particular, we used the Fake News Challenge dataset (Hanselowski et al., 2018) as source data and an Arabic benchmark dataset (Baly et al., 2018) as target data. The evaluation results have shown 2.7 and 4.0 absolute improvement in terms of macro-F1 and weighted accuracy for stance detection over the current state-of-the-art monolingual baseline, and 11.4, 14.9, 16.1, 12.9, and 13.1 points of absolute improvement in terms of precision at ranks 1–5 for extracting evidence snippets respectively. Furthermore, a key finding in our investigation is that, in contrast to other tasks (Devlin et al., 2019; Peters et al., 2018), pre-training with large amounts of source data is l"
D19-1452,P17-1066,0,0.0478141,"results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance detection which aims to identify the relative perspective of a document with respect to a claim, typi"
D19-1452,S19-2149,1,0.874721,"Missing"
D19-1452,D14-1181,0,0.00235528,"ification tasks. We elaborate on these components below. Input representation component I: It encodes documents and claims into corresponding representations. Each document d is divided into a sequence of paragraphs X = (x1 , . . . , xl ), where each xj is encoded as mj using an LSTM network, and as nj using a CNN; these representations are stored in the memory component M . Note that while LSTMs are designed to capture and memorize their inputs (Tan et al., 2016), CNNs emphasize the local interaction between individual words in sequences, which is important for obtaining good representation (Kim, 2014). 4443 Figure 1: The architecture of our cross-lingual memory network for stance detection. Thus, our I component uses both LSTM and CNN representations. It also uses separate LSTM and CNN with their own parameters to represent each input claim c as clstm and ccnn , respectively. We consider each paragraph as a single piece of evidence because a paragraph usually represents a coherent argument, unified under one or more interrelated topics. We thus use the terms paragraph and evidence interchangeably. Inference component F : Our inference component computes LSTM- and CNN-based similarity betwe"
D19-1452,S16-1003,0,0.151768,"Missing"
D19-1452,N18-1070,1,0.811925,"Missing"
D19-1452,N19-4014,1,0.832314,"rage same-label examples from different domains to map nearby in the embedding space. While supervised approaches perform better than unsupervised ones, recent work (Motiian et al., 2017) has demonstrated superior performance by additionally encouraging class separation, meaning that examples from different domains and with different labels should be projected as far apart as possible in the embedding space. Here, we combined both types of alignments for cross-lingual stance detection. Stance Detection. This is an important component for automatic fact-checking systems and veracity inference (Nadeem et al., 2019; Zhang et al., 2019; Atanasova et al., 2019). There have been some nuances in the way researchers have defined the stance detection task. Mohammad et al. (2016) and Zarrella and Marsh (2016) worked on stances regarding target propositions, e.g., entities, concepts or events, as in-favor, against, or neither. Most commonly, stance detection has been defined with respect to a claim as agree, disagree, discuss or unrelated (Hanselowski et al., 2018; Xu et al., 2018; Fang et al., 2019). Previous work mostly developed the models with rich hand-crafted features such as words, word embeddings, and s"
D19-1452,N18-1202,0,0.0237588,"cular, we used the Fake News Challenge dataset (Hanselowski et al., 2018) as source data and an Arabic benchmark dataset (Baly et al., 2018) as target data. The evaluation results have shown 2.7 and 4.0 absolute improvement in terms of macro-F1 and weighted accuracy for stance detection over the current state-of-the-art monolingual baseline, and 11.4, 14.9, 16.1, 12.9, and 13.1 points of absolute improvement in terms of precision at ranks 1–5 for extracting evidence snippets respectively. Furthermore, a key finding in our investigation is that, in contrast to other tasks (Devlin et al., 2019; Peters et al., 2018), pre-training with large amounts of source data is less effective for cross-lingual stance detection. We show that this is because pre-training can considerably bias the model toward the source language. 2 Method Assume that we are given a training dataset for a source language, Ds , which contains a set of  s s s  N triplets as follows: Ds = (ci , di ), yi i=1 , where N is the number of source samples, (csi , dsi ) is a pair of claim csi and document dsi , and yis ∈ Y , Y = {agree, disagree, discuss, unrelated}, is the corresponding label indicating the stance of the document with respect"
D19-1452,E17-2088,0,0.0825794,"Missing"
D19-1452,P16-1044,0,0.0145071,"n output from the updated memory, and encodes it to a desired format in the response component R using a prediction function, e.g., softmax for classification tasks. We elaborate on these components below. Input representation component I: It encodes documents and claims into corresponding representations. Each document d is divided into a sequence of paragraphs X = (x1 , . . . , xl ), where each xj is encoded as mj using an LSTM network, and as nj using a CNN; these representations are stored in the memory component M . Note that while LSTMs are designed to capture and memorize their inputs (Tan et al., 2016), CNNs emphasize the local interaction between individual words in sequences, which is important for obtaining good representation (Kim, 2014). 4443 Figure 1: The architecture of our cross-lingual memory network for stance detection. Thus, our I component uses both LSTM and CNN representations. It also uses separate LSTM and CNN with their own parameters to represent each input claim c as clstm and ccnn , respectively. We consider each paragraph as a single piece of evidence because a paragraph usually represents a coherent argument, unified under one or more interrelated topics. We thus use t"
D19-1452,N18-1074,0,0.0360876,"atasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance detection which aims to identify the relative perspective of a document with respect to a claim, typically modeled using labels such a"
D19-1452,W14-2508,0,0.0675751,"oach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance detection which aims to identify the relative perspective of a document with respect to a claim, typically modeled using labels such as agree, disagree, discuss, and unrelated. We aim to bridge this gap by proposing a cross-lingual model for stanc"
D19-1452,P17-2067,0,0.0200162,"tively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance d"
D19-1452,S16-1074,0,0.0484617,"7) has demonstrated superior performance by additionally encouraging class separation, meaning that examples from different domains and with different labels should be projected as far apart as possible in the embedding space. Here, we combined both types of alignments for cross-lingual stance detection. Stance Detection. This is an important component for automatic fact-checking systems and veracity inference (Nadeem et al., 2019; Zhang et al., 2019; Atanasova et al., 2019). There have been some nuances in the way researchers have defined the stance detection task. Mohammad et al. (2016) and Zarrella and Marsh (2016) worked on stances regarding target propositions, e.g., entities, concepts or events, as in-favor, against, or neither. Most commonly, stance detection has been defined with respect to a claim as agree, disagree, discuss or unrelated (Hanselowski et al., 2018; Xu et al., 2018; Fang et al., 2019). Previous work mostly developed the models with rich hand-crafted features such as words, word embeddings, and sentiment lexicons (Riedel et al., 2017; Baird et al., 2017; Hanselowski et al., 2018). More recently, Mohtarami et al. (2018) presented a mono-lingual and feature-light memory network for sta"
D19-1452,D19-3038,1,0.606293,"Missing"
D19-3038,P15-2072,0,0.0737003,"Missing"
D19-3038,D19-1565,1,0.834147,"Missing"
D19-3038,E17-3016,1,0.8158,"ehow (0.4 ≤ p &lt; 0.6), likely (0.6 ≤ p &lt; 0.8), and very likely (p ≥ 0.8). Crawlers and Translation Our crawlers collect articles from a growing list of sources10 , which currently includes 155 RSS feeds, 82 Twitter accounts and two websites. Once a link to an article has been obtained from any of these sources, we rely on the Newspaper3k Python library to extract its contents.11 After deduplication based on both URL and text content, our crawlers currently download 7k-10k articles per day. As of present, we have more than 700k articles stored in our database. We use QCRI’s Machine Translation (Dalvi et al., 2017) to translate English content into Arabic and vice versa. Since translation is performed offline, we select the most accurate system in Dalvi et al. (2017), i.e., the neural-based one. 8 http://kafka.apache.org http://kubernetes.io 10 http://www.tanbih.org/about 11 http://newspaper.readthedocs.io 9 12 http://github.com/several27/ FakeNewsCorpus 224 2.4 Framing Bias Detection We implemented our stance detection model as fine-tuning of BERT on the FNC-1 dataset from the Fake News Challenge13 . Our model outperformed the best submitted system (Hanselowski et al., 2018), obtaining an F1macro of 75"
D19-3038,C18-1158,0,0.133435,"Missing"
D19-3038,D18-1389,1,0.607642,"Missing"
D19-3038,N18-5006,1,0.812156,"Missing"
D19-3038,N19-1216,1,0.55511,"Missing"
D19-3038,P17-1092,0,0.0313814,"Missing"
D19-3038,D18-1483,0,0.0187346,"11): V (u) = tf (u,C0 ) total(C0 ) 2 tf (u,C ) tf (u,C1 ) 0 total(C0 ) + total(C1 ) −1 (1) Figure 2: The Tanbih main page. where tf (u, C0 ) is the number of times (term frequency) item u is cited by group C0 , and total(C0 ) is the sum of the term frequencies of all items cited by C0 . tf (u, C1 ) and total(C1 ) are defined in a similar fashion. We subdivided the range between -1 and 1 into 5 equal size ranges and we assigned the labels far-left, left, center, right, and far-right to those ranges. 2.9 The model achieved state-of-the-art performance on the testing partition of the corpus from Miranda et al. (2018): an F1 of 98.11 and an F1BCubed of 94.41.15 As a comparison, the best model described in (Miranda et al., 2018) achieved an F1 of 94.1. See Staykovski et al. (2019) for further details. Event Identification / Clustering 3 The clustering module aggregates news articles into stories. The pipeline is divided into two stages: (i) local topic identification and (ii) longterm topic matching for story generation. For step (i), we represent each article as a TF.IDF vector, built from the title and the body concatenated. The pre-processing consists of casefolding, lemmatization, punctuation and stopwo"
D19-6603,C18-1158,0,0.222171,"ised pretraining on multiple tasks, and then fine-tune the resulting model on our target stance prediction task. For journalists and news agencies, fact checking is the task of assessing the veracity of information and claims. Due to the large volume of claims, automating this process is of great interest to the journalism and NLP communities. A main component of automated fact-checking is stance detection which aims to automatically determine the perspective (stance) of given documents with respect to given claims as agree, disagree, discuss, or unrelated. Previous work (Riedel et al., 2017; Hanselowski et al., 2018; Baird et al., 2017; Chopra et al., 2017; Mohtarami et al., 2018; Xu et al., 2018) presented various neural models for stance prediction. One of the challenges for these models is the limited size of human-labeled data, which can adversely affect the resulting performance for this task. To overcome this limitation, we propose to supplement data from other similar Natural Language Processing (NLP) tasks. However, this is not a straightforward process due to differences between NLP tasks and data sources. We address this problem using an effective multi-task learning approach which shows sizabl"
D19-6603,D15-1075,0,0.0407375,"opy loss at each output layer. We use the Adam optimizer (Kingma and Ba, 2014) with learning rate of 3e-5, β1 = 0.9, β2 = 0.999, and mini-batch size of 16 for 10 epochs. For the SAN answer module we set K = 5 and use stochastic dropout rate of 0.1. 3 Experiments 3.1 Data The BERT model was pre-trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia. For multi-task pre-training, we use the following datasets: SNLI Stanford Natural Language Inference is the standard entailment classification task that contains 549K training sentence pairs after removing examples with no gold labels (Bowman et al., 2015). The relation labels are entailment, contradiction, and neutral. MNLI Multi-genre Natural Language Inference is a large-scale entailment classification task from a diverse set of sources with the same relation classes as SNLI (Williams et al., 2018). We use its training set that contains 393K pairs of sentences. RTE Recognizing Textual Entailment is a binary entailment task with 2.5K training examples (Wang et al., 2019). 3.2 Evaluation Metrics For evaluation, the standard measures of accuracy and macro-F1 are used. Additionally, as per previous work, weighted accuracy is also reported, which"
D19-6603,D17-1206,0,0.0310179,"ansfer learning have been long-studied problems in machine learning and NLP (Caruana, 1997; Collobert and Weston, 2008; Pan and Yang, 2010). More recently, numerous methods on unsupervised pre-training of deep contextualized models for transfer learning have been proposed (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019; Dai et al., 2019; Liu et al., 2019), and (Conneau et al., 2017; McCann et al., 2017) presented supervised pretraining methods for NLI and translation. Recent work on multi-task learning has focused on designing effective neural architectures (Hashimoto et al., 2017; Søgaard and Goldberg, 2016; Sanh et al., 2018; Ruder et al., 2017). Combining these two lines of work, (Liu et al., 2019; Clark et al., 2019) explored fine-tuning the contextualized models with multiple natural language understanding tasks. In this work, we depart from previous works by specifically studying the effects of multi-task fine-tuning for the stance prediction task with pre-trained models. Related Work Conclusion and Future Work We present an effective multi-task learning model that transfers knowledge from existing NLP tasks to improve stance prediction. Our model outperforms sta"
D19-6603,P19-1595,0,0.0142079,"recently, numerous methods on unsupervised pre-training of deep contextualized models for transfer learning have been proposed (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019; Dai et al., 2019; Liu et al., 2019), and (Conneau et al., 2017; McCann et al., 2017) presented supervised pretraining methods for NLI and translation. Recent work on multi-task learning has focused on designing effective neural architectures (Hashimoto et al., 2017; Søgaard and Goldberg, 2016; Sanh et al., 2018; Ruder et al., 2017). Combining these two lines of work, (Liu et al., 2019; Clark et al., 2019) explored fine-tuning the contextualized models with multiple natural language understanding tasks. In this work, we depart from previous works by specifically studying the effects of multi-task fine-tuning for the stance prediction task with pre-trained models. Related Work Conclusion and Future Work We present an effective multi-task learning model that transfers knowledge from existing NLP tasks to improve stance prediction. Our model outperforms state-of-the-art systems by 6.0 and 14.4 points in weighted accuracy and macro-F1 respectively on the FNC-1 benchmark dataset. In future, we plan"
D19-6603,P19-1441,0,0.0174738,"stance prediction task. In this work, we remove this limitation and used labeled data from other tasks that are similar to stance prediction through multi-task learning. 4 5 Multi-task and Transfer Learning. Multi-task and transfer learning have been long-studied problems in machine learning and NLP (Caruana, 1997; Collobert and Weston, 2008; Pan and Yang, 2010). More recently, numerous methods on unsupervised pre-training of deep contextualized models for transfer learning have been proposed (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019; Dai et al., 2019; Liu et al., 2019), and (Conneau et al., 2017; McCann et al., 2017) presented supervised pretraining methods for NLI and translation. Recent work on multi-task learning has focused on designing effective neural architectures (Hashimoto et al., 2017; Søgaard and Goldberg, 2016; Sanh et al., 2018; Ruder et al., 2017). Combining these two lines of work, (Liu et al., 2019; Clark et al., 2019) explored fine-tuning the contextualized models with multiple natural language understanding tasks. In this work, we depart from previous works by specifically studying the effects of multi-task fine-tuning for the stance predi"
D19-6603,P18-1157,0,0.0133,"sentences are packed and fed into the encoder and the embedding corresponding to the [CLS] token is used to predict whether they are adjacent sentences (Devlin et al., 2019). Task-specific Output Layers For singlesentence classification tasks, we take the vector from the first column in C, corresponding to the special token [CLS], as the semantic representation of the input sentence x. We then feed this vector through a linear layer followed by softmax to obtain the prediction probabilities. For pairwise classification tasks, we use the answer module from the stochastic answer network (SAN) (Liu et al., 2018) as the output classifier. It performs K-step reasoning over the two pieces of text with bi-linear attention and a recurrent mechanism, producing output predictions at each step and iteratively refining its predictions. At training time, some predictions are randomly discarded (stochastic dropout) before averaging, and during inference all output probabilities are utilized. In addition to learning contextual representations under an unsupervised setting with large data, we investigate whether existing NLP tasks that are conceptually similar to stance prediction can improve performance. We intr"
D19-6603,D17-1070,0,0.0332708,"In this work, we remove this limitation and used labeled data from other tasks that are similar to stance prediction through multi-task learning. 4 5 Multi-task and Transfer Learning. Multi-task and transfer learning have been long-studied problems in machine learning and NLP (Caruana, 1997; Collobert and Weston, 2008; Pan and Yang, 2010). More recently, numerous methods on unsupervised pre-training of deep contextualized models for transfer learning have been proposed (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019; Dai et al., 2019; Liu et al., 2019), and (Conneau et al., 2017; McCann et al., 2017) presented supervised pretraining methods for NLI and translation. Recent work on multi-task learning has focused on designing effective neural architectures (Hashimoto et al., 2017; Søgaard and Goldberg, 2016; Sanh et al., 2018; Ruder et al., 2017). Combining these two lines of work, (Liu et al., 2019; Clark et al., 2019) explored fine-tuning the contextualized models with multiple natural language understanding tasks. In this work, we depart from previous works by specifically studying the effects of multi-task fine-tuning for the stance prediction task with pre-trained"
D19-6603,P19-1285,0,0.0540217,"Missing"
D19-6603,P11-1015,0,0.212805,"Missing"
D19-6603,N19-1423,0,0.168829,"tween the tokenized sequences. Another special token [CLS] is inserted at the beginning of the sequence, which corresponds to the representation of the entire sequence. 13 Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 13–19 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics Figure 1: The architecture of our multi-task learning model for stance prediction. Transformer Encoder We use a bidirectional Transformer encoder that takes x as input and produces contextual embedding vectors C ∈ Rd×l via multiple layers of self-attention (Devlin et al., 2019). tion, where two sentences are packed and fed into the encoder and the embedding corresponding to the [CLS] token is used to predict whether they are adjacent sentences (Devlin et al., 2019). Task-specific Output Layers For singlesentence classification tasks, we take the vector from the first column in C, corresponding to the special token [CLS], as the semantic representation of the input sentence x. We then feed this vector through a linear layer followed by softmax to obtain the prediction probabilities. For pairwise classification tasks, we use the answer module from the stochastic answe"
D19-6603,I05-5002,0,0.133615,"Missing"
D19-6603,N18-1070,1,0.849098,"Missing"
D19-6603,N18-1074,0,0.0571292,"Missing"
D19-6603,D19-1452,1,0.875077,"Missing"
D19-6603,N18-1202,0,0.0331489,"ain adaptation with more labeled data, but they limited their model to only using data from the same stance prediction task. In this work, we remove this limitation and used labeled data from other tasks that are similar to stance prediction through multi-task learning. 4 5 Multi-task and Transfer Learning. Multi-task and transfer learning have been long-studied problems in machine learning and NLP (Caruana, 1997; Collobert and Weston, 2008; Pan and Yang, 2010). More recently, numerous methods on unsupervised pre-training of deep contextualized models for transfer learning have been proposed (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019; Dai et al., 2019; Liu et al., 2019), and (Conneau et al., 2017; McCann et al., 2017) presented supervised pretraining methods for NLI and translation. Recent work on multi-task learning has focused on designing effective neural architectures (Hashimoto et al., 2017; Søgaard and Goldberg, 2016; Sanh et al., 2018; Ruder et al., 2017). Combining these two lines of work, (Liu et al., 2019; Clark et al., 2019) explored fine-tuning the contextualized models with multiple natural language understanding tasks. In this work, we depart from"
D19-6603,N18-1101,0,0.0240139,"3 Experiments 3.1 Data The BERT model was pre-trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia. For multi-task pre-training, we use the following datasets: SNLI Stanford Natural Language Inference is the standard entailment classification task that contains 549K training sentence pairs after removing examples with no gold labels (Bowman et al., 2015). The relation labels are entailment, contradiction, and neutral. MNLI Multi-genre Natural Language Inference is a large-scale entailment classification task from a diverse set of sources with the same relation classes as SNLI (Williams et al., 2018). We use its training set that contains 393K pairs of sentences. RTE Recognizing Textual Entailment is a binary entailment task with 2.5K training examples (Wang et al., 2019). 3.2 Evaluation Metrics For evaluation, the standard measures of accuracy and macro-F1 are used. Additionally, as per previous work, weighted accuracy is also reported, which is a two-level scoring scheme that gives 0.25 weight to predicting examples as related v.s. unrelated correctly, and an additional 0.75 weight to classifying related examples as agree, disagree, and discuss correctly. 2 https://data.quora.com/ First"
D19-6603,D16-1264,0,0.0608964,"Missing"
D19-6603,D13-1170,0,0.00770664,"Missing"
D19-6603,P16-2038,0,0.0357845,"n long-studied problems in machine learning and NLP (Caruana, 1997; Collobert and Weston, 2008; Pan and Yang, 2010). More recently, numerous methods on unsupervised pre-training of deep contextualized models for transfer learning have been proposed (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019; Dai et al., 2019; Liu et al., 2019), and (Conneau et al., 2017; McCann et al., 2017) presented supervised pretraining methods for NLI and translation. Recent work on multi-task learning has focused on designing effective neural architectures (Hashimoto et al., 2017; Søgaard and Goldberg, 2016; Sanh et al., 2018; Ruder et al., 2017). Combining these two lines of work, (Liu et al., 2019; Clark et al., 2019) explored fine-tuning the contextualized models with multiple natural language understanding tasks. In this work, we depart from previous works by specifically studying the effects of multi-task fine-tuning for the stance prediction task with pre-trained models. Related Work Conclusion and Future Work We present an effective multi-task learning model that transfers knowledge from existing NLP tasks to improve stance prediction. Our model outperforms state-of-the-art systems by 6.0"
E09-1011,P03-1021,0,0.0133069,"ian pound real. This rule is motivated by the idafa construct and its properties (see Example 6). 90 The English source is aligned to the segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrasebased SMT system MOSES. We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic. We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6. We tune using Och’s algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001). For the segmented Arabic experiments, we experiment with tuning using non-segmented Arabic as a reference. This is done by recombining the output before each tuning iteration is scored and has been shown by Badr et. al (2008) to perform better than using segmented Arabic as reference. 5.2 Scheme Baseline VP NP NP+PP NP+PP+VP NP+PP+VP+The RandT S NoS 21.6 21.3 21.9 21.5 21.9 21.8 21.8 21.5 22.2 21.8 21.3 21.0 MT 05 S NoS 23.88 23.44 23.98 23.58 2"
E09-1011,P00-1056,0,0.0607784,"...N n . All N i are also made indefinite, and the definite article is added to N n , the last noun in the chain. For example, the phrase the general chief of staff of the armed forces becomes general chief staff the armed forces. We also move all adjectives in the top noun phrase to the end of the construct. So the real value of the Egyptian pound becomes value the Egyptian pound real. This rule is motivated by the idafa construct and its properties (see Example 6). 90 The English source is aligned to the segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrasebased SMT system MOSES. We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic. We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6. We tune using Och’s algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001). For the segmented Arabic experiments, we experiment with tuning using non-segmented"
E09-1011,2001.mtsummit-papers.68,0,0.0293695,"segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrasebased SMT system MOSES. We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic. We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6. We tune using Och’s algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001). For the segmented Arabic experiments, we experiment with tuning using non-segmented Arabic as a reference. This is done by recombining the output before each tuning iteration is scored and has been shown by Badr et. al (2008) to perform better than using segmented Arabic as reference. 5.2 Scheme Baseline VP NP NP+PP NP+PP+VP NP+PP+VP+The RandT S NoS 21.6 21.3 21.9 21.5 21.9 21.8 21.8 21.5 22.2 21.8 21.3 21.0 MT 05 S NoS 23.88 23.44 23.98 23.58 23.72 23.74 23.68 23.16 Table 1: Translation Results for the News Domain in terms of the BLEU Metric. the language model, we use 35 million words from"
E09-1011,popovic-ney-2006-pos,0,0.0721749,"Missing"
E09-1011,N03-1033,0,0.00617787,"ot conflict. So, the real value of the Egyptian pound → value the Egyptian the pound the real The VP reordering rule is independent. 1. NP: All nouns, adjectives and adverbs in the noun phrase are inverted. This rule is motivated by the order of the adjective with respect to its head noun, as well as the idafa construct (see Examples 6 and 7 in Section 2.2. As a result of applying this rule, the phrase the blank computer screen becomes the screen computer blank . 5 5.1 Experiments System description For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003). We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005). On the Arabic side, we normalize the data by changing final ’Y’ to ’y’, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources. We then segment the data using MADA according to the scheme explained in Section"
E09-1011,P08-1087,0,0.0229793,"Missing"
E09-1011,P08-2039,1,0.868883,"source side to better match the target side, using predefined rules. The reordered source is then used as input to the phrase-based SMT system. This approach indirectly incorporates structure information since the reordering rules are applied on the parse trees Proceedings of the 12th Conference of the European Chapter of the ACL, pages 86–93, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 86 in combination with the reordering rules. Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al., 2008) translation, although the gains tend to decrease with increasing training data size. Section 2 provides linguistic motivation for the paper. It describes the rich morphology of Arabic, and its implications on SMT. It also describes the syntax of the verb phrase and noun phrase in Arabic, and how they differ from their English counterparts. In Section 3, we describe some of the relevant previous work. In Section 4, we present the preprocessing techniques used in the experiments. Section 5 describes the translation system, the data used, and then presents and discusses the experimental results"
E09-1011,D07-1077,0,0.515141,"English-to-Arabic Statistical Machine Translation James Glass Ibrahim Badr Rabih Zbib Computer Science and Artificial Intelligence Lab Massachusetts Institute of Technology Cambridge, MA 02139, USA {iab02, rabih, glass}@csail.mit.edu Abstract of the source sentence. Obviously, the same reordering has to be applied to both training data and test data. Despite the added complexity of parsing the data, this technique has shown improvements, especially when good parses of the source side exist. It has been successfully applied to German-toEnglish and Chinese-to-English SMT (Collins et al., 2005; Wang et al., 2007). In this paper, we propose the use of a similar approach for English-to-Arabic SMT. Unlike most other work on Arabic translation, our work is in the direction of the more morphologically complex language, which poses unique challenges. We propose a set of syntactic reordering rules on the English source to align it better to the Arabic target. The reordering rules exploit systematic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs. The first is the Subject-Verb order in independent sentences, where the preferred order in wr"
E09-1011,P97-1003,0,0.205165,"ed. This rule is motivated by the order of the adjective with respect to its head noun, as well as the idafa construct (see Examples 6 and 7 in Section 2.2. As a result of applying this rule, the phrase the blank computer screen becomes the screen computer blank . 5 5.1 Experiments System description For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003). We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005). On the Arabic side, we normalize the data by changing final ’Y’ to ’y’, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources. We then segment the data using MADA according to the scheme explained in Section 4.1. 2. PP: All prepositional phrases of the form N 1 of N 2 ...of N n are transformed to N 1 N 2 ...N n . All N i are also made indefinite, and the definite article is added to N n , the last"
E09-1011,C04-1073,0,0.36806,"Missing"
E09-1011,P05-1066,0,0.122243,"Phrase Reordering for English-to-Arabic Statistical Machine Translation James Glass Ibrahim Badr Rabih Zbib Computer Science and Artificial Intelligence Lab Massachusetts Institute of Technology Cambridge, MA 02139, USA {iab02, rabih, glass}@csail.mit.edu Abstract of the source sentence. Obviously, the same reordering has to be applied to both training data and test data. Despite the added complexity of parsing the data, this technique has shown improvements, especially when good parses of the source side exist. It has been successfully applied to German-toEnglish and Chinese-to-English SMT (Collins et al., 2005; Wang et al., 2007). In this paper, we propose the use of a similar approach for English-to-Arabic SMT. Unlike most other work on Arabic translation, our work is in the direction of the more morphologically complex language, which poses unique challenges. We propose a set of syntactic reordering rules on the English source to align it better to the Arabic target. The reordering rules exploit systematic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs. The first is the Subject-Verb order in independent sentences, where the p"
E09-1011,P05-1045,0,0.00362711,"truct (see Examples 6 and 7 in Section 2.2. As a result of applying this rule, the phrase the blank computer screen becomes the screen computer blank . 5 5.1 Experiments System description For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003). We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005). On the Arabic side, we normalize the data by changing final ’Y’ to ’y’, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources. We then segment the data using MADA according to the scheme explained in Section 4.1. 2. PP: All prepositional phrases of the form N 1 of N 2 ...of N n are transformed to N 1 N 2 ...N n . All N i are also made indefinite, and the definite article is added to N n , the last noun in the chain. For example, the phrase the general chief of staff of the armed forces becomes general chief staff t"
E09-1011,2007.mtsummit-papers.29,0,0.504093,"ic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs. The first is the Subject-Verb order in independent sentences, where the preferred order in written Arabic is Verb-Subject. The second is the noun phrase structure, where many differences exist between the two languages, among them the order of adjectives, compound nouns and genitive constructs, as well as the way definiteness is marked. The implementation of these rules is fairly straightforward since they are applied to the parse tree. It has been noted in previous work (Habash, 2007) that syntactic reordering does not improve translation if the parse quality is not good enough. Since in this paper our source language is English, the parses are more reliable, and result in more correct reorderings. We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains. This paper also investigates the effect of using morphological segmentation of the Arabic target Syntactic Reordering of the source language to better match the phrase structure of the target language has been shown to improve the perf"
E09-1011,P05-1071,0,0.33802,"processing steps leads to significant gains especially for smaller training data corpora. Sarikaya and Deng use Joint Morphological-Lexical Language Models to rerank the output of an English-to-Arabic MT system. They use regular expression-based segmentation of the Arabic so as not to run into recombination issues on the output side. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. Other work on Arabicto-English SMT tries to address the word reordering problem. Habash (2007) automatically learns syntactic reordering rules that are then applied to the Arabic side of the parallel corpora. The words are aligned in a sentence pair, then the Arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the English side. No significant improvement is lm yAkl Alwld AltfAHp not eat-3SM the-boy the-apple the boy did not eat the apple Noun Phrase The Arabic noun phr"
E09-1011,N06-2013,0,0.360351,"ith long distance reordering is to reorder the source side to better match the target side, using predefined rules. The reordered source is then used as input to the phrase-based SMT system. This approach indirectly incorporates structure information since the reordering rules are applied on the parse trees Proceedings of the 12th Conference of the European Chapter of the ACL, pages 86–93, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 86 in combination with the reordering rules. Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al., 2008) translation, although the gains tend to decrease with increasing training data size. Section 2 provides linguistic motivation for the paper. It describes the rich morphology of Arabic, and its implications on SMT. It also describes the syntax of the verb phrase and noun phrase in Arabic, and how they differ from their English counterparts. In Section 3, we describe some of the relevant previous work. In Section 4, we present the preprocessing techniques used in the experiments. Section 5 describes the translation system, the data used, and then presen"
E09-1011,D07-1091,0,0.0150685,"that plural markers and subject pronouns are not split. For example, the word wlAwlAdh (’and for his children’) is segmented into wl+ AwlAd +P:3MS. Since training is done on segmented Arabic, the output of the decoder must be recombined into its original surface form. We follow the approach of Badr et. al (2008) in combining the Arabic output, which is a non-trivial task for several reasons. First, the ending of a stem sometimes changes when a suffix is attached to it. Second, word endGeneric approaches for translating from English to more morphologically complex languages have been proposed. Koehn and Hoang (2007) propose Factored Translation Models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level. They demonstrate improvements for English-to-German and English-to-Czech. Tighter integration of features is claimed to allow for better modeling of 89 3. the: The definite article the is replicated before adjectives (see Example 5 above). So the blank computer screen becomes the blank the computer the screen. This rule is applied after NP rule abote. Note that we do not replicate the before proper names. ings are no"
E09-1011,N04-4015,0,0.0387928,"not present in the opposite direction. The only works on Englishto-Arabic SMT that we are aware of are Badr et al. (2008), and Sarikaya and Deng (2007). Badr et al. show that using segmentation and recombination as pre- and post- processing steps leads to significant gains especially for smaller training data corpora. Sarikaya and Deng use Joint Morphological-Lexical Language Models to rerank the output of an English-to-Arabic MT system. They use regular expression-based segmentation of the Arabic so as not to run into recombination issues on the output side. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. Other work on Arabicto-English SMT tries to address the word reordering problem. Habash (2007) automatically learns syntactic reordering rules that are then applied to the Arabic side of the parallel corpora. The words are aligned in a sentence pair, then the Arabic sentence is parsed to extract"
E09-1011,W96-0213,0,\N,Missing
E09-1011,P02-1040,0,\N,Missing
E09-1011,N07-2037,0,\N,Missing
H05-2015,N03-5003,0,0.0285244,"increases in the amount of audio and visual recordings of lecture material. Unlike text materials, untranscribed audio data can be tedious to browse, making it difficult to utilize the information fully without time-consuming data preparation. Moreover, unlike some other forms of spoken communication such as telephone conversations or television and radio broadcasts, lecture processing has until recently received little attention or benefit from the development of human language technology. The single biggest effort, to date, is on-going work in Japan using the Corpus of Spontaneous Japanese [1,3,4]. Lectures are particularly challenging for automatic speech recognizers because the vocabulary used within a lecture can be very technical and specialized, yet the speaking style can be very spontaneous. As a result, even if parallel text materials are available in the form of textbooks or related papers, there are significant linguistic differences between written and oral communication styles. Thus, it is a challenge to predict how a written passage might be spoken, and vice versa. By helping to focus a research spotlight on spoken lecture material, we hope to begin to overcome these and ma"
H05-2015,W04-2902,1,\N,Missing
H89-1027,H89-1026,1,0.790081,"scribes those parts of our system dealing with acoustic segmentation, phonetic classification, and lexical access, and documents its current performance on the DARPA Resource Management task [1]. SYSTEM DESCRIPTION There are three major components in the SUMMIT system, as illustrated in Figure 1. The first component transforms the speech signal into an acoustic-phonetic description. The second expands a set of baseform pronunciations into a lexical network. The final component provides natural language constraints. Our preliminary efforts in natural langauge are described in a companion paper [2]. The acoustic-phonetic and lexical components will be discussed in more detail in the following sections. Signal Lexicon Higher-Level Linguistic Knowledge Lexical Expansion Language Modeling 1 Signal Representation I Acoustic segmentation Feature Extraction & Phoneme Recognition I ,I 1oder, I 1 Decoded Utterance Figure 1: The major components of the SUMMIT system. ACOUSTIC-PHONETIC REPRESENTATION The phonetic recognition subsystem of SUMMIT takes as input the speech signal and produces as output of phonetic labels with scores indicating the system&apos;s confidence in the segments and in the accur"
H89-2008,H89-2018,1,0.612237,"always utter grammatically well-formed sentences during a spoken dialogue. Over the past six months, we have constructed the skeleton of a spoken language system. The purpose of this paper is to describe the various components of this system. In related activities, we have collected a sizeable spontaneous speech database, and have used the data for analyses, system training and evaluation. The collection and analysis of the spontaneous speech database, and the preliminary evaluation of our spoken language system are described in two companion papers that appear elsewhere in these proceedings [1,2]. TASK DESCRIPTION In order to explore issues related to a fully-interactive spoken language system, we have selected a task in which the system knows about the physical environment of a specific geographical area as well as certain objects inside this area, and can provide assistance on how to get from one location to another within this area. The system, which we call VOYAGER, currently focuses on the the city of Cambridge, Massachusetts, between MIT and Harvard University, as shown in Figure 1. It can answer a number of different types of questions about certain hotels, restaurants, hospita"
H89-2008,H89-2022,1,0.523132,"always utter grammatically well-formed sentences during a spoken dialogue. Over the past six months, we have constructed the skeleton of a spoken language system. The purpose of this paper is to describe the various components of this system. In related activities, we have collected a sizeable spontaneous speech database, and have used the data for analyses, system training and evaluation. The collection and analysis of the spontaneous speech database, and the preliminary evaluation of our spoken language system are described in two companion papers that appear elsewhere in these proceedings [1,2]. TASK DESCRIPTION In order to explore issues related to a fully-interactive spoken language system, we have selected a task in which the system knows about the physical environment of a specific geographical area as well as certain objects inside this area, and can provide assistance on how to get from one location to another within this area. The system, which we call VOYAGER, currently focuses on the the city of Cambridge, Massachusetts, between MIT and Harvard University, as shown in Figure 1. It can answer a number of different types of questions about certain hotels, restaurants, hospita"
H89-2008,H89-1027,1,0.851944,"usly reported. SPEECH RECOGNITION COMPONENT The first component of VOYAGER uses the SUMMIT speech recognition system developed in our group. SUMMIT places heavy emphasis on the extraction of phonetic information from the speech signal. It achieves speech recognition by explicitly detecting acoustic landmarks in the signal in order to facilitate acousticphonetic feature extraction. The system can be trained automatically, since it does not rely on extensive knowledge engineering. The design philosophy, implementation, and evaluation of the SUMMIT system have been described in detail previously [4]. As a result, we will only report in this paper modifications to the system since the last workshop. These include the development of a new module for lexical expansion via 52 phonological rules, and a new corrective training procedure. Lexical Expansion The original SUMMIT system used a phonological expansion capability provided to us by SRI [6]. Within the last year, however, we have decided to rewrite this part of the system in order to establish increased flexibility and speed. The new version, named MARBLE, offers several new properties. A canonic set of phonemes is represented by a set"
H89-2008,H89-1026,1,0.869519,"ech recognition and natural language components are not as yet fully integrated, we currently use a word-pair language model with a perplexity of 22 to constrain the search space. NATURAL LANGUAGE COMPONENT In the context of a spoken language system, the natural language component should perform two critical functions: 1) to provide constraint for the recognizer component, and 2) to provide an interpretation of the meaning of the sentence to the back end. Our natural language system, TINA, was specifically designed to meet these two needs. The basic design of TINA has been described elsewhere [7], and therefore will only be briefly mentioned here. Instead, we would like to focus on the issue of how to incorporate semantics into the parses. We have found that an enrichment of the parse tree with semantically loaded categories at the lower levels leads to both improved word predictions and a relatively straightforward interface with the back end. General Description The grammar is entered as a set of simple context-free rules which are automatically converted to a 53 shared network structure. The nodes in the network are augmented with constraint filters (both syntactic and semantic) th"
H89-2018,H89-2008,1,0.612917,"of the city of Cambridge, Massachusetts, between MIT and Harvard University, and can deal with several distinct concepts including directions, distance and time of travel between objects, relationships such as &quot;nearest,&quot; and simple properties such as phone numbers or types of food served. VOYAQErt also has a limited amount of discourse knowledge which enables it to respond to queries such as: &quot;How do I get there?&quot; It can also deal with certain clarification fragments such as: &quot;The bank in Harvard Square.&quot; A detailed description of the VOYAGER system can be found elsewhere in these proceedings [1]. VOYAGER is made up of three components. The first component, the SUMMIT speech recognition system [2], converts the speech signal into a set of word hypotheses. The natural language component, TINA [3], provides a linguistic interpretation of the set of words. The parse tree generated by TINA is translated into a query language form, which is used to produce a response. Currently VOYAGER can generate responses in the form of text, graphics, and synthetic speech. The back end is an enhanced version of a direction assistance program developed by Jim Davis of MIT's Media Laboratory [4]. *This r"
H89-2018,H89-1027,1,0.84152,"distinct concepts including directions, distance and time of travel between objects, relationships such as &quot;nearest,&quot; and simple properties such as phone numbers or types of food served. VOYAQErt also has a limited amount of discourse knowledge which enables it to respond to queries such as: &quot;How do I get there?&quot; It can also deal with certain clarification fragments such as: &quot;The bank in Harvard Square.&quot; A detailed description of the VOYAGER system can be found elsewhere in these proceedings [1]. VOYAGER is made up of three components. The first component, the SUMMIT speech recognition system [2], converts the speech signal into a set of word hypotheses. The natural language component, TINA [3], provides a linguistic interpretation of the set of words. The parse tree generated by TINA is translated into a query language form, which is used to produce a response. Currently VOYAGER can generate responses in the form of text, graphics, and synthetic speech. The back end is an enhanced version of a direction assistance program developed by Jim Davis of MIT's Media Laboratory [4]. *This research was supported by DARPAunder Contract N00014-89-J-1332, monitoredthrough the Officeof Naval Rese"
H89-2018,H89-1026,1,0.885551,"uch as &quot;nearest,&quot; and simple properties such as phone numbers or types of food served. VOYAQErt also has a limited amount of discourse knowledge which enables it to respond to queries such as: &quot;How do I get there?&quot; It can also deal with certain clarification fragments such as: &quot;The bank in Harvard Square.&quot; A detailed description of the VOYAGER system can be found elsewhere in these proceedings [1]. VOYAGER is made up of three components. The first component, the SUMMIT speech recognition system [2], converts the speech signal into a set of word hypotheses. The natural language component, TINA [3], provides a linguistic interpretation of the set of words. The parse tree generated by TINA is translated into a query language form, which is used to produce a response. Currently VOYAGER can generate responses in the form of text, graphics, and synthetic speech. The back end is an enhanced version of a direction assistance program developed by Jim Davis of MIT's Media Laboratory [4]. *This research was supported by DARPAunder Contract N00014-89-J-1332, monitoredthrough the Officeof Naval Research. lWe looselyuse the terra spontaneous speech to mean the speech produced by a person &quot;on the fl"
H89-2018,H89-2022,1,\N,Missing
H89-2022,H89-2008,1,0.522185,"hould be able to benefit from hands-on experience with applying some candidate performance measures to working systems. The purpose of this paper is to document our experience with the preliminary evaluation of the VOYAGEa system currently under development at MIT, so that we may contribute to the evolutionary process of defining the appropriate evaluation measures. VOYAGER is a speech understanding system that can provide information and navigational assistance for a geographical area within the city of Cambridge, Massachusetts. The components of the system are described in a companion paper [2]. To evaluate VOYAGER we made use of a spontaneous speech database that we have recently collected consisting of nearly 10,000 sentences from 100 speakers. The database is described in another companion paper [3]. E V A L U A T I O N ISSUES We believe that spoken language systems should be evaluated along several dimensions. First, the accuracy of the system and its various modules should be documented. Thus, for example, one can measure a given system&apos;s phonetic, word, and sentence accuracy, as well as linguistic and task completion accuracy. Second, one must measure the coverage and habitabi"
H89-2022,H89-2018,1,0.600561,"he VOYAGEa system currently under development at MIT, so that we may contribute to the evolutionary process of defining the appropriate evaluation measures. VOYAGER is a speech understanding system that can provide information and navigational assistance for a geographical area within the city of Cambridge, Massachusetts. The components of the system are described in a companion paper [2]. To evaluate VOYAGER we made use of a spontaneous speech database that we have recently collected consisting of nearly 10,000 sentences from 100 speakers. The database is described in another companion paper [3]. E V A L U A T I O N ISSUES We believe that spoken language systems should be evaluated along several dimensions. First, the accuracy of the system and its various modules should be documented. Thus, for example, one can measure a given system&apos;s phonetic, word, and sentence accuracy, as well as linguistic and task completion accuracy. Second, one must measure the coverage and habitability of the system. This can be applied to the lexicon, the language model, and the application back-end. Third, the system&apos;s flexibility must be established. For *This research was supported by DARPAunder Contra"
H89-2022,H89-1027,1,0.861742,"Missing"
H89-2022,H89-1026,1,0.872671,"Missing"
H90-1028,W89-0222,1,0.885214,"ire sentence has been processed and the history frames have been merged, an IDIL[2] 1 query is then constructed from the completely specified frame. We had initially constructed the query &quot;on the fly,&quot; but we found that this led to much complexity at the end because of discourse effects that would require last minute alterations on the restrictions in the query. An Example As in the VOYAGER domain, we have taken the viewpoint that the parse tree contains semantically loaded nodes at the lower levels, and these are in fact the only semantic information that is used by the interface between TINA[3] and the back-end. The presence of certain nodes within specified positions in the hierarchy triggers the execution of particular functions whose action is typically to update slots in a semantic representation or event frame. The event frame is passed as the first argument to each new function that is called, and the final event, frame then has in place all of the appropriate information to be extracted from the sentence. We will illustrate how the ATIS system converts a sentence into an event frame by walking through a single example. Consider the sentence, &quot;Show me the flights on September"
H90-1028,H89-2018,1,0.91661,"ely occur. In addition, there were no sentence fragments and no sentences using indirect speech such as &quot;I want to go....&quot; For instance, the sentence &quot;Show me the flights,&quot; gives a perplexity reduction from 300 to 5 with the use of probabilities, reflecting the fact that this is a very common form. *A word-pair grammar would give a further increase in perplexity. For Voyager the perplexity increased from 28 to 73 when long-distance constraints were ignored. Data Collection We have performed a preliminary data collection session, using a procedure that mimics closely the one we used for VOYAGER[4]. We told subjects to pretend they were a traveler planning a trip. Their task was to work together with the system to select particular flights that would meet their requirements. They could book the selected flights, with the understanding that booking in real use means recording the relevant information so that a travel agent could complete the task. We reminded them of the types of information the system contains (meals, fares, times, etc.). The wizard&apos;s task was a simple one of typing in verbatim (minus false starts) what the user said. The system answered as well as it could, and identif"
H90-1028,H89-2008,1,0.912862,"eaving before 3:00 p.m. serving lunch,&quot; prior to displaying the table. We are hoping that the user will be less inclined to speak &quot;computerese&quot; if the computer behaves a little more like a person. General Description Knowledge Representation Our conception of an ATIS system is somewhat different from the one defined by the common task. First, we would like a domain that is sufficiently restrictive that we could hope to cover most sentences spoken with high probability. That is, it should be easy for the user to understand the limits of the We made a major paradigm shift in moving from VOYAGER [5] to hTm in terms of back-end function operations, one that was necessitated by the fact that the database should be accessed only once, after all restrictions are in place. Within VOYAGER, low-level functions would access the database di130 rectly, passing on sets of objects to be filtered by later lowlevel functions. Thus, for example the (restaurant) function returns the set of all restaurants, which might later be subsetted by the (serve) function to include, for example, only restaurants serving Chinese food. In ATIS, low level functions typically fill slots in an event frame with appropri"
H90-1043,H89-2027,0,0.0300984,"ere a number of incremental improvements in the word-pair grammar, pronunciation networks, and the backend capabilities. S R / N L Integration In our initial implementation of VOYAGER, the integration of speech and natural language components was accomplished by obtaining the best word sequence from the recognizer and passing that word sequence to the natural language system. Modifying the speech recognition component to produce a list of the top scoring word sequences provides a convenient means for increasing the level of integration of the speech recognition and natural language components [2]. In this way, the natural language system can be run successively on each of the word sequences to find the highest scoring sequence that passes the natural language constraints. Two-stage N-Best search Previously, to produce the top scoring word sequence, our speech recognition system used Viterbi search [4,10]. This algorithm provides an efficient search for the top word sequence but does not directly provide the top N word sequences. Others have chosen to modify this search by keeping track of the top N word sequences at each point in the search [2]. We also 206 use a modification of Viter"
H90-1043,H90-1004,0,0.583699,"to the end for each lexical node at each point in time. If the constraints we use in the Viterbi search to compute the best score to the end are a subset of the full natural language constraints, this estimate of the best score to the end is guaranteed to be an upper bound on best score to the end given the full constraints. The A* search allows a large amount of flexibility in when to apply the natural language constraints. For example, we can wait until we have entire sentence hypotheses before applying the full natural language constraints. This turns the A* search into an N-best algorithm [3] and allows us to compare it directly to the other N-best algorithms. We computed processing time and memory use for our implementation of this algorithm and plotted it in Figure 1. For the top 1 word sequence, this algorithm requires about the same amount of resources as our implementation of Viterbi search and the amount of resources increases approximately linearly with N at least for small N. We have begun to perform experiments to determine which natural language constraints to apply at an earlier stage of the A* search. There is a tradeoff between the cost of applying the constraint and"
H90-1043,H90-1028,1,0.874308,"language model. This method was quite successful. TINA can generate 100,000 sentences in an overnight run, and the resulting wordpair language model had a perplexity of only 73 with a single missed word-pair in the test set. W e therefore decided to incorporate this word-pair language model into the recognizer. Increased Coverage As we have described previously [9], the command generation component translates the natural language parse to a functional form that is evaluated by the system. This component has been made more flexible, in part due to our experience with developing an ATIS system [6]. We have extended the capabilities of the back-end functions to handle more complex manipulations. Some of these changes were motivated by an examination of our training data. In other cases, we 208 were interested in knowing if our framework could handle manipulations commonly used in other database query systems. For this reason we included conjunction and negation, even though they are rarely used by subjects (except by those with a natural language processing background!). As a result of these modifications, the system is now capable of handling queries such as ""Show me the Chinese or Jap"
H90-1043,H90-1074,1,0.770402,"rformance by incorporating some form of explicit rejection criterion. Currently we reject an utterance based on the number of word strings that fail to produce a response (by choosing an upper bound on N in the N-Best search). If we used a more explicit rejection criterion (by taking into account the scores of the top N word strings for example) we may be able to decrease the ratio of incorrect response to correct responses. There have been a number of developments in the speech recognition components that we intend to incorporate into the VOYAGER system. These are discussed in more detail in [7]. We would like to begin exploring dynamic adaptation of the natural language constraints. For example, we would like to increase the objects in VOYAGER's database to a much more complete set. In our current implementation, this would increase the perplexity of the speech recognition and result in poor performance. However, if we limit the vocabulary based on the discourse history, it is likely that we can make large increases in the size of the VOYAGER domain without Summary/Future Plans The evaluations show that compared to passing only the top scoring word string to the natural language sys"
H90-1043,H89-2018,1,0.854641,"tion will run in real time in the present hardware configuration. When combined with lexical access, the entire system will run in approximately 3 times real time on a Sun4/330 and in approximately 2 times real time on a Sun 4/490. Ethemet I Su.u..4/~O • Data Capture [ SPARCeS~t, ion I • Auditory Modelling * Natural Language • Phonetic Recognition * Response Generation • Lex:[cal Access F i g u r e 2: This figure shows the current hardware configuration of the VOYAGER, system. Evaluations At the October 1989 DARPA meeting, we presented a number of evaluations of our initial version of VOYAGER [8] and we have used the same test set to measure the effects of the changes made since that time. To measure the effects of multiple sentence hypotheses, we allowed the system evaluated in [8] to produce the top N word sequences rather than the highest scoring word sequence. Its performance is plotted as a function of N in Figure 3. For each utterance, we therefore the current representation could be run on up to 40 different processors. The dendrogram computation is difficult to divide among processors, but fortunately it runs in under real time on a single DSP32C. The computation of acoustic m"
H90-1043,H89-2008,1,0.851767,"cessary because, when semantic matches are required, generation usually picks the wrong path and aborts on constraint failure. As a consequence, paths with traces are rarely visited by the generator and may not show up in our word-pair language model. This method was quite successful. TINA can generate 100,000 sentences in an overnight run, and the resulting wordpair language model had a perplexity of only 73 with a single missed word-pair in the test set. W e therefore decided to incorporate this word-pair language model into the recognizer. Increased Coverage As we have described previously [9], the command generation component translates the natural language parse to a functional form that is evaluated by the system. This component has been made more flexible, in part due to our experience with developing an ATIS system [6]. We have extended the capabilities of the back-end functions to handle more complex manipulations. Some of these changes were motivated by an examination of our training data. In other cases, we 208 were interested in knowing if our framework could handle manipulations commonly used in other database query systems. For this reason we included conjunction and neg"
H90-1043,H89-1027,1,0.826421,"izer and passing that word sequence to the natural language system. Modifying the speech recognition component to produce a list of the top scoring word sequences provides a convenient means for increasing the level of integration of the speech recognition and natural language components [2]. In this way, the natural language system can be run successively on each of the word sequences to find the highest scoring sequence that passes the natural language constraints. Two-stage N-Best search Previously, to produce the top scoring word sequence, our speech recognition system used Viterbi search [4,10]. This algorithm provides an efficient search for the top word sequence but does not directly provide the top N word sequences. Others have chosen to modify this search by keeping track of the top N word sequences at each point in the search [2]. We also 206 use a modification of Viterbi search to produce the top N word sequences. In our algorithm, we first use Viterbi search to compute the best partial paths both arriving and leaving each lexical node at each point in time. The algorithm then successively extracts the next best complete path by searching through the precomputed matrix of part"
H90-1074,H89-1027,1,\N,Missing
H90-1074,H89-2018,1,\N,Missing
H91-1011,H90-1004,0,0.0123982,"ping and gemination. The pronunciation networks for the individual word~ are combined into a single network allowing all possible word strings. Inter-word pronunciation rules and local granmaatical constraints are taken into account when the words are combined into this network. Finding the highest scoring word sequence is accomplished by finding the best match between a path in the acoustic network and a path in the lexical network. The initial version of the system used Viterbi search to find the single best match. More recently we have been using the A*, N-best search described in [15] and [11] to find a list of top scoring sentence hypotheses. Scoring Strategy Since the overall score of a path consists of a number of components (acoustic model score, duration model score, segmentation score, and, in some cases, language model score), we must determine a way to combine them. If these were statistically independent probabilities of paths given the acoustics, we could simply combine them by multiplication. Unfortunately, it is unlikely that the component scores are statistically independent. Besides, they are likely to be poor estimates of probabilities both because of lack of trainin"
H91-1011,H89-1027,1,0.907569,"Figure 1. The acoustic processing consists of a model of the human peripheral auditory system as a front-end, a hierarchical segmentation algorithm to produce a network of possible acoustic segments, an automatically defined set of segmental measurements for each hypothesized segment, and finally, a statistical classifier for providing a probability of each label given a segment. The result of this analysis branch of the system is a network of possible phonetic interpretations of the speech signal. Each arc in the network has a list of probabilities of the labels used to represent the lexicon [13]. Signal Lexicon tem that different paths contain different acoustic segments and therefore have different observation spaces [6]. We cannot simply compare probabilities of word sequences given acoustic observations since the probabilities are computed using different observations. Normalizing the probabilities by the length of the segments helps to some degree (since all paths have the same duration), but then longer duration segments have a greater influence on the path score than short segments. Higher-Level Linguistic Knowledge Signal Representation I Acoustic Segmentation 1 Extraction & P"
H91-1011,H90-1043,1,0.814414,"h as flapping and gemination. The pronunciation networks for the individual word~ are combined into a single network allowing all possible word strings. Inter-word pronunciation rules and local granmaatical constraints are taken into account when the words are combined into this network. Finding the highest scoring word sequence is accomplished by finding the best match between a path in the acoustic network and a path in the lexical network. The initial version of the system used Viterbi search to find the single best match. More recently we have been using the A*, N-best search described in [15] and [11] to find a list of top scoring sentence hypotheses. Scoring Strategy Since the overall score of a path consists of a number of components (acoustic model score, duration model score, segmentation score, and, in some cases, language model score), we must determine a way to combine them. If these were statistically independent probabilities of paths given the acoustics, we could simply combine them by multiplication. Unfortunately, it is unlikely that the component scores are statistically independent. Besides, they are likely to be poor estimates of probabilities both because of lack o"
H91-1011,H89-1053,0,0.0679432,"Missing"
H91-1011,H91-1031,1,0.873185,"Missing"
H91-1014,H91-1072,1,0.875379,"Missing"
H91-1014,H91-1010,0,0.118809,"Missing"
H91-1014,H91-1071,1,0.837088,", particularly the back-end component that transforms the parse tree into a representation that can be used to maintain discourse, generate confirmation messages, and produce SQL queries for accessing the OAG database. We have also connected the SUMMIT speech recognizer to our ATIS system, so that it can now accept verbal input. Recognition Component The speech recognition configuration is similar to the one used in the VOYAGERsystem and is based on the SUMMIT system [6]. For the ATIS task, we used 76 context-independent phone models trained on speaker-independent data collected at TI and MIT [3]. There were 1284 TI sentences (read and spontaneous versions of 642 sentences) and 1146 spontaneous sentences taken from the MIT training corpus. The lexicon was derived from the vocabulary used by the ATIS natural language component and consisted of 577 words. In order to provide some conservative natural language constraints, the speech recognition component used a generalized word-pair grammar derived from the speech training data augmented with a large number of additional sentences pooled from all available sources of nTIS related text material. The wordpair grammar was generated by pars"
H91-1014,H91-1070,1,0.897645,"e M I T ATIS S y s t e m 1 Stephanie Seneff, James Glass, David Goddeau, David Goodine, Lynette Hirschman, Hong Leung, Michael Phillips, Joseph Polifroni, and Victor Zue Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology Cambridge, Massachusetts 02139 ABSTRACT have collected over the past few months [3]. Aspects of the system involving discourse and dialogue are based on similar principles as before, but has been modified to reflect the new semantic representations. A detailed description of our discourse model can be found in a companion paper [4]. This paper represents a status report on the MIT ATIS system. The most significant new achievement is that we now have a speech-input mode. It is based on the MIT SUMMITsystem using context independent phone models, and includes a word-pair grammar with perplexity 92 (on the June-90 test set). In addition, we have completely redesigned the back-end component, in order to emphasize portability and extensibility. The parser now produces an intermediate semantic frame representation, which serves as the focal point for all back-end operations, such as history management, text generation, and SQ"
H91-1014,H90-1028,1,0.809974,"tem that are tied to a particular domain are now entered through a set of tables associated with a small artificial language for decoding them. We have also improved the display of the database table, making it considerably easier for a subject to comprehend the information given. We report here on the results of the official DARPA February-91 evaluation, as well as on results of an evaluation on data collected at MIT, for both speech input and text input. SYSTEM DESCRIPTION In this section we will describe those aspects of the system that have changed significantly since our report last June [5]. The most significant change has been the incorporation of the speech recognition component. We begin by describing the recognizer configuration and the interface mechanism we are currently using. In the natural language component, the parser and grammar remain unchanged, except for augmentations to improve coverage. However, we have completely redesigned the component that translates from a parse tree to executable SQL queries, and the component that generates verbal responses. Both of these areas are described here in more detail. INTRODUCTION Speech In June 1990, we reported on the initial"
H91-1014,H90-1043,1,0.88538,"in the first round of DARPA common evaluation using text input [5]. Since then, a number of changes have been made to our system, particularly the back-end component that transforms the parse tree into a representation that can be used to maintain discourse, generate confirmation messages, and produce SQL queries for accessing the OAG database. We have also connected the SUMMIT speech recognizer to our ATIS system, so that it can now accept verbal input. Recognition Component The speech recognition configuration is similar to the one used in the VOYAGERsystem and is based on the SUMMIT system [6]. For the ATIS task, we used 76 context-independent phone models trained on speaker-independent data collected at TI and MIT [3]. There were 1284 TI sentences (read and spontaneous versions of 642 sentences) and 1146 spontaneous sentences taken from the MIT training corpus. The lexicon was derived from the vocabulary used by the ATIS natural language component and consisted of 577 words. In order to provide some conservative natural language constraints, the speech recognition component used a generalized word-pair grammar derived from the speech training data augmented with a large number of"
H91-1014,H90-1074,1,\N,Missing
H92-1016,H90-1016,0,0.0449197,"Missing"
H92-1016,H91-1011,1,0.766755,"(labelled as B G + P L R ) shows that further reduction in error rate is possible by incorporating the PLR. P L R is incorporated by using the parse score in place of the bigram score to reorder the 50 Nbest outputs produced by the recognizer. The sentence error rate is reduced more than the word error rate, presumably due to the fact that P L R can deal with some of the long distance constraints better than the bigram. Context-Dependent Modelling At the last DARPA meeting we first described our work towards accounting for contextual effects on the phonetic modelling component of S U M M I T [5]. We proposed using regression tree analysis to find the contex3 T h i s is r o u g h l y equivalent to p a r s i n g t h e word s t r i n g as a seq u e n c e of f r a g m e n t s r a t h e r t h a n as a c o m p l e t e sentence. 86 tual factors that provided the greatest reduction in the distortion of our phonetic models. In an initial experiment, regression tree analysis was used to form a set of context-specific models for each phonetic unit. However, we found that we were able to obtain the best performance by using the regression trees to independently learn a context-normalization fact"
H92-1016,H92-1003,1,0.839497,"Missing"
H92-1016,H91-1071,1,0.869573,"Missing"
H92-1016,H91-1014,1,0.711669,"at MIT. Some 9,711 utterances in this pool were designated as training material, and an additional 1,595 utterances were set aside as a development set for independent evaluation. To facilitate a meaningful comparison, all the experiments described in this section are performed on the October '91 ""dry-run"" test set, containing some 362 utterances collected at BBN, CMU, MIT, and SRI. The experiments that we conducted are summarized in Table 1, and will be described in this section. In order to monitor progress internally, we also ran the same test set through our system as reported a year ago [8]. Our February '91 system had a vocabulary of 577 words. T h a t system constrained the N - b e s t search with the use of a word-pair g r a m m a r with a perplexity of 92. The N - b e s t outputs were subsequently resorted using our natural language component TINA. It was trained on some 2400 utterances collected at T I and MIT. The recognition performance of t h a t system on the October '91 ""dry-run"" test set, with and without the word-pair language model, is shown in the first two rows of Table 1 (labelled as AW and WP, respectively). Lexicon With the availability of a larger amount of tr"
H92-1016,H92-1060,1,\N,Missing
H92-1075,H92-1003,0,0.0734576,"d development of very large vocabulary continuous speech recognition (CSR) systems. Since August 1991, our group has actively participated in the development of the WSJ-CSR corpus. The purpose of this paper is to document our involvement in this process, from recording and transcription to analyses and distribution. We will also present the results of an experiment investigating the preprocessing of the prompt text. INTRODUCTION One of the key ingredients that has contributed to the steady improvement in speech recognition technology in recent years is the availability of large speech corpora [1,3,7,8]. With the help of these corpora, researchers have been able to develop recognition systems and obtain reliable estimates of system parameters. Perhaps just as important, these corpora, together with standardized performance evaluation procedures and metrics, have encouraged objective comparison of different systems, leading to better understanding and cross fertilization of research ideas [4]. recently initiated an effort towards the construction of a new corpus to meet these needs. The domain chosen by the community is the Wall Street Journal (WSJ), and the text prompts are selected from the"
H92-1075,H92-1073,0,0.35883,"earchers have been able to develop recognition systems and obtain reliable estimates of system parameters. Perhaps just as important, these corpora, together with standardized performance evaluation procedures and metrics, have encouraged objective comparison of different systems, leading to better understanding and cross fertilization of research ideas [4]. recently initiated an effort towards the construction of a new corpus to meet these needs. The domain chosen by the community is the Wall Street Journal (WSJ), and the text prompts are selected from the CD-ROM distributed by A C L / D C I [5]. While the ultimate goal is to collect around 300 hours of speech from more than 100 speakers, it was thought that we should collect a pilot corpus of approximately 40 hours, partly to satisfy near term needs and partly to debug the text preprocessing and data collection processes. Since August 1991, our group is one of three that actively participated in the collection of the WSJ-CSR pilot corpus 3. The purpose of this paper is to document our involvement in this process, present some comparative analyses of the resulting data, and describe an experiment investigating the preparation of the"
H92-1075,H91-1071,1,0.892234,"Missing"
H92-1075,H89-2018,1,0.874027,"d development of very large vocabulary continuous speech recognition (CSR) systems. Since August 1991, our group has actively participated in the development of the WSJ-CSR corpus. The purpose of this paper is to document our involvement in this process, from recording and transcription to analyses and distribution. We will also present the results of an experiment investigating the preprocessing of the prompt text. INTRODUCTION One of the key ingredients that has contributed to the steady improvement in speech recognition technology in recent years is the availability of large speech corpora [1,3,7,8]. With the help of these corpora, researchers have been able to develop recognition systems and obtain reliable estimates of system parameters. Perhaps just as important, these corpora, together with standardized performance evaluation procedures and metrics, have encouraged objective comparison of different systems, leading to better understanding and cross fertilization of research ideas [4]. recently initiated an effort towards the construction of a new corpus to meet these needs. The domain chosen by the community is the Wall Street Journal (WSJ), and the text prompts are selected from the"
H94-1037,H90-1020,0,0.079663,"Missing"
H94-1037,H93-1003,0,0.0165873,"ormation, such as flight schedules from one city to another, obtained from a small relational database excised from the Official Airline Guide. By requiring that all system developers use the same database, it has been possible to compare the performance of various spoken language systems based on their ability to extract the correct information from the database, using a set of prescribed training and test data, and a set of interpretation guidelines. Indeed, periodic common evaluations have occurred at regular intervals, and steady performance improvements have been observed for all systems [2, 3, 4]. While the ATIS task has been instrumental in the development of technologies that can understand spontaneously generated verbal queries in a limited domain, it 1This research was supported by ARPA under Contract N0001489-J-1332, monitored through the Office of Naval Research. 2The authors are listed in reversed alphabetical order. does have some shortcomings. First, the current common evaluation focuses on the correctness of the information extracted from the database without any regard to the system&apos;s side of the interchange (e.g., clarification queries and helpful suggestions). Thus it has"
H94-1037,H89-1027,1,0.856684,"Missing"
H94-1037,J92-1004,1,0.853058,"Missing"
H94-1037,H93-1009,0,0.0130462,"entations. Our view on the appropriate structural units of a semantic frame has evolved over time. Our present view is that all major constituents in a sentence can be classified as one of only three distinct categories, which we label as [clause], [topic], and [ p r e d i c a t e ] . Thus, verbs, adjectives, prepositions and modifier nouns are all considered to be predicates. Furthermore, grammatical constituents such as ""subject"" and ""direct object"" are not explicitly marked in the semantic frame. We have applied this new formalism successfully across several languages in our VOYAGER domain [10], and we are also using it in P E G A S U S . An example semantic frame for the sentence, ""Is there a United flight connecting in Denver,"" is shown in Figure 2. 202 System Manager During the design phase of our project, we made a commitment not to alter the interface and protocols of EAASY SABRE. W e see no benefit, nor do we feel competent, in making changes to a proven system used by many users. In fact, PEGASUS&apos;s interface to EAASY SABRE is identical to that of a user on a PC or a travel agent EAASY SABRE cannot distinguish between a user speaking a natural utterance (such as ""I want to go"
H94-1037,H91-1070,1,0.90177,"Like a travel agent, PEGASUS needs to know the source, destination, and date before it can provide the flight information. The user utilized additional constraints to narrow down the choices before settling on a particular flight. It took two exchanges to arrive at the appropriate fare, and three more to book the return flight. The entire booking took nine exchanges, and lasted approximately 5 minutes. Note that a large fraction of the time is spent waiting for EAASY SABRE to respond. The dialogue component of PEGASUS is significantly more complicated than t h a t of the original ATIS system [11]. This is in large part due to the fact that it must monitor not only the user&apos;s dialogue state and the degree of completion of the booking, but also the state of the EAASY SABRE system. For instance, it must preprocess fare restrictions, warning the user of limits imposed on return dates for restricted fares, screening selected fares for possible constraint failure, and confirming availability on selected flights before attempting to issue bookings. Otherwise, the EAASY SABRE system would invoke a complex subdialogue which we wish to avoid. The system keeps a record of the most recently displ"
H94-1037,H92-1016,1,\N,Missing
H94-1037,H94-1011,0,\N,Missing
H94-1037,H91-1014,1,\N,Missing
H94-1037,H92-1004,0,\N,Missing
I17-1001,P17-1080,1,0.491617,"ing Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Yonatan Belinkov1 Llu´ıs M`arquez2 Hassan Sajjad2 Nadir Durrani2 Fahim Dalvi2 James Glass1 1 MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA {belinkov, glass}@mit.edu 2 Qatar Computing Research Institute, HBKU, Doha, Qatar {lmarquez, hsajjad, ndurrani, faimaduddin}@qf.org.qa Abstract One observation that has been made is that lower layers in the neural MT network learn different kinds of information than higher layers. For example, Shi et al. (2016) and Belinkov et al. (2017) found that representations from lower layers of the NMT encoder are more predictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for imp"
I17-1001,D17-1151,0,0.0125781,"al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et al. (2017) found that encoders with 2 to 4 layers performed the best. For completeness, we report here results using features extracted from models trained originally with 2 and 3 layers, in addition to our basic setting of 4 layers. Table 6 shows consistent trends with our previous observations: POS tagging does not benefit from upper layers, while SEM tagging does, although the improvement is rather small in the shallower models. 5 0 Related Work Techniques for analyzing neural network models include visualization of hidden units (Elman, 1991; Karpathy et al., 2015; K´ad´ar et al., 2016; Qian et al.,"
I17-1001,E17-2039,0,0.0170778,"Missing"
I17-1001,P07-1005,0,0.0220979,"coder are more predictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), an"
I17-1001,I17-1015,1,0.421519,"semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on e"
I17-1001,K17-1037,0,0.00554294,"ve similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations. 0 1 2 3 4 4 POS SEM 87.9 81.8 92.0 87.8 91.7 87.4 91.8 87.6 91.9 88.2 3 POS SEM 87.9 81.9 92.5 88.2 92.3 88.0 92.4 88.4 – – 2 POS SEM 87.9 82.0 92.7 88.5 92.7 88.7 – – – – Table 6: POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages. tain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wu and King, 2016; Alishahi et al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et"
I17-1001,W11-1012,0,0.0281742,"inguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are assigned different SEM t"
I17-1001,P82-1020,0,0.755314,"Missing"
I17-1001,P13-2074,0,0.0196012,"gical tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are assigned different SEM tags depending on their type (e.g., geopolitical entity, organizati"
I17-1001,C12-1083,0,0.0154324,"Missing"
I17-1001,P16-1140,0,0.0367,"Missing"
I17-1001,E17-2060,0,0.0165727,"peech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on either SEM or POS tagging using features from different l"
I17-1001,D16-1159,0,0.531034,"tter for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on either SEM or POS tagging using features"
I17-1001,D15-1246,0,0.0767367,"Missing"
I17-1001,W17-4115,0,0.0393383,"Missing"
I17-1001,Q16-1037,0,0.0323719,"0 1 2 3 4 4 POS SEM 87.9 81.8 92.0 87.8 91.7 87.4 91.8 87.6 91.9 88.2 3 POS SEM 87.9 81.9 92.5 88.2 92.3 88.0 92.4 88.4 – – 2 POS SEM 87.9 82.0 92.7 88.5 92.7 88.7 – – – – Table 6: POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages. tain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wu and King, 2016; Alishahi et al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et al. (2017) found that encoders with 2 to 4 layers performed the best. For completeness, we report here results using features extracted from models t"
I17-1001,C10-1081,0,0.031874,"ictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are ass"
I17-1001,L16-1561,0,0.00604061,"OS and SEM tags using the features hkj that are obtained from the English encoder and evaluate their accuracies. Figure 1 illustrates the process. • Consistent with previous work, we find that lower layer representations are usually better for POS tagging. However, we also find that representations from higher layers are better at capturing semantics, even though these are word-level labels. This is especially true with tags that are more semantic in nature such as discourse functions or noun concepts. 2 3 3.1 Data and Experimental Setup Data MT We use the fully-aligned United Nations corpus (Ziemski et al., 2016) for training NMT models, which includes 11 million multi-parallel sentences in six languages: Arabic (Ar), Chinese (Zh), English (En), French (Fr), Spanish (Es), and Russian (Ru). We train En-to-* models on the first 2 million sentences of the train set, using the official train/dev/test split. This dataset has the benefit of multiple alignment of the six languages, which allows for comparable cross-linguistic analysis. Note that the parallel dataset is only used for training the NMT model. The classifier is then trained on the supervised data (described next) and all accuracies are reported"
I17-1001,D16-1079,0,0.0193389,"Missing"
N18-1070,N18-2004,1,0.808385,"ores into a single snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 20"
N18-1070,D17-1317,0,0.0837272,"its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on t"
N18-1070,gencheva-etal-2017-context,1,0.834798,"aluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to so"
N18-1070,N18-5006,1,0.775642,"and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition,"
N18-1070,P16-1044,0,0.326197,"n. T imeDistributed(LST M ) (X, W, E) −−−−−−−−−−−−−−−−→ {m1 , ..., mn } (2) 768 Figure 2: The architecture of our memory network model for stance detection. Furthermore, we convert each input claim s into its representation using the corresponding LSTM and CNN networks as follows: where mj is the LSTM representation of xj , and TimeDistributed() indicates a wrapper that enables training the LSTM over all pieces of evidence by applying the same LSTM model to each time-step of a 3D input tensor, i.e., (X, W, E). While LSTM networks were designed to effectively capture and memorize their inputs (Tan et al., 2016), Convolutional Neural Networks (CNNs) emphasize the local interaction between the individual words in the input word sequence, which is important for obtaining an effective representation. Here, we use a CNN in order to encode each xj into its representation cj as shown below (see line 13 in Table 1). LST M,CN N s −−−−−−−−→ slstm , scnn (4) where slstm and scnn are the representations of s computed using LST M and CN N networks, respectively. Note that these are separate networks with different parameters from those used to encode the pieces of evidence. Lines 10–14 of Table 1 describe the ab"
N18-1070,karadzhov-etal-2017-fully,1,0.797992,"be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model"
N18-1070,N18-1074,0,0.0508096,"snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers t"
N18-1070,K15-1032,1,0.705898,"xtension of the general architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stan"
N18-1070,W14-2508,0,0.13385,"s we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have d"
N18-1070,P16-2065,1,0.613587,"architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stand-alone task. The task aims"
N18-1070,S16-1074,0,0.0172524,"ylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the"
N18-1070,S16-1003,0,0.260879,"Missing"
N18-1070,C16-1230,0,0.034862,"7; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the stance detection problem (Baird et a"
N18-1070,D14-1162,0,0.0795601,"Missing"
N18-1070,N09-2040,0,\N,Missing
N18-1143,D16-1044,0,0.0287657,"ion dataset such as ImageNet (Russakovsky et al., 2015) have proven to be excellent feature extractors for a broad range of visual tasks such as image captioning (Lu et al., 2017; Karpathy and Fei-Fei, 2015; Fang et al., 2015) and visual 1 In this paper, we do not distinguish conceptually between transfer learning and domain adaptation. A ‘domain’ in the sense we use throughout this paper is defined by datasets. 1585 Proceedings of NAACL-HLT 2018, pages 1585–1594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics question answering (Xu and Saenko, 2016; Fukui et al., 2016; Yang et al., 2016; Antol et al., 2015), among others. In NLP, transfer learning has also been successfully applied to tasks like sequence tagging (Yang et al., 2017), syntactic parsing (McClosky et al., 2010) and named entity recognition (Chiticariu et al., 2010), among others. 1.3 Transfer Learning for QA Although transfer learning has been successfully applied to various applications, its applicability to QA has yet to be well-studied. In this paper, we tackle the TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) with transfer learning from MovieQ"
N18-1143,D17-1087,0,0.205884,"Missing"
N18-1143,D10-1098,0,0.0188158,"o not distinguish conceptually between transfer learning and domain adaptation. A ‘domain’ in the sense we use throughout this paper is defined by datasets. 1585 Proceedings of NAACL-HLT 2018, pages 1585–1594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics question answering (Xu and Saenko, 2016; Fukui et al., 2016; Yang et al., 2016; Antol et al., 2015), among others. In NLP, transfer learning has also been successfully applied to tasks like sequence tagging (Yang et al., 2017), syntactic parsing (McClosky et al., 2010) and named entity recognition (Chiticariu et al., 2010), among others. 1.3 Transfer Learning for QA Although transfer learning has been successfully applied to various applications, its applicability to QA has yet to be well-studied. In this paper, we tackle the TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) with transfer learning from MovieQA (Tapaswi et al., 2016) using two existing QA models. Both models are pre-trained on MovieQA and then fine-tuned on each target dataset, so that their performance on the two target datasets are significantly improved. In particular, one of the models achieves the"
N18-1143,N10-1004,0,0.0391619,"; Fang et al., 2015) and visual 1 In this paper, we do not distinguish conceptually between transfer learning and domain adaptation. A ‘domain’ in the sense we use throughout this paper is defined by datasets. 1585 Proceedings of NAACL-HLT 2018, pages 1585–1594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics question answering (Xu and Saenko, 2016; Fukui et al., 2016; Yang et al., 2016; Antol et al., 2015), among others. In NLP, transfer learning has also been successfully applied to tasks like sequence tagging (Yang et al., 2017), syntactic parsing (McClosky et al., 2010) and named entity recognition (Chiticariu et al., 2010), among others. 1.3 Transfer Learning for QA Although transfer learning has been successfully applied to various applications, its applicability to QA has yet to be well-studied. In this paper, we tackle the TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) with transfer learning from MovieQA (Tapaswi et al., 2016) using two existing QA models. Both models are pre-trained on MovieQA and then fine-tuned on each target dataset, so that their performance on the two target datasets are significantly i"
N18-1143,P17-2081,0,0.301541,"Missing"
N18-1143,D16-1046,0,0.0335263,"he extent of the model’s understanding of the story, it is asked to answer questions about Transfer Learning Transfer learning (Pan and Yang, 2010) is a vital machine learning technique that aims to use the knowledge learned from one task and apply it to a different, but related, task in order to either reduce the necessary fine-tuning data size or improve performance. Transfer learning, also known as domain adaptation1 , has achieved success in numerous domains such as computer vision (Sharif Razavian et al., 2014), ASR (Doulaty et al., 2015; Huang et al., 2013), and NLP (Zhang et al., 2017; Mou et al., 2016). In computer vision, deep neural networks trained on a large-scale image classification dataset such as ImageNet (Russakovsky et al., 2015) have proven to be excellent feature extractors for a broad range of visual tasks such as image captioning (Lu et al., 2017; Karpathy and Fei-Fei, 2015; Fang et al., 2015) and visual 1 In this paper, we do not distinguish conceptually between transfer learning and domain adaptation. A ‘domain’ in the sense we use throughout this paper is defined by datasets. 1585 Proceedings of NAACL-HLT 2018, pages 1585–1594 c New Orleans, Louisiana, June 1 - 6, 2018. 201"
N18-1143,D14-1162,0,0.0795507,"Missing"
N18-1143,D16-1264,0,0.243101,"Missing"
N18-1143,D13-1020,0,0.0933888,"rce dataset with correct answer to each question; Target dataset without any answer; Number of training epochs. Output: Optimal QA model M ∗ 1: Pre-train QA model M on the source dataset. 2: repeat 3: For each question in the target dataset, use M to predict its answer. 4: For each question, assign the predicted answer to the question as the correct one. 5: Fine-tune M on the target dataset as usual. 6: until Reach the number of training epochs. 3 Datasets We used MovieQA (Tapaswi et al., 2016) as the source MCQA dataset, and TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) as two separate target datasets. Examples of the three datasets are shown in Table 1. MovieQA is a dataset that aims to evaluate automatic story comprehension from both video and text. The dataset provides multiple sources of information such as plot synopses, scripts, subtitles, and video clips that can be used to infer answers. We only used the plot synopses of the dataset, so our setting is the same as pure textual MCQA. The dataset contains 9,848/1,958 train/dev examples; each question comes with a set of five possible answer choices with only one correct answer. TOEFL listening comprehen"
N18-1143,D15-1237,0,0.103503,"Missing"
N18-1143,Q17-1036,0,0.0281882,"utput. To evaluate the extent of the model’s understanding of the story, it is asked to answer questions about Transfer Learning Transfer learning (Pan and Yang, 2010) is a vital machine learning technique that aims to use the knowledge learned from one task and apply it to a different, but related, task in order to either reduce the necessary fine-tuning data size or improve performance. Transfer learning, also known as domain adaptation1 , has achieved success in numerous domains such as computer vision (Sharif Razavian et al., 2014), ASR (Doulaty et al., 2015; Huang et al., 2013), and NLP (Zhang et al., 2017; Mou et al., 2016). In computer vision, deep neural networks trained on a large-scale image classification dataset such as ImageNet (Russakovsky et al., 2015) have proven to be excellent feature extractors for a broad range of visual tasks such as image captioning (Lu et al., 2017; Karpathy and Fei-Fei, 2015; Fang et al., 2015) and visual 1 In this paper, we do not distinguish conceptually between transfer learning and domain adaptation. A ‘domain’ in the sense we use throughout this paper is defined by datasets. 1585 Proceedings of NAACL-HLT 2018, pages 1585–1594 c New Orleans, Louisiana, Ju"
N18-1143,W17-2623,0,0.092414,"Missing"
N18-1143,P16-1041,0,0.012041,"ore difficult. The two chosen target datasets are challenging because the stories and questions are complicated, and only small training sets are available. Therefore, it is difficult to train statistical models on only their training sets because the small size limits the number of parameters in the models, and prevents learning any complex language concepts simultaneously with the capacity to answer questions. We demonstrate that we can effectively overcome these difficulties via transfer learning in Section 5. 4 QA Neural Network Models Among numerous models proposed for multiplechoice QA (Trischler et al., 2016; Fang et al., 2016; Tseng et al., 2016), we adopt the End-toEnd Memory Network (MemN2N)2 (Sukhbaatar et al., 2015) and Query-Based Attention CNN (QACNN)3 (Liu et al., 2017), both open-sourced, to conduct the experiments. Below we briefly introduce the two models in Section 4.1 and Section 4.2, respectively. For the details of the models, please refer to the original papers. 4.1 End-to-End Memory Networks An End-to-End Memory Network (MemN2N) first transforms Q into a vector representation with 2 MemN2N was originally designed to output a single word within a fixed vocabulary set. To apply it"
N18-1143,P15-2115,0,0.060691,"Missing"
N18-1143,K17-1029,0,0.105107,"Missing"
N18-2004,karadzhov-etal-2017-built,1,0.798906,"st of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised"
N18-2004,N09-2040,0,0.0986266,"news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Po"
N18-2004,karadzhov-etal-2017-fully,1,0.855688,"Missing"
N18-2004,D16-1011,0,0.191672,"c.). Despite the interdependency between fact checking and stance detection, research on these two problems has not been previously supported by an integrated corpus. This is a gap we aim to bridge by retrieving documents for each claim and annotating them for stance, thus ensuring a natural distribution of the stance labels. Moreover, in order to be trusted by users, a factchecking system should be able to explain the reasoning that led to its decisions. This is best supported by showing extracts (such as sentences or phrases) from the retrieved documents that illustrate the detected stance (Lei et al., 2016). Unfortunately, existing datasets do not offer manual annotation of sentence- or phrase-level supporting evidence. While deep neural networks with attention mechanisms can infer and extract such evidence automatically in an unsupervised way (Parikh et al., 2016), potentially better results can be achieved when having the target sentence provided in advance, which enables supervised or semi-supervised training of the attention. This would allow not only more reliable evidence extraction, but also better stance prediction, and ultimately better factuality prediction. Following this idea, our co"
N18-2004,gencheva-etal-2017-context,1,0.869644,"Missing"
N18-2004,P15-2085,0,0.0248319,"unifies stance detection, stance rationale, relevant document retrieval, and fact checking. This is the first corpus to offer such a combination, not only for Arabic but in general. We further demonstrated experimentally that these unified annotations, and the gold rationales in particular, are beneficial both for stance detection and for fact checking. In future work, we plan to extend the annotations to cover other important aspects of fact checking such as source reliability, language style, and temporal information, which have been shown useful in previous research (Castillo et al., 2011; Lukasik et al., 2015; Ma et al., 2016; Mukherjee and Weikum, 2015; Popat et al., 2017). Overall, the above experiments demonstrate that having a gold rationale can enable better learning. However, the results should be considered as a kind of upper bound on the expected performance improvement, since here we used gold rationales at test time, which would not be available in a real-world scenario. Still, we believe that sizable improvements would still be possible when using the gold rationales for training only. Acknowledgment This research was carried out in collaboration between the MIT Computer Science and Art"
N18-2004,W01-0515,0,0.0674273,"as in (Karadzhov et al., 2017b), we transformed each claim into sub-queries by selecting named entities, adjectives, nouns and verbs with the highest TF.DF score, calculated on a collection of documents from the claims’ sources. Then, we used these sub-queries with the claim itself as input to the search API and retrieved the first 20 returned links, from which we excluded those directing to V ERIFY and R EUTERS, and social media websites that are mostly opinionated. Finally, we calculated two similarity measures between the links’ content (documents) and the claims: the tri-gram containment (Lyon et al., 2001) and the cosine distance between average word embeddings of both texts.6 . We only kept documents with non-zero values for both measures, yielding 3,042 documents: 1,239 for false claims and 1,803 for true claims. (2a) (original false claim) FIFA intends to investigate the game between Syria and Australia.  JË@ Ð Q ª K A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j AJË@Q @ð AK Pñ (2b) (corrected claim in V ERIFY) FIFA does not intend to investigate the game between Syria and Australia, as pro-regime pages claim.  JË@ Ð Q ª K B A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j    j® ú«Y K A"
N18-2004,K15-1032,1,0.76343,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,N18-5006,1,0.869083,"Missing"
N18-2004,R15-1058,1,0.833945,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,P16-2065,1,0.789602,"ed by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 -"
N18-2004,D16-1264,0,0.0505697,"traction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detection or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels"
N18-2004,S16-1003,0,0.170265,"Missing"
N18-2004,D17-1317,0,0.384567,"t checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detect"
N18-2004,N18-1070,1,0.860201,"Missing"
N18-2004,D16-1244,0,0.101792,"Missing"
N18-2004,N18-1074,0,0.0626421,"tion or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection betwe"
N18-2004,W14-2508,0,0.248156,"ality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in pract"
N18-2004,pasha-etal-2014-madamira,0,0.0837142,"Missing"
N18-2004,P17-2067,0,0.173053,"ext analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Popat et al., 2016).1 As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from English, and focusing on US politics.2 In contrast, we start with claims that are not only relevant to the Arab world, but that were also originally made in Arabic, thus producing the first publicly available Arabic fact-checking dataset. Stance detection has been studied so far disjointly from fact checking. While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-"
N18-2082,W17-3209,0,0.0195564,"as used to train the NMT encoder, in capturing semantic proto-roles and paraphrastic inference. In Table 1, we notice a large improvement using sentence representations from an NMT encoder that was trained on en-es parallel text. The improvements are most profound when a classifier trained on DPR data predicts entailment focused on seAppendix D includes some illustrative examples. This is seen in the last columns of the top row in Table 1. 516 MNLI-1 MNLI-2 ar es zh de MAJ 45.9 46.6 45.7 46.7 46.6 48.2 48.0 48.9 35.6 36.5 Gao and Vogel (2011) add semantic-roles to improve phrase-based MT, and Carpuat et al. (2017) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation. Recent work explores how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology (Dalvi et al., 2017; Be"
N18-2082,D17-1311,0,0.0251135,"Berry Rejoins WPP Group Berry was sentient 7 3 3 Figure 1: Example sentence pairs for the different semantic phenomena. DPR deals with complex anaphora resolution, FN+ is concerned with paraphrastic inference, and SPR covers Reisinger et al. (2015)’s semantic proto-roles. 3 / 7 indicates that the first sentence entails / does not entail the second. What do neural machine translation (NMT) models learn about semantics? Many researchers suggest that state-of-the-art NMT models learn representations that capture the meaning of sentences (Gu et al., 2016; Johnson et al., 2017; Zhou et al., 2017; Andreas and Klein, 2017; Neubig, 2017; Koehn, 2017). However, there is limited understanding of how specific semantic phenomena are captured in NMT representations beyond this broad notion. For instance, how well do these representations capture Dowty (1991)’s thematic proto-roles? Are these representations sufficient for understanding paraphrastic inference? Do the sentence representations encompass complex anaphora resolution? We argue that existing semantic annotations recast as Natural Language Inference (NLI) can be leveraged to investigate whether sentence representations encoded by NMT models capture these se"
N18-2082,P07-1005,0,0.0673881,"Missing"
N18-2082,D16-1053,0,0.0226068,"Missing"
N18-2082,D17-1070,0,0.121882,"Missing"
N18-2082,D15-1075,0,0.48305,"e NLI sentence pairs with their respective labels and semantic phenomena. We evaluate NMT sentence representations of 4 NMT models from 2 domains on 4 different NLI datasets to investigate how well they capture different semantic phenomena. We use White et al. (2017)’s Unified Semantic Evaluation Framework (USEF) that recasts three semantic phenomena NLI: 1) semantic proto-roles, 2) paraphrastic inference, 3) and complex anaphora resolution. Additionally, we evaluate the NMT sentence representations on 4) Multi-NLI, a recent extension of the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) that includes multiple genres and domains (Williams et al., 1 Code developed and data used are available at https: //github.com/boknilev/nmt-repr-analysis. 2 Sometimes referred to as recognizing textual entailment (Dagan et al., 2006, 2013). 1. Introduction 513 Proceedings of NAACL-HLT 2018, pages 513–523 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics so correctly identifying them can be important for translation. For example, English does not usually explicitly mark volition, a proto-role, except by using adverbs like intentionally or accidentally."
N18-2082,I17-1015,1,0.829229,"Carpuat et al. (2017) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation. Recent work explores how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology (Dalvi et al., 2017; Belinkov et al., 2017a), syntax (Shi et al., 2016), and lexical semantics (Belinkov et al., 2017b), including word senses (Marvin and Koehn, 2018; Liu et al., 2018) Table 5: Accuracies for MNLI test sets. MNLI-1 refers to the matched case and MNLI-2 is the mismatched. mantic proto-roles or paraphrastic inference. We also note that using the NMT encoder trained on en-es parallel text results in the highest results in 5 of the 6 proto-roles in the top portion of Table 4. When using other sentence representations (Appendix A), we notice that using representations from English-German encoders co"
N18-2082,P14-2124,0,0.0670495,"Missing"
N18-2082,W11-1012,0,0.0246855,"nguage Our experiments show differences based on which target language was used to train the NMT encoder, in capturing semantic proto-roles and paraphrastic inference. In Table 1, we notice a large improvement using sentence representations from an NMT encoder that was trained on en-es parallel text. The improvements are most profound when a classifier trained on DPR data predicts entailment focused on seAppendix D includes some illustrative examples. This is seen in the last columns of the top row in Table 1. 516 MNLI-1 MNLI-2 ar es zh de MAJ 45.9 46.6 45.7 46.7 46.6 48.2 48.0 48.9 35.6 36.5 Gao and Vogel (2011) add semantic-roles to improve phrase-based MT, and Carpuat et al. (2017) demonstrate how filtering parallel sentences that are not parallel in meaning improves translation. Recent work explores how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn abo"
N18-2082,P11-1023,0,0.0503801,"Missing"
N18-2082,P14-5010,0,0.00252649,"hat our classifiers using the representations from the NMT encoder perform poorly. Although the sentences in FN+ are much longer than in the other datasets, sentence length does not seem to be responsible for the poor FN+ results. The classifiers do not noticeably perform better on shorter sentences than longer ones, as noted in Appendix C. Upon manual inspection, we noticed that in many not-entailed examples, swapped paraphrases had different part-of-speech (POS) tags. This begs the question of whether different POS tags for swapped paraphrases affects the accuracies. Using Stanford CoreNLP (Manning et al., 2014), we partition our validation set based on whether the paraphrases share the same POS tag. Table 3 reports dev set accuracies using classifiers trained on FN+. Classifiers using features from NMT encoders trained on the three languages from the UN corpus noticeably perform better on cases where paraphrases have different POS tags compared to paraphrases with the same POS tags. These difNatural Language Inference data We use four distinct datasets to train classifiers: MultiNLI (Williams et al., 2017), a recent expansion of SNLI containing a broad array of domains that was used in the 2017 RepE"
N18-2082,P02-1031,0,0.0462015,"rforms the baseline for a proto-role, all the other classifiers do as well. The classifiers outperform the majority baseline for 6 of the reported 16 proto-roles. We observe these 6 properties are more associated with proto-agents than proto-patients. The larger improvements over the majority baseline for SPR compared to FN+ and DPR is not surprising. Dowty (1991) posited that proto-agent, and -patient should correlate with English syntactic subject, and object, respectively, and empirically the necessity of [syntactic] parsing for predicate argument recognition has been observed in practice (Gildea and Palmer, 2002; Punyakanok et al., 2008). Further, recent work is suggestive that LSTM-based frameworks implicitly may encode syntax based on certain learning objectives (Linzen et al., 2016; Shi et al., 2016; Belinkov et al., 2017b). It is unclear whether NMT encoders capture semantic proto-roles specifically or just underlying syntax that affects the proto-roles. Proto-role entailment (SPR) When predicting SPR entailments using a classifier trained on SPR data, we noticeably outperform the majority baseline but are below USEF. Both ours and USEF’s accuracies are lower than Teichert et al. (2017)’s best re"
N18-2082,W18-1812,0,0.0223432,"how representations learned by NMT systems can improve semantic tasks. McCann et al. (2017) show improvements in many tasks by using contextualized word vectors extracted from a LSTM encoder trained for MT. Their goal is to use NMT to improve other tasks while we focus on using NLI to determine what NMT models learn about different semantic phenomena. Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology (Dalvi et al., 2017; Belinkov et al., 2017a), syntax (Shi et al., 2016), and lexical semantics (Belinkov et al., 2017b), including word senses (Marvin and Koehn, 2018; Liu et al., 2018) Table 5: Accuracies for MNLI test sets. MNLI-1 refers to the matched case and MNLI-2 is the mismatched. mantic proto-roles or paraphrastic inference. We also note that using the NMT encoder trained on en-es parallel text results in the highest results in 5 of the 6 proto-roles in the top portion of Table 4. When using other sentence representations (Appendix A), we notice that using representations from English-German encoders consistently outperforms using the other encoders (Tables 6 and 7). This prevents us from making generalizations regarding specific target side langu"
N18-2082,P16-1154,0,0.016256,"ses five research reactors Iran has five research reactors Berry Rejoins WPP Group Berry was sentient 7 3 3 Figure 1: Example sentence pairs for the different semantic phenomena. DPR deals with complex anaphora resolution, FN+ is concerned with paraphrastic inference, and SPR covers Reisinger et al. (2015)’s semantic proto-roles. 3 / 7 indicates that the first sentence entails / does not entail the second. What do neural machine translation (NMT) models learn about semantics? Many researchers suggest that state-of-the-art NMT models learn representations that capture the meaning of sentences (Gu et al., 2016; Johnson et al., 2017; Zhou et al., 2017; Andreas and Klein, 2017; Neubig, 2017; Koehn, 2017). However, there is limited understanding of how specific semantic phenomena are captured in NMT representations beyond this broad notion. For instance, how well do these representations capture Dowty (1991)’s thematic proto-roles? Are these representations sufficient for understanding paraphrastic inference? Do the sentence representations encompass complex anaphora resolution? We argue that existing semantic annotations recast as Natural Language Inference (NLI) can be leveraged to investigate wheth"
N18-2082,P15-2067,1,0.667806,"Missing"
N18-2082,D17-3004,1,0.884949,"Missing"
N18-2082,Q15-1034,1,\N,Missing
N18-2082,P17-1080,1,\N,Missing
N18-2082,W17-5301,0,\N,Missing
N18-2082,I17-1100,1,\N,Missing
N18-2082,W17-5703,0,\N,Missing
N18-2082,N18-1121,0,\N,Missing
N18-2082,I17-1001,1,\N,Missing
N18-2082,J13-3001,0,\N,Missing
N18-2082,2020.wmt-1.63,0,\N,Missing
N18-2082,2020.coling-tutorials.3,0,\N,Missing
N18-2117,rousseau-etal-2012-ted,0,0.0673106,"Missing"
N18-2117,P16-2030,0,0.0433843,"Missing"
N19-1216,D18-1389,1,0.852475,"treme left/right media tend to be propagandistic, while center media are more factual, and thus generally more trustworthy. This connection can be clearly seen in Figure 1. Figure 1: Correlation between bias and factuality for the news outlets in the Media Bias/Fact Check website. 2109 Proceedings of NAACL-HLT 2019, pages 2109–2116 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Despite the connection between factuality and bias, previous research has addressed them as independent tasks, even when the underlying dataset had annotations for both (Baly et al., 2018). In contrast, here we solve them jointly. Our contributions can be summarized as follows: • We study an under-explored but arguably important problem: predicting the factuality of reporting of news media. Moreover, unlike previous work, we do this jointly with the task of predicting political bias. • As factuality and bias are naturally defined on an ordinal scale (factuality: from low to high, and bias: from extreme-left to extreme-right), we address them as ordinal regression. Using multi-task ordinal regression is novel for these tasks, and it is also an under-explored direction in machine"
N19-1216,S16-1039,0,0.0212248,"atures extracted from its corresponding Wikipedia page and Twitter profile, as well as analysis of its URL structure and traffic information about it from Alexa rank. In the present work, we use a similar set of features, but we treat the problem as one of ordinal regression. Moreover, we model the political ideology and the factuality of reporting jointly in a multitask learning setup, using several auxiliary tasks. Multitask Ordinal Regression Ordinal regression is well-studied and is commonly used for text classification on an ordinal scale, e.g., for sentiment analysis on a 5-point scale (He et al., 2016; Rosenthal et al., 2017a). However, multi-task ordinal regression remains an understudied problem. Yu et al. (2006) proposed a Bayesian framework for collaborative ordinal regression, and demonstrated that modeling multiple ordinal regression tasks outperforms single-task models. 2110 Walecki et al. (2016) were interested in jointly predicting facial action units and their intensity level. They argued that, due to the high number of classes, modeling these tasks independently would be inefficient. Thus, they proposed the copula ordinal regression model for multi-task learning and demonstrated"
N19-1216,P14-1105,0,0.23407,"for “fake news” detection (Horne et al., 2018a). It has also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates, 2018). Unlike the above work, here we focus on predicting the political ideology of news media outlets. In our previous work (Baly et al., 2018), we did target the politic"
N19-1216,C18-1285,0,0.0857451,"Missing"
N19-1216,karadzhov-etal-2017-built,1,0.776933,"d have reached millions of users, and the harm caused could hardly be undone. An arguably more promising direction is to focus on fact-checking entire news outlets, which can be done in advance. Then, we could fact-check the news before they were even written: by checking how trustworthy the outlets that published them are. Knowing the reliability of a medium is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but also when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017; De Sarkar et al., 2018; Pan et al., 2018; P´erez-Rosas et al., 2018) Political ideology (or left/right bias) is a related characteristic, e.g., extreme left/right media tend to be propagandistic, while center media are more factual, and thus generally more trustworthy. This connection can be clearly seen in Figure 1. Figure 1: Correlation between bias and factuality for the news outlets in the Media Bias/Fact Check website. 2109 Proceedings of NAACL-HLT 2019, pages 2109–2116 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Despite the connection"
N19-1216,D18-1388,0,0.286431,"multi-task learning setup, using several auxiliary tasks. Predicting Political Ideology In previous work, political ideology, also known as media bias, was used as a feature for “fake news” detection (Horne et al., 2018a). It has also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates,"
N19-1216,P16-2065,1,0.857213,"k has modeled the factuality of reporting at the medium level by checking the general stance of the target medium with respect to known manually factchecked claims, without access to gold labels about the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017, 2018). The trustworthiness of Web sources has also been studied from a Data Analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims. In social media, there has been research targeting the user, e.g., finding malicious users (Mihaylov and Nakov, 2016; Mihaylova et al., 2018; Mihaylov et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017). Unlike the above work, here we study source reliability as a task in its own right, using manual gold annotations specific for the task and assigned by independent fact-checking journalists. Moreover, we address the problem as one of ordinal regression on a three-point scale, and we solve it jointly with political ideology prediction in a multi-task learning setup, using several auxiliary tasks. Predicting Political Ideology In p"
N19-1216,C18-1287,0,0.129334,"Missing"
N19-1216,P18-1022,0,0.0920354,"for the task and assigned by independent fact-checking journalists. Moreover, we address the problem as one of ordinal regression on a three-point scale, and we solve it jointly with political ideology prediction in a multi-task learning setup, using several auxiliary tasks. Predicting Political Ideology In previous work, political ideology, also known as media bias, was used as a feature for “fake news” detection (Horne et al., 2018a). It has also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on prop"
N19-1216,P17-1068,0,0.165651,"Missing"
N19-1216,D17-1317,0,0.141066,"vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates, 2018). Unlike the above work, here we focus on predicting the political ideology of news media outlets. In our previous work (Baly et al., 2018), we did target the political bias of entire news outlets, as opposed to working at the article level (we also modeled factuality of reporting, but as a separate task without trying multi-task learning). In addition to the text of the articles published by the target news medium, we used features extracted fr"
N19-1216,S17-2088,1,0.917538,"from its corresponding Wikipedia page and Twitter profile, as well as analysis of its URL structure and traffic information about it from Alexa rank. In the present work, we use a similar set of features, but we treat the problem as one of ordinal regression. Moreover, we model the political ideology and the factuality of reporting jointly in a multitask learning setup, using several auxiliary tasks. Multitask Ordinal Regression Ordinal regression is well-studied and is commonly used for text classification on an ordinal scale, e.g., for sentiment analysis on a 5-point scale (He et al., 2016; Rosenthal et al., 2017a). However, multi-task ordinal regression remains an understudied problem. Yu et al. (2006) proposed a Bayesian framework for collaborative ordinal regression, and demonstrated that modeling multiple ordinal regression tasks outperforms single-task models. 2110 Walecki et al. (2016) were interested in jointly predicting facial action units and their intensity level. They argued that, due to the high number of classes, modeling these tasks independently would be inefficient. Thus, they proposed the copula ordinal regression model for multi-task learning and demonstrated that it can outperform"
N19-1216,D13-1010,0,0.070197,"as also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates, 2018). Unlike the above work, here we focus on predicting the political ideology of news media outlets. In our previous work (Baly et al., 2018), we did target the political bias of entire news outlets, as opposed to worki"
N19-4014,D18-1389,1,0.83326,"tant sentences according to their stance scores with respect to the claim. The stance detection results for the document are further shown as pie chart at the left side of the view (Section 2.3), and the linguistic analysis is shown at the bottom of the view (Section 2.4). 5 Related Work Automatic fact checking (Xu et al., 2018) centers on evidence extraction for given claims, re6 http://fakta.mit.edu http://fakta.mit.edu/video 8 https://github.com/moinnadeem/fakta 7 5 We used Intel AI’s Distiller (Zmora et al., 2018) to compress the model. 81 References liability evaluation of media sources (Baly et al., 2018a), stance detection of documents with respect to claims (Mohtarami et al., 2018; Xu et al., 2018; Baly et al., 2018b), and fact checking of claims (Mihaylova et al., 2018). These steps correspond to different Natural Language Processing (NLP) and Information Retrieval (IR) tasks including information extraction and question answering (Shiralkar et al., 2017). Veracity inference has been mostly approached as text classification problem and mainly tackled by developing linguistic, stylistic, and semantic features (Rashkin et al., 2017; Mihaylova et al., 2018; Nakov et al., 2017), as well as usi"
N19-4014,N18-2004,1,0.902392,"Missing"
N19-4014,karadzhov-etal-2017-fully,0,0.080083,"Missing"
N19-4014,D18-1143,0,0.0267702,"Missing"
N19-4014,K15-1032,0,0.0293593,"e Section 2.5). We describe the components below. We present FAKTA which is a unified framework that integrates various components of a fact checking process: document retrieval from media sources with various types of reliability, stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality of given claims and provides evidence at the document and sentence level to explain its predictions. 1 FAKTA Introduction With the rapid increase of fake news in social media and its negative influence on people and public opinion (Mihaylov et al., 2015; Mihaylov and Nakov, 2016; Vosoughi et al., 2018), various organizations are now performing manual fact checking on suspicious claims. However, manual factchecking is a time consuming and challenging process. As an alternative, researchers are investigating automatic fact checking which is a multi-step process and involves: (i) retrieving potentially relevant documents for a given claim (Mihaylova et al., 2018; Karadzhov et al., 2017), (ii) checking the reliability of the media sources from which documents are retrieved, (iii) predicting the stance of each document with respect to the claim ("
N19-4014,P13-1162,0,0.0280323,"d the aggregated scores are shown for each agree, disagree and discuss categories at the top of the ranked list of retrieved documents. Higher agree score indicates the claim is factually true, and higher disagree score indicates the claim is factually false. We analyze the language used in documents using the following linguistic markers: —Subjectivity lexicon (Riloff and Wiebe, 2003): which contains weak and strong subjective terms (we only consider the strong subjectivity cues), —Sentiment cues (Liu et al., 2005): which contains positive and negative sentiment cues, and —Wiki-bias lexicon (Recasens et al., 2013): which involves bias cues and controversial words (e.g., abortion and execute) extracted from the Neutral Point of View Wikipedia corpus (Recasens et al., 2013). Finally, we compute a score for the document using these cues according to Equation (2), where for each lexicon type Li and document Dj , the frequency of the cues for Li in Dj is normalized by the total number of words in Dj : P Li (Dj ) = count(wk , Dj ) (2) wk ∈Dj These scores are shown in a radar chart in Figure 2. Furthermore, FAKTA provides the option to see a lexicon-specific word cloud of frequent words in each documents (see"
N19-4014,P16-2065,0,0.0147438,"ribe the components below. We present FAKTA which is a unified framework that integrates various components of a fact checking process: document retrieval from media sources with various types of reliability, stance detection of documents with respect to given claims, evidence extraction, and linguistic analysis. FAKTA predicts the factuality of given claims and provides evidence at the document and sentence level to explain its predictions. 1 FAKTA Introduction With the rapid increase of fake news in social media and its negative influence on people and public opinion (Mihaylov et al., 2015; Mihaylov and Nakov, 2016; Vosoughi et al., 2018), various organizations are now performing manual fact checking on suspicious claims. However, manual factchecking is a time consuming and challenging process. As an alternative, researchers are investigating automatic fact checking which is a multi-step process and involves: (i) retrieving potentially relevant documents for a given claim (Mihaylova et al., 2018; Karadzhov et al., 2017), (ii) checking the reliability of the media sources from which documents are retrieved, (iii) predicting the stance of each document with respect to the claim (Mohtarami et al., 2018; Xu"
N19-4014,W03-1014,0,0.110076,"or a document retrieved for the claim “ISIS infilitrates the United States.” 2.4 Linguistic Analysis retrieved by our document retrieval component from each type of sources. All the stance scores are averaged across these documents, and the aggregated scores are shown for each agree, disagree and discuss categories at the top of the ranked list of retrieved documents. Higher agree score indicates the claim is factually true, and higher disagree score indicates the claim is factually false. We analyze the language used in documents using the following linguistic markers: —Subjectivity lexicon (Riloff and Wiebe, 2003): which contains weak and strong subjective terms (we only consider the strong subjectivity cues), —Sentiment cues (Liu et al., 2005): which contains positive and negative sentiment cues, and —Wiki-bias lexicon (Recasens et al., 2013): which involves bias cues and controversial words (e.g., abortion and execute) extracted from the Neutral Point of View Wikipedia corpus (Recasens et al., 2013). Finally, we compute a score for the document using these cues according to Equation (2), where for each lexicon type Li and document Dj , the frequency of the cues for Li in Dj is normalized by the total"
N19-4014,N18-1070,1,0.8894,"Missing"
N19-4014,nakov-etal-2017-trust,0,0.0193739,"of media sources (Baly et al., 2018a), stance detection of documents with respect to claims (Mohtarami et al., 2018; Xu et al., 2018; Baly et al., 2018b), and fact checking of claims (Mihaylova et al., 2018). These steps correspond to different Natural Language Processing (NLP) and Information Retrieval (IR) tasks including information extraction and question answering (Shiralkar et al., 2017). Veracity inference has been mostly approached as text classification problem and mainly tackled by developing linguistic, stylistic, and semantic features (Rashkin et al., 2017; Mihaylova et al., 2018; Nakov et al., 2017), as well as using information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017). These steps are typically handled in isolation. For example, previous works (Wang, 2017; OBrien et al., 2018) proposed algorithms to predict factuality of claims by mainly focusing on only input claims (i.e., step (iv) and their metadata information (e.g., the speaker of the claim). In addition, recent works on the Fact Extraction and VERification (FEVER) (Thorne et al., 2018) has focused on a specific domain (e.g., Wikipedia). To the best of our knowledge, there is currently no end-to-end sy"
N19-4014,N18-1074,0,0.0913751,"Missing"
N19-4014,P17-2067,0,0.0768173,"., 2018). These steps correspond to different Natural Language Processing (NLP) and Information Retrieval (IR) tasks including information extraction and question answering (Shiralkar et al., 2017). Veracity inference has been mostly approached as text classification problem and mainly tackled by developing linguistic, stylistic, and semantic features (Rashkin et al., 2017; Mihaylova et al., 2018; Nakov et al., 2017), as well as using information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017). These steps are typically handled in isolation. For example, previous works (Wang, 2017; OBrien et al., 2018) proposed algorithms to predict factuality of claims by mainly focusing on only input claims (i.e., step (iv) and their metadata information (e.g., the speaker of the claim). In addition, recent works on the Fact Extraction and VERification (FEVER) (Thorne et al., 2018) has focused on a specific domain (e.g., Wikipedia). To the best of our knowledge, there is currently no end-to-end systems for fact checking which can search through Wikipedia and mainstream media sources across the Web to fact check given claims. To address these gaps, our FAKTA system covers all fact-che"
N19-4014,D17-1317,0,0.0298169,"the model. 81 References liability evaluation of media sources (Baly et al., 2018a), stance detection of documents with respect to claims (Mohtarami et al., 2018; Xu et al., 2018; Baly et al., 2018b), and fact checking of claims (Mihaylova et al., 2018). These steps correspond to different Natural Language Processing (NLP) and Information Retrieval (IR) tasks including information extraction and question answering (Shiralkar et al., 2017). Veracity inference has been mostly approached as text classification problem and mainly tackled by developing linguistic, stylistic, and semantic features (Rashkin et al., 2017; Mihaylova et al., 2018; Nakov et al., 2017), as well as using information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017). These steps are typically handled in isolation. For example, previous works (Wang, 2017; OBrien et al., 2018) proposed algorithms to predict factuality of claims by mainly focusing on only input claims (i.e., step (iv) and their metadata information (e.g., the speaker of the claim). In addition, recent works on the Fact Extraction and VERification (FEVER) (Thorne et al., 2018) has focused on a specific domain (e.g., Wikipedia). To the best of our k"
P07-1064,W01-0514,0,0.484523,"ure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment boundaries by identifying homogeneous regions within a similarity matrix that encodes pairwise similarity between textual units, such as sentences. Our segmentation algorithm operates over a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related"
P07-1064,P03-1071,0,0.109752,"r applications where a speech recognizer is not available, or its output has a high word error rate. 1 Introduction An important practical application of topic segmentation is the analysis of spoken data. Paragraph breaks, section markers and other structural cues common in written documents are entirely missing in spoken data. Insertion of these structural markers can benefit multiple speech processing applications, including audio browsing, retrieval, and summarization. Not surprisingly, a variety of methods for topic segmentation have been developed in the 504 past (Beeferman et al., 1999; Galley et al., 2003; Dielmann and Renals, 2005). These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript. This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components. However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation. In order to process such data, we need a method for topic segmentation that does not require transcribed in"
P07-1064,P94-1002,0,0.038429,"scourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment boundaries by identifying homogeneous regions within a similarity matrix that encodes pairwise similarity between textual units, such as sentences. Our segmentation algorithm operates over a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data"
P07-1064,P96-1038,0,0.0266596,"cripts are unavailable or highly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers ex505 tensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment bounda"
P07-1064,E03-1058,0,0.316955,"these path-free blocks dilute segment homogeneity. This is problematic because it is not always possible to tell whether a sudden shift in scores signifies a transition or if it is just an artifact of irregularities in acoustic matching. Without additional matrix processing, these irregularities will lead the system astray. We further refine the acoustic comparison matrix using anisotropic diffusion. This technique has been developed for enhancing edge detection accuracy in image processing (Perona and Malik, 1990), and has been shown to be an effective smoothing method in text segmentation (Ji and Zha, 2003). When applied to a comparison matrix, anisotropic diffusion reduces score variability within homogeneous reMatrix Smoothing Equipped with a block distortion measure, we can now construct an acoustic comparison matrix. In principle, this matrix can be processed employing standard methods developed 2 We converted the original comparison distortion matrix to for text segmentation. However, as Figure 1 illus- the similarity matrix by subtracting the component distortions trates, the structure of the acoustic matrix is quite from the maximum alignment distortion score. 508 gions of the matrix and"
P07-1064,P06-1004,1,0.705878,"we can observe that the boundary structure in the diffused comparison matrix becomes more salient and corresponds more closely to the reference segmentation. 3.3 Matrix Partitioning Given a target number of segments k, the goal of the partitioning step is to divide a matrix into k square submatrices along the diagonal. This process is guided by an optimization function that maximizes the homogeneity within a segment or minimizes the homogeneity across segments. This optimization problem can be solved using one of many unsupervised segmentation approaches (Choi et al., 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006). In our implementation, we employ the minimumcut segmentation algorithm (Shi and Malik, 2000; Malioutov and Barzilay, 2006). In this graphtheoretic framework, segmentation is cast as a problem of partitioning a weighted undirected graph that minimizes the normalized-cut criterion. The minimum-cut method achieves robust analysis by jointly considering all possible partitionings of a document, moving beyond localized decisions. This allows us to aggregate comparisons from multiple locations, thereby compensating for the noise of individual matches. 4 Evaluation Set-Up Data We use a publicly ava"
P07-1064,J02-1002,0,0.377253,"Missing"
P07-1064,P01-1064,0,0.0639328,"Missing"
P07-1064,J01-3002,0,0.0210106,"er a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related to research on unsupervised lexical acquisition from continuous speech. These methods aim to infer vocabulary from unsegmented audio streams by analyzing regularities in pattern distribution (de Marcken, 1996; Brent, 1999; Venkataraman, 2001). Traditionally, the speech signal is first converted into a string-like representation such as phonemes and syllables using a phonetic recognizer. Park and Glass (2006) have recently shown the feasibility of an audio-based approach for word discovery. They induce the vocabulary from the audio stream directly, avoiding the need for phonetic transcription. Their method can accurately discover words which appear with high frequency in the audio stream. While the results obtained by Park and Glass inspire our approach, we cannot directly use their output as proxies for words in topic segmentation"
P08-2039,2007.iwslt-1.1,0,0.01466,"ll set of 3 million words, we also experimented with subsets of 1.6 million and 600K words. For the language model, we used 20 million words from the LDC Arabic Gigaword corpus plus 3 million words from the training data. After experimenting with different language model orders, we used 4-grams for the baseline system and 6-grams for the segmented Arabic. The English source is downcased and the punctuations are separated. The average sentence length is 33 for English, 25 for non-segmented Arabic and 36 for segmented Arabic. For the spoken language domain, we use the IWSLT 2007 Arabic-English (Fordyce, 2007) corpus which consists of a 200,000 word training set, a 500 sentence tuning set and a 500 sentence test set. We use the Arabic side of the training data to train the language model and use trigrams for the baseline system and a 4-grams for segmented Arabic. The average sentence length is 9 for English, 8 for Arabic, and 10 for segmented Arabic. 2 Since most of the data was originally intended for Arabicto-English translation our test and tuning sets have only one reference 4.2 Recombination Results To test the different recombination schemes described in Section 3.2, we run these schemes on t"
P08-2039,N06-2013,0,0.207882,"and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation. 1 2 Introduction Arabic has a complex morphology compared to English. Words are inflected for gender, number, and sometimes grammatical case, and various clitics can attach to word stems. An Arabic corpus will therefore have more surface forms than an English corpus of the same size, and will also be more sparsely populated. These factors adversely affect the performance of Arabic↔English Statistical Machine Translation (SMT). In prior work (Lee, 2004; Habash and Sadat, 2006), it has been shown that morphological segmentation of the Arabic source benefits the performance of Arabic-to-English SMT. The use of similar techniques for English-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task. In this paper, we present an initial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding improves Previous Work Th"
P08-2039,D07-1091,0,0.0802569,"egmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. Both works show that the improvements obtained from segmentation decrease as the corpus size increases. As will be shown later, we observe the same trend, which is due to the fact that the model becomes less sparse with more training data. There has been work on translating from English to other morphologically complex languages. Koehn and Hoang (2007) present Factored Translation Models as an extension to phrase-based statistical machine translation models. Factored models allow the integration of additional morphological fea153 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 153–156, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tures, such as POS, gender, number, etc. at the word level on both source and target sides. The tighter integration of such features was claimed to allow more explicit modeling of the morphology, and is better than using pre-processing and post-processing techn"
P08-2039,N04-4015,0,0.284711,"e corpora, and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation. 1 2 Introduction Arabic has a complex morphology compared to English. Words are inflected for gender, number, and sometimes grammatical case, and various clitics can attach to word stems. An Arabic corpus will therefore have more surface forms than an English corpus of the same size, and will also be more sparsely populated. These factors adversely affect the performance of Arabic↔English Statistical Machine Translation (SMT). In prior work (Lee, 2004; Habash and Sadat, 2006), it has been shown that morphological segmentation of the Arabic source benefits the performance of Arabic-to-English SMT. The use of similar techniques for English-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task. In this paper, we present an initial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding"
P08-2039,P03-1021,0,0.00911379,"Missing"
P08-2039,P00-1056,0,0.197475,"Missing"
P08-2039,2001.mtsummit-papers.68,0,0.0402414,"Missing"
P08-2039,N07-2037,0,0.203855,"benefits the performance of Arabic-to-English SMT. The use of similar techniques for English-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task. In this paper, we present an initial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding improves Previous Work The only previous work on English-to-Arabic SMT that we are aware of is by Sarikaya and Deng (2007). It uses shallow segmentation, and does not make use of contextual information. The emphasis of that work is on using Joint Morphological-Lexical Language Models to rerank the output. Most of the related work, though, is on Arabic-toEnglish SMT. Lee (2004) uses a trigram language model to segment Arabic words. She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they pro"
P08-2039,P05-1071,0,\N,Missing
P08-2039,P02-1040,0,\N,Missing
P12-1005,P08-2042,0,0.670473,"deling by solving three sub-problems of the task: segmentation, clustering and modeling each cluster. The key difference, however, is that our model does not assume independence among the three aspects of the problem, which allows our model to refine its solution to one sub-problem by exploiting what it has learned about other parts of the problem. Second, unlike (Lee et al., 1988; Garcia and Gish, 2006) in which the number of sub-word units to be learned is assumed to be known, our model learns the proper size from the training data directly. Instead of segmenting utterances, the authors of (Varadarajan et al., 2008) trained a single state HMM using all data at first, and then iteratively split the HMM states based on objective functions. This method achieved high performance in a phone recognition task using a label-to-phone transducer trained from some transcriptions. However, the performance seemed to rely on the quality of the transducer. For our work, we assume no transcriptions are available and measure the quality of the learned acoustic units via a spoken query detection task as in Jansen and Church (2011). Jansen and Church (2011) approached the task of unsupervised acoustic modeling by first dis"
P17-1047,W08-0704,0,0.0340729,"k and Glass (2008), which discovers repetitions of the same words and phrases in a collection of untranscribed acoustic data. Many subsequent efforts extended these ideas (Jansen et al., 2010; Jansen and Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang and Glass, 2009). Alternative approaches based on Bayesian nonparametric modeling (Lee and Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phonemelike tokens into higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015). Experimental Data We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data. The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner. The data collection scheme is described in detail in Harwath et al. (2016), but the experiments in this paper leverage nearly twice the amount of data. For training our multimodal neural network as well as the p"
P17-1047,D10-1045,0,0.0186303,"has garnered much attention recently is unsupervised 506 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 506–517 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1047 2 speech pattern discovery. Segmental Dynamic Time Warping (S-DTW) was introduced by Park and Glass (2008), which discovers repetitions of the same words and phrases in a collection of untranscribed acoustic data. Many subsequent efforts extended these ideas (Jansen et al., 2010; Jansen and Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang and Glass, 2009). Alternative approaches based on Bayesian nonparametric modeling (Lee and Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phonemelike tokens into higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015). Experimental Data We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data."
P17-1047,P12-1005,1,0.23208,"Linguistics, pages 506–517 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1047 2 speech pattern discovery. Segmental Dynamic Time Warping (S-DTW) was introduced by Park and Glass (2008), which discovers repetitions of the same words and phrases in a collection of untranscribed acoustic data. Many subsequent efforts extended these ideas (Jansen et al., 2010; Jansen and Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang and Glass, 2009). Alternative approaches based on Bayesian nonparametric modeling (Lee and Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phonemelike tokens into higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015). Experimental Data We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data. The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them ve"
P17-1047,Q15-1028,1,0.404611,"Missing"
P17-1047,C16-1124,0,0.12443,"ctrograms as if they were black and white images. Our spectrograms are computed using 40 log Mel The computer vision and NLP communities have begun to leverage deep learning to create multimodal models of images and text. Many works have focused on generating annotations or text captions for images (Socher and Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy and Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016). One interesting intersection between word induction from phoneme strings and multimodal modeling of images and text is that of Gelderloos and Chrupaa (2016), who uses images to segment words within captions at the phoneme string level. Other work has taken these ideas beyond text, and attempted to relate images to spoken audio captions directly at the waveform level (Roy, 2003; Harwath and Glass, 2015; Harwath et al., 2016). The work of (Harwath et al., 2016) is the most similar to ours, in which the authors learned embeddings at the entire image and entire spoken caption level and then used the embeddings to perform bidirectional retrieval. In this work, we go further by automatically segmenting and clustering the spoken captions into individual"
P17-1047,Q14-1017,0,0.00748223,"onnected layer into our 1024-dimensional multimodal embedding space. In our experiments, the weights of this projection layer are trained, but the layers taken from the VGG network below it are kept fixed. The second branch of our network analyzes speech spectrograms as if they were black and white images. Our spectrograms are computed using 40 log Mel The computer vision and NLP communities have begun to leverage deep learning to create multimodal models of images and text. Many works have focused on generating annotations or text captions for images (Socher and Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy and Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016). One interesting intersection between word induction from phoneme strings and multimodal modeling of images and text is that of Gelderloos and Chrupaa (2016), who uses images to segment words within captions at the phoneme string level. Other work has taken these ideas beyond text, and attempted to relate images to spoken audio captions directly at the waveform level (Roy, 2003; Harwath and Glass, 2015; Harwath et al., 2016). The work of (Harwath et al., 2016) is the most similar to o"
P17-1080,2015.iwslt-evaluation.11,0,0.0948384,"Missing"
P17-1080,P16-1100,0,0.0161522,"Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012). In neural MT, such units are obtained in a pre-processing step—e.g. by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016)— or learned during training using a character-based convolutional/recurrent sub-network (Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016; Jozefowicz et al., 2016; Sajjad et al., 2017). We evaluate the quality of different representations learned by an MT system augmented with a character CNN in terms of POS and morphological tagging, and contrast them with a purely word-based system. 7 Conclusion Neural networ"
P17-1080,D10-1015,0,0.0631953,"echniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Word Char POS Accuracy ENC DEC BLEU Ar-En En-Ar 89.62 95.35 24.69 28.42 43.93 44.54 13.37 13.00 Table 4: POS tagging accuracy using word-based and char-based encoder/decoder representations. Knight, 2003; Badr et al., 2008) and factored translation and reordering models (Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012). In neural MT, such units are obtained in a pre-processing step—e.g. by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016)— or learned during training using a character-based convolutional/recurrent sub-network (Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language"
P17-1080,D13-1032,0,0.0236083,"Missing"
P17-1080,P12-2059,0,0.0233233,"(Nieflen and Ney, 2000; Koehn and Word Char POS Accuracy ENC DEC BLEU Ar-En En-Ar 89.62 95.35 24.69 28.42 43.93 44.54 13.37 13.00 Table 4: POS tagging accuracy using word-based and char-based encoder/decoder representations. Knight, 2003; Badr et al., 2008) and factored translation and reordering models (Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012). In neural MT, such units are obtained in a pre-processing step—e.g. by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016)— or learned during training using a character-based convolutional/recurrent sub-network (Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa"
P17-1080,C00-2162,0,0.212466,"om a neural MT encoder to predict syntactic properties on the English source side. In contrast, we focus on representations in morphologically-rich languages and evaluate both source and target sides across several criteria. Vylomova et al. (2016) also analyze different representations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations. Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Word Char POS Accuracy ENC DEC BLEU Ar-En En-Ar 89.62 95.35 24.69 28.42 43.93 44.54 13.37 13.00 Table 4: POS tagging accuracy using word-based and char-based encoder/decoder representations. Knight, 2003; Badr et al., 2008) and factored translation and reordering models (Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 20"
P17-1080,E03-1076,0,0.199308,"Missing"
P17-1080,pasha-etal-2014-madamira,0,0.026797,"Missing"
P17-1080,D15-1246,0,0.0820648,"Missing"
P17-1080,D16-1079,0,0.0296875,"itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward neural network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to extract useful information from them.2 We empha2 We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings. 862 Train Tokens Dev Tokens Test Tokens POS Tags Morph Tags Ar De Fr Cz Gold Pred BLEU Gold/Pred Gold/Pred Pred Pred 0.5M/2.7M 63K/114K 62K/16K 0.9M/4M 45K/50K 44K/25K 5.2M 55K 23K 2M 35K 20K Word/Char Word/Char Word/Char 42 1969 54 214 33 – 368 – 80.31/93.66 78.20/92.48 87.68/94.57 – – 89.62/95.35 88.33/94.66 93.54/94.63 94.61/95.55 75.71/79.10 24.7/28.4 9.9/10.7 29.6/30.4 37.8/38.8 23.2/25.4 Table 1: Statistics for annotated corpora in Arabic (Ar), German (De), French (Fr), and Czech (Cz). size that our goal is not to beat the state-of-the-art on a given task, b"
P17-1080,P16-1140,0,0.150611,"itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward neural network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to extract useful information from them.2 We empha2 We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings. 862 Train Tokens Dev Tokens Test Tokens POS Tags Morph Tags Ar De Fr Cz Gold Pred BLEU Gold/Pred Gold/Pred Pred Pred 0.5M/2.7M 63K/114K 62K/16K 0.9M/4M 45K/50K 44K/25K 5.2M 55K 23K 2M 35K 20K Word/Char Word/Char Word/Char 42 1969 54 214 33 – 368 – 80.31/93.66 78.20/92.48 87.68/94.57 – – 89.62/95.35 88.33/94.66 93.54/94.63 94.61/95.55 75.71/79.10 24.7/28.4 9.9/10.7 29.6/30.4 37.8/38.8 23.2/25.4 Table 1: Statistics for annotated corpora in Arabic (Ar), German (De), French (Fr), and Czech (Cz). size that our goal is not to beat the state-of-the-art on a given task, b"
P17-1080,P17-2095,1,0.861424,"Missing"
P17-1080,Q16-1027,0,0.0145163,"PUNC) whose accuracy improves quite a lot. Then there are plural nouns (NNS, DT+NNS) where the char-based model really shines, which makes sense linguistically as plurality in Arabic is usually expressed by certain suffixes (“-wn/yn” for masc. plural, “-At” for fem. plural). The charbased model is thus especially good with frequent tags and infrequent words, which is understandable given that infrequent words typically belong to frequent open categories like nouns and verbs. 4.2 Effect of encoder depth Modern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016). We would like to understand what kind of information different layers capture. Given a trained NMT model with multiple layers, we extract feature representations from the different layers in the encoder. Let ENCli (s) denote the encoded representation of word wi after the l-th layer. We can vary l and train different classifiers to predict POS or morphological tags. Here we focus on the case of a 2-layer encoder-decoder model for simplicity (l 2 {1, 2}). Figure 6: POS tagging accuracy using representations from layers 0 (word vectors), 1, and 2, taken from encoders of different language pair"
P17-1080,C94-1027,0,0.0759209,"Missing"
P17-1080,P16-1162,0,0.118047,"Missing"
P17-1080,E17-1100,0,0.0558147,"Missing"
P17-1080,D07-1091,0,\N,Missing
P17-1080,P08-2039,1,\N,Missing
P17-1080,P10-1048,1,\N,Missing
P17-1080,C14-1041,1,\N,Missing
P17-1080,P16-2058,0,\N,Missing
P17-1080,C16-1124,0,\N,Missing
P17-1080,D16-1159,0,\N,Missing
P17-1080,2012.eamt-1.60,0,\N,Missing
P19-1144,N16-1024,0,0.196716,"Missing"
P19-1144,D14-1162,0,0.0837707,"Missing"
P19-1144,P18-1108,0,0.099894,"Missing"
P19-1144,N16-1036,0,0.0698135,"Missing"
Q15-1028,E09-3001,0,0.0226941,"supervision, but have simplified the joint learning problem discussed in the introduction in different ways. Spoken Term Discovery Spoken term discovery is the problem of using unsupervised pattern discovery methods to find previously unknown keywords in speech. Most models in this literature have typically made use of a two-stage procedure: First, subsequences of the input that are similar in an acoustic feature space are identified, and, then clustered to discover categories corresponding to lexical items (Park and Glass, 2008; Zhang and Glass, 2009; Zhang et al., 2012; Jansen et al., 2010; Aimetti, 2009; McInnes and Goldwater, 2011). This problem was first examined by Park and Glass (2008) 390 who used Dynamic Time Warping to identify similar acoustic sequences across utterances. The input sequences discovered by this method were then treated as nodes in a similarity-weighted graph, and graph clustering algorithms were applied to produce a number of densely connected groups of acoustic sequences, corresponding to lexical items. Building on this work, Zhang and Glass (2009) and Zhang et al. (2012) proposed robust features that allowed lexical units to be discovered from spoken documents gener"
Q15-1028,C04-1152,0,0.0438165,"symbols and attempt to identify subsequences corresponding to lexical items. The problem has been the focus of many years of intense research, and there are a large variety of proposals in the literature (Harris, 1955; Saffran et al., 1996a; Harris, 1955; Olivier, 1968; Saffran et al., 1996b; Brent, 1999b; Frank et al., 2010; Frank et al., 2013). Of particular interest here are models which treat segmentation as a secondary consequence of discovering a compact lexicon which explains the distribution of phoneme sequences in the input (Cartwright and Brent, 1994; Brent, 1999a; Goldsmith, 2001; Argamon et al., 2004; Goldsmith, 2006; Creutz and Lagus, 2007; Goldwater et al., 2009; Mochihashi et al., 2009; Elsner et al., 2013; Neubig et al., 2012; Heymann et al., 2013; De Marcken, 1996c; De Marcken, 1996a). Recently, a number of such models have been introduced which make use of Bayesian nonparametric distributions such as the Dirichlet Process (Ferguson, 1973) or its two-parameter generalization, the Pitman-Yor Process (Pitman, 1992), to define a prior which favors smaller lexicons with more reusable lexical items. The first such models were proposed in Goldwater (2006) and, subsequently, have been exten"
Q15-1028,Q14-1008,0,0.140964,"Missing"
Q15-1028,P96-1044,0,0.408579,"Missing"
Q15-1028,D13-1005,0,0.0860207,"many years of intense research, and there are a large variety of proposals in the literature (Harris, 1955; Saffran et al., 1996a; Harris, 1955; Olivier, 1968; Saffran et al., 1996b; Brent, 1999b; Frank et al., 2010; Frank et al., 2013). Of particular interest here are models which treat segmentation as a secondary consequence of discovering a compact lexicon which explains the distribution of phoneme sequences in the input (Cartwright and Brent, 1994; Brent, 1999a; Goldsmith, 2001; Argamon et al., 2004; Goldsmith, 2006; Creutz and Lagus, 2007; Goldwater et al., 2009; Mochihashi et al., 2009; Elsner et al., 2013; Neubig et al., 2012; Heymann et al., 2013; De Marcken, 1996c; De Marcken, 1996a). Recently, a number of such models have been introduced which make use of Bayesian nonparametric distributions such as the Dirichlet Process (Ferguson, 1973) or its two-parameter generalization, the Pitman-Yor Process (Pitman, 1992), to define a prior which favors smaller lexicons with more reusable lexical items. The first such models were proposed in Goldwater (2006) and, subsequently, have been extended in a number of ways (Goldwater et al., 2009; Neubig et al., 2012; Heymann et al., 2013; Mochihashi et al.,"
Q15-1028,W06-1673,0,0.0117767,"uch as a rule or a symbol. More specifically, we combine the PCFG that approximates the adaptor grammar with the noisychannel PCFG whose rules are weighted as in Eq. 2 to form a new PCFG G0 . The new PCFG G0 is thus a grammar that can be used to parse the terminals ~v i and generate derivations that are rooted at the start symbol of the AG. Therefore, we transform the task of sampling d0i and o0i to the task of generating a parse for ~v i using G0 , which can be efficiently solved by using an variant of the Inside algorithm for PCFGs (Lari and Young, 1990; Johnson et al., 2007; Goodman, 1998; Finkel et al., 2006). 4.2 Sampling ~v i and z i Given the top-layer PLUs ui and the speech data xi , sampling the boundary variables z i and the bottomlayer PLUs ~v i is equivalent to sampling an alignment between ui and xi . Therefore, we use the probabilities defined in Eq. 2 and employ the backward message-passing and forward-sampling algorithm described in Lee et al. (2013), designed for aligning a letter sequence and speech signals, to propose samples for ~v i and z i . The proposals are then accepted by using the standard MH criterion. 4.3 Sampling π Given z i and ~v i of each utterance in the corpus, gener"
Q15-1028,W04-2902,1,0.288818,"for each feature vector. From the state and mixture assignments, we can collect the counts that are needed to update the priors for the transition probability and the emission distribution of each state in πl . New samples for the parameters of πl can thus be yielded from the updated priors. 5 5.1 Experimental Setup Dataset To the best of our knowledge, there are no standard corpora for evaluating models of unsupervised lexicon discovery. In this paper, we perform experiments on the six lecture recordings used in (Park and Glass, 2008; Zhang and Glass, 2009), a part of the MIT Lecture corpus (Glass et al., 2004). A brief summary of the six lectures is listed in Table 1. 5.2 Systems Full system We constructed two systems based on the model described in Sec. 3. These systems, FullDP and Full50, differ only in the size of the PLU inventory (K). For FullDP, we set the value of K to be the number of PLUs discovered for each lecture by the DPHMM framework presented in (Lee and Glass, 2012). These numbers were: Economics, 99; Speech Processing, 111; Clustering, 91; Speaker Adaptation, 83; Physics, 90; and Algebra, 79. For Full50, we used a fixed number of PLUs, K = 50. The acoustic component of the FullDP s"
Q15-1028,J01-2001,0,0.133084,"mented strings of symbols and attempt to identify subsequences corresponding to lexical items. The problem has been the focus of many years of intense research, and there are a large variety of proposals in the literature (Harris, 1955; Saffran et al., 1996a; Harris, 1955; Olivier, 1968; Saffran et al., 1996b; Brent, 1999b; Frank et al., 2010; Frank et al., 2013). Of particular interest here are models which treat segmentation as a secondary consequence of discovering a compact lexicon which explains the distribution of phoneme sequences in the input (Cartwright and Brent, 1994; Brent, 1999a; Goldsmith, 2001; Argamon et al., 2004; Goldsmith, 2006; Creutz and Lagus, 2007; Goldwater et al., 2009; Mochihashi et al., 2009; Elsner et al., 2013; Neubig et al., 2012; Heymann et al., 2013; De Marcken, 1996c; De Marcken, 1996a). Recently, a number of such models have been introduced which make use of Bayesian nonparametric distributions such as the Dirichlet Process (Ferguson, 1973) or its two-parameter generalization, the Pitman-Yor Process (Pitman, 1992), to define a prior which favors smaller lexicons with more reusable lexical items. The first such models were proposed in Goldwater (2006) and, subsequ"
Q15-1028,C10-1060,0,0.237946,"arcken, 1996c; De Marcken, 1996a). Recently, a number of such models have been introduced which make use of Bayesian nonparametric distributions such as the Dirichlet Process (Ferguson, 1973) or its two-parameter generalization, the Pitman-Yor Process (Pitman, 1992), to define a prior which favors smaller lexicons with more reusable lexical items. The first such models were proposed in Goldwater (2006) and, subsequently, have been extended in a number of ways (Goldwater et al., 2009; Neubig et al., 2012; Heymann et al., 2013; Mochihashi et al., 2009; Elsner et al., 2013; Johnson et al., 2006; Johnson and Demuth, 2010; Johnson and Goldwater, 2009b; Johnson, 2008a; Johnson, 2008b). One important lesson that has emerged from this literature is that models which jointly represent multiple levels of linguistic structure often benefit from synergistic interactions (Johnson, 2008b) where different levels of linguistic structure provide mutual constraints on one another which can be exploited simultaneously (Goldwater et al., 2009; Johnson et al., 2006; Johnson and Demuth, 2010; Johnson and Goldwater, 2009b; Johnson, 2008a; B¨orschinger and Johnson, 2014; Johnson, 2008b). For example, Elsner et al. (2013) show th"
Q15-1028,N09-1036,0,0.0590422,"addressing the unsupervised discovery of phone-like units from acoustic data—in particular the Dirichlet Process Hidden Markov Model (DPHMM) of Lee and Glass (2012)—and the un389 Transactions of the Association for Computational Linguistics, vol. 3, pp. 389–403, 2015. Action Editor: Eric Fosler-Lussier. Submission batch: 11/2014; Revision batch 2/2015; Published 7/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. supervised learning of lexicons from unsegmented symbolic sequences using the Adaptor Grammar (AG) framework (Johnson et al., 2006; Johnson and Goldwater, 2009a). We integrate these models with a component modeling variability in the surface realization of phoneme-like units within lexical units. In the next section, we give an overview of related work. Following this, we present our model and inference algorithms. We then turn to preliminary evaluations of the model’s performance, showing that the model is competitive with state-of-theart single-speaker spoken term discovery systems, and providing several analyses which examine the kinds of structures learned by the model. We also suggest that the ability of the system to successfully unify multipl"
Q15-1028,N07-1018,0,0.0120641,"which w can be any countable entity such as a rule or a symbol. More specifically, we combine the PCFG that approximates the adaptor grammar with the noisychannel PCFG whose rules are weighted as in Eq. 2 to form a new PCFG G0 . The new PCFG G0 is thus a grammar that can be used to parse the terminals ~v i and generate derivations that are rooted at the start symbol of the AG. Therefore, we transform the task of sampling d0i and o0i to the task of generating a parse for ~v i using G0 , which can be efficiently solved by using an variant of the Inside algorithm for PCFGs (Lari and Young, 1990; Johnson et al., 2007; Goodman, 1998; Finkel et al., 2006). 4.2 Sampling ~v i and z i Given the top-layer PLUs ui and the speech data xi , sampling the boundary variables z i and the bottomlayer PLUs ~v i is equivalent to sampling an alignment between ui and xi . Therefore, we use the probabilities defined in Eq. 2 and employ the backward message-passing and forward-sampling algorithm described in Lee et al. (2013), designed for aligning a letter sequence and speech signals, to propose samples for ~v i and z i . The proposals are then accepted by using the standard MH criterion. 4.3 Sampling π Given z i and ~v i o"
Q15-1028,W08-0704,0,0.12135,"ing that the model is competitive with state-of-theart single-speaker spoken term discovery systems, and providing several analyses which examine the kinds of structures learned by the model. We also suggest that the ability of the system to successfully unify multiple acoustic sequences into single lexical items relies on the phonological-variability (noisy channel) component of the model—demonstrating the importance of modeling symbolic variation in phonological units. We provide preliminary evidence that simultaneously learning sound and lexical structure leads to synergistic interactions (Johnson, 2008b)—the various components of the model mutually constrain one another such that the linguistic structures learned by each are more accurate than if they had been learned independently. 2 Related Work Previous models of lexical unit discovery have primarily fallen into two classes: models of spoken term discovery and models of word segmentation. Both kinds of models have sought to identify lexical items from input without direct supervision, but have simplified the joint learning problem discussed in the introduction in different ways. Spoken Term Discovery Spoken term discovery is the problem"
Q15-1028,P08-1046,0,0.295038,"ing that the model is competitive with state-of-theart single-speaker spoken term discovery systems, and providing several analyses which examine the kinds of structures learned by the model. We also suggest that the ability of the system to successfully unify multiple acoustic sequences into single lexical items relies on the phonological-variability (noisy channel) component of the model—demonstrating the importance of modeling symbolic variation in phonological units. We provide preliminary evidence that simultaneously learning sound and lexical structure leads to synergistic interactions (Johnson, 2008b)—the various components of the model mutually constrain one another such that the linguistic structures learned by each are more accurate than if they had been learned independently. 2 Related Work Previous models of lexical unit discovery have primarily fallen into two classes: models of spoken term discovery and models of word segmentation. Both kinds of models have sought to identify lexical items from input without direct supervision, but have simplified the joint learning problem discussed in the introduction in different ways. Spoken Term Discovery Spoken term discovery is the problem"
Q15-1028,P12-1005,1,0.492566,"ce Laboratory 2 Department of Brain and Cognitive Sciences Massachusetts Institute of Technology Cambridge, MA 02139, USA Abstract acoustic signal correspond to which phonemes— while discounting surface variation in the realization of individual units—and infer which sequences of phonemes correspond to which words (amongst other challenges). We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns. 1 Introduction One of the most basic problems of language acquisition is accounting for how children learn the inventory of word forms from speech—phonological lexicon discovery. In learning a language, childr"
Q15-1028,D13-1019,1,0.832677,"orm the task of sampling d0i and o0i to the task of generating a parse for ~v i using G0 , which can be efficiently solved by using an variant of the Inside algorithm for PCFGs (Lari and Young, 1990; Johnson et al., 2007; Goodman, 1998; Finkel et al., 2006). 4.2 Sampling ~v i and z i Given the top-layer PLUs ui and the speech data xi , sampling the boundary variables z i and the bottomlayer PLUs ~v i is equivalent to sampling an alignment between ui and xi . Therefore, we use the probabilities defined in Eq. 2 and employ the backward message-passing and forward-sampling algorithm described in Lee et al. (2013), designed for aligning a letter sequence and speech signals, to propose samples for ~v i and z i . The proposals are then accepted by using the standard MH criterion. 4.3 Sampling π Given z i and ~v i of each utterance in the corpus, generating new samples for the parameters of each HMM πl for l ∈ L is straightforward. For each PLU l, we gather all speech segments that are mapped to 2 We use di and d0i to denote the current and the proposed parses. The same relationship is also defined for oi and o0i . 395 Duration 75 mins 85 mins 78 mins 74 mins 51 mins 47 mins Table 1: A brief summary of th"
Q15-1028,D07-1072,0,0.00766206,"re l0 ∈ L, which correspond to bottom-layer PLUs ~v i that are depicted as units in circles in Fig. 1(b)-(iv). Note that {l} and {l0 } are drawn from the same inventory of PLUs, and the notation is meant to signal that {l0 } are the terminal symbols of this grammar. The three sets of rules respectively map to the sub(·), split(·), and del(·) operations; thus, the probability of each edit operation is automatically captured by the corresponding rule probability. Note that to simultaneously infer a phonetic inventory of an unknown size and model phonetic variation, we can use the infinite PCFG (Liang et al., 2007) to formulate the noisy-channel model. However, for computational efficiency, in our experiments, we infer the size of the PLU inventory before training the full model, and impose a Dirichlet prior on the rule probability distribution associated with each nonterminal l. We explain how inventory size is determined in Sec. 5.2. 394 3. For vi,j,k ∈ v i,j , 1 ≤ k ≤ |v i,j |, generates the speech features using πvi,j,k , which deterministically sets the value of zi,t . Thus, the latent variables our model defines for each utterance are: di , ui , oi , ~v i , z i , π, {θ~q }q∈Nag ∪Nnoisy-channel . I"
Q15-1028,P09-1012,0,0.199532,"em has been the focus of many years of intense research, and there are a large variety of proposals in the literature (Harris, 1955; Saffran et al., 1996a; Harris, 1955; Olivier, 1968; Saffran et al., 1996b; Brent, 1999b; Frank et al., 2010; Frank et al., 2013). Of particular interest here are models which treat segmentation as a secondary consequence of discovering a compact lexicon which explains the distribution of phoneme sequences in the input (Cartwright and Brent, 1994; Brent, 1999a; Goldsmith, 2001; Argamon et al., 2004; Goldsmith, 2006; Creutz and Lagus, 2007; Goldwater et al., 2009; Mochihashi et al., 2009; Elsner et al., 2013; Neubig et al., 2012; Heymann et al., 2013; De Marcken, 1996c; De Marcken, 1996a). Recently, a number of such models have been introduced which make use of Bayesian nonparametric distributions such as the Dirichlet Process (Ferguson, 1973) or its two-parameter generalization, the Pitman-Yor Process (Pitman, 1992), to define a prior which favors smaller lexicons with more reusable lexical items. The first such models were proposed in Goldwater (2006) and, subsequently, have been extended in a number of ways (Goldwater et al., 2009; Neubig et al., 2012; Heymann et al., 2013"
Q19-1004,D16-1216,0,0.0227532,"at the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be Figure 2: A visualization of attention weights, showing soft alignment between source and target sentences in an NMT model. Reproduced from Bahdanau et al. (2014), with permission. noticed, as in the reordering of noun and adjective when translating the phrase ‘‘European Economic Area.’’ Another line of work computes various saliency measures to attribute predictions to input features. The important or salient features can then be visualized in selected examples (Li et al., 2016a; Aubakirova and Bansal, 2016; Sundararajan et al., 2017; Arras et al., 2017a,b; Ding et al., 2017; Murdoch et al., 2018; Mudrakarta et al., 2018; Montavon et al., 2018; Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018).7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989, 1990). Similar techniques have been followed by others. Recent examples include clustering o"
Q19-1004,K17-1037,0,0.344757,"ent models or model components. Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information. Methods The most common approach for associating neural network components with linguistic properties is to predict such properti"
Q19-1004,D17-1042,0,0.0106428,", 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007; Zhang et al., 2016),15 but this approach requires manual annotations of explanations, which may be hard to collect. An alternative approach is to use parts of the input as explanations. For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis. Alvarez-Melis and Jaakkola (2017) discovered input–output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations. Gupta and Sch¨utze (2018) inspected how information is accumulated in RNNs towards a prediction, and associated peaks in prediction scores with important input segments. As these methods use input segments to explain predictions, they do not shed much light on the internal computations that take place in the network. At present, despite the recognized importance for interpretability, our ability to explain predictions of neural networks in NLP i"
Q19-1004,N18-1205,0,0.0363702,"Missing"
Q19-1004,S17-2001,0,0.0170672,"Missing"
Q19-1004,P17-1057,0,0.048326,"Missing"
Q19-1004,P18-1126,0,0.0383012,"Missing"
Q19-1004,P18-1198,0,0.100808,"tions and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references.5 It is referred to by various names, including ‘‘auxiliary prediction tasks’’ (Adi et al., 2017b), ‘‘diagnostic classifiers’’ (Veldhoen et al., 2016), and ‘‘probing tasks’’ (Conneau et al., 2018). As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data—English→ French and English→German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn 2.2 Linguistic Phenomena Different kinds of linguistic information have been analyzed, rangi"
Q19-1004,P18-1241,0,0.22318,"ficity Adversarial attacks can be classified to targeted vs. non-targeted attacks (Yuan et al., 2017). A targeted attack specifies a specific false class, l0 , while a nontargeted attack cares only that the predicted class is wrong, l0 6= l. Targeted attacks are more difficult to generate, as they typically require knowledge of model parameters; that is, they are white-box attacks. This might explain why the majority of adversarial examples in NLP are nontargeted (see Table SM3). A few targeted attacks include Liang et al. (2018), which specified a desired class to fool a text classifier, and Chen et al. (2018a), which specified words or captions to generate in an image captioning model. Others targeted specific words to omit, replace, or include when attacking seq2seq models (Cheng et al., 2018; Ebrahimi et al., 2018a). Methods for generating targeted attacks in NLP could possibly take more inspiration from adversarial attacks in other fields. For instance, in attacking malware detection systems, several studies developed targeted attacks in a blackbox scenario (Yuan et al., 2017). A black-box targeted attack for MT was proposed by Zhao et al. (2018c), who used GANs to search for 14 These criteria"
Q19-1004,D15-1092,0,0.0260232,"embeddings. Some analysis has also been devoted to joint language–vision or audio–vision models, or to similarities between word embeddings and con volutional image representations. Table SM1 provides detailed references. 2.4 Limitations The classification approach may find that a certain amount of linguistic information is captured in the neural network. However, this does not necessarily mean that the information is used by the network. For example, Vanmassenhove et al. (2017) 6 Others found that even simple binary trees may work well in MT (Wang et al., 2018b) and sentence classification (Chen et al., 2015). 52 Figure 1: A heatmap visualizing neuron activations. In this case, the activations capture position in the sentence. (e.g., tree depth, coordination inversion) gain the most from using a deeper classifier. However, the approach is usually taken for granted; given its prevalence, it appears that better theoretical or empirical foundations are in place. 3 Visualization Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they cor"
Q19-1004,P18-2006,0,0.168718,"as they typically require computing gradients with respect to the input, which would be discrete in the text case. One option is to compute gradients with respect to the input word embeddings, and perturb the embeddings. Since this may result in a vector that does not correspond to any word, one could search for the closest word embedding in a given dictionary (Papernot et al., 2016b); Cheng et al. (2018) extended this idea to seq2seq models. Others computed gradients with respect to input word embeddings to identify and rank words to be modified (Samanta and Mehta, 2017; Liang et al., 2018). Ebrahimi et al. (2018b) developed an alternative method by representing text edit operations in vector space (e.g., a binary vector specifying which characters in a word would be changed) and approximating the change in loss with the derivative along this vector. Given the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples. Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so 5.2 Attack Specificity Adversarial attacks can be classified to targ"
Q19-1004,N15-1004,0,0.0508987,"Missing"
Q19-1004,W16-2524,0,0.0493146,"Missing"
Q19-1004,W16-2506,0,0.0221101,"rse-grained level, but some provide a more fine-grained evaluation of similarity or relatedness. For example, some datasets are dedicated for specific word classes such as verbs (Gerz et al., 2016) or rare words (Luong et al., 2013), or for evaluating compositional knowledge in sentence embeddings (Marelli et al., 2014). Multilingual and cross-lingual versions have also been collected (Leviant and Reichart, 2015; Cer et al., 2017). Although these datasets are widely used, this kind of evaluation has been criticized for its subjectivity and questionable correlation with downstream performance (Faruqui et al., 2016). 4.2 4.3 Languages As unfortunately usual in much NLP work, especially neural NLP, the vast majority of challenge sets are in English. This situation is slightly better in MT evaluation, where naturally all datasets feature other languages (see Table SM2). A notable exception is the work by Gulordava et al. (2018), who constructed examples for evaluating number agreement in language modeling in English, Russian, Hebrew, and Italian. Clearly, there is room for more challenge sets in nonEnglish languages. However, perhaps more pressing is the need for large-scale non-English datasets (besides M"
Q19-1004,N18-1091,0,0.0285527,"Missing"
Q19-1004,D16-1235,0,0.029272,"Missing"
Q19-1004,I17-1004,0,0.0615521,"Missing"
Q19-1004,D18-1537,0,0.0169249,"ermission. noticed, as in the reordering of noun and adjective when translating the phrase ‘‘European Economic Area.’’ Another line of work computes various saliency measures to attribute predictions to input features. The important or salient features can then be visualized in selected examples (Li et al., 2016a; Aubakirova and Bansal, 2016; Sundararajan et al., 2017; Arras et al., 2017a,b; Ding et al., 2017; Murdoch et al., 2018; Mudrakarta et al., 2018; Montavon et al., 2018; Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018).7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989, 1990). Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017). A few online tools for visualizing neural networks have recently become available. LSTMVis 7"
Q19-1004,P17-1047,1,0.872227,"Missing"
Q19-1004,W18-5408,0,0.0245989,"iers lead to overall better results, but do not alter the respective trends when comparing different models or components (Qian et al., 2016b; Belinkov, 2018). Interestingly, Conneau et al. (2018) found that tasks requiring more nuanced linguistic knowledge Neural Network Components In terms of the object of study, various neural network components were investigated, including word embeddings, RNN hidden states or gate activations, sentence embeddings, and attention weights in sequence-to-sequence (seq2seq) models. Generally less work has analyzed convolutional neural networks in NLP, but see Jacovi et al. (2018) for a recent exception. In speech processing, researchers have analyzed layers in deep neural networks for speech recognition and different speaker embeddings. Some analysis has also been devoted to joint language–vision or audio–vision models, or to similarities between word embeddings and con volutional image representations. Table SM1 provides detailed references. 2.4 Limitations The classification approach may find that a certain amount of linguistic information is captured in the neural network. However, this does not necessarily mean that the information is used by the network. For exam"
Q19-1004,W18-1807,0,0.0283029,"Missing"
Q19-1004,W18-2702,0,0.0252175,"1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron. The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rockt¨aschel et al., 2016; Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018). Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al. Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows). As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be Figure 2: A visualization of attention weights, showing"
Q19-1004,J15-4004,0,0.0351455,"de a comprehensive categorization like the one compiled here. 8 RNNVis (Ming et al., 2017) is a similar tool, but its online demo does not seem to be available at the time of writing. 54 and Isabelle and Kuhn (2018) prepared challenge sets for MT evaluation covering fine-grained phenomena at morpho-syntactic, syntactic, and lexical levels. Generally, datasets that are constructed programmatically tend to cover less fine-grained linguistic properties, while manually constructed datasets represent more diverse phenomena. to evaluate word embeddings (Finkelstein et al., 2002; Bruni et al., 2012; Hill et al., 2015, inter alia) or sentence embeddings; see the many shared tasks on semantic textual similarity in SemEval (Cer et al., 2017, and previous editions). Many of these datasets evaluate similarity at a coarse-grained level, but some provide a more fine-grained evaluation of similarity or relatedness. For example, some datasets are dedicated for specific word classes such as verbs (Gerz et al., 2016) or rare words (Luong et al., 2013), or for evaluating compositional knowledge in sentence embeddings (Marelli et al., 2014). Multilingual and cross-lingual versions have also been collected (Leviant and"
Q19-1004,D17-1215,0,0.0437099,"ammaticality or similarity of the adversarial examples to the original ones (Zhao et al., 2018c; Alzantot et al., 2018). Given the inherent difficulty in generating imperceptible changes in text, more such evaluations are needed. attacks on Google’s MT system after mapping sentences into continuous space with adversarially regularized autoencoders (Zhao et al., 2018b). 5.3 Linguistic Unit Most of the work on adversarial text examples involves modifications at the character- and/or word-level; see Table SM3 for specific references. Other transformations include adding sentences or text chunks (Jia and Liang, 2017) or generating paraphrases with desired syntactic structures (Iyyer et al., 2018). In image captioning, Chen et al. (2018a) modified pixels in the input image to generate targeted attacks on the caption text. 5.4 6 Explaining specific predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary"
Q19-1004,D17-1263,0,0.118606,"ogical properties (Burlot and Yvon, 2017). See Table SM2 for references to datasets targeting other phenomena. Other challenge sets cover a more diverse range of linguistic properties, in the spirit of some of the earlier work. For instance, extending the categories in Cooper et al. (1996), the GLUE analysis set for NLI covers more than 30 phenomena in four coarse categories (lexical semantics, predicate–argument structure, logic, and knowledge). In MT evaluation, Burchardt et al. (2017) reported results using a large test suite covering 120 phenomena, partly based on Lehmann et al. (1996).11 Isabelle et al. (2017) 4.4 Scale The size of proposed challenge sets varies greatly (Table SM2). As expected, datasets constructed by hand are smaller, with typical sizes in the hundreds. Automatically built datasets are much larger, ranging from several thousands to close to a hundred thousand (Sennrich, 2017), or even more than one million examples (Linzen et al., 2016). In the latter case, the authors argue that such a large test set is needed for obtaining a sufficient representation of rare cases. A few manually constructed datasets contain a fairly large number of examples, up to 10 thousand (Burchardt et al."
Q19-1004,P18-1027,0,0.0214438,"nonyms and other word lists (Samanta and Mehta, 2017; Yang et al., 2018). Some reported whether a human can classify the adversarial example correctly (Yang et al., 7 Other Methods We briefly mention here several analysis methods that do not fall neatly into the previous sections. A number of studies evaluated the effect of erasing or masking certain neural network components, such as word embedding dimensions, hidden units, or even full words (Li et al., 2016b; 15 Other work considered learning textual-visual explanations from multimodal annotations (Park et al., 2018). 58 Feng et al., 2018; Khandelwal et al., 2018; Bau et al., 2018). For example, Li et al. (2016b) erased specific dimensions in word embeddings or hidden states and computed the change in probability assigned to different labels. Their experiments revealed interesting differences between word embedding models, where in some models information is more focused in individual dimensions. They also found that information is more distributed in hidden layers than in the input layer, and erased entire words to find important words in a sentiment analysis task. Several studies conducted behavioral experiments to interpret word embeddings by defin"
Q19-1004,Q16-1023,0,0.0211532,"nterpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work. 1 Introduction The rise of deep learning has transformed the field of natural language processing (NLP) in recent years. Models based on neural networks have obtained impressive improvements in various tasks, including language modeling (Mikolov et al., 2010; Jozefowicz et al., 2016), syntactic parsing (Kiperwasser and Goldberg, 2016), machine translation (MT) (Bahdanau et al., 2014; Sutskever et al., 2014), and many other tasks; see Goldberg (2017) for example success stories. This progress has been accompanied by a myriad of new neural network architectures. In many cases, traditional feature-rich systems are being replaced by end-to-end neural networks that aim to map input text to some output prediction. As end-to-end systems are gaining prevalence, one may point to two trends. First, some push back against the abandonment of linguistic knowledge and call for incorporating it inside 1 See, for instance, Noah Smith’s in"
Q19-1004,N16-1082,0,0.249773,"on demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be Figure 2: A visualization of attention weights, showing soft alignment between source and target sentences in an NMT model. Reproduced from Bahdanau et al. (2014), with permission. noticed, as in the reordering of noun and adjective when translating the phrase ‘‘European Economic Area.’’ Another line of work computes various saliency measures to attribute predictions to input features. The important or salient features can then be visualized in selected examples (Li et al., 2016a; Aubakirova and Bansal, 2016; Sundararajan et al., 2017; Arras et al., 2017a,b; Ding et al., 2017; Murdoch et al., 2018; Mudrakarta et al., 2018; Montavon et al., 2018; Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018).7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989, 1990). Similar techniques have been followed by others. Recent"
Q19-1004,2001.mtsummit-papers.35,0,0.0713273,"judgments. Datasets containing such similarity scores are often used Challenge Sets The majority of benchmark datasets in NLP are drawn from text corpora, reflecting a natural frequency distribution of language phenomena. While useful in practice for evaluating system performance in the average case, such datasets may fail to capture a wide range of phenomena. An alternative evaluation framework consists of challenge sets, also known as test suites, which have been used in NLP for a long time (Lehmann et al., 1996), especially for evaluating MT systems (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001). Lehmann et al. (1996) noted several key properties of test suites: systematicity, control over data, inclusion of negative data, 9 One could speculate that their decrease in popularity can be attributed to the rise of large-scale quantitative evaluation of statistical NLP systems. 10 Another typology of evaluation protocols was put forth by Burlot and Yvon (2017). Their criteria are partially overlapping with ours, although they did not provide a comprehensive categorization like the one compiled here. 8 RNNVis (Ming et al., 2017) is a similar tool, but its online demo does not seem to be av"
Q19-1004,D15-1246,0,0.0879949,"Missing"
Q19-1004,Q16-1037,0,0.250987,"eural network models for speech, or in joint audio-visual models. See Table SM1 for references. While it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena. These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn. 5 A similar method has been used to analyze hierarchical structure in neural networks trained on arithmetic expressions (Veldhoen et al., 2016; Hupkes et al., 2018). 51 Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject–verb agreement in many common cases, while direct supervision is required for solving harder cases. Another theme that emerges in several studies is the hierarchical nature of the learned representations. We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017). Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018). Finally, a couple of papers discovered that mo"
Q19-1004,W18-3024,0,0.0104913,"nted; given its prevalence, it appears that better theoretical or empirical foundations are in place. 3 Visualization Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991). Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015; K´ad´ar et al., 2017; Qian et al., 2016a; Liu et al., 2018) and speech (Wu and King, 2016; Nagamine et al., 2015; Wang et al., 2017b). Figure 1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron. The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rockt¨aschel et al., 20"
Q19-1004,D16-1011,0,0.0259874,"c predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007; Zhang et al., 2016),15 but this approach requires manual annotations of explanations, which may be hard to collect. An alternative approach is to use parts of the input as explanations. For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis. Alvarez-Melis and Jaakkola (2017) discovered input–output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations. Gupta and Sch¨utze (2018) inspected how information is accumulated in RNNs towards a prediction, and associated peaks in prediction scores with important input segments. As these methods use input segments to explain predictions, they do not shed much light on th"
Q19-1004,W13-3512,0,0.0607766,"d linguistic properties, while manually constructed datasets represent more diverse phenomena. to evaluate word embeddings (Finkelstein et al., 2002; Bruni et al., 2012; Hill et al., 2015, inter alia) or sentence embeddings; see the many shared tasks on semantic textual similarity in SemEval (Cer et al., 2017, and previous editions). Many of these datasets evaluate similarity at a coarse-grained level, but some provide a more fine-grained evaluation of similarity or relatedness. For example, some datasets are dedicated for specific word classes such as verbs (Gerz et al., 2016) or rare words (Luong et al., 2013), or for evaluating compositional knowledge in sentence embeddings (Marelli et al., 2014). Multilingual and cross-lingual versions have also been collected (Leviant and Reichart, 2015; Cer et al., 2017). Although these datasets are widely used, this kind of evaluation has been criticized for its subjectivity and questionable correlation with downstream performance (Faruqui et al., 2016). 4.2 4.3 Languages As unfortunately usual in much NLP work, especially neural NLP, the vast majority of challenge sets are in English. This situation is slightly better in MT evaluation, where naturally all dat"
Q19-1004,W18-2903,0,0.0438856,"Missing"
Q19-1004,N18-1100,0,0.0283759,"Missing"
Q19-1004,C12-1118,0,0.0203432,"d the change in probability assigned to different labels. Their experiments revealed interesting differences between word embedding models, where in some models information is more focused in individual dimensions. They also found that information is more distributed in hidden layers than in the input layer, and erased entire words to find important words in a sentiment analysis task. Several studies conducted behavioral experiments to interpret word embeddings by defining intrusion tasks, where humans need to identify an intruder word, chosen based on difference in word embedding dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015).16 In this kind of work, a word embedding model may be deemed more interpretable if humans are better able to identify the intruding words. Since the evaluation is costly for high-dimensional representations, alternative automatic metrics were considered (Park et al., 2017; Senel et al., 2018). A long tradition in work on neural networks is to evaluate and analyze their ability to learn different formal languages (Das et al., 1992; Casey, 1996; Gers and Schmidhuber, 2001; Bod´en and Wiles, 2002; Chalup and Blair, 2003). This trend continues today, wi"
Q19-1004,C18-1198,0,0.0205847,"this comes at the expense of how natural the examples are. 4.6 contrastive pairs evaluation of Sennrich (2017). Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale. However, they may miss certain aspects. Thus a few studies report human evaluation on their challenge sets, such as in MT (Isabelle et al., 2017; Burchardt et al., 2017). We note here also that judging the quality of a model by its performance on a challenge set can be tricky. Some authors emphasize their wish to test systems on extreme or difficult cases, ‘‘beyond normal operational capacity’’ (Naik et al., 2018). However, whether one should expect systems to perform well on specially chosen cases (as opposed to the average case) may depend on one’s goals. To put results in perspective, one may compare model performance to human performance on the same task (Gulordava et al., 2018). 5 Adversarial Examples Understanding a model also requires an understanding of its failures. Despite their success in many tasks, machine learning systems can also be very sensitive to malicious attacks or adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). In the vision domain, small changes to the input"
Q19-1004,W18-5441,0,0.0314285,"Missing"
Q19-1004,K18-1047,0,0.0428315,"Missing"
Q19-1004,S18-2023,0,0.0379384,"Missing"
Q19-1004,D16-1079,0,0.398683,"ormation tends to be stored in the upper layer.’’ These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components. Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower laye"
Q19-1004,P16-1140,0,0.281048,"ormation tends to be stored in the upper layer.’’ These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components. Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower laye"
Q19-1004,D17-1041,0,0.0221322,"d entire words to find important words in a sentiment analysis task. Several studies conducted behavioral experiments to interpret word embeddings by defining intrusion tasks, where humans need to identify an intruder word, chosen based on difference in word embedding dimensions (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015).16 In this kind of work, a word embedding model may be deemed more interpretable if humans are better able to identify the intruding words. Since the evaluation is costly for high-dimensional representations, alternative automatic metrics were considered (Park et al., 2017; Senel et al., 2018). A long tradition in work on neural networks is to evaluate and analyze their ability to learn different formal languages (Das et al., 1992; Casey, 1996; Gers and Schmidhuber, 2001; Bod´en and Wiles, 2002; Chalup and Blair, 2003). This trend continues today, with research into modern architectures and what formal languages they can learn (Weiss et al., 2018; Bernardy, 2018; Suzgun et al., 2019), or the formal properties they possess (Chen et al., 2018b). 8 visualization via saliency measures or evaluation by adversarial examples. But even those sometimes require non-trivi"
Q19-1004,P18-1079,0,0.0634436,"Missing"
Q19-1004,D18-1179,0,0.117576,"Missing"
Q19-1004,W17-4702,0,0.0278654,"ate the probability of two candidate translations that are designed to reflect specific linguistic properties. Sennrich generated such pairs programmatically by applying simple heuristics, such as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples. This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017). Finally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018; Rudinger et al., 2018; Zhao et al., 2018a). Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are. 4.6 contrastive pairs evaluation of Sennrich (2017). Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale. However, they may miss certain aspects. Thus a few studies report human"
Q19-1004,N18-1179,0,0.0203177,"n, semi-automatic methods are used to compile an initial list of examples that is manually verified by annotators. The specific method also affects the kind of language use and how natural or artificial/synthetic the examples are. We describe here some trends in dataset construction methods in the hope that they may be useful for researchers contemplating new datasets. 11 Their dataset does not seem to be available yet, but more details are promised to appear in a future publication. 55 Several datasets were constructed by modifying or extracting examples from existing datasets. For instance, Sanchez et al. (2018) and Glockner et al. (2018) extracted examples from SNLI (Bowman et al., 2015) and replaced specific words such as hypernyms, synonyms, and antonyms, followed by manual verification. Linzen et al. (2016), on the other hand, extracted examples of subject–verb agreement from raw texts using heuristics, resulting in a large-scale dataset. Gulordava et al. (2018) extended this to other agreement phenomena, but they relied on syntactic information available in treebanks, resulting in a smaller dataset. Several challenge sets utilize existing test suites, either as a direct source of examples (Burch"
Q19-1004,N18-2002,0,0.0284845,"Missing"
Q19-1004,E17-2060,0,0.130236,"number agreement in language modeling in English, Russian, Hebrew, and Italian. Clearly, there is room for more challenge sets in nonEnglish languages. However, perhaps more pressing is the need for large-scale non-English datasets (besides MT) to develop neural models for popular NLP tasks. Linguistic Phenomena One of the primary goals of challenge sets is to evaluate models on their ability to handle specific linguistic phenomena. While earlier studies emphasized exhaustivity (Cooper et al., 1996; Lehmann et al., 1996), recent ones tend to focus on a few properties of interest. For example, Sennrich (2017) introduced a challenge set for MT evaluation focusing on five properties: subject–verb agreement, noun phrase agreement, verb–particle constructions, polarity, and transliteration. Slightly more elaborated is an MT challenge set for morphology, including 14 morphological properties (Burlot and Yvon, 2017). See Table SM2 for references to datasets targeting other phenomena. Other challenge sets cover a more diverse range of linguistic properties, in the spirit of some of the earlier work. For instance, extending the categories in Cooper et al. (1996), the GLUE analysis set for NLI covers more"
Q19-1004,D15-1044,0,0.0518396,"ne et al., 2015; Wang et al., 2017b). Figure 1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron. The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rockt¨aschel et al., 2016; Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018). Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al. Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows). As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be Figure 2"
Q19-1004,C18-1315,0,0.0214353,"Missing"
Q19-1004,D16-1248,0,0.21726,", part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references.5 It is referred to by various names, including ‘‘auxiliary prediction tasks’’ (Adi et al., 2017b), ‘‘diagnostic classifiers’’ (Veldhoen et al., 2016), and ‘‘probing tasks’’ (Conneau et al., 2018). As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data—English→ French and English→German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn 2.2 Linguistic Phenomena Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic inf"
Q19-1004,D18-1503,0,0.0603768,"Missing"
Q19-1004,P18-1117,0,0.0280104,"d sentence level. They also compared representations at different encoding layers and found that ‘‘local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.’’ These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components. Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared wh"
Q19-1004,W19-0128,1,0.858898,"Missing"
Q19-1004,W18-5446,0,0.471801,"orks for speech recognition and different speaker embeddings. Some analysis has also been devoted to joint language–vision or audio–vision models, or to similarities between word embeddings and con volutional image representations. Table SM1 provides detailed references. 2.4 Limitations The classification approach may find that a certain amount of linguistic information is captured in the neural network. However, this does not necessarily mean that the information is used by the network. For example, Vanmassenhove et al. (2017) 6 Others found that even simple binary trees may work well in MT (Wang et al., 2018b) and sentence classification (Chen et al., 2015). 52 Figure 1: A heatmap visualizing neuron activations. In this case, the activations capture position in the sentence. (e.g., tree depth, coordination inversion) gain the most from using a deeper classifier. However, the approach is usually taken for granted; given its prevalence, it appears that better theoretical or empirical foundations are in place. 3 Visualization Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial"
Q19-1004,W18-6304,0,0.0457514,"Missing"
Q19-1004,N07-1033,0,0.0192489,"g paraphrases with desired syntactic structures (Iyyer et al., 2018). In image captioning, Chen et al. (2018a) modified pixels in the input image to generate targeted attacks on the caption text. 5.4 6 Explaining specific predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007; Zhang et al., 2016),15 but this approach requires manual annotations of explanations, which may be hard to collect. An alternative approach is to use parts of the input as explanations. For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis. Alvarez-Melis and Jaakkola (2017) discovered input–output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations. Gupta and Sch¨utze (2018) inspected how infor"
Q19-1004,D18-1509,0,0.326737,"orks for speech recognition and different speaker embeddings. Some analysis has also been devoted to joint language–vision or audio–vision models, or to similarities between word embeddings and con volutional image representations. Table SM1 provides detailed references. 2.4 Limitations The classification approach may find that a certain amount of linguistic information is captured in the neural network. However, this does not necessarily mean that the information is used by the network. For example, Vanmassenhove et al. (2017) 6 Others found that even simple binary trees may work well in MT (Wang et al., 2018b) and sentence classification (Chen et al., 2015). 52 Figure 1: A heatmap visualizing neuron activations. In this case, the activations capture position in the sentence. (e.g., tree depth, coordination inversion) gain the most from using a deeper classifier. However, the approach is usually taken for granted; given its prevalence, it appears that better theoretical or empirical foundations are in place. 3 Visualization Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial"
Q19-1004,P18-2117,0,0.0195342,"dding model may be deemed more interpretable if humans are better able to identify the intruding words. Since the evaluation is costly for high-dimensional representations, alternative automatic metrics were considered (Park et al., 2017; Senel et al., 2018). A long tradition in work on neural networks is to evaluate and analyze their ability to learn different formal languages (Das et al., 1992; Casey, 1996; Gers and Schmidhuber, 2001; Bod´en and Wiles, 2002; Chalup and Blair, 2003). This trend continues today, with research into modern architectures and what formal languages they can learn (Weiss et al., 2018; Bernardy, 2018; Suzgun et al., 2019), or the formal properties they possess (Chen et al., 2018b). 8 visualization via saliency measures or evaluation by adversarial examples. But even those sometimes require non-trivial adaptations to work with text input. Some methods are more specific to the field, but may prove useful in other domains. Challenge sets or test suites are such a case. Throughout this survey, we have identified several limitations or gaps in current analysis work: • The use of auxiliary classification tasks for identifying which linguistic properties neural networks capture h"
Q19-1004,D16-1076,0,0.0229308,"sired syntactic structures (Iyyer et al., 2018). In image captioning, Chen et al. (2018a) modified pixels in the input image to generate targeted attacks on the caption text. 5.4 6 Explaining specific predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007; Zhang et al., 2016),15 but this approach requires manual annotations of explanations, which may be hard to collect. An alternative approach is to use parts of the input as explanations. For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis. Alvarez-Melis and Jaakkola (2017) discovered input–output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations. Gupta and Sch¨utze (2018) inspected how information is accumulated"
Q19-1004,Q18-1019,0,0.0483906,"Missing"
Q19-1004,N18-2003,0,0.360403,"ch as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples. This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017). Finally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018; Rudinger et al., 2018; Zhao et al., 2018a). Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are. 4.6 contrastive pairs evaluation of Sennrich (2017). Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale. However, they may miss certain aspects. Thus a few studies report human evaluation on their challenge sets, such as in MT (Isabelle et al., 2017; Burchardt et al., 2017). We note here also that judging the quality of a model by its performance on a challenge"
Q19-1004,Q16-1019,0,0.0684951,"Missing"
Q19-1004,D18-1316,0,0.25584,"ch as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples. This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017). Finally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018; Rudinger et al., 2018; Zhao et al., 2018a). Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are. 4.6 contrastive pairs evaluation of Sennrich (2017). Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale. However, they may miss certain aspects. Thus a few studies report human evaluation on their challenge sets, such as in MT (Isabelle et al., 2017; Burchardt et al., 2017). We note here also that judging the quality of a model by its performance on a challenge"
Q19-1004,D15-1075,0,\N,Missing
Q19-1004,D15-1002,0,\N,Missing
Q19-1004,J17-4003,0,\N,Missing
Q19-1004,P17-1080,1,\N,Missing
Q19-1004,P17-1106,0,\N,Missing
Q19-1004,W17-4705,0,\N,Missing
Q19-1004,P18-2103,0,\N,Missing
Q19-1004,W18-5418,0,\N,Missing
Q19-1004,W18-5402,0,\N,Missing
Q19-1004,D18-1007,0,\N,Missing
Q19-1004,D18-1277,0,\N,Missing
Q19-1004,D18-1313,0,\N,Missing
Q19-1004,P17-1183,0,\N,Missing
S15-2048,W14-5201,0,0.0238006,"Missing"
S15-2048,D14-1070,0,0.00899199,"Missing"
S15-2048,P14-2050,0,0.00385405,"refers to all the staff in your company ... you are all considered employees of your company A2: your qid should specify what is the actual profession you have. I think for me, your chances to have a drivers license is low. A3: his asking if he can obtain. means he have the driver license. Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers among the many responses can Answer selectio"
S15-2048,P14-5010,0,0.00175448,"ector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed in both languages. As shown in Table 1, we compute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used for a linear SVM classifier for answer selection and YES/NO answer inference tasks. YES/NO answer inference is only performed on good YES/NO question answers, using the YES/NO majority class, and unsure otherwise. SVM parameters are set by grid-search and cross-validation. Text-based features These features are"
S15-2048,S15-2047,0,0.0869744,"Missing"
S15-2048,N13-1090,0,0.17161,"n your company ... you are all considered employees of your company A2: your qid should specify what is the actual profession you have. I think for me, your chances to have a drivers license is low. A3: his asking if he can obtain. means he have the driver license. Introduction Continuous word and phrase vectors, in which similar words and phrases are associated with similar vectors, have been useful in many NLP tasks (AlRfou et al., 2013; Bansal et al., 2014; Bowman et al., 2014; Boyd-Graber et al., 2012; Chen and Rudnicky, 2014; Guo et al., 2014; Iyyer et al., 2014; Levy and Goldberg, 2014; Mikolov et al., 2013c). To evaluate the effectiveness of continuous vector representations for Community question answering (CQA), we focused on using simple features derived from vector similarity as input to a multi-class linear SVM classifier. Our approach is language independent and was evaluated on both English and Arabic. Most of the vectors we use are domain-independent. CQA services provide forums for users to ask or answer questions on any topic, resulting in high variance answer quality (M`arquez et al., 2015). Searching for good answers among the many responses can Answer selection aims to automaticall"
S15-2048,W09-1501,0,0.0202031,"king scores Table 1: The different types of features. 2 Method Continuous vector representations, described by Sch¨utze (Sch¨utze, 1992a; Sch¨utze, 1992b), associate similar vectors with similar words and phrases. Most approaches to computing vector representations use the observation that similar words appear in similar contexts (Firth, 1957). The theses of Sahlgren (Sahlgren, 2006), Mikolov (Mikolov, 2012), and Socher (Socher, 2014) provide extensive information on vector representations. Our system analyzes questions and answers with a DkPro (Eckart de Castilho and Gurevych, 2014) uimaFIT (Ogren and Bethard, 2009) pipeline. The DkPro OpenNLP (Apache Software Foundation, 2014) segmenter and chunker tokenize and find sentences and phrases in the English questions and answers, followed by lemmatization with the Stanford lemmatizer (Manning et al., 2014). In Arabic, we only apply lemmatization, with no chunking, using MADAMIRA (Pasha et al., 2014). Stop words are removed in both languages. As shown in Table 1, we compute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used for a linear SVM classifier for answer selection and YES/NO answer infer"
S15-2048,pasha-etal-2014-madamira,0,0.0212372,"Missing"
S15-2048,W13-3520,0,\N,Missing
S15-2048,D12-1118,0,\N,Missing
S15-2048,P14-2131,0,\N,Missing
S19-2182,D18-1389,1,0.904156,"Missing"
S19-2182,N19-1216,1,0.832379,"Missing"
S19-2182,N18-2004,1,0.896708,"Missing"
S19-2182,C18-1158,0,0.0258872,"al election (Brill, 2001; Finberg et al., 2002; Castillo et al., 2011; Baly et al., 2018a; Kulkarni et al., 2018; Mihaylov et al., 2018; Baly et al., 2019). Most approaches have focused on predicting credibility, bias or stance. Stance detection was considered as an intermediate step for detecting fake claims, where the veracity of a claim is checked by aggregating the stances of the retrieved relevant articles (Baly et al., 2018b; Nakov et al., 2019). Several stance detection models have been proposed including deep convolutional neural networks (Baird et al., 2017), multi-layer perceptrons (Hanselowski et al., 2018), and end-to-end memory networks (Mohtarami et al., 2018). 1041 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1041–1046 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from"
S19-2182,S19-2145,0,0.0557502,"distant supervision. Additional experiments showed that significant performance gains can be achieved with better feature pre-processing.1 1 Introduction The rise of social media has enabled people to easily share information with a large audience without regulations or quality control. This has allowed malicious users to spread disinformation and misinformation (a.k.a. “fake news”) at an unprecedented rate. Fake news is typically characterized as being hyperpartisan (one-sided), emotional and riddled with lies (Potthast et al., 2018). The SemEval-2019 Task 4 on Hyperpartisan News Detection (Kiesel et al., 2019) focused on the challenge of automatically identifying whether a text is hyperpartisan or not. While hyperpartisanship is defined as “exhibiting one or more of blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person”, we model this task as a binary document classification problem. Scholars have argued that all biased messages can be considered propagandistic, regardless of whether the bias was intentional or not (Ellul, 1965, p. XV). 1 Our system is available at https://github.com/ AbdulSaleh/QCRI-MIT-SemEval2019-Task4 Thus, we approached the task departing from an"
S19-2182,D18-1388,0,0.165557,"Missing"
S19-2182,N18-1070,1,0.913935,"Missing"
S19-2182,H05-1044,0,0.0505862,"1042 X biasi (Dj ) = count(cue, Dj ) cue∈BLi X wk ∈Dj (3) count(wk , Dj ) Lexicon-based Features. Rashkin et al. (2017) studied the occurrence of specific types of words in different kinds of articles, and showed that words from certain lexicons (e.g., negation and swear words) appear more frequently in propaganda, satire, and hoax articles than in trustworthy articles. We capture this by extracting features that reflect the frequency of words from particular lexicons. We use 18 lexicons from Wiktionary, Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), Wilson’s subjectives (Wilson et al., 2005), Hyland’s hedges (Hyland, 2015), and Hooper’s assertives (Hooper, 1975). For each lexicon, we count the total number of words in the article that appear in the lexicon. This resulted in 18 features, one for each lexicon. Vocabulary Richness Potthast et al. (2018) showed that hyperpartisan outlets tend to use a writing style that is different from mainstream outlets. Different topic-independent features have been proposed to characterize the vocabulary richness, style and complexity of a text. For this task, we used the following vocabulary richness features: (i) type–token ratio (TTR), or the"
S19-2182,P18-1022,0,0.211987,"d by aggregating the stances of the retrieved relevant articles (Baly et al., 2018b; Nakov et al., 2019). Several stance detection models have been proposed including deep convolutional neural networks (Baird et al., 2017), multi-layer perceptrons (Hanselowski et al., 2018), and end-to-end memory networks (Mohtarami et al., 2018). 1041 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1041–1046 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from fake news and satire. They used features such as the number of occurrences of different part-of-speech tags, swearing and slang words, stop words, punctuation, and negation as stylistic markers. They also used a number of readability measures. Rashkin et al. (2017) focused on a multi-class setting (real news vs. satire vs. hoax vs. propaganda) a"
S19-2182,D17-1317,0,0.206809,"l Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from fake news and satire. They used features such as the number of occurrences of different part-of-speech tags, swearing and slang words, stop words, punctuation, and negation as stylistic markers. They also used a number of readability measures. Rashkin et al. (2017) focused on a multi-class setting (real news vs. satire vs. hoax vs. propaganda) and relied on word n-grams. Similarly to Potthast et al. (2018), we believe that there is an inherent style in propaganda, regardless of the source publishing it. Many stylistic features were proposed for authorship identification, i.e., the task of predicting whether a piece of text has been written by a particular author. One of the most successful representations for such a task are character-level n-grams (Stamatatos, 2009), and they turn out to represent some of our most important stylistic features. More det"
S19-2182,C18-1283,0,0.0315537,"e believe that there is an inherent style in propaganda, regardless of the source publishing it. Many stylistic features were proposed for authorship identification, i.e., the task of predicting whether a piece of text has been written by a particular author. One of the most successful representations for such a task are character-level n-grams (Stamatatos, 2009), and they turn out to represent some of our most important stylistic features. More details about research on fact-checking and the spread of fake news online can be found in recent surveys (Lazer et al., 2018; Vosoughi et al., 2018; Thorne and Vlachos, 2018). 3 System Description We developed our system for detecting hyperpartisanship in news articles by training a logistic regression classifier using features such as character and word n-grams, lexicon-based indicators, and readability and vocabulary richness measures. Below, we describe these features in detail. Character 3-grams. Stamatatos (2009) argued that, for tasks where the topic is irrelevant, character-level representations are more sensitive than token-level ones. We hypothesize that this applies to hyperpartisan news detection, since articles on both sides of the political spectrum m"
S19-2182,P02-1053,0,0.0234741,"= α + i:yi =0 xi , and we set the smoothing parameter α to 1. Finally, we calculate the vector:  r = log p/ k p k q/ k q k  (1) which is used to scale the TF.IDF features to create the NB-TF.IDF features as follows: x0i = r ◦ xi , ∀i (2) Bias Analysis We analyze the bias in the language used in the documents by (i) creating bias lexicons that contain left and right bias cues, and (ii) using these lexicons to compute two scores for each document, indicating the intensity of bias towards each ideology. To generate the list of cues that signal biased language, we use Semantic Orientation (SO) (Turney, 2002) to identify the words that are strongly associated with each of the left and right documents in the training dataset. Those SO values can be either positive or negative, indicating association with right or left biases, respectively. Then, we select words whose absolute SO value is ≥ 0.4 to create two bias lexicons: BLlef t and BLright . Finally, we use these lexicons to compute two bias scores per document according to Equation (3), where for each document Dj , the frequency of cues in the lexicon BLi that are present in Dj is normalized by the total number of words in Dj : 1042 X biasi (Dj"
S19-2182,P12-2018,0,0.0995675,"Missing"
W03-0707,N03-1005,1,0.564415,"come a reality, a number of specific technology goals must be met. First and foremost, it is essential to develop tools that will enable rapid configuration of dialogue systems in new domains of knowledge, guided mainly from domain-dependent information sources. Our efforts in generic dialogue development represent a strong initiative toward that goal (Polifroni and Chung, 2002). Secondly, we need to be able to support incremental update of vocabularies and language models for speech recognition and understanding, in essentially instantaneous time (Schalkwyk et al., 2003; Seneff et al., 1998; Chung et al., 2003). This would allow great flexibility within a single dialogue where the user might ask about a named entity that is not yet known to the system. Third, while we can make use of a large lexical resource for pronunciation modeling, we must have available as well a high-performance letter-to-sound capability, integrating multiple knowledge sources such as a Web page, a spoken name, a spoken spelling of the name, and/or a key-padded name (Chung and Seneff, 2002). Fourth, we need to have intelligent knowledge acquisition systems, capable of populating a database from Web sources, and extracting and"
W03-0707,W00-0303,1,0.845498,"t some of these, with pointers to the literature for an in-depth description. SpeechBuilder: Over the past few years, we have been developing a set of utilities that would enable research results to be migrated directly into application development (Glass and Weinstein, 2001). Our goal is to enable natural, mixed-initiative interfaces similar to those now created manually by a relatively small group of expert developers. We make no distinction between the technology components of SpeechBuilder and those of our most sophisticated dialogue systems, such as the Mercury flight reservation domain (Seneff and Polifroni, 2000). SpeechBuilder employs a Web-based interface where developers type in the specifics of their domain, guided by forms and pull-down menus. Components such as recognition vocabulary, parse rules, and semantic mappings are created automatically from example sentences entered by the developer. In several recent short courses, naive developers have been able to implement a new domain and converse with it on the telephone in a matter of hours. Language Modelling: Patchwork Grammars A serious limitation in today’s technology to immediate deployment of a new system is the chicken-and-egg problem of t"
W04-2902,H01-1064,0,0.106635,"rease in the availability of on-line academic lecture material. These educational resources can potentially change the way people learn — students with disabilities can enhance their educational experience, professionals can keep up with recent advancements in their field, and people of all ages can satisfy their thirst for knowledge. In contrast to many other communicative activities however, lecture processing has until recently enjoyed little benefit from the development of human language technology. Although there has been significant research directed toward audio indexing and retrieval (Bacchiani et al., 2001; Foote, 1999; Jourlin et al., 2000; Makhoul et al., 2000; Franz et al., 2003; Renals et al., 2000), lecture transcription and analysis is a relatively unexplored area in speech and natural language research. The most substantial research on lectures has been performed as part of the Spontaneous Speech Project in Japan (Furui, 2003), where researchers are processing a variety of Japanese monologues such as academic and simulated presentations, news commentaries, etc. There has also been some work reported on German lectures (Hurst et al., 2002). One of the reasons for the minimal research in t"
W04-2902,N03-5003,0,0.0819319,"rast to many other communicative activities however, lecture processing has until recently enjoyed little benefit from the development of human language technology. Although there has been significant research directed toward audio indexing and retrieval (Bacchiani et al., 2001; Foote, 1999; Jourlin et al., 2000; Makhoul et al., 2000; Franz et al., 2003; Renals et al., 2000), lecture transcription and analysis is a relatively unexplored area in speech and natural language research. The most substantial research on lectures has been performed as part of the Spontaneous Speech Project in Japan (Furui, 2003), where researchers are processing a variety of Japanese monologues such as academic and simulated presentations, news commentaries, etc. There has also been some work reported on German lectures (Hurst et al., 2002). One of the reasons for the minimal research in this area is due to the limited availability of relevant data. The only publicly available corpus of academic presentations in English is TED, which includes 48 hours of audio recordings of 188 presentations given at Eurospeech ’93 (Lamel et al., 1994). Only 6 of the presenters were native English speakers however, and only 39 of the"
W06-1644,P96-1041,0,0.0132213,"Missing"
W06-1644,W04-2902,1,0.915187,"th in style and topic, becomes essential for efficient access and management of this information. As a prime example, successful language modeling for academic lectures not only enables the initial transcription via automatic speech recognition, but also assists educators and students in the creation and navigation of these materials through annotation, retrieval, summarization, and even translation of the embedded content. Compared with other types of audio content, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with specific terminology (Furui, 2003; Glass et al, 2004). Unfortunately, training corpora available for language modeling rarely match the target lecture in both style and topic. While transcripts from other lectures better match the style of the target lecture than written text, it is often difficult to find transcripts on the target topic. On the other hand, although topic-specific vocabulary can be gleaned from related text materials, such as the textbook and lecture slides, written language is a poor predictor of how words are actually spoken. Furthermore, given that the precise topic of a target lecture is often unknown a priori and may even s"
W06-1644,N03-5003,0,0.0317219,"Missing"
W08-0801,P07-1048,0,0.0579195,"Missing"
W08-0801,J92-1004,1,0.674037,"Missing"
W08-0801,N07-2039,1,0.923845,"t-free grammar to generate a supplemental corpus of synthetic utterances. The corpus is used to train probabilities for the natural language parsing grammar (described immediately below), which in turn is used to derive a class ngram language model (Seneff et al., 2003). Classes in the language model which correspond to contents of the database are marked as dynamic, and are populated at runtime from the database (Chung et al., 2004; Hetherington, 2005). Database entries are heuristically normalized into spoken forms. Pronunciations not in our 150,000 word lexicon are automatically generated (Seneff, 2007). Parser Grammar The TINA parser uses a probabilistic context-free grammar enhanced with support for wh-movement and grammatical agreement constraints. We have developed a generic syntactic grammar by examining hundreds of thousands of utterances collected from real user interactions with various existing dialogue systems. In addition, we have developed libraries which parse and interpret common semantic classes like dates, times, and numbers. The grammar and semantic libraries provide good coverage for spoken dialogue systems in database-query domains. To build a grammar for a new domain, a d"
W16-1616,D13-1106,0,0.0237027,"er, 1997) has been designed to improve the learning of long-term dependencies. pairs of answers at any distance in the thread, which they combine in a graph-cut and in an Integer Linear Programming (ILP) frameworks. They then proposed a fully connected pairwise CRFs (FCCRF) with global normalization and an Isinglike edge potential. 2.3 Neural Networks Neural based approaches have wide applications including speech recognition (Graves and Jaitly, 2014), language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sutskever et al., 2011), translation (Liu et al., 2014; Sutskever et al., 2014; Auli et al., 2013), and image captioning (Karpathy and Fei-Fei, 2015). In addition, recent work shows the effectiveness of neural models in answer selection (Severyn and Moschitti, 2015b; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. Dos Santos et al. (2015) developed CNN and bag-of-words (BOW) representation models for the question similarity task. Cosine similarity between the representations of the input questions were used to compute the CNN and BOW similarity scores for the question-question pairs. The convolutional representations i"
W16-1616,P15-2113,0,0.0340885,"Missing"
W16-1616,N10-1145,0,0.0249037,"agnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers (Wang et al., 2007); Continuous word and phrase vectors to encode semantic similarity (Belinkov et al., 2015); Tree Edit Distance (TED) to learn tree transformations in pairs (Heilman and Smith, 2010); probabilistic model to learn tree-edit operations on dependency parse trees (Wang and Manning, 2010); and linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers (Yao et al., 2013). In addition to the usual local features that only look at the question-answer pair, automatic answer selection algorithms can rely on global threadlevel features, such as the position of the answer in the thread (Hou et al., 2015), or the context of an answer in a thread (Nicosia et al., 2015), or dependencies between thread answers using struct"
W16-1616,S15-2035,0,0.0208655,"semantic similarity (Belinkov et al., 2015); Tree Edit Distance (TED) to learn tree transformations in pairs (Heilman and Smith, 2010); probabilistic model to learn tree-edit operations on dependency parse trees (Wang and Manning, 2010); and linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers (Yao et al., 2013). In addition to the usual local features that only look at the question-answer pair, automatic answer selection algorithms can rely on global threadlevel features, such as the position of the answer in the thread (Hou et al., 2015), or the context of an answer in a thread (Nicosia et al., 2015), or dependencies between thread answers using structured prediction models (Barr´on-Cedeno et al., 2015). Joty et al. (2015) modeled the relations between 138 tion because it allows information to persist across states. The output of each loop is utilized as input to the following loop through hidden states that capture information about the preceding sequence. RNNs are trained using backpropagation through time (BPTT) where the gradient at each output depends on the current and previous time steps. The BPTT approach is not effec"
W16-1616,J93-2003,0,0.0756283,"platform. In addition, with many users contributing to a single question, it has become harder to identify 1 2 https://www.quora.com/ http://stackexchange.com/ 137 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 137–147, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics ural language processing (Berger et al., 2000). Jeon et al. (2005) presented question retrieval methods that are based on using the similarity between answers in the archive to estimate probabilities for a translation-based retrieval model. They run the IBM model 1 (Brown et al., 1993) to learn word translation probabilities on a collection of question pairs. Given a new question, a translation based information retrieval model exploits the word relationships to retrieve similar questions from Q&A archives. They show that with this model it is possible to find semantically similar questions with relatively little word overlap. larity scores. These scores are subsequently employed to rank the list of existing questions and answers with respect to the given question. We evaluate our model on a public benchmark cQA data (Nakov et al., 2016), and show that the results of our mo"
W16-1616,P15-2114,0,0.0256252,"airwise CRFs (FCCRF) with global normalization and an Isinglike edge potential. 2.3 Neural Networks Neural based approaches have wide applications including speech recognition (Graves and Jaitly, 2014), language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sutskever et al., 2011), translation (Liu et al., 2014; Sutskever et al., 2014; Auli et al., 2013), and image captioning (Karpathy and Fei-Fei, 2015). In addition, recent work shows the effectiveness of neural models in answer selection (Severyn and Moschitti, 2015b; Tan et al., 2015; Feng et al., 2015) and question similarity (dos Santos et al., 2015) in community question answering. Dos Santos et al. (2015) developed CNN and bag-of-words (BOW) representation models for the question similarity task. Cosine similarity between the representations of the input questions were used to compute the CNN and BOW similarity scores for the question-question pairs. The convolutional representations in conjunction with other vectors are then passed to a MLP to compute the similarity score of the question pair. Furthermore, recent research has shown the effectiveness of CNNs for answer ranking of short textual contents (Severyn and Moschitti, 2015b). In"
W16-1616,D15-1068,0,0.0284876,"Missing"
W16-1616,W02-1904,0,0.020153,"ding into automatic FAQ answering, where the matching of a question to FAQs is based on keyword comparison enhanced by limited language processing techniques. However, the quality and structure of current knowledge databases are, based on the results of previous experiments, not good enough for reliable performance. The second type of approach employed manual rules or templates. These methods are expensive and hard to scale for large size collections. Sneiders (2002) proposed template based FAQ retrieval systems, while Kim and Seo (2006) proposed using user click logs to find similar queries. Lai et al. (2002) proposed an approach to automatically mine FAQs from the web; However, they did not study the use of these FAQs after they were collected. Berger et al. (2000) proposed a statistical lexicon correlation method. These previous approaches were tested with relatively small sized collections and are hard to scale because they are based on specific knowledge databases or handcrafted rules. The third type of approach uses statistical techniques developed in information retrieval and nat2.2 Answer Selection Passage reordering or reranking has always been an essential step of automatic answer selecti"
W16-1616,S16-1083,0,0.0412192,"Missing"
W16-1616,P14-1140,0,0.047461,"Missing"
W16-1616,P02-1054,0,0.0136656,"o scale because they are based on specific knowledge databases or handcrafted rules. The third type of approach uses statistical techniques developed in information retrieval and nat2.2 Answer Selection Passage reordering or reranking has always been an essential step of automatic answer selection (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015a; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Surdeanu et al., 2008). Many methods have been proposed, such as exploring web redundancy information for answer validation (Magnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers (Wang et al., 2007); Continuous word and phrase vectors to encode semantic similarity (Belinkov et al., 2015); Tree Edit Distance (TED) to learn tree transformations in pairs (Heilman and Smith, 2"
W16-1616,S15-2036,0,0.0488194,"Missing"
W16-1616,D13-1044,0,0.0129321,"ring or reranking has always been an essential step of automatic answer selection (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015a; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Surdeanu et al., 2008). Many methods have been proposed, such as exploring web redundancy information for answer validation (Magnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers (Wang et al., 2007); Continuous word and phrase vectors to encode semantic similarity (Belinkov et al., 2015); Tree Edit Distance (TED) to learn tree transformations in pairs (Heilman and Smith, 2010); probabilistic model to learn tree-edit operations on dependency parse trees (Wang and Manning, 2010); and linear chain CRFs with features derived from TED to automatically learn associations between questions and cand"
W16-1616,S16-1128,1,0.789722,"Results on (a) development and (b) test data for answer selection task in cQA. ble 4b indicate that while the MAPs of the “Double BLSTM” and BOV baseline are comparable, the “Double BLSTM” achieves the highest performance over the other metrics, especially F1. Performance for Answer selection: The results of the answer selection task on development and test data are respectively shown in Tables 4a and 4b. In the tables, the first four rows show the baseline results, and the following rows show the neural models results. The “Single LSTM - Faug ” row shows the results of the model presented by Mohtarami et al. (2016) when only one LSTM is used instead of two bidirectional LSTMs, and no augmented features Faug are used. The “Single BLSTM - Faug ” row indicates the results when one bidirectional LSTM is used in our model, and no augmented features Faug are used. Using a BLSTM improves the performance compared to the single LSTM, as can be seen in Tables 4a and 4b. The “Single BLSTM” row shows the results for one bidirectional LSTM using Faug . Faug is a 10-length binary vector that encodes the order of the answers in their threads corresponding to their time of posting. Faug helps improve the performance, a"
W16-1616,P07-1098,0,0.0104708,"did not study the use of these FAQs after they were collected. Berger et al. (2000) proposed a statistical lexicon correlation method. These previous approaches were tested with relatively small sized collections and are hard to scale because they are based on specific knowledge databases or handcrafted rules. The third type of approach uses statistical techniques developed in information retrieval and nat2.2 Answer Selection Passage reordering or reranking has always been an essential step of automatic answer selection (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015a; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Surdeanu et al., 2008). Many methods have been proposed, such as exploring web redundancy information for answer validation (Magnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic transformations from the que"
W16-1616,D07-1002,0,0.0527343,"the web; However, they did not study the use of these FAQs after they were collected. Berger et al. (2000) proposed a statistical lexicon correlation method. These previous approaches were tested with relatively small sized collections and are hard to scale because they are based on specific knowledge databases or handcrafted rules. The third type of approach uses statistical techniques developed in information retrieval and nat2.2 Answer Selection Passage reordering or reranking has always been an essential step of automatic answer selection (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015a; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Surdeanu et al., 2008). Many methods have been proposed, such as exploring web redundancy information for answer validation (Magnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic tran"
W16-1616,P08-1082,0,0.0157257,"ical lexicon correlation method. These previous approaches were tested with relatively small sized collections and are hard to scale because they are based on specific knowledge databases or handcrafted rules. The third type of approach uses statistical techniques developed in information retrieval and nat2.2 Answer Selection Passage reordering or reranking has always been an essential step of automatic answer selection (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015a; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Surdeanu et al., 2008). Many methods have been proposed, such as exploring web redundancy information for answer validation (Magnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers (Wang et al., 2007); Continuous word and phrase vectors to encode semant"
W16-1616,C10-1131,0,0.0667452,"Missing"
W16-1616,D07-1003,0,0.0322667,"Moschitti, 2008; Tymoshenko and Moschitti, 2015; Surdeanu et al., 2008). Many methods have been proposed, such as exploring web redundancy information for answer validation (Magnini et al., 2002) and using non-textual features (Jeon et al., 2006). Recently, many advanced models have been developed for automating answer selection based on syntactic structures (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Grundstr¨om and Nugues, 2014) and textual entailment. These models include quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers (Wang et al., 2007); Continuous word and phrase vectors to encode semantic similarity (Belinkov et al., 2015); Tree Edit Distance (TED) to learn tree transformations in pairs (Heilman and Smith, 2010); probabilistic model to learn tree-edit operations on dependency parse trees (Wang and Manning, 2010); and linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers (Yao et al., 2013). In addition to the usual local features that only look at the question-answer pair, automatic answer selection algorithms can rely on global threadlevel features, suc"
W16-1616,N13-1106,0,0.0287535,"Missing"
W16-1616,C18-1250,0,0.0688579,"Missing"
W16-4819,W09-0807,0,0.276691,"Missing"
W16-4819,W15-5403,0,0.682166,"Missing"
W16-4819,L16-1284,0,0.176545,"Missing"
W16-4819,D14-1181,0,0.0100157,"ength that we set empirically. Longer texts are truncated and shorter ones are padded with a special PAD symbol. Each 146 character c in the alphabet is represented as a real-valued vector xc ∈ Rdemb . This character embedding is learned during training. Our neural network has the following structure: • Input layer: mapping the character sequence c to a vector sequence x. The embedding layer is followed by dropout. • Convolutional layers: multiple parallel convolutional layers, mapping the vector sequence x to a hidden sequence h. We use filters that slide over character vectors, similarly to Kim (2014)’s CNN over words. A single filter k ∈ Rwdemb of width w creates a new feature fi ∈ R by: fi = k · xi:i+w−1 + b, where xi:i+w−1 is a concatenation of xi , ...xi+w−1 and b ∈ R is a bias term. Each convolution is followed by a Rectified Linear Unit (ReLU) non-linearity (Glorot et al., 2011). The outputs of all the convolutional layers are concatenated. • Pooling layer: a max-over-time pooling layer, mapping the vector sequence h to a single hidden vector h representing the sequence. The size of h is Σj nj wj , where there are nj filters of width wj . • Fully-connected layer: one hidden layer wit"
W16-4819,P16-1100,0,0.0703305,"Missing"
W16-4819,W16-4801,0,0.0863856,"Missing"
W16-4819,W14-5314,0,0.173149,"Missing"
W16-4819,W14-5307,0,0.143673,"Missing"
W16-4819,W15-5401,0,0.135765,"Missing"
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
