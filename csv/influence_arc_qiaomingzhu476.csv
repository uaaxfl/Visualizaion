2020.ccl-1.18,D15-1045,0,0.0574416,"Missing"
2020.ccl-1.18,C18-1046,0,0.0573909,"Missing"
2020.ccl-1.18,P14-1002,0,0.0543118,"Missing"
2020.ccl-1.18,C18-1296,1,0.884711,"Missing"
2020.ccl-1.18,P13-1048,0,0.0298062,"Missing"
2020.ccl-1.18,D13-1070,0,0.0328532,"Missing"
2020.ccl-1.18,P19-1410,0,0.0494024,"Missing"
2020.ccl-1.18,D17-1136,0,0.0246647,"Missing"
2020.ccl-1.18,C04-1007,0,0.137754,"Missing"
2020.ccl-1.59,W19-4322,0,0.0598215,"Missing"
2020.ccl-1.59,D18-1254,0,0.0504864,"Missing"
2020.ccl-1.59,N10-1021,0,0.136845,"Missing"
2020.ccl-1.59,D15-1310,0,0.030584,"Missing"
2020.coling-main.506,D15-1263,0,0.0196959,"nce the differentiation of coherence between adjacent discourse units. The experimental results on Chinese MCDTB demonstrate that our model outperforms all strong baselines. 1 Introduction Discourse parsing aims to study the internal structure of texts and understand the semantic relationship between various kinds of text units, such as clauses, sentences, sentence groups, paragraphs, and chapters. Since it is fundamental to understanding the overall text semantics, discourse parsing has been widely applied to various Natural Language Processing (NLP) applications, such as sentiment analysis (Bhatia et al., 2015), question answering (Sadek and Meziane, 2016), and summarization (Cohan and Goharian, 2018; Xu et al., 2020). As one of the most influential theories in discourse parsing, Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) represents text as a hierarchical structure known as a discourse tree. In the literature, various RST-style corpora have been built, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Macro Chinese Discourse Treebank (MCDTB) (Jiang et al., 2018b). In RST-style discourse parsing, the parser first identifies whether there is a rhetorical relationship"
2020.coling-main.506,C16-1179,0,0.0202472,"7 doc_len=4 doc_len=5 0.15 doc_len=6 doc_len=7 0.13 doc_len=8 0.11 doc_len=9 doc_len=10 0.09 1 2 3 4 5 6 7 8 9 10 11 12 #paragraph doc_len=11 doc_len=12 (a) Chinese MCDTB doc_len=2 doc_len=3 0.17 doc_len=4 0.15 doc_len=5 doc_len=6 0.13 doc_len=7 0.11 doc_len=8 0.09 doc_len=9 doc_len=10 0.07 1 2 3 4 5 6 7 8 9 10 11 12 #paragraph doc_len=11 doc_len=12 (b) English RST-DT Figure 2: Distribution of average information entropy in documents of different lengths in MCDTB and RST-DT at the paragraph-level. semantic features from the first and last EDU of a large discourse unit. Neural network methods (Braud et al., 2016; Li et al., 2016) tended to choose all EDUs of the larger discourse unit as its semantic representation. Other works (Sagae, 2009; Braud et al., 2017) selected only one EDU (e.g. the first one) as the representation according to nuclearity. In Chinese, a few discourse corpora (Li et al., 2014b; Zhou and Xue, 2015) were annotated at the clause-level. To the best of our knowledge, the MCDTB (Jiang et al., 2018b) is the only available paragraph-level Chinese corpus. The bottom-up algorithm (Chu et al., 2018; Jiang et al., 2018a) was the earliest applied to construct paragraph-level discourse str"
2020.coling-main.506,E17-1028,0,0.0800638,"12 (a) Chinese MCDTB doc_len=2 doc_len=3 0.17 doc_len=4 0.15 doc_len=5 doc_len=6 0.13 doc_len=7 0.11 doc_len=8 0.09 doc_len=9 doc_len=10 0.07 1 2 3 4 5 6 7 8 9 10 11 12 #paragraph doc_len=11 doc_len=12 (b) English RST-DT Figure 2: Distribution of average information entropy in documents of different lengths in MCDTB and RST-DT at the paragraph-level. semantic features from the first and last EDU of a large discourse unit. Neural network methods (Braud et al., 2016; Li et al., 2016) tended to choose all EDUs of the larger discourse unit as its semantic representation. Other works (Sagae, 2009; Braud et al., 2017) selected only one EDU (e.g. the first one) as the representation according to nuclearity. In Chinese, a few discourse corpora (Li et al., 2014b; Zhou and Xue, 2015) were annotated at the clause-level. To the best of our knowledge, the MCDTB (Jiang et al., 2018b) is the only available paragraph-level Chinese corpus. The bottom-up algorithm (Chu et al., 2018; Jiang et al., 2018a) was the earliest applied to construct paragraph-level discourse structure trees in MCDTB. Recently, Zhou et al. (2019) built discourse structure trees by the shift-reduce algorithm in MCDTB. All of them used all PDUs f"
2020.coling-main.506,C18-1045,1,0.84213,"features from the first and last EDU of a large discourse unit. Neural network methods (Braud et al., 2016; Li et al., 2016) tended to choose all EDUs of the larger discourse unit as its semantic representation. Other works (Sagae, 2009; Braud et al., 2017) selected only one EDU (e.g. the first one) as the representation according to nuclearity. In Chinese, a few discourse corpora (Li et al., 2014b; Zhou and Xue, 2015) were annotated at the clause-level. To the best of our knowledge, the MCDTB (Jiang et al., 2018b) is the only available paragraph-level Chinese corpus. The bottom-up algorithm (Chu et al., 2018; Jiang et al., 2018a) was the earliest applied to construct paragraph-level discourse structure trees in MCDTB. Recently, Zhou et al. (2019) built discourse structure trees by the shift-reduce algorithm in MCDTB. All of them used all PDUs for the semantic representation of a larger discourse unit. 3 Motivation and Methods In this section, we propose a global backward reading mode to construct a paragraph-level discourse tree by the shift-reduce algorithm to utilize the bias of Chinese discourse structure. In particular, we propose a triple semantic matching model based on BERT as a local mode"
2020.coling-main.506,P14-1048,0,0.362502,"hown in Figure 1. According to the granularity of the leaf nodes, the discourse tree is divided into three levels: clause level, sentence level and paragraph level (Kobayashi et al., 2020). This paper focuses on constructing paragraph-level Chinese discourse trees where the leaf node is a paragraph. We call a leaf node that contains only one paragraph as PDU (Paragraph-level Discourse Unit) to distinguish the elementary discourse unit (EDU) at the clause level. It is more important for downstream tasks as the upper part of a complete discourse tree. In English, there is a series of successes (Feng and Hirst, 2014; Ji and Eisenstein, 2014; Wang et al., 2017) in RST-DT (Carlson et al., 2003), especially Lin et al. (2019) and Liu et al. (2019) got excellent performance in clause-level discourse parsing. However, owing to linguistic and cultural differences, discourse structure is different between Chinese and English: English is linear while Chinese is spiral (Kaplan, 1966). Therefore, the state-of-the-art model in English cannot be directly transformed into Chinese well. Besides, fewer studies (Sporleder and Lascarides, 2004; Zhou et al., 2019) on paragraphlevel discourse parsing may suffer from data sp"
2020.coling-main.506,P14-1002,0,0.422593,"rding to the granularity of the leaf nodes, the discourse tree is divided into three levels: clause level, sentence level and paragraph level (Kobayashi et al., 2020). This paper focuses on constructing paragraph-level Chinese discourse trees where the leaf node is a paragraph. We call a leaf node that contains only one paragraph as PDU (Paragraph-level Discourse Unit) to distinguish the elementary discourse unit (EDU) at the clause level. It is more important for downstream tasks as the upper part of a complete discourse tree. In English, there is a series of successes (Feng and Hirst, 2014; Ji and Eisenstein, 2014; Wang et al., 2017) in RST-DT (Carlson et al., 2003), especially Lin et al. (2019) and Liu et al. (2019) got excellent performance in clause-level discourse parsing. However, owing to linguistic and cultural differences, discourse structure is different between Chinese and English: English is linear while Chinese is spiral (Kaplan, 1966). Therefore, the state-of-the-art model in English cannot be directly transformed into Chinese well. Besides, fewer studies (Sporleder and Lascarides, 2004; Zhou et al., 2019) on paragraphlevel discourse parsing may suffer from data sparsity and its performanc"
2020.coling-main.506,P18-2070,0,0.0200716,"experimental results on Chinese MCDTB demonstrate that our proposed model with global backward and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are th"
2020.coling-main.506,P13-1048,0,0.0271934,"ur proposed model with global backward and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse"
2020.coling-main.506,J15-3002,0,0.0200893,"lt et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most traditional machine learning methods (Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017) extracted DU1-6 (Behavior-Purpose) DU1-5 (Elaboration) DU1-3 (Background) DU1-2 (Supplement) P1 P2 DU4-5 (Joint) P3 P4 P5 P6 Figure 1: The paragraph-level discourse tree of chtb 0022 in MCDTB. P1-P6 refer to six PDUs and directed edges indicate nucleus discourse units. The relation (e.g., Elaboration and Joint) of two related discourse units is shown in the bracket. 5750 average information entropy average information entropy 0.19 doc_len=2 0.19 doc_len=3 0.17 doc_len=4 doc_len=5 0.15 doc_len=6 doc_len=7 0.13 doc_len=8 0.11 doc_len=9 doc_len=10 0.09 1 2 3 4 5 6 7 8 9 10 11"
2020.coling-main.506,D19-1587,0,0.0150909,"-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most traditional machine learning methods (Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017) extracted DU1-6 (Behavi"
2020.coling-main.506,W17-3002,0,0.0192155,"TM-BERT) that views three adjacent discourse units as a triangle to capture the relationship across discourse units better. To deal with longer paragraph-level discourse units, we used only 1-2 PDUs to represent a discourse unit and proposed a local reverse reading mode to reverse the order of PDUs. The experimental results on Chinese MCDTB demonstrate that our proposed model with global backward and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu"
2020.coling-main.506,D14-1220,0,0.283694,"ith global backward and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most trad"
2020.coling-main.506,D14-1224,1,0.866626,"ith global backward and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most trad"
2020.coling-main.506,D16-1035,0,0.263032,"d and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most traditional machine lea"
2020.coling-main.506,P19-1410,0,0.0989865,"ls: clause level, sentence level and paragraph level (Kobayashi et al., 2020). This paper focuses on constructing paragraph-level Chinese discourse trees where the leaf node is a paragraph. We call a leaf node that contains only one paragraph as PDU (Paragraph-level Discourse Unit) to distinguish the elementary discourse unit (EDU) at the clause level. It is more important for downstream tasks as the upper part of a complete discourse tree. In English, there is a series of successes (Feng and Hirst, 2014; Ji and Eisenstein, 2014; Wang et al., 2017) in RST-DT (Carlson et al., 2003), especially Lin et al. (2019) and Liu et al. (2019) got excellent performance in clause-level discourse parsing. However, owing to linguistic and cultural differences, discourse structure is different between Chinese and English: English is linear while Chinese is spiral (Kaplan, 1966). Therefore, the state-of-the-art model in English cannot be directly transformed into Chinese well. Besides, fewer studies (Sporleder and Lascarides, 2004; Zhou et al., 2019) on paragraphlevel discourse parsing may suffer from data sparsity and its performance is much lower than that of ∗ Corresponding author This work is licensed under a C"
2020.coling-main.506,D19-1093,0,0.0648973,"ence level and paragraph level (Kobayashi et al., 2020). This paper focuses on constructing paragraph-level Chinese discourse trees where the leaf node is a paragraph. We call a leaf node that contains only one paragraph as PDU (Paragraph-level Discourse Unit) to distinguish the elementary discourse unit (EDU) at the clause level. It is more important for downstream tasks as the upper part of a complete discourse tree. In English, there is a series of successes (Feng and Hirst, 2014; Ji and Eisenstein, 2014; Wang et al., 2017) in RST-DT (Carlson et al., 2003), especially Lin et al. (2019) and Liu et al. (2019) got excellent performance in clause-level discourse parsing. However, owing to linguistic and cultural differences, discourse structure is different between Chinese and English: English is linear while Chinese is spiral (Kaplan, 1966). Therefore, the state-of-the-art model in English cannot be directly transformed into Chinese well. Besides, fewer studies (Sporleder and Lascarides, 2004; Zhou et al., 2019) on paragraphlevel discourse parsing may suffer from data sparsity and its performance is much lower than that of ∗ Corresponding author This work is licensed under a Creative Commons Attrib"
2020.coling-main.506,D19-1233,0,0.0113076,"nio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most traditional machine learning methods (Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017) extracted DU1-6 (Behavior-Purpose) DU1-5 (Elaboration) DU1-3 (Background) DU1-2 (Supplement) P1 P2 DU4-5 ("
2020.coling-main.506,D17-1136,0,0.0343619,"Missing"
2020.coling-main.506,N06-2033,0,0.0197803,"orward reading (b) The two stages of the backward reading Figure 3: The two stages of the shift-reduce algorithm in two global reading modes. Existing methods on RST-DT often process discourse units from the beginning to the end of a document to build discourse trees, such as the shift-reduce algorithm (Wang et al., 2017). However, these successful methods in English may not be transformed into Chinese well because of the bias of discourse structure mentioned above. Therefore, inspired by Bi-LSTM (Hochreiter and Schmidhuber, 1997) in processing text streams and bi-direction in syntax parsing (Sagae and Lavie, 2006), we propose a global backward reading mode that processes discourse units from the end to the beginning by the shift-reduce algorithm to utilize this branching bias of Chinese better. In a typical shift-reduce algorithm, the parsing process is modeled as a sequence of shift and reduce actions on a stack and a queue; the stack is initially empty and the queue contains all PDUs in a document. At each step, the parser performs either shift or reduce: while shift pushes the first PDU in the queue to the top of the stack, reduce pops and merges the top 2 elements in the stack to yield a new sub-tr"
2020.coling-main.506,W09-3813,0,0.0488278,"n=11 doc_len=12 (a) Chinese MCDTB doc_len=2 doc_len=3 0.17 doc_len=4 0.15 doc_len=5 doc_len=6 0.13 doc_len=7 0.11 doc_len=8 0.09 doc_len=9 doc_len=10 0.07 1 2 3 4 5 6 7 8 9 10 11 12 #paragraph doc_len=11 doc_len=12 (b) English RST-DT Figure 2: Distribution of average information entropy in documents of different lengths in MCDTB and RST-DT at the paragraph-level. semantic features from the first and last EDU of a large discourse unit. Neural network methods (Braud et al., 2016; Li et al., 2016) tended to choose all EDUs of the larger discourse unit as its semantic representation. Other works (Sagae, 2009; Braud et al., 2017) selected only one EDU (e.g. the first one) as the representation according to nuclearity. In Chinese, a few discourse corpora (Li et al., 2014b; Zhou and Xue, 2015) were annotated at the clause-level. To the best of our knowledge, the MCDTB (Jiang et al., 2018b) is the only available paragraph-level Chinese corpus. The bottom-up algorithm (Chu et al., 2018; Jiang et al., 2018a) was the earliest applied to construct paragraph-level discourse structure trees in MCDTB. Recently, Zhou et al. (2019) built discourse structure trees by the shift-reduce algorithm in MCDTB. All of"
2020.coling-main.506,C04-1007,0,0.0414634,"us studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the literature, there are three kinds of strategies for representing larger discourse units. Most traditional machine learning methods (Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017) extracted DU1-6 (Behavior-Purpose) DU1-5 (Elaboration) DU1-3 (Background) DU1-2 (Supplement) P1 P2 DU4-5 (Joint) P3 P4 P5 P6 Figure 1: The paragraph-level discourse tree of chtb 0022 in MCDTB. P1-P6 refer to six PDUs and directed edges indicate nucleus discourse units"
2020.coling-main.506,N09-1064,0,0.0970484,"Missing"
2020.coling-main.506,P17-2029,0,0.264267,"of the leaf nodes, the discourse tree is divided into three levels: clause level, sentence level and paragraph level (Kobayashi et al., 2020). This paper focuses on constructing paragraph-level Chinese discourse trees where the leaf node is a paragraph. We call a leaf node that contains only one paragraph as PDU (Paragraph-level Discourse Unit) to distinguish the elementary discourse unit (EDU) at the clause level. It is more important for downstream tasks as the upper part of a complete discourse tree. In English, there is a series of successes (Feng and Hirst, 2014; Ji and Eisenstein, 2014; Wang et al., 2017) in RST-DT (Carlson et al., 2003), especially Lin et al. (2019) and Liu et al. (2019) got excellent performance in clause-level discourse parsing. However, owing to linguistic and cultural differences, discourse structure is different between Chinese and English: English is linear while Chinese is spiral (Kaplan, 1966). Therefore, the state-of-the-art model in English cannot be directly transformed into Chinese well. Besides, fewer studies (Sporleder and Lascarides, 2004; Zhou et al., 2019) on paragraphlevel discourse parsing may suffer from data sparsity and its performance is much lower than"
2020.coling-main.506,2020.acl-main.451,0,0.0394828,"demonstrate that our model outperforms all strong baselines. 1 Introduction Discourse parsing aims to study the internal structure of texts and understand the semantic relationship between various kinds of text units, such as clauses, sentences, sentence groups, paragraphs, and chapters. Since it is fundamental to understanding the overall text semantics, discourse parsing has been widely applied to various Natural Language Processing (NLP) applications, such as sentiment analysis (Bhatia et al., 2015), question answering (Sadek and Meziane, 2016), and summarization (Cohan and Goharian, 2018; Xu et al., 2020). As one of the most influential theories in discourse parsing, Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) represents text as a hierarchical structure known as a discourse tree. In the literature, various RST-style corpora have been built, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Macro Chinese Discourse Treebank (MCDTB) (Jiang et al., 2018b). In RST-style discourse parsing, the parser first identifies whether there is a rhetorical relationship between discourse units to construct a naked tree and then recognizes the nuclearity and relation labels for"
2020.coling-main.506,C18-1047,0,0.0112937,"rder of PDUs. The experimental results on Chinese MCDTB demonstrate that our proposed model with global backward and local reverse reading outperforms all strong baselines. 2 Related Work In English, RST-DT (Carlson et al., 2003) is one of the popular discourse corpora (Subba and Di Eugenio, 2009; Zeldes, 2017; Kolhatkar and Taboada, 2017), which annotates the discourse structure, nuclearity, and relationship of a document. Most previous studies have focused on complete discourse parsing and can be mainly categorized into the shift-reduce algorithm (Ji and Eisenstein, 2014; Wang et al., 2017; Yu et al., 2018; Jia et al., 2018), the probabilistic CKY-like algorithm (Joty et al., 2013; Li et al., 2014a; Li et al., 2016), and the bottom-up algorithm (Hernault et al., 2010; Feng and Hirst, 2014; Kobayashi et al., 2019; Kobayashi et al., 2020). Recently, the generative algorithm (Mabona et al., 2019) and the top-down algorithm (Liu et al., 2019; Lin et al., 2019) tried out discourse parsing. At the high level (paragraph-level), Sporleder and Lascarides (2004) built paragraph-level discourse trees by bottom-up algorithm after pruning and revising the original discourse trees in the RST-DT. In the liter"
2020.emnlp-main.291,P17-1067,0,0.0252471,"multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video seg"
2020.emnlp-main.291,P19-2045,0,0.0493231,"Missing"
2020.emnlp-main.291,D19-1566,0,0.0224143,"motion detection rely on special knowledge of emotion, such as context information (Li et al., 2015), cross-domain transferring (Yu et al., 2018) and external resource (Ying et al., 2019). In fact, when there is no special knowledge (Kim et al., 2018), it can be normally handled by multi-label text classiﬁcation approaches. In the multi-modal community, related studies normally focus on single-label emotion task and the studies for multi-label emotion task are much less and limited to be transformed to multiple binary classiﬁcation (Zadeh et al., 2018b; Wang et al., 2019; Akhtar et al., 2019; Chauhan et al., 2019). In the following, we give an overview of multi-label emotion/text classiﬁcation and multi-modal emotion detection. Multi-label Emotion/Text Classiﬁcation. Recent studies normally cast multi-label emotion detection task as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for t"
2020.emnlp-main.291,S18-1019,0,0.0211402,"g, we give an overview of multi-label emotion/text classiﬁcation and multi-modal emotion detection. Multi-label Emotion/Text Classiﬁcation. Recent studies normally cast multi-label emotion detection task as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for that. Kant et al. (2018) leverage the pre-trained BERT to perform multi-label emotion task and Kim et al. (2018) propose an attention-based classiﬁer that predicts multiple emotions of a given sentence. More recently, Yang et al. (2018) propose a sequence generation model and Yang et al. (2019) leverage a reinforced approach to ﬁnd a better sequence than a baseline sequence, but it still relies on the pretrained seq2seq model with a pre-deﬁned order of ground-truth. Different from above studies, we focus on multilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label"
2020.emnlp-main.291,P15-1101,1,0.814811,"Missing"
2020.emnlp-main.291,D14-1162,0,0.0827516,"Missing"
2020.emnlp-main.291,N19-1321,0,0.0449293,"Missing"
2020.emnlp-main.291,P19-1656,0,0.174944,"ultilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multi-label emotion detection in a multi-modal scenario. Multi-modal Emotion Detection. Recent studies on multi-modal emotion detection largely depend on multi-modal fusion framework to perform binary classiﬁcation within each emotion category. Recently, Wang et al. (2019) introduce a recurrent attended variation embedding network for multimodal language analysis with non-verbal shifted word representation. Tsai et al. (2019) employ the Transformer-based architecture to capture the long-range interactions inside and across different modalities. However, they still cast the multi-label emotion detection as multiple binary classiﬁcation problems. Different from above studies, we focus on multimodal emotion detection in a multi-label scenario by considering the label dependence besides the modality dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multimodal emotion detection in a multi-label scenario. 3585 3 Output Probabilities Data Pre-processing Softmax We extract low-level handcrafted"
2020.emnlp-main.291,P18-1208,0,0.0263778,"Missing"
2020.emnlp-main.291,D19-1444,0,0.0275047,"parameters of β1 and β2 , 0.9 and 0.999 respectively. The beam size K is set to be 5 at both training and inference stages. To motivate future research, the code will be released via github 3 . Evaluation Metrics and Signiﬁcance Test. In our study, we employ three evaluation metrics to measure the performances of different approaches to multi-modal multi-label emotion detection, i.e., multi-label Accuracy (Acc), Hamming Loss (HL) and micro F1 measure (F1 ). These metrics have been popularly used in some multi-label classiﬁcation problems (Li et al., 2015; Yang et al., 2019; Aly et al., 2019; Wu et al., 2019). Note that smaller Hamming Loss corresponds to better classiﬁcation quality, while larger Accuracy 2 3 For a thorough comparison, we implement various baseline approaches in three groups: Multi-label Classiﬁcation Approaches. In this group, the baselines use different approaches to deal with the multi-label issue without considering the modality dependence issue. Speciﬁcally, in these approaches, the multi-modal inputs are early fused (simply concatenated) as a new input. (1) BR5 (Shen et al., 2004), which transforms the multi-label task into multiple single-label binary classiﬁcation problem"
2020.emnlp-main.291,D19-1044,0,0.0370485,"ultiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance as shown in Figure 1, both the Sad and Disgust emotions are more likely to exist, rather than the conﬂicting emotions of Sad and Happy. Recent studies, such as (Yang et al., 2019) and (Xiao et al., 2019), have begun to address this challenge. However, almost all existing studies in multilabel emotion detection focus on one modality (e.g., textual modality). Only very recently, the research community has become increasingly aware of the need on multi-modal emotion detection (Zadeh et al., 2018b) due to its wide potential applications, e.g., with the massively growing importance of analyzing conversations in speech (Gu et al., 2019) and video (Majumder et al., 2019). In this study, we aim to tackle multi-modal multi-label emotion detection. Compared with single modality, multimodal multi-label"
2020.emnlp-main.291,P19-1518,0,0.255733,"sifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance as shown in Figure 1, both the Sad and Disgust emotions are more likely to exist, rather than the conﬂicting emotions of Sad and Happy. Recent studies, such as (Yang et al., 2019) and (Xiao et al., 2019), have begun to address this challenge. However, almost all existing studies in multilabel emotion detection focus on one modality (e.g., textual modality). Only very recently, the research community has become increasingly aware of the need on multi-modal emotion detection (Zadeh et al., 2018b) due to its wide potential applications, e.g., with the massively growing importance of analyzing conversations in speech (Gu et al., 2019) and video (Majumder et al., 2019). In this study, we aim to tackle multi-modal multi-label emotion detection. Compared with single modality,"
2020.emnlp-main.291,C18-1330,0,0.116609,"ask as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for that. Kant et al. (2018) leverage the pre-trained BERT to perform multi-label emotion task and Kim et al. (2018) propose an attention-based classiﬁer that predicts multiple emotions of a given sentence. More recently, Yang et al. (2018) propose a sequence generation model and Yang et al. (2019) leverage a reinforced approach to ﬁnd a better sequence than a baseline sequence, but it still relies on the pretrained seq2seq model with a pre-deﬁned order of ground-truth. Different from above studies, we focus on multilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multi-label emotion detection in a multi-modal scenario. Multi-modal Emotion Detection. Recent studies on multi-modal emotion detecti"
2020.emnlp-main.291,D19-1552,1,0.831171,". The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction"
2020.emnlp-main.291,P19-1045,1,0.763654,"and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcul"
2020.emnlp-main.291,D19-5541,0,0.142045,"otion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance"
2020.emnlp-main.291,D18-1137,0,0.0783276,"is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For examp"
2021.acl-short.70,2020.emnlp-main.457,0,0.023036,"ng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhan"
2021.acl-short.70,D18-1529,0,0.0242889,"Missing"
2021.acl-short.70,I13-1181,0,0.0120815,"xperimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies l"
2021.acl-short.70,2020.coling-main.183,0,0.021417,"Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we randomly 2 https://www.xuexi.cn/ Size 250 50.5"
2021.acl-short.70,P14-1028,0,0.0182098,"CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020"
2021.acl-short.70,N19-1276,0,0.01271,"r CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we randomly 2 https://www.xue"
2021.acl-short.70,C04-1081,0,0.235078,"aracters Total Words Total Time(s) Finally, we leverage the CRF to perform sequence labeling on the basis of the above character representation. We evaluate our approach on the newly annotated small-scale dataset with different size of training sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020;"
2021.acl-short.70,2020.findings-emnlp.260,0,0.0593949,"Missing"
2021.acl-short.70,D11-1090,0,0.0337871,"aining sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models,"
2021.acl-short.70,2020.acl-main.735,0,0.048718,"Missing"
2021.acl-short.70,2020.acl-main.734,0,0.0661243,"ard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collect"
2021.acl-short.70,P19-1656,0,0.0280817,"timedependent uni-modal interaction, time-dependent multi-modal interaction and CRF labeling. Figure 2 shows the overall architecture of our TMIN. 4.1 Time-dependent Uni-modal Interaction To better capture the temporal correspondences between different modalities (Zhang et al., 2019; Ju et al., 2020), we ﬁrst align two modalities by extracting the exact time stamp of each phoneme and character using Montreal Forced Aligner (McAuliffe et al., 2017). For machines to understand human utterance, they must be ﬁrst able to understand the intramodal dynamics (Zadeh et al., 2018; Wang et al., 2019b; Tsai et al., 2019) in each modality, such as the word order and grammar in text, breathe and 551 Multi-attention Gating B h I  αK ĊĊ CRF Layer  α2 CRF Labeling  α1 ^ha O ĊĊ Multi-modal Hybrid Representation h1 ^hx LSTHMG Gating LSTM h1a h2 Gating LSTM h1 x ĊĊ Gating LSTM LSTM LSTM hn ĊĊ Time-depedent Multi-modal Interaction LSTM ĊĊ Transformerbased Unit Transformerbased Unit Transformerbased Unit Transformerbased Unit ᗵ 享 i=1 Transformerbased Unit ĊĊ ĊĊ Transformerbased Unit Time-dependent Uni-modal Text Interaction ભ ĊĊ i=2 Audio i=n ĊĊ Figure 2: The overview of our proposed TMIN. tone in audio. Textual Mod"
2021.acl-short.70,I17-1017,0,0.0125628,"ture engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we"
2021.acl-short.70,O03-4002,0,0.261908,"Sentences Avg. Length (Character) Avg. Length (Word) Avg. Length (Time)(s) Max Length (Character) Max Length (Word) Max Length (Time)(s) Total Characters Total Words Total Time(s) Finally, we leverage the CRF to perform sequence labeling on the basis of the above character representation. We evaluate our approach on the newly annotated small-scale dataset with different size of training sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al.,"
2021.acl-short.70,P17-1078,0,0.0125572,"k, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, whi"
2021.acl-short.70,2020.emnlp-main.291,1,0.714341,"be represented as: X = (x1 , x2 , · · · , xn ) ∈ Rn×d1 . Acoustic Modality. We use a famous audio processing tool, i.e., OpenSMILE (Eyben et al., 2010), to extract the MFCC, LP-coefﬁcients, pure FFT spectrum, etc. from dual channels (Jayram et al., 2002; Sakran et al., 2017), and leverage multiple Transformer layers (Vaswani et al., 2017) to perform intra-modal interactions. Then, each character-level audio feature can be represented as: A = (a1 , a2 , · · · , an ) ∈ Rn×d2 . 4.2 Time-dependent Multi-modal Interaction To better capture the cross-modal semantic correspondences (Wu et al., 2020; Zhang et al., 2020), we design a long- and short-term hybrid memory gating (LSTHMG) block, which is a extension of standard LSTM. We ﬁrst obtain the current memory of each character-level representation for both modalities. ˆ x , cx = LSTMx (xi , hx , cx ) h i i i i−1 i−1 a a a ˆ , c = LSTM (ai , ha , ca ) h i i i i−1 i−1 (1) (2) where LSTM denotes the standard LSTM (Graves et al., 2013). After current updating, we employ multiattention to control the different contributions of each hidden state. ˆ i + MA(h ˆ x, h ˆ a) hi = h i i ˆi + = h L  l=0 (softmax( (3) Ql (K l ) √ d )V l ) (4) where MA denotes the multi"
2021.acl-short.70,D13-1031,0,0.0153164,"emonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external info"
2021.acl-short.70,D13-1061,0,0.0394832,"Missing"
2021.acl-short.70,D17-1079,0,0.0120733,"quence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with"
2021.acl-short.70,P19-2029,0,0.0283995,"Missing"
2021.emnlp-main.187,2020.acl-main.480,0,0.0214799,"nit is longer, and the connection between discourse units is more obscure in Chinese, we first conduct the experiments on MCDTB. Then, we conduct the experiments on PDTB, one of the most popular discourse relation corpus in English, to verify the generality of our model. Besides, we also conduct the experiments on another Chinese dataset CDTB and the results are shown in Appendix D.3. MCDTB: following previous work (Jiang et al., 2019; Sun et al., 2020), we use the same dataset division and five-fold cross-validation for the experiments. PDTB: following previous work (Ji and Eisenstein, 2015; Kim et al., 2020), we adopt the mostused dataset splitting PDTB-Ji that takes the sections 2-20 as the training set, 0-1 as the development set, and 21-22 for testing. We use Pytorch and Huggingface (Wolf et al., 2020)5 as a deep learning framework, and the key parameter settings of our model are described in Appendix C. Since there is no official Chinese T5 model, we use the parameter weights provided by the third party6 . It is a T5 (base) model with 12layer encoders and 12-layer decoders trained by automatic summarization task on about a 30G corpus. In English, we use the official parameter weight7 of T5 (b"
2021.emnlp-main.187,D17-1134,0,0.023764,"a the shallow convolutional neural network (Zhang et al., 2015), recursive neural network (Ji and Eisenstein, 2015), collaborative gated neural network (Qin et al., 2016), or attention mechanism (Liu and Li, 2016). To enhance the interaction between DUs, Guo et al. (2018) and Ruan et al. (2020) proposed various interactive attention mechanisms for IDRR. Guo et al. (2020) proposed a knowledge-enhanced attention neural network to introduce external knowledge to enhance the interaction. Besides, a few studies combined IDRR with other tasks for joint learning, e.g., explicit relation recognition (Lan et al., 2017), connective prediction (Bai and Zhao, 2018; Shi and Demberg, 2019), and label embedding learning (Nguyen et al., 2019; He et al., 2020). In Chinese, Zhou et al. (2019) used the macrostructural features and macro-semantic representations to enhance DU representation. Sun et al. (2020) established a large heterogeneous discourse graph on the entire corpus and used the GCN-based model to enhance the interaction between DUs. Xu et al. (2019) and Jiang et al. (2019) jointly learned the relation recognition with the topic modeling and the nuclearity recognition, respectively. 2.3 Formalizing Fine-g"
2021.emnlp-main.187,2020.acl-main.519,0,0.0402967,"tic representations to enhance DU representation. Sun et al. (2020) established a large heterogeneous discourse graph on the entire corpus and used the GCN-based model to enhance the interaction between DUs. Xu et al. (2019) and Jiang et al. (2019) jointly learned the relation recognition with the topic modeling and the nuclearity recognition, respectively. 2.3 Formalizing Fine-grained Tasks as QA A few fine-grained tasks can be formalized as QA and have achieved success due to introducing prior knowledge to their tasks, such as relation extraction (Li et al., 2019), named entity recognition (Li et al., 2020), and co-reference resolution (Wu et al., 2020). It is worth noting that the above studies all take questions as the input and extract the answers from the context as the output. 3 IDRR as Text Generation mainly describe our solution for the first challenge of obtaining the target sentence: its different forms and its corresponding construction method. DU1 DU2 DU1 DU2 Classification Model Relation Type Generation Model Target Sentence map Relation Type Figure 1: The classification task (upper) and the generation task (lower) for the IDRR. 3.1 Forms of Target Sentence Unlike other fine-grained"
2021.emnlp-main.187,2020.acl-main.451,0,0.0252494,"bes the logical connection between two discourse units (e.g., clauses, sentences, or paragraphs). As an essential discourse analysis task, discourse relation recognition is to recover what rhetorical relation exists between discourse units (DUs). Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) is still a challenging task and research hotspot. Moreover, IDRR is beneficial to many downstream natural language processing (NLP) applications, such as machine translation (Webber et al., 2017), text generation (Bosselut et al., 2018), and text summarization (Xu et al., 2020). With the success of representation learning in discourse analysis, most existing methods of IDRR ∗ Corresponding author Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2418–2431 c November 7–11, 2021. 2021 Association for Computational Linguistics achieved success in a few fine-grained tasks, such as named entity recognition (Li et al., 2019) and coreference resolution (Wu et al., 2020). However, it is a challenge to directly apply the traditional Question-Answering method to IDRR due to the following two issues. First, unlike the above finegrain"
2021.emnlp-main.187,P19-1058,1,0.83725,"xternal knowledge to enhance the interaction. Besides, a few studies combined IDRR with other tasks for joint learning, e.g., explicit relation recognition (Lan et al., 2017), connective prediction (Bai and Zhao, 2018; Shi and Demberg, 2019), and label embedding learning (Nguyen et al., 2019; He et al., 2020). In Chinese, Zhou et al. (2019) used the macrostructural features and macro-semantic representations to enhance DU representation. Sun et al. (2020) established a large heterogeneous discourse graph on the entire corpus and used the GCN-based model to enhance the interaction between DUs. Xu et al. (2019) and Jiang et al. (2019) jointly learned the relation recognition with the topic modeling and the nuclearity recognition, respectively. 2.3 Formalizing Fine-grained Tasks as QA A few fine-grained tasks can be formalized as QA and have achieved success due to introducing prior knowledge to their tasks, such as relation extraction (Li et al., 2019), named entity recognition (Li et al., 2020), and co-reference resolution (Wu et al., 2020). It is worth noting that the above studies all take questions as the input and extract the answers from the context as the output. 3 IDRR as Text Generation mai"
2021.emnlp-main.187,D15-1266,0,0.027702,"paragraphs. Since there are few classification task and then use the T5 decoder for connectives between paragraphs, it annotated all the generation task. Finally, we combine these two discourse relations as implicit relations in MCDTB. 2419 2.2 Implicit Discourse Relation Recognition Most previous studies on IDRR can be divided into the following three categories: enhancing DU representation, enhancing the interaction between DUs, and joint learning of IDRR and other tasks. In English, early work explored the methods of enhancing DU representation via the shallow convolutional neural network (Zhang et al., 2015), recursive neural network (Ji and Eisenstein, 2015), collaborative gated neural network (Qin et al., 2016), or attention mechanism (Liu and Li, 2016). To enhance the interaction between DUs, Guo et al. (2018) and Ruan et al. (2020) proposed various interactive attention mechanisms for IDRR. Guo et al. (2020) proposed a knowledge-enhanced attention neural network to introduce external knowledge to enhance the interaction. Besides, a few studies combined IDRR with other tasks for joint learning, e.g., explicit relation recognition (Lan et al., 2017), connective prediction (Bai and Zhao, 2018; S"
2021.findings-emnlp.100,N19-1423,0,0.00874502,"e, regardless of whether the knowledge is relevant or less-relevant. We implement the interceptor and soft filter by self-attention network and attention pooling layer, which are collectively referred to as “Headhunter”. We couple a certain pre-trained model with Headhunter for encoding, and deploy them along with ElasticSearch in the commonly-used two-stage MQA architecture. We experiment over the CommonsenseQA dataset (Talmor et al., 2019). Experimental results show that Headhunter yields significant performance gains all along when coupled with different pre-trained models, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and Albert (Lan et al., 2020). Besides, the case of combining Albert and Headhunter achieves better performance than most state-of-the-art models, and it is ranked second on the CommonsenseQA leaderboard for track 1. The developed results show that 2.2 Headhunter’s Soft Filter the performance advantages can be safely attributed to the constraint on the use of less-relevant knowl- Attention pooling layer is used as the filter. It only edge in Headhunter. To some extent, it successfully comes into play when positioned behind the selfavoids the severe performance degr"
2021.findings-emnlp.100,D18-1220,0,0.0611339,"Missing"
2021.findings-emnlp.100,D19-1282,0,0.025204,"Missing"
2021.findings-emnlp.100,2021.ccl-1.108,0,0.0588968,"Missing"
2021.findings-emnlp.100,P19-1487,0,0.0177882,"sefully retrieving a larger number of knowledge items actually results in performance degradation. Therefore, actively acquiring the closely related As illustrated in Figure 1, the performance curve commonsense knowledge from external sources is of the retrieval-based MQA model (green curve) ∗ Corresponding author 1 shows a trend of fluctuating downward when the https://github.com/unlimitedaki/ HeadHunter number of the adopted highly-ranked search results 1157 Multi-choice question answering (MQA for short) is required to select an answer from a set of candidate options when given a question (Rajani et al., 2019). The task is slightly different from multichoice reading comprehension which provides the passage containing background knowledge for reasoning (Richardson et al., 2013). Frankly, due to the lack of commonsense knowledge, MQA is more challenging. For example, it appears to be difficult for MQA to determine the true answer in the following case, where the commonsense knowledge regarding “island country” deterministically contributes to reasoning, though such knowledge is not offered in any form by default: Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1157–1165 N"
2021.findings-emnlp.100,D13-1020,0,0.0131165,"Figure 1, the performance curve commonsense knowledge from external sources is of the retrieval-based MQA model (green curve) ∗ Corresponding author 1 shows a trend of fluctuating downward when the https://github.com/unlimitedaki/ HeadHunter number of the adopted highly-ranked search results 1157 Multi-choice question answering (MQA for short) is required to select an answer from a set of candidate options when given a question (Rajani et al., 2019). The task is slightly different from multichoice reading comprehension which provides the passage containing background knowledge for reasoning (Richardson et al., 2013). Frankly, due to the lack of commonsense knowledge, MQA is more challenging. For example, it appears to be difficult for MQA to determine the true answer in the following case, where the commonsense knowledge regarding “island country” deterministically contributes to reasoning, though such knowledge is not offered in any form by default: Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1157–1165 November 7–11, 2021. ©2021 Association for Computational Linguistics is increased. It is most likely caused by the noises that sneak into the list of retrieval knowledge i"
2021.findings-emnlp.100,N19-1421,0,0.0236314,"e Section 2.3). Thus, by the attention modeling mentioned above, each hidden ˇ i in H ˇ intercepts and absorbs the relevant state h information from other commonsense knowledge, regardless of whether the knowledge is relevant or less-relevant. We implement the interceptor and soft filter by self-attention network and attention pooling layer, which are collectively referred to as “Headhunter”. We couple a certain pre-trained model with Headhunter for encoding, and deploy them along with ElasticSearch in the commonly-used two-stage MQA architecture. We experiment over the CommonsenseQA dataset (Talmor et al., 2019). Experimental results show that Headhunter yields significant performance gains all along when coupled with different pre-trained models, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and Albert (Lan et al., 2020). Besides, the case of combining Albert and Headhunter achieves better performance than most state-of-the-art models, and it is ranked second on the CommonsenseQA leaderboard for track 1. The developed results show that 2.2 Headhunter’s Soft Filter the performance advantages can be safely attributed to the constraint on the use of less-relevant knowl- Attention poo"
2021.findings-emnlp.100,2020.findings-emnlp.369,0,0.0348392,"Missing"
C08-1088,P04-1054,0,0.295638,"rmance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve"
C08-1088,P04-1043,0,0.0215852,"ty nodes, or directly combined with the entity nodes as in Figure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation"
C08-1088,Y07-1043,1,0.375439,"stituents outside the linking path should be removed) and CS-CSPT (Zhou et al., 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. 4 Entity-related Semantic Tree Entity semantic features, such as entity headword, entity type and subtype etc., impose a strong constraint on relation types in terms of relation definition by the ACE RDC task. Experiments by Zhang et al. (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. Qian et al. (2007) further indicates that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb, contribute most while the contribution of other features, such as entity class, headword and GPE role, can be ignored. In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2. In the example sentence “they ’re here”, which is excerpted from the ACE RDC 2004 corpus, there exists a relati"
C08-1088,J03-4003,0,0.042424,"ancestor of the two entities under consideration) as the representation of each relation instance, along the path connecting two entities, the head child of every node is found according to various constituent dependencies. Then the path nodes and their head children are kept while any other nodes are removed from the tree. Eventually we arrive at a tree called Dynamic Syntactic Parse Tree (DSPT), which is dynamically determined by constituent dependencies and only contains necessary information as expected. There exist a considerable number of constituent dependencies in CFG as described by Collins (2003). However, since our task is to extract the relationship between two named entities, our focus is on how to condense Noun-Phrases (NPs) and other useful constituents for relation extraction. Therefore constituent dependencies can be classified according to constituent types of the CFG rules: 699 entities. In the contrast, our DSPT can avoid this problem by keeping the constituent “’s” and the headword “plants”. (2) Modification to NPs: except base-NPs, other modification to NPs can be classified into this type. Usually these NPs are recursive, meaning that they contain another NP as their chil"
C08-1088,P06-1104,1,0.709373,"set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods."
C08-1088,P05-1052,0,0.0469553,"lation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT. In order to fully utilize the advantages of feature-based methods and kernel-based methods, researchers turn to composite kernel methods. Zhao and Grishman (2005) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus. Zhang et al. (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. Zhou et al. (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. It achieves the so far best F-measure of 75.8 on the 7 relation types in the ACE RDC 2004 corpus. In this paper, we"
C08-1088,P05-1053,1,0.901875,"accuracy in state-of-the-art syntactic and semantic parsing, reliably extracting semantic relationships between named entities in natural language documents is still a difficult, unresolved problem. In the literature, feature-based methods have dominated the research in semantic relation extraction. Featured-based methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to repres"
C08-1088,D07-1076,1,0.7458,"d semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods. However, there still"
C08-1088,P01-1017,0,\N,Missing
C08-1088,H05-1091,0,\N,Missing
C10-1068,P09-1074,0,0.0807521,"r D-DSPT-based anaphoricity determination module on coreference resolution by including 605 phoricity determination, with D-DSPT-based anaphoricity determination and. with golden anaphoricity determination. Table 3 shows that: 1) There is a performance gap of 6.4, 6.1 and 7.0 in F1-measure on the NWIRE, NPAPER and BNEWS domain, respectively, between the coreference resolution system with golden anaphoricity determination and the baseline system without anaphoricity determination. This suggests the usefulness of proper anaphoricity determination in coreference resolution. This also agrees with Stoyanov et al. (2009) which measured the impact of golden anaphoricity determination on coreference resolution using only the annotated anaphors in both training and testing. 2) Compared to the baseline system without anaphoricity determination, the D-DSPT-based anaphoricity determination module improves the performance by 4.1(***), 3.9(***) and 5.0(***) to 63.2, 67.4 and 61.6 in F1-measure on the NWIRE, NPAPER and BNEWS domains, respectively, due to a large gain in precision and a much smaller drop in recall. In addition, D-DSPT-based anaphoricity determination can not only much improve the performance of corefer"
C10-1068,P03-1023,1,0.861855,"roduction Coreference resolution aims to identify which noun phrases (NPs, or mentions) refer to the same real-world entity in a text. According to Webber (1979), coreference resolution can be decomposed into two complementary sub-tasks: (1) anaphoricity determination, determining whether a given NP is anaphoric or not; and (2) anaphor resolution, linking together multiple mentions of a given entity in the world. Although machine learning approaches have performed reasonably well in coreference resolution without explicit anaphoricity determination (e.g. Soon et al. 2001; Ng and Cardie 2002b; Yang et al. 2003, 2008; Kong et al. 2009), knowledge of NP anaphoricity is expected to much improve the performance of a coreference resolution system, since a * non-anaphoric NP does not have an antecedent and therefore does not need to be resolved. Recently, anaphoricity determination has been drawing more and more attention. One common approach involves the design of some heuristic rules to identify specific types of non-anaphoric NPs, such as pleonastic it (e.g. Paice and Husk 1987; Lappin and Leass 1994, Kennedy and Boguraev 1996; Denber 1998) and definite descriptions (e.g. Vieira and Poesio 2000). Alte"
C10-1068,P05-1052,0,0.0192837,"anning 2008; Zhou and Kong 2009). As a representative, Zhou and Kong (2009) directly employ a tree kernel-based method to automatically mine the non-anaphoric information embedded in the syntactic parse tree. One main advantage of the kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structured information in various NLP applications like syntactic parsing (Collins and Duffy, 2001; Moschitti, 2004), semantic relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005; Zhou et al. 2007; Qian et al., 2008), semantic role labeling (Moschitti, 2004); coreference resolution (Yang et al., 2006; Zhou et al., 2008). One of the key problems for the kernel-based methods is how to effectively capture the structured information according to the nature of the structured object in the specific task. This paper advances the state-of-the-art performance in anaphoricity determination by efCorresponding author 599 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599–607, Beijing, August 2010 fectively capturing the structur"
C10-1068,D07-1076,1,0.932713,"g 2009). As a representative, Zhou and Kong (2009) directly employ a tree kernel-based method to automatically mine the non-anaphoric information embedded in the syntactic parse tree. One main advantage of the kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structured information in various NLP applications like syntactic parsing (Collins and Duffy, 2001; Moschitti, 2004), semantic relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005; Zhou et al. 2007; Qian et al., 2008), semantic role labeling (Moschitti, 2004); coreference resolution (Yang et al., 2006; Zhou et al., 2008). One of the key problems for the kernel-based methods is how to effectively capture the structured information according to the nature of the structured object in the specific task. This paper advances the state-of-the-art performance in anaphoricity determination by efCorresponding author 599 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599–607, Beijing, August 2010 fectively capturing the structured syntactic infor"
C10-1068,P99-1048,0,0.275587,"ove the performance of a coreference resolution system, since a * non-anaphoric NP does not have an antecedent and therefore does not need to be resolved. Recently, anaphoricity determination has been drawing more and more attention. One common approach involves the design of some heuristic rules to identify specific types of non-anaphoric NPs, such as pleonastic it (e.g. Paice and Husk 1987; Lappin and Leass 1994, Kennedy and Boguraev 1996; Denber 1998) and definite descriptions (e.g. Vieira and Poesio 2000). Alternatively, some studies focus on using statistics to tackle this problem (e.g., Bean and Riloff 1999; Bergsma et al. 2008) and others apply machine learning approaches (e.g. Evans 2001;Ng and Cardie 2002a, 2004,2009; Yang et al. 2005; Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009). As a representative, Zhou and Kong (2009) directly employ a tree kernel-based method to automatically mine the non-anaphoric information embedded in the syntactic parse tree. One main advantage of the kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully app"
C10-1068,W05-0612,0,0.135509,"related tasks. Section 3 presents our dependency-driven scheme to determine the syntactic parse tree structure. Section 4 reports the experimental results. Finally, we conclude our work in Section 5. 2 Related Work This section briefly overviews the related work on both anaphoricity determination and exploring syntactic parse tree structures. 2.1 Anaphoricity Determination Previous work on anaphoricity determination can be broadly divided into three categories: heuristic rule-based (e.g. Paice and Husk 1987;Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Vieira and Poesio 2000; Cherry and Bergsma 2005), statistics-based (e.g. Bean and Riloff 1999; Cherry and Bergsma 2005; Bergsma et al. 2008) and learning-based methods (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al. 2005; Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009). The heuristic rule-based methods focus on designing some heuristic rules to identify specific types of non-anaphoric NPs. Representative work includes: Paice and Husk (1987), Lappin and Leass (1994) and Kennedy and Boguraev (1996). For example, Kennedy and Boguraev (1996) looked for modal adjectives (e.g. “necessary”) or"
C10-1068,N07-1030,0,0.104068,"Missing"
C10-1068,P04-1020,0,0.0697443,"rk in Section 5. 2 Related Work This section briefly overviews the related work on both anaphoricity determination and exploring syntactic parse tree structures. 2.1 Anaphoricity Determination Previous work on anaphoricity determination can be broadly divided into three categories: heuristic rule-based (e.g. Paice and Husk 1987;Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Vieira and Poesio 2000; Cherry and Bergsma 2005), statistics-based (e.g. Bean and Riloff 1999; Cherry and Bergsma 2005; Bergsma et al. 2008) and learning-based methods (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al. 2005; Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009). The heuristic rule-based methods focus on designing some heuristic rules to identify specific types of non-anaphoric NPs. Representative work includes: Paice and Husk (1987), Lappin and Leass (1994) and Kennedy and Boguraev (1996). For example, Kennedy and Boguraev (1996) looked for modal adjectives (e.g. “necessary”) or cognitive verbs (e.g. “It is 600 thought that…” in a set of patterned constructions) in identifying pleonastic it. Among the statistics-based methods, Bean and Riloff"
C10-1068,N07-1010,0,\N,Missing
C10-1068,N09-1065,0,\N,Missing
C10-1068,D09-1102,1,\N,Missing
C10-1068,D09-1103,1,\N,Missing
C10-1068,C02-1139,0,\N,Missing
C10-1068,J08-3002,0,\N,Missing
C10-1068,D09-1133,1,\N,Missing
C10-1068,P06-1006,0,\N,Missing
C10-1068,P02-1060,1,\N,Missing
C10-1068,J94-4002,0,\N,Missing
C10-1068,I05-1063,0,\N,Missing
C10-1068,P08-1002,0,\N,Missing
C10-1068,P02-1014,0,\N,Missing
C10-1068,J01-4004,0,\N,Missing
C10-1068,C08-1088,1,\N,Missing
C10-1068,I08-1004,1,\N,Missing
C10-1068,P06-1104,1,\N,Missing
C10-1076,W05-0630,0,0.037794,"Missing"
C10-1076,W05-0620,0,0.0191159,"Missing"
C10-1076,W08-0606,0,0.299169,"Missing"
C10-1076,I05-2038,0,0.0167196,"Missing"
C10-1076,E99-1043,0,0.0851759,"wordn. That is to say, X’s parent constituent must cross-bracket or include the scope of wordm, …, wordn. <sentence id=""S26.8"">These findings <xcope id=""X26.8.2""><cue type=""speculation"" ref=""X26.8.2"">indicate that</cue> <xcope id=""X26.8.1"">corticosteroid resistance in bronchial asthma <cue type=""negation"" ref=""X26.8.1"">can not</cue> be explained by abnormalities in corticosteroid receptor characteristics</xcope></xcope>.</sentence> Figure 1: An annotated sentence in the BioScope corpus. The Bioscope corpus consists of three subcorpora: the full papers and the abstracts from the GENIA corpus (Collier et al., 1999), and clinical (radiology) reports. Among them, the full papers subcorpus and the abstracts subcorpus come from the same genre, and thus share some common characteristics in statistics, such as the number of words in the negation scope to the right (or left) of the negation signal and the average scope length. In comparison, the clinical reports subcorpus consists of clinical radiology reports with short sentences. For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). 1 Negation Scope Finding via Shallow Semantic Parsing 2 http://code.google.com/p/berkele"
C10-1076,P05-1073,0,0.0576361,"Missing"
C10-1076,P02-1031,0,0.0202022,"Missing"
C10-1076,P02-1053,0,0.00295295,"Missing"
C10-1076,W06-1617,0,0.0115245,"eatures To capture more useful information in the negation signal-scope structures, we also explore various kinds of additional features. Table 2 shows the features in better capturing the details regarding the argument candidate and the negation signal. In particular, we categorize the additional features into three groups according to their relationship with the argument candidate (AC, in short) and the given negation signal (NS, in short). Some features proposed above may not be effective in argument identification. Therefore, we adopt the greedy feature selection algorithm as described in Jiang and Ng (2006) to pick up positive features incrementally according to their contributions on the development data. The algorithm repeatedly selects one feature each time which contributes most, and stops when adding any of the remaining features fails to improve the performance. As far as the negation scope finding task concerned, the whole feature selection process could be done by first running the selection algorithm with the basic features (b1-b4) and then incrementally picking up effective features from (ac1-ac6, AC1-AC2, 4.4 Post-Processing Although a negation signal in the BioScope corpus always has"
C10-1076,W04-3212,0,0.0242302,"finding does not involve semantic label classification and thus could be divided into three consequent phases: argument pruning, argument identification and post-processing. 674 4.2 Argument Pruning Similar to the predicate-argument structures in common shallow semantic parsing, the negation signal-scope structures in negation scope finding can be also classified into several certain types and argument pruning can be done by employing several heuristic rules to filter out constituents, which are most likely non-arguments of a negation signal. Similar to the heuristic algorithm as proposed in Xue and Palmer (2004) for argument pruning in common shallow semantic parsing, the argument pruning algorithm adopted here starts from designating the negation signal as the current node and collects its siblings. It then iteratively moves one level up to the parent of the current node and collects its siblings. The algorithm ends when it reaches the root of the parse tree. To sum up, except the negation signal and its ancestral constituents, any constituent in the parse tree whose parent covers the given negation signal will be collected as argument candidates. Taking the negation signal node “RB7,7” in Figure 2"
C10-1076,J08-2004,0,0.0124589,". (NP<S>VP>RB) nsac2 whether AC and NS are adjacent in position. “yes” or “no”. (no) combined features (NSAC1-NSAC7) b1&b2, b1&b3, b1&nsac1, b3&NS1, b3&NS2, b4&NS1, b4&NS2 Table 2: Additional features and their instantiations for argument identification in negation scope finding, with NP4,5 as the focus constituent (i.e., the argument candidate) and “can not” as the given negation signal, regarding Figure 2. Basic Features Table 1 lists the basic features for argument identification. These features are also widely used in common shallow semantic parsing for both verbal and nominal predicates (Xue, 2008; Li et al., 2009). Feature Remarks b1 Negation: the stem of the negation signal, e.g., not, rather_than. (can_not) b2 Phrase Type: the syntactic category of the argument candidate. (NP) b3 Path: the syntactic path from the argument candidate to the negation signal. (NP<S>VP>RB) b4 Position: the positional relationship of the argument candidate with the negation signal. “left” or “right”. (left) Table 1: Basic features and their instantiations for argument identification in negation scope finding, with NP4,5 as the focus constituent (i.e., the argument candidate) and “can not” as the given neg"
C10-1076,D08-1075,0,0.171304,"his paper is organized as follows. Section 2 reviews related work. Section 3 introduces the Bioscope corpus on which our approach is evaluated. Section 4 describes our parsing approach by formulating negation scope finding as a simplified shallow semantic parsing problem. Section 5 presents the experimental results. Finally, Section 6 concludes the work. 672 2 Related Work While there is a certain amount of literature within the NLP community on negated terms finding (Chapman et al., 2001; Huang and Lowe, 2007; Goldin and Chapman, 2003), there are only a few studies on negation scope finding (Morante et al., 2008; Morante and Daelemans, 2009). Negated terms finding Rule-based methods dominated the initial research on negated terms finding. As a representative, Chapman et al. (2001) developed a simple regular expression-based algorithm to detect negation signals and identify medical terms which fall within the negation scope. They found that their simple regular expression-based algorithm can effectively identify a large portion of the pertinent negative statements from discharge summaries on determining whether a finding or disease is absent. Besides, Huang and Lowe (2007) first proposed some heuristi"
C10-1076,W09-1105,0,0.453568,"as follows. Section 2 reviews related work. Section 3 introduces the Bioscope corpus on which our approach is evaluated. Section 4 describes our parsing approach by formulating negation scope finding as a simplified shallow semantic parsing problem. Section 5 presents the experimental results. Finally, Section 6 concludes the work. 672 2 Related Work While there is a certain amount of literature within the NLP community on negated terms finding (Chapman et al., 2001; Huang and Lowe, 2007; Goldin and Chapman, 2003), there are only a few studies on negation scope finding (Morante et al., 2008; Morante and Daelemans, 2009). Negated terms finding Rule-based methods dominated the initial research on negated terms finding. As a representative, Chapman et al. (2001) developed a simple regular expression-based algorithm to detect negation signals and identify medical terms which fall within the negation scope. They found that their simple regular expression-based algorithm can effectively identify a large portion of the pertinent negative statements from discharge summaries on determining whether a finding or disease is absent. Besides, Huang and Lowe (2007) first proposed some heuristic rules from a parse tree pers"
C10-1076,D09-1145,0,0.0181298,"8) pioneered the research on negation scope finding, largely due to the availability of a large-scale annotated corpus, the Bioscope corpus. They approached the negation scope finding task as a chunking problem which predicts whether a word in the sentence is inside or outside of the negation scope, with proper post-processing to ensure consecutiveness of the negation scope. Morante and Daelemans (2009) further improved the performance by combing several classifiers. Similar to SoN learning, there are some efforts in the NLP community on learning the scope of speculation. As a representative, Özgür and Radev (2009) divided speculation learning into two subtasks: speculation signal finding and speculation scope finding. In particular, they formulated speculation signal finding as a classification problem while employing some heuristic rules from the parse tree perspective on speculation scope finding. 3 For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser 2 (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al., 2005) 3 , which is a bracketed corpus in (almost) PTB style. 10-fold cross-validation on GTB1.0 shows that"
C10-1076,N07-1051,0,0.0744542,"Missing"
C10-1076,D09-1133,1,\N,Missing
C10-2034,P06-1062,0,0.251546,"Missing"
C10-2034,P09-1098,0,0.265271,"Missing"
C10-2034,J03-3002,0,\N,Missing
C10-2034,P09-1075,0,\N,Missing
C12-1100,N06-1046,0,0.467183,"Missing"
C12-1100,W09-2209,0,0.0926762,"10) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and event type determination. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the special issues in Chinese. They also constructed a global errata table to record the inconsistency in the training set and used it to correct the inconsistency in the test set. The other studies focused on automatic expansion of event triggers to improve the recall. Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Ji (2009) first extracts some cross-lingual predicate clusters using bilingual parallel corpora and a cross-lin"
C12-1100,N09-2053,0,0.156331,"10) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and event type determination. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the special issues in Chinese. They also constructed a global errata table to record the inconsistency in the training set and used it to correct the inconsistency in the test set. The other studies focused on automatic expansion of event triggers to improve the recall. Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Ji (2009) first extracts some cross-lingual predicate clusters using bilingual parallel corpora and a cross-lin"
C12-1100,N07-1030,0,0.157608,"del may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments"
C12-1100,D12-1062,0,0.124274,"ure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events mentioned in a given text which proposed a joint inference module that enforced global coherency constraints on the final outputs of the two pairwise classifiers, one between event mentions and time intervals, and one between event mentions themselves. Our joint model is inspired by both Roth and Yih (2004) on joint named entity recognition and relation extraction and Denis and Baldridge (2007) on joint anaphoricity determination and coreference resolution. However, as far as we know, there are no successful models for jointly solving Chinese trigger identific"
C12-1100,P05-1045,0,0.0257747,"ing an event can be recast as identifying a corresponding trigger. Trigger mention: a reference to a trigger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a"
C12-1100,D09-1015,0,0.0174881,"its in F1-measure on the ACE 2005 Chinese corpus, ignoring the post-processing – discourse consistency. 1638 2.2 Joint modeling While a pipeline model may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and M"
C12-1100,P09-2093,0,0.0310181,"gger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and eve"
C12-1100,P11-1113,1,0.924792,"redefined event type, and their participants and attributes. It can be typically divided into four components: trigger identification, event type determination, argument identification and argument role determination. Due to the central role of the contained events in a text, it is critical to mine their semantics in order to understand a text. Unfortunately, event extraction has been proven its performance is still very low. In the literature, most studies focus on English event extraction and have achieved certain success (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Hong et al., 2011; Lu and Roth, 2012; Llorens et al., 2012). However, there are few successful stories regarding Chinese event extraction due to the special characteristics in Chinese trigger identification. Besides unknown triggers1 and word segmentation errors (Li et al., 2012), the low quality of annotated corpora and the high ratio of pseudo trigger mentions to true ones are also blamed for the low performance of Chinese event extraction. To examine the low quality of annotated corpora in Chinese event extraction, we take the ACE (Automatic Content Extraction) 2005 Chinese corpus (with 8 types and 33 subty"
C12-1100,P11-1081,0,0.0964903,"ors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model en"
C12-1100,W09-1704,0,0.0193835,"Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and eve"
C12-1100,P08-1030,0,0.0745871,"Trigger mention: a reference to a trigger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted f"
C12-1100,W09-1419,0,0.0199403,"e class distribution imbalance by discarding harmful or superfluous instances. In the literature, instance filtering has been widely employed in various NLP tasks. As for event extraction, there are also a few relevant studies. Patwardhan and Riloff (2009) first applied a selftrained relevant sentence classifier to identify relevant regions and split all candidate sentences into two sets: relevant and irrelevant sentences. Then, they used a pattern-based classifier to recognize events from those relevant sentences and a SVM-based classifier to recognize events from those irrelevant sentences. Landeghem et al. (2009) provided a negative-instances filter to check whether the length of the sub-sentence spanned by a candidate event does not exceed a certain value. Landeghem et al. (2010) further designed a false-positive filter using specific categories of relations to serve as negative indicators in Bio-NLP. Liao and Grishman (2010) applied a pseudo co-testing algorithm based on various criteria, such as informativeness, representativeness and diversity of the sentence, to filter out those pseudo samples to reduce annotation labour in event corpus annotation. 1639 3 Joint modeling of trigger identification"
C12-1100,W10-1921,0,0.0188036,"event extraction, there are also a few relevant studies. Patwardhan and Riloff (2009) first applied a selftrained relevant sentence classifier to identify relevant regions and split all candidate sentences into two sets: relevant and irrelevant sentences. Then, they used a pattern-based classifier to recognize events from those relevant sentences and a SVM-based classifier to recognize events from those irrelevant sentences. Landeghem et al. (2009) provided a negative-instances filter to check whether the length of the sub-sentence spanned by a candidate event does not exceed a certain value. Landeghem et al. (2010) further designed a false-positive filter using specific categories of relations to serve as negative indicators in Bio-NLP. Liao and Grishman (2010) applied a pseudo co-testing algorithm based on various criteria, such as informativeness, representativeness and diversity of the sentence, to filter out those pseudo samples to reduce annotation labour in event corpus annotation. 1639 3 Joint modeling of trigger identification and event type determination In this section, an ILP (Integer Logic Programming) -based inference framework is proposed to jointly model trigger identification and event t"
C12-1100,P10-1113,1,0.748089,"– discourse consistency. 1638 2.2 Joint modeling While a pipeline model may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event ex"
C12-1100,D12-1092,1,0.881305,"he high ratio of pseudo trigger mentions to true ones, Table 3 shows top 5 imbalanced triggers from the training set of the ACE 2005 Chinese corpus and justifies the difficulty for a classifier to identify a true trigger mention, especially for those of a particular event type, which appears only a few times in the training set. Trigger2 #True trigger mentions #Pseudo trigger mentions 投资 (invest) 1 67 建设 (set up) 1 66 取得 (obtain) 1 52 发 (provide) 1 36 给 (give) 2 64 TABLE 3 – Top 5 triggers with the highest ratios of pseudo trigger mentions to true ones in the ACE 2005 Chinese corpus Recently, Li et al. (2012) justified that trigger identification was most critical for the performance of Chinese event extraction. In this paper, we also focus on trigger identification and its impact on overall Chinese event extraction. In order to address the above-mentioned two critical issues in Chinese event extraction, this paper proposes a joint model of trigger identification and event type determination to improve the performance of trigger identification and overall Chinese event extraction. Besides, several trigger filtering schemas are introduced to filter out those pseudo trigger mentions as many as possi"
C12-1100,P10-1081,0,0.257082,"ype/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and event type determination. Che"
C12-1100,I11-1080,0,0.0568574,"Missing"
C12-1100,P12-1088,0,0.141156,"Missing"
C12-1100,P07-1075,0,0.072223,"nce of an event, so recognizing an event can be recast as identifying a corresponding trigger. Trigger mention: a reference to a trigger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et"
C12-1100,D07-1075,0,0.3712,"Missing"
C12-1100,D09-1016,0,0.643633,"ntify trigger mentions of a predefined event type, and their participants and attributes. It can be typically divided into four components: trigger identification, event type determination, argument identification and argument role determination. Due to the central role of the contained events in a text, it is critical to mine their semantics in order to understand a text. Unfortunately, event extraction has been proven its performance is still very low. In the literature, most studies focus on English event extraction and have achieved certain success (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Hong et al., 2011; Lu and Roth, 2012; Llorens et al., 2012). However, there are few successful stories regarding Chinese event extraction due to the special characteristics in Chinese trigger identification. Besides unknown triggers1 and word segmentation errors (Li et al., 2012), the low quality of annotated corpora and the high ratio of pseudo trigger mentions to true ones are also blamed for the low performance of Chinese event extraction. To examine the low quality of annotated corpora in Chinese event extraction, we take the ACE (Automatic Content Extraction) 2005 Chinese corpus (with 8"
C12-1100,N10-1123,0,0.144878,"joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events men"
C12-1100,W09-1406,0,0.0300702,"tainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the"
C12-1100,D11-1001,0,0.0214976,"nning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events mentioned in a given text which proposed a joint inference module that enforced global coherency constraints on the final outputs of the two"
C12-1100,W04-2401,0,0.163796,"model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events mentioned in a given text which proposed a joint inference module that enforced global coherency constraints on the final outputs of the two pairwise classifiers, one between event mentions and time intervals, and one between event mentions themselves. Our joint model is inspired by both Roth and Yih (2004) on joint named entity recognition and relation extraction and Denis and Baldridge (2007) on joint anaphoricity determination and coreference resolution. However, as far as we know, there are no successful models for jointly solving Chinese trigger identification and event type determination. 2.3 Trigger filtering With the high ratio of pseudo trigger mentions to true ones, it is natural to filter out those unlikely trigger mentions in a preprocessing step. Basically, the general purpose for instance filtering is to reduce the class distribution imbalance by discarding harmful or superfluous i"
C12-1100,W06-0901,0,\N,Missing
C12-1139,P02-1034,0,0.0190685,". That is, different mappings should have different weights to exploit such difference. Thus, Formula (5) can be recast as follows: |Ψ| SimT ( ws , wt ) = α 0 × SimDW ( ws , wt ) + ∑ α i SimiDM ( ws , wt ) i =1 (5’) In this paper, we propose a simple perceptron algorithm to optimize those weights for different mappings using the development lexicon. Generally, the perceptron algorithm is guaranteed to find a hyper-plane that classifies all training points, if the data is separable. Even the data is nonseparable as in most practical cases, the variants of perceptron (Freund and Schapire, 1999; Collins and Duffy, 2002), such as averaged perceptron (AP) or voted perceptron (VP), can generalize well. Input: training examples (si,ti) Output: w Initialize: w= w0 Steps: 1. for i = 1…T 2. for j =1…N 3. Calculate tˆj = arg max t∈GEN ( s ) Φ(s j , t ) × w j 4. 5. 6. 7. 8. If tˆj ! = t j then w = w + Φ( s j , t j ) − Φ( s j , tˆj ) ui , j = w end for end for output w = ∑ ui , j / NT i, j Figure 4 – Perceptron algorithm for weight learning 2283 Fig. 4 shows our averaged perceptron algorithm, where  d, the number of features in a vector, is set to the order of the mapping set plus 1 for the dependency word similarity"
C12-1139,R11-1018,0,0.0313754,"Missing"
C12-1139,W95-0114,0,0.19051,"y between word contexts via an online dictionary. Particularly she analyzed the impact of polysemous words, Chinese tokenization and English morphological information. Zhang et al. (2006) built a Chinese-English financial lexicon from a comparable corpus with focus on the impact of seed lexicon selection. Haghighi et al. (2008) proposed a generative model to construct lexicons for multiple language pairs, including EnglishChinese, via canonical correlation analysis, which effectively explores monolingual lexicons in terms of latent matching. In particular for BLC without any external lexicon, Fung (1995) focused on context heterogeneity in Chinese and English languages, which measures how productive the context of a word is, instead of its absolute occurrence frequency. She suggested that bilingual translation words tend to share similar context heterogeneity in non-parallel corpora. Specifically, she calculated the similarity between two bilingual words using the ratios of unique words in the right and left contexts. Yu and Tsujii (2009) proposed the notion of dependency heterogeneity, which assumes that a word and its translation should share similar modifiers and heads in comparable corpor"
C12-1139,W09-1117,0,0.257847,", 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words between two closely related languages (Ficšer and Ljubešić, 2011) or by aligning translation pairs from parallel sentences, which is mined in advance from a comparable corpus (Morin and Prochasson, 2011). The problem with above method is that they only consider the words involved in the contexts and ignore other rich information therein, such as syntactic relationships, t"
C12-1139,D11-1084,1,0.799809,"上下文更为可靠的信息。我们还进 一步展示了在没有人工干预的情况下可以产生和利用这种双语依存关系。从英文到中文的 双语词表构建实验表明，通过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, vario"
C12-1139,1998.amta-tutorials.5,0,0.0982495,"过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann a"
C12-1139,kaji-etal-2008-automatic,0,0.0215735,"refore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera"
C12-1139,W02-0902,0,0.470016,"corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words between two closely related languages (Ficšer and Ljubešić, 2011) or by aligning translation pairs from parallel sentences, which is mined in advance from a comparable corpus (Morin and Prochasson, 2011). The problem with above method is t"
C12-1139,N01-1020,0,0.105907,"Missing"
C12-1139,de-marneffe-etal-2006-generating,0,0.00571831,"7 3.1 Comparable corpus In this paper, we generate a comparable corpus from the parallel Chinese-English Foreign Broadcast Information Service (FBIS) corpus, gathered from the news domain. This bilingual corpus contains about 240k sentences, 6.9 million words in Chinese and 8.9 million words in English. Similar to the way adopted in (Garera et al., 2009; Haghighi et al., 2008), we couple the first half of Chinese corpus and the second half on the English side as our comparable corpus. For corpus pre-processing, we use the Stanford POS tagger (Toutanova and Manning, 2000) and syntactic parser (Marneffe et al., 2006) to generate the POS and dependency information for each sentence in both Chinese and English corpora. Particularly, English words are transformed to their respective lemmas using the TreeTagger package (Helmut, 1994). 3.2 Bilingual lexicons for evaluation: seed, test and development In the literature, different scales of bilingual seed lexicons have been used. For example, Rapp (1999) and Fung (2000) used large-scale dictionaries of 10-20k word pairs while other studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009) used only small dictionaries of about 100-1000 word pai"
C12-1139,P09-1030,0,0.0248923,"with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore wo"
C12-1139,W11-1205,0,0.0252602,"etween the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words between two closely related languages (Ficšer and Ljubešić, 2011) or by aligning translation pairs from parallel sentences, which is mined in advance from a comparable corpus (Morin and Prochasson, 2011). The problem with above method is that they only consider the words involved in the contexts and ignore other rich information therein, such as syntactic relationships, thus usually suffering from low performance especially when they are applied to two distinct languages such as English and Chinese. For example, our preliminary experiment with the dependency-based model (Garera et al., 2009) shows that English source word “profit” matches wrongly with Chinese target word “企业” (enterprise), instead of the correct one “利润”, due to the higher similarity score with the former than that with the l"
C12-1139,P00-1056,0,0.053703,"lish corpora. Particularly, English words are transformed to their respective lemmas using the TreeTagger package (Helmut, 1994). 3.2 Bilingual lexicons for evaluation: seed, test and development In the literature, different scales of bilingual seed lexicons have been used. For example, Rapp (1999) and Fung (2000) used large-scale dictionaries of 10-20k word pairs while other studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009) used only small dictionaries of about 100-1000 word pairs. In this paper, we adopt a small scale one. In particular, we use the GIZA++ package (Och and Ney, 2000) to extract the most frequently occurring 1000 word pairs as the bilingual seed lexicon (denoted as Ls) and the subsequent 500 noun pairs (denoted as LNt) as the primary bilingual test lexicon. This way of generating the test lexicon for nouns has been commonly used in previous studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). Besides, we frame a secondary test lexicon (denoted as LAt) including 200 nouns, verbs and adjectives respectively, which spread evenly in the four ranges of 1001-2000, 2001-3000, 3001-4000 and 4001-5000. The goal of LAt is to evaluate the ada"
C12-1139,J03-1002,0,0.00281751,"合方法为双语词表构建提供了比单一的词语上下文更为可靠的信息。我们还进 一步展示了在没有人工干预的情况下可以产生和利用这种双语依存关系。从英文到中文的 双语词表构建实验表明，通过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994"
C12-1139,P99-1067,0,0.479653,"ltiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words"
C12-1139,W02-2026,0,0.0302353,"for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and wor"
C12-1139,P10-1011,0,0.0138969,"turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dep"
C12-1139,C94-1048,0,0.224167,"(MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languag"
C12-1139,W00-1308,0,0.0815763,"as well as a state-of-the-art baseline for BLC. 2277 3.1 Comparable corpus In this paper, we generate a comparable corpus from the parallel Chinese-English Foreign Broadcast Information Service (FBIS) corpus, gathered from the news domain. This bilingual corpus contains about 240k sentences, 6.9 million words in Chinese and 8.9 million words in English. Similar to the way adopted in (Garera et al., 2009; Haghighi et al., 2008), we couple the first half of Chinese corpus and the second half on the English side as our comparable corpus. For corpus pre-processing, we use the Stanford POS tagger (Toutanova and Manning, 2000) and syntactic parser (Marneffe et al., 2006) to generate the POS and dependency information for each sentence in both Chinese and English corpora. Particularly, English words are transformed to their respective lemmas using the TreeTagger package (Helmut, 1994). 3.2 Bilingual lexicons for evaluation: seed, test and development In the literature, different scales of bilingual seed lexicons have been used. For example, Rapp (1999) and Fung (2000) used large-scale dictionaries of 10-20k word pairs while other studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009) used only"
C12-1139,1994.amta-1.26,0,0.0960705,"存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information ha"
C12-1139,N09-2031,0,0.237284,"via canonical correlation analysis, which effectively explores monolingual lexicons in terms of latent matching. In particular for BLC without any external lexicon, Fung (1995) focused on context heterogeneity in Chinese and English languages, which measures how productive the context of a word is, instead of its absolute occurrence frequency. She suggested that bilingual translation words tend to share similar context heterogeneity in non-parallel corpora. Specifically, she calculated the similarity between two bilingual words using the ratios of unique words in the right and left contexts. Yu and Tsujii (2009) proposed the notion of dependency heterogeneity, which assumes that a word and its translation should share similar modifiers and heads in comparable corpora, no matter whether they occur in similar contexts or not. In this sense, our approach is similar to theirs. However, while their distance measure of dependency heterogeneity is limited to three easily-mapping common relationships between two languages, namely SUB, OBJ and NMOD, we further generalize to automatic mappings of any bilingual dependency relationships. Another difference is that our method considers dependent words and their r"
C12-1139,C02-1144,0,\N,Missing
C12-1139,P08-1088,0,\N,Missing
C12-2130,W05-0305,0,0.0873018,"In Section 3 the PDTB corpus is briefly introduced. Section 4 describes the methodology used for exact argument identification. In Section 5 the results of the research experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the P"
C12-2130,I11-1120,0,0.386259,"rch experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then cond"
C12-2130,W12-1622,0,0.0651841,"e subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and"
C12-2130,W11-4616,0,0.0388588,"Missing"
C12-2130,W10-2910,0,0.0381901,"Missing"
C12-2130,P09-1078,0,0.0295215,"‘and ensure that’, after the connective ‘and’, sometimes denotes abstract objects located in the previous sentence. Based on this observation, we add 8 new features: next1, next1 POS, next1+C, next1 POS+C POS, next2, next2 POS, next2+C, next2 POS+C POS. It is hard to decide which feature-set is more effective for Arg1 position identification, even if we use the Hill-climbing (greedy) feature selection technique, Caruana and Freitag (1994), due to the combination of a large number of different features. Therefore, we adopt the Information Gain (IG), which is widely used in text classification, Li et al. (2009), to calculate the efficacy of features and select an approximate optimal feature-set. After Arg1’s position is identified, we handle the DAI according to intra-sentence and intersentence cases methodologies, as follows. 4.1 Formulating Intra-sentence DAI as a Simplified Semantic Parsing Problem Given a parse tree and a predicate, shallow semantic parsing detects and classifies each of the constituents in the sentence into either their corresponding semantic argument (role) for the predicate, or as a non-argument. Similarly, the discourse connective can be taken as the predicate, while its sco"
C12-2130,P10-1113,1,0.848622,"lobal sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-based coherence Relatio"
C12-2130,C10-1076,1,0.723365,"lobal sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-based coherence Relatio"
C12-2130,D09-1036,0,0.0232391,"notated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-bas"
C12-2130,P11-3009,0,0.0281253,"Missing"
C12-2130,W09-1105,0,0.0648834,"Missing"
C12-2130,P04-1043,0,0.0519013,", 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-bas"
C12-2130,P09-1077,0,0.0437391,"lden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al."
C12-2130,P09-2004,0,0.104768,"Missing"
C12-2130,prasad-etal-2008-penn,0,0.0608383,"t identification. In Section 5 the results of the research experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether"
C12-2130,prasad-etal-2010-exploiting,0,0.0616553,"corpus is briefly introduced. Section 4 describes the methodology used for exact argument identification. In Section 5 the results of the research experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010)"
C12-2130,D09-1018,0,0.0740607,"Missing"
C12-2130,P10-1073,0,0.0119903,". Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. ("
C12-2130,D07-1010,0,0.37298,"Missing"
C12-2130,W04-3212,0,0.0970137,"Missing"
C12-2130,C10-2172,0,0.0124451,"proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow se"
C12-2130,D10-1070,1,0.724788,"nt difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-based coherence Relation(EntRel) and No Relation (NoRel). PDTB regards connectives as the discourse predicate, taking two text spans as two arguments, Arg1 and Arg2, which describe the events, facts and/or propositions. Of the two arguments Arg2 is syntactically bound to the connective, while Arg 1 is not. In addition, 3-layered hierarchy, sema"
C12-2130,J03-4003,0,\N,Missing
C14-1176,D10-1117,0,0.0238802,"s by sampling them from the posterior P (T |S), where T is a latent synchronous tree of a sentence pair S. As presented in the beginning of section 3, the posterior depends on P (αij,pq |Bij,pq = b) and P (βij,pq |Bij,pq = b), on which we put the PYP prior and the Dirichlet prior respectively. Because of integrating out all Gs in all of the priors, interdependency between samples of αij,pq |Bij,pq = b or βij,pq |Bij,pq = b is introduced, resulting in simultaneously obtaining multiple samples impractical. On the other hand, blocked sampling, which obtains sentence-level samples simultaneously (Blunsom and Cohn, 2010; Cohn et al., 2010; Johnson et al., 2007a) is attractive for the fast mixing speed and the easy application of standard dynamic programming algorithms. 4.1 Metropolis-Hastings (MH) Sampler We apply a MH sampler similar to (Johnson et al., 2007a) to overcome the difficulty of obtaining multiple samples simultaneously from posterior. The MH sampler is a MCMC technique that draws samples from a true distribution by first drawing samples simultaneously from a proposal distribution, and then correcting the samples to the true distribution by using an accept/reject test. In practical, the proposal"
C14-1176,J93-2003,0,0.0585004,"roves the quality of SMT results. 1 Introduction Traditional Statistical Machine Translation (SMT) learns translation model from bilingual corpus that is sentence aligned. No large-scale hand aligned structures inside the parallel sentences are usually available to the SMT community, while the aligned structures are essential for training the translation model. Thus, various unsupervised methods had been explored to automatically obtain aligned structures inside the parallel sentences. Currently, the dominant method is a two step pipeline that obtains word alignments by unsupervised learning (Brown et al., 1993) at the first step, then obtains aligned structures at the second step by heuristically extracting all bilingual structures that are consistent with the word alignments. The second step in this two step pipeline is problematic due to its obtained aligned structures, whose counts are heuristically collected and violate valid translation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction ai"
C14-1176,D09-1037,0,0.0224342,". The second step in this two step pipeline is problematic due to its obtained aligned structures, whose counts are heuristically collected and violate valid translation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilin"
C14-1176,P13-1077,0,0.184589,"on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is valuable to the synchronous grammar induction community due to its standard evaluation on released monolingual treebanks, while no hand ann"
C14-1176,P07-1035,0,0.0339036,"priors, the Pitman-Yor-Process (PYP) prior, is defined on αij,pq |Bij,pq . The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is commonly observed in natural languages, and can flexibly model distributions on layer structures due to its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b; Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et al., 2010). We use the PYP to model the constituents’ layered structure by using the PYP’s distribution hierarchy. On βij,pq |Bij,pq , we use the Dirichlet distribution for its simplicity because contexts appear in much fewer kinds of surface strings than those of constituents. 3.1 The PYP Prior over Bilingual Constituents Constituents consist of both words and POS tags. Though in much monolingual grammar induction works, only POS tag sequences were used as the observed constituents for their significant hints of phrases (Klein and Manning, 200"
C14-1176,N07-1018,0,0.513429,"contexts, the proposed Bayesian prior over αij,pq |Bij,pq is more complicate than that over βij,pq |Bij,pq . Specifically, one of the non-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is defined on αij,pq |Bij,pq . The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is commonly observed in natural languages, and can flexibly model distributions on layer structures due to its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b; Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et al., 2010). We use the PYP to model the constituents’ layered structure by using the PYP’s distribution hierarchy. On βij,pq |Bij,pq , we use the Dirichlet distribution for its simplicity because contexts appear in much fewer kinds of surface strings than those of constituents. 3.1 The PYP Prior over Bilingual Constituents Constituents consist of both words and POS tags. Though in much mon"
C14-1176,P02-1017,0,0.399976,"orresponding Author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1865 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1865–1874, Dublin, Ireland, August 23-29 2014. grammar induction. According to the evaluation results, the state-of-the-art monolingual grammar induction was achieved by Bayesian modeling of the Constituent Context Model (CCM) (Duan et al., 2013; Klein and Manning, 2002), while traditional CFGs based monolingual grammar induction methods perform well below the CCM. In view of the significant achievements of the CCM in monolingual grammar induction, we propose the SCCM to apply the CCM to the bilingual case. The tremendous possible constituents and contexts incurred in this bilingual case put a challenge for the SCCM to model such kind of sparse variables. We further propose a non-parametric Bayesian Modeling of the SCCM to cope with the sparse variables. Experiments on Chinese-English machine translation show that meaningful synchronous phrases can be detecte"
C14-1176,N03-1017,0,0.0725834,"used metric: the alignment error rate (AER) to evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments. The AER is given by (the lower the better): AER(a, s, p) = 1 − |a ∩ s |+ |a ∩ p| |a |+ |s| In the HIT corpus, only sure alignments were annotated, possible alignments were bypassed because of the strict annotation standard of semantic equivalence. The word alignments evaluation results are reported in Table 2. The baseline was GIZA++ model 4 in both directions with symmetrization by the grow-diag-final-and heuristic (Koehn et al., 2003). A 1 The initialization with different random split bi-trees results in marginal variance of performances. HIT corpus is designed and constructed by HIT-MITLAB. http://mitlab.hit.edu.cn/index.php/resources.html 3 We did not use annotated tree node alignments for synchronous structure evaluation because the coverage of tree nodes that can be aligned is quite low. The reason of low coverage is that Chinese and English exhibit great syntax divergences from monolingual treebank point of view. 2 1871 released induction system - PIALIGN (Neubig et al., 2011)4 was also experimented to compare with o"
C14-1176,D12-1021,0,0.0932313,"and violate valid translation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, who"
C14-1176,D07-1072,0,0.0323014,"-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is defined on αij,pq |Bij,pq . The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is commonly observed in natural languages, and can flexibly model distributions on layer structures due to its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b; Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et al., 2010). We use the PYP to model the constituents’ layered structure by using the PYP’s distribution hierarchy. On βij,pq |Bij,pq , we use the Dirichlet distribution for its simplicity because contexts appear in much fewer kinds of surface strings than those of constituents. 3.1 The PYP Prior over Bilingual Constituents Constituents consist of both words and POS tags. Though in much monolingual grammar induction works, only POS tag sequences were used as the observed constituents for their significant hints of phrases ("
C14-1176,P11-1064,0,0.30999,"m leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is valuable to the synchronous grammar induction community due to its standard evaluation on released monolingual tre"
C14-1176,P03-1021,0,0.0215303,"onsists of a parallel corpus extracted from the Basic T ravel Expression Corpus (BTEC), which had been used in evaluation campaigns of the yearly International Workshop on Spoken Language Translation (IWSLT). Table 3 lists statistics of the corpus used in the experiment. Table 3: Statistics of the corpus used by IWSLT sent word avg. len. ch 23k 190k 8.3 en 213k 9.2 We used CSTAR03 as development set, used IWSLT04 and IWSLT05 official test set for test. A 4-gram language model with modified Kneser-Ney smoothing was trained on English side of parallel corpus. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. Experimental results were evaluated by case-insensitive BLEU-4 (Papineni et al., 2001). Closest reference sentence length was used for brevity penalty. 5.3.2 Results Following (Levenberg et al., 2012; Neubig et al., 2011; Cohn and Haffari, 2013), we evaluate our model by using the SCCM’s output word alignments to construct a phrase table. As a baseline, we train a phrase-based model using the moses toolkit 5 based on the word alignments obtained using GIZA++ 4 5 http://www.phontron.com/pialign/ http://www.s"
C14-1176,W09-3804,0,0.0198619,"al/word analysis, while CCM has obtained state-of-the-art performance on the more complex unsupervised task - inducing syntactic trees. In view of CCM’s successful monolingual application, we generalize it to bilingual case. In depth comparison: our SCCM deals with both consituents and distituents, and contexts of them, while PIALIGN only deals with constituents. Furthermore, SCCM does not model non-terminal rewriting rules, while PIALIGN model those rules which can rewrite a non-terminal into a complete subtree as adaptor grammars does. In addition, PIALIGN adopts a beam search algorithm of (Saers et al., 2009). Through setting small beam size, PIALIGN’s time complexity is almost O(n3 ). But as critisized by (Cohn and Haffari, 2013), their heuristic beam search algorithm does not meet either of the Markov Chain Monte Carlo (MCMC) criteria of ergodicity or detailed balance. Our method adopts MCMC sampling (Johnson et al., 2007a) which meets the MCMC criteria. We can see that the two induction systems perform significantly better than GIZA++, and our proposed SCCM performs better than PIALIGN. Manual evaluation for the quality of the phrase pairs generated from word alignments is also reported in Tabl"
C14-1176,D13-1026,0,0.018532,"while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is valuable to the synchrono"
C14-1176,C12-1176,0,0.0152293,"lation derivations, while most SMT decoders perform translation via valid translation derivations. This problem leads to researches on synchronous grammar induction that discards the heuristic method and the two separate steps pipeline. Synchronous grammar induction aims to directly obtain aligned structures by using one statistically sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level (Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al., 2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or discriminative modeling. This paper departs from using the above two traditional CFGs-based grammars, and proposes Synchronous Constituent Context Model (SCCM) which models synchronous constituents and contexts directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is inspired by researches on monolingual grammar induction, whose experience is va"
C14-1204,D13-1178,0,0.037849,"Missing"
C14-1204,P08-1090,0,0.0926641,"Missing"
C14-1204,P09-1068,0,0.0867911,"Missing"
C14-1204,P11-1098,0,0.0373346,"Missing"
C14-1204,W09-2209,0,0.0625834,"Missing"
C14-1204,N09-2053,0,0.0634474,"Missing"
C14-1204,E12-1029,0,0.314158,"from free texts is a higher-level Information Extraction (IE) task, which is still a challenge due to the complexity of natural language and the domain-specific nature, especially in Chinese for its specific characteristics. In particular, most of previous studies have focused on English event extraction, while only a few concern Chinese. Currently, supervised learning models have dominated event extraction. To reduce the labeled data required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classifier-based model needs dozens of annotated documents to train model, most of previous semisupervised models focused on pattern-based approach, which only needed a few seed (event) patterns. In those pattern-based approaches, frequent event patterns, which occur in many documents, were chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in a Chinese sentence is rather agile for its open and flexible structure, and different orders might express the same meaning due to the semantics-driven nature of the Chinese language. This results"
C14-1204,P08-1030,0,0.034344,"Missing"
C14-1204,C12-1099,1,0.904356,"o single-morpheme word to generate two candidate trigger mentions when the following three conditions are satisfied: 1) verb  POS( m1 )  verb∈ POS( m2 ) 2) Max ( Wsim ( m1 , s1 ))  1∧ Max ( Wsim ( m2 , s2 ))  1∧ Etype( s1 ) ≠ Etype( s2 ) s1 ∈seeds s2∈seeds 3) Morph(m1 m2)= Coordination where POS(m) returns all possible parts of speech of morpheme m in Hownet and Etype(s) is to obtain the event type of seed trigger s; WSim(m,s) is defined in Subsection 3.3 and returns 1 when one word m is the synonym of the other word s; Morph(w) is to obtain the morphological structure of word w following Li and Zhou (2012). Since there is a strong trigger consistency in those two-morpheme words of Coordination structure which refers to two distinct events, we propose an event inference mechanism as follows. Compositional semantics: For each two-morpheme word identified by the above three conditions, if one of its morphemes has been extracted as an trigger mention of a specific event type, the other morpheme in the same word will refer to an a relevant event type. 4.3 Event Inference on Coreference Events To mine more event mentions, we use the simple trigger-entity pair to represent event pattern in this paper."
C14-1204,P13-1145,1,0.884039,"Missing"
C14-1204,C10-1077,0,0.581209,"ct the event mentions with infrequent patterns. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 2161 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2161–2171, Dublin, Ireland, August 23-29 2014. In this paper, we first implement a pattern-based semi-supervised model for Chinese event extraction as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover missing event mentions. These event inference mechanisms can capture the linguistic knowledge from semantics of argument role, compositional semantics of trigger, consistency on coreference events and relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mechanisms dramatically outperform the baseline. The rest of this paper is organized as follows. Section 2"
C14-1204,P10-1081,0,0.362818,"ct the event mentions with infrequent patterns. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 2161 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2161–2171, Dublin, Ireland, August 23-29 2014. In this paper, we first implement a pattern-based semi-supervised model for Chinese event extraction as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover missing event mentions. These event inference mechanisms can capture the linguistic knowledge from semantics of argument role, compositional semantics of trigger, consistency on coreference events and relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mechanisms dramatically outperform the baseline. The rest of this paper is organized as follows. Section 2"
C14-1204,P11-2045,0,0.498629,"Missing"
C14-1204,O02-2003,0,0.0610631,") = Max( WSim( t p ,t s ) ×ESim( e p ,es ) ×DSim( d p , d s ) s∈P (3) where t, e and d represent the trigger, entity type and dependency path in candidate pattern p(tp, ep, dp) or seed pattern s(ts, es, ds) in the set of extracted patterns P, respectively; ESim identifies whether two entities have the same type, and assigned 1 if two entities have the same entity type and otherwise a small number 0.1; DSim calculates the similarity between two dependency paths in edit distance. Finally, WSim is to obtain the trigger similarity in lexical semantics, using Hownet (Dong and Dong, 2006) following Liu and Li (2002): WSim (t p , t s )   Dis(t p , t s )   (4) where Dis(tp,ts) is the distance between the sememes of triggers tp and ts, in HowNet’s sememe hierarchical architecture, with parameter ϕ assigned 0.75 following Liu and Li (2002). 4 Event Inference The pattern-based semi-supervised model cannot extract those event mentions matching infrequent patterns or without matching patterns. The knowledge from linguistic aspect (e.g., definition of events, compositional semantics of Chinese words, coreference events and relevant events, etc.) is helpful to further recover missing event mentions or filter"
C14-1204,E12-1030,0,0.0300677,"Missing"
C14-1204,P07-1075,0,0.0352049,"Missing"
C14-1204,D09-1016,0,0.0441251,"Missing"
C14-1204,P05-1047,0,0.0347413,"Automatically extracting events from free texts is a higher-level Information Extraction (IE) task, which is still a challenge due to the complexity of natural language and the domain-specific nature, especially in Chinese for its specific characteristics. In particular, most of previous studies have focused on English event extraction, while only a few concern Chinese. Currently, supervised learning models have dominated event extraction. To reduce the labeled data required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classifier-based model needs dozens of annotated documents to train model, most of previous semisupervised models focused on pattern-based approach, which only needed a few seed (event) patterns. In those pattern-based approaches, frequent event patterns, which occur in many documents, were chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in a Chinese sentence is rather agile for its open and flexible structure, and different orders might express the same meaning due to the semantics-driven nature of the Chines"
C14-1204,H01-1009,0,0.0679108,"Missing"
C14-1204,P03-1029,0,0.113623,"Missing"
C14-1204,P03-1044,0,0.117077,"Missing"
C14-1204,C00-2136,0,\N,Missing
C14-1204,P11-1113,1,\N,Missing
C14-1204,P03-1028,0,\N,Missing
C16-1137,W06-1623,0,0.138231,"action. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). However, their gains are rather small, largely due to the common disconnectedness in the sparsely annotated corpora (Chambers et al., 2014). To overcome this problem, Denis and Muller (2011) decomposed temporal entities into sub-graphs and enforced the coherence only within these substructures, while Do et al. (2012) proposed a joint eventevent and event-time classification model to enforce various coreference constraints. Different from previous studies, we build an event-driven fully-annotated Chinese corpus and propose 1452 a discourse"
C16-1137,P14-2082,0,0.428167,"lems with the TimeBank corpus are that it only annotates a small subset of easily-identified event mention pairs and that it largely ignores those temporal relations between event mentions in nonadjacent sentences. These lead to fragmented relations and much limit its applications. To overcome above problems, Do et al. (2012) produced an event-driven corpus on the ACE 2005 English corpus. However, “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, as stated in their paper. This makes the annotation inconsistent and difficult to follow. Recently, Cassidy et al. (2014) enriched the TimeBank-Dense corpus, on the top of TimeBank. Specifically, they approximated the completeness by labeling locally complete graphs over neighboring sentences. In comparison, there are few corpora for Chinese temporal relation extraction. Li et al. (2004) annotated a Chinese corpus including 700 sentences. The TempEval-2 competition (Verhagen et al., 2010) provided 780 instances of Chinese temporal event relations. Obviously, both corpora are rather small and largely impede the research in Chinese temporal relation extraction. For example, no team participated in the TempEval-2 c"
C16-1137,P07-2044,0,0.156417,"on. For example, no team participated in the TempEval-2 competition on Chinese temporal relation extraction. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Ch"
C16-1137,D08-1073,0,0.119331,"few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). However, their gains are rather small, largely due to the common disconnectedness in the sparsely annotated corpora (Chambers et al., 2014). To overcome this problem, Denis and Muller (2011) decomposed temporal entities into sub-graphs and enforced the coherence only within these substructures, while Do et al. (2012) proposed a joint eventevent and event-time classification model to enforce various coreference constraints. Different from previous studies, we build an event-driven fully-annotated Chinese corpus and propose 1452 a discourse-level global inference model"
C16-1137,Q14-1022,0,0.0143842,"l transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivi"
C16-1137,I08-4005,0,0.042478,"Missing"
C16-1137,N13-1112,0,0.0404303,"Missing"
C16-1137,D12-1062,0,0.353459,"al relation extraction, the TimeBank corpus (Pustejovsky et al., 2003) has been adopted in a series of TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; Uz-Zaman et al., 2013), facilitating the development and evaluation of temporal relation extraction systems. The problems with the TimeBank corpus are that it only annotates a small subset of easily-identified event mention pairs and that it largely ignores those temporal relations between event mentions in nonadjacent sentences. These lead to fragmented relations and much limit its applications. To overcome above problems, Do et al. (2012) produced an event-driven corpus on the ACE 2005 English corpus. However, “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, as stated in their paper. This makes the annotation inconsistent and difficult to follow. Recently, Cassidy et al. (2014) enriched the TimeBank-Dense corpus, on the top of TimeBank. Specifically, they approximated the completeness by labeling locally complete graphs over neighboring sentences. In comparison, there are few corpora for Chinese temporal relation extraction. Li et al. (2004) annotated a Chinese corpus including"
C16-1137,S13-2015,0,0.0180355,"tion. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presen"
C16-1137,P06-1095,0,0.318732,"emporal event relations. Obviously, both corpora are rather small and largely impede the research in Chinese temporal relation extraction. For example, no team participated in the TempEval-2 competition on Chinese temporal relation extraction. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those diffe"
C16-1137,P04-1074,0,0.241398,"cations. To overcome above problems, Do et al. (2012) produced an event-driven corpus on the ACE 2005 English corpus. However, “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, as stated in their paper. This makes the annotation inconsistent and difficult to follow. Recently, Cassidy et al. (2014) enriched the TimeBank-Dense corpus, on the top of TimeBank. Specifically, they approximated the completeness by labeling locally complete graphs over neighboring sentences. In comparison, there are few corpora for Chinese temporal relation extraction. Li et al. (2004) annotated a Chinese corpus including 700 sentences. The TempEval-2 competition (Verhagen et al., 2010) provided 780 instances of Chinese temporal event relations. Obviously, both corpora are rather small and largely impede the research in Chinese temporal relation extraction. For example, no team participated in the TempEval-2 competition on Chinese temporal relation extraction. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from En"
C16-1137,E14-1033,0,0.0201211,"r neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global"
C16-1137,W04-2401,0,0.07367,"ne the interaction among events in a document, we optimize the predicted temporal graph, formed by prediction from CE-E, with various kinds of discourse-level constraints derived from event semantics. Let E={e1,e2,…,en} denote the set of event mentions in a document, ε={(ei,ej)∈E×E|ei,ej∈E, i≠j} the set of event mention pairs, and R={ B, A, O,U } the set of temporal relations. Besides, let P<i,j,r> denote the prediction probability of (ei,ej) with relation r (r∈R), given by the event-event classifier CE-E, and x<i,j,r> the binary indicator on the existence of relation r for (ei,ej). Following Roth and Yih (2004) and Li et al. (2013) in information extraction, we define the following log costs: ci , j ,r    log( Pi , j ,r  ) (2) ci , j ,r   log(1  Pi , j ,r ) (3) Specifically, ILP (Integer Logical Programming), a global inference is employed to achieve global optimization with the following objective function to maximize over a document as follows: arg min x   (c ( ei ,e j ) rR i , j ,r   xi , j ,r   (1  xi , j ,r  )  c i , j ,r  ) (4) s.t. xi , j ,r  {0,1} (5) x (6) rR i , j ,r  1 while binary constraint (5) ensures that x<i,j,r> is binary value and equality cons"
C16-1137,S13-2001,0,0.164888,"Missing"
C16-1137,S07-1014,0,0.358501,"Missing"
C16-1137,P09-1046,0,0.0235071,"relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). However, their gains are rather small, largely due to the common disconnectedness in the sparsely annotated corpora (Chambers et al., 2014). To overcome this problem, Denis and Muller (2011) decomposed temporal entities into sub-graphs and enforced the coherence only within these substructures, while Do et al. (2012) proposed a joint eventevent and event-time classification model to enforce various coreference constraints. Different from previous studies, we build an event-driven fully-annotated Chinese corpus and propose 1452 a discourse-level global inference model to extract temporal rela"
C18-1044,P16-1163,0,0.0219228,"ations of two discourse units can be used as an effective feature to determine their nuclearity. We also calculated the cosine similarities ???(?1 , ?? ) and ???(?2 , ?? ) between the discourse unit and the paragraph to measure the similarity between the discourse unit and the topic of the paragraph. These two similarities are helpful for identifying the mononuclear relations. Bilinear is a simple way to incorporate the linear interactions between two vectors and is defined as follows: ?(?1 , ?2 ) = ?1⊤ ??2 , (4) ?×? where ? ∈ ℝ is the parameter matrix. Usually, when using the Bilinear model (Chen et al., 2016; Wan et al., 2016; Wu et al., 2017), the Bilinear value ?(??? , ??? ) = ???? ???? is calculated for any two words in the two word sequences {?1 , ?2 , … , ?? } and {?1 , ?2 , … , ?? } to obtain a matching matrix, where ??? , ??? are semantic vectors that correspond to word ?? and ?? . A discourse unit or a paragraph could contain a larger number of words, and it will lead to generating an enormous matching matrix. However, the number of training samples that can be used in our model is relatively small, which results in great difficulty with training the parameter ?. Therefore, we simplified"
C18-1044,P14-1048,0,0.403779,"e., a in Figure 1 (中国机电产品进出口贸易继续增加 The import and export trade of China’s mechanical and electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, th"
C18-1044,C14-1045,0,0.0562207,"Missing"
C18-1044,P14-1002,0,0.0240827,"Section 3 gives the details of our model TMN, Section 4 reports the experimental results and Section 5 gives the conclusions. 2 Related Work Previous studies on nuclearity recognition mainly focused on English, with RST Discourse Treebank (RST-DT) (Carlson et al., 2003) being the most popular corpus. However, most of them only regard 526 nuclearity recognition as a trivial component of overall discourse parsing, and they ignore its specific characteristics and critical importance. The algorithms of nuclearity recognition published on RST-DT can mainly be categorised as shiftreduce algorithms (Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Wang et al., 2017), probabilistic CKY-like algorithms (Joty et al., 2013; Li et al., 2014; Li et al., 2016) and greedy bottom-up algorithms (Feng and Hirst, 2014). Li et al. (2016) applied different classifiers to three discourse parsing subtasks separately, but they share the high level representation of discourse units by the same network structures. Wang et al. (2017) used a transition-based system to build discourse trees with nuclearity labels and then used Support Vector Machines (SVMs) to determine the discourse relations at different text levels. Most of the"
C18-1044,P13-1048,0,0.0201529,"ves the conclusions. 2 Related Work Previous studies on nuclearity recognition mainly focused on English, with RST Discourse Treebank (RST-DT) (Carlson et al., 2003) being the most popular corpus. However, most of them only regard 526 nuclearity recognition as a trivial component of overall discourse parsing, and they ignore its specific characteristics and critical importance. The algorithms of nuclearity recognition published on RST-DT can mainly be categorised as shiftreduce algorithms (Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Wang et al., 2017), probabilistic CKY-like algorithms (Joty et al., 2013; Li et al., 2014; Li et al., 2016) and greedy bottom-up algorithms (Feng and Hirst, 2014). Li et al. (2016) applied different classifiers to three discourse parsing subtasks separately, but they share the high level representation of discourse units by the same network structures. Wang et al. (2017) used a transition-based system to build discourse trees with nuclearity labels and then used Support Vector Machines (SVMs) to determine the discourse relations at different text levels. Most of the previous studies used SVMs and variants of Conditional Random Fields (CRFs); only Li et al. (2014)"
C18-1044,D14-1220,0,0.258813,"d electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic features), which are widely used in English"
C18-1044,D16-1035,0,0.142759,"ucts continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic features), which are widely used in English, might not be eff"
C18-1044,D14-1224,1,0.66149,"d electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic features), which are widely used in English"
C18-1044,W10-4327,0,0.0325169,"ion about the nucleus (Stede, 2008), while a multinuclear relation holds two or more discourse units, which are all nuclei. Therefore, three types of nuclearity exist: Nucleus-Satellite if the left subtree is the nucleus and the right subtree is the satellite, Satellite-Nucleus if the order of the satellite and nucleus is inverted, and Nucleus-Nucleus for multinuclear relations. Nuclearity recognition is helpful in detecting discourse relations (Iruskieta et al., 2014) and extracting the main content of a document, and it is widely used in various NLP tasks, including automatic summarisation (Louis et al., 2010; Marcu, 2000), question answering (Verberne et al., 2007) and information extraction (Zou et al., 2014). Consider the following document as an example: Example 1: 中国机电产品进出口贸易继续增加 a，占总进出口的比重继续上升 b。其中，出口 五十七点九亿美元 c，占总出口的百分之三十二点五 d；进口八十五点二亿美元 e，占总进口的 百分之四十六点四 f，均比去年同期有所上升 g。The import and export trade of China’s mechanical and electronic products continues to increase a, and its proportion of the total imports and exports also continues to rise b. Among them, the exports amounted to 5.79 billion dollars c, accounting for 32.5 percent of the total exports d; and the imports of 8.52 billion dollar"
C18-1044,prasad-etal-2008-penn,0,0.0632604,"Missing"
C18-1044,P16-1044,0,0.033033,"a mononuclear relation, the nucleus unit is usually semantically closer to the topic of the paragraph. Therefore, our TMN model makes the semantic match between not only the different discourse units but also the discourse unit and paragraph, by three similarity metrics, namely, the Cosine, Bilinear and Single Layer Network, which can capture the features that are related to nuclearity recognition adequately. 3.1 Text Encoding Our Text Encoding module combines Bi-LSTM and CNN to encode the discourse unit ??? and the paragraph ????, which is the modification of the Convolutional-pooling LSTM (Tan et al., 2016) in question answering. Its input is a sequence of words (?1 , ?2 , … , ?? ) in a discourse unit ??? or a paragraph ???? where ? is the number of words in the discourse unit or paragraph. Each word ?? in the sequence is represented as the combination of its word embedding ?? and POS (Part-Of-Speech) tag embedding ?? as follows: (1) ?? = [?? , ?? ]. LSTM models successfully keep the useful information from long-range dependency, but they focus more on the words that are behind. Due to the need for our model to treat each word equally, Bi-LSTM is introduced to the Text Encoding module. At each p"
C18-1044,P17-2029,0,0.0557675,"nd export trade of China’s mechanical and electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic featur"
C18-1044,P17-1046,0,0.0313515,"used as an effective feature to determine their nuclearity. We also calculated the cosine similarities ???(?1 , ?? ) and ???(?2 , ?? ) between the discourse unit and the paragraph to measure the similarity between the discourse unit and the topic of the paragraph. These two similarities are helpful for identifying the mononuclear relations. Bilinear is a simple way to incorporate the linear interactions between two vectors and is defined as follows: ?(?1 , ?2 ) = ?1⊤ ??2 , (4) ?×? where ? ∈ ℝ is the parameter matrix. Usually, when using the Bilinear model (Chen et al., 2016; Wan et al., 2016; Wu et al., 2017), the Bilinear value ?(??? , ??? ) = ???? ???? is calculated for any two words in the two word sequences {?1 , ?2 , … , ?? } and {?1 , ?2 , … , ?? } to obtain a matching matrix, where ??? , ??? are semantic vectors that correspond to word ?? and ?? . A discourse unit or a paragraph could contain a larger number of words, and it will lead to generating an enormous matching matrix. However, the number of training samples that can be used in our model is relatively small, which results in great difficulty with training the parameter ?. Therefore, we simplified this process to calculate the Biline"
C18-1044,P14-1049,1,0.82053,"ch are all nuclei. Therefore, three types of nuclearity exist: Nucleus-Satellite if the left subtree is the nucleus and the right subtree is the satellite, Satellite-Nucleus if the order of the satellite and nucleus is inverted, and Nucleus-Nucleus for multinuclear relations. Nuclearity recognition is helpful in detecting discourse relations (Iruskieta et al., 2014) and extracting the main content of a document, and it is widely used in various NLP tasks, including automatic summarisation (Louis et al., 2010; Marcu, 2000), question answering (Verberne et al., 2007) and information extraction (Zou et al., 2014). Consider the following document as an example: Example 1: 中国机电产品进出口贸易继续增加 a，占总进出口的比重继续上升 b。其中，出口 五十七点九亿美元 c，占总出口的百分之三十二点五 d；进口八十五点二亿美元 e，占总进口的 百分之四十六点四 f，均比去年同期有所上升 g。The import and export trade of China’s mechanical and electronic products continues to increase a, and its proportion of the total imports and exports also continues to rise b. Among them, the exports amounted to 5.79 billion dollars c, accounting for 32.5 percent of the total exports d; and the imports of 8.52 billion dollars e, accounting for 46.4 percent of the total imports f; all of them were higher than those in the same"
C18-1045,P12-1007,0,0.135105,"to binary relations. There are two shortcomings of these approach, on the one hand, the number of non-original samples is added, and on the other hand, it is difficult to automatically build a complete real structure tree. We build the local models of structure identification and nuclearity recognition respectively. Our local models are implemented using CRFs. In this way, we are able to take into account the sequential information from contextual discourse units, which cannot be naturally represented with Support Vector Machine (SVM) or Maximum Entropy (ME) as local classifiers. As shown by Feng and Hirst (2012; 2014), for a pair of discourse units of interest, the sequential information from contextual units is crucial for determining structures. Therefore, it is well motivated to use CRF, which is a discriminative probabilistic graphical model, to make predictions for a sequence of units surrounding the pair of interest. Figure 3 shows our structure identification model Mstruct implemented with conditional random field algorithm. The first layer of the chain is composed of discourse units Uj ’s, and the second layer is composed of nodes of Sj ’s to indicate the probability of merging adjacent disc"
C18-1045,P14-1048,0,0.429441,"classifier is applied to assign the type of discourse relation between the chosen pair of constituents. Joty et al. (2013) approach the document-level discourse parsing using a model trained by Conditional Random Fields (CRF). They decomposed the problem of document-level discourse parsing into two stage: intra-sentential and multi-sentential parsing. Specifically, they employed two separate models for intra- and multi-sentential parsing. They jointly modeled the structure and the relation for a given pair of discourse units, such that information from each aspect can interact with the other. Feng and Hirst (2014) develop a much faster model whose time complexity is linear in the number of sentences. Their model adopts a greedy bottom-up approach, with two linear-chain CRFs applied as local classifiers. An approach of post-editing is performed, which modified a fully-built tree by considering information from upper-levels, to improve the accuracy. There is no relevant research on document-level discourse parsing in Chinese so far. For micro level discourse structure analysis, Li (2015) proposes a Connective-driven Dependency Tree (CDT) schema to represent the discourse rhetorical structure in Chinese l"
C18-1045,P14-1065,0,0.0504555,"Missing"
C18-1045,D12-1083,0,0.113864,"work. 2 Related Work Discourse parsing is the task of discovering the presence and type of the discourse relations between discourse units. The existing discourse parsing researches are mainly based on Rhetorical Structure Theory Discourse Treebank (RST-DT). The RST-DT (Carlson et al., 2003) is built in the framework of Rhetorical Structure Theory, consisting of 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) and representing over 176,000 words of text. While recent advances in sentence-level discourse parsing have attained accuracies close to human performance (Joty et al., 2012), discourse parsing at the document-level still poses challenges. The HILDA discourse parser (Hernault et al., 2010) is the first attempt at document-level discourse parsing on RST-DT. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom-up. In particular, at each step of the tree-building, a binary Support Vector Machine (SVM) classifier is applied to determine which pair of adjacent discourse constituents should be merged to form a larger span, and then another multi-class SVM classifier is applied to assign the type of discourse relation between the chosen"
C18-1045,P13-1048,0,0.506931,"the document-level still poses challenges. The HILDA discourse parser (Hernault et al., 2010) is the first attempt at document-level discourse parsing on RST-DT. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom-up. In particular, at each step of the tree-building, a binary Support Vector Machine (SVM) classifier is applied to determine which pair of adjacent discourse constituents should be merged to form a larger span, and then another multi-class SVM classifier is applied to assign the type of discourse relation between the chosen pair of constituents. Joty et al. (2013) approach the document-level discourse parsing using a model trained by Conditional Random Fields (CRF). They decomposed the problem of document-level discourse parsing into two stage: intra-sentential and multi-sentential parsing. Specifically, they employed two separate models for intra- and multi-sentential parsing. They jointly modeled the structure and the relation for a given pair of discourse units, such that information from each aspect can interact with the other. Feng and Hirst (2014) develop a much faster model whose time complexity is linear in the number of sentences. Their model"
C18-1045,P10-1113,1,0.75867,"etween Uj+1 and the topic of the discourse. Whether the similarity between Uj and the topic is greater than the similarity between Uj+1 and the topic. Table 2: Features used in local models. 541 Used in SI Used in NR Y Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y 6 Joint Learning with Integer Linear Programing While a pipeline model may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint syntactic parsing and semantic role labeling (Li et al., 2010), joint argument identification and role determination (Li et al., 2013), joint structure identification and relation recognition (Joty et al., 2012), etc. In our joint model, an ILP (Integer Logic Programming) -based inference framework is introduced to integrate two CRF-based local models, the structure identifier and the nuclearity recognizer. In this section, we propose a joint model of structure identification and nuclearity recognition with some intrainstance and contextual constraints. We assume pSI (s&lt;i,j&gt; |seqi ) the probability of Mstruct identifying s&lt;i,j&gt; as a structure of an seque"
C18-1045,D14-1224,1,0.832482,"formance has not yet achieve the level for application. However, the macro level research still stays in the theoretical research, and there is no available corpus resource, nor a corresponding computational model. For the two reasons mentioned above, in this paper, we take the macro discourse structure as the main research object, that is different from the previous research. We explore a macro discourse structure presentation schema to present the macro level discourse structure, and propose a Macro Chinese Discourse Treebank (MCDTB) on the top of existing Chinese Discourse Treebank (CDTB) (Li et al., 2014). On the basis of the presentation schema and annotated corpus, we divide the macro level discourse structure analysis into four tasks, including structure identification, nuclearity recognition, relation classification and discourse tree building. There are certain differences from the analysis of the micro level discourse structure. For example, macro discourse structure analysis takes paragraphs as the elementary discourse units, and the relations between the units are fairly loose, so it brings difficulties to the task of structure identification. Furthermore, there are virtually no connec"
C18-1045,J93-2004,0,0.0611808,"ibes the local models, and the joint approach we used with ILP is introduced in Section 6. Section 7 presents the experimental results. Section 8 gives the conclusion and future work. 2 Related Work Discourse parsing is the task of discovering the presence and type of the discourse relations between discourse units. The existing discourse parsing researches are mainly based on Rhetorical Structure Theory Discourse Treebank (RST-DT). The RST-DT (Carlson et al., 2003) is built in the framework of Rhetorical Structure Theory, consisting of 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) and representing over 176,000 words of text. While recent advances in sentence-level discourse parsing have attained accuracies close to human performance (Joty et al., 2012), discourse parsing at the document-level still poses challenges. The HILDA discourse parser (Hernault et al., 2010) is the first attempt at document-level discourse parsing on RST-DT. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom-up. In particular, at each step of the tree-building, a binary Support Vector Machine (SVM) classifier is applied to determine which pair of adjacent dis"
C18-1045,C04-1007,0,0.743973,"For macro level discourse analysis, the particles of the N-gram model are too small to represent the information of a paragraph, so the lexical features are not used in our tasks. 2) Syntactic information and dominance set features are very useful for micro level discourse analysis. However, the elementary units of the macro level are paragraphs, and these features are not applicable. 3) Since there is no tense in Chinese, we cannot use temporal features in macro level structure analysis. Due to the appearance of word vector representation (Mikolov et al., 2013), the methods of cooccurrence (Sporleder and Lascarides, 2004) and word pairs (Feng and Hirst, 2012) are not necessary when the semantic similarity is calculated. We use the word2vec model to train word vector representation on the CTB 8.0, and use the method proposed by Jiang et al. (2018) to calculate the semantic similarity (including the semantic similarity between adjacent discourse units and the similarity between the discourse unit and the topic). In particular, in order to prevent the sparsity of the features, we discretize the semantic similarity into 10 levels. Features Organization features The beginning and end location of Uj . Distances of U"
C18-1045,C02-1145,0,0.074799,"relations. Specifically, the edges with arrows point to the “Nucleus” units, and the edges without arrows point to the “Satellite” units. 3.2 Corpus Annotating Guided by the macro discourse structure framework defined in Section 3.1, we have carried out annotating work of macro Chinese discourse structure, which we call Macro Chinese Discourse Treebank (MCDTB)1 . In the process of annotating, the structure definition and annotating criteria are modified iteratively. After nearly a year of annotation, 720 news wire articles are annotated, the source of which is Chinese Treebank 8.0 (CTB 8.0). (Xue et al., 2002; Xue et al., 2013) Because the discourse units are not isolated from the overall discourse, it’s difficult to judge whether the discourse units are important or not and what relations are between the discourse units simply from the units themselves. It is necessary to have a comprehensive understanding of the overall article before the annotating. We divide the annotating work into three main stages. The first stage lasted four months, with three annotators participating. We selected the first 50 news articles from CTB 8.0, and annotated them together. After a lot of discussions, a preliminar"
C18-1203,W11-1701,0,0.376929,"on of documents and the corresponding feature sets. • We make a systematic comparison of LSTM with different linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic features for stance detection. For example, Somasundaran and Wiebe (2010) developed a baseline for stance detection by modeling verbs and"
C18-1203,D16-1084,0,0.368152,"Missing"
C18-1203,P11-1151,0,0.193774,"erarchical attention based neural model tailored to stance detection task, which considers the mutual influence between the representation of documents and the corresponding feature sets. • We make a systematic comparison of LSTM with different linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic feat"
C18-1203,C16-1154,0,0.301653,"t and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and thus, support the advancement of the human race. (Target: Gay Rights) dobj nmod:of Arguments pairs: (adopt,they),(adopt,children),(support,advancement),(advancement,race) However, besides the word sequence, stance is correlated with some explicit and implicit linguistic factors. Take E1 for example, both sentimental words (i.e., understand, support) and argument sentence (i.e., They can adopt ..."
C18-1203,P15-1033,0,0.0132835,"is employed to adjust the weight of different feature sets. In the following subsections, we firstly show the representation of a document, and then show the representation of different linguistic features. Finally, we illustrate hierarchical attention model, and training process. 3.1 Document Representation We use a standard Long Short-Term Memory (LSTM) model (Hochreiter and Schmidhuber, 1997) to learn the document representation. LSTM has been proven to be effective in many natural language processing (NLP) tasks such as machine translation (Sutskever et al., 2014) and dependency parsing (Dyer et al., 2015), and it is adept in harnessing long sentences. Let X = (w1 , w2 , ..., wn ) be a document, where n is the document length and wi is the i-th token. We transform each token wi into a real-valued vector xi using the word embedding vector of wi , obtained by looking up a pre-trained word embedding 2401 table D. We use the skip-gram algorithm to train embeddings (Mikolov et al., 2013). The LSTM is used over X to generate a hidden vector sequence (h1 , h2 , ..., hn ). At each step t, the hidden vector ht of LSTM model is computed based on the current vector xt and the previous vector ht−1 , and ht"
C18-1203,D14-1083,0,0.242867,"tection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic features for stance detection. For example, Somasundaran and Wiebe (2010) developed a baseline for stance detection by modeling verbs and sentiments. Anand et al. (2011) augmented the n-gram features with lexicon-based and dependency-based features. Except the above transitional linguistic factors, Hasan and Ng (2014) considered the importance of argument for stance detection, and explored the relations between stance and argument, and proposed a pipeline system to predict the stance and extract argument sentence from documents jointly. 2.2 Structure Features Although linguistic features show effectiveness for stance detection in many works, stance detection has been newly posed as collective classification task with extra structure information. For example, citation structure (Burfoot et al., 2011) or rebuttal links (Walker et al., 2012), was used as extra information to model agreements or disagreements"
C18-1203,S16-1003,0,0.530544,"ent linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic features for stance detection. For example, Somasundaran and Wiebe (2010) developed a baseline for stance detection by modeling verbs and sentiments. Anand et al. (2011) augmented the n-gram features with lexicon-based and dependency-based featu"
C18-1203,C10-2100,0,0.0887726,"ence from documents jointly. 2.2 Structure Features Although linguistic features show effectiveness for stance detection in many works, stance detection has been newly posed as collective classification task with extra structure information. For example, citation structure (Burfoot et al., 2011) or rebuttal links (Walker et al., 2012), was used as extra information to model agreements or disagreements in debate posts and to infer their labels. Moreover, since similar users should express a similar opinion toward the same topic, user information is always used in stance detection. For example, Murakami and Raymond (2010) proposed a maximum cut method to aggregate stances in multiple posts to infer a user’s stance on the target. Rajadesingan and Liu (2014) used a retweet-based label propagation method, which started from a set of opinionated users and labeled tweets by the people who are in the retweet network. Sridhar et al. (2015) utilized Probabilistic Soft Logic (PSL) to collectively classify the stance of users and stance in posts. 2400 q Hyper Attention q2 q1 Attention H q3 Attention H(sent) X X(sent) Document Representation Sentiment Representation H(argument) H(dep) LSTM LSTM LSTM Attention X(dep) Depe"
C18-1203,P16-1205,0,0.0134273,"but use attention mechanism to measure the correlation between document representation and sentiment information. Particularly, we simply extract the sentimental word sequence X (sent) = {xs1 , xs2 , ..., xsm } of each document from sentiment lexicon, where xsi means sentiment word. We then learn the representation of sentiment information through this sentimental word sequence using LSTM model, and use H (sent) = LST M (xsm , hm−1 ) as sentiment representation. Dependency Representation The dependency-based features can be utilized to capture the inter-word relationships (Anand et al., 2011; Persing and Ng, 2016). The dependency structure involves some stance-taking text related to the given target. As in E4, we notice that the important words that express a stance in the sentence are “murder”, “never” and “necessity”. By analyzing the dependency structure in this and other prompt parts, we can extract the stance-taking dependency information we identify as “murder-never-necessity”. nsubj cop neg det E4: Murder is never a necessity. NN VBZ RB DT NN Hence, we extract the pair of arguments sequence X (dep) = {xd1 , xd2 , ..., xdm } = {x1 ⊕ x3 , ..., xi ⊕ xj } involved in each dependency relation from a"
C18-1203,W10-0214,0,0.595108,"e polarity expressed in a customer review (Pang et al., 2008; Liu, 2012), researchers have begun exploring new opinion mining tasks in recent years. One such task is stance detection: given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for and against) its author is taking. Stance detection system can potentially have a positive social impact and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and"
C18-1203,P15-1012,0,0.487386,"ang et al., 2008; Liu, 2012), researchers have begun exploring new opinion mining tasks in recent years. One such task is stance detection: given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for and against) its author is taking. Stance detection system can potentially have a positive social impact and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and thus, support the advancement of the"
C18-1203,W06-1639,0,0.309099,"We present a novel hierarchical attention based neural model tailored to stance detection task, which considers the mutual influence between the representation of documents and the corresponding feature sets. • We make a systematic comparison of LSTM with different linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various k"
C18-1203,N12-1072,0,0.208958,"ncy-based features. Except the above transitional linguistic factors, Hasan and Ng (2014) considered the importance of argument for stance detection, and explored the relations between stance and argument, and proposed a pipeline system to predict the stance and extract argument sentence from documents jointly. 2.2 Structure Features Although linguistic features show effectiveness for stance detection in many works, stance detection has been newly posed as collective classification task with extra structure information. For example, citation structure (Burfoot et al., 2011) or rebuttal links (Walker et al., 2012), was used as extra information to model agreements or disagreements in debate posts and to infer their labels. Moreover, since similar users should express a similar opinion toward the same topic, user information is always used in stance detection. For example, Murakami and Raymond (2010) proposed a maximum cut method to aggregate stances in multiple posts to infer a user’s stance on the target. Rajadesingan and Liu (2014) used a retweet-based label propagation method, which started from a set of opinionated users and labeled tweets by the people who are in the retweet network. Sridhar et al"
C18-1203,W05-0308,0,0.0108991,"xd2 , ..., xdm } = {x1 ⊕ x3 , ..., xi ⊕ xj } involved in each dependency relation from a dependency parser as a feature, where xdi = xj ⊕ xk is arguments pair from dependency relation. We then learn the representation of dependency information through the dependency sequence X (dep) using LSTM model, and we use H (dep) = LST M (xdm , hm−1 ) as dependency representation. Argument Representation In general, there are not only the stance of the author toward the target in a post, but also personal beliefs about what is true or what action should be taken. This personal belief is called argument (Wilson and Wiebe, 2005). There exist some arguments behind the stance people express on a specified target. If we can extract the argument sentence from the post, then it will be beneficial to detect the author’s stance. 2402 As in E5, with the help of argument sentence with bold, it will be easy to detect that the author support the legalization of marijuana. E5: Most issues like this, such as sex between minors and alcohol, come down to one thing: it’s your choice. If you want to ruin your life, be my guest. It isn’t the government’s job to control that. (Target: Legalization of marijuana) In order to utilize argu"
C18-1203,S16-1074,0,0.306704,"en a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for and against) its author is taking. Stance detection system can potentially have a positive social impact and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and thus, support the advancement of the human race. (Target: Gay Rights) dobj nmod:of Arguments pairs: (adopt,they),(adopt,children),(support,advancement),(advancement,race) However, be"
C18-1296,D15-1045,0,0.271598,"Missing"
C18-1296,H91-1060,0,0.133876,"f corpus annotation. For evaluating the consistency of the discourse structure tree, tree consistency can more objectively reflect the quality of annotation. CDTB and PDTB only evaluated the consistency of certain indicators, such as discourse relation, and did not evaluate the tree consistency. RST-DT proposed a method of evaluating the complete consistency of a discourse tree, but it also had several shortcomings. The evaluation objects of RST-DT are the discourse structure, nuclearity and relationship annotated on the children nodes, while the evaluation object of the standard syntax tree (Black et al., 1991) is the annotations on the parent node. In this way, RST-DT will be at least double the standard syntax tree evaluation of the sample, and 3498 because of the mutual constraint relationship between children nodes, the consistency of corpus annotation of RST-DT is objectively higher. In the annotation of a macro discourse relationship, the leaf nodes are natural paragraphs and do not need to be manually divided. Therefore, unlike the assessment method of the RST-DT agreement, we do not use the leaf nodes as an example of a consistent assessment. In addition, we adopted the standard syntax tree"
C18-1296,P12-1007,0,0.0292046,"most critical step in the related tasks of discourse analysis. We use a Conditional Random Field (CRF) model to experiment with the parameter C of 4, the feature window of 3, and the rest of the parameters as a default value. There were 8,863 samples in total, including 3,261 positive samples and 5,602 negative samples. Because of the language differences between English and Chinese, as well as the microscopic and macroscopic differences in features, such as the absence of syntax trees and the absence of tense features, we select only several of the structural features of the previous study (Feng and Hirst, 2012) as our Feature Set 1 as follows:  The position of the beginning and end of a discourse unit;  The number of sentences and the number of paragraphs contained in a discourse unit;  The comparison of the number of sentences in the discourse unit to the previous unit;  The comparison of the number of paragraphs in the discourse unit to the previous unit. 3501 Considering the process of constructing discourse structure trees, we may need some procedural characteristics. We regard whether the discourse unit is a leaf node or whether the discourse unit is merged in the previous round as organiza"
C18-1296,P14-1048,0,0.0603068,"Missing"
C18-1296,D14-1224,1,0.78642,"style to annotate 164 Chinese documents from CTB (Xue, 2005; Xue et al., 2005). The experiment demonstrated the transferability of the English discourse analysis theory in Chinese language. Lv et al. (2015) constructed a Chinese corpus of 496 news articles from People&apos;s Daily under the framework of the Chinese Framing Network (CFN) (Hao et al., 2007). Each news article annotated the discourse frame, structure and relationship. However, the discourse units are also smaller with a minimum of one sentence and a maximum of five sentences, and their research object is primarily at the word level. Li et al. (2014) integrated RST-DT and PDTB and used the representation method based on the connection dependence tree to annotate the Chinese Dependency Treebank (CDTB) containing 500 Chinese Xinhua news articles. However, this method only considered the discourse relationship annotation inside the paragraph and did not annotate the discourse relationship between paragraphs and the macro discourse information of the whole text. Compared with the well-studied micro discourse analysis, macro discourse analysis is in the trial stage both in English and Chinese. In English, Sporleder and Lascarides (2004) attemp"
C18-1296,D13-1070,0,0.423657,"Missing"
C18-1296,W99-0307,0,0.233266,"ded into micro and macro discourse analysis. The theory of micro discourse primarily includes Hobbs model (Hobbs, 1985), rhetorical structure theory (RST) (Mann and Thompson, 1987; Mann et al., 1992), sentence group theory (Wu and Tian, 2000) and complex sentence theory (Fu, 2001), and the macro discourse theory has macro hyper-theme theory (Martin and Rose, 3494 2003) and macro structure theory (Van Dijk, 1980). Under the guidance of these theories, various corpora have been developed accordingly. In micro discourse analysis, the most popular corpora are RST-DT and PDTB. Based on RST, RSTDT (Marcu et al., 1999; Carlson et al., 2003) contained 385 articles from the Wall Street Journal annotated by 16 categories and 78 classes&apos; rhetorical relations, to represent the relationship between two or more discourse units. Most of the elements of discourse units in RST-DT are phrases including partial clauses. To ensure the universality of the corpus, PDTB (Parsad et al., 2008) uses the LTAG (Lexicalized Tree-Adjoining Grammars) theory, and the annotation is entirely based on lexicalization. This approach primarily annotated the argument structure, including the conjunction and the semantic distinction infor"
C18-1296,prasad-etal-2008-penn,0,0.218524,"Missing"
C18-1296,C04-1007,0,0.632026,"ly at the word level. Li et al. (2014) integrated RST-DT and PDTB and used the representation method based on the connection dependence tree to annotate the Chinese Dependency Treebank (CDTB) containing 500 Chinese Xinhua news articles. However, this method only considered the discourse relationship annotation inside the paragraph and did not annotate the discourse relationship between paragraphs and the macro discourse information of the whole text. Compared with the well-studied micro discourse analysis, macro discourse analysis is in the trial stage both in English and Chinese. In English, Sporleder and Lascarides (2004) attempted to perform an experiment on macro discourse analysis on RST-DT. However, RST-DT focuses on the micro discourse level, which is only tagging the relations on the sentence level and ignoring the relations on the paragraph level. Thus, it leads to some tailoring and correction when Sporleder carries out macro discourse analysis on the paragraph level. In Chinese, Chu et al. (2017) carried out relevant research on the nuclearity of macro discourse and made corresponding attempts to construct a macro discourse corpus. In the field of Chinese discourse annotation, the number of relevant c"
C18-1296,P17-2029,0,0.0505954,"Missing"
C18-1296,W05-0312,0,0.025408,"e corpus, PDTB (Parsad et al., 2008) uses the LTAG (Lexicalized Tree-Adjoining Grammars) theory, and the annotation is entirely based on lexicalization. This approach primarily annotated the argument structure, including the conjunction and the semantic distinction information. The basic unit of argument is the event or state, and the specific form is the clause or sentence. The corpus is relatively large with 2,159 articles and approximately 1 million words. Regarding Chinese micro discourse corpus, Zhou et al. (2015) used the PDTB annotation style to annotate 164 Chinese documents from CTB (Xue, 2005; Xue et al., 2005). The experiment demonstrated the transferability of the English discourse analysis theory in Chinese language. Lv et al. (2015) constructed a Chinese corpus of 496 news articles from People&apos;s Daily under the framework of the Chinese Framing Network (CFN) (Hao et al., 2007). Each news article annotated the discourse frame, structure and relationship. However, the discourse units are also smaller with a minimum of one sentence and a maximum of five sentences, and their research object is primarily at the word level. Li et al. (2014) integrated RST-DT and PDTB and used the rep"
C18-1296,P14-1049,1,0.900527,"Missing"
D07-1076,P05-1053,1,0.924919,"” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (person name) and “Microsoft Corporation” (organization name). Extraction of semantic relations between entities can be very useful in many applic ations such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States”via his relationship with “the United States”. Many researches have been done in relation extraction. Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entityrelated information to syntactic parse trees, dependency trees and semantic information. However, it is difficult for them to effectively capture structured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction. As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implicitly structured features by directly computing the similarity between two trees. Alth"
D07-1076,P06-1016,1,0.776606,"Missing"
D07-1076,H05-1091,0,0.866971,"Missing"
D07-1076,P04-1054,0,\N,Missing
D07-1076,P05-1052,0,\N,Missing
D07-1076,P06-1104,1,\N,Missing
D07-1076,P01-1017,0,\N,Missing
D09-1103,P95-1017,0,0.301061,"Missing"
D09-1103,W99-0629,0,0.047712,"Missing"
D09-1103,C88-1021,0,0.134457,"nce resolution is to determine the antecedent for each referring expression in a text. The ability of linking referring expressions both within a sentence and across the sentences in a text is critical * to discourse and language understanding in general. For example, coreference resolution is a key task in information extraction, machine translation, text summarization, and question answering. There is a long tradition of research on coreference resolution within computational linguistics. While earlier knowledge-lean approaches heavily depend on domain and linguistic knowledge (Carter 1987; Carbonell and Brown 1988) and have significantly influenced the research, the later approaches usually rely on diverse lexical, syntactic and semantic properties of referring expressions (Soon et al., 2001;Ng and Cardie, 2002; Zhou et al., 2004). Current research has been focusing on exploiting semantic information in coreference resolution. For example, Yang et al (2005) proposed a template-based statistical approach to compute the semantic compatibility between a pronominal anaphor and an antecedent candidate, and Yang and Su (2007) explored semantic relatedness information from automatically discovered patterns, wh"
D09-1103,J95-2003,0,0.493741,"(in a left-to-right order) by mapping each pronoun into 5 levels: a) rank 1 for pronouns with semantic role ARG0/agent of the main predicate; b) rank 2 for pronouns with semantic role ARG1/patient of the main predicate; c) rank 3 for pronouns with semantic role ARG0/agent of other predicates; d) rank 4 for pronouns with semantic role ARG1/patient of other predicates; e) rank 5 for remaining pronouns. Furthermore, for those pronouns with the same ranking level, they are ordered according to their surface positions in a left-toright order, motivated by previous research on the centering theory (Grosz et al. 1995). 3) Detailed pronominal subcategory features. Given a pronominal expression, its detailed pronominal subcategory features, such as whether it is a first person pronoun, second person pronoun, third person pronoun, neuter pronoun or others, are explored to enhance the discriminative power of both the semantic role features and the relative pronominal ranking feature, considering the predominant importance of pronouns in tracking the local focus structure in discourse. 4.3 Comparison with Previous Work As a representative in explicitly employing semantic role labeling in coreference resolution,"
D09-1103,P98-2143,0,0.0606747,"Missing"
D09-1103,P02-1014,0,0.333918,"discourse and language understanding in general. For example, coreference resolution is a key task in information extraction, machine translation, text summarization, and question answering. There is a long tradition of research on coreference resolution within computational linguistics. While earlier knowledge-lean approaches heavily depend on domain and linguistic knowledge (Carter 1987; Carbonell and Brown 1988) and have significantly influenced the research, the later approaches usually rely on diverse lexical, syntactic and semantic properties of referring expressions (Soon et al., 2001;Ng and Cardie, 2002; Zhou et al., 2004). Current research has been focusing on exploiting semantic information in coreference resolution. For example, Yang et al (2005) proposed a template-based statistical approach to compute the semantic compatibility between a pronominal anaphor and an antecedent candidate, and Yang and Su (2007) explored semantic relatedness information from automatically discovered patterns, while Ng (2007) automatically induced semantic class knowledge from a treebank and explored its application in coreference resolution. Particularly, this paper focuses on the centering theory (Sidner,19"
D09-1103,J05-1004,0,0.0929658,"assigns them appropriate semantic roles. During the last few years, there has been growing interest in SRL. For example, CoNLL 2004 and 2005 have made this problem a well-known shared task. However, there is still little consensus in the linguistic and NLP communities about what set of semantic role labels are most appropriate. Typical semantic roles include core roles, such as agent, patient, instrument, and adjunct roles (such as locative, temporal, manner, and cause). For core roles, only agent and patient are consistently defined across different predicates, e.g. in the popular PropBank (Palmer et al. 2005) and the derived version evaluated in the CoNLL 2004 and 2005 shared tasks, as ARG0 and ARG1. In this paper, we extend the centering theory from the grammatical level to the semantic level for its better application in pronoun resolution via proper semantic role features due to three reasons: Sentence Grammatical Role Semantic Role Bob opened the Bob: Bob: door with a key. SUBJECT AGENT The key opened The key: The key : the door. SUBJECT INSTRUMENT The door opened. The door: The door: SUBJECT PATIENT Table 3: Relationship between grammatical roles and semantic roles: an example 1) Semantic rol"
D09-1103,E06-2015,0,0.209078,"3) Detailed pronominal subcategory features. Given a pronominal expression, its detailed pronominal subcategory features, such as whether it is a first person pronoun, second person pronoun, third person pronoun, neuter pronoun or others, are explored to enhance the discriminative power of both the semantic role features and the relative pronominal ranking feature, considering the predominant importance of pronouns in tracking the local focus structure in discourse. 4.3 Comparison with Previous Work As a representative in explicitly employing semantic role labeling in coreference resolution, Ponzetto and Strube (2006) explored two semantic role features to capture the predicateargument structure information to benefit coreference resolution: I_SEMROLE, the predicate-argument pairs of one referring expression, and J_SEMROLE, the predicate-argument pairs of another referring expression. Their experiments on the ACE 2003 corpus shows that, while the two semantic role features much improve the performance of common noun resolution by 3.8 and 2.7 in F-measure on the BNEWS and NWIRE domains respectively, they only 1 According to the centering theory, the backward-looking center Cb is preferentially realized by a"
D09-1103,A88-1003,0,0.376098,"Missing"
D09-1103,D07-1002,0,0.0750626,"Missing"
D09-1103,J81-4001,0,0.389356,"Missing"
D09-1103,P04-1017,1,0.920801,"Missing"
D09-1103,P05-1021,0,0.262157,"Missing"
D09-1103,P07-1067,0,0.0948564,"an approaches heavily depend on domain and linguistic knowledge (Carter 1987; Carbonell and Brown 1988) and have significantly influenced the research, the later approaches usually rely on diverse lexical, syntactic and semantic properties of referring expressions (Soon et al., 2001;Ng and Cardie, 2002; Zhou et al., 2004). Current research has been focusing on exploiting semantic information in coreference resolution. For example, Yang et al (2005) proposed a template-based statistical approach to compute the semantic compatibility between a pronominal anaphor and an antecedent candidate, and Yang and Su (2007) explored semantic relatedness information from automatically discovered patterns, while Ng (2007) automatically induced semantic class knowledge from a treebank and explored its application in coreference resolution. Particularly, this paper focuses on the centering theory (Sidner,1981;Grosz et al.,1995; Tetreault,2001), which reveals the significant impact of the local focus on referring expressions in that the antecedent of a referring expression usually depends on the center of attention throughout the local discourse segment (Mitkov,1998). Although the centering theory has been considered"
D09-1103,C04-1075,1,0.920198,"Missing"
D09-1103,A00-1020,0,\N,Missing
D09-1103,P99-1079,0,\N,Missing
D09-1103,P08-2029,0,\N,Missing
D09-1103,J01-4003,0,\N,Missing
D09-1103,P87-1022,0,\N,Missing
D09-1103,J01-4004,0,\N,Missing
D09-1103,P86-1031,0,\N,Missing
D09-1103,C98-2138,0,\N,Missing
D09-1103,P07-1068,0,\N,Missing
D09-1103,J01-4007,0,\N,Missing
D09-1133,P98-1013,0,0.0167375,"n and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse t"
D09-1133,W04-2412,0,0.0205686,"lution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and"
D09-1133,W05-0620,0,0.21267,"Missing"
D09-1133,D08-1034,0,0.0979658,"Missing"
D09-1133,W06-1617,0,0.63749,"L could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X"
D09-1133,P07-1027,0,0.0832518,"Missing"
D09-1133,meyers-etal-2004-annotating,0,0.0395649,"s and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small priv"
D09-1133,P04-1043,0,0.12312,"arsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. In order to handle multi-classification 1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ POSs are not counted in evaluating the performance of word-based syntactic parser, but they are counted in evaluating the performance of character-based parser. Therefore the F1-measure for the later is higher than that for the former. 3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 2 1285 problem in argument classification, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For argument identification an"
D09-1133,C04-1100,0,0.0221351,"nominal SRL system much outperforms the state-of-the-art ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), P"
D09-1133,W04-3236,0,0.0211059,"les (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007) 1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. In order to handle mul"
D09-1133,J05-1004,0,0.108985,"tion extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden"
D09-1133,N07-1051,0,0.0314267,"ese Penn TreeBank 5.1. Following the experimental setting in Xue (2008), 648 files (chtb_081 to 899.fid) are selected as the training data, 72 files (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007) 1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmen"
D09-1133,E06-2015,0,0.310819,"vided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X"
D09-1133,N04-4036,0,0.0189779,"& 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended the"
D09-1133,N04-1032,0,0.0110768,"(Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively"
D09-1133,P03-1002,0,0.0353596,"ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecuti"
D09-1133,W03-1707,0,0.0158673,"similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods"
D09-1133,xue-2006-annotating,0,0.493591,"a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"
D09-1133,N06-1055,0,0.353081,"a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"
D09-1133,J08-2004,0,0.609138,"arious nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP por"
D09-1133,W08-2121,0,\N,Missing
D09-1133,C98-1013,0,\N,Missing
D09-1149,J03-4003,0,0.0199954,"tances using 5-fold cross-validation. That is, these relation instances are first divided into 5 sets, then, one of them (about 860 instances) is used as the test data set, while the others are regarded as the training data set, from which the initial seed set is sampled. In the ACE 2003 corpus, the training set consists of 674 documents and 9683 positive relation instances while the test data consists of 97 documents and 1386 positive relation instances. The ACE RDC 2003 task defines 5 relation types and 24 subtypes between 5 entity types. The corpora are first parsed using Collins’s parser (Collins, 2003) with the boundaries of all the entity mentions kept. Then, the parse trees are converted into chunklink format using chunklink.pl 1. Finally, various useful lexical and syntactic features, as described in Subsection 3.1, are extracted and computed accordingly. For the purpose of comparison, we define our task as the classification of the 5 or 7 major relation types in the ACE RDC 2003 and 2004 corpora. For LIBSVM parameters, we adopted the polynomial kernel, and c is set to 10, g is set to 0.15. Under this setting, we achieved the best classification performance. 5.2 Experimental Results In t"
D09-1149,P06-1017,0,0.549966,"IST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation. Therefore, semi-supervised learning has rece"
D09-1149,P04-1054,0,0.0956219,"Missing"
D09-1149,P04-1053,0,0.848952,"ity between patterns and relations to augment the target relation starting from a small sample. However, it only extracts simple relations such as (author, title) pairs from the WWW. Snowball (Agichtein and Gravano, 2000) is another bootstrappingbased system that extracts relations from unstructured text. Snowball shares much in common with DIPRE, including the use of both the bootstrapping framework and the pattern matching approach to extract new unlabeled instances. Due to pattern matching techniques, their systems are hard to be adapted to the general problem of relation extraction. Zhang (2004) approaches the relation classification problem with bootstrapping on top of SVM. He uses various lexical and syntactic features in the BootProject algorithm based on random feature projection to extract top-level relation types in the ACE corpus. Evaluation shows that bootstrapping can alleviate the burden of hand annotations for supervised learning methods to a certain extent. Chen et al. (2006) investigate a semisupervised learning algorithm based on label propagation for relation extraction, where labeled and unlabeled examples and their distances are represented as the nodes and the weigh"
D09-1149,A00-2030,0,0.0443339,"E) is such a technology that IE systems are expected to identify relevant information (usually of pre-defined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast,"
D09-1149,C08-1088,1,0.938606,"7 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437–1445, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 100 instances), the imbalance of relation types or manifold structure (cluster structure) in it will severely weaken the strength of bootstrapping. Therefore, it is critical for a bootstrapping approach to select the most appropriate initial seed set. However, current systems (Zhang, 2004; Chen et al., 2006) use a randomly sampling strategy, which fails to explore the affinity nature among the training instances. Alternatively, Zhou et al. (2008) bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM. Nevertheless, the initial labeled data is still randomly generated only to ensure that there are at least 5 instances for every relation subtype. This paper presents a new approach to selecting the initial seed set based on stratified sampling strategy in the bootstrapping procedure for semi-supervised semantic relation classification. The motivation behind the stratified sampling is that every relation type should be as much as possible represented in the initial seed set, thus leading to more instanc"
D09-1149,P95-1026,0,0.10208,"ere are no criteria for evaluation. Therefore, semi-supervised learning has received more and more attention, as it can balance the advantages and disadvantages between supervised and unsupervised methods. With the plenitude of unlabeled natural language data at hand, semi-supervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance. Specifically, a bootstrapping algorithm chooses the unlabeled instances with the highest probability of being correctly labeled and use them to augment labeled training data iteratively. Although previous work (Yarowsky, 1995; Blum and Mitchell, 1998; Abney, 2000; Zhang, 2004) has tackled the bootstrapping approach from both the theoretical and practical point of view, many key problems still remain unresolved, such as the selection of initial seed set. Since the size of the initial seed set is usually small (e.g. 1437 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437–1445, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 100 instances), the imbalance of relation types or manifold structure (cluster structure) in it will severely weaken the strength of bootstrapping"
D09-1149,W02-1010,0,0.0615167,"Missing"
D09-1149,P06-1104,1,0.85742,"ned types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult"
D09-1149,I05-1034,1,0.893397,"as three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation. Therefore, semi-supervised learning has received more and more attention, as it can balance the advantages and disadvantages between s"
D09-1149,P05-1053,1,0.94391,"usually of pre-defined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data,"
D09-1149,I08-1005,1,0.69481,"nt Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation. Therefore, semi-supervised learning has received more and more a"
D09-1149,P02-1046,0,\N,Missing
D10-1070,W05-0620,0,0.319338,"n evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance. Alternatively, even if the rule-based methods may be effective for a special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning). Instead, this paper explores scope learning from parse tree perspective and formulates it as a simplified shallow semantic parsing problem, which has been extensively studied in the past few years (Carreras and Màrquez, 2005). In particular, the cue is recast as the predicate and the scope is recast as the arguments of the cue. The motivation behind is that the structured syntactic information plays a critical role in scope learning and should be paid much more attention, as indicated by previous studies in shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005). Although our approach is evaluated only on negation and speculation scope learning here, it is portable to other kinds of scope learning. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 introdu"
D10-1070,P02-1031,0,0.0103291,"other scope learning tasks (e.g., negation scope learning). Instead, this paper explores scope learning from parse tree perspective and formulates it as a simplified shallow semantic parsing problem, which has been extensively studied in the past few years (Carreras and Màrquez, 2005). In particular, the cue is recast as the predicate and the scope is recast as the arguments of the cue. The motivation behind is that the structured syntactic information plays a critical role in scope learning and should be paid much more attention, as indicated by previous studies in shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005). Although our approach is evaluated only on negation and speculation scope learning here, it is portable to other kinds of scope learning. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 introduces the Bioscope corpus on which our approach is evaluated. Section 4 describes our parsing approach by formulating scope learning as a simplified shallow semantic parsing problem. Section 5 presents the experimental results. Finally, Section 6 concludes the work. 715 2 Related Work Most of the previous research on scope learning falls"
D10-1070,W06-1617,0,0.0962286,"Missing"
D10-1070,W02-1006,0,0.0211184,"ght et al., 2004); 2) machine learning approaches, which train a classifier with either supervised or semisupervised learning methods (e.g., Özgür and Radev, 2009; Szarvas, 2008). Without loss of generality, we adopt a machine learning approach and train a classifier with supervised learning. In particular, we make an independent classification for each word with a BIO label to indicate whether it 719 is the first word of a cue, inside a cue, or outside of it, respectively. Inspired by previous studies on similar tasks such as WSD and nominal predicate recognition in shallow semantic parsing (Lee and Ng, 2002; Li et al., 2009), where various features on the word itself, surrounding words and syntactic information have been successfully used, we believe that such information is also valuable to automatic recognition of cues. Table 3 shows the features employed for cue recognition. In particular, we categorize these features into three groups: 1) features about the cue candidate itself (CC in short); 2) features about surrounding words (SW in short); and 3) structural features derived from the syntactic parse tree (SF in short). Feature Remarks Cue Candidate (CC) related CC1 The cue candidate itself"
D10-1070,P07-1125,0,0.183215,"’2010 shared task (Farkas et al., 2010) aims to detect uncertain information in resolving the scopes of speculation cues. Most of the initial research in this literature focused on either recognizing negated terms or identifying speculative sentences, using some heuristic Corresponding author 714 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714–724, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics rules (Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree persp"
D10-1070,C10-1155,0,0.068977,"(Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively. Although the chunking approach has been evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance. Alternatively, even if the rule-based methods may be effective for a special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning). Instead, this paper explores sco"
D10-1070,D09-1145,0,0.37542,"tational Linguistics rules (Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively. Although the chunking approach has been evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance. Alternatively, even if the rule-based methods may be effective for a special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning). Instea"
D10-1070,N07-1051,0,0.0292449,"thus share some common characteristics in statistics, such as the number of words in the negation/speculation scope to the right (or left) of the negation/speculation cue and the average scope length. In comparison, the clinical reports subcorpus consists of clinical radiology reports with short sentences. For detailed statistics and annotation 1 http://www.inf.u-szeged.hu/rgai/bioscope 716 guidelines about the three subcorpora, please see Morante and Daelemans (2009a & 2009b). For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) 2 trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al., 2005) 3 , which is a bracketed corpus in (almost) PTB style. 10-fold cross-validation on GTB1.0 shows that the parser achieves the performance of 86.57 in F1-measure. It is worth noting that the GTB1.0 corpus includes all the sentences in the abstracts subcorpus of the Bioscope corpus. 4 Scope Learning via Simplified Shallow Semantic Parsing In this section, we first formulate the scope learning task as a simplified shallow semantic parsing problem. Then, we deal with it using a simplified shallow semantic parsing framework. 4.1 Formu"
D10-1070,W05-0639,0,0.0529931,"Missing"
D10-1070,P08-1033,0,0.0746707,"boundary. k k * = arg max ∏ Pi ∗ k i =1 m ∏ (1 − Pi ) i = k +1 Similarly, the right boundary of the given cue can be decided. 4.5 Cue Recognition Automatic recognition of cues of a special interest is the prerequisite for a scope learning system. The approaches to recognizing cues of a special interest usually fall into two categories: 1) substring matching approaches, which require a set of cue words or phrases in advance (e.g., Light et al., 2004); 2) machine learning approaches, which train a classifier with either supervised or semisupervised learning methods (e.g., Özgür and Radev, 2009; Szarvas, 2008). Without loss of generality, we adopt a machine learning approach and train a classifier with supervised learning. In particular, we make an independent classification for each word with a BIO label to indicate whether it 719 is the first word of a cue, inside a cue, or outside of it, respectively. Inspired by previous studies on similar tasks such as WSD and nominal predicate recognition in shallow semantic parsing (Lee and Ng, 2002; Li et al., 2009), where various features on the word itself, surrounding words and syntactic information have been successfully used, we believe that such infor"
D10-1070,W08-0606,0,0.20885,"of the initial research in this literature focused on either recognizing negated terms or identifying speculative sentences, using some heuristic Corresponding author 714 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714–724, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics rules (Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively. Although the chunking approach has been evaluated on negation and speculation scope learning and c"
D10-1070,P02-1053,0,0.00400064,"Missing"
D10-1070,W04-3212,0,0.0195919,"ic label, scope identification does not involve semantic label classification and thus could be divided into three consequent phases: argument pruning, argument identification and postprocessing. 4.2 Argument Pruning Similar to the predicate-argument structures in common shallow semantic parsing, the cue-scope structures in scope learning can be also classified into several certain types and argument pruning can be done by employing several heuristic rules accordingly to filter out constituents, which are most likely non-arguments of a given cue. Similar to the heuristic algorithm proposed in Xue and Palmer (2004) for argument pruning in common shallow semantic parsing, the argument pruning algorithm adopted here starts from designating the cue node as the current node and collects its siblings. It then iteratively moves one level up to the parent of the current node and collects its siblings. The algorithm ends when it reaches the root of the parse tree. To sum up, except the cue node itself and its ancestral constituents, any constituent in the parse tree whose parent covers the given cue will be collected as argument candidates. Taking the negation cue node “RB7,7” in Figure 2 as an example, constit"
D10-1070,I05-2038,0,\N,Missing
D10-1070,D08-1075,0,\N,Missing
D10-1070,J08-2004,0,\N,Missing
D10-1070,W09-1105,0,\N,Missing
D10-1070,W10-3001,0,\N,Missing
D10-1070,D09-1133,1,\N,Missing
D10-1070,E99-1043,0,\N,Missing
D10-1070,W09-1304,0,\N,Missing
D10-1070,W04-3103,0,\N,Missing
D12-1092,W06-0901,0,0.391218,"e Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phr"
D12-1092,W09-2307,0,0.0270667,"type is correctly determined if its event type and position in the document match a reference trigger;  An argument is correctly identified if its involved event type and position in the document match any of the reference argument mentions;  An argument role is correctly determined if its involved event type, position in the document, and role match any of the reference argument mentions. Finally, all sentences in the corpus are divided into words using a word segmentation tool ICTCLAS3 with all entities annotated in the corpus kept. Besides, we use Stanford Parser (Levy and Manning, 2003, Chang, et al., 2009) to create the constituent and dependency parse trees and employ the ME model to train individual component classifiers. 3.2 Experimental Results Table 2 and 3 show the Precision (P), Recall (R) and F1-Measure (F) on the held-out test set. It shows that our baseline system outperforms Chen and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1measure on trigger identification, trigger type 3 http://ictclas.org/ determination, argument identification and argument role determination, respectively, with both gains in precision and recall. This is simply due to contribution of the newly-added refined and d"
D12-1092,W09-2209,0,0.623209,"d crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination). Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the specific issues in Chinese. They also constructed a global errata table to 1007 record the inconsistency in the training set and used it to correct the inconsistency in the test set. Ji (2009) extracted cross-lingual predicate clusters using bilingual parallel corpora and a cross-lingual information extraction system, and then used the derived clusters to improve the performa"
D12-1092,N09-2053,0,0.59694,"d crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination). Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the specific issues in Chinese. They also constructed a global errata table to 1007 record the inconsistency in the training set and used it to correct the inconsistency in the test set. Ji (2009) extracted cross-lingual predicate clusters using bilingual parallel corpora and a cross-lingual information extraction system, and then used the derived clusters to improve the performa"
D12-1092,P05-1045,0,0.153706,"Missing"
D12-1092,P09-2093,0,0.510214,"inese triggers and discourse consistency between Chinese trigger mentions. Section 6 presents the experimental results. Section 7 concludes the paper and points out future work. 2 Related Work Almost all the existing studies on event extraction concern English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination"
D12-1092,W09-1704,0,0.46078,"gers and discourse consistency between Chinese trigger mentions. Section 6 presents the experimental results. Section 7 concludes the paper and points out future work. 2 Related Work Almost all the existing studies on event extraction concern English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination"
D12-1092,P08-1030,0,0.55668,"valuation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phrase occurring in the training data is called a known trigger and otherwise, an unknown trigg"
D12-1092,P03-1056,0,0.0203705,"ce trigger;  A trigger type is correctly determined if its event type and position in the document match a reference trigger;  An argument is correctly identified if its involved event type and position in the document match any of the reference argument mentions;  An argument role is correctly determined if its involved event type, position in the document, and role match any of the reference argument mentions. Finally, all sentences in the corpus are divided into words using a word segmentation tool ICTCLAS3 with all entities annotated in the corpus kept. Besides, we use Stanford Parser (Levy and Manning, 2003, Chang, et al., 2009) to create the constituent and dependency parse trees and employ the ME model to train individual component classifiers. 3.2 Experimental Results Table 2 and 3 show the Precision (P), Recall (R) and F1-Measure (F) on the held-out test set. It shows that our baseline system outperforms Chen and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1measure on trigger identification, trigger type 3 http://ictclas.org/ determination, argument identification and argument role determination, respectively, with both gains in precision and recall. This is simply due to contribution of the new"
D12-1092,P10-1081,0,0.210927,"ional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Section 6 presents the experimental results. Section 7 concludes the paper and points out future work. 2 Related Work Almost all the existing studies on event extraction concern English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argume"
D12-1092,P11-1141,0,0.0152721,"2.2 Compositional Semantics Almost all the related studies on compositional semantics focus on how to combine words together to convey complex meanings, such as semantic parser (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Liang et al., 2011). However, the compositional semantics mentioned in this paper is more fined-grained and focuses on how to construct Chinese characters into a word and mine the semantics of words from the word structures, especially of verbs as event triggers. To our knowledge, there is only one paper associated with compositional semantics inside Chinese words. Li (2011) discussed the internal structures inside Chinese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results f"
D12-1092,P11-1060,0,0.0580709,"Missing"
D12-1092,N07-1042,0,0.0603607,"ese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results for disease name, location and start/end time. Mann (2007) proposed some specific inference rules to improve extraction of personal information. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topicrelated documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2011) also used a similar approach and a self-training strategy to extract events. Liao and Grishman (2010) employed cross-event consistency information to improve sentence-level event extraction. Hong et al. (2011) regarded entity type consistency as a key feature to"
D12-1092,P07-1075,0,0.222956,"e consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phrase occurring in the training data is called a k"
D12-1092,D07-1075,0,0.105167,"Missing"
D12-1092,D09-1016,0,0.584146,"05 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phrase occurring in the training data is called a known trigger and otherwise, an unknown trigger. Language Chinese English"
D12-1092,P07-1121,0,0.0049963,"ouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results for disease name, location and start/end time. Mann (2007) proposed some specific inference rules to improve extraction of personal information. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topicrelated documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2011) also used a similar approach and a self-training strategy to extract events. Liao and Grishman (2010) employed cross-event consistency information to improve sentence-level event extraction. Hong et al. (2011) regarded entity type consistency as a key feature to"
D12-1092,H05-1008,0,0.0526678,"Missing"
D12-1092,P95-1026,0,0.0413084,"ctures, especially of verbs as event triggers. To our knowledge, there is only one paper associated with compositional semantics inside Chinese words. Li (2011) discussed the internal structures inside Chinese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results for disease name, location and start/end time. Mann (2007) proposed some specific inference rules to improve extraction of personal information. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topicrelated documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2011) also used a similar approach and a self-training strategy to"
D12-1092,D07-1071,0,0.00703224,"ddress the specific issues in Chinese. They also constructed a global errata table to 1007 record the inconsistency in the training set and used it to correct the inconsistency in the test set. Ji (2009) extracted cross-lingual predicate clusters using bilingual parallel corpora and a cross-lingual information extraction system, and then used the derived clusters to improve the performance of Chinese event extraction. 2.2 Compositional Semantics Almost all the related studies on compositional semantics focus on how to combine words together to convey complex meanings, such as semantic parser (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Liang et al., 2011). However, the compositional semantics mentioned in this paper is more fined-grained and focuses on how to construct Chinese characters into a word and mine the semantics of words from the word structures, especially of verbs as event triggers. To our knowledge, there is only one paper associated with compositional semantics inside Chinese words. Li (2011) discussed the internal structures inside Chinese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been"
D12-1092,P11-1113,1,\N,Missing
D13-1099,P11-2049,0,0.12218,"Missing"
D13-1099,P06-2010,0,0.0655085,"Missing"
D13-1099,E06-1015,0,0.0089802,"et al, 2009; Øvrelid et al, 2010). However, these flat features are hardly to reflect the information implicit in syntactic parse tree structures. Our intuition is that the segments of the syntactic parse tree around a negative or speculative cue is effective for scope detection. The related structures normally underlay the indirect clues to identify the relations between cues and their scopes, e.g., in sentence 1), “but something”, as a frequently co-occurred syntactic structure with “not something”, is an effective clue to determine the linguistic scope of “not”. The tree kernel classifier (Moschitti, 2006) based on support vector machines uses a kernel function between two trees, affording a comparison between their substructures. Therefore, a tree kernel-based scope detection approach with structured syntactic parse tree is employed. The tree 968 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 968–976, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics kernel has been already proved to be effective in semantic role labeling (Che et al, 2006) and relation extraction (Zhou et al, 2007). In addition, the emp"
D13-1099,C10-1155,0,0.0303614,"he second stage. That is, by given golden cues, we detect their linguistic scopes. We propose a tree kernel-based negation and speculation scope detection with structured syntactic parse features. In detail, we regard the scope detection task as a binary classification issue, which is to classify the tokens in a sentence as being inside or outside the scope. In the basic framework, we focus on the analysis and application of structured syntactic parse features as follows: Both constituent and dependency syntactic features have been proved to be effective in scope detection (Özgür et al, 2009; Øvrelid et al, 2010). However, these flat features are hardly to reflect the information implicit in syntactic parse tree structures. Our intuition is that the segments of the syntactic parse tree around a negative or speculative cue is effective for scope detection. The related structures normally underlay the indirect clues to identify the relations between cues and their scopes, e.g., in sentence 1), “but something”, as a frequently co-occurred syntactic structure with “not something”, is an effective clue to determine the linguistic scope of “not”. The tree kernel classifier (Moschitti, 2006) based on support"
D13-1099,D09-1145,0,0.10024,"Missing"
D13-1099,N07-1051,0,0.018078,"Missing"
D13-1099,E99-1043,0,0.228952,"a <cue type=”negation” ref=”X26.8.1”> can not </cue> be explained by abnormalities in corticosteroid receptor characteristics </xcope></xcope> . </sentence> (Note: <Sentence> denotes one sentence and the tag “id” denotes its serial number; <xcope> denotes the scope of a cue; <cue> denotes the cue, the tag “type” denotes the specific kind of cues and the tag “ref” is the cue’s serial number.) Figure 1. An annotated sentence in BioScope. The BioScope corpus consists of three subcorpora: biological Full Papers from FlyBase and BMC Bioinformatics, biological paper Abstracts from the GENIA corpus (Collier et al, 1999), and Clinical Reports. Among them, the Full Papers sub-corpus and the Abstracts sub-corpus come from the same genre. In comparison, the Clinical Reports sub-corpus consists of clinical radiology reports with short sentences. 1 In our experiments, if there is more than one cue in a sentence, we treat them as different cue and scope (two independent instances). The statistical data for our corpus is presented in Table 1 in below. The average length of sentences in the negation portion is almost as long as that in speculation, while the average length of scope in negation is shorter than that in"
D13-1099,W10-3001,0,0.0191079,"Missing"
D13-1099,W08-0606,0,0.0306192,"ristic rule based methods have further involved the syntactic features. Huang et al (2007) implemented a hybrid approach to automated negation scope detection. They combined the regular expression matching with grammatical parsing: negations are classified on the basis of syntactic categories and located in parse trees. Their hybrid approach is able to identify negated concepts in radiology reports even when they are located at some distance from the negative term. 969 Machine Learning based Methods The machine learning based methods have been ignored until the release of the BioScope corpus (Szarvas et al, 2008), where the large-scale data of manually annotated cues and corresponding scopes can support machine learning well. Morante et al (2008) formulated scope detection as a chunk classification problem. It is worth noting that they also proposed an effective proper post-processing approach to ensure the consecutiveness of scope. Then, for further improving the scope detection, Morante et al (2009a) applied a meta-learner that uses the predictions of the three classifiers (TiMBL/SVM/CRF) to predict the scope. For the competitive task in CoNLL’2010 (Farkas et al, 2010), Morante et al (2010) used a m"
D13-1099,I05-2038,0,0.0251458,"Missing"
D13-1099,W06-1617,0,0.188693,"Missing"
D13-1099,D08-1075,0,0.0780942,"Missing"
D13-1099,W09-1105,0,0.136727,"Missing"
D13-1099,W09-1304,0,0.0206763,"Missing"
D13-1099,W10-3006,0,0.0296829,"Missing"
D13-1099,D07-1076,1,0.816069,"Missing"
D13-1099,W10-3018,0,\N,Missing
D13-1099,J12-2005,0,\N,Missing
D15-1187,D10-1036,0,0.0276342,"osed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculates the word scores with respect to different topics. When setting the edge weights, only word co-occurrence is considered. Different from their work, our word-topic graph model runs on a twolayers (word layer and topical layer) graph model and sets the edge weights by measuring both word similarity and topic distribution. 3 Methods The word-topic graph model consists of word layer and topical layer, as shown in Figure 1. While the word layer is constructed according to word co-occurrence within a"
D15-1187,S12-1035,0,0.0896157,"rimental results and analysis. Finally, we conclude our work in Section 5. Corresponding author 1632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2 3.1 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspire"
D15-1187,S12-1039,0,0.264952,"e 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2 3.1 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculate"
D15-1187,P14-1049,1,0.731887,"Missing"
D15-1187,P11-1059,0,0.288678,"e due to manual preparation of annotated corpus. To address this problem, we propose an unsupervised word-topic graph model to represent and measure the focus candidates from both lexical and topic perspectives. Moreover, we propose a document-sensitive biased PageRank algorithm to optimize the ranking scores of focus candidates. Evaluation on the *SEM 2012 shared task corpus shows that our proposed method outperforms the state of the art on negation focus identification. * 1 Introduction Negation is used to reverse the polarity of part of statements that are otherwise affirmative by default (Blanco and Moldovan, 2011), which is common in natural language. Negation focus is defined as the special part in sentence, which is most prominently or explicitly negated by a negative expression. For example, sentence (1) could be interpreted as He stopped, but not until he got to Jackson Hole with a positive part he stopped and a negative part until he got to Jackson Hole. (1) He didn&apos;t stop until he got to Jackson Hole. Our previous work (Zou et al., 2014) showed that contextual information plays a critical role on negation focus identification. For better illustration of this conclusion, they manually analyze the"
D15-1187,W04-3252,0,\N,Missing
D16-1078,P15-1017,0,0.323926,"ection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al."
D16-1078,E99-1043,0,0.269878,"tences 25.47 24.54 7.71 #Sentences 2101 519 855 #Scopes 2659 672 1112 Spe Ave. Len Sentences 29.77 30.76 11.96 Ave. Len Scopes 15.10 13.38 4.92 #Sentences 1597 339 865 #Scopes 1719 376 870 Neg Ave. Len Sentences 29.28 30.55 8.53 Ave. Len Scopes 7.60 7.35 3.87 (Notes: “Ave. Len” denotes average length; “Abs”, “Papers” and “Cli” denote Abstracts, Full Papers and Clinical Records, respectively; “Spe” and “Neg” denote speculation and negation, respectively.) Table 1: Statistics on the BioScope corpus. BioScope includes 3 different sub-corpora: Abstracts of biological papers from the GENIA corpus (Collier et al., 1999), Full scientific Papers from Flybase and BMC Bioinformatics website, and Clinical radiology Records corpus. These texts in three sub-corpora ensure that BioScope can capture the heterogeneity of language use in biomedical domain. While Abstracts and Full Papers share the same genre, Clinical Records consists of shorter sentences. Previous studies regarded Abstracts as the main resource for text mining applications due to its public accessibility (e.g. through PubMed). Table 1 shows the statistics of the BioScope corpus. While in both Abstracts and Full Papers, the average lengths of speculati"
D16-1078,P15-2088,0,0.0231757,"rk (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful inform"
D16-1078,S12-1042,0,0.722304,"guistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the relate"
D16-1078,C10-1076,1,0.942203,"Missing"
D16-1078,P15-2029,0,0.0611826,"bility of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the related work. Section 3 describes our CNN-based model with probabilistic weighted average pooling for scope detection. Section 4 illustrates the experimental settings, and reports the experimental results and analysis. Finally, Sectio"
D16-1078,P15-1003,0,0.0212434,"tional Neural Network (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extrac"
D16-1078,D08-1075,0,0.845804,"Missing"
D16-1078,W09-1304,0,0.107602,"Missing"
D16-1078,W09-1105,0,0.0242881,"Missing"
D16-1078,P15-2060,0,0.0543651,"Missing"
D16-1078,D09-1145,0,0.33263,"Missing"
D16-1078,C10-1155,0,0.0817697,"the speculative cue “may” governs the scope “may harm our lungs”, while the negative cue “not” governs the scope “not like playing football” in sentence S2. Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in Tang et al. (2010)). In comparison, speculation and negation scope detection is still a challenge due to its inherent difficulties and those upstream errors. In this paper, we focus on scope detection. Previous work on scope detection can be classified into heuristic rules based methods (e.g., Özgür et al., 2009; Øvrelid et al., 2010), machine learning based methods (e.g., Tang et al., 2010; Zou et al., 2013), and hybrid approaches which integrate empirical models with manual rules (Velldal et al., 2012). Different from those previous studies, this paper presents a Convolutional Neural Network (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference o"
D16-1078,W08-0606,0,0.442727,"and θ={W0, W1, b1, W2, b2, W3, b3} is the set of parameters. To train the CNN-based model, the Stochastic Gradient Descent algorithm is applied to fine-tune θ. 4 Experimentation In this section, we first introduce the evaluation data, and then describe the experimental settings. Finally, we report the experimental results and analysis. 4.1 scopes in Clinical Records are shorter than those of other two sub-corpora (Average length: 11.96 (speculation sentence), 8.53 (negation sentence), 4.92 (speculation scope) and 3.87 (negation scope)). 4.2 Corpus We evaluate our CNN-based model on BioScope (Szarvas et al., 2008; Vincze et al., 2008), a widely used and freely available resource consisting of sentences annotated with speculative and negative cues and their scopes in biomedical domain. Abs Papers Cli #Documents 1273 9 1954 11871 2670 6383 Total #Sentences Ave. Len Sentences 25.47 24.54 7.71 #Sentences 2101 519 855 #Scopes 2659 672 1112 Spe Ave. Len Sentences 29.77 30.76 11.96 Ave. Len Scopes 15.10 13.38 4.92 #Sentences 1597 339 865 #Scopes 1719 376 870 Neg Ave. Len Sentences 29.28 30.55 8.53 Ave. Len Scopes 7.60 7.35 3.87 (Notes: “Ave. Len” denotes average length; “Abs”, “Papers” and “Cli” denote Abstr"
D16-1078,P15-1150,0,0.0470671,"Missing"
D16-1078,W10-3002,0,0.611654,"ainty and negation, while negation is a grammatical category which reverses the truth value of a proposition. Commonly, speculation and negation extraction involves two typical subtasks: cue identification and scope detection. Here, a cue is a word or phrase that has speculative or negative meaning In sentence S1, the speculative cue “may” governs the scope “may harm our lungs”, while the negative cue “not” governs the scope “not like playing football” in sentence S2. Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in Tang et al. (2010)). In comparison, speculation and negation scope detection is still a challenge due to its inherent difficulties and those upstream errors. In this paper, we focus on scope detection. Previous work on scope detection can be classified into heuristic rules based methods (e.g., Özgür et al., 2009; Øvrelid et al., 2010), machine learning based methods (e.g., Tang et al., 2010; Zou et al., 2013), and hybrid approaches which integrate empirical models with manual rules (Velldal et al., 2012). Different from those previous studies, this paper presents a Convolutional Neural Network (CNN)based approa"
D16-1078,I05-2038,0,0.0331896,"Missing"
D16-1078,W10-3003,0,0.072964,"Missing"
D16-1078,D15-1062,0,0.062542,"al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the related work. Section 3 describes our CNN-based model with probabilistic weighted average pooling for scope detection. Section 4 illustrates the experimental setting"
D16-1078,D15-1206,0,0.0923259,"al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the related work. Section 3 describes our CNN-based model with probabilistic weighted average pooling for scope detection. Section 4 illustrates the experimental setting"
D16-1078,C14-1220,0,0.0625728,"d a CRF model with POS, chunks, NERs, dependency relations as features. Similarly, Lapponi et al. (2012) employed a CRF model with lexical and dependency features for negation scope and event resolution on the Conan Doyle corpus. These machine learning methods manifest the effectiveness of syntactic features. 2.2 CNN based NLP Applications Currently, CNNs have obtained certain success on various NLP tasks, e.g., part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011). Specifically, CNNs have been proven effective in extracting sentence-level features. For instance, Zeng et al. (2014) utilized a CNN-based model to extract sentence-level features for relation classification. Zhang et al. (2015) proposed a shallow CNN-based model for implicit discourse relation recognition. Chen et al. (2015) presented a CNN-based model with dynamic multi-pooling on event extraction. More recently, researchers tend to learn features from complex syntactic trees. Ma et al. (2015) use Input Input Position Features & W0 Word embeddings of a path Embeddings Features may↑MD↑VP↓VP↓NP↓PRPP↓our not↑neg↑does↓prep↓like↓pcomp↓playing Path Features W1× Convolutional Layer Position features + b1 Path fea"
D16-1078,D15-1266,0,0.0713733,"yed a CRF model with lexical and dependency features for negation scope and event resolution on the Conan Doyle corpus. These machine learning methods manifest the effectiveness of syntactic features. 2.2 CNN based NLP Applications Currently, CNNs have obtained certain success on various NLP tasks, e.g., part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011). Specifically, CNNs have been proven effective in extracting sentence-level features. For instance, Zeng et al. (2014) utilized a CNN-based model to extract sentence-level features for relation classification. Zhang et al. (2015) proposed a shallow CNN-based model for implicit discourse relation recognition. Chen et al. (2015) presented a CNN-based model with dynamic multi-pooling on event extraction. More recently, researchers tend to learn features from complex syntactic trees. Ma et al. (2015) use Input Input Position Features & W0 Word embeddings of a path Embeddings Features may↑MD↑VP↓VP↓NP↓PRPP↓our not↑neg↑does↓prep↓like↓pcomp↓playing Path Features W1× Convolutional Layer Position features + b1 Path features concatenate Max Dropout Layer ×Mmask Softmax Layer softmax(W3× +b3) Hidden Layer CNN-based Modeling with"
D16-1078,D13-1099,1,0.572046,"ative cue “not” governs the scope “not like playing football” in sentence S2. Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in Tang et al. (2010)). In comparison, speculation and negation scope detection is still a challenge due to its inherent difficulties and those upstream errors. In this paper, we focus on scope detection. Previous work on scope detection can be classified into heuristic rules based methods (e.g., Özgür et al., 2009; Øvrelid et al., 2010), machine learning based methods (e.g., Tang et al., 2010; Zou et al., 2013), and hybrid approaches which integrate empirical models with manual rules (Velldal et al., 2012). Different from those previous studies, this paper presents a Convolutional Neural Network (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin,"
D16-1078,J12-2005,0,\N,Missing
D19-1230,W12-3808,0,0.503265,"s for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives, respectively. Almost all of the above studies have demonstrated that the information contained in context plays a critical role in negative focus detection. In this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describe the contribution of negative focus to a sentence via the theory of question under discussion, and reannotated a part of the SEM’12 shared task corpus. Matsuyoshi et al. (2014) annotated 1,327 instances of negative foci in Japanese text, and proposed a heuristic rulebased approach to identify them. Banjade and Rus (2016) annotated the DT-Neg corpus on tutorial dialogue genre, which contains 1,088 of negative focus instances. Considering the scale and accessibility of the dataset, we utilize the SEM’12 shared task corpus for experimentation. 3 Contextual Atten"
D19-1230,P11-2049,0,0.0194198,"zed as follows. In Section 2, we give a brief review of related work. In Section 3, we introduce the details of the proposed approach. We show our experimental results and discussions in Section 4. Finally, Section 5 concludes with possible directions for future work. 2 Related Work Earlier studies of negation processing in computational linguistics mainly lie in biomedical information extraction (Chapman et al., 2001; Goldin and Chapman, 2003). With the release of the BioScope corpus (Vincze et al., 2008), the negation processing techniques received a substantial boost (Morante et al., 2008; Apostolova et al., 2011; Zou et al., 2013). In addition, negation also was studied as a critical factor in polarity shifting in sentiment and opinion analysis with various lexical and syntactic features (Socher et al., 2013; Cruz et al., 2016). In general, existing studies of negation processing mainly concentrate on three aspects: 1) cue detection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a negative statement. A ne"
D19-1230,L16-1597,0,0.226586,"this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describe the contribution of negative focus to a sentence via the theory of question under discussion, and reannotated a part of the SEM’12 shared task corpus. Matsuyoshi et al. (2014) annotated 1,327 instances of negative foci in Japanese text, and proposed a heuristic rulebased approach to identify them. Banjade and Rus (2016) annotated the DT-Neg corpus on tutorial dialogue genre, which contains 1,088 of negative focus instances. Considering the scale and accessibility of the dataset, we utilize the SEM’12 shared task corpus for experimentation. 3 Contextual Attention-based Negative Focus Detection In this section, we first introduce the BiLSTMCRF framework. On the basis, we then come up with two kinds of attention mechanisms to model the contextual information, i.e. word-level attention and topic-level attention. 3.1 BiLSTM-CRF Framework In this paper, we recast negative focus detection problem as an I/O tagging"
D19-1230,P11-1059,0,0.738492,"etection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a negative statement. A negative focus could be considered as the semantic part in scope that is intended to be interpreted as false to make other parts to be true. Among above these subtasks, negative focus detection is most extremely challenging in negation processing. In the literature, research on negative focus detection was pioneered by Blanco and Moldovan (2011), who proposed a supervised learning model with a set of highly hand-crafted features as a benchmark for this task. On the basis, Blanco and Moldvan (2013) incorporates negation into other existing semantic representations. The dataset annotated by Blanco and Moldovan is used as a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixe"
D19-1230,matsuyoshi-etal-2014-annotating,0,0.0212634,". Almost all of the above studies have demonstrated that the information contained in context plays a critical role in negative focus detection. In this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describe the contribution of negative focus to a sentence via the theory of question under discussion, and reannotated a part of the SEM’12 shared task corpus. Matsuyoshi et al. (2014) annotated 1,327 instances of negative foci in Japanese text, and proposed a heuristic rulebased approach to identify them. Banjade and Rus (2016) annotated the DT-Neg corpus on tutorial dialogue genre, which contains 1,088 of negative focus instances. Considering the scale and accessibility of the dataset, we utilize the SEM’12 shared task corpus for experimentation. 3 Contextual Attention-based Negative Focus Detection In this section, we first introduce the BiLSTMCRF framework. On the basis, we then come up with two kinds of attention mechanisms to model the contextual information, i.e. wor"
D19-1230,morante-2010-descriptive,0,0.0687896,"Missing"
D19-1230,S12-1035,0,0.598287,"be interpreted as false to make other parts to be true. Among above these subtasks, negative focus detection is most extremely challenging in negation processing. In the literature, research on negative focus detection was pioneered by Blanco and Moldovan (2011), who proposed a supervised learning model with a set of highly hand-crafted features as a benchmark for this task. On the basis, Blanco and Moldvan (2013) incorporates negation into other existing semantic representations. The dataset annotated by Blanco and Moldovan is used as a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixed embeddings softmax w5 Sp Contextual embeddings Xn Xw S Word-level contextual attention Sn Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative focus detection. kinds of heuristic rules for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence featur"
D19-1230,D08-1075,0,0.0450462,"f this paper is organized as follows. In Section 2, we give a brief review of related work. In Section 3, we introduce the details of the proposed approach. We show our experimental results and discussions in Section 4. Finally, Section 5 concludes with possible directions for future work. 2 Related Work Earlier studies of negation processing in computational linguistics mainly lie in biomedical information extraction (Chapman et al., 2001; Goldin and Chapman, 2003). With the release of the BioScope corpus (Vincze et al., 2008), the negation processing techniques received a substantial boost (Morante et al., 2008; Apostolova et al., 2011; Zou et al., 2013). In addition, negation also was studied as a critical factor in polarity shifting in sentiment and opinion analysis with various lexical and syntactic features (Socher et al., 2013; Cruz et al., 2016). In general, existing studies of negation processing mainly concentrate on three aspects: 1) cue detection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a"
D19-1230,J05-1004,0,0.402828,"Missing"
D19-1230,N18-1202,0,0.0638671,"total, this corpus contains 3,544 instances of negative focus. For fair comparison, we adopt the same partition as SEM’12 shared task in all experiments, i.e., with 2,304 for training, 531 for development, and 712 for testing. For each instance, the corpus provides the previous and next sentences as contexts. We evaluate our results in terms of accuracy (acc for short), i.e., for each negation, the predicted focus is considered correct if it is a perfect match with its gold annotation. Table 1 lists the hyper-parameters in our experiments. We utilize the pre-trained word embeddings by ELMo5 (Peters et al., 2018). The semantic role embeddings are initialized randomly by a continuous uniform distribution. All the models are optimized using the stochastic gradient descent (SGD). We pick the parameters showing the best performance on the development set when no further improvement occurs within 50 epochs, and report the performances on the test set. To verify the effectiveness of our approach, we compare with several baselines on negative focus detection, which are briefly introduced as follows. LSTM: The LSTM model which contains only a forward LSTM layer with a fully-connected layer followed by a softm"
D19-1230,D16-1078,1,0.847874,"n input layer within hidden layer Acc 70.22 69.38 70.51 69.80 Embeddings word word+chunking label word+dependency label word+PoS tag word+semantic role all features Table 6: Performance comparison for different composition manners of attention matrices. Word Embedding Senna Glove Word2vec BERT ELMo Dimension 50 100 300 768 1,024 Table 8: Performance comparison for different features with the BiLSTM-CRF model. Acc 70.08 69.66 69.10 70.22 70.51 Table 7: Performance comparison for different pretrained word embeddings on the T-Att BiLSTM-CRF model. lexical or syntactic features (Zou et al., 2013; Qian et al., 2016), the negative focus detection task tends to consider more semantic relation between the focus and other words and phrases. Impact of Attention Manner. In this paper, our contextual attention matrices are concatenated into the input embeddings. In addition, we also explore another manner of adding attention mechanisms, which directly concatenates the attention matrix into the hidden layer of BiLSTMCRF framework. As shown in Table 6, it indicates that the composition manner within the input layer is more effective than that within the hidden layer for both word-level and topic-level attention m"
D19-1230,S12-1039,0,0.836723,"be true. Among above these subtasks, negative focus detection is most extremely challenging in negation processing. In the literature, research on negative focus detection was pioneered by Blanco and Moldovan (2011), who proposed a supervised learning model with a set of highly hand-crafted features as a benchmark for this task. On the basis, Blanco and Moldvan (2013) incorporates negation into other existing semantic representations. The dataset annotated by Blanco and Moldovan is used as a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixed embeddings softmax w5 Sp Contextual embeddings Xn Xw S Word-level contextual attention Sn Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative focus detection. kinds of heuristic rules for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence features with inter-sentence features from both lexical a"
D19-1230,D13-1170,0,0.00505769,"y, Section 5 concludes with possible directions for future work. 2 Related Work Earlier studies of negation processing in computational linguistics mainly lie in biomedical information extraction (Chapman et al., 2001; Goldin and Chapman, 2003). With the release of the BioScope corpus (Vincze et al., 2008), the negation processing techniques received a substantial boost (Morante et al., 2008; Apostolova et al., 2011; Zou et al., 2013). In addition, negation also was studied as a critical factor in polarity shifting in sentiment and opinion analysis with various lexical and syntactic features (Socher et al., 2013; Cruz et al., 2016). In general, existing studies of negation processing mainly concentrate on three aspects: 1) cue detection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a negative statement. A negative focus could be considered as the semantic part in scope that is intended to be interpreted as false to make other parts to be true. Among above these subtasks, negative focus detection is most"
D19-1230,W08-0606,0,0.416054,"Missing"
D19-1230,D16-1058,0,0.0886288,"Missing"
D19-1230,W10-3111,0,0.526958,"Missing"
D19-1230,P16-2034,0,0.0914165,"Missing"
D19-1230,D13-1099,1,0.896942,"Missing"
D19-1230,P14-1049,1,0.790292,"a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixed embeddings softmax w5 Sp Contextual embeddings Xn Xw S Word-level contextual attention Sn Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative focus detection. kinds of heuristic rules for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives, respectively. Almost all of the above studies have demonstrated that the information contained in context plays a critical role in negative focus detection. In this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describ"
D19-1230,D15-1187,1,0.743173,"Missing"
I08-1004,P06-1006,0,0.620807,"structured objects in NLP, e.g. the parse tree structure in coreference resolution. During recent years, various tree kernels, such as the convolution tree kernel (Collins and Duffy 2001), the shallow parse tree kernel (Zelenko et al 2003) and the dependency tree kernel (Culota and Sorensen 2004), have been proposed in the literature. Among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by Collins and Duffy (2002) on parsing, Moschitti (2004) on semantic role labeling, Zhang et al (2006) on semantic relation extraction and Yang et al (2006) on pronoun resolution. However, there exist two problems in Collins and Duffy’s kernel. The first is that the sub-trees enumerated in the tree kernel are context-free. That is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree. The second is how to decide a proper tree span in the tree kernel computation according to the particular application. To resolve above two problems, this paper proposes a new tree span scheme and applies a new tree kernel and to better capture syntactic structured information in pronoun Abstract This paper propo"
I08-1004,P06-1104,1,0.834442,"tive at reducing the burden of feature engineering for structured objects in NLP, e.g. the parse tree structure in coreference resolution. During recent years, various tree kernels, such as the convolution tree kernel (Collins and Duffy 2001), the shallow parse tree kernel (Zelenko et al 2003) and the dependency tree kernel (Culota and Sorensen 2004), have been proposed in the literature. Among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by Collins and Duffy (2002) on parsing, Moschitti (2004) on semantic role labeling, Zhang et al (2006) on semantic relation extraction and Yang et al (2006) on pronoun resolution. However, there exist two problems in Collins and Duffy’s kernel. The first is that the sub-trees enumerated in the tree kernel are context-free. That is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree. The second is how to decide a proper tree span in the tree kernel computation according to the particular application. To resolve above two problems, this paper proposes a new tree span scheme and applies a new tree kernel and to better capture syntactic struc"
I08-1004,P05-1053,1,0.865224,"Missing"
I08-1004,D07-1076,1,0.936292,"in pronoun resolution. However, there is one problem with this tree kernel: the subtrees involved in the tree kernel computation are context-free (That is, they do not consider the information outside the sub-trees.). This is contrast to the tree kernel proposed in Culota and Sorensen (2004) which is context-sensitive, that is, it considers the path from the tree root node to the sub-tree root node. In order to integrate the advantages of both tree kernels and resolve the problem in Collins and Duffy’s kernel, this paper applies the same context-sensitive convolution tree kernel, proposed by Zhou et al (2007) on relation extraction. It works by taking ancestral information (i.e. the root node path) of sub-trees into consideration: m K C (T [1], T [2]) = ∑ i =1 ∑ ∆(n [1], n [2]) i 1 i 1 (1) n1i [1]∈ N 1i [1] n1i [ 2 ]∈ N 1i [ 2 ] Scheme/m 1 2 3 4 Min 78.5 79.8 80.8 80.8 Simple 79.8 81.0 81.7 81.6 Full 78.3 80.1 81.0 81.1 Dynamic 80.8 82.3 83.0 82.9 Table 1: Comparison of different context-sensitive convolution tree kernels and tree span schemes (with entity type info attached at both the anaphor and the antecedent candidate nodes by default) In this paper, the m parameter in our contextsensitive co"
I08-1004,P95-1017,0,0.142746,"Missing"
I08-1004,P06-1005,0,0.0673405,"Missing"
I08-1004,P01-1017,0,0.0324075,"context-sensitive sub-trees by taking their ancestor node paths into consideration. Moreover, this paper also implements a dynamic-expansion tree span scheme by taking predicate- and antecedent competitor-related information into consideration. 3 Context Sensitive Convolution Tree Kernel for Pronoun Resolution In this section, we first propose an algorithm to dynamically determine a proper tree span for pronoun resolution and then present a contextsensitive convolution tree kernel to compute similarity between two tree spans. In this paper, all the texts are parsed using the Charniak parser (Charniak 2001) based on which the tree span is determined. 3.1 Dynamic-Expansion Tree Span Scheme Normally, parsing is done on the sentence level. To deal with the cases that an anaphor and an antecedent candidate do not occur in the same sentence, we construct a pseudo parse tree for an entire text by attaching the parse trees of all its sentences to an upper “S”node, similar to Yang et al (2006). Given the parse tree of a text, the problem is how to choose a proper tree span to well cover syntactic structured information in the tree kernel computation. Generally, the more a tree span includes, the more sy"
I08-1004,P04-1054,0,0.0389763,"Missing"
I08-1004,P98-2143,0,0.0856917,"Missing"
I08-1004,P04-1043,0,0.0988994,"the kernel-based methods could be very effective at reducing the burden of feature engineering for structured objects in NLP, e.g. the parse tree structure in coreference resolution. During recent years, various tree kernels, such as the convolution tree kernel (Collins and Duffy 2001), the shallow parse tree kernel (Zelenko et al 2003) and the dependency tree kernel (Culota and Sorensen 2004), have been proposed in the literature. Among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by Collins and Duffy (2002) on parsing, Moschitti (2004) on semantic role labeling, Zhang et al (2006) on semantic relation extraction and Yang et al (2006) on pronoun resolution. However, there exist two problems in Collins and Duffy’s kernel. The first is that the sub-trees enumerated in the tree kernel are context-free. That is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree. The second is how to decide a proper tree span in the tree kernel computation according to the particular application. To resolve above two problems, this paper proposes a new tree span scheme and applies a new tre"
I08-1004,J01-4004,0,0.755499,"Missing"
I08-1004,P03-1023,1,0.895263,"s done by: 1) Determining the min-expansion tree span via the shortest path, as shown in Figure 1(a). 2) Attaching all the antecedent competitors along the corresponding paths to the shortest path. As shown in Figure 1(b), “the woman”is attached while “the room”is not attached since the former is compatible with the anaphor and the latter is not compatible with the anaphor. In this way, the competition between the considered candidate and other compatible candidates can be included in the tree span. In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. 3) For each node in the tree span, attaching the path from the node to the predicate terminal node if it is a predicate-headed node. As shown in Figure 1(c), “said”and “bit”are attached. 4) Pruning those nodes (except POS nodes) with the single in-arc and the single out-arc and with its syntactic phrase type same as its child node. As shown in Figure 1(d), the left child of the “SBAR”node, the “NP”node, is removed and the sub-tree (NP the/DT woman/NN) is attached to the “SBAR”node directly. To show the difference amon"
I08-1004,J03-4003,0,\N,Missing
I08-1004,J94-4002,0,\N,Missing
I08-1004,C98-2138,0,\N,Missing
I08-1005,I05-1034,1,0.894483,"wer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States” via his relationship with “the United States”. During the last decade, many methods have been proposed in relation extraction, such as supervised learning (Miller et al 2000; Zelenko et al 2003; Culota and Sorensen 2004; Zhao and Grishman 2005; Zhang et al 2006; Zhou et al 2005, 2006), semi-supervised learning (Brin 1998; Agichtein and Gravano 2000; Zhang 2004; Chen et al 2006), and unsupervised learning (Hasegawa et al 2004; Zhang et al 2005). Among these methods, supervised learning-based methods perform much 2 Related Work Generally, supervised learning is preferable to unsupervised learning due to prior knowledge in the 32 annotated training data and better performance. However, the annotated data is usually expensive to obtain. Hence, there has been growing interest in semi-supervised learning, aiming at inducing classifiers by leveraging a small amount of labeled data and a large amount of unlabeled data. Related work in relation extraction using semi-supervised learning can be classified into two categories: bootstrapping-ba"
I08-1005,P05-1052,0,0.0406849,"uction Relation extraction is to detect and classify various predefined semantic relations between two entities from text and can be very useful in many NLP applications such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States” via his relationship with “the United States”. During the last decade, many methods have been proposed in relation extraction, such as supervised learning (Miller et al 2000; Zelenko et al 2003; Culota and Sorensen 2004; Zhao and Grishman 2005; Zhang et al 2006; Zhou et al 2005, 2006), semi-supervised learning (Brin 1998; Agichtein and Gravano 2000; Zhang 2004; Chen et al 2006), and unsupervised learning (Hasegawa et al 2004; Zhang et al 2005). Among these methods, supervised learning-based methods perform much 2 Related Work Generally, supervised learning is preferable to unsupervised learning due to prior knowledge in the 32 annotated training data and better performance. However, the annotated data is usually expensive to obtain. Hence, there has been growing interest in semi-supervised learning, aiming at inducing classifiers b"
I08-1005,P05-1053,1,0.913338,"Missing"
I08-1005,P06-1016,1,0.773743,"Missing"
I08-1005,A00-2030,0,\N,Missing
I08-1005,P06-1017,0,\N,Missing
I08-1005,P04-1043,0,\N,Missing
I08-1005,P04-1054,0,\N,Missing
I08-1005,P04-1053,0,\N,Missing
I08-1005,P05-1049,0,\N,Missing
I08-1005,P06-1104,1,\N,Missing
I08-1005,P01-1017,0,\N,Missing
I11-1118,I08-2103,0,0.0826325,"Missing"
I11-1118,P03-1069,0,0.604833,"unds. Therefore, no natural order of texts can be extracted as the basis of sentence ordering judgment. How to conduct an efficient and effective method for sentences ordering is a difficult but important task for both multidocument summarization and other text processing job, e.g. Question Answering. Currently, a variety of studies have been reported on sentence ordering. Some methods adopted chronological information (McKeown et al., 1999; Lin et al., 2001; Barzilay et al., 2002; Okazaki et al., 2004) while others learned the natural order of sentences from source documents or large corpus (Lapata, 2003; Barzilay and Lee, 2004; Nie et al., 2006; Ji and Nie, 2008). However, chronological information cannot be easily extracted from those non-news documents and constructing a large corpus also is not so easy. Furthermore, those results of all above methods are far from satisfactory. Therefore, how to achieve coherent summarization still is an issue for us. This paper proposes a novel method to infer the order of summarization sentences for multidocument summarization according to their context. We first judge whether or not two summarization sentences should be adjacent based on the similarity"
I11-1118,C04-1108,0,0.455505,"ization because multiple documents are created by different authors who have different writing styles and backgrounds. Therefore, no natural order of texts can be extracted as the basis of sentence ordering judgment. How to conduct an efficient and effective method for sentences ordering is a difficult but important task for both multidocument summarization and other text processing job, e.g. Question Answering. Currently, a variety of studies have been reported on sentence ordering. Some methods adopted chronological information (McKeown et al., 1999; Lin et al., 2001; Barzilay et al., 2002; Okazaki et al., 2004) while others learned the natural order of sentences from source documents or large corpus (Lapata, 2003; Barzilay and Lee, 2004; Nie et al., 2006; Ji and Nie, 2008). However, chronological information cannot be easily extracted from those non-news documents and constructing a large corpus also is not so easy. Furthermore, those results of all above methods are far from satisfactory. Therefore, how to achieve coherent summarization still is an issue for us. This paper proposes a novel method to infer the order of summarization sentences for multidocument summarization according to their contex"
I11-1118,H01-1065,0,0.559381,"Missing"
I11-1118,I05-1055,0,\N,Missing
L18-1302,P14-1065,0,0.0453923,"Missing"
L18-1302,P13-1048,0,0.0307114,"the circumstance of the event, then it cannot be determined whether it corresponds to Circumstance or Background in the RST definition. These differences have been identified in previous studies, so there are many attempts at annotating corpora in both Chinese and English, such as Carlson et al. (2003), Yue (2008), and Li et al. (2014). Carlson et al. (2003) also discussed the annotation problem of macro-level. In the study of computational models, more and more researchers have constructed intra-sentential, multi-sentential, and multiparagraph models separately to achieve higher performance (Joty et al., 2013; Wang et al., 2017). The difficulties of annotating work: 1) the annotation processing is very subjectivity because of the different understandings among different annotators, so the consistency is not very high; 2) a lot of discussion is needed to achieve consistent understanding; 3) the structure framework and relation set have been repeatedly confirmed to give structure definition and annotation guidelines more clearly. Because of these reasons, the current annotation scale is not large enough, and our following research work will continue to expand the scale. 8. Conclusion In this paper,"
L18-1302,D14-1224,0,0.0594937,"Missing"
L18-1302,P17-2029,0,0.0200342,"the event, then it cannot be determined whether it corresponds to Circumstance or Background in the RST definition. These differences have been identified in previous studies, so there are many attempts at annotating corpora in both Chinese and English, such as Carlson et al. (2003), Yue (2008), and Li et al. (2014). Carlson et al. (2003) also discussed the annotation problem of macro-level. In the study of computational models, more and more researchers have constructed intra-sentential, multi-sentential, and multiparagraph models separately to achieve higher performance (Joty et al., 2013; Wang et al., 2017). The difficulties of annotating work: 1) the annotation processing is very subjectivity because of the different understandings among different annotators, so the consistency is not very high; 2) a lot of discussion is needed to achieve consistent understanding; 3) the structure framework and relation set have been repeatedly confirmed to give structure definition and annotation guidelines more clearly. Because of these reasons, the current annotation scale is not large enough, and our following research work will continue to expand the scale. 8. Conclusion In this paper, we expand the discou"
L18-1302,C02-1145,0,0.0716609,"red by Rhetorical Structure Theory (Mann and Thompson, 1987) and Macrostructure Theory (Van Dijk, 1980), we explore a macro discourse structure representation schema. Furthermore, we construct the logical semantic structure and functional pragmatic structure on the macro level of discourse analysis respectively. For each structure we define the structural elements such as leaf nodes, non-leaf nodes and edges pointing. Elaboration Background P1 P2 Joint P3 P4 P5 Figure 1: Logical semantic structure of chtb 0019. Take the chtb 0019 for example, which is a typical news wire article from CTB 8.0 (Xue et al., 2002). There are five paragraphs in the news “Significant achievements in the construction of Ningbo Bonded Area”, and the discourse logical semantic structure of this article is shown as Figure 1. Limited to the length of this paper, the full discourse text of this example is not included, please refer to the corpus CTB 8.0. The main contents of the five paragraphs respectively are: P1) Ningbo Bonded Area achieved fruitful results after three years of construction; P2) the basic situation of the Ningbo Bonded Area; P3) the situation of import 1920 and export trade, warehouses, storage area, etc. P"
L18-1302,P14-1049,1,0.839121,"se structure corpus, logical semantic structure, functional pragmatic structure 1. Introduction A discourse is not formed by independent and isolated discourse units, but by related and structure units. The task of discourse analysis is to segment sentences into elementary discourse units(EDUs) and recognize the relations among them to form a complete discourse structure. Due to the semantic integrity of discourse units, discourse relations and their well-formed structure, discourse informations have been applied to many natural language processing applications, such as information retrieval (Zou et al., 2014), automatic summarization (Ferreira et al., 2014; Cohan and Goharian, 2017), question and answering (Sadek and Meziane, 2016) and statistical machine translation (Guzm´an et al., 2014). Previous research works have proven that discourse informations are beneficial to these NLP applications. The advent of large-scale collections of annotated data shifted the research community of natural language processing. These corpora have accelerated the development efforts and energized the research community. Generally speaking, there exist two hierarchical levels of discourse structures: micro level and"
N19-1287,D18-1389,0,0.124693,"ed the corpora annotated with sentence-level event factuality information, such as ACE 20051 , LU (Diab et al., 2009), FactBank (Saur´ı and Pustejovsky, 2009), and UDS-IH2 (Rudinger et al., 2018). On the other hand, previous studies only con1 https://catalog.ldc.upenn.edu/LDC2006T06 2799 Proceedings of NAACL-HLT 2019, pages 2799–2809 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics sidered information within sentences, using rules (Saur´ı, 2008; Saur´ı and Pustejovsky, 2012), machine learning models (de Marneffe et al., 2012; Werner et al., 2015; Baly et al., 2018), and combinations of them (Qian et al., 2015; Stanovsky et al., 2017) for modeling. Neural network models have also recently been used for the sentence-level event factuality identification (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018). According to Figure 1, document-level event factuality can not be deduced from each sentence-level factuality separately, but depends on the comprehensive semantic information of sentences. However, no suitable model for document-level task has been proposed yet. To solve the issues above, this paper focuses on document-level event factuality ide"
N19-1287,W17-4803,0,0.0245325,"have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with PropS tree annotation structure (Lotan et al., 2013). Baly et al. (2018) considered a set of features and predicted the factuality of reporting and bias of news media. 2806 Saur´ı (2008) and Saur´ı and Pustejovsky (2012) proposed a rule-based model to identify event factuality on FactBank. de Marneffe et al. (2012) used a machine learning model and Qian et al. (2015) utilized a two-step framework combining machine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et"
N19-1287,N18-2055,0,0.0312686,"de Marneffe et al. (2012) used a machine learning model and Qian et al. (2015) utilized a two-step framework combining machine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et al. (2015) proposed a ongoing framework for a larger corpus based on LU, and Cao et al. (2013) constructed a Chinese corpus annotated with event factuality based on ACE 2005. However, no previous work annotated a document-level corpus. We construct DLEF corpus with document-level event factuality for the first time. Some studies focused on document-level event identification task. Choubey et al. (2018) designed a rule-based classifier to identify central events according to event coreference relations. Liu et al. (2018) utilized a kernel-based neural model that captured semantic relations between discourse units for event salience identification. However, they did not consider the documentlevel event factuality. To our best knowledge, this paper is the first work on document-level event factuality identification task. Previous studies (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018) have tried neural network models on sentence-level factuality identification. Recent research has"
N19-1287,W09-3012,0,0.0813525,"n answer questions about the text. According to the document in Figure 1, the answer of the following question should be “No”, which is consistent with the document-level factuality of the event “reach” (CT-): Q: Does the U.S. reach an agreement with Mexico on the new trade deal before December 2017? A: No. Previous studies mostly reported on sentencelevel event factuality identification tasks. On one hand, due to the scarcity of document-level event factuality corpus, these studies only considered the corpora annotated with sentence-level event factuality information, such as ACE 20051 , LU (Diab et al., 2009), FactBank (Saur´ı and Pustejovsky, 2009), and UDS-IH2 (Rudinger et al., 2018). On the other hand, previous studies only con1 https://catalog.ldc.upenn.edu/LDC2006T06 2799 Proceedings of NAACL-HLT 2019, pages 2799–2809 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics sidered information within sentences, using rules (Saur´ı, 2008; Saur´ı and Pustejovsky, 2012), machine learning models (de Marneffe et al., 2012; Werner et al., 2015; Baly et al., 2018), and combinations of them (Qian et al., 2015; Stanovsky et al., 2017) for modeling. Neural network"
N19-1287,S15-1009,0,0.0395026,"Missing"
N19-1287,D17-1054,0,0.0294498,"cro-averaged F1 of joint optimization model on English and Chinese corpus are 82.89/75.64 and 83.83/81.48, respectively. Although document-level event factuality is based on the factuality information in sentences, sentencelevel factuality value of an event only depends on the current sentence, and is likely to have a different value compared to the current document-level factuality. Therefore, the joint model can not improve the performance of document-level task. 5 Related Work Researchers have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with PropS tree ann"
N19-1287,D15-1189,0,0.0544598,"e sentence before and after the current sentence containing the event as the input. Compared to Att 2, Att 2+AT considers Adversarial Training (AT, the same below). We also consider the following baselines for the comparison with our models: MaxEntVote is a maximum entropy model that only considers the view of AUTHOR (de Marneffe et al., 2012). We use maximum entropy model to identify sentence-level event factuality, and consider voting mechanism, i.e., choose the value committed by the most sentences as the document-level factuality value. We also consider other machine learning models, e.g. Lee et al. (2015), but obtain lower micro-/macro-averaged F1 on English (59.38/33.36) and Chinese corpus (53.91/43.20). SentVote identifies sentence-level event factuality, and does not consider inter-sequence attention in the model proposed in Section 3. Similar to MaxEntVote model, voting mechanism is used to identify document-level event factuality in this SentVote model. MP 2 considers Max-Pooling instead of attention compared with Att 2. Att 1 considers only intra-sequence attention, but not the inter-sequence attention. For an event, we concatenate its i dependency paths and j sentences into one path and"
N19-1287,D16-1130,0,0.0293199,"elations between discourse units for event salience identification. However, they did not consider the documentlevel event factuality. To our best knowledge, this paper is the first work on document-level event factuality identification task. Previous studies (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018) have tried neural network models on sentence-level factuality identification. Recent research has shown that neural networks with multi-level attention can extract meaningful information from heterogeneous input and improve the performance of NLP tasks, e.g., discourse relation (Liu and Li, 2016), relation classification (Wang et al., 2016), and question answering (Yu et al., 2017). Moreover, to improve the robustness of neural networks, related studies considered adversarial perturbation and training on text classification (Miyato et al., 2016) and relation extraction (Wu et al., 2017). This paper is in line in proposing an adversarial neural network with both intra- and inter-sequence attention. 6 Conclusion We investigated document-level event factuality identification task by constructing a corpus annotated with document- and sentence-level event factuality based on both English a"
N19-1287,D18-1154,0,0.0249346,"ine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et al. (2015) proposed a ongoing framework for a larger corpus based on LU, and Cao et al. (2013) constructed a Chinese corpus annotated with event factuality based on ACE 2005. However, no previous work annotated a document-level corpus. We construct DLEF corpus with document-level event factuality for the first time. Some studies focused on document-level event identification task. Choubey et al. (2018) designed a rule-based classifier to identify central events according to event coreference relations. Liu et al. (2018) utilized a kernel-based neural model that captured semantic relations between discourse units for event salience identification. However, they did not consider the documentlevel event factuality. To our best knowledge, this paper is the first work on document-level event factuality identification task. Previous studies (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018) have tried neural network models on sentence-level factuality identification. Recent research has shown that neural networks with multi-level attention can extract meaningful information from heterogeneous input and im"
N19-1287,N13-1091,0,0.0218295,"with PropS tree annotation structure (Lotan et al., 2013). Baly et al. (2018) considered a set of features and predicted the factuality of reporting and bias of news media. 2806 Saur´ı (2008) and Saur´ı and Pustejovsky (2012) proposed a rule-based model to identify event factuality on FactBank. de Marneffe et al. (2012) used a machine learning model and Qian et al. (2015) utilized a two-step framework combining machine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et al. (2015) proposed a ongoing framework for a larger corpus based on LU, and Cao et al. (2013) constructed a Chinese corpus annotated with event factuality based on ACE 2005. However, no previous work annotated a document-level corpus. We construct DLEF corpus with document-level event factuality for the first time. Some studies focused on document-level event identification task. Choubey et al. (2018) designed a rule-based classifier to identify central events according to event coreference relations. Liu et al. (2018) utilized a kernel-based neural model that captured semantic relations between discourse units for event salience identification. However, they did not consider the docu"
N19-1287,J12-2003,0,0.0294517,"Missing"
N19-1287,C10-2117,0,0.0207184,"entences, sentencelevel factuality value of an event only depends on the current sentence, and is likely to have a different value compared to the current document-level factuality. Therefore, the joint model can not improve the performance of document-level task. 5 Related Work Researchers have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with PropS tree annotation structure (Lotan et al., 2013). Baly et al. (2018) considered a set of features and predicted the factuality of reporting and bias of news media. 2806 Saur´ı (2008) and Saur´ı and Pustejovsky (2012) proposed a ru"
N19-1287,E17-1110,0,0.0273382,"Figure 2, to extract feature representations of events from the view of documents, we consider both intra- and inter-sequence attention for dependency paths and sentences. In addition, due to the diversity of contents of docIn addition, we also consider the above features in contexts of each sentence containing the event as the input, and set the windows size as 3, i.e., one sentence before and after the current one. If adjacent sentences contain speculative or negative cues, the dependency path is the concatenation of the path from the cue to the root and the path from the root to the event (Quirk and Poon, 2017). 2802 3.2 LSTM with Two Attention Layers A dependency path or sentence can be represented as X0 according to the embedding table. We employ LSTM with hidden units nh to model the sequences from both directions to produce the for− → ward hidden sequence H, the backward hidden − → − → sequence H, and the output sequence H = H + ← − H. We adopt the attention mechanism to capture the most important information from H, and obtain the output h: Hm = tanh(H) (1) α = softmax(v T H) (2) T h = tanh(Hα ) (3) where v ∈ Rnh is the parameter. One event can have k sequences X0 , X1 , . . . , Xk−1 , whose re"
N19-1287,N18-1067,0,0.0369035,"Missing"
N19-1287,J12-2002,0,0.0660719,"Missing"
N19-1287,P17-2056,0,0.034055,"Missing"
N19-1287,W08-0606,0,0.142283,"Missing"
N19-1287,P16-1123,0,0.0769809,"Missing"
N19-1287,W15-1304,0,0.0160473,"studies only considered the corpora annotated with sentence-level event factuality information, such as ACE 20051 , LU (Diab et al., 2009), FactBank (Saur´ı and Pustejovsky, 2009), and UDS-IH2 (Rudinger et al., 2018). On the other hand, previous studies only con1 https://catalog.ldc.upenn.edu/LDC2006T06 2799 Proceedings of NAACL-HLT 2019, pages 2799–2809 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics sidered information within sentences, using rules (Saur´ı, 2008; Saur´ı and Pustejovsky, 2012), machine learning models (de Marneffe et al., 2012; Werner et al., 2015; Baly et al., 2018), and combinations of them (Qian et al., 2015; Stanovsky et al., 2017) for modeling. Neural network models have also recently been used for the sentence-level event factuality identification (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018). According to Figure 1, document-level event factuality can not be deduced from each sentence-level factuality separately, but depends on the comprehensive semantic information of sentences. However, no suitable model for document-level task has been proposed yet. To solve the issues above, this paper focuses on document-level"
N19-1287,D17-1187,0,0.0573663,"Missing"
N19-1287,D16-1172,0,0.0158433,"8. The micro-/macro-averaged F1 of joint optimization model on English and Chinese corpus are 82.89/75.64 and 83.83/81.48, respectively. Although document-level event factuality is based on the factuality information in sentences, sentencelevel factuality value of an event only depends on the current sentence, and is likely to have a different value compared to the current document-level factuality. Therefore, the joint model can not improve the performance of document-level task. 5 Related Work Researchers have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with Pr"
N19-1287,P15-1064,1,0.903845,"Missing"
O06-4006,H94-1019,0,0.119777,"Missing"
O06-4006,2001.mtsummit-papers.15,0,0.125201,"Missing"
O06-4006,2001.mtsummit-eval.11,0,0.127509,"Missing"
O06-4006,2001.mtsummit-papers.25,0,0.0533928,"Missing"
O06-4006,J81-4005,0,0.68348,"Missing"
O06-4006,2001.mtsummit-papers.67,0,0.0580536,"Missing"
O06-4006,2001.mtsummit-papers.35,0,0.0717133,"Missing"
O06-4006,N03-2021,0,0.0599146,"Missing"
O06-4006,niessen-etal-2000-evaluation,0,0.0620782,"Missing"
O06-4006,C02-1057,1,0.799255,"Missing"
O06-4006,2001.mtsummit-papers.3,0,0.0295326,"Missing"
O06-4006,1999.mtsummit-1.84,0,0.131404,"Missing"
P11-1113,W06-0901,0,0.285055,"d-Position revoke Role=Person a single doctor Role=Position doctor Role=Time-within the last five years Table 1: Event extraction example It is noteworthy that event extraction depends on previous phases like name identification, entity mention co-reference and classification. Thereinto, the name identification is another hard task in ACE evaluation and not the focus in this paper. So we skip the phase and instead directly use the entity labels provided by ACE. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardyet al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a patternbased framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simul"
P11-1113,P05-1045,0,0.051541,"ided by ACE. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardyet al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a patternbased framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Ya1 Selected"
P11-1113,P09-2093,0,0.219939,"tion. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. Liao and Grishman (2010) propose document level cross-event inference to improve event extraction. In contrast to Gupta’s work, Liao do not limit themselves to time information for events, but rather use related events and event-type consistency to make predictions or resolve ambiguities regarding a given event. 4 Motivation In event extraction, current transductive inference methods focus on the issue that many events are missing or spuriously tagged because the local information"
P11-1113,P08-1030,0,0.226986,"ncy paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Ya1 Selected from the file “CNN_CF_20030304.1900.02” in ACE-2005 corpus. 1129 rowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction mo"
P11-1113,P10-1081,0,0.154457,"ecognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. Liao and Grishman (2010) propose document level cross-event inference to improve event extraction. In contrast to Gupta’s work, Liao do not limit themselves to time information for events, but rather use related events and event-type consistency to make predictions or resolve ambiguities regarding a given event. 4 Motivation In event extraction, current transductive inference methods focus on the issue that many events are missing or spuriously tagged because the local information is not sufficient to make a confident decision. The solution is to mine credible evidences of event occurrences from global information an"
P11-1113,P07-1075,0,0.0279729,"xtraction example It is noteworthy that event extraction depends on previous phases like name identification, entity mention co-reference and classification. Thereinto, the name identification is another hard task in ACE evaluation and not the focus in this paper. So we skip the phase and instead directly use the entity labels provided by ACE. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardyet al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a patternbased framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local struc"
P11-1113,D07-1075,0,0.0177436,"Missing"
P11-1113,D09-1016,0,0.613694,"ion template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Ya1 Selected from the file “CNN_CF_20030304.1900.02” in ACE-2005 corpus. 1129 rowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-eve"
P11-1113,P95-1026,0,0.0796203,"Missing"
P13-1145,W06-0901,0,0.632042,"ork. Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 reports experimental results and gives deep analysis. Finally, we conclude our work in Section 6. 2 Related Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al., 2010; Li et al., 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under"
P13-1145,N06-1046,0,0.0302296,"the former always uses a verbal noun to refer to an event mentioned in current or previous sentence and the latter is that an event is mentioned twice or more actually. For example, the relation between E2 and E3 in D1 is NC while the trigger of E3 is only a verbal noun without any direct arguments and it refers to E2. We adopt a simple rule to recognize those NC relations: for each event mention whose trigger is a noun and doesn’t act as the subject/object, we regard their relation as NC if there is another event mention with the same trigger in current or previous sentence. Inspired by Ahn (2006), we use the following conditions to infer the EC relations between two event mentions with the same event type: 1) Their trigger mentions refer to the same trigger; 2) They have at least one same or similar 1 It acts as the governing semantic element in a Chinese word. 2 If they have the same event type, they will be regarded as a single event mention. In algorithm 1, HM(tri) is to identify the head morpheme in trigger tri and FindAllMP(hm1, et1, hm2, et2) is to find all event mention pairs in the training set which satisfy the condition that their head morphemes are hm1 and hm2, and their ev"
P13-1145,P11-1062,0,0.0302435,"nd also explore the relation between the argument and its role. Besides, those entities act as non-argument also have the consistency with high probabilities. To let the global argument inference model combine those knowledges of argument semantics, we compute the prior probabilities P(X<i,j&gt;=1) and P(Y<i,j,m&gt;=1) that entity enj occurrs in a specific event type eti as an argument and its role is Rm respectively. To overcome the sparsity of the entities, we cluster those entities into more cohesive subtype following Hong et al. (2011). Hence, following the independence assumptions described by Berant et al. (2011), we modify the fI(EZ) and fD(EZ,Rm)in Eq. 1 as follows: f I ( E Z ) = log f D ( EZ , Rm ) = log where δ and λ are the thresholds learned from the P( X Z = 1 |FZ ) P( X Z = 1) (1 − P( X Z = 1 |FZ ) P( X Z = 0) P (Y< Z , m &gt; = 1 |F< Z , m &gt; ) P ( X < Z , m &gt; = 1) (1 − P ( X < Z , m &gt; = 1 |F< Z , m &gt; ) P( X < Z , m &gt; = 0) (8) (9) where P( X Z = 1 |FZ ) and P(Y< Z ,m &gt; = 1 |F< Z ,m &gt; ) are the probabilities from the AI and AD respectively while FZ and F<Z,m&gt; are the feature vectors. Besides, P ( X < Z ,m &gt; = 1) and P( X Z = 1) are the prior probabilities learning from the training set. 5 Experime"
P13-1145,P08-1090,0,0.0575506,"ese argument extraction. Li et al. (2012b) introduce more refined features to the system of Chen and Ji (2009b) as their baseline. Specially, several studies have successfully incorporated cross-document or document-level information and argument semantics into event extraction, most of them focused on English. Yangarber et al. (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. Chambers and Jurafsky (2008) propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve sentence-level trigger extraction and they also propose an inference method to infer the arguments following role consistency in a document. Hong et al"
P13-1145,P11-1098,0,0.0722166,"Missing"
P13-1145,W09-2209,0,0.351314,"eline. 1 Introduction The task of event extraction is to recognize event mentions of a predefined event type and their arguments (participants and attributes). Generally, it can be divided into two subtasks: trigger extraction, which aims to identify trigger/event mentions and determine their event type, and argument extraction, which aims to extract various arguments of a specific event and assign the roles to them. In this paper, we focus on argument extraction in Chinese event extraction. While most of previous studies in Chinese event extraction deal with Chinese trigger extraction (e.g., Chen and Ji, 2009a; Qin et al., 2010; Li et al., 2012a, 2012b), there are only a few on Chinese argument extraction (e.g., Tan et al., 2008; Chen and Ji, 2009b). Following previous studies, we divide argument extraction into two components, argument identification and role determination, where the former recognizes the arguments in a specific event mention and the latter classifies these arguments by roles. With regard to methodology, most of previous studies on argument extraction recast it as a Semantic Role Labeling (SRL) task and focus on intra-sentence information to identify the arguments and their roles"
P13-1145,N09-2053,0,0.60189,"eline. 1 Introduction The task of event extraction is to recognize event mentions of a predefined event type and their arguments (participants and attributes). Generally, it can be divided into two subtasks: trigger extraction, which aims to identify trigger/event mentions and determine their event type, and argument extraction, which aims to extract various arguments of a specific event and assign the roles to them. In this paper, we focus on argument extraction in Chinese event extraction. While most of previous studies in Chinese event extraction deal with Chinese trigger extraction (e.g., Chen and Ji, 2009a; Qin et al., 2010; Li et al., 2012a, 2012b), there are only a few on Chinese argument extraction (e.g., Tan et al., 2008; Chen and Ji, 2009b). Following previous studies, we divide argument extraction into two components, argument identification and role determination, where the former recognizes the arguments in a specific event mention and the latter classifies these arguments by roles. With regard to methodology, most of previous studies on argument extraction recast it as a Semantic Role Labeling (SRL) task and focus on intra-sentence information to identify the arguments and their roles"
P13-1145,D12-1062,0,0.102635,"following discourse as a sample: D3: 这批战俘离开(E6)阿尔及利亚西部城市廷 杜夫前往(E7)摩洛哥西南部城市阿加迪尔。 (These prisoners left (E6) Tindouf, a western city of Algeria, and went (E7) to Agadir, a southwestern city of Morocco.) - From Xin20001215.2000.0158 In D3, there are two Transport mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and 廷杜夫 (Tindouf) as the Origin role of E7 via their Sequence relation. 1480 4.3 Identifying Relations of Event Mention Pairs Currently, there are only few studies focusing on such area (e.g., Ahn, 2006; Chamber and Jurafsky, 2007; Huang and Rillof, 2012; Do et al., 2012) and their approaches cannot be introduced to our system directly for the language nature and the different goal. We try to achieve a higher accuracy in this stage so that our argument inference can recover more true arguments. Inspired by Li and Zhou (2012), we also use the morphological structure to identify the Parallel relation. Two parallel event mentions with the adjacent trigger mentions w1 and w2 must satisfy follows two conditions: subject/object; 3) The score of cosine similarity of two event mentions is more than a threshold3. Finally, for the Sequence relation, instead of identifyi"
P13-1145,P08-1030,0,0.124638,"nt semantics into event extraction, most of them focused on English. Yangarber et al. (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. Chambers and Jurafsky (2008) propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve sentence-level trigger extraction and they also propose an inference method to infer the arguments following role consistency in a document. Hong et al. (2011) employ the background information to divide an entity type into more cohesive subtypes to create the bridge between two entities and then infer arguments and their roles using cross-entity inference on the subtypes of entities. H"
P13-1145,C10-1068,1,0.897317,"Missing"
P13-1145,C12-1100,1,0.791859,"Missing"
P13-1145,C12-1099,1,0.919259,"port mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and 廷杜夫 (Tindouf) as the Origin role of E7 via their Sequence relation. 1480 4.3 Identifying Relations of Event Mention Pairs Currently, there are only few studies focusing on such area (e.g., Ahn, 2006; Chamber and Jurafsky, 2007; Huang and Rillof, 2012; Do et al., 2012) and their approaches cannot be introduced to our system directly for the language nature and the different goal. We try to achieve a higher accuracy in this stage so that our argument inference can recover more true arguments. Inspired by Li and Zhou (2012), we also use the morphological structure to identify the Parallel relation. Two parallel event mentions with the adjacent trigger mentions w1 and w2 must satisfy follows two conditions: subject/object; 3) The score of cosine similarity of two event mentions is more than a threshold3. Finally, for the Sequence relation, instead of identifying and classifying the relations clearly and correctly, our goal is to identify whether there are relevant event mentions in a long sentence or two adjacent short sentences who share arguments. Algorithm 1 illustrates a knowledge-based approach to identify t"
P13-1145,P06-1047,0,0.0311251,". For the errors in the syntactic parsing, the second single-morpheme trigger is often assigned a wrong tag (e.g., NN, JJ) and this leads to the errors in the argument extraction. Therefore, inferring the arguments of the second singlemorpheme trigger from that of the first one based on Parallel relation is also an available way to recover arguments. Like that the topic is an axis in a discourse, the relations among those relevant event mentions with the different types is the bone to link them into a narration. There are a few studies on using the event relations in NLP (e.g., summarization (Li et al., 2006), learning narrative event chains (Chambers and Jurafsky, 2007)) to ensure its effectiveness. In this paper, we define two types of Sequence relations of relevant event mentions: Cause and Temporal for their high probabilities of sharing arguments. The Cause relation between the event mentions are similar to that in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008). For example, an Attack event often is the cause of an Die or Injure event. Our Temporal relation is limited to those mentions with the same or relevant event types (e.g., Transport and Arrest) for the high probabilities of shar"
P13-1145,P10-1081,0,0.744362,"ences omit many of these entities already mentioned in previous sentences. Similarly, it is hard to recognize 两名以色列人 (two Israelites) as the Target role for event mention E2 and identify 炸 弹 (bomb) as the Instrument role for event mention E1. An alternative way is to employ various relationships among relevant event mentions in a discourse to infer those intersentence arguments. The contributions of this paper are: 1) We propose a novel global argument inference model, in which various kinds of event relations are involved to infer more arguments on their semantic relations. 2) Different from Liao and Grishman (2010) and Hong et al. (2011), which only consider document-level consistency, we propose a more fine-gained consistency model to enforce the consistency in the sentence, discourse and document layers. 3) We incorporate argument semantics into our global argument inference model to unify the semantics of the event and its arguments. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 r"
P13-1145,P12-1088,0,0.0454713,"art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 reports experimental results and gives deep analysis. Finally, we conclude our work in Section 6. 2 Related Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al., 2010; Li et al., 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under the same framework as English trigger identificat"
P13-1145,N07-1042,0,0.015109,"e special issues in Chinese argument extraction. Fu et al. (2010) use a feature weighting scheme to re-weight various features for Chinese argument extraction. Li et al. (2012b) introduce more refined features to the system of Chen and Ji (2009b) as their baseline. Specially, several studies have successfully incorporated cross-document or document-level information and argument semantics into event extraction, most of them focused on English. Yangarber et al. (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. Chambers and Jurafsky (2008) propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve"
P13-1145,D07-1075,0,0.0324836,"l to unify the semantics of the event and its arguments. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 reports experimental results and gives deep analysis. Finally, we conclude our work in Section 6. 2 Related Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al.,"
P13-1145,P11-1114,0,0.0171445,"ated Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al., 2010; Li et al., 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under the same framework as English trigger identification. In additional, there are only very few of them focusing on Chinese argument extraction and almost all aim to feature engineering and are based on sentence-level information and recast this task as an SRL-style task. Tan et al. (2008) introduce multipl"
P13-1145,prasad-etal-2008-penn,0,0.047479,"topic is an axis in a discourse, the relations among those relevant event mentions with the different types is the bone to link them into a narration. There are a few studies on using the event relations in NLP (e.g., summarization (Li et al., 2006), learning narrative event chains (Chambers and Jurafsky, 2007)) to ensure its effectiveness. In this paper, we define two types of Sequence relations of relevant event mentions: Cause and Temporal for their high probabilities of sharing arguments. The Cause relation between the event mentions are similar to that in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008). For example, an Attack event often is the cause of an Die or Injure event. Our Temporal relation is limited to those mentions with the same or relevant event types (e.g., Transport and Arrest) for the high probabilities of sharing arguments. Take the following discourse as a sample: D3: 这批战俘离开(E6)阿尔及利亚西部城市廷 杜夫前往(E7)摩洛哥西南部城市阿加迪尔。 (These prisoners left (E6) Tindouf, a western city of Algeria, and went (E7) to Agadir, a southwestern city of Morocco.) - From Xin20001215.2000.0158 In D3, there are two Transport mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and"
P13-1145,D12-1092,1,\N,Missing
P13-1145,P11-2111,0,\N,Missing
P13-1145,P10-1113,1,\N,Missing
P13-1145,P11-1113,1,\N,Missing
P13-1145,D09-1016,0,\N,Missing
P14-1049,P11-1059,0,0.102688,"mally, negation focus is defined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negative expression in boldface and negation focus underlined. (2) He didn&apos;t stop until he got to Jackson Hole. While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora. This poses serious challenges on negation focus identification. Current studies (e.g., Blanco and Moldovan, 2011; Rosenberg and Bergler, 2012) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and so on, ignoring less-obvious inter-sentence information. This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification. Take following sentence as an example. (3) Helen didn’t allow her youngest son to play the violin. In sentence (3), there are several scenarios on identification of negation focus, with rega"
P14-1049,de-marneffe-etal-2006-generating,0,0.0392268,"Missing"
P14-1049,P05-1045,0,0.0467056,"Missing"
P14-1049,P09-1083,0,0.0321475,"intra-sentence information provides the local features from lexical, syntactic and semantic perspectives, both have their own contributions on negation focus identification. In this paper, we first propose a graph model to gauge the importance of contextual discourse 524 information. Then, we incorporate both intraand inter-sentence features into a machine learning-based framework for negation focus identification. 4.1 Graph Model Graph models have been proven successful in many NLP applications, especially in representing the link relationships between words or sentences (Wan and Yang, 2008; Li et al., 2009). Generally, such models could construct a graph to compute the relevance between document theme and words. In this paper, we propose a graph model to represent the contextual discourse information from both lexical and topic perspectives. In particular, a word-based graph model is proposed to represent the explicit relatedness among words in a discourse from the lexical perspective, while a topic-driven word-based model is proposed to enrich the implicit relatedness between words, by adding one more layer to the word-based graph model in representing the global topic distribution of the whole"
P14-1049,S12-1039,0,0.487036,"fined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negative expression in boldface and negation focus underlined. (2) He didn&apos;t stop until he got to Jackson Hole. While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora. This poses serious challenges on negation focus identification. Current studies (e.g., Blanco and Moldovan, 2011; Rosenberg and Bergler, 2012) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and so on, ignoring less-obvious inter-sentence information. This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification. Take following sentence as an example. (3) Helen didn’t allow her youngest son to play the violin. In sentence (3), there are several scenarios on identification of negation focus, with regard to negation expression n’t,"
P14-1049,C10-1076,1,0.857993,"ics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more at"
P14-1049,W09-1401,0,0.0204889,"cal scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one tea"
P14-1049,P03-1054,0,0.0133396,"Missing"
P14-1049,D08-1075,0,0.0622497,"ere almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawin"
P14-1049,S12-1035,0,0.696511,"s previous sentence, the negation focus should be play the violin, yielding interpretation Helen didn’t allow her youngest son to play the violin, but it didn’t show whether he was allowed to do other things. In this paper, to well accommodate such contextual discourse information in negation focus identification, we propose a graph model to enrich normal intra-sentence features with various kinds of inter-sentence features from both lexical and topic perspectives. Besides, the standard PageRank algorithm is employed to optimize the graph model. Evaluation on the *SEM 2012 shared task corpus (Morante and Blanco, 2012) justifies our approach over several strong baselines. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 presents several strong baselines on negation focus identification with only intra-sentence features. Section 4 introduces our topic-driven word-based graph model with contextual discourse information. Section 5 reports the experimental results and analysis. Finally, we conclude our work in Section 6. 2 Related Work Earlier studies of negation were almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natura"
P14-1049,J12-2001,0,0.0547728,"ve expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in"
P14-1049,morante-daelemans-2012-conandoyle,0,0.196969,"Missing"
P14-1049,W08-0606,0,\N,Missing
P14-1049,J08-2005,0,\N,Missing
P14-1055,P11-1056,0,0.10782,"ge Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of"
P14-1055,P07-1007,0,0.0247155,"number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspe"
P14-1055,P09-1049,0,0.0196458,"represented as Tc and Te. In order to take full advantage of bilingual resources, we translate both labeled and unlabeled instances in one language to ones in the other language as follows: each other when jointly performed in the BAL framework. Yet, to our knowledge, this issue remains unexplored. An important issue for bilingual learning is how to obtain two language views for relation instances from multilingual resources. There are three solutions to this problem, i.e. parallel corpora (Lu et al., 2011), translated corpora (aka. pseudo parallel corpora) (Wan 2009), and bilingual lexicons (Oh et al., 2009). We adopt the one with pseudo parallel corpora, using the machine translation method to generate instances from one language to the other in the BAL paradigm, as depicted in Fig. 2. Lc Æ Let Uc Æ Uet Le Æ Lct Ue Æ Lct The objective is to learn SVM classifiers in both languages, denoted as SVMc and SVMe respectively, in a BAL fashion to improve their classification performance. 4.2 Labeled Chinese Instances (Lc) Machine Translation Labeled Translated Chinese Instances (Lct) Machine Translation Unlabeled Chinese Instances (Uc) Machine Translation Unlabeled Translated Chinese Instances (Uct) Mac"
P14-1055,N04-1012,0,0.0850822,"digm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sark"
P14-1055,P06-1017,0,0.0283279,"tic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled inst"
P14-1055,D10-1034,1,0.905524,"(ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add"
P14-1055,P04-1054,0,0.206954,"Missing"
P14-1055,P96-1042,0,0.28825,"e and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings"
P14-1055,N09-1047,0,0.02862,"se Instances (Uc) Machine Translation Unlabeled Translated Chinese Instances (Uct) Machine Translation Chinese View Labeled Translated English Instances (Let) Labeled English Instances (Le) Unlabeled Translated English Instances (Uet) Unlabeled English Instances (Ue) English View Bilingual active learning Bilingual Active Learning Framework Currently, AL is widely used in NLP tasks in a single language, i.e., during iterations unlabeled instances least confident only in one language are picked and manually labeled to augment the training data. The only exception is AL for machine translation (Haffari et al., 2009; Haffari and Sarkar, 2009), whose purpose is to select the most informative sentences in the source language to be manually translated into the target language. Previous studies (Reichart et al., 2008; Haffari and Sarkar, 2009) show that multi-task active learning (MTAL) can yield promising overall results, no matter whether they are two different tasks or the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit Test Chinese Inst"
P14-1055,P09-1021,0,0.113151,"Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of multilingual corpora to decrease human annotation efforts by selecting highly informative sentences for a newly added language in multilingual parallel corpora. While machine translation inherently deals with multilingual parallel corpora, our task focuses on relation extraction by pseudo parallel corpora in two languages. 3 Baseline Systems This section first introduces the fundamental supervised learning method, and then describes a baseline active learning algorithm. 3.1 Algorithm uncertainty-based activ"
P14-1055,J04-3001,0,0.167303,"e last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52n"
P14-1055,C10-1064,0,0.428561,"n extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer relation annotations via parallel corpora from the resource-rich language (English) to the resourcepoor language (Korean), b"
P14-1055,P12-2010,0,0.0149777,"., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer relation annotations via parallel corpora from the resource-rich language (English) to the resourcepoor language (Korean), but not vice versa. Based on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the perform"
P14-1055,N06-2018,0,0.0288255,"n at least two words in between WBL: the last word in between when at least two words in between WBO: other words in between except the first and last words when at least three words in between b) Entity type ET12: combination of entity types EST12: combination of entity subtypes EC12: combination of entity classes c) Mention level ML12: combination of entity mention levels MT12: combination of LDC mention types d) Overlap #WB: number of other mentions in between #MB: number of words in between M1>M2 or M1&lt;M2: flag indicating whether M2/M1 is included in M1/M2. 3.2 Culotta and McCallum, 2005; Kim et al., 2006) for both Chinese and English relation classification as illustrated in Fig. 1. During iterations a batch of unlabeled instances are chosen in terms of their informativeness to the current classifier, labeled by an oracle and in turn added into the labeled data to retrain the classifier. Due to our focus on the effectiveness of bilingual active learning on relation classification, we only use uncertainty sampling without incorporating more complex measures, such as diversity and representativeness (Settles and Craven, 2008), and leave them for future work. Active Learning Algorithm We use a po"
P14-1055,P08-2023,0,0.0169982,"ere are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Bot"
P14-1055,D12-1013,1,0.845398,"in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across tw"
P14-1055,P11-1033,0,0.0372471,"Missing"
P14-1055,C08-1088,1,0.949018,"l as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances"
P14-1055,P08-1098,0,0.122012,"otential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of multilingual corpora to decrease human annotation efforts by selecting highly"
P14-1055,W07-1516,0,0.020613,"on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided an"
P14-1055,D08-1112,0,0.384465,"rned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of the Association for Computati"
P14-1055,P04-1075,1,0.799897,"both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilin"
P14-1055,P09-1117,0,0.0222932,"arning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of"
P14-1055,D07-1051,0,0.440042,"de researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of th"
P14-1055,P09-1027,0,0.278507,"e complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one language into the other language, forming pseudo parallel corpora. These translated"
P14-1055,N01-1026,0,0.0811263,"instances translated from another language (e.g. English). This demonstrates that there is some complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one lan"
P14-1055,H01-1035,0,0.0500654,"Missing"
P14-1055,I05-1034,1,0.784011,"NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially activ"
P14-1055,P06-1104,1,0.885509,"traction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number o"
P14-1055,P05-1052,0,0.132596,"Missing"
P14-1055,I08-1005,1,0.825005,"tween two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manuall"
P14-1055,P05-1053,1,0.840665,"k of Information Extraction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, gi"
P14-1055,D07-1082,0,0.0353247,"d instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an"
P14-1055,P04-1053,0,\N,Missing
P15-1064,baker-etal-2010-modality,0,0.0224804,"computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of reasonable size annotated with negation and speculation. Second, this may be attributed to th"
P15-1064,N13-1093,0,0.0296922,"tification is very relevant for almost all NLP applications involving text understanding which need to discriminate between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of relat"
P15-1064,W10-3110,0,0.101512,"hich need to discriminate between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard"
P15-1064,W06-1617,0,0.17891,"us constituent (i.e., the scope candidate) and “不(not)” as the given cue, regarding candidate b1 in Figure 1(2)). For clarity, we categorize the features into three groups according to their relevance with the given cue (C, in short), scope candidate (S, in short), and the relationship between cue andcandidate (R, in short). Figure 2 shows four kinds of positional features between cue and scope candidate we defined (R4). Figure 2. Positional features. Some features proposed above may not be effective in classification. Therefore, we adopt a greedy feature se-lection algorithm as described in (Jiang and Ng, 2006) to pick up positive features incrementally according to their contribu661 tions on the development data. Additionally, a cue should have one continuous block as its scope, but the scope identifier may result in discontinuous scope due to independent candidate in classification. For this reason, we employ a post-processing algorithm as described in Zhu et al. (2010) to identify the boundaries. 5 Experimentation In this section, we evaluate our feature-based sequence labeling model and cross-lingual cue expansion strategy on cue detection, and report the experimental results to justify the appr"
P15-1064,P14-2093,0,0.0330872,"Missing"
P15-1064,D08-1075,0,0.145443,"n and speculation identification is very relevant for almost all NLP applications involving text understanding which need to discriminate between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource serious"
P15-1064,J12-2001,0,0.0289766,"priateness of our syntactic structure-based framework which obtained significant improvement over the stateof-the-art on negation and speculation identification in Chinese language. * 1 Introduction Negation and speculation are ubiquitous phenomena in natural language. While negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition, speculation is a grammatical category which expresses the attitude of a speaker towards a statement in terms of degree of certainty, * reliability, subjectivity, sources of information, and perspective (Morante and Sporleder, 2012). Current studies on negation and speculation identification mainly focus on two tasks: 1) cue detection, which aims to detect the signal of a negative or speculative expression, and 2) scope resolution, which aims to determine the linguistic coverage of a cue in sentence, in distinguishing unreliable or uncertain information from facts. For example, (E1) and (E2) include a negative cue and a speculative cue respectively, both denoted in boldface with their linguistic scopes denoted in square brackets (adopted hereinafter). In sentence (E1), the negative cue “不(not)” triggers the scope of “不会追"
P15-1064,J03-1002,0,0.00637992,"Missing"
P15-1064,C10-3004,0,0.0610142,"Missing"
P15-1064,N06-1005,0,0.0266548,"treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of reasonable size annotated with negation and speculat"
P15-1064,W12-4203,0,0.0189381,"istics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of reasonable size annotated with negation and speculation. Second, this may be attributed to the limitations of Chinese"
P15-1064,D10-1070,1,0.847272,"tures between cue and scope candidate we defined (R4). Figure 2. Positional features. Some features proposed above may not be effective in classification. Therefore, we adopt a greedy feature se-lection algorithm as described in (Jiang and Ng, 2006) to pick up positive features incrementally according to their contribu661 tions on the development data. Additionally, a cue should have one continuous block as its scope, but the scope identifier may result in discontinuous scope due to independent candidate in classification. For this reason, we employ a post-processing algorithm as described in Zhu et al. (2010) to identify the boundaries. 5 Experimentation In this section, we evaluate our feature-based sequence labeling model and cross-lingual cue expansion strategy on cue detection, and report the experimental results to justify the appropriateness of our syntactic structure-based framework on scope resolution in Chinese language. The performance is measured by Precision (P), Recall (R), and F1-score (F). In addition, for scope resolution, we also report the accuracy in PCS (Percentage of Correct Scopes), within which a scope is fully correct if the output of scope resolution system and the correct"
P15-1064,P14-1029,0,0.0177105,"te between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of"
P15-1064,D13-1099,1,0.723881,"Missing"
P15-1064,W08-0606,0,\N,Missing
P15-1064,P04-1035,0,\N,Missing
P18-1048,doddington-etal-2004-automatic,0,0.569132,"contrary, D the parameters θdˇ to minimize the loss: Task Deﬁnition The task of event detection is to determine whether there is one or more event triggers in a sentence. Trigger is deﬁned as a token or nugget that best signals the occurrence of an event. If successfully identiﬁed, a trigger is required to be assigned a tag to indicate the event type: Input: Either its bad or good Output: its &lt;trigger&gt;; Marry &lt;type&gt; We formalize the event detection problem as a multi-class classiﬁcation problem. Given a sentence, we classify every token of the sentence into one of the predeﬁned event classes (Doddington et al., 2004) or non-trigger class. 3 Self-Regulated Learning (SELF) SELF is a double-channel model (Figure 1), consisted of a cooperative network (Islam et al., 2003) and a generative adversarial net (GAN) (Goodfellow et al., 2014). A memory suppressor S is used to regulate communication between the channels. 3.1 θgˇ = argmax L(ˆ y , y) (2) y , y) θdˇ = argmin L(ˆ (3) Numerous studies have conﬁrmed that the twoˇ and D ˇ to implayer minmax game enables both G prove their methods (Goodfellow et al., 2014; Liu and Tuzel, 2016; Huang et al., 2017). Cooperative Network In channel 1, the generator G is speciﬁed"
P18-1048,W06-0901,0,0.869053,"networks that consist of a Bi-LSTM and a CNN. Besides, we compare our model with Liu et al (2016b)’s artiﬁcial neural networks (ANNs), Liu et al (2017b)’s attention-based ANN (ANN-S2) and Chen et al (2017)’s DM-CNN∗ . The models recently have become popular because, although simple in structure, they are very analytic by learning from richer event examples, such as those in Experimentation 5.1 Resource and Experimental Datasets We test the presented model on the ACE 2005 corpus. The corpus is annotated with single-token event triggers and has 33 predeﬁned event types (Doddington et al., 2004; Ahn, 2006), along with one class “None” for the non-trigger tokens, constitutes a 34-class classiﬁcation problem. For comparison purpose, we use the corpus in the traditional way, randomly selecting 30 articles in English from different genres as the development set, and utilizing a separate set of 40 English newswire articles as the test set. The remaining 529 English articles are used as the training set. 5.2 Compared Systems Hyperparameter Settings The word embeddings are initialized with the 300dimensional real-valued vectors. We follow Chen et al (2015) and Feng et al (2016) to pre-train the embedd"
P18-1048,W06-1615,0,0.223765,"Missing"
P18-1048,P16-2011,0,0.639539,"here W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and G trouble), the values of the parameters in the two LSTM based Generator Given a sentence, we follow Chen et al (2015) to take all the tokens of the whole sentence as t"
P18-1048,C16-1309,0,0.0428892,"Missing"
P18-1048,R15-1010,0,0.0405548,"Missing"
P18-1048,P16-2060,0,0.671957,"ct−1  ft ct =  (7) where W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and G trouble), the values of the parameters in the two LSTM based Generator Given a sentence, we follow Chen et al (2015) to take all the tokens of the"
P18-1048,R15-1011,0,0.214348,"Missing"
P18-1048,P17-1038,0,0.102877,"and Grishman, 2015), the non-consecutive Ngrams based CNN (NC-CNN) (Nguyen and Grishman, 2016) and the CNN that is assembled with a dynamic multi-pooling layer (DM-CNN) (Chen et al., 2015). Others include Ghaeini et al (2016)’s forward-backward recurrent neural network (FBRNN) which is developed using gated recurrent units (GRU), Nguyen et al (2016)’s bidirectional RNN (Bi-RNN) and Feng et al (2016)’s Hybrid networks that consist of a Bi-LSTM and a CNN. Besides, we compare our model with Liu et al (2016b)’s artiﬁcial neural networks (ANNs), Liu et al (2017b)’s attention-based ANN (ANN-S2) and Chen et al (2017)’s DM-CNN∗ . The models recently have become popular because, although simple in structure, they are very analytic by learning from richer event examples, such as those in Experimentation 5.1 Resource and Experimental Datasets We test the presented model on the ACE 2005 corpus. The corpus is annotated with single-token event triggers and has 33 predeﬁned event types (Doddington et al., 2004; Ahn, 2006), along with one class “None” for the non-trigger tokens, constitutes a 34-class classiﬁcation problem. For comparison purpose, we use the corpus in the traditional way, randomly selecting 30 art"
P18-1048,P15-1017,0,0.827042,"chuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and G trouble), the values of the parameters in the two LSTM based Generator Given a sentence, we follow Chen et al (2015) to take all the tokens of the whole sentence as the in517 2 where,  · F denotes the squared Frobenius norm (Bousmalis et al., 2016), which is used to calculate the similarity between matrices. It is noteworthy that the feature vectors a generator outputs are required to serve as the rows in the matrix, deployed in a top-down manner and arranged in the order in which they are generated. For example, the feature vector og,t the generator G outputs at the time t needs to be placed in the t-th row of the matrix Og . At the very beginning of the measurement, the similarity between every feature"
P18-1048,P16-1098,0,0.0606388,"structure as the generators. And both the discriminators are implemented as a fullyconnected layer followed by a softmax layer. 4 (6) ct  it + ct−1  ft ct =  (7) where W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and"
P18-1048,P11-1113,1,0.958583,"Soochow University No.1, Shizi ST, Suzhou, China, 215006 {tianxianer, wxchow024, jlzhang05}@gmail.com {qmzhu, gdzhou}@suda.edu.cn Abstract 2016; Feng et al., 2016; Liu et al., 2017b; Chen et al., 2017), which allows semantics of event mentions (trigger plus context) to be encoded in a high-dimensional latent feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Transport: Due to the"
P18-1048,P17-1001,0,0.0687126,"Missing"
P18-1048,P16-1201,0,0.210946,"structure as the generators. And both the discriminators are implemented as a fullyconnected layer followed by a softmax layer. 4 (6) ct  it + ct−1  ft ct =  (7) where W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and"
P18-1048,P17-1164,0,0.519794,"t feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Transport: Due to the ability of encoding and mapping semantic information into a highdimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we"
P18-1048,N13-1090,0,0.0056182,"e” for the non-trigger tokens, constitutes a 34-class classiﬁcation problem. For comparison purpose, we use the corpus in the traditional way, randomly selecting 30 articles in English from different genres as the development set, and utilizing a separate set of 40 English newswire articles as the test set. The remaining 529 English articles are used as the training set. 5.2 Compared Systems Hyperparameter Settings The word embeddings are initialized with the 300dimensional real-valued vectors. We follow Chen et al (2015) and Feng et al (2016) to pre-train the embeddings over NYT corpus using Mikolov et al (2013)’s skip-gram tool. The entity type embeddings, as usual (Nguyen et al., 2016; Feng et al., 2016; Liu et al., 2017b), are speciﬁed as the 50dimensional real-valued vectors. They are initialized with the 32-bit ﬂoating-point values, which are all randomly sampled from the uniformly distributed values in [-1, 1]1 . We initialize other adjustable parameters of the back-propagation algorithm by randomly sampling in [-0.1, 0.1]. We follow Feng et al (2016) to set the dropout rate as 0.2 and the mini-batch size as 10. We 2 https://github.com/JoeZhouWenxuan/Self-regulationEmploying-a-Generative-Advers"
P18-1048,N16-1034,0,0.526925,"KBP evaluations for nugget and coreference detection (Hong et al., 2014, 2015; Yu et al., 2016). It is based on structured perceptron and combines the local and global features. Neural network based approaches: including the convolutional neural network (CNN) (Nguyen and Grishman, 2015), the non-consecutive Ngrams based CNN (NC-CNN) (Nguyen and Grishman, 2016) and the CNN that is assembled with a dynamic multi-pooling layer (DM-CNN) (Chen et al., 2015). Others include Ghaeini et al (2016)’s forward-backward recurrent neural network (FBRNN) which is developed using gated recurrent units (GRU), Nguyen et al (2016)’s bidirectional RNN (Bi-RNN) and Feng et al (2016)’s Hybrid networks that consist of a Bi-LSTM and a CNN. Besides, we compare our model with Liu et al (2016b)’s artiﬁcial neural networks (ANNs), Liu et al (2017b)’s attention-based ANN (ANN-S2) and Chen et al (2017)’s DM-CNN∗ . The models recently have become popular because, although simple in structure, they are very analytic by learning from richer event examples, such as those in Experimentation 5.1 Resource and Experimental Datasets We test the presented model on the ACE 2005 corpus. The corpus is annotated with single-token event trigger"
P18-1048,P13-1008,0,0.718851,"No.1, Shizi ST, Suzhou, China, 215006 {tianxianer, wxchow024, jlzhang05}@gmail.com {qmzhu, gdzhou}@suda.edu.cn Abstract 2016; Feng et al., 2016; Liu et al., 2017b; Chen et al., 2017), which allows semantics of event mentions (trigger plus context) to be encoded in a high-dimensional latent feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Transport: Due to the ability of encodin"
P18-1048,P14-2012,0,0.035215,"Missing"
P18-1048,D14-1198,1,0.918773,"Missing"
P18-1048,P15-2060,0,0.157074,"n the development set. Grid search (Liu et al., 2017a) is used to seek for the optimal parameters. Eventually, we take the coefﬁcient λ of 0.1+3 , learning rate of 0.3 and L2 norm of 0. The source code of SELF2 to reproduce the experiments has been made publicly available. where λ is a hyper-parameter, which is used to harmonize the two losses. The min-max game is utilized for training the ygˇ, y); adversarial net in SELF: θgˇ = argmax L(ˆ ygˇ, y). θdˇ = argmin L(ˆ All the networks in SELF are trained jointly using the same batches of samples. They are trained via stochastic gradient descent (Nguyen and Grishman, 2015) with shufﬂed mini-batches and the AdaDelta update rule (Zeiler, 2012). The gradients are computed using back propagation. And regularization is implemented by a dropout (Hinton et al., 2012). 5 5.3 The state-of-the-art models proposed in the past decade are compared with ours. By taking learning framework as the criterion, we divide the models into three classes: Minimally supervised approach: is Peng et al (2016)’s MSEP-EMD. Feature based approaches: primarily including Liao and Grishman (2010)’s Cross-Event inference model, which is based on the max-entropy classiﬁcation and embeds the docu"
P18-1048,P10-1081,0,0.666678,"r Science and Technology, Soochow University No.1, Shizi ST, Suzhou, China, 215006 {tianxianer, wxchow024, jlzhang05}@gmail.com {qmzhu, gdzhou}@suda.edu.cn Abstract 2016; Feng et al., 2016; Liu et al., 2017b; Chen et al., 2017), which allows semantics of event mentions (trigger plus context) to be encoded in a high-dimensional latent feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Tra"
P18-1048,D16-1085,0,0.496184,". To address the challenge, we suggest to regulate the learning process with a two-channel selfregulated learning strategy. In the self-regulation process, on one hand, a generative adversarial network is trained to produce the most spurious features, while on the other hand, a neural network 1) Generality – taken home &lt;Transport&gt; Ambiguity 1 – campaign in Iraq &lt;Attack&gt; Ambiguity 2 – political campaign &lt;Elect&gt; Coreference – Either its bad or good &lt;Marry&gt; A promising solution to this challenge is through semantic understanding. Recently, neural networks have been widely used in this direction (Nguyen and Grishman, 2016; Ghaeini et al., ∗ Corresponding author 515 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 515–526 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 濵濴濶濾澳瀃瀅瀂瀃濴濺濴瀇濼瀂瀁澳 澳   Therefore, G and D cooperate with each other during training, developing the parameters θg and θd with the same goal – to minimize the performance loss L(ˆ y , y) in the detection task:   θg = argmin L(ˆ y , y) (1) θd 濣瀅濸濷濼濶瀇濼瀂瀁澳 ݔ ෙ ෙ  濵濴濶濾澳瀃瀅瀂瀃濴濺濴瀇濼瀂瀁澳 Figure 1: Self-regulated learning scheme where, y denotes the g"
P18-1048,D09-1016,0,0.0759906,"or the out-domain case, ideally, both Hybrid and SELF encounter the problem that there is lack of target domain data available for training. In this case, SELF displays less performance degradation • It relies on the use of spurious features to implement self-regulation during training. 522 Event mentions And it still does We had no part in it Nobody questions if this is right or ... And that is what ha- what is happening Oh, yeah, it wasn’t perfect Type Die Arrest-Jail Attack End-Position Marry ety of strategies have emerged for converting classiﬁcation clues into feature vectors (Ahn, 2006; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013, 2014; Wei et al., 2017). Beneﬁting from the general modeling framework, the methods enable the fusion of multiple features, and more importantly, they are ﬂexible to use by feature selection. But considerable expertise is required for feature engineering. Recently, the use of neural networks for event detection has become a promising line of research. The closely related work has been presented in section 5.3. The primary advantages of neural networks have been demonstrated in the work, such as performance enhancement, self-learnin"
P18-1048,D16-1038,0,0.0701099,"Missing"
P18-1048,P13-1147,0,0.0208894,"Missing"
P18-1048,P10-1040,0,0.00938638,"between Og and Ogˇ for measuring the loss of self-regulation learning Ldif f . The higher the similarity, the greater the loss. During training, the generator G is required to develop the parameters θg to minimize the loss: θg = argmin Ldif f (og , ogˇ) put. Before feeding the tokens into the network, we transform each of them into a real-valued vector x ∈ Re . The vector is formed by concatenating a word embedding with an entity type embedding. • Word Embedding: It is a ﬁxed-dimensional real-valued vector which represents the hidden semantic properties of a token (Collobert and Weston, 2008; Turian et al., 2010). • Entity Type Embedding: It is specially used to characterize the entity type associated with a token. The BIO2 tagging scheme (Wang and Manning, 2013; Huang et al., 2015) is employed for assigning a type label to each token in the sentence. (4) For the input token xt at the current time step t, the LSTM generates the latent feature vector ot ∈ Rd by the previous memory. Meanwhile, the token is used to update the current memory. The LSTM possesses a long-term memory unit ct ∈ Rd . In addition, it ct ∈ Rd and short-term  is equipped with the input gate it , forgetting gate ft and a hidden st"
P18-1048,I13-1183,0,0.0170587,"enerator G is required to develop the parameters θg to minimize the loss: θg = argmin Ldif f (og , ogˇ) put. Before feeding the tokens into the network, we transform each of them into a real-valued vector x ∈ Re . The vector is formed by concatenating a word embedding with an entity type embedding. • Word Embedding: It is a ﬁxed-dimensional real-valued vector which represents the hidden semantic properties of a token (Collobert and Weston, 2008; Turian et al., 2010). • Entity Type Embedding: It is specially used to characterize the entity type associated with a token. The BIO2 tagging scheme (Wang and Manning, 2013; Huang et al., 2015) is employed for assigning a type label to each token in the sentence. (4) For the input token xt at the current time step t, the LSTM generates the latent feature vector ot ∈ Rd by the previous memory. Meanwhile, the token is used to update the current memory. The LSTM possesses a long-term memory unit ct ∈ Rd . In addition, it ct ∈ Rd and short-term  is equipped with the input gate it , forgetting gate ft and a hidden state ht , which are assembled together to promote the use of memory, as well as dynamic memory updating. Similarly, they are deﬁned as a d-dimensional ve"
P18-1048,P17-2046,0,0.0252994,"f target domain data available for training. In this case, SELF displays less performance degradation • It relies on the use of spurious features to implement self-regulation during training. 522 Event mentions And it still does We had no part in it Nobody questions if this is right or ... And that is what ha- what is happening Oh, yeah, it wasn’t perfect Type Die Arrest-Jail Attack End-Position Marry ety of strategies have emerged for converting classiﬁcation clues into feature vectors (Ahn, 2006; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013, 2014; Wei et al., 2017). Beneﬁting from the general modeling framework, the methods enable the fusion of multiple features, and more importantly, they are ﬂexible to use by feature selection. But considerable expertise is required for feature engineering. Recently, the use of neural networks for event detection has become a promising line of research. The closely related work has been presented in section 5.3. The primary advantages of neural networks have been demonstrated in the work, such as performance enhancement, self-learning capability and robustness. The generative adversarial network (Goodfellow et al., 20"
P19-1058,P14-1048,0,0.0258586,"evaluated their models on PDTB (Prasad et al., 2008) and RST-DT (Carlson et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arg"
P19-1058,C18-1046,0,0.209915,"Missing"
P19-1058,C18-1048,0,0.0648367,"ra that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representatio"
P19-1058,P16-1163,0,0.17057,"itler et al., 2009; Lin et al., 2009; Wang et al., 2017; Kong and Zhou, 2017) that directly rely on feature engineering, recent neural network models (Liu et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018) can capture deeper semantic cues and learn better representations (Zhang et al., 2015). In particular, most neural network-based methods encode arguments using variants of BiLSTM or CNN (Qin et al., 2016; Guo et al., 2018) and propose various models (e.g., the gated relevance network, the encoder-decoder model, and interactive attention) to measure the semantic relevance (Chen et al., 2016; Cianflone and Kosseim, 2018; Guo et al., 2018) Due to the large differences between the hypotactic English language and the paratactic Chinese language, English-based models, which rely heavily on sentence-level representations, may not function well on Chinese. Due to its paratactic nature, Chinese is flooded with a broad range of flexible sentence structures and semantic cohesion, such as ellipses, references, substitutions, and conIn the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide"
P19-1058,D17-1134,0,0.456089,"ly, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex interactions. Bai and Zhao (2018) proposed a deep enhanced representation to represent arguments at the character, subword, word, and sentence levels. Chen et al. (2016) introduced a gated relevance network to model both the linear and nonlinear correlations between two arguments. Guo et a"
P19-1058,D16-1246,0,0.061779,"multi-level attention model that simulates the repeated reading process by stacking multiple attention layers with external memory; (2) R¨onnqvist (R¨onnqvist et al., 2017): a Bi-LSTM model with attention mechanism that first links argument pairs by inserting special labels; and (3) Guo (Guo et al., 2018): a neural tensor network that encodes the arguments by BiLSTM and interactive attention. Among them, GCN uses the same settings as our model. Following Liu and Li (2016), the hidden size for each direction of Bi-LSTM is set to 350, the same as the dimension of the word embeddings. Following Qin et al. (2016), the convolution kernel size and the number in CNN are set to 2 and 1024, respectively. The three state-of-the-art models are reproduced following their corresponding work. The experimental results on CDTB are illustrated in Table 2. It shows that our TTN model outperforms the other baselines in both the micro and macro F1-scores. This indicated that topiclevel information is a vital evidence to reveal the relationships among arguments and justify the effectiveness of our TTN model. Compared with the basic recurrent neural network Bi-LSTM, the CNN and GCN significantly improve the micro and m"
P19-1058,D14-1224,1,0.906946,"Missing"
P19-1058,P17-1093,0,0.240562,"Missing"
P19-1058,D09-1036,0,0.0871272,"Missing"
P19-1058,D16-1130,0,0.614404,"on et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From th"
P19-1058,P17-2040,0,0.337484,"Missing"
P19-1058,I17-1049,0,0.0141754,"all Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex interactions. Bai and Zhao (2018) proposed a deep enhanced representation to repr"
P19-1058,P17-2029,0,0.0190496,"on PDTB (Prasad et al., 2008) and RST-DT (Carlson et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to in"
P19-1058,P14-1028,0,0.0341615,"and POS embedding, each gated convolutional layer hl is computed as follows: where fn (·) is a standard nonlinear function, M ∈ Rd×d×m is a 3rd-order transformation tensor, U ∈ Rm×2d and s ∈ Rm are parameters. The tensor product x&gt; M [1:m] y results in a vector c ∈ Rm , where each entry is computed by slice i of the tensor M as ci = x&gt; M [i] y, and it is equivalent to including m Bilinear models that simultaneously capture multiple linear interactions between vectors. However, it increases the parameters and the computational complexity of the model; therefore, we adopt tensor factorization (Pei et al., 2014), which uses two low rank matrices to approximate each tensor slice M [i] , as follows: hl (X) = (X · W + b) ⊗ σ(X · V + c) + X (6) where X ∈ RN ×D is the input of layer hl (either the input sequence E or the outputs of previous layers), W ∈ RC×D×D , b ∈ RD , V ∈ RC×D×D , c ∈ RD are model parameters, and C is the size of the convolution kernel. σ(·) is the sigmoid function and ⊗ is the element-wise product between matrices. After stacking L layers on top of the input, we can obtain the semantic representation sequence of the argument H = hL ◦ ... ◦ h1 (E) ∈ RN ×D . Finally, the Mean Pooling op"
P19-1058,D18-1079,1,0.847383,"ng the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex interactions. Bai and Zhao (2018) proposed a deep enhanced representation to represent arguments at th"
P19-1058,P09-1077,0,0.17798,". 2 Related Work Most previous studies evaluated their models on PDTB (Prasad et al., 2008) and RST-DT (Carlson et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods"
P19-1058,K16-2001,0,0.0256395,"Missing"
P19-1058,D18-1351,0,0.0210975,"ss entropy beb k recontween the BoW representation Bk and B structed by the decoder. Since decreasing the KL (Kullback-Leibler) divergence makes all p(Z|B) approximate the standard normal distribution, the noise can be prevented from being zero with the result as follows. Similar to the LDA-style topic models, we believe that there is an association between the word distribution Bk of an argument and its topic distribution Zk . For each Bk , we can infer a latent topic distribution Zk ∈ RK through our topic model, where K denotes the number of topics. Inspired by the Neural Topic Model (NTM) (Zeng et al., 2018; Miao et al., 2016), we propose a simplified topic model STM based on the Variational AutoEncoder (VAE) (Kingma and Welling, 2013). Unlike NTM, our model does not attempt to reconstruct the document during the decoding phase, and it only restores the word distributions. Although STM cannot learn the semantic word embeddings, it significantly reduces the training parameters to perform unsupervised training on the discourse corpus with a small sample size. Similar to NTM, we can interpret our STM as a VAE: a neural network encoder p(Z|B) first compresses the BoW representation Bk into a continu"
P19-1058,D15-1266,0,0.399428,"Missing"
P19-1058,C10-2172,0,0.0258068,"PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex i"
W08-2137,J02-3001,0,0.0407468,"valuation on the shared task shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, mor"
W08-2137,P02-1031,0,0.0222461,"k shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers tu"
W08-2137,C04-1186,0,0.177864,"mmercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers turn to dependency tree-based SRL with hope to advance SRL from viewpoint of dependency parsing. Hacioglu (2004) pioneered this work by formulating SRL as a classification problem of mapping various dependency relations into semantic roles. Compared with previous researches on constituent structure tree-based SRL which adopts constituents as labeling units, dependency tree-based SRL adopts dependency relations as labeling units. Due to the difference between constituent structure trees and dependency trees, their feature spaces are expected to be somewhat different. In the CoNLL 2008 shared task, we extend the framework by Hacioglu (2004) with maximum entropy as our classifier. For evaluation, we will m"
W08-2137,H05-1066,0,0.0268603,"Missing"
W08-2137,P05-1013,0,0.0350793,"problem of mapping various dependency relations into semantic roles. Compared with previous researches on constituent structure tree-based SRL which adopts constituents as labeling units, dependency tree-based SRL adopts dependency relations as labeling units. Due to the difference between constituent structure trees and dependency trees, their feature spaces are expected to be somewhat different. In the CoNLL 2008 shared task, we extend the framework by Hacioglu (2004) with maximum entropy as our classifier. For evaluation, we will mainly report our official SRL performance using MaltParser (Nivre and Nilsson, 2005). Besides, we will also present our unofficial system by 1) applying a new effective pruning algorithm; 2) including additional features; and 3) adopting a better dependency parser, MSTParser (McDonald, 2005). In the remainder of this paper, we will briefly describe our system architecture, present various features used by our models and report the performance on CoNLL 2008 shared task (both official and unofficial). 253 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253–257 Manchester, August 2008 2 System Description In CoNLL 2008 shared task"
W08-2137,P05-1072,0,0.0300297,"Missing"
W08-2137,W05-0625,0,0.0238577,"chieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers turn to dependency tree-bas"
W08-2137,W08-2121,0,\N,Missing
W08-2137,W04-3212,0,\N,Missing
W08-2137,N04-1030,0,\N,Missing
W10-4158,S07-1012,0,0.0169028,"name segmentation errors. 3 Data sets z Training dataset: They contain about 30 Chinese personal names, and a document set of about 100-300 news articles from collection of Xinhua news documents in a time span of fourteen years are provided for each personal name. z External dataset: Chinese Wikipedia2 personal attribution (Cucerzan, 2007; Nguyen and Cao,2008). z Test dataset: There are about 26 Chinese personal names, which are similar to train data sets. 4 Experiments The systems that run on test dataset are evaluated by both B-Cubed (Bagga and Baldwin, 1998; Artiles et al.,2009A) and P-IP (Artiles et al., 2007 ;Artiles et al.,2009A). And the systems that run on training dataset were only evaluated by B-Cubed. In experiments, we firstly evaluate the performance of name segmentation error detection on the training dataset. For comparison, we additionally perform another detection method which only using Name Entity Identifcation (NER CRF++ tools) to distinguish name-strings from the discarded ones. The results are shown in table 1. We can find that our error detection method can achieve more recall than NER, but lower precision. Besides, we evaluate the performance of the two-stage clustering in the"
W10-4158,D09-1056,0,0.0246393,"Missing"
W10-4158,P98-1012,0,0.266632,"r each document, we generate its Jumping Tree, and regard the nodes in the tree as the features. After that, we combine the weights of the same features and normalized the value by dividing that by the average weight in the specific class of features. Based on the two classes of features, given a target string and the document where it occurs, the detection component firstly generate the Jumping Tree of the document, and then determines whether the string is person name or things by measuring the similarity of the tree to the classes of features. Here, we simply use the VSM and Cosine metric ΰBagga and Baldwin, 1998α to obtain the similarity. The second component, viz. DJ-based person name disambiguation, firstly generates the Jumping trees for all documents that involve specific person name. And a two-stage clustering algorithm is adopted to divide the documents and refer each cluster to a person. The first stage of the algorithm runs a strict division which focuses on obtaining high precision. The second stage performs a soft division which is used to improve recall. The two-stage clustering algorithm(Ikeda et al.,2009) initially obtains the optimal parameters that respectively refer to the maximum pre"
W10-4158,W06-0116,0,0.0717938,"Missing"
W10-4158,D07-1074,0,0.126784,"be roughly divided into different clusters of Named Entities without name segmentation errors. After that, we additionally adopt the two-stage clustering algorithm to further divide each cluster. Thus we can deal with the issue of disambiguation without the interruption of name segmentation errors. 3 Data sets z Training dataset: They contain about 30 Chinese personal names, and a document set of about 100-300 news articles from collection of Xinhua news documents in a time span of fourteen years are provided for each personal name. z External dataset: Chinese Wikipedia2 personal attribution (Cucerzan, 2007; Nguyen and Cao,2008). z Test dataset: There are about 26 Chinese personal names, which are similar to train data sets. 4 Experiments The systems that run on test dataset are evaluated by both B-Cubed (Bagga and Baldwin, 1998; Artiles et al.,2009A) and P-IP (Artiles et al., 2007 ;Artiles et al.,2009A). And the systems that run on training dataset were only evaluated by B-Cubed. In experiments, we firstly evaluate the performance of name segmentation error detection on the training dataset. For comparison, we additionally perform another detection method which only using Name Entity Identifcat"
Y06-1055,C00-1055,0,0.0427839,"Missing"
Y06-1055,H94-1019,0,0.107595,"Missing"
Y06-1055,2001.mtsummit-papers.67,0,0.0711756,"Missing"
Y06-1055,2001.mtsummit-papers.3,0,0.0325151,"Missing"
Y06-1055,2001.mtsummit-papers.25,0,0.127745,"Missing"
Y06-1055,N03-2021,0,\N,Missing
