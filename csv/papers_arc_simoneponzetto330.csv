2021.emnlp-main.615,Come hither or go away? Recognising pre-electoral coalition signals in the news,2021,-1,-1,2,0,5564,ines rehbein,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the news coverage leading up to an election the (un)willingness of political parties to form a government coalition. We decompose our problem into two related, but distinct tasks: (i) predicting whether a reported statement from a politician or a journalist refers to a potential coalition and (ii) predicting the polarity of the signal {--} namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of multi-task learning and investigate which setup and task formulation is best suited for each sub-task. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, Germany, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline."
2021.eacl-main.56,{F}ake{F}low: Fake News Detection by Modeling the Flow of Affective Information,2021,-1,-1,2,0,524,bilal ghanem,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Fake news articles often stir the readers{'} attention by means of emotional appeals that arouse their feelings. Unlike in short news texts, authors of longer articles can exploit such affective factors to manipulate readers by adding exaggerations or fabricating events, in order to affect the readers{'} emotions. To capture this, we propose in this paper to model the flow of affective information in fake news articles using a neural architecture. The proposed model, FakeFlow, learns this flow by combining topic and affective information extracted from text. We evaluate the model{'}s performance with several experiments on four real-world datasets. The results show that FakeFlow achieves superior results when compared against state-of-the-art methods, thus confirming the importance of capturing the flow of the affective information in news articles."
2021.eacl-demos.11,{D}eb{IE}: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces,2021,-1,-1,3,0,11028,niklas friedrich,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"Recent research efforts in NLP have demonstrated that distributional word vector spaces often encode stereotypical human biases, such as racism and sexism. With word representations ubiquitously used in NLP models and pipelines, this raises ethical issues and jeopardizes the fairness of language technologies. While there exists a large body of work on bias measures and debiasing methods, to date, there is no platform that would unify these research efforts and make bias measuring and debiasing of representation spaces widely accessible. In this work, we present DebIE, the first integrated platform for (1) measuring and (2) mitigating bias in word embeddings. Given an (i) embedding space (users can choose between the predefined spaces or upload their own) and (ii) a bias specification (users can choose between existing bias specifications or create their own), DebIE can (1) compute several measures of implicit and explicit bias and modify the embedding space by executing two (mutually composable) debiasing models. DebIE{'}s functionality can be accessed through four different interfaces: (a) a web application, (b) a desktop application, (c) a REST-ful API, and (d) as a command-line application. DebIE is available at: debie.informatik.uni-mannheim.de."
2020.wanlp-1.17,{A}ra{WEAT}: Multidimensional Analysis of Biases in {A}rabic Word Embeddings,2020,-1,-1,3,0.526316,7441,anne lauscher,Proceedings of the Fifth Arabic Natural Language Processing Workshop,0,"Recent work has shown that distributional word vector spaces often encode human biases like sexism or racism. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely: embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in embeddings trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available."
2020.semeval-1.2,{S}em{E}val-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment,2020,-1,-1,4,0.366273,7439,goran glavavs,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Lexical entailment (LE) is a fundamental asymmetric lexico-semantic relation, supporting the hierarchies in lexical resources (e.g., WordNet, ConceptNet) and applications like natural language inference and taxonomy induction. Multilingual and cross-lingual NLP applications warrant models for LE detection that go beyond language boundaries. As part of SemEval 2020, we carried out a shared task (Task 2) on multilingual and cross-lingual LE. The shared task spans three dimensions: (1) monolingual vs. cross-lingual LE, (2) binary vs. graded LE, and (3) a set of 6 diverse languages (and 15 corresponding language pairs). We offered two different evaluation tracks: (a) Dist: for unsupervised, fully distributional models that capture LE solely on the basis of unannotated corpora, and (b) Any: for externally informed models, allowed to leverage any resources, including lexico-semantic networks (e.g., WordNet or BabelNet). In the Any track, we recieved runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction."
2020.lrec-1.728,Word Sense Disambiguation for 158 Languages using Word Embeddings Only,2020,37,0,9,0,1966,varvara logacheva,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this task. However, (i) the inherent Zipfian distribution of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these languages. Models and system are available online."
S19-2018,{HHMM} at {S}em{E}val-2019 Task 2: Unsupervised Frame Induction using Contextualized Word Embeddings,2019,5,1,4,0,11070,saba anwar,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (Qasem-iZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages."
P19-4004,Computational Analysis of Political Texts: Bridging Research Efforts Across Communities,2019,0,1,3,0.464354,7439,goran glavavs,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"In the last twenty years, political scientists started adopting and developing natural language processing (NLP) methods more actively in order to exploit text as an additional source of data in their analyses. Over the last decade the usage of computational methods for analysis of political texts has drastically expanded in scope, allowing for a sustained growth of the text-as-data community in political science. In political science, NLP methods have been extensively used for a number of analyses types and tasks, including inferring policy position of actors from textual evidence, detecting topics in political texts, and analyzing stylistic aspects of political texts (e.g., assessing the role of language ambiguity in framing the political agenda). Just like in numerous other domains, much of the work on computational analysis of political texts has been enabled and facilitated by the development of resources such as, the topically coded electoral programmes (e.g., the Manifesto Corpus) or topically coded legislative texts (e.g., the Comparative Agenda Project). Political scientists created resources and used available NLP methods to process textual data largely in isolation from the NLP community. At the same time, NLP researchers addressed closely related tasks such as election prediction, ideology classification, and stance detection. In other words, these two communities have been largely agnostic of one another, with NLP researchers mostly unaware of interesting applications in political science and political scientists not applying cutting-edge NLP methodology to their problems. The main goal of this tutorial is to systematize and analyze the body of research work on political texts from both communities. We aim to provide a gentle, all-round introduction to methods and tasks related to computational analysis of political texts. Our vision is to bring the two research communities closer to each other and contribute to faster and more significant developments in this interdisciplinary research area."
P19-1490,Multilingual and Cross-Lingual Graded Lexical Entailment,2019,0,2,2,0.026334,4035,ivan vulic,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs."
K19-1024,Policy Preference Detection in Parliamentary Debate Motions,2019,0,0,4,0,6286,gavin abercrombie,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Debate motions (proposals) tabled in the UK Parliament contain information about the stated policy preferences of the Members of Parliament who propose them, and are key to the analysis of all subsequent speeches given in response to them. We attempt to automatically label debate motions with codes from a pre-existing coding scheme developed by political scientists for the annotation and analysis of political parties{'} manifestos. We develop annotation guidelines for the task of applying these codes to debate motions at two levels of granularity and produce a dataset of manually labelled examples. We evaluate the annotation process and the reliability and utility of the labelling scheme, finding that inter-annotator agreement is comparable with that of other studies conducted on manifesto data. Moreover, we test a variety of ways of automatically labelling motions with the codes, ranging from similarity matching to neural classification methods, and evaluate them against the gold standard labels. From these experiments, we note that established supervised baselines are not always able to improve over simple lexical heuristics. At the same time, we detect a clear and evident benefit when employing BERT, a state-of-the-art deep language representation model, even in classification scenarios with over 30 different labels and limited amounts of training data."
J19-3002,Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction,2019,145,2,4,1,756,dmitry ustalov,Computational Linguistics,0,"We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph, which reflects the {``}ambiguity{''} of its nodes. Then, it uses hard clustering to discover clusters in this {``}disambiguated{''} intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can also be applied to other networks of linguistic data."
D19-3034,{SEAGLE}: A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval,2019,0,0,3,0,26744,fabian schmidt,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"We introduce Seagle, a platform for comparative evaluation of semantic text encoding models on information retrieval (IR) tasks. Seagle implements (1) word embedding aggregators, which represent texts as algebraic aggregations of pretrained word embeddings and (2) pretrained semantic encoders, and allows for their comparative evaluation on arbitrary (monolingual and cross-lingual) IR collections. We benchmark Seagle{'}s models on monolingual document retrieval and cross-lingual sentence retrieval. Seagle functionality can be exploited via an easy-to-use web interface and its modular backend (micro-service architecture) can easily be extended with additional semantic search models."
W18-5206,An Argument-Annotated Corpus of Scientific Publications,2018,0,5,3,0.526316,7441,anne lauscher,Proceedings of the 5th Workshop on Argument Mining,0,"Argumentation is an essential feature of scientific language. We present an annotation study resulting in a corpus of scientific publications annotated with argumentative components and relations. The argumentative annotations have been added to the existing Dr. Inventor Corpus, already annotated for four other rhetorical aspects. We analyze the annotated argumentative structures and investigate the relations between argumentation and other rhetorical aspects of scientific writing, such as discourse roles and citation contexts."
S18-1132,{U}ni{M}a at {S}em{E}val-2018 Task 7: Semantic Relation Extraction and Classification from Scientific Publications,2018,0,0,7,0,28865,thorsten keiper,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"Large repositories of scientific literature call for the development of robust methods to extract information from scholarly papers. This problem is addressed by the SemEval 2018 Task 7 on extracting and classifying relations found within scientific publications. In this paper, we present a feature-based and a deep learning-based approach to the task and discuss the results of the system runs that we submitted for evaluation."
P18-2010,Unsupervised Semantic Frame Induction using Triclustering,2018,20,3,5,1,756,dmitry ustalov,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task."
L18-1093,Enriching Frame Representations with Distributionally Induced Senses,2018,17,0,4,1,17226,stefano faralli,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce a new lexical resource that enriches the Framester knowledge graph, which links Framnet, WordNet, VerbNet and other resources, with semantic features from text corpora. These features are extracted from distributionally induced sense inventories and subsequently linked to the manually-constructed frame representations to boost the performance of frame disambiguation in context. Since Framester is a frame-based knowledge graph, which enables full-fledged OWL querying and reasoning, our resource paves the way for the development of novel, deeper semantic-aware applications that could benefit from the combination of knowledge from text and complex symbolic representations of events and participants. Together with the resource we also provide the software we developed for the evaluation in the task of Word Frame Disambiguation (WFD)."
L18-1164,An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages,2018,13,1,6,1,756,dmitry ustalov,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this paper, we present Watasense, an unsupervised system for word sense disambiguation. Given a sentence, the system chooses the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index."
L18-1244,Improving Hypernymy Extraction with Distributional Semantic Classes,2018,0,1,4,1,1663,alexander panchenko,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this paper, we show for the first time how distributionally-induced semantic classes can be helpful for extraction of hypernyms. We present a method for (1) inducing sense-aware semantic classes using distributional semantics and (2) using these induced semantic classes for filtering noisy hypernymy relations. Denoising of hypernyms is performed by labeling each semantic class with its hypernyms. On one hand, this allows us to filter out wrong extractions using the global structure of the distributionally similar senses. On the other hand, we infer missing hypernyms via label propagation to cluster terms. We conduct a large-scale crowdsourcing study showing that processing of automatically extracted hypernyms using our approach improves the quality of the hypernymy extraction both in terms of precision and recall. Furthermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a benchmarking dataset."
L18-1286,Building a Web-Scale Dependency-Parsed Corpus from {C}ommon{C}rawl,2018,-1,-1,4,1,1663,alexander panchenko,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1321,{MI}s{A}: Multilingual {``}{I}s{A}{''} Extraction from Corpora,2018,0,0,3,1,17226,stefano faralli,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1615,{CATS}: A Tool for Customized Alignment of Text Simplification Corpora,2018,0,7,4,0.326991,24998,sanja vstajner,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1370,Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models,2018,0,1,3,0.526316,7441,anne lauscher,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Exponential growth in the number of scientific publications yields the need for effective automatic analysis of rhetorical aspects of scientific writing. Acknowledging the argumentative nature of scientific text, in this work we investigate the link between the argumentative structure of scientific publications and rhetorical aspects such as discourse categories or citation contexts. To this end, we (1) augment a corpus of scientific publications annotated with four layers of rhetoric annotations with argumentation annotations and (2) investigate neural multi-task learning architectures combining argument extraction with a set of rhetorical classification tasks. By coupling rhetorical classifiers with the extraction of argumentative components in a joint multi-task learning setting, we obtain significant performance gains for different rhetorical analysis tasks."
W17-6809,If Sentences Could See: Investigating Visual Information for Semantic Textual Similarity,2017,38,2,3,1,7439,goran glavavs,{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers,0,None
W17-5030,Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers,2017,-1,-1,4,0.326991,24998,sanja vstajner,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Eye tracking studies from the past few decades have shaped the way we think of word complexity and cognitive load: words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability)."
W17-2906,Cross-Lingual Classification of Topics in Political Texts,2017,-1,-1,3,1,7439,goran glavavs,Proceedings of the Second Workshop on {NLP} and Computational Social Science,0,"In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we exploit continuous semantic text representations and induce a joint multilingual semantic vector spaces to enable supervised learning using manually-coded sentences across different languages. Our experimental results show that classifiers trained on multilingual data yield performance boosts over monolingual topic classification."
W17-1909,Using Linked Disambiguated Distributional Networks for Word Sense Disambiguation,2017,28,5,3,1,1663,alexander panchenko,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,"We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks reduces the sparsity of sense representations used for WSD. We evaluate these enriched representations within two lexical sample sense disambiguation benchmarks. Our results indicate that (1) features extracted from the corpus-based resource help to significantly outperform a model based solely on the lexical resource; (2) our method achieves results comparable or better to four state-of-the-art unsupervised knowledge-based WSD systems including three hybrid systems that also rely on text corpora. In contrast to these hybrid methods, our approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems."
P17-2014,Exploring Neural Text Simplification Models,2017,10,26,3,0,18363,sergiu nisioi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems"
P17-2016,Sentence Alignment Methods for Improving Text Simplification Systems,2017,7,4,3,0.326991,24998,sanja vstajner,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We provide several methods for sentence-alignment of texts with different complexity levels. Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems. We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems."
E17-2083,Improving Neural Knowledge Base Completion with Cross-Lingual Projections,2017,27,5,2,0,33003,patrick klein,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"In this paper we present a cross-lingual extension of a neural tensor network model for knowledge base completion. We exploit multilingual synsets from BabelNet to translate English triples to other languages and then augment the reference knowledge base with cross-lingual triples. We project monolingual embeddings of different languages to a shared multilingual space and use them for network initialization (i.e., as initial concept embeddings). We then train the network with triples from the cross-lingually augmented knowledge base. Results on WordNet link prediction show that leveraging cross-lingual information yields significant gains over exploiting only monolingual triples."
E17-2109,Unsupervised Cross-Lingual Scaling of Political Texts,2017,16,6,3,1,7439,goran glavavs,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Political text scaling aims to linearly order parties and politicians across political dimensions (e.g., left-to-right ideology) based on textual content (e.g., politician speeches or party manifestos). Existing models scale texts based on relative word usage and cannot be used for cross-lingual analyses. Additionally, there is little quantitative evidence that the output of these models correlates with common political dimensions like left-to-right orientation. Experimental results show that the semantically-informed scaling models better predict the party positions than the existing word-based models in two different political dimensions. Furthermore, the proposed models exhibit no drop in performance in the cross-lingual compared to monolingual setting."
E17-1009,Unsupervised Does Not Mean Uninterpretable: The Case for Word Sense Induction and Disambiguation,2017,56,16,4,1,1663,alexander panchenko,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"The current trend in NLP is the use of highly opaque models, e.g. neural networks and word embeddings. While these models yield state-of-the-art results on a range of tasks, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable model that matches the state-of-the-art models in accuracy. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels: word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other unsupervised systems while offering the possibility to justify its decisions in human-readable form."
E17-1056,The {C}ontrast{M}edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links,2017,40,6,4,1,17226,stefano faralli,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input knowledge graph is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction."
D17-2016,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",2017,12,10,6,1,1663,alexander panchenko,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration."
D17-1185,Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations,2017,28,14,2,1,7439,goran glavavs,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Detection of lexico-semantic relations is one of the central tasks of computational semantics. Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike. In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embeddings via a pair of tensors. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on hypernymy and meronymy detection suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the model can account for polysemy and that it exhibits stable performance across languages."
D17-1318,Topic-Based Agreement and Disagreement in {US} Electoral Manifestos,2017,24,5,3,0,5466,stefano menini,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present a topic-based analysis of agreement and disagreement in political manifestos, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in computational social science."
S16-2016,Unsupervised Text Segmentation Using Semantic Relatedness Graphs,2016,19,13,3,1,7439,goran glavavs,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"Segmenting text into semantically coherentn fragments improves readability of textn and facilitates tasks like text summarizationn and passage retrieval. In this paper,n we present a novel unsupervised algorithmn for linear text segmentation (TS)n that exploits word embeddings and a measuren of semantic relatedness of short textsn to construct a semantic relatedness graphn of the document. Semantically coherentn segments are then derived from maximaln cliques of the relatedness graph. The algorithmn performs competitively on a standardn synthetic dataset and outperforms then best-performing method on a real-worldn (i.e., non-artificial) dataset of political manifestos."
S16-1206,"{TAXI} at {S}em{E}val-2016 Task 13: a Taxonomy Induction Method based on Lexico-Syntactic Patterns, Substrings and Focused Crawling",2016,32,19,7,1,1663,alexander panchenko,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"We present a system for taxonomy construction that reached the first place in all subtasks of the SemEval 2016 challenge on Taxonomy Extraction Evaluation. Our simple yet effective approach harvests hypernyms with substring inclusion and Hearst-style lexicosyntactic patterns from domain-specific texts obtained via language model based focused crawling. Extracted taxonomies are evaluated on English, Dutch, French and Italian for three domains each (Food, Environment and Science). Evaluations against a gold standard and by human judgment show that our method outperforms more complex and knowledge-rich approaches on most domains and languages. Furthermore, to adapt the method to a new domain or language, only a small amount of manual labour is needed."
L16-1056,A Large {D}ata{B}ase of Hypernymy Relations Extracted from the Web.,2016,20,34,7,0,34788,julian seitner,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Hypernymy relations (those where an hyponym term shares a {``}isa{''} relationship with his hypernym) play a key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, such relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. We present a publicly available database containing more than 400 million hypernymy relations we extracted from the CommonCrawl web corpus. We describe the infrastructure we developed to iterate over the web corpus for extracting the hypernymy relations and store them effectively into a large database. This collection of relations represents a rich source of knowledge and may be useful for many researchers. We offer the tuple dataset for public download and an Application Programming Interface (API) to help other researchers programmatically query the database."
W15-2808,Image with a Message: Towards Detecting Non-Literal Image Usages by Visual Linking,2015,24,1,3,0,36898,lydia weiland,Proceedings of the Fourth Workshop on Vision and Language,0,"A key task to understand an image and itsn corresponding caption is not only to findn out what is shown on the picture and describedn in the text, but also what is then exact relationship between these two elements.n The long-term objective of ourn work is to be able to distinguish differentn types of relationship, including literaln vs. non-literal usages, as well as finegrainedn non-literal usages (i.e., symbolicn vs. iconic). Here, we approach this challengingn problem by answering the question:n xe2x80x98How can we quantify the degreesn of similarity between the literal meaningsn expressed within images and their captions?xe2x80x99.n We formulate this problem as an ranking task, where links between entitiesn and potential regions are created andn ranked for relevance. Using a Rankingn SVM allows us to leverage from the preferencen ordering of the links, which help usn in the similarity calculation for the casesn of visual or textual ambiguity, as well asn misclassified data. Our experiments shown that aggregating different features using an supervised ranker achieves better resultsn than a baseline knowledge-base method.n However, much work still lies ahead, andn we accordingly conclude the paper with an detailed discussion of a short- and longtermn outlook on how to push our work onn relationship classification one step further."
W14-5410,Weakly supervised construction of a repository of iconic images,2014,8,1,3,0,36898,lydia weiland,Proceedings of the Third Workshop on Vision and Language,0,"We present a first attempt at semi-automatically harvesting a dataset of iconic images, namelyn images that depict objects or scenes, which arouse associations to abstract topics. Our methodn starts with representative topic-evoking images from Wikipedia, which are labeled with relevantn concepts and entities found in their associated captions. These are used to query an online imagen repository (i.e., Flickr), in order to further acquire additional examples of topic-specific iconicn relations. To this end, we leverage a combination of visual similarity measures, image clusteringn and matching algorithms to acquire clusters of iconic images that are topically connected to then original seed images, while also allowing for various degrees of diversity. Our first results aren promising in that they indicate the feasibility of the task and that we are able to build a firstn version of our resource with minimal supervision."
titze-etal-2014-dbpedia,{DB}pedia Domains: augmenting {DB}pedia with domain information,2014,22,3,4,0,39488,gregor titze,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present an approach for augmenting DBpedia, a very large ontology lying at the heart of the Linked Open Data (LOD) cloud, with domain information. Our approach uses the thematic labels provided for DBpedia entities by Wikipedia categories, and groups them based on a kernel based k-means clustering algorithm. Experiments on gold-standard data show that our approach provides a first solution to the automatic annotation of DBpedia entities with domain labels, thus providing the largest LOD domain-annotated ontology to date."
P13-5004,Exploiting Social Media for Natural Language Processing: Bridging the Gap between Language-centric and Real-world Applications,2013,22,0,1,1,9871,simone ponzetto,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials),0,"Social media like Twitter and micro-blogs provide a goldmine of text, shallow markup annotations and network structure. These information sources can all be exploited together in order to automatically acquire vast amounts of up-to-date, widecoverage structured knowledge. This knowledge, in turn, can be used to measure the pulse of a variety of social phenomena like political events, activism and stock prices, as well as to detect emerging events such as natural disasters (earthquakes, tsunami, etc.)."
P12-3012,Multilingual {WSD} with Just a Few Lines of Code: the {B}abel{N}et {API},2012,32,31,2,0.222222,1617,roberto navigli,Proceedings of the {ACL} 2012 System Demonstrations,0,In this paper we present an API for programmatic access to BabelNet -- a wide-coverage multilingual lexical knowledge base -- and multilingual knowledge-rich Word Sense Disambiguation (WSD). Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction.
D12-1128,Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation,2012,48,49,2,0.222222,1617,roberto navigli,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We present a multilingual joint approach to Word Sense Disambiguation (WSD). Our method exploits BabelNet, a very large multilingual knowledge base, to perform graph-based WSD across different languages, and brings together empirical evidence from these languages using ensemble methods. The results show that, thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods, we are able to achieve the state of the art in both monolingual and multilingual WSD settings."
W10-2415,Assessing the Challenge of Fine-Grained Named Entity Recognition and Classification,2010,31,26,4,0,363,asif ekbal,Proceedings of the 2010 Named Entities Workshop,0,Named Entity Recognition and Classification (NERC) is a well-studied NLP task typically focused on coarse-grained named entity (NE) classes. NERC for more fine-grained semantic NE classes has not been systematically studied. This paper quantifies the difficulty of fine-grained NERC (FG-NERC) when performed at large scale on the people domain. We apply unsupervised acquisition methods to construct a gold standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a 'strong' baseline for future research.
S10-1021,{BART}: A Multilingual Anaphora Resolution System,2010,10,29,3,0,10004,samuel broscheit,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"BART (Versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering. For the SemEval task 1 on Coreference Resolution, BART runs have been submitted for German, English, and Italian.n n BART relies on a maximum entropy-based classifier for pairs of mentions. A novel entity-mention approach based on Semantic Trees is at the moment only supported for English."
S10-1027,{UHD}: Cross-Lingual Word Sense Disambiguation Using Multilingual Co-Occurrence Graphs,2010,11,12,2,0,18053,carina silberer,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We describe the University of Heidelberg (UHD) system for the Cross-Lingual Word Sense Disambiguation SemEval-2010 task (CL-WSD). The system performs CL-WSD by applying graph algorithms previously developed for monolingual Word Sense Disambiguation to multilingual co-occurrence graphs. UHD has participated in the Best and out-of-five (OOF) evaluations and ranked among the most competitive systems for this task, thus indicating that graph-based approaches represent a powerful alternative for this task."
P10-1023,{B}abel{N}et: Building a Very Large Multilingual Semantic Network,2010,41,308,2,0.222222,1617,roberto navigli,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present BabelNet -- a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition Machine Translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource."
P10-1154,Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems,2010,50,168,1,1,9871,simone ponzetto,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"One of the main obstacles to high-performance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets."
broscheit-etal-2010-extending,Extending {BART} to Provide a Coreference Resolution System for {G}erman,2010,26,10,2,0,10004,samuel broscheit,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a flexible toolkit-based approach to automatic coreference resolution on German text. We start with our previous work aimed at reimplementing the system from Soon et al. (2001) for English, and extend it to duplicate a version of the state-of-the-art proposal from Klenner and Ailloud (2009). Evaluation performed on a benchmarking dataset, namely the TueBa-D/Z corpus (Hinrichs et al., 2005b), shows that machine learning based coreference resolution can be robustly performed in a language other than English."
P09-5006,State-of-the-art {NLP} Approaches to Coreference Resolution: Theory and Practical Recipes,2009,10,12,1,1,9871,simone ponzetto,Tutorial Abstracts of {ACL}-{IJCNLP} 2009,0,The identification of different nominal phrases in a discourse as used to refer to the same (discourse) entity is essential for achieving robust natural language understanding (NLU). The importance of this task is directly amplified by the field of Natural Language Processing (NLP) currently moving towards high-level linguistic tasks requiring NLU capabilities such as e.g. recognizing textual entailment. This tutorial aims at providing the NLP community with a gentle introduction to the task of coreference resolution from both a theoretical and an application-oriented perspective. Its main purposes are: (1) to introduce a general audience of NLP researchers to the core ideas underlying state-of-the-art computational models of coreference; (2) to provide that same audience with an overview of NLP applications which can benefit from coreference information.
N09-4004,Extracting World and Linguistic Knowledge from {W}ikipedia,2009,0,2,1,1,9871,simone ponzetto,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts",0,"Many research efforts have been devoted to develop robust statistical modeling techniques for many NLP tasks. Our field is now moving towards more complex tasks (e.g. RTE, QA), which require to complement these methods with a semantically rich representation based on world and linguistic knowledge (i.e. annotated linguistic data). In this tutorial we show several approaches to extract this knowledge from Wikipedia. This resource has attracted the attention of much work in the AI community, mainly because it provides semi-structured information and a large amount of manual annotations. The purpose of this tutorial is to introduce Wikipedia as a resource to the NLP community and to provide an introduction for NLP researchers both from a scientific and a practical (i.e. data acquisition and processing issues) perspective."
P08-4003,{BART}: A Modular Toolkit for Coreference Resolution,2008,26,17,2,0,4696,yannick versley,Proceedings of the {ACL}-08: {HLT} Demo Session,0,"Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al. (2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers."
versley-etal-2008-bart-modular,{BART}: A modular toolkit for coreference resolution,2008,26,17,2,0,4696,yannick versley,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort. Accordingly, there is very limited availability of off-the shelf tools for researchers whose interests are not primarily in coreference or others who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of Soon et al.s proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers. BART has been released as open source software and is available from http://www.sfs.uni-tuebingen.de/{\textasciitilde}versley/BART"
P07-2013,An {API} for Measuring the Relatedness of Words in {W}ikipedia,2007,21,18,1,1,9871,simone ponzetto,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,None
N07-3003,Creating a Knowledge Base from a Collaboratively Generated Encyclopedia,2007,21,12,1,1,9871,simone ponzetto,Proceedings of the {NAACL}-{HLT} 2007 Doctoral Consortium,0,"We present our work on using Wikipedia as a knowledge source for Natural Language Processing. We first describe our previous work on computing semantic relatedness from Wikipedia, and its application to a machine learning based coreference resolution system. Our results suggest that Wikipedia represents a semantic resource to be treasured for NLP applications, and accordingly present the work directions to be explored in the future."
N06-1025,"Exploiting Semantic Role Labeling, {W}ord{N}et and {W}ikipedia for Coreference Resolution",2006,30,209,1,1,9871,simone ponzetto,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns."
E06-2015,Semantic Role Labeling for Coreference Resolution,2006,12,30,1,1,9871,simone ponzetto,Demonstrations,0,Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance.
W05-0633,Semantic Role Labeling Using Lexical Statistical Information,2005,6,3,1,1,9871,simone ponzetto,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"Our system for semantic role labeling is multi-stage in nature, being based on tree pruning techniques, statistical methods for lexicalised feature encoding, and a C4.5 decision tree classifier. We use both shallow and deep syntactic information from automatically generated chunks and parse trees, and develop a model for learning the semantic arguments of predicates as a multi-class decision problem. We evaluate the performance on a set of relatively 'cheap' features and report an F1 score of 68.13% on the overall test set."
