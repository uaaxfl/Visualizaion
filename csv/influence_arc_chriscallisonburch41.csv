2001.mtsummit-papers.12,J93-2003,0,0.0041013,"Missing"
2001.mtsummit-road.3,C86-1077,0,0.0806238,"al (IR), interactive keyword expansion and human rating of search results are two methods of querying the user for information which can then be used to improve system performance (Koenemann & Belkin, 1996; Schatz, et al., 1996). In speech recognition, mechanisms are provided for users to correct mistranscriptions and thereby dynamically retrain speech models with the corrections (IBM, 2000). And in the field of MT, human interaction has been proposed as a way to disambiguate words, choose between multiple parses, and provide missing information for target language generation (Blanchon, 1997; Whitelock, et al., 1986; Yamabana, 1997). Limits However, it is notoriously difficult to generate feedback queries which clearly and correctly prompt for the necessary information and which are understandable by the average user. And worse yet, some information is essentially impossible for a monolingual user to provide, regardless of the feedback queries which the system presents. Bond & Ikehara (1996) give the example of differentiating count and mass nouns and selecting determiners when translating from Japanese into English. The countability of a noun like “scales” depends on the dialect of the reader and cannot"
2004.eamt-1.4,C88-1016,0,0.0501204,"stem improves by dynamically learning the correct translations of new phrases. These new phrases are extracted from the corrected sentence pair using the existing translation models, and can be used immediately for subsequent translations. • The second is through a mechanism that allows an advanced user to inspect which phrases the system&apos;s translation was composed from. If a particular phrase was mistranslated, the user can examine the set of sentence pairs that a particular translation was learned from, and make corrections. Introduction Statistical machine translation was first proposed in Brown et al (1988). Since statistical machine translation systems are created by automatically analyzing a corpus of example translations they have a number of advantages over systems that are built using more traditional approaches to MT: • They make few linguistic assumptions and can therefore be applied to nearly any language pair, given a sufficiently large corpus. • They can be developed in a matter of weeks or days, whereas systems that are hand-crafted by linguists and lexicographers can take years. • They can be improved with little additional effort as more data becomes available. More recent advances"
2004.eamt-1.4,J93-2003,0,0.00578124,"es example output produced by a system trained on data from the Canadian parliament. Section 3 shows how our system dynamically integrates edited output by extracting the translations of new phrases, and weighting the corrected translations more heavily than existing translations in the model. Section 4 described the advanced editing technique that allows a user to inspect the sentence pairs which a faulty translation was learned from, and correct the statistical models by explicitly showing the system which phrases ought to be learned from those sentence pairs instead. 2. 2.1 Word Alignments Brown et al (1993) define a series of translation models, which are commonly referred to as IBM Models 1 to 5. The IBM Models formulate translation essentially as a word-level operation. The probability that a foreign sentence is the translation of an English sentence is calculating by summing over the probabilities of all possible wordlevel alignments, a, between the sentences: p(f|e) = ∑a p(f,a|e) Phrase-based Statistical Translation Thus they decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the"
2004.eamt-1.4,P04-1023,1,0.771564,"ountC2(f,e) / λ1 ∑f countC1(f,e) + λ2 ∑f countC2(f,e) The λ1 and λ2 co-efficients allow us to scale the contribution of phrase alignments extracted from the newly created corpus (C2) of edited translations, and downweight the counts of phrases from the original training corpus (C1). This allows us to create a system from existing public domain data, such as the 30 learned from we hope to demystify statistical machine translation. A user can find out why our system translated a sentence in a certain way, and change the behavior of the system by altering its underlying representation. Moreover, Callison-Burch et al (2004) shows that training data that has been manually aligned on the word-level can be used to significantly improve the alignment accuracy and the translation quality of statistical machine translation. Thus, by incorporating the manual modifications of our training data into the next round of training, we hope to improve the word alignments of the rest of the sentence pairs as well. was translated by our system as The crime punishable by was abolished in France during François Mitterand. Simple editing could be performed to change the translation to The death penalty was abolished in France under"
2004.eamt-1.4,N03-1017,0,0.0137562,"ated by automatically analyzing a corpus of example translations they have a number of advantages over systems that are built using more traditional approaches to MT: • They make few linguistic assumptions and can therefore be applied to nearly any language pair, given a sufficiently large corpus. • They can be developed in a matter of weeks or days, whereas systems that are hand-crafted by linguists and lexicographers can take years. • They can be improved with little additional effort as more data becomes available. More recent advances in phrase-based approaches to statistical translation (Koehn et al (2003), Marcu and Wong (2002), Och et al (1999)) have led to a dramatic increase in the quality of the translation systems. Phrase-based translation systems produce higher-quality translation since they use longer segments of human translated text. Using longer segments of human translated text reduces problems associated with literal word-for-word translations. For example, multi-word expressions such as idioms are better translated. Linear B is a commercial provider of statistical machine translation systems. This paper These features mean that our system is capable of improving with use and adapt"
2004.eamt-1.4,W02-1018,0,0.053121,"y analyzing a corpus of example translations they have a number of advantages over systems that are built using more traditional approaches to MT: • They make few linguistic assumptions and can therefore be applied to nearly any language pair, given a sufficiently large corpus. • They can be developed in a matter of weeks or days, whereas systems that are hand-crafted by linguists and lexicographers can take years. • They can be improved with little additional effort as more data becomes available. More recent advances in phrase-based approaches to statistical translation (Koehn et al (2003), Marcu and Wong (2002), Och et al (1999)) have led to a dramatic increase in the quality of the translation systems. Phrase-based translation systems produce higher-quality translation since they use longer segments of human translated text. Using longer segments of human translated text reduces problems associated with literal word-for-word translations. For example, multi-word expressions such as idioms are better translated. Linear B is a commercial provider of statistical machine translation systems. This paper These features mean that our system is capable of improving with use and adapting to be more appropri"
2004.eamt-1.4,J03-1002,0,0.00432676,"e Canada devrait jouer un plus grand rôle à cet égard. p(f|e) = count(f,e) / ∑f count(f,e) The trick is how to go about extracting the counts for phrase alignments from a training corpus. Many methods for calculating phrase alignments use word-level alignments as a starting point.1 There are various heuristics for extracting phrase alignments from word alignments, some are described in Koehn (2003), Tillmann (2003), and Vogel et al (2003). The online version of this paper gives a graphical illustration of the method of extracting incrementally larger phrases2 from word alignments described in Och and Ney (2003). Counts are collected over phrases extracted from word alignments of all sentence pairs in the training corpus. These counts are then used to calculate phrasal translation probabilities. The act of translating with phrase-based translation model involves breaking an input sentence into all possible substrings, looking up all translations that were aligned with each substring in the training corpus, and then searching through all possible translations to find the best translation of source sentence. Au Canada, nous disposons en abondance de blé et d’autres produits alimentaires. Nous pouvons é"
2004.eamt-1.4,W99-0604,0,0.0382433,"example translations they have a number of advantages over systems that are built using more traditional approaches to MT: • They make few linguistic assumptions and can therefore be applied to nearly any language pair, given a sufficiently large corpus. • They can be developed in a matter of weeks or days, whereas systems that are hand-crafted by linguists and lexicographers can take years. • They can be improved with little additional effort as more data becomes available. More recent advances in phrase-based approaches to statistical translation (Koehn et al (2003), Marcu and Wong (2002), Och et al (1999)) have led to a dramatic increase in the quality of the translation systems. Phrase-based translation systems produce higher-quality translation since they use longer segments of human translated text. Using longer segments of human translated text reduces problems associated with literal word-for-word translations. For example, multi-word expressions such as idioms are better translated. Linear B is a commercial provider of statistical machine translation systems. This paper These features mean that our system is capable of improving with use and adapting to be more appropriate for a new doma"
2004.eamt-1.4,W03-1001,0,0.025608,"Missing"
2004.tc-1.10,J93-2003,0,0.0157984,"this string, the system will not be able to retrieve translations for these from the database. Having a translation database that can be freely searched, and is able to return previous translations of a user’s input where that input (or a part of that input) matches only a part of a previous segment promises to greatly increase the usefulness of the archive to a translator. This paper describes a system that offers precisely that facility. This tool is the result of research into exploiting translation memories for the building of machine translation systems. Statistical machine translation (Brown et al., 1993) is a data-driven approach to machine translation. Rather than relying on the more conventional approach of having a team of linguists and lexicographers laboriously hand-craft rules, statistical machine translation systems are created by automatically analyzing a parallel corpus. A parallel corpus is a collection of example translations which have been aligned on the sentence level, such as a translation memory. By examining the co-occurrence of the words in one language paired with words in another language a bilingual dictionary is induced. The grammatical structure of the languages is very"
2004.tc-1.10,W02-1018,0,0.0168837,"tence pairs it phrase translation 1 occurred in → possible sentence pairs it translation 2 occurred in After we have built such an index, it is a simple matter to query it with a certain source phrase, retrieve all possible translations and their contexts, and rank the translations using the phrase translation probability calculation described in Section 2.2. We can then build suitable interfaces that will allow a translator to search the database via a simple search window, or through some other mechanism. 1 There are other ways of calculating phrasal translation probabilities. For instance, Marcu and Wong (2002) estimate them directly rather than starting from word-level alignments. 2 Note that the ‘phrases’ in phrase-based translation are not congruous with the traditional notion of syntactic constituents; they might be more aptly described as ‘substrings’ or ‘blocks’. PRECISION = number of phrases which we correctly retrieved / total number of phrases that we retrieved RECALL = number of phrases in gold standard which we retrieved / total number of phrases in gold standard Figure 4 gives the a sample of results for the experiment. The first column shows the German phrase, the second column shows th"
2004.tc-1.10,J03-1002,0,0.004023,"text sentences from which the translation was extracted may also optionally be displayed, as shown in Figure 1. 3 The trick is how to go about extracting the counts for phrase alignments from a training corpus. Many methods for calculating phrase alignments use word-level alignments as a starting point.1 There are various heuristics for extracting phrase alignments from word alignments, some are described in Koehn (2003), Tillmann (2003), and Vogel et al. (2003). Figure 3 gives a graphical illustration of the method of extracting incrementally larger phrases2 from word alignments described in Och and Ney (2003). Counts are collected over phrases extracted from word alignments of all sentence pairs in the training corpus. These counts are then used to calculate phrasal translation probabilities. 2.3 Evaluation In order to evaluate our searchable translation memory we first constructed a sentence-aligned translation memory using 50,000 sentences from the German-English section of Europarl Corpus (Koehn, 2002). We selected a set of 120 German phrases to use as query terms, and retrieved all sentence pairs containing those phrases. We had two bilingual native German speakers manually align the German ph"
2004.tc-1.10,W03-1001,0,0.0139508,"ount(f¯, e¯) p(f¯|¯ e) = P ¯ ¯) f¯ count(f , e The results of the search can be presented as a list of possible translations with the most probable translation first. The context sentences from which the translation was extracted may also optionally be displayed, as shown in Figure 1. 3 The trick is how to go about extracting the counts for phrase alignments from a training corpus. Many methods for calculating phrase alignments use word-level alignments as a starting point.1 There are various heuristics for extracting phrase alignments from word alignments, some are described in Koehn (2003), Tillmann (2003), and Vogel et al. (2003). Figure 3 gives a graphical illustration of the method of extracting incrementally larger phrases2 from word alignments described in Och and Ney (2003). Counts are collected over phrases extracted from word alignments of all sentence pairs in the training corpus. These counts are then used to calculate phrasal translation probabilities. 2.3 Evaluation In order to evaluate our searchable translation memory we first constructed a sentence-aligned translation memory using 50,000 sentences from the German-English section of Europarl Corpus (Koehn, 2002). We selected a set"
2004.tc-1.10,2003.mtsummit-papers.53,0,0.0181772,") = P ¯ ¯) f¯ count(f , e The results of the search can be presented as a list of possible translations with the most probable translation first. The context sentences from which the translation was extracted may also optionally be displayed, as shown in Figure 1. 3 The trick is how to go about extracting the counts for phrase alignments from a training corpus. Many methods for calculating phrase alignments use word-level alignments as a starting point.1 There are various heuristics for extracting phrase alignments from word alignments, some are described in Koehn (2003), Tillmann (2003), and Vogel et al. (2003). Figure 3 gives a graphical illustration of the method of extracting incrementally larger phrases2 from word alignments described in Och and Ney (2003). Counts are collected over phrases extracted from word alignments of all sentence pairs in the training corpus. These counts are then used to calculate phrasal translation probabilities. 2.3 Evaluation In order to evaluate our searchable translation memory we first constructed a sentence-aligned translation memory using 50,000 sentences from the German-English section of Europarl Corpus (Koehn, 2002). We selected a set of 120 German phrases to"
2005.eamt-1.9,2004.tc-1.9,0,0.0172746,"increased if it could be easily searched, for example by returning focused translations when a user queries it with a single phrase. This paper describes tools which offer precisely that facility. We present searchable translation memories which allow Google-style searching of translation archives. Figure 1 illustrates the use of the technology. The figure shows example results of querying a searchable translation memory built from French and English portions of the proceedings of the European Parliament. The user has typed the search phrase west bank, and similar to a parallel concordancer (Barlow, 2004), the system has returned a list of sentences that the phrase occurs in. However, unlike a concordancer, the searchable translation memory picks out those phrases which constitute the likely translations of the phrase (cisjordanie, territoires de cisjordanie, rive ouest, and rive gauche du jourdain), groups retrieved sentences by these translations, and ranks the groups according to their probability. There are two primary technical challenges for searchable translation memories. The first is the ability to index a translation memory so that it contains the correspondences between translated w"
2005.eamt-1.9,C88-1016,0,0.0601601,"nd Myers, 1990). In this paper we: • Review the concepts in statistical machine translation which are relevant to the alignment of phrases • Demonstrate how a word- and phrase-aligned parallel corpus can be efficiently indexed for searching with suffix arrays • Evaluate the accuracy of the phrases retrieved by our system using the precision and recall metrics that are standard in information retrieval • Discuss why tools such as these may ultimately be more useful than fully-automatic machine translation 60 2 Statistical Machine Translation Usually the goal of statistical machine translation (Brown et al., 1988) is to be able to choose that target language (English) sentence, e, that is the most probable translation of a given sentence, f , in a foreign language. Rather than choosing e∗ that directly maximizes the conditional probability p(e|f ), Bayes’ rule is generally applied: e∗ = arg max p(e)p(f |e) e (1) In this equation p(e) is a language model probability of the translation and p(f |e) is a translation model describing the stochastic mapping of a source sentence onto a target sentence. The effect of applying Bayes’ rule is to divide the task into estimating two probabilities: a language model"
2005.eamt-1.9,J93-2003,0,0.115912,"In this equation p(e) is a language model probability of the translation and p(f |e) is a translation model describing the stochastic mapping of a source sentence onto a target sentence. The effect of applying Bayes’ rule is to divide the task into estimating two probabilities: a language model probability p(e) which can be estimated using a monolingual corpus, and a translation model probability p(f |e) which is estimated using a bilingual sentence-aligned corpus, such as a translation memory. The next two subsections examine how the transEAMT 2005 Conference Proceedings 2.1 Word Alignments Brown et al. (1993) formulate translation essentially as a word-level operation. The probability that a foreign sentence is the translation of an English sentence is calculated by summing over the probabilities of all permissible word-level alignments, a, between the sentences: p(f |e) =  a p(f , a|e) (2) where an alignment a is defined as a subset of the Cartesian product of the word positions in e of length I and f of length J: a ⊆ {(i, j) : i = 1...I; j = 1...J} Phrase Alignments Phrase-based translation uses larger segments of human translated text. Phrase-based translation models provide an estimate of phr"
2005.eamt-1.9,P05-1032,1,0.512215,"hrase translation 1 occurred in → possible sentence pairs it translation 2 occurred in With such an index, it is a simple matter to query it with a certain source phrase, retrieve all possible translations and their contexts, and rank the translations using the phrase translation probability calculation in Equation 2.2. However from an engineering perspective such an index would be unwieldy in terms of the amount of space needed to save an enumeration of all possible subphrases in the corpus. The problem of memory usage when enumerating phrases and their translations is described in detail in Callison-Burch et al. (2005). The next section describes our use of suffix arrays as an alternative to an enumerated index. 4 Suffix Arrays The suffix array data structure (Manber and Myers, 1990) was introduced as a space-economical way of creating an index for string searches. The suffix array data structure makes it convenient to compute the frequency and location of any substring or n62 Index of words: 0 Corpus 1 Spain declined 2 3 4 5 6 7 8 9 to confirm that Spain declined to aid Morocco Initialized, unsorted Suffix Array Suffixes denoted by s[i] s[0] 0 s[1] 1 declined to confirm that Spain declined to aid Morocco s"
2005.eamt-1.9,J93-1004,0,0.124143,"ork of any translator or translation agency contains significant amounts of repetition, and translation archives are consequently a vital asset. Current translation memory systems provide a valuable means for translators to exploit this resource in order to increase productivity and to ensure consistency. Existing translation memory systems work by retrieving the translation of full sentences that are exactly or approximately matched in a database of a translator’s past work (Trujillo, 1999). Translation memories provide facilities for the automatic alignment of sentences and paragraph units (Gale and Church, 1993; Kay and R¨oscheisen, 1993), but aligning subsentential units is usually an involved, manual process. Matching on the sentence-level is a rather severe restriction which means that only very limited reuse is made of the information contained within a EAMT 2005 Conference Proceedings Josh Schroeder Linear B Ltd. 39 B Cumberland Street Edinburgh EH3 6RA josh@linearb.co.uk translation archive. A translator will frequently use phrases, words and other subsentential strings that s/he has translated before. However, unless these are contained as a whole unit within the database, conventional transl"
2005.eamt-1.9,J93-1006,0,0.0938203,"Missing"
2005.eamt-1.9,N03-1017,0,0.00915977,"of political prisoners and implementation of the financial commitments . ... port de gaza et de la voie de transit nord entre gaza et la rive ouest , ainsi que la poursuite des libérations de prisonniers et la mise en uvre ... rive gauche du jourdain - [ 1 sentence matched ] in his government , mr sharon has appointed ministers who want to reclaim the west bank . m . sharon a inclus dans son gouvernement des ministres qui veulent annexer la rive gauche du jourdain . ©2005 Linear B Figure 1: Search results for the English phrase “west bank” rely on phrase-based statistical machine translation (Koehn et al., 2003). The second is the efficient storage of the index, for which we use a variant of the suffix array data structure (Manber and Myers, 1990). In this paper we: • Review the concepts in statistical machine translation which are relevant to the alignment of phrases • Demonstrate how a word- and phrase-aligned parallel corpus can be efficiently indexed for searching with suffix arrays • Evaluate the accuracy of the phrases retrieved by our system using the precision and recall metrics that are standard in information retrieval • Discuss why tools such as these may ultimately be more useful than ful"
2005.eamt-1.9,koen-2004-pharaoh,0,0.0228,"ing whether there is a sensible mapping between the words in the sentences.1 Figure 2 illustrates a probable wordlevel alignment between a sentence pair in the Canadian Hansard bilingual corpus. 2.2 grown up lation model probability is calculated using subsentential alignments. We use these alignments to construct a searchable translation memory, rather than for the task of fully automatic machine translation. Those people have A Compact Data Structure for Searchable Translation Memories There are various heuristics for extracting phrase alignments from word alignments,2 some are described in Koehn (2004), Tillmann (2003), and Vogel et al. (2003). We define phrase alignments as follows. A substring e¯ consisting of the words at positions l...m is aligned with the phrase f¯ by way of the subalignment s s = a ∩ {(i, j) : i = l...m, j = 1...J} (5) f¯ is the phrase corresponding to the words formed by ordering the set of indices j in (i, j) in s. Note that the ‘phrases’ in phrase-based translation do not correspond to the traditional notion of syntactic constituents; they might be more aptly described as ‘substrings’ or ‘blocks’. Some examples of phrase alignments that can be extracted from Figure"
2005.eamt-1.9,W02-1018,0,0.0202934,"phrase-based translation do not correspond to the traditional notion of syntactic constituents; they might be more aptly described as ‘substrings’ or ‘blocks’. Some examples of phrase alignments that can be extracted from Figure 2 include: lived and worked → v´ecu et oeuvr´e, many years → des dizaines d’ann`ees, a farming district → le domaine agricole. Strictly speaking our method for extracting phrase alignments does not require that f¯ be a contiguous phrase. We insert an placeholder element to indicate 2 There are other ways of calculating phrasal translation probabilities. For instance, Marcu and Wong (2002) estimate them directly rather than starting from word-level alignments. 61 Callison-Burch et al. any discontinuous span in f¯. For example we retrieve the alignment a farming → le ... agricole. Our definition for phrase alignments is useful because it ensures that we are able to retrieve a possible translation for any phrase that occurred in the source corpus. 3 Constructing an Index for Searchable Translation Memories Once we have defined a method for extracting phrase alignments, then we can construct an index for our searchable translation memories. This index will allow us to retrieve the"
2005.eamt-1.9,J03-1002,0,0.020741,"ty of an English phrase e¯ translating as a foreign phrase f¯ can be calculated using maximum likelihood estimation count(f¯, e¯) p(f¯|¯ e) =  ¯ ¯) f¯ count(f , e (4) where counts are collected over each instance where e¯ is aligned with f¯ in any sentence pair in the training corpus. Prior to applying this probability assignment one must define a method for determining which phrases are aligned. 1 For brevity, we have omitted the details about how the parameters of p(f , a|e) are estimated using expectation maximization, and instead refer the reader to Brown et al. (1993), Knight (1999) and Och and Ney (2003). EAMT 2005 Conference Proceedings district . many years in a farming and worked , lived Ces gens ont grandi , vécu et oeuvré des dizaines d&apos; années dans le domaine agricole . Figure 2: A word-level alignment for a sentence pair that occurs in our training data (3) Thus Brown et al. decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the words in the sentences.1 Figure 2 illustrates a probable wordlevel alignment between a sentence pair in the Canadian Hansard bilingual corpus. 2.2"
2005.eamt-1.9,P02-1040,0,0.0718646,"(1980) described a translator’s workbench, and argued that the proper place for machines in translation is as a non-obtrusive aid to human translators. The Transtype project (Foster et al., 2002) has investigated the use of statistical machine translation in particular for text prediction for human translators. Church and Hovy (1993) describes creative ways in which machine translation can be usefully applied. Our research also relates to the evaluation of statistical models of translation. Previous work in this area has focused on the automatic evaluation of machine translation systems Bleu (Papineni et al., 2002), and on the accuracy of automatic alignments (Och and Ney, 2003). Searchable translation memories can be thought of as a task-based evaluation of statistical translation models. 7 Discussion In this paper we investigate a useful application for machine translation technology in its current state. Rather than use statistical machine translation to perform fully automated translation, we have shown how it might instead be integrated into the human translation process by increasing the utility of translation memories. Existing translation memories only allow the reuse of previous work when whole"
2005.eamt-1.9,W03-1001,0,0.011935,"ere is a sensible mapping between the words in the sentences.1 Figure 2 illustrates a probable wordlevel alignment between a sentence pair in the Canadian Hansard bilingual corpus. 2.2 grown up lation model probability is calculated using subsentential alignments. We use these alignments to construct a searchable translation memory, rather than for the task of fully automatic machine translation. Those people have A Compact Data Structure for Searchable Translation Memories There are various heuristics for extracting phrase alignments from word alignments,2 some are described in Koehn (2004), Tillmann (2003), and Vogel et al. (2003). We define phrase alignments as follows. A substring e¯ consisting of the words at positions l...m is aligned with the phrase f¯ by way of the subalignment s s = a ∩ {(i, j) : i = l...m, j = 1...J} (5) f¯ is the phrase corresponding to the words formed by ordering the set of indices j in (i, j) in s. Note that the ‘phrases’ in phrase-based translation do not correspond to the traditional notion of syntactic constituents; they might be more aptly described as ‘substrings’ or ‘blocks’. Some examples of phrase alignments that can be extracted from Figure 2 include: lived"
2005.eamt-1.9,2003.mtsummit-papers.53,0,0.0127947,"ping between the words in the sentences.1 Figure 2 illustrates a probable wordlevel alignment between a sentence pair in the Canadian Hansard bilingual corpus. 2.2 grown up lation model probability is calculated using subsentential alignments. We use these alignments to construct a searchable translation memory, rather than for the task of fully automatic machine translation. Those people have A Compact Data Structure for Searchable Translation Memories There are various heuristics for extracting phrase alignments from word alignments,2 some are described in Koehn (2004), Tillmann (2003), and Vogel et al. (2003). We define phrase alignments as follows. A substring e¯ consisting of the words at positions l...m is aligned with the phrase f¯ by way of the subalignment s s = a ∩ {(i, j) : i = l...m, j = 1...J} (5) f¯ is the phrase corresponding to the words formed by ordering the set of indices j in (i, j) in s. Note that the ‘phrases’ in phrase-based translation do not correspond to the traditional notion of syntactic constituents; they might be more aptly described as ‘substrings’ or ‘blocks’. Some examples of phrase alignments that can be extracted from Figure 2 include: lived and worked → v´ecu et oe"
2005.eamt-1.9,J01-1001,0,0.0230056,"reating a list of references to each of the suffixes in a corpus. Figure 3 shows how a suffix array is initialized for a corpus with one sentence. Each index of a word in the corpus has a corresponding place in the suffix array, which is identical in length to the corpus. Figure 4 shows the final state of the suffix array, which is as a list of the indices of words in the corpus that corresponds to an alphabetically sorted list of the suffixes. The advantages of this representation are that it is compact and easily searchable. The total size of the suffix array is a constant amount of memory. Yamamoto and Church (2001) show how to use suffix arrays to calculate a number of statistics that EAMT 2005 Conference Proceedings Spain declined to conﬁrm that Spain declined to aid Morocco A Compact Data Structure for Searchable Translation Memories L&apos; Espagne a refusé de conﬁrmer que l&apos; Espagne avait refusé d&apos; aider le Maroc Figure 5: A word-level alignment for the sentence in the suffix array are interesting in natural language processing applications. They demonstrate how to calculate term frequency / inverse document frequency (tf / idf) for all n-grams in very large corpora. Here we show how to apply suffix arra"
2005.iwslt-1.8,P05-1066,1,0.16829,"inal (default) 48.8 40.4 33.9 28.9 15.4 final-and 48.5 39.9 35.7 32.4 9.6 grow-diag 49.9 39.0 27.7 31.7 8.1 grow 39.9 39.1 13.5 32.8 15.4 intersect 47.5 45.1 35.4 34.6 15.2 Table 2: BLEU scores for systems trained using different alignment methods We also carried out experiments to optimise GIZA++ parameters, but this did not yield any significant improvements. We would like to re-visit these experiments at some future time, since we did not have sufficient time for a thorough treatment at this time. We also tried to deal with language-specific problems, as previously done for German–English (Collins et al., 2005). We created hand-written rules that move the Japanese verb from the end of the sentence to the beginning. However, we could not consistently achieve improvements using these rules. Since we did not have a part-of-speech tagger for Japanese, we had to rely on the assumption that the last word of a Japanese sentence is the verb. We did not apply these rules in our official submission. 3.1 Optimising Word Alignment Our experience with GIZA++ alignments has been that IBM Model training performs poorly for source words that occur only once in the training corpus. These words are often incorrectly"
2005.iwslt-1.8,P04-1083,0,0.0145603,"s. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it also meant that we had to re-examine various components of our system. In this paper we present an overview of our current out-of-the-"
2005.iwslt-1.8,koen-2004-pharaoh,0,0.0508616,"s. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it also meant that we had to re-examine various components of our system. In this paper we present an overview of our current out-of-the-"
2005.iwslt-1.8,N03-1017,1,0.060637,"ranscription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs. 1 Introduction The statistical machine translation group at the University of Edinburgh has been focused on open domain text translation, so we welcomed the challenge to work on the IWSLT 2005 limited domain speech translation task. We participated in the transcription translation tasks for all five language pairs, using only the supplied corpora. Our MT system was originally developed for translation of European parliament texts from German to English (Koehn et al., 2003). We extended the system while working on the DARPA challenges to translate Chinese and Arabic news texts into English (Koehn, 2004a; Koehn et al., 2005). Now, we were faced with the challenge of speech data in mostly Asian languages. The translation of transcribed speech differs in many ways from our traditional translation scenario: Much less training data is available, the domain is more limited, and the text style is very different — short questions and statements. In some respect, the task is easier, since smaller training corpora result in faster training times for the system. But it als"
2005.iwslt-1.8,W02-1018,0,0.0130598,"code of the grow-diag-final method to symmetrise word alignments. See Section 2.3 for variations of this method. See Figure 5 for an illustration. Note that unaligned words may be included within and at the border of extracted phrase pairs (third example in Figure 5). Each phrase pair, however, must include at least one alignment point. Using word-level alignments to induce phrasebased translation models is common practise in the statistical machine translation community. It has been adopted by most groups participating in the NIST MT Evaluation (Lee and Przybocki, 2005). In contrast to this, Marcu and Wong (2002) have defined a method for directly estimating phrasal translation models from parallel corpora, rather than using heuristic methods to induce phrase alignments from word alignments. Their joint probability phrase-based model is computationally demanding, and as such has not been applied to large data sets. Our group has been implementing a scalable version of the joint probability model (Mayne, 2005), and we hope to submit it as a contrastive system in next year’s IWSLT. 2.5 GIZA++ alignments. Three neighbouring points are added. The alignment point between did and a is added in the grow(-dia"
2005.iwslt-1.8,P03-1021,0,0.0327751,"ght exists • discontinuous: no alignment points to the top left or top right Given these counts, we can learn probability distributions of the form: pr (orientation|¯ e, f¯) (5) For the estimation of the probability distribution, we smooth the collected counts. This lexicalised reordering model is motivated by similar work by Tillmann (2004). Discriminative Training Recall that the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translation quality, we employ the BLEU score (Papineni et al., 2002). The search for the best weight setting is a line search for each λm , which is repeated until no improvement can be achieved. We thank David Chiang of the University of Maryland for providing us with a fas"
2005.iwslt-1.8,J03-1002,0,0.0170359,"in the corpus, extracting phrase pairs that are consistent with the word alignment, and then assigning probabilities (or scores) to the obtained phrase translations. 2.3 Figure 2: Obtaining a high precision, low recall word alignment by intersecting two GIZA++ alignments Word Alignment Word alignments are obtained by first using the GIZA++ toolkit in both translation directions and then symmetrising the two alignments. Since the IBM Models implemented in GIZA++ are not able to map one target (English) word to multiple source (foreign) words, the method of symmetrising — called refined method (Och and Ney, 2003) — effectively overcomes this deficiency. Figure 2 shows the first step in the symmetrisation process: The intersection of the two GIZA++ alignments is taken. Only word alignment points that occur in both alignments are preserved. This is the intersection alignment. In a second step, additional alignment points are added. Only alignment points that are in either of the two GIZA++ alignments (or, in the union of these alignments) are considered. In the growing step, potential alignment points that connect currently unaligned words and that neighbour established alignment points are added. Neigh"
2005.iwslt-1.8,P02-1040,0,0.110837,"the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translation quality, we employ the BLEU score (Papineni et al., 2002). The search for the best weight setting is a line search for each λm , which is repeated until no improvement can be achieved. We thank David Chiang of the University of Maryland for providing us with a faster version of our implementation. 3 Adaptations to IWSLT’05 Task In a period of one month, we optimised our system to the IWSLT’05 task. We chose to only participate in the transcription task using the supplied data, since we did not have adequate additional resources or tools for these language pairs, and also had not enough time to investigate these. The advantage of limiting ourselves t"
2005.iwslt-1.8,N04-4026,0,0.382449,"nected to the previous phrase at all (discontinuous). See Figure 6 for an illustration. When collecting phrase pairs, can classify them into these three categories based on: • monotone: a word alignment point to the top left exists • swap: an alignment point to the top right exists • discontinuous: no alignment points to the top left or top right Given these counts, we can learn probability distributions of the form: pr (orientation|¯ e, f¯) (5) For the estimation of the probability distribution, we smooth the collected counts. This lexicalised reordering model is motivated by similar work by Tillmann (2004). Discriminative Training Recall that the components of our machine translation system are combined in a log-linear way. The weight of the feature functions, or model components, is set by minimum error rate training. We reimplemented a method suggested by Och (2003). In short, we optimise the value of the parameter weights λm by iteratively: (a) running the decoder with a currently best weight setting, (b) extracting an n-best list of possible translations, and (c) finding a better weight setting that re-ranks the n-best-list, so that a better translation score is obtained. To score translati"
2006.amta-papers.2,N03-1017,0,0.241887,"acent words, and, because word alignments inadequately represent the real dependencies between translations. Also, by heuristically creating phrasal alignments from the Viterbi word-level alignments, we throw away the probabilities that were estimated when learning word alignment parameters and we can introduce errors. In contrast, the Joint Model can search areas of the alignment space in order to learn a distribution of possible phrasal alignments that better handles the uncertainty inherent in the translation process. Models Standard Phrase-based Model Most phrase-based models (Och, 2003b; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based model as the Standard Model. The Standard Model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative di"
2006.amta-papers.2,koen-2004-pharaoh,0,0.0349132,"which is about a factor of eight. Experiments The experiments were run using the GermanEnglish Europarl corpus (Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model"
2006.amta-papers.2,2005.mtsummit-papers.11,0,0.0426336,"tively slow even for the smallest data sets, so the first experiment explores the gains to be made by using fast hill-climbing on a training corpus of 5000 sentences. Figure 2. Time taken for EM training in minutes per iteration for 5,000 sentences on a machine with 2Gb RAM and a 2.4GHz CPU In Figure 2 we can see that fast hill-climbing is much faster than the normal hill-climbing. We have reduced the time taken to perform the first iteration from nearly 5 hours to about 40 minutes, which is about a factor of eight. Experiments The experiments were run using the GermanEnglish Europarl corpus (Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et a"
2006.amta-papers.2,W02-1018,0,0.489907,"irch Chris Callison-Burch Miles Osborne School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK a.c.birch-mayne@sms.ed.ac.uk Abstract word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrasebased models extract their phrase pairs from previously word-aligned corpora using ad-hoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigourous statistical framework of the IBM Models. Marcu and Wong (2002) proposed a Joint Probability Model which directly estimates phrase translation probabilities from the corpus. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The Joint Model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). However, considering all possible phrases and all their possible alignme"
2006.amta-papers.2,P02-1038,0,0.0294569,"s which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model Joint Model + IBM + IBM + Lex the first few iterations, but then fast hill-climbing overtakes it. The difference in performance between the two methods is small and we apply fast hillclimbing in the remaining experiments. 5.2 IBM Constraints 10,000 21.69 19.93 22.13 22.79 20,000 23.61 23.08 24."
2006.amta-papers.2,J04-4002,0,0.0123472,"hout compromising the quality of the resulting translations. 2 2.1 for optimal phrase pairs. Instead, it extracts phrase pairs (f i , ei ) in the following manner. First, it uses the IBM Models to learn the Viterbi alignments for English to Foreign and Foreign to English. It then uses a heuristic to reconcile the two alignments, starting from the points of high confidence in the intersection of the two Viterbi alignments and growing towards the points in the union. Points from the union are selected if they are adjacent to points from the intersection and their words are previously unaligned. Och and Ney (2004) discusses and compares variations on this strategy. Phrases are then extracted by selecting phrase pairs which are ‘consistent’ with the symmetrised alignment. Here ‘consistent’ means that all words within the source language phrase are only aligned to the words of the target language phrase and vice versa. Finally the phrase translation probability distribution is estimated using the relative frequencies of the extracted phrase pairs. This approach to phrase extraction means that phrasal alignments are locked into the symmetrised alignment. This is problematic because the symmetrisation proc"
2006.amta-papers.2,P03-1021,0,0.509867,"oduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learned only word-to-word alignment probabilities which made it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the Alignment Template Model (Och, 2003b), improve on 10 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 10-18, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas space of the Joint Model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are still explored. The Joint Model is constrained to phrasal alignments which do not contradict a set high confidence word alignments for each sentence. These high confidence alignments can incorporate information from both statisti"
2006.amta-papers.2,P02-1040,0,0.108735,"(Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model Joint Model + IBM + IBM + Lex the first few iterations, but then fast hill-climbing overtakes it. The difference in perfor"
2006.amta-papers.2,P92-1017,0,0.0866123,"18288740923 4.4145633531e+32 2.7340255177e+83 Table 1. The number of possible phrasal alignments for sentence pairs calculated using Stirling numbers of the second kind. Table 1 shows just how many phrasal alignments are possible between sentences of different length. Even for medium length sentences that are 20 words in lengths, the total number of alignments is huge. Apart from being intractable, when one has a very large parameter estimation space the EM algorithm struggles to discover good parameters. One approach to dealing with this problem is to constrain the search space. For example, Pereira and Schabes (1992) proposed a method for dealing with this problem for PCFG estimation from treebanks. They encouraged the probabilities into good regions of the parameter space by constraining the search to only consider parses that did not cross Penn-Treebank nodes. We adopt a similar approach for constraining the joint model, by only considering alignments that do not contradict high probability word alignments. During EM a very small proportion of the possible alignments are searched and many good alignments are likely to be missed. Normally alignments 2.2.2 Expectation Maximisation After initialising the t"
2006.amta-papers.2,J93-2003,0,0.0103816,"and linguistically motivated word alignments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material. 1 Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learned only word-to-word alignment probabilities which made it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the Alignment Template Model (Och, 2003b), improve on 10 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 10-18, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas space of the Joint Model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are s"
2006.amta-papers.2,P04-1023,1,0.129086,"al Viterbi alignments. Concepts which contain many f c(e, f ) = (1 − λ)p(e, f |E, F ) + λpc(e, f ) The fractional count for each concept in each sentence is calculated by interpolating the joint probability of the concept, based on the Stirling numbers, and the prior count, which reflects the probability of the phrasal alignment given the high confidence word alignments. The use of the weight to balance the two contributions allows us to adjust for differences in scale and our confidence in each of the two measures. After testing various settings for λ the value 0.5 gave the best Bleu scores. Callison-Burch et al. (2004) used a similar technique for combining word and sentence aligned data. However, they inserted data from labelled word alignments which meant that they did not need to sum over all possible alignments for a sentence pair. 4.2 Fast Hill-climbing The constraints on the Joint Model reduce its size by restricting the initialisation phase of the training. This is one of the two major drawbacks of the 14 three features were used for both the Joint and the Standard Model: p(e|f ), p(f |e) and the language model, and they were given equal weights. model discussed by Marcu and Wong (2002). The other ma"
2006.amta-papers.2,2003.mtsummit-papers.53,0,0.0230242,"cause word alignments inadequately represent the real dependencies between translations. Also, by heuristically creating phrasal alignments from the Viterbi word-level alignments, we throw away the probabilities that were estimated when learning word alignment parameters and we can introduce errors. In contrast, the Joint Model can search areas of the alignment space in order to learn a distribution of possible phrasal alignments that better handles the uncertainty inherent in the translation process. Models Standard Phrase-based Model Most phrase-based models (Och, 2003b; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based model as the Standard Model. The Standard Model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion probability"
2006.amta-papers.2,W06-3105,0,0.415441,"ature functions were optimized using minimum error rate training (Och, 2003a). Joint + IBM Standard Model BLEU 26.17 28.35 Pruning eliminates many phrase pairs, but further investigation indicates that this has little impact on BLEU scores. The fact that only a small proportion of the alignment space is searched is very likely to be hampering the Joint Model’s performance. The small number of alignments visited leads to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 6 Related Work DeNero et al. (2006) argue that training a translation model at the phrase level results in inferior parameters to the standard, heuristic phrase-based models. They suggest that the reason for this is that EM optimizes by selecting different segmentations and loses important phrase translation ambiguity. They say that the model results in a very peaked distribution and entropy drops too low. However, their argument only holds for conditional models. In a conditional model, there is competition for the probability mass of the conditioned word, and instead of spreading that mass between different translations, diff"
2006.amta-papers.2,W06-3114,0,0.00807513,"no such competition and the resulting phrase table’s entropy is in fact higher than that of the Standard Model. Size 2.28 19.04 Table 6. Bleu scores and model size in millions of phrase pairs for Spanish-English The results in Table 6 show that the Joint Model is capable of training on larger data sets, with a reasonable performance. On smaller data sets, as shown in sections 5.2 and 5.3 the Joint Model shows performance superior or comparable to the Standard Model. However, here it seems that the Standard Model has an advantage which is statistically significant according to the sign method (Koehn and Monz, 2006). This is almost certainly related to the fact that the Joint Model results in a much smaller phrase table. The size of the resulting Joint Model is in fact comparable to the size of the model in previous experiments when training with just 20,000 sentences. This is because the model must be kept in memory for collecting fractional counts in EM and even though the corpus is bigger, the memory available remains the same (the Standard Model phrase table is created on disk). To keep the Joint Model within memory, pruning is necessary after initialization because this is where most phrase pairs ar"
2006.amta-papers.2,P97-1063,0,\N,Missing
2006.amta-papers.2,P05-1066,0,\N,Missing
2006.amta-papers.2,2005.iwslt-1.8,1,\N,Missing
2006.amta-papers.2,P00-1056,0,\N,Missing
2010.amta-papers.12,P07-1083,0,0.0383305,"ted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008) improves on this idea by focusing on selecting better pairwise features. Following this line of work, Sproat et al. (2008) developed a toolkit for computing the cost of mapping between two strings in any two scripts. Their toolkit also includes generative pronunciation modules for Chinese and English. In 2009, the Named Entities Workshop (NEWS) at the ACL-IJCNLP conference included a Machine Transliteration shared task (Li et al., 2009a). Over thirty teams participated in the task, which involved transliterating from English to the following languages: Hindi, Tamil"
2010.amta-papers.12,P05-1033,0,0.0123383,"ion is simpler than translation, since phrases are often reordered in translation, but characters sequence are monotonic in transliteration. Our feature functions include a character sequence mapping probability (similar to the phrase translation probability), a character substitution probability (similar to the lexical probability), and a character-based language model probability. For our experiments, we use the off-the-shelf Joshua open source statistical machine translation system (Li et al., 2009b). Joshua’s translation model uses synchronous context free grammars, like the Hiero system (Chiang, 2005; Chiang, 2007). However, because transliteration is strictly a monotone task, we do not extract grammar rules that involve any hierarchical structure by restricting the number of nonterminals to zero. We have the grammar extractor identify rules for character-based phrases up to length ten. Our language models are also trained on up to 10-gram sequences of target language characters. Unlike in machine translation, our phrase tables and language models can support very large n-gram sizes because the number of characters in a given script is small compared to word vocabularies. As a preprocessi"
2010.amta-papers.12,J07-2003,0,0.0180398,"than translation, since phrases are often reordered in translation, but characters sequence are monotonic in transliteration. Our feature functions include a character sequence mapping probability (similar to the phrase translation probability), a character substitution probability (similar to the lexical probability), and a character-based language model probability. For our experiments, we use the off-the-shelf Joshua open source statistical machine translation system (Li et al., 2009b). Joshua’s translation model uses synchronous context free grammars, like the Hiero system (Chiang, 2005; Chiang, 2007). However, because transliteration is strictly a monotone task, we do not extract grammar rules that involve any hierarchical structure by restricting the number of nonterminals to zero. We have the grammar extractor identify rules for character-based phrases up to length ten. Our language models are also trained on up to 10-gram sequences of target language characters. Unlike in machine translation, our phrase tables and language models can support very large n-gram sizes because the number of characters in a given script is small compared to word vocabularies. As a preprocessing step, we app"
2010.amta-papers.12,P07-1003,0,0.0323291,"ting from many languages to English. We compare our systems to previous work where it is possible. 2 Following Virga and Khudanpur (2003), we treat transliteration as a monotone character translation task. Rather than using a noisy channel model, our transliteration models is based on the log-linear formulation of statistical machine translation (SMT) described in Och and Ney (2002). Whereas SMT systems are trained on parallel sentences and use word-based n-gram language models, we use pairs of transliterated words along with character-based ngram language models. We use the Berkeley aligner (DeNero and Klein, 2007) to automatically align characters in pairs of transliterations. This is analogous to word-based alignment in SMT. Transliteration is simpler than translation, since phrases are often reordered in translation, but characters sequence are monotonic in transliteration. Our feature functions include a character sequence mapping probability (similar to the phrase translation probability), a character substitution probability (similar to the lexical probability), and a character-based language model probability. For our experiments, we use the off-the-shelf Joshua open source statistical machine tr"
2010.amta-papers.12,D08-1037,0,0.0175098,"Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008) improves on this idea by focusing on selecting better pairwise features. Following this line of work, Sproat et al. (2008) developed a toolkit for computing the cost of mapping between two strings in any two scripts. Their toolkit also includes generative pronunciation modules for Chinese and English. In 2009, the Named Entities Workshop (NEWS) at the ACL-IJCNLP conference included a Machine Transliteration shared task (Li et al., 2009a). Over thirty teams participated in the task, which involved transliterating from English to the following languages: Hindi, Tamil, Kannada, Russian, Chinese,"
2010.amta-papers.12,P04-1021,0,0.0914548,"tart-ofword and end-of-word symbols to all training pairs and test words. Table 1 shows examples of Russian to English and Greek to English transliteration rules Previous Work There has been a large amount of research focused on the task of transliteration with both discriminative and generative methods achieving good performance. Knight and Graehl (1997) reported the results of a generative model for back-transliterating from Japanese to English using a weighted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008) improves on this idea by focusing on selecting better pairwise features. Following this line of work, S"
2010.amta-papers.12,H05-1055,0,0.0206294,"reference is on the left, and hypothesis is on the right. E indicates that the letter E is dropped from the hypothesis, and I indicates I is inserted. out additional context. These examples highlight the difficulty in evaluating transliteration performance and suggest that evaluation metrics are likely to be pessimistic. In future work, we plan to use human annotations to measure the upper limit of transliteration performance. The system incorrectly transliterated the final set of names. These mistakes suggest that using additional evidence may assist transliteration. For example, as shown in Huang (2005), if a name’s origin is known, we may be able to infer the proper spelling of the erroneous transliterations in the third part of Table 8. 5 Conclusion In this work, we have demonstrated that freely available resources (both systems and data) are sufficient to build models capable of producing high quality transliterations from a large set of languages into English. We have shown that we can build high performing systems even for languages for which there are few available NLP resources. We have also shown that more data is better. We could supplement the training data pairs that we extracted"
2010.amta-papers.12,W09-3504,0,0.0292571,"p-1 Accuracy Top-1 F-score Mean Avg Prec. at 10 Training Pairs Top-1 Accuracy Top-1 F-score Mean Avg Prec. at 10 Training Pairs Our System Others English→Russian .55 .35 - .61 .91 .87 - .93 .20 .13 - .29 5977 English→Hindi .45 .00 - .50 .87 .01 - .89 .18 .00 - .20 4840 Table 5: A comparison of our performance against the systems submitted to the Russian and Hindi transliteration shared tasks at the 2009 Named Entities Workshop. and evaluated them using the workshop metrics. The results are presented in Table 5. In general, although our systems do not outperform the best participating systems (Jiampojamarn et al., 2009; Oh et al., 2009), they generate results that are comparable to the state of the art in English to Hindi and English to Russian transliteration. Thus, with a competitive system framework, we turn to our main focus, which is transliterating from a large, diverse set of languages into English. 3.3 Evaluation Metric It is often the case that imperfect transliterations (i.e., inexact matches with the reference transliteration) are still readable in text. Since our goal is to integrate our model into an SMT system, it is important to know not only how frequently we produce perfect transliterations"
2010.amta-papers.12,P06-1103,1,0.948992,"of research focused on the task of transliteration with both discriminative and generative methods achieving good performance. Knight and Graehl (1997) reported the results of a generative model for back-transliterating from Japanese to English using a weighted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008) improves on this idea by focusing on selecting better pairwise features. Following this line of work, Sproat et al. (2008) developed a toolkit for computing the cost of mapping between two strings in any two scripts. Their toolkit also includes generative pronunciation modules for Chinese and English. In 2009, t"
2010.amta-papers.12,P97-1017,0,0.196927,"ces of target language characters. Unlike in machine translation, our phrase tables and language models can support very large n-gram sizes because the number of characters in a given script is small compared to word vocabularies. As a preprocessing step, we append start-ofword and end-of-word symbols to all training pairs and test words. Table 1 shows examples of Russian to English and Greek to English transliteration rules Previous Work There has been a large amount of research focused on the task of transliteration with both discriminative and generative methods achieving good performance. Knight and Graehl (1997) reported the results of a generative model for back-transliterating from Japanese to English using a weighted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discr"
2010.amta-papers.12,W09-0424,1,0.90984,"align characters in pairs of transliterations. This is analogous to word-based alignment in SMT. Transliteration is simpler than translation, since phrases are often reordered in translation, but characters sequence are monotonic in transliteration. Our feature functions include a character sequence mapping probability (similar to the phrase translation probability), a character substitution probability (similar to the lexical probability), and a character-based language model probability. For our experiments, we use the off-the-shelf Joshua open source statistical machine translation system (Li et al., 2009b). Joshua’s translation model uses synchronous context free grammars, like the Hiero system (Chiang, 2005; Chiang, 2007). However, because transliteration is strictly a monotone task, we do not extract grammar rules that involve any hierarchical structure by restricting the number of nonterminals to zero. We have the grammar extractor identify rules for character-based phrases up to length ten. Our language models are also trained on up to 10-gram sequences of target language characters. Unlike in machine translation, our phrase tables and language models can support very large n-gram sizes b"
2010.amta-papers.12,W09-3525,0,0.0481466,"Missing"
2010.amta-papers.12,P02-1038,0,0.0596972,"we describe experiments in transliterating from other languages to English. We conclude with some thoughts on future work in Section 5. two language pairs. In this work, we evaluate a single transliteration framework for transliterating from many languages to English. We compare our systems to previous work where it is possible. 2 Following Virga and Khudanpur (2003), we treat transliteration as a monotone character translation task. Rather than using a noisy channel model, our transliteration models is based on the log-linear formulation of statistical machine translation (SMT) described in Och and Ney (2002). Whereas SMT systems are trained on parallel sentences and use word-based n-gram language models, we use pairs of transliterated words along with character-based ngram language models. We use the Berkeley aligner (DeNero and Klein, 2007) to automatically align characters in pairs of transliterations. This is analogous to word-based alignment in SMT. Transliteration is simpler than translation, since phrases are often reordered in translation, but characters sequence are monotonic in transliteration. Our feature functions include a character sequence mapping probability (similar to the phrase"
2010.amta-papers.12,W09-3506,0,0.021295,"Mean Avg Prec. at 10 Training Pairs Top-1 Accuracy Top-1 F-score Mean Avg Prec. at 10 Training Pairs Our System Others English→Russian .55 .35 - .61 .91 .87 - .93 .20 .13 - .29 5977 English→Hindi .45 .00 - .50 .87 .01 - .89 .18 .00 - .20 4840 Table 5: A comparison of our performance against the systems submitted to the Russian and Hindi transliteration shared tasks at the 2009 Named Entities Workshop. and evaluated them using the workshop metrics. The results are presented in Table 5. In general, although our systems do not outperform the best participating systems (Jiampojamarn et al., 2009; Oh et al., 2009), they generate results that are comparable to the state of the art in English to Hindi and English to Russian transliteration. Thus, with a competitive system framework, we turn to our main focus, which is transliterating from a large, diverse set of languages into English. 3.3 Evaluation Metric It is often the case that imperfect transliterations (i.e., inexact matches with the reference transliteration) are still readable in text. Since our goal is to integrate our model into an SMT system, it is important to know not only how frequently we produce perfect transliterations but how similar o"
2010.amta-papers.12,N09-1005,0,0.0405025,"e number of characters in a given script is small compared to word vocabularies. As a preprocessing step, we append start-ofword and end-of-word symbols to all training pairs and test words. Table 1 shows examples of Russian to English and Greek to English transliteration rules Previous Work There has been a large amount of research focused on the task of transliteration with both discriminative and generative methods achieving good performance. Knight and Graehl (1997) reported the results of a generative model for back-transliterating from Japanese to English using a weighted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and"
2010.amta-papers.12,W06-1630,0,0.0724711,"task of transliteration with both discriminative and generative methods achieving good performance. Knight and Graehl (1997) reported the results of a generative model for back-transliterating from Japanese to English using a weighted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008) improves on this idea by focusing on selecting better pairwise features. Following this line of work, Sproat et al. (2008) developed a toolkit for computing the cost of mapping between two strings in any two scripts. Their toolkit also includes generative pronunciation modules for Chinese and English. In 2009, the Named Entities"
2010.amta-papers.12,W03-1508,0,0.687541,"follows. In Section 2 we discuss prior work in transliteration, and in Section 3 we describe our model and dataset in detail. In Section 3 we also compare our systems’ performance with that of other systems and explain our evaluation metric. In Section 4, we describe experiments in transliterating from other languages to English. We conclude with some thoughts on future work in Section 5. two language pairs. In this work, we evaluate a single transliteration framework for transliterating from many languages to English. We compare our systems to previous work where it is possible. 2 Following Virga and Khudanpur (2003), we treat transliteration as a monotone character translation task. Rather than using a noisy channel model, our transliteration models is based on the log-linear formulation of statistical machine translation (SMT) described in Och and Ney (2002). Whereas SMT systems are trained on parallel sentences and use word-based n-gram language models, we use pairs of transliterated words along with character-based ngram language models. We use the Berkeley aligner (DeNero and Klein, 2007) to automatically align characters in pairs of transliterations. This is analogous to word-based alignment in SMT."
2010.amta-papers.12,P07-1015,0,0.0353638,"ation with both discriminative and generative methods achieving good performance. Knight and Graehl (1997) reported the results of a generative model for back-transliterating from Japanese to English using a weighted FST. Recently, Ravi and Knight (2009) trained the same Japanese-English models on unsupervised data. Virga and Khudanpur (2003) and Haizhou et al. (2004) suggest using the traditional source-channel SMT model to ‘translate’ the sounds of one language into another and present results on ChineseEnglish transliteration. Other recent work (Klementiev and Roth, 2006; Tao et al., 2006; Yoon et al., 2007) proposes to view transliteration as a classification task and suggests training a discriminative model to determine whether a pair of words are transliterations of one another. Subsequent work (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008) improves on this idea by focusing on selecting better pairwise features. Following this line of work, Sproat et al. (2008) developed a toolkit for computing the cost of mapping between two strings in any two scripts. Their toolkit also includes generative pronunciation modules for Chinese and English. In 2009, the Named Entities Workshop (NEWS) at t"
2010.amta-papers.12,N07-1046,0,0.0371351,"Missing"
2010.amta-papers.7,baker-etal-2010-modality,1,0.741353,"es of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now (deontic possibility). Many semanticists (Kratzer, 2009; von Fintel and Iatridou, 2009) define modality as quantification over possible worlds. John might go means that there exist some possible worlds in which John goes. Another view of modality relates more to a speaker’s attitude toward a proposition (Nirenburg and McShane, 2008; McShane et al., 2004). Modality resources built for this purpose have been described previously (Baker et al., 2010). This paper will focus on a tree-grafting mechanism used to enrich the machine-translation output and on the resulting improvements to translation quality when the training process for the machinetranslation systems included tagging of named entities and modality. The next section provides the motivation behind the SIMT approach. Section 3 presents implementation details of the semantically-informed syntactic system. Section 4 describes the tree grafting algorithm. Section 5 provides the results of this work. Standard Hierarchical Rules Syntactic Enhancements Semantically Informed Rules Figur"
2010.amta-papers.7,P05-1033,0,0.172966,"duEnglish test set, and the blind test set was NIST09. Tree-Grafting to refine translation grammars with semantic categories We use synchronous context free grammars (SCFGs) as the underlying formalism for our statistical models of translation. SCFGs provide a convenient and theoretically grounded way of incorporating linguistic information into statistical models of translation, by specifying grammar rules with syntactic non-terminals in the source and target languages. We refine the set of non-terminal symbols so that they not only include syntactic categories, but also semantic categories. Chiang (2005) re-popularized the use of SCFGs for machine translation, with the introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero uses grammars with a single nonterminal symbol “X” rather than using linguistically informed non-terminal symbols. When moving to linguistic grammars, we use the Syntax Augmented Machine Translation (SAMT) developed by Venugopal et al. (2007). In SAMT the “X” symbols in translation grammars are replaced with nonterminal categories derived from parse trees that label the English side of the Urdu-English parallel corpus.1 We refine the syntacti"
2010.amta-papers.7,C96-1079,0,0.0150098,"ut has been accused of assam that the power of this on a large region has taken in the affairs and one Place, vice Center of which he is also Figure 1: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort. Named entities have been the focus of information extraction research since the Message Understanding Conferences of the 1980s (Grishman and Sundheim, 1996). Automatic taggers identify semantic types such as person, organization, location, date, facility, etc. In this research effort we tagged English documents using an HMM-based tagger derived from Identifinder (Bikel et al., 1999). Modality is an extra-propositional component of meaning. In John may go to NY, the basic proposition is John go to NY and the word may indicates modality. Van der Auwera and Amman (2005) define core cases of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now"
2010.amta-papers.7,N06-1031,0,0.0612688,"The semantic annotations were done manually by students following a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP"
2010.amta-papers.7,P03-1054,0,0.00597936,"he training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. In the parsing domain, the work of (Petrov and Klein, 2007) is related to the current work. Petrov and Klein use a technique of rule splitting and rule merging in order to refine parse trees during machine learning. Hierarchical splitting leads to the creation of learned categories that have linguistic relevance, such as a brea"
2010.amta-papers.7,P07-2045,1,0.00864032,"erstand the challenges of translating important semantic entities when working with a lowresource language pair. Figure 1 shows an example taken from the 2008 NIST Urdu-English translation task, and illustrates the translation quality of a stateof-the-art Urdu-English system (prior to the SIMT effort). The small amount of training data for this language pair (see Table 1) results in significantly degraded translation quality compared, e.g., to an Arabic-English system that has more than 100 times the amount of training data. The machine translation output in Figure 1 was produced using Moses (Koehn et al., 2007), a stateof-the-art phrase-based machine translation system that by default does not incorporate any linguistic information (e.g., syntax or morphology or translit3 Figure 3: Workflow for producing semantically-grafted parse trees. The English side of the parallel corpus is automatically parsed, and also tagged with modality and named-entity markers. These tags are then grafted onto the syntactic parse trees. The relation finder was designed for additional tagging but was not implemented in the current work. (Future work will test relations as another component of meaning that may contribute t"
2010.amta-papers.7,W09-0424,1,0.845363,"note that while our framework is general, we focus the discussion here on the particular semantic elements (named entities and modalities) that were incorporated during the SIMT effort. Once the semantically-grafted trees have been produced for the parallel corpus, the trees are presented, along with word alignments (produced by an aligner such as GIZA++), to the rule extraction software to extract synchronous grammar rules that are both syntactically and semantically informed. These grammar rules are used by the decoder to produce translations. In our experiments, we used the Joshua decoder (Li et al., 2009), the SAMT grammar extraction software (Venugopal and Zollmann, 2009), and special purpose-built tree-grafting software. Figure 5 shows example semantic rules that are used by the decoder. The noun-phrase rules are augmented with named entities, and the verb phrase rules are augmented with modalities. The semantic categories are listed in Table 2 and Table 3. Because these get marked on the Urdu source as well as the English translation, semantically enriched grammars also act as very simple named entity or modality taggers for Urdu. However, only entities and modalities that occurred in the p"
2010.amta-papers.7,J93-2004,0,0.0365987,"with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. In the parsi"
2010.amta-papers.7,A00-2030,1,0.678482,"Missing"
2010.amta-papers.7,P08-1001,0,0.0522391,"the Palestinian Authority fired several missiles GPE weapon GPE GPE Figure 4: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with a named entity tagger. The tags are then grafted onto the syntactic parse tree to form new categories like NP-GPE and NP-weapon. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees. context free grammar parser provided by Basis Technology Corporation. 2. The English sentences are named-entity-tagged by the Phoenix tagger (Richman and Schone, 2008) and modality-tagged by the system described in (Baker et al., 2010). 3. The named entities and modalities are grafted onto the syntactic parse trees using a treegrafting procedure. The grafting procedure was implemented as a part of the SIMT effort. Details are spelled out further in Section 4. The workflow for producing semantically-grafted trees is illustrated in Figure 3. Figure 4 illustrates how named-entity tags are grafted onto a parse tree. We note that while our framework is general, we focus the discussion here on the particular semantic elements (named entities and modalities) that"
2010.amta-papers.7,P99-1039,0,0.237079,"Missing"
2013.iwslt-papers.14,2005.iwslt-1.20,0,0.0595346,"er, and a host of factors that alter how an individual speaks (such as heartrate, stress, emotional state). Machine translation accuracy is affected by different factors, such as domain (e.g., newswire, medical, SMS, speech), register, and the typological differences between the languages. Because these technologies are imperfect themselves, their inaccuracies tend to multiply when they are chained together in the task of speech translation. Cross-lingual speech applications are typically built by combining speech recognition and machine translation systems, each trained on disparate datasets [1, 2]. The recognizer makes mistakes, passing text to the MT system with vastly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful f"
2013.iwslt-papers.14,D08-1027,0,0.0480385,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,D09-1030,1,0.309927,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,N10-1024,1,0.431154,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,N12-1006,1,0.287152,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,P11-1122,1,0.487136,"rily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceeded 25 words, it was split on the next utterance boundary. We pr"
2013.iwslt-papers.14,W12-3152,1,0.743004,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,2005.mtsummit-papers.11,0,0.0360278,"d practice, we took steps to deter wholesale use of automated translation services by our translators. • Utterances were presented as images rather than text; this prevented cutting and pasting into online translation services.5 • We obtained translations from Google Translate for the utterances before presenting them to workers. HITs which had a small edit distance from these translations were manually reviewed and rejected if they were too similar (in particular, if they contained many of the same errors). • We also included four consecutive short sentences from the Europarl parallel corpus [17] in each HIT. HITs which had low overlap with the reference translations of these sentences were manually reviewed and rejected if they were of low quality. We obtained four redundant translations of sixty randomly chosen conversations from the Fisher corpus. In total, 115 workers completed 2463 HITs, producing 46,324 utterance-level translations and a little less than half a million words. 2.3. Selection of Preferred Translators We then extended a strategy devised by [16] to select highquality translators from the first round of translations. We designed a second-pass HIT which was used to ra"
2013.iwslt-papers.14,W13-2226,1,0.868991,"performance of the MT system, and we report experiments varying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty •"
2013.iwslt-papers.14,J07-2003,0,0.0593645,"arying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Inter"
2013.iwslt-papers.14,P08-1115,0,0.0451524,"we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Interface Transcript 1-best Lattice Oracle Path Euro 41.8 24.3 32.1 LDC 58.7 35.4 37.1 46.2 ASR 54.6 34.7 35.9 44.3 LDC +ASR 58.7 35.5 36.8 46.3 Table 4: BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems. Training set Interface Tra"
2013.iwslt-papers.14,N12-1047,0,0.0637002,"isher Train, as described above. • ASR. An in-domain model trained on pairs of Spanish ASR outputs and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Trans"
2013.iwslt-papers.14,P02-1040,0,0.106207,"and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Transcript 1-best Lattice Reference 1-best → MT Lattice → MT 1-best → Google sí hablar de cuáles y cosas"
2014.iwslt-papers.13,2005.iwslt-1.20,0,0.0317057,"stems for spontaneous, conversational, human-human speech. In contrast to machine directed or scripted conversations (broadcast news), most conversational speech has by nature, variability in recording environment and vocal registers and a high number of disfluencies and out-of-vocabulary words. It also exhibits difficult challenges associated with code switching and regional dialects. This directly relates to an increase of difficulty for both ASR and SMT systems. Since SLT systems are generally built by feeding the output of the ASR system to an SMT system, each trained on separate datasets [1, 2], errors produced by the systems compound. With respect to Egyptian Arabic specifically, unscripted, spontaneous, telephone conversations have been available through the Callhome Egyptian Arabic corpus (speech and transcripts) since 1997. However, since this dataset did not come with translations for the transcriptions in Arabic, researchers had to resort to using out-of-domain data to train the SMT systems. Transcripts for spontaneous conversations (speech), vary significantly from transcripts for scripted conversations and informal written conversations (web, forum, SMS, chat). To bridge thi"
2014.iwslt-papers.13,N10-1024,1,0.808571,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,D08-1027,0,0.0103059,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,D09-1030,1,0.792712,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,P11-1122,1,0.929626,"ntional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each utterance in the corpus contains channel and segment information. These were inco"
2020.aacl-main.35,P19-4007,0,0.0613488,"Missing"
2020.aacl-main.35,N19-1055,0,0.0381682,"Missing"
2020.aacl-main.35,N18-2118,0,0.0237008,"zon Alexa, and Google Assistant have become pervasive in smartphones and smart speakers. To support a wide range of functions, dialog systems must be able to map a user’s natural language instruction onto the desired skill or API. Performing this mapping is called intent detection. Intent detection is usually formulated as a sentence classification task. Given an utterance (e.g. “wake me up at 8”), a system needs to predict its intent (e.g. “Set an Alarm”). Most modern approaches use neural networks to jointly model intent detection and slot filling (Xu and Sarikaya, 2013; Liu and Lane, 2016; Goo et al., 2018; Zhang et al., 2019). In response to a rapidly growing range of services, more attention has been given to zero-shot intent detection (Ferreira et al., 2015a,b; Yazdani and Henderson, 2015; Chen et al., 2016; Kumar et al., 2017; Gangadharaiah and 1 The data and models are available at https:// github.com/zharry29/wikihow-intent. Narayanaswamy, 2019). While most existing research on intent detection proposed novel model architectures, few have attempted data augmentation. One such work (Hu et al., 2009) showed that models can learn much knowledge that is important for intent detection from mas"
2020.aacl-main.35,2021.ccl-1.108,0,0.177248,"Missing"
2020.aacl-main.35,N19-1380,0,0.0252872,"dozens of domains and services, used in the DSTC8 challenge (Rastogi et al., 2020) with dozens of team submissions. Schemas are provided with at most 4 intents per dialog turn. Examples of these intents include Buy Movie Tickets for a Particular show, Make a Reservation with the Therapist, Book an Appointment at a Hair Stylist, Browse attractions in a given city, etc. At each turn, we use the last 3 utterances as input. An example: “That sounds fun. What other attractions do you recommend? There is a famous place of worship called Akshardham.” The Facebook multilingual datasets (FBen/es/th) (Schuster et al., 2019) is a single-turn multilingual dataset. It is the only multilingual dialog dataset to the best of our knowledge, containing utterances annotated with intents and slots in English (en), Spanish (es), and Thai (th). It involves 12 intents, including Set Reminder, Check Sunrise, Show Alarms, Check Sunset, Cancel Reminder, Show Reminders, Check Time Left on Alarm, Modify Alarm, Cancel Alarm, Find Weather, Set Alarm, and Snooze Alarm. Some example utterances are “Is my alarm set for 10 am today?” “Colocar una alarma para ma˜nana a las 3 am,” Table 2: The accuracy of intent detection on English data"
2020.aacl-main.35,D15-1027,0,0.0171687,"language instruction onto the desired skill or API. Performing this mapping is called intent detection. Intent detection is usually formulated as a sentence classification task. Given an utterance (e.g. “wake me up at 8”), a system needs to predict its intent (e.g. “Set an Alarm”). Most modern approaches use neural networks to jointly model intent detection and slot filling (Xu and Sarikaya, 2013; Liu and Lane, 2016; Goo et al., 2018; Zhang et al., 2019). In response to a rapidly growing range of services, more attention has been given to zero-shot intent detection (Ferreira et al., 2015a,b; Yazdani and Henderson, 2015; Chen et al., 2016; Kumar et al., 2017; Gangadharaiah and 1 The data and models are available at https:// github.com/zharry29/wikihow-intent. Narayanaswamy, 2019). While most existing research on intent detection proposed novel model architectures, few have attempted data augmentation. One such work (Hu et al., 2009) showed that models can learn much knowledge that is important for intent detection from massive online resources such as Wikipedia. We propose a pretraining task based on wikiHow, a comprehensive instructional website with over 110,000 professionally edited articles. Their topics"
2020.aacl-main.35,P19-1519,0,0.0840576,"gle Assistant have become pervasive in smartphones and smart speakers. To support a wide range of functions, dialog systems must be able to map a user’s natural language instruction onto the desired skill or API. Performing this mapping is called intent detection. Intent detection is usually formulated as a sentence classification task. Given an utterance (e.g. “wake me up at 8”), a system needs to predict its intent (e.g. “Set an Alarm”). Most modern approaches use neural networks to jointly model intent detection and slot filling (Xu and Sarikaya, 2013; Liu and Lane, 2016; Goo et al., 2018; Zhang et al., 2019). In response to a rapidly growing range of services, more attention has been given to zero-shot intent detection (Ferreira et al., 2015a,b; Yazdani and Henderson, 2015; Chen et al., 2016; Kumar et al., 2017; Gangadharaiah and 1 The data and models are available at https:// github.com/zharry29/wikihow-intent. Narayanaswamy, 2019). While most existing research on intent detection proposed novel model architectures, few have attempted data augmentation. One such work (Hu et al., 2009) showed that models can learn much knowledge that is important for intent detection from massive online resources"
2020.acl-main.164,1983.tc-1.13,0,0.252777,"Missing"
2020.acl-main.666,D18-2029,0,0.0220026,". . , N } as an integer index into that set of possible next sentences. This strategy resembles negative sampling in word2vec (Mikolov et al., 2013). Our model represents sentences with precomputed vector embeddings. Specifically, sentences are represented by the mean of the 768dimensional contextual word embeddings of the second-to-last layer of BERT (Devlin et al., 2019). This representation has shown to encode more transferable features compare to other layers (Liu et al., 2019). Alternative sentence representations were considered, including embeddings from the universal sentence encoder (Cer et al., 2018) and a weighted mean of the BERT embeddings using inverse document frequency weighting (Zhang et al., 2019). None of these alternatives improved our results however. Motivated by simplicity, we consider a classical multi-layer perceptron (MLP) fθ which takes as input the context sentence embeddings concatenated into a single vector. At the output layer, we perform a softmax operation. If we represent candidate sentences {1, . . . , N } by the embeddings {ei }N i=1 , our model estimates the probability that i is the next 1 Code for ROC Stories experiments can be found at https://github.com/goog"
2020.acl-main.666,P19-1606,0,0.0133728,"Bengio et al., 2003; Devlin et al., 2019), but coherency is still a major challenge (See et al., 2019). The generation of coherent stories has recently been addressed with additional conditioning: Fan et al. (2018) suggest conditioning on a story prompt, Clark et al. (2018) propose collaboration between a generative model and a human writer, and Guan et al. (2019) suggest attending to a commonsense graph relevant to the story plot. Conditioning based on a generated story plan (Martin et al., 2018; Fan et al., 2019; Yao et al., 2019), a se∗ University of Pennsylvania, †Google quence of images (Chandu et al., 2019) or character roles (Liu et al., 2020) have also been considered. Our work is orthogonal to these efforts. Rather than considering additional conditioning, we propose a model which takes as input several sentences of context and selects the best next sentence within a large set of fluent candidate sentences. We leverage pre-trained BERT embeddings (Devlin et al., 2019) to build this sentence-level language model. Given the embeddings of the previous sentences of the story, our model learns to predict a likely embedding of the next sentence. This task isolates the modeling of long-range depende"
2020.acl-main.666,D17-1168,0,0.0226971,"ories train set. The validation and test sets each contain 1.8k stories consisting of four sentences followed by two alternative endings: one ending is coherent with the context; the other is not. The dataset was introduced for the Story Cloze task, inspired by Taylor (1953), where the goal is to select the coherent ending. While the dataset and task were introduced as a way to probe for coherence and commonsense in models trained only on the unlabeled portion, most research derived from this dataset focuses on a supervised setting, using the validation set as a smaller, labeled training set (Chaturvedi et al., 2017; Sun et al., 2019; Cui et al., 2019; Li et al., 2019; Zhou et al., 2019). Our work is faithful to the original task objective. We train solely on the training set, i.e. the model never sees incoherent endings at training time. Model We consider two models, an MLP and a residual MLP. They take as input the previous sentences represented as the concatenation of their embeddings. Alternative context aggregation strategies were considered with recurrent (Sundermeyer et al., 2012) and attention (Vaswani et al., 2017) architectures, without strong empirical advantages. The models maps its input to"
2020.acl-main.666,P03-1069,0,0.128372,"ghout training) or dynamic (picked at random from a larger set for each train batch). In this case, the “vocabulary” of next values to choose from changes with each train step, similar to negative sampling (Mikolov et al., 2013). At test time, novel sentences can be embedded with BERT and scored by our model. Like a classical language model, we optimize for the likelihood of the true next sentence’s embedding. However, when training we found that the sentences from the context (s1 , . . . , st ) often ended up being given very high scores by our model. Inspired by work in sentence reordering (Lapata, 2003; Logeswaran and Lee, 2018), we incorporated an auxiliary loss, which we refer to as CSLoss, that only includes the context sentences s1:t in the distractor set. Lastly, we consider a residual variant of the MLP (referred to as resMLP) with skip connection between layers, as described in He et al. (2016). The residual model trains faster and sometimes achieves higher accuracy than the non-residual model. Though we experimented with recurrent (Sundermeyer et al., 2012) and self-attention (Vaswani et al., 2017) models, we did not observe improvements, perhaps because the input to our model is al"
2020.acl-main.666,N19-1423,0,0.0742832,"ring training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks. 1 Introduction Computer generation of stories and other kinds of creative writing is a challenging endeavor. It entangles two difficult tasks: the generation of fluent natural language and the generation of a coherent storyline. In the recent year, neural language models have made tremendous progress with respect to fluency (Bahdanau et al., 2015; Vaswani et al., 2017; Bengio et al., 2003; Devlin et al., 2019), but coherency is still a major challenge (See et al., 2019). The generation of coherent stories has recently been addressed with additional conditioning: Fan et al. (2018) suggest conditioning on a story prompt, Clark et al. (2018) propose collaboration between a generative model and a human writer, and Guan et al. (2019) suggest attending to a commonsense graph relevant to the story plot. Conditioning based on a generated story plan (Martin et al., 2018; Fan et al., 2019; Yao et al., 2019), a se∗ University of Pennsylvania, †Google quence of images (Chandu et al., 2019) or character roles ("
2020.acl-main.666,N19-1112,0,0.0269171,"ntences, we consider a finite but large set of N valid, fluent sentences. Without loss of generality, we can consider st+1 ∈ {1, . . . , N } as an integer index into that set of possible next sentences. This strategy resembles negative sampling in word2vec (Mikolov et al., 2013). Our model represents sentences with precomputed vector embeddings. Specifically, sentences are represented by the mean of the 768dimensional contextual word embeddings of the second-to-last layer of BERT (Devlin et al., 2019). This representation has shown to encode more transferable features compare to other layers (Liu et al., 2019). Alternative sentence representations were considered, including embeddings from the universal sentence encoder (Cer et al., 2018) and a weighted mean of the BERT embeddings using inverse document frequency weighting (Zhang et al., 2019). None of these alternatives improved our results however. Motivated by simplicity, we consider a classical multi-layer perceptron (MLP) fθ which takes as input the context sentence embeddings concatenated into a single vector. At the output layer, we perform a softmax operation. If we represent candidate sentences {1, . . . , N } by the embeddings {ei }N i=1"
2020.acl-main.666,P18-1082,0,0.0318045,"ext sentence prediction tasks. 1 Introduction Computer generation of stories and other kinds of creative writing is a challenging endeavor. It entangles two difficult tasks: the generation of fluent natural language and the generation of a coherent storyline. In the recent year, neural language models have made tremendous progress with respect to fluency (Bahdanau et al., 2015; Vaswani et al., 2017; Bengio et al., 2003; Devlin et al., 2019), but coherency is still a major challenge (See et al., 2019). The generation of coherent stories has recently been addressed with additional conditioning: Fan et al. (2018) suggest conditioning on a story prompt, Clark et al. (2018) propose collaboration between a generative model and a human writer, and Guan et al. (2019) suggest attending to a commonsense graph relevant to the story plot. Conditioning based on a generated story plan (Martin et al., 2018; Fan et al., 2019; Yao et al., 2019), a se∗ University of Pennsylvania, †Google quence of images (Chandu et al., 2019) or character roles (Liu et al., 2020) have also been considered. Our work is orthogonal to these efforts. Rather than considering additional conditioning, we propose a model which takes as inpu"
2020.acl-main.666,P19-1254,0,0.0556802,"Missing"
2020.acl-main.666,N16-1098,0,0.0679285,"Missing"
2020.acl-main.666,K17-1019,0,0.0605609,"Missing"
2020.acl-main.666,W17-0910,0,0.0607818,"Missing"
2020.acl-main.666,W17-0907,0,0.0214562,"experiment with an auxiliary loss where just sentences from the context were used as distractors. Table 3 reports the results. 4 Results Figure 1: The impact of the number of negative sentences used during training on the rank of the true ending out of 98k distractors. Results are with the resMLP on the 2018 valid set. We evaluate on the Story Cloze task, a binary classification task, as well as on the task of ranking a large set of possible next sentences. are not comparable to our unsupervised approach as they require training on the labeled validation set. The language model approach from Schwartz et al. (2017) also falls into this category. 4.1 4.2 Story Cloze Task Table 1 shows that our method outperforms unsupervised alternatives. The introduction of the CSLoss which considers only context sentences as candidates improves accuracy compared to only using a loss over all possible fifth sentences. For comparison, we include the accuracies of the best unsupervised methods in the literature. Schenk and Chiarcos (2017) construct negative examples for their binary classification task by pairing contexts with random fifth sentences selected from the training set. Peng et al. (2017) train a language model"
2020.acl-main.666,K19-1079,0,0.0210661,"with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks. 1 Introduction Computer generation of stories and other kinds of creative writing is a challenging endeavor. It entangles two difficult tasks: the generation of fluent natural language and the generation of a coherent storyline. In the recent year, neural language models have made tremendous progress with respect to fluency (Bahdanau et al., 2015; Vaswani et al., 2017; Bengio et al., 2003; Devlin et al., 2019), but coherency is still a major challenge (See et al., 2019). The generation of coherent stories has recently been addressed with additional conditioning: Fan et al. (2018) suggest conditioning on a story prompt, Clark et al. (2018) propose collaboration between a generative model and a human writer, and Guan et al. (2019) suggest attending to a commonsense graph relevant to the story plot. Conditioning based on a generated story plan (Martin et al., 2018; Fan et al., 2019; Yao et al., 2019), a se∗ University of Pennsylvania, †Google quence of images (Chandu et al., 2019) or character roles (Liu et al., 2020) have also been considered. Our work is orth"
2020.acl-main.666,N19-1270,0,0.0274585,"ult tasks: the generation of fluent natural language and the generation of a coherent storyline. In the recent year, neural language models have made tremendous progress with respect to fluency (Bahdanau et al., 2015; Vaswani et al., 2017; Bengio et al., 2003; Devlin et al., 2019), but coherency is still a major challenge (See et al., 2019). The generation of coherent stories has recently been addressed with additional conditioning: Fan et al. (2018) suggest conditioning on a story prompt, Clark et al. (2018) propose collaboration between a generative model and a human writer, and Guan et al. (2019) suggest attending to a commonsense graph relevant to the story plot. Conditioning based on a generated story plan (Martin et al., 2018; Fan et al., 2019; Yao et al., 2019), a se∗ University of Pennsylvania, †Google quence of images (Chandu et al., 2019) or character roles (Liu et al., 2020) have also been considered. Our work is orthogonal to these efforts. Rather than considering additional conditioning, we propose a model which takes as input several sentences of context and selects the best next sentence within a large set of fluent candidate sentences. We leverage pre-trained BERT embeddi"
2020.crac-1.14,W19-3816,0,0.0120214,".org/licenses/by/4.0/ License details: 133 Proceedings of the 3rd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2020), pages 133–138, Barcelona, Spain (online), December 12, 2020. participants annotating documents were tracked to gain insights to some of the processes people use in coreference resolution. Lee et al. (2017) proposed an end-to-end coreference resolution model that is based on a neural model; the proposed model takes into consideration all the spans in a given text and given a span, it determines if there is a previous span which is an antecedent. Abzaliev (2019) proposed a coreference resolution model which uses fine-tuned BERT embeddings. In Kocijan et al. (2019), a large coreference resolution dataset was constructed. In Rudinger et al. (2018), gender bias in coreference resolution was studied and it was shown that gender bias exists in some coreference resolution systems. In Zhao et al. (2018), a coreference resolution dataset focused on gender bias was constructed and similar to Zhao et al. (2018) it was shown that some coreference resolution systems are gender biased; a model was proposed to remove these biases while maintaining the performance"
2020.crac-1.14,W18-0701,0,0.0297147,"Missing"
2020.crac-1.14,W17-4405,1,0.884623,"an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event. 1 Introduction Pronoun resolution is an important task in natural language processing (Denis and Baldridge, 2007; Clark and Manning, 2015; Cheri et al., 2016; Yin et al., 2018). However, not a lot of work has been done to address pronoun resolution in Twitter streams related to events. During a televised event such as a Presidential debate or TV-shows, individuals publish tweets about people in the context in which they are being portrayed in the event (Andy et al., 2017; Andy et al., 2019). Some of these tweets make reference to people using third-person singular pronouns like he, him, his, she, and her. For example, here are some tweets about an episode of the TV-show, Game of Thrones season 7 (GoTS7) that were published during the same minute while the episode was airing: (i) ”she took his face”, (ii) ”walder frey?! probably just before arya killed him”, and (iii) ”wait where is arya did she change to his face”. With short text such as these event-related tweets, although some tweets might mention the referents in the same tweets as the pronouns (e.g., in"
2020.crac-1.14,W19-3412,1,0.671668,"solving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event. 1 Introduction Pronoun resolution is an important task in natural language processing (Denis and Baldridge, 2007; Clark and Manning, 2015; Cheri et al., 2016; Yin et al., 2018). However, not a lot of work has been done to address pronoun resolution in Twitter streams related to events. During a televised event such as a Presidential debate or TV-shows, individuals publish tweets about people in the context in which they are being portrayed in the event (Andy et al., 2017; Andy et al., 2019). Some of these tweets make reference to people using third-person singular pronouns like he, him, his, she, and her. For example, here are some tweets about an episode of the TV-show, Game of Thrones season 7 (GoTS7) that were published during the same minute while the episode was airing: (i) ”she took his face”, (ii) ”walder frey?! probably just before arya killed him”, and (iii) ”wait where is arya did she change to his face”. With short text such as these event-related tweets, although some tweets might mention the referents in the same tweets as the pronouns (e.g., in (ii) where ”him” mak"
2020.crac-1.14,W16-1904,0,0.0583122,"yanietie@gmail.com ccb@cis.upenn.edu Derry Tanti Wijaya Boston University wijaya@bu.edu Abstract Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event. 1 Introduction Pronoun resolution is an important task in natural language processing (Denis and Baldridge, 2007; Clark and Manning, 2015; Cheri et al., 2016; Yin et al., 2018). However, not a lot of work has been done to address pronoun resolution in Twitter streams related to events. During a televised event such as a Presidential debate or TV-shows, individuals publish tweets about people in the context in which they are being portrayed in the event (Andy et al., 2017; Andy et al., 2019). Some of these tweets make reference to people using third-person singular pronouns like he, him, his, she, and her. For example, here are some tweets about an episode of the TV-show, Game of Thrones season 7 (GoTS7) that were published during the same minute w"
2020.crac-1.14,P15-1136,0,0.187402,"rsity of Pennsylvania andyanietie@gmail.com ccb@cis.upenn.edu Derry Tanti Wijaya Boston University wijaya@bu.edu Abstract Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event. 1 Introduction Pronoun resolution is an important task in natural language processing (Denis and Baldridge, 2007; Clark and Manning, 2015; Cheri et al., 2016; Yin et al., 2018). However, not a lot of work has been done to address pronoun resolution in Twitter streams related to events. During a televised event such as a Presidential debate or TV-shows, individuals publish tweets about people in the context in which they are being portrayed in the event (Andy et al., 2017; Andy et al., 2019). Some of these tweets make reference to people using third-person singular pronouns like he, him, his, she, and her. For example, here are some tweets about an episode of the TV-show, Game of Thrones season 7 (GoTS7) that were published duri"
2020.crac-1.14,D19-1439,0,0.0110258,"of Reference, Anaphora and Coreference (CRAC 2020), pages 133–138, Barcelona, Spain (online), December 12, 2020. participants annotating documents were tracked to gain insights to some of the processes people use in coreference resolution. Lee et al. (2017) proposed an end-to-end coreference resolution model that is based on a neural model; the proposed model takes into consideration all the spans in a given text and given a span, it determines if there is a previous span which is an antecedent. Abzaliev (2019) proposed a coreference resolution model which uses fine-tuned BERT embeddings. In Kocijan et al. (2019), a large coreference resolution dataset was constructed. In Rudinger et al. (2018), gender bias in coreference resolution was studied and it was shown that gender bias exists in some coreference resolution systems. In Zhao et al. (2018), a coreference resolution dataset focused on gender bias was constructed and similar to Zhao et al. (2018) it was shown that some coreference resolution systems are gender biased; a model was proposed to remove these biases while maintaining the performance on coreference datasets. Not a lot of prior work has been done to resolve pronouns in Twitter data. The"
2020.crac-1.14,D17-1018,0,0.13659,"esolve coreference was proposed. Yin et al. (2018) proposed a self-attention method to model zero pronouns. In Cheri et al. (2016), the eye movements of This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/ License details: 133 Proceedings of the 3rd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2020), pages 133–138, Barcelona, Spain (online), December 12, 2020. participants annotating documents were tracked to gain insights to some of the processes people use in coreference resolution. Lee et al. (2017) proposed an end-to-end coreference resolution model that is based on a neural model; the proposed model takes into consideration all the spans in a given text and given a span, it determines if there is a previous span which is an antecedent. Abzaliev (2019) proposed a coreference resolution model which uses fine-tuned BERT embeddings. In Kocijan et al. (2019), a large coreference resolution dataset was constructed. In Rudinger et al. (2018), gender bias in coreference resolution was studied and it was shown that gender bias exists in some coreference resolution systems. In Zhao et al. (2018)"
2020.crac-1.14,P14-5010,0,0.00256766,"pronoun is referring to. Last person mentioned in tweet by author: For each given tweet t with a third-person singular pronoun, we select the last candidate/character that was mentioned in a tweet by the author of t. Tables 1 and 2 show the results from our algorithm compared to these baselines on the Presidential debate and GoTS7 datasets, respectively. 5.2 Case 2: Tweets with pronoun mentions and possible person referents: In this section, we compare our algorithm to the baseline, Spike per minute, described in Section 5.1. We also compare our algorithm to the Stanford coreference toolkit (Manning et al., 2014) and a neural coreference resolution model (neural model) (Lee et al., 2017); this model, when given a span of text, determines if any of the previous spans of text is an antecedent. Tables 3 and 4 show the results. 6 Error Analysis and Future Work In this work, we focused on gathering context in the form of tweets published in the same minute as the tweet with the pronomial mention. One of the challenges we observed is that in some cases a pronomial mention might make reference to a character or person not mentioned in the same minute, hence in the future, we plan to explore how far back in t"
2020.crac-1.14,N18-2002,0,0.0346805,"Missing"
2020.crac-1.14,C18-1002,0,0.149374,"cb@cis.upenn.edu Derry Tanti Wijaya Boston University wijaya@bu.edu Abstract Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event. 1 Introduction Pronoun resolution is an important task in natural language processing (Denis and Baldridge, 2007; Clark and Manning, 2015; Cheri et al., 2016; Yin et al., 2018). However, not a lot of work has been done to address pronoun resolution in Twitter streams related to events. During a televised event such as a Presidential debate or TV-shows, individuals publish tweets about people in the context in which they are being portrayed in the event (Andy et al., 2017; Andy et al., 2019). Some of these tweets make reference to people using third-person singular pronouns like he, him, his, she, and her. For example, here are some tweets about an episode of the TV-show, Game of Thrones season 7 (GoTS7) that were published during the same minute while the episode wa"
2020.crac-1.14,N18-2003,0,0.0173445,"Lee et al. (2017) proposed an end-to-end coreference resolution model that is based on a neural model; the proposed model takes into consideration all the spans in a given text and given a span, it determines if there is a previous span which is an antecedent. Abzaliev (2019) proposed a coreference resolution model which uses fine-tuned BERT embeddings. In Kocijan et al. (2019), a large coreference resolution dataset was constructed. In Rudinger et al. (2018), gender bias in coreference resolution was studied and it was shown that gender bias exists in some coreference resolution systems. In Zhao et al. (2018), a coreference resolution dataset focused on gender bias was constructed and similar to Zhao et al. (2018) it was shown that some coreference resolution systems are gender biased; a model was proposed to remove these biases while maintaining the performance on coreference datasets. Not a lot of prior work has been done to resolve pronouns in Twitter data. The closest related work to our work is Aktas¸ et al. (2018), which studies pronominal anaphora on conversations in Twitter and constructs a corpus to determine relevant factors for resolving anaphora in Twitter conversation data. Our algori"
2020.emnlp-demos.25,P18-1082,0,0.111956,"Missing"
2020.emnlp-demos.25,L16-1502,0,0.0157854,", with ChatEval (Sedoc et al., 2019) and ConvAI (Pavlopoulos et al., 2019) being two examples. However, RoFT was primarily influenced by other “real or fake” websites that attempt to gamify the detection task, such as http://www. 194 for generated face images and https://faketrump.ai/ for generated Tweets. Our task is similar to the one used for human evaluation in Ippolito et al. (2020), except in their task the text shown to raters was either entirely human-written or entirely machine-generated. The boundary detection task we propose was inspired by the Dialog Breakdown Detection Challenge (Higashinaka et al., 2016), in which the goal is to automatically detect the first system utterance in a conversation between a human and a chatbot system that causes a dialogue breakdown. whichfaceisreal.com/ 6 Conclusion and Future Work In this work, we have introduced RoFT and have shown how it can be used to collect annotations on how well human raters can tell when an article transitions from being human-written to being machine-generated. Ultimately, we plan to use RoFT to conduct a large-scale systematic study of the impact of decoding strategy, fine-tuning dataset, prompt genre, and other factors on the detecta"
2020.emnlp-demos.25,2020.acl-main.164,1,0.4887,"challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off humanwritten transitions to being machine-generated. We show preliminary results of using RoFT to evaluate detection of machine-generated news articles. spacing abbreviations competitive Kolb s atorie labor dash doesnt player ∗ seem local States armor related steel probably Center explained passage 1 much many make Abstract erated by a system from those written by another human (Ippolito et al., 2020; Zellers et al., 2019). However, due to the prohibitive cost of running human evaluation studies, most prior work in this area has been rather limited in scope. For example, analyses usually show results on only a single category of text (news articles, stories, webtext, etc.). This could be problematic since different domains have different levels of named entities, world facts, narrative coherence, and other properties that impact the success of NLG systems. In addition, most papers only evaluate on a very limited selection of decoding strategy hyperparameters. Holtzman et al. (2019) and Ip"
2020.emnlp-demos.25,W19-8643,0,0.0514548,"Missing"
2020.emnlp-demos.25,S19-2102,0,0.0156847,"le to use data mining techniques to extract an error taxonomy from the provided natural langauge description of errors. 5 Related Work Nearly all papers in NLG do some form of human evaluation, usually using Amazon Mechanical Turk (van der Lee et al., 2019). Typically the interfaces for these evaluations are simple web forms. van der Lee et al. (2019) offers a survey of many of these methods. Custom-designed websites for collecting or displaying human evaluations of generated text have become increasingly prominent in the openended dialog domain, with ChatEval (Sedoc et al., 2019) and ConvAI (Pavlopoulos et al., 2019) being two examples. However, RoFT was primarily influenced by other “real or fake” websites that attempt to gamify the detection task, such as http://www. 194 for generated face images and https://faketrump.ai/ for generated Tweets. Our task is similar to the one used for human evaluation in Ippolito et al. (2020), except in their task the text shown to raters was either entirely human-written or entirely machine-generated. The boundary detection task we propose was inspired by the Dialog Breakdown Detection Challenge (Higashinaka et al., 2016), in which the goal is to automatically detect th"
2020.emnlp-demos.25,J09-4008,0,0.0960916,"Missing"
2020.emnlp-demos.25,N19-4011,1,0.826182,"gies. Additionally, it is possible to use data mining techniques to extract an error taxonomy from the provided natural langauge description of errors. 5 Related Work Nearly all papers in NLG do some form of human evaluation, usually using Amazon Mechanical Turk (van der Lee et al., 2019). Typically the interfaces for these evaluations are simple web forms. van der Lee et al. (2019) offers a survey of many of these methods. Custom-designed websites for collecting or displaying human evaluations of generated text have become increasingly prominent in the openended dialog domain, with ChatEval (Sedoc et al., 2019) and ConvAI (Pavlopoulos et al., 2019) being two examples. However, RoFT was primarily influenced by other “real or fake” websites that attempt to gamify the detection task, such as http://www. 194 for generated face images and https://faketrump.ai/ for generated Tweets. Our task is similar to the one used for human evaluation in Ippolito et al. (2020), except in their task the text shown to raters was either entirely human-written or entirely machine-generated. The boundary detection task we propose was inspired by the Dialog Breakdown Detection Challenge (Higashinaka et al., 2016), in which"
2021.dash-1.14,lin-etal-2010-new,0,0.0233881,"dding space. Multilingual Information Retrieval DAPRA KAIROS’ events are similar to the events found in the IARPA BETTER multilingual information retrieval project.11 A future application of TopGuNN could be querying in English and retrieving training examples in another language (or vice versa) by substituting BERT for GigaBERT (Lan et al., 2020) in TopGuNN. With this modification, TopGuNN could help facilitate multilingual retrieval of training examples. 6 Getting started with TopGuNN Related Work Previous work that parallels our work to search and index large corpora includes projects like Lin et al. (2010), which created an index of n-gram counts over a web-scale sized corpus. Similarly, 11 https://www.iarpa.gov/index.php/research-programs/ better 91 Acknowledgements References 2018. Named entity recognition with Bert. Tobias Sterbak Consulting, Akazienstraße 3A, 10823 Berlin, Germany. We would like to thank Erik Bernhardsson for the useful feedback on integrating Annoy indexing. Special thanks to Ashley Nobi for spearheading the annotation effort and Katie Conger at University of Colorado at Boulder for the training sessions on semantic role labeling she gave for the span annotation effort. We"
2021.dash-1.14,2020.acl-main.537,0,0.014535,"pe, sid loc, gpe, fac, per, com, veh, wea, sid com, veh, wea fac, loc, gpe Temporal Start and End Duration (times specific to event) 1 second to multiple years Embedding Model TopGuNN creates contextualized word embeddings for each content word in the corpus and for each query word in the query sentences. We use BERT (Devlin et al., 2019a) to create the embeddings because BERT produces contextually-aware embeddings unlike word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014).3 FastBERT or DistilBERT would also be appropriate choices, but come with an accuracy trade-off for speed (Liu et al., 2020; Sanh et al., 2019). We also investigated running TopGuNN at the sentence-level using sentence embeddings from SBERT and computing averaged sentence embeddings using BERT (Reimers and Gurevych, 2019). Qualitatively, the results from using BERT at the word-level gave us diversity in the results that we desired (see Appendix B). Conflict.Attack a violent physical act causing harm or damage Slot Role Attacker Target Instr./Means Place Corpus 2.3 Retrieving Event Primitives A total of 60 event primitives were annotated using TopGuNN. On average, we were given 2 seed sentences per event and 1-2 vi"
2021.dash-1.14,P10-2041,0,0.0457921,"entire Gigaword corpus can be seen in Table 4. The first query word in the batch of queries takes longer as it must load each Annoy index into memory from disk. For subsequent queries in the batch, the Annoy index is already loaded into memory. (see Section 3.4.1) 10 For example, after searching a query word &quot;identify&quot; on a particular Annoy index all subsequent queried words like “hired” or “launched” on that same Annoy index will leverage the operating system page cache of the Annoy index file and perform faster 90 as an extension to work completed by Lin et al. (1997) and Gao et al. (2002), Moore and Lewis (2010) propose a method for gathering domainspecific training data for languages models for use in tasks such as Machine Translation. By utilizing contextual word embeddings from a modern language model like BERT instead of techniques like n-grams or perplexity analysis as seen in previous approaches, TopGuNN aims to achieve higher quality results. Our work directly builds upon prior research on approximate k-NN algorithms for cosine similarity search. We chose to use the Annoy package for indexing our embeddings in TopGuNN for its particular ability to build on-disk indexes, however, another packag"
2021.dash-1.14,D18-2021,1,0.926766,"ults. We use our look-up dictionaries to return the document, the sentence, and the word of each result. Search results from each of the query words over the Annoy indexes are combined at the end and exported to a .tsv for human annotation and active learning. Indexing All of the embeddings saved in the previous step for each of the 960 partitions are added to an Annoy index, to create 960 Annoy indexes that span our entire corpus. We use Spotify’s Annoy indexing system created by Bernhardsson (2018) for approximate k-NN search, which has been shown to be significantly faster than exact k-NN (Patel et al., 2018). While, there are various competing implementations for approximate k-NN, we ultimately used Annoy to power our similarity search for its 3.4.1 Enhancing Query Performance Sequentially searching each query word against the 960 Annoy indexes before moving on to the next query word is slow. To perform searches more efficiently, we sequentially query each of the 960 Annoy indexes with all query words. This leverages the operating system page cache in such a way that allows for the system to scale better to larger batches of queries. By querying in this manner, we only need to load each of the 96"
2021.dash-1.14,D14-1162,0,0.0890715,"vent’s semantic roles. An example of a KAIROS event primitive is Attack: Label Description 2.2 Slot Argument Constraints per, org, gpe, sid loc, gpe, fac, per, com, veh, wea, sid com, veh, wea fac, loc, gpe Temporal Start and End Duration (times specific to event) 1 second to multiple years Embedding Model TopGuNN creates contextualized word embeddings for each content word in the corpus and for each query word in the query sentences. We use BERT (Devlin et al., 2019a) to create the embeddings because BERT produces contextually-aware embeddings unlike word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014).3 FastBERT or DistilBERT would also be appropriate choices, but come with an accuracy trade-off for speed (Liu et al., 2020; Sanh et al., 2019). We also investigated running TopGuNN at the sentence-level using sentence embeddings from SBERT and computing averaged sentence embeddings using BERT (Reimers and Gurevych, 2019). Qualitatively, the results from using BERT at the word-level gave us diversity in the results that we desired (see Appendix B). Conflict.Attack a violent physical act causing harm or damage Slot Role Attacker Target Instr./Means Place Corpus 2.3 Retrieving Event Primitives"
2021.emnlp-main.124,N19-1253,0,0.127204,"19.7 – 18.8 18.9 23.4 40.6* gu→en – – 0.61 0.7 4.2 9.1 13.8 13.3 – 14.21 en→gu – – 0.61 1.1 4.9 7.8 13.9 15.2 – 4.01 kk→en – – 2.01 2.3 2.8 7.3 7.0 9.0 – 12.51 en→kk – – 0.81 1.0 1.6 2.3 12.1 10.8 – 3.11 ro→en 31.8 33.1 37.64 2.0 22.1 23.2 25.2 32.5 – 39.93 en→ro 33.3 35.2 36.32 1.9 21.6 21.5 28.1 33.0 – 38.53 Table 2: BLEU scores for different models. Reference results are from *: Our implementation, 1: Kim et al. (2020), 2: Li et al. (2020), 3: Liu et al. (2020) (supervised), 4: Tran et al. (2020) (unsupervised with mined parallel data). Projection Previous Method Rasooli and Collins (2019) Ahmad et al. (2019) Kurniawan et al. (2021) Version Token and POS 2.0 2.2 2.2 Wikily translation Gold-standard Parallel data 2.7 Supervised UAS gold/supervised 61.2 gold 38.1 gold 48.3 gold 62.5 supervised 60.2 gold 61.5 supervised 59.1 supervised 84.2 Arabic LAS BLEX 48.8 – 28.0 – 29.9 – 50.7 46.3 48.7 42.1 47.3 42.4 45.3 38.5 79.8 72.7 UAS – – – 46.8 46.2 22.2 21.8 48.0 Kazakh LAS BLEX – – – – – – 28.5 25.0 27.8 14.1 9.3 7.9 9.2 3.8 29.8 13.7 Romanian UAS LAS BLEX 76.3 64.3 – 65.1 54.1 – – – – 74.1 57.7 52.6 73.6 57.4 50.9 75.9 62.4 57.3 75.6 62.0 55.6 90.8 86.0 80.0 Outputs Table 3: Dependency parsing results"
2021.emnlp-main.124,N19-1121,0,0.028365,"that our translation models can be 6 Related Work used in downstream cross-lingual natural language Kim et al. (2020) has shown that unsupervised processing tasks. In the future, we plan to extend translation models often fail to provide good trans- our approach beyond Wikipedia to other comparalation systems for distant languages. Our work ble datasets like the BBC World Service. A clear solves this problem by leveraging the Wikipedia extension of this work is to try our approach on data. Using pivot languages has been used in previ- other cross-lingual tasks. Moreover, as many capous work (Al-Shedivat and Parikh, 2019), as well as tions of the same images in Wikipedia are similar using related languages (Zoph et al., 2016; Nguyen sentences and sometimes translations, multimodal and Chiang, 2017). Our work only explores a sim- machine translation (Specia et al., 2016; Caglayan ple idea of adding one similar language pair. Most et al., 2019; Hewitt et al., 2018; Yao and Wan, likely, adding more language pairs and using ideas 2020) based on this data or the analysis of the data, from recent work might improve the performance. such as whether more similar languages may share Wikipedia is an interesting dataset"
2021.emnlp-main.124,D18-1399,0,0.34504,"oli1∗ Chris Callison-Burch2 Derry Tanti Wijaya3 2 Department 1 Microsoft of Computer and Information Science, University of Pennsylvania of Computer Science, Boston University 3 Department mrasooli@microsoft.com, ccb@seas.upenn.edu, wijaya@bu.edu Abstract could be used in downstream cross-lingual tasks in which annotated data does not exist for some We present a simple but effective approach for languages. There has recently been a great deal leveraging Wikipedia for neural machine transof interest in unsupervised neural machine translation as well as cross-lingual tasks of image lation (e.g. Artetxe et al. (2018a); Lample et al. captioning and dependency parsing without us(2018a,c); Conneau and Lample (2019); Song et al. ing any direct supervision from external paral(2019a); Kim et al. (2020); Tae et al. (2020)). Unlel data or supervised models in the target language. We show that first sentences and titles supervised neural machine translation models ofof linked Wikipedia pages, as well as crossten perform nearly as well as supervised models lingual image captions, are strong signals for when translating between similar languages, but a seed parallel data to extract bilingual dictiothey fail to perf"
2021.emnlp-main.124,J82-2005,0,0.501866,"Missing"
2021.emnlp-main.124,P19-1309,0,0.0133859,"er than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya and Fraser, 2019; Keung et al., 2020; Tran et al., 2020; Kuwanto et al., 2021). Ruiter et al. (2019) focus on using vector similarity of sentences to extract parallel text from Wikipedia. Their work does not leverage structural signals from Wikipedia. Cross-lingual and unsupervised image captioning has been studied in previous work (Gu et al., 2018; Feng et al., 2019; Song et al., 2019b; Gu et al., 2019; Gao et al., 2020; Burns et"
2021.emnlp-main.124,W19-5301,0,0.0464215,"Missing"
2021.emnlp-main.124,W15-3402,0,0.020991,"ower than that of Rasooli and Collins (2019) which uses a combination of multi-source annotation projection and direct model transfer. Our work on Arabic outperforms all previous work and performs even better than using gold-standard parallel data. One clear highlight is our result in Kazakh. As mentioned before, by projecting the part-of-speech tags, we achieve roughly 2 percent absolute improvement. Our final results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et"
2021.emnlp-main.124,bojar-etal-2014-hindencorp,0,0.0445377,"Missing"
2021.emnlp-main.124,N19-1422,0,0.0245614,"Missing"
2021.emnlp-main.124,D14-1179,0,0.0396005,"Missing"
2021.emnlp-main.124,N19-1423,0,0.231203,"These language jection method (Yarowsky et al., 2001; Hwa et al., 1 Our code: https://github.com/rasoolims/ 2005). Our results show that our approach performs2 ImageTranslate. Our modification to Stanza for trainsimilarly compared to using gold-standard parallel ing on partially projected trees: https://github.com/ text in high-resource scenarios, and significantly rasoolims/stanza. 1656 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 models usually mask parts of every input sentence, and try to uncover the masked words (Devlin et al., 2019). The monolingual language models are used along with iterative back-translation (Hoang et al., 2018) to learn unsupervised translation. An input sentence s is translated to t0 using current model θ, then the model assumes that (t0 , s) is a goldstandard translation, and uses the same training objective as of supervised translation. Wikipedia documents are rough translations of each other. Moreover, captions of images in different languages are usually similar but not necessarily direct translations of each other. We leverage this information to extract many parallel sentences from Wikipedia w"
2021.emnlp-main.124,N13-1073,0,0.0449112,"(j) = k and a(i) = m, we project a dependency k → m (i.e. hm = k) to the target side. Previous work (Rasooli and Collins, 2017, 2019) has shown that annotation projection only works when a large amount of translation data exists. In the absence of parallel data, we create artificial parallel data using our translation models. Figure 2 shows an example of annotation projection using translated text. 3 Learning Translation from Wikipedia 3.1 3.2 Data Definitions Bilingual Dictionary Extraction and Cross-Lingual Word Embeddings Having the seed parallel data S, we run unsupervised word alignment (Dyer et al., 2013) in both English-to-target, and target-to-English directions. We use the intersected alignments to extract highly confident word-to-word connections. Finally, we pick the most frequently aligned word for each word in English as translation. This set serves as a bilingual dictionary D. Given two monolingual trained word embeddings ve ∈ RNe ×d and vf ∈ RNf ×d , and the extracted bilingual dictionary D, we use the method of Faruqui and Dyer (2014) to project these two embedding vectors to a shared cross-lingual space.2 This method uses a bilingual dictionary along with The key component of our ap"
2021.emnlp-main.124,D18-1045,0,0.0191775,"5) in which Arabic translations are provided by our translation model. Furthermore, to augment our learning capability, we initialize our decoder with decoding ∗ , and also continue training with parameters of θ both English captioning and translation. Finally, we use the back-translation technique to improve the quality of our models. Backtranslation is done by translating a large amount of monolingual text to and from the target language. The translated texts serve as noisy input text along with the monolingual data as the silverstandard translations. Previous work (Sennrich et al., 2016b; Edunov et al., 2018) has shown that back-translation is a very simple but effective technique to improve the quality of translation models. 4.2 Cross-Lingual Dependency Parsing Henceforth, we refer to this method as one-shot Assuming that we have a large body of monolinback-translation. Another approach is to use iter- gual text, we translate that monolingual text to creative back-translation (Hoang et al., 2018), the ate artificial parallel data. We run unsupervised most popular approach in unsupervised transla- word alignments on the artificial parallel text. Foltion (Artetxe et al., 2018b; Conneau and Lample,"
2021.emnlp-main.124,W19-6721,0,0.0333601,"Missing"
2021.emnlp-main.124,E14-1049,0,0.0172286,"3.2 Data Definitions Bilingual Dictionary Extraction and Cross-Lingual Word Embeddings Having the seed parallel data S, we run unsupervised word alignment (Dyer et al., 2013) in both English-to-target, and target-to-English directions. We use the intersected alignments to extract highly confident word-to-word connections. Finally, we pick the most frequently aligned word for each word in English as translation. This set serves as a bilingual dictionary D. Given two monolingual trained word embeddings ve ∈ RNe ×d and vf ∈ RNf ×d , and the extracted bilingual dictionary D, we use the method of Faruqui and Dyer (2014) to project these two embedding vectors to a shared cross-lingual space.2 This method uses a bilingual dictionary along with The key component of our approach is to leverage the multilingual cues from linked Wikipedia pages across languages. Wikipedia is a great comparable data in which many of its pages explain entities in the world in different languages. In most cases, 2 There are more recent approaches such as (Lample et al., first sentences define or introduce the mentioned 2018b). Comparing different embedding methods is not the entity in that page (e.g. Figure 1). Therefore, we focus of"
2021.emnlp-main.124,L18-1550,0,0.0435664,"Missing"
2021.emnlp-main.124,W18-6317,0,0.019348,"results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya and Fraser, 2019; Keung et al., 2020; Tran et al., 2020; Kuwanto et al., 2021). Ruiter et al. (2019) focus on using vector similarity of sentences to extract parallel text from Wikipedia. Their work does not leverage structural signals from Wikipedia. Cross-lingual and unsupervised image captioning has been studied in previous work (Gu et al., 2018; Feng et al., 2019; Song et al., 2"
2021.emnlp-main.124,P19-1118,0,0.0221684,"al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya and Fraser, 2019; Keung et al., 2020; Tran et al., 2020; Kuwanto et al., 2021). Ruiter et al. (2019) focus on using vector similarity of sentences to extract parallel text from Wikipedia. Their work does not leverage structural signals from Wikipedia. Cross-lingual and unsupervised image captioning has been studied in previous work (Gu et al., 2018; Feng et al., 2019; Song et al., 2019b; Gu et al., 2019; Gao et al., 2020; Burns et al., 2020). Unlike previous work, we do not have a supervised translation model. Cross-lingual transfer of dependency parser have a long history. We encourage the reader to read a r"
2021.emnlp-main.124,P18-1239,1,0.537703,"like the BBC World Service. A clear solves this problem by leveraging the Wikipedia extension of this work is to try our approach on data. Using pivot languages has been used in previ- other cross-lingual tasks. Moreover, as many capous work (Al-Shedivat and Parikh, 2019), as well as tions of the same images in Wikipedia are similar using related languages (Zoph et al., 2016; Nguyen sentences and sometimes translations, multimodal and Chiang, 2017). Our work only explores a sim- machine translation (Specia et al., 2016; Caglayan ple idea of adding one similar language pair. Most et al., 2019; Hewitt et al., 2018; Yao and Wan, likely, adding more language pairs and using ideas 2020) based on this data or the analysis of the data, from recent work might improve the performance. such as whether more similar languages may share Wikipedia is an interesting dataset for solving more similar captions (Khani et al., 2021) are other NLP problems including machine translation (Li interesting avenues. 1663 Acknowledgments We would like to thank reviewers and the editor for their useful comments. We also would like to thank Alireza Zareian, Daniel (Joongwon) Kim, Qing Sun, and Afra Feyza Akyurek for their help an"
2021.emnlp-main.124,W18-2703,0,0.27459,"olims/ 2005). Our results show that our approach performs2 ImageTranslate. Our modification to Stanza for trainsimilarly compared to using gold-standard parallel ing on partially projected trees: https://github.com/ text in high-resource scenarios, and significantly rasoolims/stanza. 1656 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 models usually mask parts of every input sentence, and try to uncover the masked words (Devlin et al., 2019). The monolingual language models are used along with iterative back-translation (Hoang et al., 2018) to learn unsupervised translation. An input sentence s is translated to t0 using current model θ, then the model assumes that (t0 , s) is a goldstandard translation, and uses the same training objective as of supervised translation. Wikipedia documents are rough translations of each other. Moreover, captions of images in different languages are usually similar but not necessarily direct translations of each other. We leverage this information to extract many parallel sentences from Wikipedia without using any external supervision. In this section, we describe our algorithm which is briefly sh"
2021.emnlp-main.124,2020.tacl-1.53,0,0.0323199,"2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya and Fraser, 2019; Keung et al., 2020; Tran et al., 2020; Kuwanto et al., 2021). Ruiter et al. (2019) focus on using vector similarity of sentences to extract parallel text from Wikipedia. Their work does not leverage structural signals from Wikipedia. Cross-lingual and unsupervised image captioning has been studied in previous work (Gu et al., 2018; Feng et al., 2019; Song et al., 2019b; Gu et al., 2019; Gao et al., 2020; Burns et al., 2020). Unlike previous work, we do not have a supervised translation model. Cross-lingual transfer of dependency parser have a long history. We encourage the reader to read a recent survey on this"
2021.emnlp-main.124,2021.naacl-main.19,1,0.521495,"es in Wikipedia are similar using related languages (Zoph et al., 2016; Nguyen sentences and sometimes translations, multimodal and Chiang, 2017). Our work only explores a sim- machine translation (Specia et al., 2016; Caglayan ple idea of adding one similar language pair. Most et al., 2019; Hewitt et al., 2018; Yao and Wan, likely, adding more language pairs and using ideas 2020) based on this data or the analysis of the data, from recent work might improve the performance. such as whether more similar languages may share Wikipedia is an interesting dataset for solving more similar captions (Khani et al., 2021) are other NLP problems including machine translation (Li interesting avenues. 1663 Acknowledgments We would like to thank reviewers and the editor for their useful comments. We also would like to thank Alireza Zareian, Daniel (Joongwon) Kim, Qing Sun, and Afra Feyza Akyurek for their help and useful comments througout this project. This work is supported in part by the DARPA HR001118S0044 (the LwLL program), and the Department of the Air Force FA8750-19- 2-3334 (Semi-supervised Learning of Multimodal Representations). The U.S. Government is authorized to reproduce and distribute reprints for"
2021.emnlp-main.124,2020.eamt-1.5,0,0.220251,"t mrasooli@microsoft.com, ccb@seas.upenn.edu, wijaya@bu.edu Abstract could be used in downstream cross-lingual tasks in which annotated data does not exist for some We present a simple but effective approach for languages. There has recently been a great deal leveraging Wikipedia for neural machine transof interest in unsupervised neural machine translation as well as cross-lingual tasks of image lation (e.g. Artetxe et al. (2018a); Lample et al. captioning and dependency parsing without us(2018a,c); Conneau and Lample (2019); Song et al. ing any direct supervision from external paral(2019a); Kim et al. (2020); Tae et al. (2020)). Unlel data or supervised models in the target language. We show that first sentences and titles supervised neural machine translation models ofof linked Wikipedia pages, as well as crossten perform nearly as well as supervised models lingual image captions, are strong signals for when translating between similar languages, but a seed parallel data to extract bilingual dictiothey fail to perform well in low-resource or disnaries and cross-lingual word embeddings for tant languages (Kim et al., 2020) or out-of-domain mining parallel text from Wikipedia. Our fimonolingual da"
2021.emnlp-main.124,2005.mtsummit-papers.11,0,0.412069,"Missing"
2021.emnlp-main.124,P07-2045,1,0.0165718,"zes of different types of datasets in our experiments. We pick comparable candidates for sentence pairs whose lengths are within a range of half to twice of each other. As we see, the final size of mined datasets heavily depends on the number of paired English-target language Wikipedia documents. We train our translation models initialized by pretrained models. More details about our hyperparameters are in the supplementary material. All of our evaluations are conducted using SacreBLEU (Post, 2018) except for en↔ro in which we use BLEU score (Papineni et al., 2002) from Moses decoder scripts (Koehn et al., 2007) for the sake of comparison to previous work. Image Captioning We use the Flickr (Hodosh et al., 2013) and MS-Coco (Chen et al., 2015) datasets for English4 , and the gold-standard Arabic Flickr dataset (ElJundi. et al., 2020) for evaluation. The Arabic test set has 1000 images with 3 captions Monolingual and Translation Datasets We use a shared SentencePiece vocabulary (Kudo and Richardson, 2018) with size 60K. Table 1 shows 3 the sizes of Wikipedia data in different languages. https://github.com/NVIDIA/apex 4 For evaluation, we use the Arabic-English UN We have also tried Conceptual Captions"
2021.emnlp-main.124,D18-2012,0,0.15379,"ingual language These languagewe about the reader models mask parts ofdictionary every inputtosentence, see Kübler et al. (2009). use theusually extracted bilingual boost the to transformer-based translation model (Vaswani accuracy of the scoring function. For a pair of sen- et al., 2017) with a six-layer BERT-based (Detences (s, t) where s = s1 . . . sn and t = t1 . . . tm , vlin et al., 2019) encoder-decoder architecture 1658 parallel data P , 3) t (x|m) translates input x given model m, , 4) from HuggingFace (Wolf et al., 2019) and Pytorch (Paszke et al., 2019) with a shared SentencePiece (Kudo and Richardson, 2018) vocabulary. All input and output token embeddings are summed up with the language id embedding. First tokens of every input and output sentence are shown by the language ID. Our training pipeline assumes that the encoder and decoder are shared across different languages, except that we use a separate output layer for each language in order to prevent input copying (Artetxe et al., 2018b; Sen et al., 2019). We pretrain the model on a tuple of three Wikipedia datasets for the three languages g, f , and e using the MASS model (Song et al., 2019a). The MASS model masks a contiguous span of input"
2021.emnlp-main.124,L18-1548,0,0.0792649,"Missing"
2021.emnlp-main.124,2021.eacl-main.254,0,0.08317,"Missing"
2021.emnlp-main.124,D12-1127,0,0.0681279,"Missing"
2021.emnlp-main.124,W11-2206,0,0.0374426,"t of Ahmad et al. (2019) and slightly lower than that of Rasooli and Collins (2019) which uses a combination of multi-source annotation projection and direct model transfer. Our work on Arabic outperforms all previous work and performs even better than using gold-standard parallel data. One clear highlight is our result in Kazakh. As mentioned before, by projecting the part-of-speech tags, we achieve roughly 2 percent absolute improvement. Our final results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on un"
2021.emnlp-main.124,2020.tacl-1.47,0,0.0310488,"Missing"
2021.emnlp-main.124,P14-1126,0,0.0310902,"of translation models. 4.2 Cross-Lingual Dependency Parsing Henceforth, we refer to this method as one-shot Assuming that we have a large body of monolinback-translation. Another approach is to use iter- gual text, we translate that monolingual text to creative back-translation (Hoang et al., 2018), the ate artificial parallel data. We run unsupervised most popular approach in unsupervised transla- word alignments on the artificial parallel text. Foltion (Artetxe et al., 2018b; Conneau and Lample, lowing previous work (Rasooli and Collins, 2015; 2019; Song et al., 2019a). The main difference Ma and Xia, 2014), we run Giza++ (Och and Ney, 1659 This is an open box containing four cucumbers. .وهذا صندوق مفتوح يحتوي على أربعة خيار An open food container box with four unknown food items. .صندوق حاوية طعام مفتوح مع أربعة مواد غذائية مجهولة A small box filled with four green vegetables. .ضراءKضروات اKمربع صغير مليء بأربعة ا An opened box of four chocolate bananas. .وزPعلبة مفتوحة من أربعة من ا An open box contains an unknown, purple object رجوانTمربع مفتوح يحتوي على كائن غير معروف ا Figure 4: An image from MS-Coco (Chen et al., 2015) with gold-standard English captions, and Arabic trans"
2021.emnlp-main.124,2020.wmt-1.68,0,0.0150473,"e et al. (2020)). Unlel data or supervised models in the target language. We show that first sentences and titles supervised neural machine translation models ofof linked Wikipedia pages, as well as crossten perform nearly as well as supervised models lingual image captions, are strong signals for when translating between similar languages, but a seed parallel data to extract bilingual dictiothey fail to perform well in low-resource or disnaries and cross-lingual word embeddings for tant languages (Kim et al., 2020) or out-of-domain mining parallel text from Wikipedia. Our fimonolingual data (Marchisio et al., 2020). In pracnal model achieves high BLEU scores that are tice, the highest need for unsupervised models is close to or sometimes higher than strong suto expand beyond high resource, similar European pervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 language pairs. from our model in English-to-Kazakh. MoreThere are two key goals in this paper: Our first over, we tailor our “wikily” supervised transgoal is developing accurate translation models for lation models to unsupervised image captionlow-resource distant languages without any superviing, and cross-lingual"
2021.emnlp-main.124,I17-2050,0,0.0441105,"Missing"
2021.emnlp-main.124,J03-1002,0,0.159227,"the seed parallel data: S = F ∪ C ∪ T . Annotation projection Annotation projection is an effective method for transferring supervised annotation from a rich-resource language to a low-resource language through translated text (Yarowsky et al., 2001). Having a parallel data P = {(si , ti )}ni=1 , and supervised source annotations for source sentences si , we transfer those annotations through word translation links (j) (j) 0 ≤ ai ≤ |ti |for 1 ≤ j ≤ |si |where ai = 0 shows a null alignment. The alignment links are learned in an unsupervised fashion using unsupervised word alignment algorithms (Och and Ney, 2003a). In dependency parsing, if hi = j and a(j) = k and a(i) = m, we project a dependency k → m (i.e. hm = k) to the target side. Previous work (Rasooli and Collins, 2017, 2019) has shown that annotation projection only works when a large amount of translation data exists. In the absence of parallel data, we create artificial parallel data using our translation models. Figure 2 shows an example of annotation projection using translated text. 3 Learning Translation from Wikipedia 3.1 3.2 Data Definitions Bilingual Dictionary Extraction and Cross-Lingual Word Embeddings Having the seed parallel da"
2021.emnlp-main.124,P02-1040,0,0.110027,"y of 0.1. Translation Training Table 1 shows the sizes of different types of datasets in our experiments. We pick comparable candidates for sentence pairs whose lengths are within a range of half to twice of each other. As we see, the final size of mined datasets heavily depends on the number of paired English-target language Wikipedia documents. We train our translation models initialized by pretrained models. More details about our hyperparameters are in the supplementary material. All of our evaluations are conducted using SacreBLEU (Post, 2018) except for en↔ro in which we use BLEU score (Papineni et al., 2002) from Moses decoder scripts (Koehn et al., 2007) for the sake of comparison to previous work. Image Captioning We use the Flickr (Hodosh et al., 2013) and MS-Coco (Chen et al., 2015) datasets for English4 , and the gold-standard Arabic Flickr dataset (ElJundi. et al., 2020) for evaluation. The Arabic test set has 1000 images with 3 captions Monolingual and Translation Datasets We use a shared SentencePiece vocabulary (Kudo and Richardson, 2018) with size 60K. Table 1 shows 3 the sizes of Wikipedia data in different languages. https://github.com/NVIDIA/apex 4 For evaluation, we use the Arabic-E"
2021.emnlp-main.124,W11-1212,0,0.0215614,"ch is much higher than that of Ahmad et al. (2019) and slightly lower than that of Rasooli and Collins (2019) which uses a combination of multi-source annotation projection and direct model transfer. Our work on Arabic outperforms all previous work and performs even better than using gold-standard parallel data. One clear highlight is our result in Kazakh. As mentioned before, by projecting the part-of-speech tags, we achieve roughly 2 percent absolute improvement. Our final results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers"
2021.emnlp-main.124,W18-6319,0,0.082284,"rate of 10−4 , 4000 warm-up steps, and dropout probability of 0.1. Translation Training Table 1 shows the sizes of different types of datasets in our experiments. We pick comparable candidates for sentence pairs whose lengths are within a range of half to twice of each other. As we see, the final size of mined datasets heavily depends on the number of paired English-target language Wikipedia documents. We train our translation models initialized by pretrained models. More details about our hyperparameters are in the supplementary material. All of our evaluations are conducted using SacreBLEU (Post, 2018) except for en↔ro in which we use BLEU score (Papineni et al., 2002) from Moses decoder scripts (Koehn et al., 2007) for the sake of comparison to previous work. Image Captioning We use the Flickr (Hodosh et al., 2013) and MS-Coco (Chen et al., 2015) datasets for English4 , and the gold-standard Arabic Flickr dataset (ElJundi. et al., 2020) for evaluation. The Arabic test set has 1000 images with 3 captions Monolingual and Translation Datasets We use a shared SentencePiece vocabulary (Kudo and Richardson, 2018) with size 60K. Table 1 shows 3 the sizes of Wikipedia data in different languages."
2021.emnlp-main.124,2020.acl-demos.14,0,0.124812,"ome nivelul statului , case compound nmod obj mark pentru punct a mark of case det the problems elimina unele obj dintre nmod case . probleme . obl root advcl punct Figure 2: An example of annotation projection for which the source on top is a translation of the Romanian target via our wikily translation model. The supervised source tree is projected using intersected word alignments. Figure 2: An example of annotation projection for which the source (English, on top) is a translation of the target (Romanian) with our wikily translation model. The source side is parsed with supervised Stanza (Qi et al., 2020) and the parse tree is projected using Giza++ (Och and Ney, 2003) alignments. Aswith shown in the figure, after intersected filtering sentence pairs different numerDefinitions: 1) e is English, f is the foreign language, and g is a lansome words have missing dependencies. guage similar to f , 2) learn_dict (P ) extracts a bilingual dictionary from ical values (e.g. sentences containing 2019 in the source and 1987 in the target), we use a modified pretrain (x) pretrains on monolingual data x using MASS (Song et al., version of cosinethe similarity between words: Supervised neural machine transl"
2021.emnlp-main.124,D15-1039,1,0.889634,"Missing"
2021.emnlp-main.124,Q17-1020,1,0.844906,"source language to a low-resource language through translated text (Yarowsky et al., 2001). Having a parallel data P = {(si , ti )}ni=1 , and supervised source annotations for source sentences si , we transfer those annotations through word translation links (j) (j) 0 ≤ ai ≤ |ti |for 1 ≤ j ≤ |si |where ai = 0 shows a null alignment. The alignment links are learned in an unsupervised fashion using unsupervised word alignment algorithms (Och and Ney, 2003a). In dependency parsing, if hi = j and a(j) = k and a(i) = m, we project a dependency k → m (i.e. hm = k) to the target side. Previous work (Rasooli and Collins, 2017, 2019) has shown that annotation projection only works when a large amount of translation data exists. In the absence of parallel data, we create artificial parallel data using our translation models. Figure 2 shows an example of annotation projection using translated text. 3 Learning Translation from Wikipedia 3.1 3.2 Data Definitions Bilingual Dictionary Extraction and Cross-Lingual Word Embeddings Having the seed parallel data S, we run unsupervised word alignment (Dyer et al., 2013) in both English-to-target, and target-to-English directions. We use the intersected alignments to extract h"
2021.emnlp-main.124,N19-1385,1,0.902412,"Missing"
2021.emnlp-main.124,resnik-1998-parallel,0,0.411335,"entioned before, by projecting the part-of-speech tags, we achieve roughly 2 percent absolute improvement. Our final results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya and Fraser, 2019; Keung et al., 2020; Tran et al., 2020; Kuwanto et al., 2021). Ruiter et al. (2019) focus on using vector similarity of sentences to extract parallel text from Wikipedia. Their work does not leverage structural signals from Wikipedia. Cross-lingual"
2021.emnlp-main.124,J03-3002,0,0.338462,"e, by projecting the part-of-speech tags, we achieve roughly 2 percent absolute improvement. Our final results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya and Fraser, 2019; Keung et al., 2020; Tran et al., 2020; Kuwanto et al., 2021). Ruiter et al. (2019) focus on using vector similarity of sentences to extract parallel text from Wikipedia. Their work does not leverage structural signals from Wikipedia. Cross-lingual and unsupervised image c"
2021.emnlp-main.124,P19-1178,0,0.0357891,"Missing"
2021.emnlp-main.124,2021.eacl-main.115,0,0.0614792,"Missing"
2021.emnlp-main.124,P19-1297,0,0.0212048,"itecture 1658 parallel data P , 3) t (x|m) translates input x given model m, , 4) from HuggingFace (Wolf et al., 2019) and Pytorch (Paszke et al., 2019) with a shared SentencePiece (Kudo and Richardson, 2018) vocabulary. All input and output token embeddings are summed up with the language id embedding. First tokens of every input and output sentence are shown by the language ID. Our training pipeline assumes that the encoder and decoder are shared across different languages, except that we use a separate output layer for each language in order to prevent input copying (Artetxe et al., 2018b; Sen et al., 2019). We pretrain the model on a tuple of three Wikipedia datasets for the three languages g, f , and e using the MASS model (Song et al., 2019a). The MASS model masks a contiguous span of input tokens, and recovers that span in the output sequence. To facilitate multi-task learning with image captioning, our model has an image encoder that is used in cases of image captioning (more details in §4.1). In other words, the decoder is shared between the translation and captioning tasks. We use the pretrained ResNet-152 model (He et al., 2016) from Pytorch to encode every input image. We extract the fi"
2021.emnlp-main.124,W16-2323,0,0.112479,"S-Coco (Chen et al., 2015) in which Arabic translations are provided by our translation model. Furthermore, to augment our learning capability, we initialize our decoder with decoding ∗ , and also continue training with parameters of θ both English captioning and translation. Finally, we use the back-translation technique to improve the quality of our models. Backtranslation is done by translating a large amount of monolingual text to and from the target language. The translated texts serve as noisy input text along with the monolingual data as the silverstandard translations. Previous work (Sennrich et al., 2016b; Edunov et al., 2018) has shown that back-translation is a very simple but effective technique to improve the quality of translation models. 4.2 Cross-Lingual Dependency Parsing Henceforth, we refer to this method as one-shot Assuming that we have a large body of monolinback-translation. Another approach is to use iter- gual text, we translate that monolingual text to creative back-translation (Hoang et al., 2018), the ate artificial parallel data. We run unsupervised most popular approach in unsupervised transla- word alignments on the artificial parallel text. Foltion (Artetxe et al., 2018"
2021.emnlp-main.124,P16-1009,0,0.260844,"S-Coco (Chen et al., 2015) in which Arabic translations are provided by our translation model. Furthermore, to augment our learning capability, we initialize our decoder with decoding ∗ , and also continue training with parameters of θ both English captioning and translation. Finally, we use the back-translation technique to improve the quality of our models. Backtranslation is done by translating a large amount of monolingual text to and from the target language. The translated texts serve as noisy input text along with the monolingual data as the silverstandard translations. Previous work (Sennrich et al., 2016b; Edunov et al., 2018) has shown that back-translation is a very simple but effective technique to improve the quality of translation models. 4.2 Cross-Lingual Dependency Parsing Henceforth, we refer to this method as one-shot Assuming that we have a large body of monolinback-translation. Another approach is to use iter- gual text, we translate that monolingual text to creative back-translation (Hoang et al., 2018), the ate artificial parallel data. We run unsupervised most popular approach in unsupervised transla- word alignments on the artificial parallel text. Foltion (Artetxe et al., 2018"
2021.emnlp-main.124,P18-1238,0,0.0642128,"Missing"
2021.emnlp-main.124,W16-2346,0,0.389821,"Missing"
2021.emnlp-main.124,L16-1680,0,0.0174761,"ning batch contains 20 images. We accumulate gradients for 16 steps, and use a dropout of 0.1 for the projected image output representations. Other training parameters are the same as our translation training. To make our pipeline fully unsupervised, we use translated development sets to pick the best checkpoint during training. Dependency Parsing We use the Universal Dependencies v2.7 collection (Zeman et al., 2020) for Arabic, Kazakh, and Romanian. We use the Stanza (Qi et al., 2020) pretrained supervised models for getting supervised parse trees for Arabic and Romanian, and use the UDPipe (Straka et al., 2016) pretrained model for Kazakh. We translate about 2 million sentences from each language to English, and also 2 million English sentences to Arabic. We use a simple modification to Stanza to facilitate training on partially projected trees by masking dependency and label assignments for words with missing dependencies. All of our training on projected dependencies is blindly conducted with 100k training steps with default parameters of Stanza (Qi et al., 2020). As for gold-standard parallel data, we use our supervised translation training data for Romanian-English and KazakhEnglish and use a sa"
2021.emnlp-main.124,D17-1152,1,0.827253,"d Collins (2019) which uses a combination of multi-source annotation projection and direct model transfer. Our work on Arabic outperforms all previous work and performs even better than using gold-standard parallel data. One clear highlight is our result in Kazakh. As mentioned before, by projecting the part-of-speech tags, we achieve roughly 2 percent absolute improvement. Our final results on Kazakh are significantly higher than that of using gold-standard parallel text (7K sentences). et al., 2012; Patry and Langlais, 2011; Lin et al., 2011; Tufi¸s et al., 2013; Barrón-Cedeño et al., 2015; Wijaya et al., 2017; Ruiter et al., 2019; Srinivasan et al., 2021). The WikiMatrix data (Schwenk et al., 2019a) is the most similar effort to ours in terms of using Wikipedia, but with using supervised translation models. Bitext mining has a longer history of research (Resnik, 1998; Resnik and Smith, 2003) in which most efforts are spent on using a seed supervised translation model (Guo et al., 2018; Schwenk et al., 2019b; Artetxe and Schwenk, 2019; Schwenk et al., 2019a; Jones and Wijaya, 2021). Recently, a number of papers have focused on unsupervised extraction of parallel data (Ruiter et al., 2019; Hangya an"
2021.emnlp-main.124,2020.acl-main.400,0,0.533099,"Missing"
2021.emnlp-main.124,H01-1035,0,0.545739,"ng show a BLEU score of 5.72 that is high-quality translations of MS-COCO (Chen et al., slightly better than a supervised captioning model 2015) and Flickr (Hodosh et al., 2013) datasets, and with a BLEU score of 5.22. As another task, in detrain a cross-lingual captioning model in a pendency parsing, weimage first translate a large amount multi-task pipeline paired translation of monolingual data usingwith our machine translation models in and which the model is initialized by the parameters then apply transfer using the annotation profrom ourmethod translation model. Our results Arajection (Yarowsky et al., 2001; Hwaon et al., bic2005). captioning show a BLEU of 5.72 that is Our results show that ourscore approach performs similarly compared using gold-standard parallel slightly better than atosupervised captioning model texta in high-resource scenarios, and significantly with BLEU score of 5.22. As another task, in debetter in low-resource languages. pendency parsing, we first translate a large amount A summary of ourusing contribution is as follows: of monolingual data our translation models better in low-resource proach towards usinglanguages. the Wikipedia monoA summary of our contribution is as"
2021.emnlp-main.124,L16-1561,0,0.106806,"tion of ti 2documents l2 . For a whigh-quality decoder preword embedding vectors ve and vf , 3) Set of linked pages from Wikipedia translation model, we usually need a large amount dicts the masked words. These monolingual lanCOMP , their aligned titles T , and their first sentence pairs F , 4) Set of Using the above definition word similarity, (e,g) of parallel Arabic-English United guage models are used along of with iterative back-we paired imagetext, captionse.g. C, andthe 5) Gold-standard parallel data P . use the average-maximum similarity between pairs Algorithm: Nations parallel text (Ziemski et al., 2016) con- translation (Hoang et al., 2018) to learn unsuper→ Learn bilingual dictionary and embeddings of sentences. tains n F⇠∪18M vised translation. In other words, an input sentence S= C ∪ T sentences. Neural machine transPn D (f,e) = learn_dict (S) lation(g,e) uses sequence-to-sequence models with ats is translated to t0 using current model ✓.i ,Then maxm sim(s ti ) j=1 i=1 (e,g) D = learn_dict (P ) . Related language score(s, t) = 0 tention (Cho 0et al., 2014; Bahdanau et al., 2015; the model assumes that (t , s) is a gold-standard n Learn ve → ve and vf → vf0 using D (f,e) ∪ D (g,e) Vaswani"
2021.emnlp-main.124,D16-1163,0,0.0216629,") has shown that unsupervised processing tasks. In the future, we plan to extend translation models often fail to provide good trans- our approach beyond Wikipedia to other comparalation systems for distant languages. Our work ble datasets like the BBC World Service. A clear solves this problem by leveraging the Wikipedia extension of this work is to try our approach on data. Using pivot languages has been used in previ- other cross-lingual tasks. Moreover, as many capous work (Al-Shedivat and Parikh, 2019), as well as tions of the same images in Wikipedia are similar using related languages (Zoph et al., 2016; Nguyen sentences and sometimes translations, multimodal and Chiang, 2017). Our work only explores a sim- machine translation (Specia et al., 2016; Caglayan ple idea of adding one similar language pair. Most et al., 2019; Hewitt et al., 2018; Yao and Wan, likely, adding more language pairs and using ideas 2020) based on this data or the analysis of the data, from recent work might improve the performance. such as whether more similar languages may share Wikipedia is an interesting dataset for solving more similar captions (Khani et al., 2021) are other NLP problems including machine translati"
2021.emnlp-main.500,P17-1099,0,0.0505589,"I S PLIT for evaluation, because this corpus was constructed explicitly to be used only as training data, as it contains inherent noise and biases. While B I SECT contains 928,440/9,079 train and dev pairs, W IKI S PLIT contains 989,944/5,000 train and dev pairs. Note that we constructed B I SECT test set by manually selecting 583 high-quality sentence splits from 1000 random source-target pairs from EMEA and JRCACQUIS corpora. We compare our approach with Copy512 (Aharoni and Goldberg, 2018), a state-of-the-art model consisting of an attention-based LSTM encoderdecoder with a copy mechanism (See et al., 2017). We use our base model trained on W IKI S PLIT (Rothe et al., 2020) as another state-of-the-art baseline. 5.2 Automatic Evaluation Existing automatic metrics, such as BLEU (Papineni et al., 2002) and SAMSA (Sulem et al., 2018), yˆi =(1 − δi )xi + δi yi are not optimal for the Split and Rephrase task as ( they rely on lexical overlap between the output and 0, if xi is copied δi = the target (or source) and underestimate the split1, otherwise ting capability of the models that rephrase often. where m is the number of training examples and We focus on BERTScore (Zhang et al., 2020b) and yˆ&lt;i rep"
2021.emnlp-main.500,D18-1081,0,0.028008,"m the classifier and the Transformer using the cross entropy loss and our custom split-focused loss. We provide model and training details in Appendix A. 5 Experiments and Results In this section, we compare different split and rephrase models trained on our new B I SECT corpus. We also conduct a carefully designed human evaluation as automatic metrics are not totally reliable. Our model trained on B I SECT establishes a new start-of-the-art for the task. 5.1 Data and Baselines We train the models on B I SECT and W IKI S PLIT corpora. For evaluation, we select the B I SECT and HS PLIT-W IKI (Sulem et al., 2018) test sets to represent splitting with a high degree and minimal of rephrasing respectively. HS PLIT-W IKI is a human annotated dataset with 359 complex sentences and 4 references for each complex sentence. Following previous work (Botha et al., 2018; Zhang et al., 2020a), we do not use W IKI S PLIT for evaluation, because this corpus was constructed explicitly to be used only as training data, as it contains inherent noise and biases. While B I SECT contains 928,440/9,079 train and dev pairs, W IKI S PLIT contains 989,944/5,000 train and dev pairs. Note that we constructed B I SECT test set b"
2021.emnlp-main.500,tiedemann-nygaard-2004-opus,0,0.14168,"lit and Rephrase data that is both meaning preserving and sufficient in size dent, 2014). However, the structural paraphrasing required to split a sentence makes for an interest- for training, we present the B I SECT corpus. ing problem in itself, with many downstream NLP 3.1 Corpus Creation Procedure applications. Thus, Narayan et al. (2017) proposed the Split and Rephrase task, and introduced the The construction of the B I SECT corpus relies W EB S PLIT corpus, created by aligning sentences on leveraging the sentence-level alignments from in WebNLG (Gardent et al., 2017). W EB S PLIT OPUS (Tiedemann and Nygaard, 2004), a publicly contains duplicate instances and phrasal repetitions available collection of bilingual parallel corpora (Aharoni and Goldberg, 2018; Botha et al., 2018), over many language pairs. While most of the transand most splitting operations can be trivially classi- lated sentences in OPUS are aligned 1-1, i.e., one fied (Zhang et al., 2020a), so subsequent Split and sentence in Language A is mapped to one sentence Rephrase corpora have been created to improve in Language B, there are many aligned pairs contraining (Botha et al., 2018) and evaluation (Sulem sisting of multiple sentences fr"
2021.emnlp-main.500,P18-1042,0,0.153258,"editors are not only trying into shorter sentences. This task is referred to as to split a sentence, but also often simultaneously Split and Rephrase (Narayan et al., 2017). modifying the sentence for other purposes, which Several past efforts have created Split and Rephrase training sets, which consist of long, com- results in changes of the initial meaning. In this paper, we introduce a novel methodology plex input sentences paired with multiple shorter for creating Split and Rephrase corpora via bilin∗ Equal contribution. 1 Our code and data are available at https://github. gual pivoting (Wieting and Gimpel, 2018; Hu et al., com/mounicam/BiSECT. 2019b). Figure 1 demonstrates the process. First, 6193 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6193–6209 c November 7–11, 2021. 2021 Association for Computational Linguistics we extract all 1-2 and 2-1 sentence-level alignments (Gale and Church, 1993) from bilingual parallel corpora, where a single sentence in one language aligns to two sentences in the other language. We then machine translate the foreign sentences into English. The result is our B I SECT corpus. Split and Rephrase corpora, including B I S"
2021.emnlp-main.500,Q15-1021,1,0.877802,"Missing"
2021.findings-emnlp.38,2020.emnlp-main.437,0,0.0555104,"rmance on the recent ELI5 long-answers dataset. We release G OOAQ to facilitate further research on improving QA with diverse response types.1 1 Introduction Research in “open” question answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such"
2021.findings-emnlp.38,P17-1147,0,0.0314502,"uestion answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such as unlocking a phone (‘how to’), etc. Even when the answer is short, it can have rich types, e.g., unit conversion, time zone conversion, or a variety of knowledge look-up (‘h"
2021.findings-emnlp.38,2021.naacl-main.393,0,0.0234617,"cles. While our questions (extracted via autocomplete) were also likely frequently asked by Google users, our dataset represents a different and wider distribution of questions (§3.3), likely because it encompasses different classes of answers, particularly snippet and collection responses. Specifically, while NQ is dominated by ‘who’, ‘when’, and ‘how many’ questions (cf. Fig. 3(d)), G OOAQ has notably few ‘who’ questions and a substantial portion of questions starting with ‘how to’, ‘what is’, ‘what does’, ‘can you’. One notable QA dataset with long-form responses is ELI5 (Fan et al., 2019; Krishna et al., 2021), containing questions/answers mined from Reddit forums. In contrast, G OOAQ is collected differently and is several orders of magnitude larger than ELI5. Empirically, we show that models trained on G OOAQ transfer surprisingly well to ELI5 (§5.3), indicating G OOAQ’s broad coverage. It is worth highlighting that there is precedent for using search engines to create resources for the analysis of AI systems. Search engines harness colossal amounts of click information to help them effectively map input queries to a massive collection of information available in their index (Brin and Page, 1998;"
2021.findings-emnlp.38,Q19-1026,0,0.0606875,"Missing"
2021.findings-emnlp.38,P19-1612,0,0.0906374,"lso referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such as unlocking a phone (‘how to’), etc. Even when the answer is short, it can have rich types, e.g., unit conversion, time zone conversion, or a variety of knowledge look-up (‘how much’, ‘when is"
2021.findings-emnlp.38,2020.acl-main.703,0,0.0448029,"Missing"
2021.findings-emnlp.38,2021.eacl-main.86,0,0.0827255,"LI5 long-answers dataset. We release G OOAQ to facilitate further research on improving QA with diverse response types.1 1 Introduction Research in “open” question answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such as unlocking a phone"
2021.inlg-1.19,P16-1027,0,0.0237287,"ere declarative, or descriptive knowledge is distilled from narrative texts like news or stories (Mujtaba and Mahapatra, 2019). Such scripts are not goal-oriented, but descriptions of sequential events (e.g. a traffic accident involves a collision, injuries, police intervention, etc.). Chambers and Jurafsky (2008) introduced the classic Narrative Cloze Test, where a model is asked to fill in the blank given a script with one missing event. Following the task, a few papers made extensions on representation (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014) or modeling (Jans et al., 2012; Pichotta and Mooney, 2016a,c,b), achieving better performance on Narrative Cloze. Meanwhile, other work re-formalized Narrative Cloze as language modeling (LM) (Rudinger et al., 2015) or multiplechoice (Granroth-Wilding and Clark, 2016) tasks. However, the evolving evaluation datasets contain more spurious scripts, with many uninformative events such as “say” or “be”, and the LMs tend to capture such cues (Chambers, 2017). The other line of work focuses on procedural scripts, where events happen in a scenario, usually in order to achieve a goal. For example, to “visit a doctor”, one should “make an appointment”, “go t"
2021.inlg-1.19,2021.naacl-main.41,0,0.0289343,"rticles within 186 4 We ignore this hierarchical relation and flatten all steps in all Sections as the Steps of the script. 5 See Appendix A for our corpus statistics. Figure 3: Our Step-Inference-Ordering pipeline for the GOSC Retrieval task. An example ordered script is shown with example steps in the input and output. Those that appear in the ground-truth script is in bold. the same wikiHow category for each script. 5 Models We develop two systems based on state-of-the-art Transformers for the GOSC task.6 5.1 Generation Approach: Multilingual T5 For the Generation setting, we finetune mT5 (Xue et al., 2021), a pretrained generation model that is not only state-of-the-art on many tasks but also the only available massively multilingual one to date. During finetuning, we provide the goal of each article in the training set as a prompt, and train the model to generate the sequence of all the steps conditioned on the goal. Therefore, the model’s behavior is similar to completing the task of inferring relevant steps and sorting them at once. At inference time, the model generates a list of steps given a goal in the test set. 5.2 Retrieval Approach: Step-Inference-Ordering Pipeline We then implement a"
2021.inlg-1.19,P10-1100,0,0.259344,"lving evaluation datasets contain more spurious scripts, with many uninformative events such as “say” or “be”, and the LMs tend to capture such cues (Chambers, 2017). The other line of work focuses on procedural scripts, where events happen in a scenario, usually in order to achieve a goal. For example, to “visit a doctor”, one should “make an appointment”, “go to the hospital”, etc. To obtain data, Event Sequence Descriptions (ESD) are collected usually by crowdsourcing, and are cleaned to produce scripts. Thus, most such datasets are small-scale, including OMICS (Singh et al., 2002), SMILE (Regneri et al., 2010), the Li et al. (2012) corpus, and DeScript (Wanzare et al., 2016). The evaluation tasks are diverse, ranging from event clustering, event ordering (Regneri et al., 2010), text-script alignment (Ostermann et al., 2017) and next event prediction (Nguyen et al., 2017). There are also efforts on domain extensions (Yagcioglu et al., 2018; Berant et al., 2014) and modeling improvements (Frermann et al., 2014; Modi and Titov, 2014). In both lines, it still remains an open problem what kind of automatic task most accurately evaluates a system’s understanding of scripts. Most prior work has designed t"
2021.inlg-1.19,D15-1195,0,0.144396,"Missing"
2021.inlg-1.19,P19-1472,0,0.0249531,"ouching upon a specific piece of script knowledge nonetheless. Recent work has also brought forth generation-based tasks, but mostly within an open-ended/specialized domain like story or recipe generation (Fan et al., 2018; Xu et al., 2020). 185 Regarding data source, wikiHow has been used in multiple NLP efforts, including knowledge base construction (Jung et al., 2010; Chu et al., 2017), household activity prediction (Nguyen et al., 2017), summarization (Koupaee and Wang, 2018; Ladhak et al., 2020), event relation classification (Park and Motahari Nezhad, 2018), and next passage completion (Zellers et al., 2019). A few recent papers (Zhou et al., 2019; Zhang et al., 2020b) explored a set of separate goal-step inference tasks, mostly in binary-classification/multiple-choice formats, with few negative candidates. Our task is more holistic and realistic, simulating an open-ended scenario with retrieval/generation settings. We combine two of our existing modules from Zhang et al. (2020b) into a baseline, but a successful GOSC system can certainly include other functionalities (e.g. paraphrase detection). Also similar is Zhang et al. (2020a), which doesn’t include an extrinsic evaluation on other datasets"
2021.inlg-1.19,2020.emnlp-main.119,0,0.425789,"Recent work has also brought forth generation-based tasks, but mostly within an open-ended/specialized domain like story or recipe generation (Fan et al., 2018; Xu et al., 2020). 185 Regarding data source, wikiHow has been used in multiple NLP efforts, including knowledge base construction (Jung et al., 2010; Chu et al., 2017), household activity prediction (Nguyen et al., 2017), summarization (Koupaee and Wang, 2018; Ladhak et al., 2020), event relation classification (Park and Motahari Nezhad, 2018), and next passage completion (Zellers et al., 2019). A few recent papers (Zhou et al., 2019; Zhang et al., 2020b) explored a set of separate goal-step inference tasks, mostly in binary-classification/multiple-choice formats, with few negative candidates. Our task is more holistic and realistic, simulating an open-ended scenario with retrieval/generation settings. We combine two of our existing modules from Zhang et al. (2020b) into a baseline, but a successful GOSC system can certainly include other functionalities (e.g. paraphrase detection). Also similar is Zhang et al. (2020a), which doesn’t include an extrinsic evaluation on other datasets/domains though. In summary, our work has the following impo"
2021.inlg-1.19,2020.emnlp-main.226,0,0.0339924,"curately evaluates a system’s understanding of scripts. Most prior work has designed tasks focusing on various fragmented pieces of such understanding. For example, Narrative Cloze assesses a model’s knowledge for completing a close-to-finished script. The ESD line of work, on the other hand, evaluates script learning systems with the aforementioned variety of tasks, each touching upon a specific piece of script knowledge nonetheless. Recent work has also brought forth generation-based tasks, but mostly within an open-ended/specialized domain like story or recipe generation (Fan et al., 2018; Xu et al., 2020). 185 Regarding data source, wikiHow has been used in multiple NLP efforts, including knowledge base construction (Jung et al., 2010; Chu et al., 2017), household activity prediction (Nguyen et al., 2017), summarization (Koupaee and Wang, 2018; Ladhak et al., 2020), event relation classification (Park and Motahari Nezhad, 2018), and next passage completion (Zellers et al., 2019). A few recent papers (Zhou et al., 2019; Zhang et al., 2020b) explored a set of separate goal-step inference tasks, mostly in binary-classification/multiple-choice formats, with few negative candidates. Our task is mor"
2021.inlg-1.19,W19-5808,0,0.0130134,"ledge nonetheless. Recent work has also brought forth generation-based tasks, but mostly within an open-ended/specialized domain like story or recipe generation (Fan et al., 2018; Xu et al., 2020). 185 Regarding data source, wikiHow has been used in multiple NLP efforts, including knowledge base construction (Jung et al., 2010; Chu et al., 2017), household activity prediction (Nguyen et al., 2017), summarization (Koupaee and Wang, 2018; Ladhak et al., 2020), event relation classification (Park and Motahari Nezhad, 2018), and next passage completion (Zellers et al., 2019). A few recent papers (Zhou et al., 2019; Zhang et al., 2020b) explored a set of separate goal-step inference tasks, mostly in binary-classification/multiple-choice formats, with few negative candidates. Our task is more holistic and realistic, simulating an open-ended scenario with retrieval/generation settings. We combine two of our existing modules from Zhang et al. (2020b) into a baseline, but a successful GOSC system can certainly include other functionalities (e.g. paraphrase detection). Also similar is Zhang et al. (2020a), which doesn’t include an extrinsic evaluation on other datasets/domains though. In summary, our work ha"
2021.naacl-demos.16,D14-1148,0,0.0290568,". We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very fe"
2021.naacl-demos.16,C16-1201,0,0.0607333,"Missing"
2021.naacl-demos.16,N13-1073,0,0.043681,"luster, we apply these patterns as highprecision patterns before two statistical temporal ordering models separately. The schema matching algorithm will select the best matching from two graphs as the final instantiated schema results. Because the annotation for non-English data can be expensive and time-consuming, the temporal event tracking component has only been trained on English input. To extend the temporal event tracking capability to cross-lingual setting, we apply Google Cloud neural machine translation 6 to translate Spanish documents into English and apply the FastAlign algorithm (Dyer et al., 2013) to obtain word alignment. 2.6 Cross-media Information Grounding and Fusion Visual event and argument role extraction: Our goal is to extract visual events along with their argument roles from visual data, i.e., images and videos. In order to train event extractor from visual data, we have collected a new dataset called Video M2E2 which contains 1,500 video-article pairs by searching over YouTube news channels using 18 event primitives related to visual concepts as search keywords. We have extensively annotated the the videos and sampled key frames for annotating bounding boxes of argument rol"
2021.naacl-demos.16,2020.acl-main.718,0,0.068856,"Missing"
2021.naacl-demos.16,D19-5102,0,0.0208286,"arch purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Pr"
2021.naacl-demos.16,glavas-etal-2014-hieve,0,0.0312373,"l cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: ht"
2021.naacl-demos.16,R13-2011,0,0.025761,"ross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeli"
2021.naacl-demos.16,D19-1041,0,0.0480812,"Missing"
2021.naacl-demos.16,N19-1085,0,0.0191037,"oend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE sy"
2021.naacl-demos.16,W18-3101,0,0.0195422,"ly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who di"
2021.naacl-demos.16,N16-1056,0,0.030098,"nologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of even"
2021.naacl-demos.16,W18-5620,0,0.0245812,"ts described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, first-generati"
2021.naacl-demos.16,L16-1545,0,0.0202604,"rstand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but th"
2021.naacl-demos.16,2021.naacl-main.274,1,0.542222,"e postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents, we create N (N2−1) pairs of documents and treat each pair as a single “mega-document”. We apply our model to each mega-document and, at the end, aggregate the predictions across all megadocuments to extract the coreference clusters. Finally, we also apply a simple heuristic rule that prevents two entity mentions from being merged together if they are linked to different entities with high confidence. Our event coreference resolution method (Lai et al., 2021) is similar to entity coreference resolution, while incorporating additional symbolic features such as the event type information. If the input documents are all about one specific complex event, we apply some schema-guided heuristic rules to further refine the predictions of the neural event coreference resolution model. For example, in a bombing schema, there is typically only one bombing event. Therefore, in a document cluster, if there are two event mentions of type bombing and they have several arguments in common, these two mentions will be considered as coreferential. 2.5 Cross-document"
2021.naacl-demos.16,D17-1018,0,0.0120578,"5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that app"
2021.naacl-demos.16,2020.acl-main.703,0,0.0204692,"racted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments. Our model is based on BART (Lewis et al., 2020), which is an encoder-decoder language model. To utilize the encoder-decoder LM for argument extraction, we construct an input sequence of hsi template hsih/sidocument h/si. All argument names (arg1, arg2 etc.) in the template are replaced by a special placeholder token hargi. This model is trained in an end-to-end fashion by directly optimizing the generation probability. To align the extracted arguments back to the document, we adopt a simple postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents"
2021.naacl-demos.16,N19-4019,1,0.796479,"ence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide ra"
2021.naacl-demos.16,2020.acl-demos.11,1,0.922974,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2020.emnlp-main.50,1,0.880674,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2021.naacl-main.69,1,0.764758,"chemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments."
2021.naacl-demos.16,2020.acl-main.713,1,0.86722,"und the extracted knowledge elements onto our extracted graph via cross-media event coreference resolution (Section 2.6). Finally, our system selects the schema from a schema repository that best matches the extracted IE graph and merges these two graphs (Section 2.7). Our system can extract 24 types of entities, 46 types of relations and 67 types of events as defined in the DARPA KAIROS3 ontology. times for each detected words, as well as potential alternative transcriptions. Then from the speech recognition results and text input, we extract entity, relation, and event mentions using OneIE (Lin et al., 2020), a stateof-the-art joint neural model for sentence-level information extraction. Given a sentence, the goal of this module is to extract an information graph G = (V, E), where V is the node set containing entity mentions and event triggers and E is the edge set containing entity relations and event-argument links. We use a pre-trained BERT encoder (Devlin et al., 2018) to obtain contextualized word representations for the input sentence. Next, we adopt separate conditional random field-based taggers to identify entity mention and event trigger spans from the sentence. We represent each span,"
2021.naacl-demos.16,2021.ccl-1.108,0,0.0312676,"Missing"
2021.naacl-demos.16,L16-1631,0,0.0207744,"ences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolutio"
2021.naacl-demos.16,P17-1009,0,0.0621125,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.253,0,0.0765544,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.344,0,0.0324985,"match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columb"
2021.naacl-demos.16,D17-1108,1,0.83481,"on. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolution by grounding the extracted visual poral attributes through shared arguments. Furtherentities to text. We are the first to perform cross- more, we take advantage of the schema repository media joint event extraction and coreference reso- knowledge by using the frequent temporal"
2021.naacl-demos.16,P18-1212,1,0.895436,"Missing"
2021.naacl-demos.16,D19-1642,1,0.844444,"Missing"
2021.naacl-demos.16,P18-1122,1,0.846572,"oss-document Temporal Event Ordering Based on the event coreference resolution component described above, we group all mentions into clusters. Next we aim to order events along a timeline. We follow Zhou et al. (2020) to design a component for temporal event ordering. Specifically, we further pre-train a T5 model (Raffel et al., 2020) with distant temporal ordering supervision signals. These signals are acquired through two set of syntactic patterns: 1) before/after keywords in text and 2) explicit date and time mentions. We take such a pre-trained temporal T5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time"
2021.naacl-demos.16,P17-1178,1,0.849941,"he audio signal. It cific global feature. We compute the global feature returns the transcription with starting and ending score as uf , where u is a learnable weight vec3 https://www.darpa.mil/program/knowledge-di tor. Finally, we use a beam search-based decoder rected-artificial-intelligence-reasoning-overto generate the information graph with the highest schemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple argument"
2021.naacl-demos.16,W15-0812,0,0.0200332,"11 139 1,213 Videos 31 Table 4: Data statistics for schema matching corpus (LDC2020E39). Schema-guided Information Extraction. The Category Extracted Schema Instantiated Events Steps Steps performance of each component is shown in Table 3. We evaluate the end-to-end perfor# 3,180 1,738 958 mance of our system on a complex event corTable 5: Results of schema matching. pus (LDC2020E39), which contains multi-lingual multi-media document clusters. The data statistics are shown in Table 4. We train our mention ex3.3 Qualitative Analysis traction component on ACE 2005 (Walker et al., 2006) and ERE (Song et al., 2015); document- Figure 2 illustrates a subset of examples for the best matched results from our end-to-end system. We level argument exraction on ACE 2005 (Walker 7 et al., 2006) and RAMS (Ebner et al., 2020); corefLDC2017E03 8 erence component on ACE 2005 (Walker et al., LDC2017E52 137 Extracted Graph Old Bailey A court in British legal history Max Hill Manchester Communicator Place JudgeCourt JudgeCourt Place JudgeCourt Broadcast ReleaseParole Resident ... ... ChargeIndict TrialHearing Defendant Sentence ArrestJailDetain Defendant Defendant Detainee Defendant ReleaseParole Defendant Salman Abedi"
2021.naacl-demos.16,W02-2024,0,0.567788,"Missing"
2021.naacl-demos.16,D19-1585,0,0.0252013,".. ... ChargeIndict TrialHearing Defendant Defendant Sentence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et"
2021.naacl-demos.16,W12-4501,0,0.0999034,"Missing"
2021.naacl-demos.16,2021.naacl-main.6,1,0.690677,"Missing"
2021.naacl-demos.16,P18-4009,0,0.0278418,"lized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a compl"
2021.naacl-demos.16,N18-5009,1,0.831121,"Missing"
2021.naacl-main.19,D18-1549,0,0.0240373,"o the increasing awareness of the lack of linguistic provides an opportunity for us to examine how and geographic diversity in NLP research (Joshi geographical and cultural relatedness between lanet al., 2020; Orife et al.). Since parallel data guages affect translation of words via images. As for these languages is scarce, it necessitates the the use of parallel data from related languages have use of other data to help translation e.g., mono- been found to improve MT for low resource lanlingual texts in unsupervised MT (Lample et al., guages (Zoph et al., 2016; Nguyen and Chiang, 2018b,a,c; Artetxe et al., 2018) or images in multi- 2017; Dabre et al., 2017), we want to study if the modal MT (Barrault et al., 2018). same extends to translation via images. Specifi198 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 198–209 June 6–11, 2021. ©2021 Association for Computational Linguistics cally, we want to explore if translatability of words between two languages via images is influenced by the cultural similarity and geographical proximity of their communities. A recent study, (Thompson et al., 2020), ha"
2021.naacl-main.19,W18-6402,0,0.0225742,"geographic diversity in NLP research (Joshi geographical and cultural relatedness between lanet al., 2020; Orife et al.). Since parallel data guages affect translation of words via images. As for these languages is scarce, it necessitates the the use of parallel data from related languages have use of other data to help translation e.g., mono- been found to improve MT for low resource lanlingual texts in unsupervised MT (Lample et al., guages (Zoph et al., 2016; Nguyen and Chiang, 2018b,a,c; Artetxe et al., 2018) or images in multi- 2017; Dabre et al., 2017), we want to study if the modal MT (Barrault et al., 2018). same extends to translation via images. Specifi198 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 198–209 June 6–11, 2021. ©2021 Association for Computational Linguistics cally, we want to explore if translatability of words between two languages via images is influenced by the cultural similarity and geographical proximity of their communities. A recent study, (Thompson et al., 2020), has observed such correlations of culture and geography to semantic alignment of word meanings between lan"
2021.naacl-main.19,P11-1135,0,0.0395504,"of ResNet-50, which gives Conversely, if the images are either spread out or us a 2048 dimensional vector embedding for each disjoint, it means that the images have greater diimage. For each word, we call the embeddings of versity (high dispersion) or differ between ws and the associated images the word’s image embedding. wt , indicating potentially poor translation between Because cosine similarity, which underlies parts of them. We refer to the degree of overlap between this work and previous works for bilingual lexicon two clusters of images associated with ws and wt induction via images (Bergsma et al., 2011; Kiela respectively as their inter-cluster similarity, and to et al., 2015; Hewitt et al., 2018), is non-invariant the degree of tightness or looseness of the images to translation (Korenius et al., 2007) we treat all in each cluster as their intra-cluster similarity. vectors with respect to the origin rather than some 5 Our conjecture is that this is equivalent to repremean center for each image cluster . senting image embeddings as samples from some Since the MTurk word translations that come generator distribution G. We can call the generwith MMID (Pavlick et al., 2014) are limited in cove"
2021.naacl-main.19,Y17-1038,0,0.0182066,"stic provides an opportunity for us to examine how and geographic diversity in NLP research (Joshi geographical and cultural relatedness between lanet al., 2020; Orife et al.). Since parallel data guages affect translation of words via images. As for these languages is scarce, it necessitates the the use of parallel data from related languages have use of other data to help translation e.g., mono- been found to improve MT for low resource lanlingual texts in unsupervised MT (Lample et al., guages (Zoph et al., 2016; Nguyen and Chiang, 2018b,a,c; Artetxe et al., 2018) or images in multi- 2017; Dabre et al., 2017), we want to study if the modal MT (Barrault et al., 2018). same extends to translation via images. Specifi198 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 198–209 June 6–11, 2021. ©2021 Association for Computational Linguistics cally, we want to explore if translatability of words between two languages via images is influenced by the cultural similarity and geographical proximity of their communities. A recent study, (Thompson et al., 2020), has observed such correlations of culture and ge"
2021.naacl-main.19,N19-1423,0,0.0108197,"ness scores predictions on the held-out validation set of 1,000 words from Brysbaert et al. (2014). The Spearman correlation coefficient calculated for ground-truth and predicted concreteness scores is noted. dataset into train and test sets, randomly picking 39,000 words for training. Similar to Hewitt et al. (2018), our concreteness prediction model is a twolayer perceptron, with one 32-units hidden layer, and a ReLU activation function, trained with an L2 loss. For each word, the model input is the concatenation of the single word embeddings obtained from the top four hidden layers of BERT Devlin et al. (2019), a practice recommended as the best performing feature-extraction method by the authors. Figure 1 shows the results of our evaluation on the test set of 1,000 words, depicting the distributions of the different part-of-speech categories. We provide the Spearman correlation coefficient between the ground-truth and predicted concreteness scores, which shows the improved effectiveness of our BERT embeddings-based method compared to the Salle et al. (2016) embeddings employed by Hewitt et al. (2018). Using this trained model, we predict the concreteness score of each of the words in our dataset b"
2021.naacl-main.19,W17-4718,0,0.0238999,"guage, images that to train the models. In the absence of paralground their meanings should also be invariant to lel data, several approaches have turned to the the language. However, to the best of our knowluse of images to learn translations. Since images of words, e.g., horse may be unchanged edge, this conjecture on image-language invariance across languages, translations can be identihas never been tested. As images’ usefulness for fied via images associated with words in diftranslation has only been shown to be marginal ferent languages that have a high degree of vi(Specia et al., 2016; Elliott et al., 2017; Barrault sual similarity. However, translating via imet al., 2018), it is important to study this conjecture ages has been shown to improve upon textin relation to the characteristics of languages to only models only marginally. To better underunderstand when and to what extent images can stand when images are useful for translation, we study image translatability of words, which aid translation. An alternative view would be that we define as the translatability of words via images may be different to some extent in different images, by measuring intra- and inter-cluster languages since they"
2021.naacl-main.19,P98-1069,0,0.768749,"Missing"
2021.naacl-main.19,D17-1303,0,0.0145134,"vious research on image-aided word interlingual links (Wijaya et al., 2017). translation, and how roots, geography and cultural characteristics of languages correlate with semantic The core idea in a large number of vision-based alignment of words. In section 3 we describe our methods is using images to learn word and image dataset and text-image corpora. We also introduce embeddings that integrate all linguistic and visual the language pairs we examine and estimate their information available to improve word translation closeness in culture and geography. In section 4, (Calixto et al., 2017; Gella et al., 2017; Karpathy we present our approach for measuring translatabil- and Fei-Fei, 2017; Vuli´c et al., 2016). Recent reity of words in terms of the similarity of their image search in this area extends prior ideas in learning representations. Section 5 shows an analysis of our multilingual word-image embeddings, extracting 199 more complex and useful information from images, and applying the methods in few shot scenarios. Singhal et al. (2019) learn multilingual and multimodal word embeddings from weakly-supervised image-text data with a simple bag-of-words-based embedding model that incorporates wo"
2021.naacl-main.19,P18-1239,1,0.569374,"ranslatability (i.e., have that speak the languages. more similar images for similar words) comWhile most multimodal MT datasets are limited pared to its converse, regardless of their geto a small set of European languages that come ographic proximity. In addition, in line with from the same language family, and are spoken by previous works that show images help more in communities that are culturally and geographically translating concrete words, we found that conclose, the Massively Multilingual Image Dataset crete words have improved image translatability compared to abstract ones. (MMID) (Hewitt et al., 2018) is constructed specifically to facilitate large-scale multilingual research 1 Introduction in translating words via images. Neural machine translation (NMT) for lowMMID consists of up to 10K words and 100 resource languages has drawn a lot of attention due images per word in 98 languages. This dataset to the increasing awareness of the lack of linguistic provides an opportunity for us to examine how and geographic diversity in NLP research (Joshi geographical and cultural relatedness between lanet al., 2020; Orife et al.). Since parallel data guages affect translation of words via images. As"
2021.naacl-main.19,2020.acl-main.560,0,0.0457497,"Missing"
2021.naacl-main.19,J82-2005,0,0.576902,"Missing"
2021.naacl-main.19,I17-2050,0,0.107161,"Missing"
2021.naacl-main.19,P14-2135,0,0.0170911,"ource language s, and wt in the target language t, we define two measures that determine how well a word can be translated by its images. The first measures whether ws and wt have overlapping or disjoint image embeddings. The second measures whether the spread of the image embedding for ws , and, similarly, for wt , is tight or loose: 3.2 Dataset collection and preparation such a measure of image dispersion has been found 4 We download MMID images for all the source and to help predict the usefulness of image representatarget languages in our language pairs. In order to tions for translation (Kiela et al., 2014, 2015). get vector embeddings for the images, we scale Specifically, when images of ws and wt are tight the images to 224 x 224 pixels, normalize and feed and overlapping in the embedding space, it shows them as input into the ResNet-50 network (He et al., that the images have little diversity (low disper2015), using network weights pre-trained on Imasion) and are similar between ws and wt , indigeNet. We obtain image embeddings from the last cating potentially good translation between them. average pooling layer of ResNet-50, which gives Conversely, if the images are either spread out or us"
2021.naacl-main.19,D15-1015,0,0.128928,"Missing"
2021.naacl-main.19,W18-6325,0,0.0153014,"actors, views and languages in the Indo-European family. tradition, apart from word meaning, for producing high quality translations is indisputable (Nida, 3 Data 1945; Wakabayashi, 1991; Janfaza et al., 2012). 3.1 The Massively Multilingual Image Despite the importance of language, culture Dataset and geography in translation, and findings that parallel data from similar, higher resource lan- The dataset we use is the Massively Multilingual guages can help improve MT of low resource lan- Image Dataset (MMID) from Hewitt et al. (2018). It covers 98 languages, containing at most 10,000 guages (Kocmi and Bojar, 2018), no previous work words per language and 100 images per word. For has studied how language similarity may influence each word, in any language, we are given the coltranslation via images. The most notable recent lected images matching the word meaning, and the work in this area, that is most similar to ours, is word’s English translation. They use a language filthat of Thompson et al. (2020). The authors predict tering step to ensure that images for each language semantic similarity of words in 41 languages from are collected only from web pages that are identithe NEL dataset (Dellert et al.,"
2021.naacl-main.19,J86-2003,0,0.315558,"Missing"
2021.naacl-main.19,1983.tc-1.13,0,0.491694,"Missing"
2021.naacl-main.19,P99-1067,0,0.66674,"r image cept synonymous to concreteness is imageability translatability across different language pairs. (Kastner et al., 2020). We identify how close word translations are in In terms of word translation, there exists a signifterms of their image representations (embeddings). icant body of work in the area of bilingual lexicon Our findings suggest that languages with cultural induction, which is the task of translating words similarity (defined as a combination of linguistic, across languages without any parallel data (Fung ethnicity, or religious similarity of the communities and Yee, 1998; Rapp, 1999). Approaches can be at the cultural centres of the languages by Glottolog divided into two types, text-based, which aim to (Hammarström et al., 2020)) coincides with their find word translations by employing the words’ lintranslatability via images, and that the translata- guistic information, and vision-based that use the bility of languages with cultural similarity outper- words’ images as pivots for translation (Bergsma forms that in those with geographical proximity. and Van Durme, 2011; Kiela et al., 2015). Additionally, there are works that have incorporated Our paper is structured as fo"
2021.naacl-main.19,D07-1043,0,0.0296887,"mplies that Gs 6= Gr and Gt 6= Gr . Thus, we have sufficient conditions when we have high inter- and intra-cluster similarities to say Gs = Gt . 4.1 M ED M AXw Inter-Cluster Similarity To measure the degree of overlap (inter-cluster similarity) between images associated with the word ws in the source language, and those associated with the word wt in the target language, we first cluster their image embeddings with a k-means clustering algorithm (k = 2). Then, we measure the degree of overlap between images of the two words by the homogeneity score of the resulting clusters, hws ,wt ∈ [0, 1] (Rosenberg and Hirschberg, 2007), calculated given the words ws and wt as image labels. A homogeneity score of 0 signals that all the image embeddings come from the distribution of a single class, hence represent the same word or concept (ws = wt ). In this case, we say that the images of the two words have high inter-cluster similarity. A score of 1 means that the k-means clustering was able to identify two mutually exclusive clusters of images indicative that the images come from two different generators (Gs 6= Gt ). In other words, the image embeddings were sampled from two different words or senses (ws 6= wt ). However,"
2021.naacl-main.19,P18-1084,0,0.044023,"Missing"
2021.naacl-main.19,P16-2068,0,0.029264,"n L2 loss. For each word, the model input is the concatenation of the single word embeddings obtained from the top four hidden layers of BERT Devlin et al. (2019), a practice recommended as the best performing feature-extraction method by the authors. Figure 1 shows the results of our evaluation on the test set of 1,000 words, depicting the distributions of the different part-of-speech categories. We provide the Spearman correlation coefficient between the ground-truth and predicted concreteness scores, which shows the improved effectiveness of our BERT embeddings-based method compared to the Salle et al. (2016) embeddings employed by Hewitt et al. (2018). Using this trained model, we predict the concreteness score of each of the words in our dataset by first translating the word to English and lemmatizing it using spaCy. Using the predicted concreteness scores, we distinguish words in our dataset to concrete (having a predicted score of &gt; 3) and abstract (≤ 3). 4.4 Inter-Cluster Similarity Threshold 4.5 Intra-Cluster Similarity Threshold Similarly, we define an intra-cluster similarity threshold to determine if an image cluster associated with a word w is sufficiently tight. Since intra-cluster simi"
2021.naacl-main.19,W19-1807,0,0.0179951,"ge pairs we examine and estimate their information available to improve word translation closeness in culture and geography. In section 4, (Calixto et al., 2017; Gella et al., 2017; Karpathy we present our approach for measuring translatabil- and Fei-Fei, 2017; Vuli´c et al., 2016). Recent reity of words in terms of the similarity of their image search in this area extends prior ideas in learning representations. Section 5 shows an analysis of our multilingual word-image embeddings, extracting 199 more complex and useful information from images, and applying the methods in few shot scenarios. Singhal et al. (2019) learn multilingual and multimodal word embeddings from weakly-supervised image-text data with a simple bag-of-words-based embedding model that incorporates word context and image information. Similarly, in Chen et al. (2019) the authors suggest mapping linguistic features based on sentence context and localized image features from image regions into a joint space. Aside from translation, multilingual text representations aligned to images has also been used to boost performance in vision-language tasks such as multilingual image-sentence retrieval (Gella et al., 2017; Wehrmann et al., 2019; K"
2021.naacl-main.19,W16-2346,0,0.331259,"Missing"
2021.naacl-main.19,P16-2031,0,0.0435834,"Missing"
2021.naacl-main.19,D17-1152,1,0.780438,", 2020)) coincides with their find word translations by employing the words’ lintranslatability via images, and that the translata- guistic information, and vision-based that use the bility of languages with cultural similarity outper- words’ images as pivots for translation (Bergsma forms that in those with geographical proximity. and Van Durme, 2011; Kiela et al., 2015). Additionally, there are works that have incorporated Our paper is structured as follows: In section 2 additional signals for translation such as Wikipedia we discuss previous research on image-aided word interlingual links (Wijaya et al., 2017). translation, and how roots, geography and cultural characteristics of languages correlate with semantic The core idea in a large number of vision-based alignment of words. In section 3 we describe our methods is using images to learn word and image dataset and text-image corpora. We also introduce embeddings that integrate all linguistic and visual the language pairs we examine and estimate their information available to improve word translation closeness in culture and geography. In section 4, (Calixto et al., 2017; Gella et al., 2017; Karpathy we present our approach for measuring translat"
2021.naacl-main.19,D16-1163,0,0.046503,"e images per word in 98 languages. This dataset to the increasing awareness of the lack of linguistic provides an opportunity for us to examine how and geographic diversity in NLP research (Joshi geographical and cultural relatedness between lanet al., 2020; Orife et al.). Since parallel data guages affect translation of words via images. As for these languages is scarce, it necessitates the the use of parallel data from related languages have use of other data to help translation e.g., mono- been found to improve MT for low resource lanlingual texts in unsupervised MT (Lample et al., guages (Zoph et al., 2016; Nguyen and Chiang, 2018b,a,c; Artetxe et al., 2018) or images in multi- 2017; Dabre et al., 2017), we want to study if the modal MT (Barrault et al., 2018). same extends to translation via images. Specifi198 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 198–209 June 6–11, 2021. ©2021 Association for Computational Linguistics cally, we want to explore if translatability of words between two languages via images is influenced by the cultural similarity and geographical proximity of their com"
C08-1013,P05-1074,1,0.9157,"tion Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a number of existing paraphrasing methods. 2 Related Work"
C08-1013,N03-1003,0,0.555789,"orrespondences between words in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMe"
C08-1013,P01-1008,0,0.389996,"are created by annotating correspondences between words in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the ali"
C08-1013,J93-2003,0,0.0107748,"veres El mar arroja tantos cadáveres de inmigrantes ilegales ahogados playa ....73 Align .62a la .65 P rec corpses of drowned illegals get washed.14 up on .33 beaches.68 ... LB-P recision So many AlignRecall .11 .10 .46 Rel-Recall .07 .03 .01 Figure 3: Bannard and Callison-Burch (2005) extracted paraphrases by equating English phrases that share a common translation. Table 2: Summary results for scoring the different paraphrasing techniques using our proposed automatic evaluations. Where p(e1 |e2 ) is estimated by training word alignment models over the “parallel corpus” as in the IBM Models (Brown et al., 1993), and phrase translations are extracted from word alignments as in the Alignment Template Model (Och, 2002). Bannard and Callison-Burch (2005) also used techniques from statistical machine translation to identify paraphrases. Rather than drawing pairs of English sentences from a comparable corpus, Bannard and Callison-Burch (2005) used bilingual parallel corpora. They identified English paraphrases by pivoting through phrases in another language. They located foreign language translations of an English phrase, and treated the other English translations of those foreign phrases as potential par"
C08-1013,N06-1003,1,0.731127,"Missing"
C08-1013,W03-1608,0,0.602423,"from the alignments; • Report ParaMetric scores for a number of existing paraphrasing methods. 2 Related Work No consensus has been reached with respect to the proper methodology to use when evaluating paraphrase quality. This section reviews past methods for paraphrase evaluation. Researchers usually present the quality of their automatic paraphrasing technique in terms of a subjective manual evaluation. These have used a variety of criteria. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 97 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97–104 Manchester, August 2008 3 sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” These subjective"
C08-1013,J03-1002,0,0.00444764,"gure 1. For the results reported in this paper, annotators aligned 50 groups of 10 pairs of equivalent sentences, for a total of 500 sentence pairs. These were assembled by pairing the first of the LDC translations with the other ten (i.e. 1-2, 1-3, 1-4, ..., 1-11). The choice of pairing one sentence with the others instead of doing all pairwise combinations was made simply because the latter would not seem to add much information. However, the choice of using the first translator as the key was arbitrary. Annotators corrected a set of automatic word alignments that were created using Giza++ (Och and Ney, 2003), which was trained on a total of 109,230 sentence pairs created from all pairwise combinations of the eleven translations of 993 Chinese sentences. The average amount of time spent on each of the sentence pairs was 77 seconds, with just over eleven hours spent to annotate all 500 sentence 5 ParaMetric Scores We can exploit the manually aligned data to compute scores in two different fashions. First, we can calculate how well an automatic paraphrasing technique is able to align the paraphrases in a sentence pair. Second, we can calculate the lowerbound on precision for a paraphrasing technique"
C08-1013,J04-4002,0,0.0204135,"rase. Figure 1 shows the alignments that were created between one sentence and three of its ten corresponding translations. Table 1 gives a list of non-identical words and phrases that can be paired by way of the word alignments. These are the basic paraphrases contained within the three sentence pairs. Each phrase has up to three paraphrases. The maximum number of paraphrases for a given span in each sentence is bounded by the number of equivalent sentences that it is paired with. In addition to these basic paraphrases, longer paraphrases can also be obtained using the heuristic presented in Och and Ney (2004) for extracting phrase pairs (PP) from word alignments A, between a foreign sentence f1J and an English sensome want to impeach him and others expect him to step down . there are those who propose impeaching him and those who want him to tender his resignation . some are proposing an indictment against him and some want him to leave office voluntarily . Figure 1: Pairs of English sentences were aligned by hand. Black squares indicate paraphrase correspondences. 1 99 See LDC catalog number 2002T01. some want to impeach and others expect step down some people, there are those who propose, are pr"
C08-1013,N03-1024,0,0.768729,"ords in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a n"
C08-1013,P02-1040,0,0.116829,"erence paraphrases can be extracted from the gold standard alignments. While these sets will obviously be fragmentary, we attempt to make them more complete by aligning groups of equivalent sentences rather than only pairs. The paraphrase sets that we extract are appropriate for the particular contexts. Moreover they may potentially be used to study structural paraphrases, although we do not examine that Others evaluate paraphrases in terms of whether they improve performance on particular tasks. Callison-Burch et al. (2006b) measure improvements in translation quality in terms of Bleu score (Papineni et al., 2002) and in terms of subjective human evaluation when paraphrases are integrated into a statistical machine translation system. Lin and Pantel (2001) manually judge whether a paraphrase might be used to answer questions from the TREC question-answering track. To date, no one has used task-based evaluation to compare different paraphrasing methods. Even if such an evaluation were performed, it is unclear whether the results would hold for a different task. Because of this, we strive for a general evaluation rather than a task-specific one. Dolan et al. (2004) create a set of manual word alignments"
C08-1013,W04-3219,0,0.80361,"anslations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a number of existing pa"
C08-1013,P07-1058,0,0.0301292,"If a list of reference paraphrases is incomplete, then using it to calculate precision will give inaccurate numbers. Precision will be falsely low if the system produces correct paraphrases which are not in the reference list. Additionally, recall is indeterminable because there is no way of knowing how many correct paraphrases exist. There are further impediments to automatically evaluating paraphrases. Even if we were able to come up with a reasonably exhaustive list of paraphrases for a phrase, the acceptability of each paraphrase would vary depending on the context of the original phrase (Szpektor et al., 2007). While lexical and phrasal paraphrases can be evaluated by comparing them against a list of known paraphrases (perhaps customized for particular contexts), this cannot be naturally done for structural paraphrases which may transform whole sentences. We attempt to resolve these problems by having annotators indicate correspondences in pairs of equivalent sentences. Rather than having people enumerate paraphrases, we asked that they perform the simper task of aligning paraphrases. After developing these manual “gold standard alignments” we can gauge how well different automatic paraphrases are"
C08-1013,C04-1051,0,\N,Missing
C08-1013,J08-4005,1,\N,Missing
cotterell-callison-burch-2014-multi,N12-1006,1,\N,Missing
cotterell-callison-burch-2014-multi,P11-2007,1,\N,Missing
cotterell-callison-burch-2014-multi,W10-0701,1,\N,Missing
cotterell-callison-burch-2014-multi,D13-1007,0,\N,Missing
cotterell-callison-burch-2014-multi,W13-2317,0,\N,Missing
cotterell-callison-burch-2014-multi,elfardy-diab-2012-simplified,0,\N,Missing
cotterell-callison-burch-2014-multi,al-sabbagh-girju-2012-yadac,0,\N,Missing
cotterell-callison-burch-2014-multi,P13-2081,0,\N,Missing
cotterell-callison-burch-2014-multi,al-sabbagh-girju-2010-mining,0,\N,Missing
cotterell-callison-burch-2014-multi,W13-1102,0,\N,Missing
D08-1021,P05-1074,1,0.856653,"hou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments. English phrases are considered to be potential paraphrases of each other if they share a common foreign language phrase among their translations. Multiple paraphrases are frequently extracted for each phrase and can be ranked using a paraphrase probability based on phrase translation probabilities. We find that the quality of the paraphrases that are generated in this fashion improves significantly whe"
D08-1021,N06-1003,1,0.65011,"ity over the baseline method. 1 Introduction Paraphrases are alternative ways of expressing the same information. Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase ex"
D08-1021,W07-0718,1,0.452492,"+ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison. 4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007). The test sentences were also parsed with the Bikel parser. The phrases to be evaluated were selected such that there was an even balance of phrase lengths (from one word long up to five words long), with half of the phrases being valid syntactic constituents and half being arbitrary sequences of words. 410 phrases were selected at random for evaluation. 30 items were excluded from our results subsequent eˆ2 = arg max arg max p(e2 |e1 , s) (10) to evaluation on the grounds that they consisted e2 :e2 6=e1 s∈∩∀T in C CCG-labels(e1 ,T ) 201 solely of punctuation and stop words like determiners,"
D08-1021,J96-2004,0,0.0213569,"Missing"
D08-1021,P05-1033,0,0.0376881,"a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated"
D08-1021,W02-1039,0,0.0201143,"employing syntactic constraints in statistical machine translation. Wu (1997) introduced the inversion transduction grammar formalism which treats translation as a process of parallel parsing of the source and target language via a synchronized grammar. The synchronized grammar places constraints on which words can be aligned across bilingual sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic in"
D08-1021,N04-1035,0,0.0372204,"sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-traini"
D08-1021,N06-1031,0,0.00904946,"used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing long"
D08-1021,W03-1608,0,0.194777,"xt free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted pat"
D08-1021,N06-1058,0,0.449491,"Missing"
D08-1021,N03-1017,0,0.00571138,"hrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard and Callison-Burch (2005) extract paraphrases from bilingual parallel corpora. They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation: eˆ2 = arg max p(e2 |e1 ) (1) e2 :e2 6=e1 where p(e2 |e1 ) = X p(f |e1 )p(e2 |f, e1 ) (2) p(f |e1 )p(e2 |f ) (3) f ≈ X f Phrase translation probabilities p(f |e1 ) and p(e2 |f ) are commonly calculated using maximum likelihood estimation (Koehn et al., 2003): p(f |e) = count(e, f ) P f count(e, f ) (4) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the el proyecto europeo no ha conseguido the european project has failed la igualdad to create de oportunidades equal opportunities . Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal. equal equal same equality equals word alignments for sentence pairs in a bilingual parallel corpus. Various phrase extraction heuristics"
D08-1021,2005.mtsummit-papers.11,0,0.0311689,"particular sentence significantly reduces the paraphrases that can be used. For instance, VP/(NP/NNS) is the only label for the paraphrases in Table 4 that is compatible with the parse tree given in Figure 2. Because the CCG labels for a given sentence are so specific, many times there are no matches. Therefore we also investigated a looser constraint. We choose the highest probability paraphrase with any label (i.e. the set of labels extracted from all parse trees in our parallel corpus): 4 4.1 Experimental design Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words. Automatic word alignments were created for these using Giza++ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphr"
D08-1021,W07-0716,0,0.541306,"Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments. English phrases are considered"
D08-1021,J03-1002,0,0.00378838,"lso investigated a looser constraint. We choose the highest probability paraphrase with any label (i.e. the set of labels extracted from all parse trees in our parallel corpus): 4 4.1 Experimental design Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words. Automatic word alignments were created for these using Giza++ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison. 4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al."
D08-1021,J04-4002,0,0.0360475,"nt(e, f ) P f count(e, f ) (4) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the el proyecto europeo no ha conseguido the european project has failed la igualdad to create de oportunidades equal opportunities . Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal. equal equal same equality equals word alignments for sentence pairs in a bilingual parallel corpus. Various phrase extraction heuristics are possible. Och and Ney (2004) defined consistent bilingual phrase pairs as follows: BP (f1J , eI1 , A) = {(fjj+m , ei+n ): i .35 .07 .03 .02 equally the fair equal rights .02 .02 .01 .01 Table 1: The baseline method’s paraphrases of equal and their probabilities (excluding items with p &lt; .01). ∀(i0 , j 0 ) ∈ A : j ≤ j 0 ≤ j + m ↔ i ≤ i0 ≤ i + n ∧∃(i0 , j 0 ) ∈ A : j ≤ j 0 ≤ j + m∧ ↔ i ≤ i0 ≤ i + n} where f1J is a foreign sentence, eI1 is an English sentence and A is a set of word alignment points. The heuristic allows unaligned words to be included at the boundaries of the source or target language phrases. For example, w"
D08-1021,W06-3112,0,0.0219743,"Missing"
D08-1021,N03-1024,0,0.431073,"s for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used depende"
D08-1021,P05-1034,0,0.0290522,"chieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 The"
D08-1021,P07-1059,0,0.493606,"Missing"
D08-1021,P07-1058,0,0.0285112,"ch test phrase and substituted the phrase with automatically-generated paraphrases. Annotators judged whether the paraphrases had the same meaning as the original and whether the resulting sentences were grammatical. They assigned two values to each sentence using the 5-point scales given in Table 5. We considered an item to have the same meaning if it was assigned a score of 3 or greater, and to be grammatical if it was assigned a score of 4 or 5. We evaluated several instances of a phrase when it occurred multiple times in the test corpus, since paraphrase quality can vary based on context (Szpektor et al., 2007). There were an average of 3.1 instances for each phrase, with a maximum of 6. There were a total of 1,195 sentences that paraphrases were substituted into, with a total of 8,422 judgements collected. Note that 7 different paraphrases were judged on average for every instance. This is because annotators judged paraphrases for eight conditions, and because we collected judgments for the 5-best paraphrases for many of the conditions. We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common. The two annotators assig"
D08-1021,J97-3002,0,0.178667,"judged to be correct. This paper is structured as follows: Section 2 describes related work in syntactic constraints on phrase-based SMT and work utilizing syntax in paraphrase discovery. Section 3 details the problems with extracting paraphrases from parallel corpora and our improvements to the technique. Section 4 describes our experimental design and evaluation methodology. Section 5 gives the results of our experiments, and Section 6 discusses their implications. 2 Related work A number of research efforts have focused on employing syntactic constraints in statistical machine translation. Wu (1997) introduced the inversion transduction grammar formalism which treats translation as a process of parallel parsing of the source and target language via a synchronized grammar. The synchronized grammar places constraints on which words can be aligned across bilingual sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subt"
D08-1021,P08-1116,0,0.503951,"and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used dependency parses on the English side of the parallel corpus. Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard an"
D08-1021,P08-1089,0,0.693723,"and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. 197 They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used dependency parses on the English side of the parallel corpus. Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard an"
D08-1021,W06-1610,0,0.39392,"Missing"
D08-1021,N03-2017,0,\N,Missing
D08-1021,P01-1008,0,\N,Missing
D08-1021,P99-1039,0,\N,Missing
D09-1006,P03-1021,0,0.181005,", 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation. Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair. Treated as a log-linear model, we need to assign a weight for each of the features. Och (2003) 52 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 52–61, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP time except when building a database of reusable judgments, and furthermore show that RYPT is a better predictor of translation quality than BLEU, making it an excellent candidate for MERT tuning. The paper is organized as follows. We start by describing the core idea of MERT before introducing our new metric, RYPT, and describing the data collection effort we undertook to collect the needed human judgments. We analyze a MERT run optimizing B"
D09-1006,E06-1032,1,0.359588,"Missing"
D09-1006,P02-1040,0,0.12021,"meaning we only need to evaluate it at the “critical” points corresponding to line intersection points. Furthermore, we only need to calculate the sufficient statistics once, at the smallest critical point, and then simply adjust the sufficient statistics to reflect changes in the set of 1-best candidates. Och’s Line Search Method A common approach to translating a source sentence f in a foreign language is to select the candidate translation e that maximizes the posterior probability: 2.1 exp(sΛ (e, f )) P r(e |f ) = P . 0 e0 exp(sΛ (e , f )) def The metric most often used with MERT is BLEU (Papineni et al., 2002), where the score of a candidate c against a reference translation r is: This defines P r(e |f ) using a log-linear model that associates a sentence pair (e, f ) with a feature vector Φ(e, f ) = {φ1 (e, f ), ..., φM (e, f )}, and assigns a score def sΛ (e, f ) = Λ · Φ(e, f ) = M X BLEU = BP (len(c), len(r))·exp( 4 X 1 n=1 4 log pn ), where pn is the n-gram precision2 and BP is a brevity penalty meant to penalize short outputs, to discourage improving precision at the expense of recall. There are several compelling reasons to optimize to BLEU. It is the most widely reported metric in MT researc"
D09-1006,D08-1027,0,0.0594581,"Missing"
D09-1006,D08-1064,0,0.022284,"e42 : [2,15] [14,25] [8,25] 0.56 TER e12 e42 0.32 ÷ 0.12 0.24 + λd TER suff. stats for candidates. The SS for e11 mean 6 edits are needed to match a 10-word reference. Figure 1: Och’s method applied to a set of two foreign sentences. This figure is essentially a visualization of equation (1). We show here sufficient statistics for TER for simplicity, since there are only 2 of them, but the metric optimized in MERT is usually BLEU. In spite of these advantages, recent work has pointed out a number of problematic aspects of BLEU that should cause one to pause and reconsider the reliance on it. Chiang et al. (2008) investigate several weaknesses in BLEU and show there are realistic scenraios where the BLEU score should not be trusted, and in fact behaves in a counter-intuitive manner. Furthermore, CallisonBurch et al. (2006) point out that it is not always appropriate to use BLEU to compare systems to each other. In particular, the quality of rule-based systems is usually underestimated by BLEU. All this raises doubts regarding BLEU’s adequacy as a proxy for human judgment, which is a particularly important issue in the context of setting parameters during the MERT phase. But what is the alternative? 2."
D09-1006,J07-2003,0,0.00850319,"cribe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 1 Introduction Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation. Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair. Treated as a log-linear model, we need to assign a weight for each of the features. Och (2003) 52 Procee"
D09-1006,2003.mtsummit-papers.9,0,0.0165989,"s P r(e |f ) using a log-linear model that associates a sentence pair (e, f ) with a feature vector Φ(e, f ) = {φ1 (e, f ), ..., φM (e, f )}, and assigns a score def sΛ (e, f ) = Λ · Φ(e, f ) = M X BLEU = BP (len(c), len(r))·exp( 4 X 1 n=1 4 log pn ), where pn is the n-gram precision2 and BP is a brevity penalty meant to penalize short outputs, to discourage improving precision at the expense of recall. There are several compelling reasons to optimize to BLEU. It is the most widely reported metric in MT research, and has been shown to correlate well with human judgment (Papineni et al., 2002; Coughlin, 2003). But BLEU is also particularly suitable for MERT, because it can be computed quite efficiently, and its sufficient statistics are decomposable, as required by MERT.3,4 λm φm (e, f ) m=1 for that sentence pair, with the feature weights Λ = {λ1 , ..., λM } being the parameters of the model. Therefore, the system selects the translation eˆ: eˆ = argmax P r(e |f ) = argmax sΛ (e, f ). (1) e The BLEU Metric e Och (2003) provides evidence that Λ should be chosen by optimizing an objective function basd on the evaluation metric of interest, rather than likelihood. Since the error surface is not smoo"
D09-1006,P05-1039,0,0.0139695,"by the dashed rectangle, including the highlighting (though excluding the word alignment links). from the sentence pair: der patient wurde isoliert . the patient was isolated . then this would apply to any candidate translation of this source sentence. And so all of the following substrings are labeled YES as well: the patient isolated . the patient was in isolation . the patient has been isolated . tence, and penalize constituents that do not. For instance, consider the source-candidate sentence pair of Figure 2. To evaluate the candidate translation, the source parse tree is first obtained (Dubey, 2005), and each subtree is matched with a substring in the candidate string. If the source substring covered by this subtree is translated into an acceptable substring in the candidate, that node gets a YES label. Otherwise, the node gets a NO label. The metric we propose is taken to be the ratio of YES nodes in the parse tree (or RYPT). The candidate in Figure 2, for instance, would get a RYPT score of 13/18 = 0.72. To justify its use as a proxy for HTER-like metrics, we need to demonstrate that this metric correlates well with human judgment. But it is also important to show that we can obtain th"
D09-1006,N03-1017,0,0.00230295,"aunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 1 Introduction Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation. Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair. Treated as a log-linear model, we need to assign a weight for each of the features. Och (2"
D09-1006,P07-2045,1,0.0228205,"tric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 1 Introduction Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation. Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair. Treated as a log-linear model, we need to assign a weight for each of the features. Och (2003) 52 Proceedings of the 2009 Co"
D09-1006,W09-0424,1,0.156588,"kes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 1 Introduction Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation. Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair. Treated as a log-linear model, we need to assign a weight for each of the features. Och (2003) 52 Proceedings of the 2009 Conference on Empiri"
D09-1006,niessen-etal-2000-evaluation,0,0.0915191,"Missing"
D09-1006,P02-1038,0,0.0284908,"tric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 1 Introduction Many state-of-the-art machine translation (MT) systems over the past few years (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2007; Koehn et al., 2007; Li et al., 2009) rely on several models to evaluate the “goodness” of a given candidate translation in the target language. The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation. Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair. Treated as a log-linear model, we need to assign a weight for each of"
D09-1006,W09-0401,1,\N,Missing
D09-1030,P04-1079,0,0.0132433,"rums like the NIST MT Evaluation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program. Various types of human judgments are used. NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and Introduction Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct. Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality. Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments. Therefore, having people evaluate translation output would be preferable, if it were more practical. In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought. We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums 286 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, c Singapore, 6-7 August 2009. 2"
D09-1030,E06-1032,1,0.559904,"uation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program. Various types of human judgments are used. NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and Introduction Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct. Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality. Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments. Therefore, having people evaluate translation output would be preferable, if it were more practical. In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought. We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums 286 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP WMT collect"
D09-1030,W08-0309,1,0.407219,"8), and it is clear that they only loosely approximate human judgments. Therefore, having people evaluate translation output would be preferable, if it were more practical. In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought. We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums 286 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP WMT collect relative rankings (Callison-Burch et al., 2008; Paul, 2006), and DARPA evaluates using HTER (Snover et al., 2006). The details of these are provided later in the paper. Public evaluation campaigns provide a ready source of goldstandard data that non-expert annotations can be compared to. 3 have a specialized background in the subject, so there is an obvious tradeoff between hiring individuals from this non-expert labor pool and seeking out annotators who have a particular expertise. 4 Experts versus non-experts We use Mechanical Turk as an inexpensive way of evaluating machine translation. In this section, we measure the level of agreemen"
D09-1030,D08-1064,0,0.00479925,"T, as well as the project-specific Go/No Go evaluations for the DARPA GALE program. Various types of human judgments are used. NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and Introduction Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct. Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality. Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments. Therefore, having people evaluate translation output would be preferable, if it were more practical. In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought. We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums 286 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP WMT collect relative rankings (Ca"
D09-1030,E06-1005,0,0.0108157,"Missing"
D09-1030,P02-1040,0,0.126647,"be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually. These include public forums like the NIST MT Evaluation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program. Various types of human judgments are used. NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and Introduction Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct. Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality. Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments. Therefore, having people evaluate translation output would be preferable, if it were more practical. In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought. We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people"
D09-1030,N07-1029,0,0.0101817,"Missing"
D09-1030,2006.amta-papers.25,0,0.0421924,"erefore, having people evaluate translation output would be preferable, if it were more practical. In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought. We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums 286 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP WMT collect relative rankings (Callison-Burch et al., 2008; Paul, 2006), and DARPA evaluates using HTER (Snover et al., 2006). The details of these are provided later in the paper. Public evaluation campaigns provide a ready source of goldstandard data that non-expert annotations can be compared to. 3 have a specialized background in the subject, so there is an obvious tradeoff between hiring individuals from this non-expert labor pool and seeking out annotators who have a particular expertise. 4 Experts versus non-experts We use Mechanical Turk as an inexpensive way of evaluating machine translation. In this section, we measure the level of agreement between expert and non-expert judgments of translation quality. T"
D09-1030,D08-1027,0,0.751362,"Missing"
D09-1030,D09-1006,1,0.635237,"e created, administered, and graded, with only very minimal intervention. We believe that it is feasible to use Mechanical Turk for a wide variety of other machine translated tasks like creating word alignments for sentence pairs, verifying the accuracy of document- and sentence-alignments, performing non-simulated active learning experiments for statistical machine translation, even collecting training data for low resource languages like Urdu. The cost of using Mechanical Turk is low enough that we might consider attempting quixotic things like human-in-the-loop minimum error rate training (Zaidan and Callison-Burch, 2009), or doubling the amount of training data available for Urdu. • Why did the bystander call emergency services? He was concerned for Ms. Locklear’s life. • Why was Heather Locklear arrested in Santa Barbara? Because she was driving under the influence of drugs • Where did the witness see her acting abnormally? Pulling out of parking in Montecito • Where was Ms. Locklear two months ago? She was at a specialist clinic in Arizona. 5 questions were excluded as being redundant • What was Heather Locklear arrested for? Driving under the influence of drugs • Where was she taken for testing? A speciali"
D09-1030,W09-0401,1,\N,Missing
D09-1030,2006.iwslt-evaluation.1,0,\N,Missing
D09-1040,P01-1008,0,0.560903,"search directions in Section 6. 2 More recently, Callison-Burch (2008) has improved performance of this pivoting technique by imposing syntactic constraints on the paraphrases. The limitation of such an approach is the reliance on a good parser (in addition to reliance on bitexts), but a good parser is not available in all languages, especially not in resource-poor languages. Another approach using a pivoting technique augments the human reference translation with paraphrases, creating additional translation “references” (Madnani et al., 2007). Both approaches have shown gains in B LEU score. Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. In addition to the parallel corpus usage limitations described above, this technique is further limited by the small size of such materials, which are even scarcer than the resources in the pivoting case. Dolan et al. (2004) explore generating paraphrases by edit-distance and headlines of timeand topic-clustered news articles; they do not address the OOV problem directly, as their focus is sentence-level paraphrases; although they use a standard SMT measure, alignment error rate (AER),"
D09-1040,N06-1003,1,0.193059,"s used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this"
D09-1040,C04-1051,0,0.245926,"ally not in resource-poor languages. Another approach using a pivoting technique augments the human reference translation with paraphrases, creating additional translation “references” (Madnani et al., 2007). Both approaches have shown gains in B LEU score. Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. In addition to the parallel corpus usage limitations described above, this technique is further limited by the small size of such materials, which are even scarcer than the resources in the pivoting case. Dolan et al. (2004) explore generating paraphrases by edit-distance and headlines of timeand topic-clustered news articles; they do not address the OOV problem directly, as their focus is sentence-level paraphrases; although they use a standard SMT measure, alignment error rate (AER), they only report results of the alignment quality, and not of an end-to-end SMT system. Much of the previous research largely focused on morphological analysis in order to reduce type sparseness; Callison-Burch et al. (2006) list some of the influential work in that direction. Related Work This is not the first to attempt to amelio"
D09-1040,D08-1094,0,0.0671729,"Missing"
D09-1040,W06-1605,0,0.0164058,"ntic similarity). Collocational Profiles The sliding window and word association (SoA) measures. Some researchers count positional collocations in a sliding window, i.e., the cocounts and SoA measures are calculated per relative position (e.g., for some word/token u, position 1 is the token immediately after u; position -2 is the token preceding the token that precedes u) (Rapp, 1999); other researchers use nonpositional (which we dub here flat) collocations, meaning, they count all token occurrences within the sliding window, regardless of their positions in it relative to u (McDonald, 2000; Mohammad and Hirst, 2006). We use here flat collocations in a 6-token sliding window. Beside simple cooccurrence counts within sliding windows, other SoA measures include functions based on TF/IDF (Fung and Yee, 1998), mutual information (PMI) (Lin, 1998), conditional probabilities (Schuetze and Pedersen, 1997), chi-square test, and the loglikelihood ratio (Dunning, 1993). The distributional hypothesis and distributional profiles. Natural language processing (NLP) applications that assume the distributional hypothesis (Harris, 1940; Firth, 1957) typically keep track of word co-occurrences in distributional profiles (a"
D09-1040,J05-4003,0,0.110194,", and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraph"
D09-1040,C04-1151,0,0.0548243,"anslations from them, and in a sufficiently fast, memory-efficient, and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity"
D09-1040,P00-1056,0,0.109801,"or all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al., 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). Feature weights were set with minimum error rate training (Och, 2003) on a development set using B LEU (Papineni et al., 2002) as the objective function. Test results were evaluated using B LEU and TER (Snover et al., 2005). The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training (Och and Ney, 2000) on both source and target sides of the parallel training sets. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. The paraphrase-augmented systems were identical to the corresponding baseline system, with the exception of additional (paraphrase-based) translation rules, and additional feature(s). Similarly to Callison-Burch et al. (2006), we added the following feature: 5.1 English-to-Chinese Translation For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIS tests ("
D09-1040,P98-1069,0,0.542655,"ems cannot learn from non-aligned corpora, while sentence-aligned parallel corpora (bitexts) are a limited resource (See Section 2 for discussion of automaticallycompiled bitexts). Another direction might be to make use of non-parallel corpora for training. However, this requires developing techniques to extract alignments or translations from them, and in a sufficiently fast, memory-efficient, and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006;"
D09-1040,P02-1038,0,0.094899,"that this method is insensitive to the order in which the paraphrases are processed. We only augment the phrase table with a single rule from f to e, and in it are the feature values of the phrase fi for which the score sim(fi , f ) was the highest. We examined the application of the system’s paraphrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al., 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). Feature weights were set with minimum error rate training (Och, 2003) on a development set using B LEU (Papineni et al., 2002) as the objective function. Test results were evaluated using B LEU and TER (Snover et al., 2005). The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training (Och and Ney, 2000) on both source and target sides of the parallel training sets. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the forei"
D09-1040,P03-1021,0,0.0293555,"essed. We only augment the phrase table with a single rule from f to e, and in it are the feature values of the phrase fi for which the score sim(fi , f ) was the highest. We examined the application of the system’s paraphrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al., 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). Feature weights were set with minimum error rate training (Och, 2003) on a development set using B LEU (Papineni et al., 2002) as the objective function. Test results were evaluated using B LEU and TER (Snover et al., 2005). The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training (Och and Ney, 2000) on both source and target sides of the parallel training sets. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. The paraphrase-augmented systems were"
D09-1040,N03-1017,0,0.0945165,"Missing"
D09-1040,P99-1067,0,0.850725,"n-aligned corpora, while sentence-aligned parallel corpora (bitexts) are a limited resource (See Section 2 for discussion of automaticallycompiled bitexts). Another direction might be to make use of non-parallel corpora for training. However, this requires developing techniques to extract alignments or translations from them, and in a sufficiently fast, memory-efficient, and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al.,"
D09-1040,J03-3002,1,0.403077,"999; Diab and Finch, 2000). This approach is sometimes viewed as, or combined with, an information retrieval (IR) approach, and normalizes strength-of-association measures (see Section 3) with IR-related measures such as TF/IDF (Fung and Yee, 1998). To date, reported implementations suffer from scalability issues, as they pre-compute and hold in memory a huge collocation matrix; we know of no report of using this approach in an end-to-end SMT system. Another approach aiming to reduce OOV rate concentrates on increasing parallel training set size without using more dedicated human translation (Resnik and Smith, 2003; Oard et al., 2003). 3 ∀u, v, w ∈ V, [semsim(u, v) > semsim(u, w)] =⇒ [psim(DPu , DPv ) > psim(DPu , DPw )], where V is the language vocabulary, DPword is the distributional profile of word, and psim() is a 2-place vector similarity function (all further described below). Paraphrasing and other NLP applications that are based on the distributional hypothesis assume entailment in the reverse direction: the right-hand-side of Formula (2) (profile/vector similarity) entails the left-hand-side (semantic similarity). Collocational Profiles The sliding window and word association (SoA) measures. So"
D09-1040,W04-3250,0,0.0661927,"Missing"
D09-1040,koen-2004-pharaoh,0,0.013606,"Missing"
D09-1040,2005.mtsummit-papers.11,0,0.0791843,"ontroversial question about our obviously with the developments this morning community staffing of community centres perhaps we are getting rather impatient er around the inner edge interested in going to the topics and that is the day that as a as a final point left which it may still have Spanish-to-English Translation In order to to permit a more direct comparison with the pivoting technique, we also experimented with Spanish to English (S2E) translation, following Callison-Burch et al. (2006). For baseline we used the Spanish and English sides of the Europarl multilingual parallel corpus (Koehn, 2005), with the standard training, development, and test sets. We created training subset models of 10,000, 20,000, and 80,000 aligned sentences, as described in Callison-Burch et al. (2006). For better comparison with their pivoting system, we used the same 5-gram language model, development and test sets: For development, we used the Europarl dev2006 Spanish and English sides, and for testing we used the Europarl 2006 test set. We trained the Spanish paraphrase generation system on the Spanish corpora available from Score 0.56 0.53 0.45 0.42 0.33 0.32 0.30 0.87 0.82 0.68 0.67 0.65 0.56 0.54 0.74"
D09-1040,W07-0716,1,0.938011,"and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this problem by deriving par"
D09-1040,D09-1081,1,0.833919,"Missing"
D09-1040,J93-2003,0,\N,Missing
D09-1040,N03-2026,1,\N,Missing
D09-1040,C98-1066,0,\N,Missing
D09-1040,D08-1021,1,\N,Missing
D09-1040,P07-2045,1,\N,Missing
D09-1040,P05-1033,0,\N,Missing
D09-1040,J07-2003,0,\N,Missing
D09-1040,I05-3027,0,\N,Missing
D11-1108,P05-1074,1,0.945191,"rity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank t"
D11-1108,W03-1004,0,0.0534231,"istical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-"
D11-1108,N03-1003,0,0.352985,"Describe how training paradigms for syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. • Demonstrate our framework’s suitability for a variety of text-to-text generation tasks by obtaining state-of-the-art results on the example task of sentence compression. 2 Related Work Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. These include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKe"
D11-1108,P01-1008,0,0.863018,"tated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expression"
D11-1108,P99-1071,0,0.227065,"scuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meanin"
D11-1108,P08-1077,0,0.373054,"ollections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to ad"
D11-1108,J93-2003,0,0.0293765,"with an equal number of nonterminals cNT (γ) = cNT (α) and ∼: {1 . . . cNT (γ)} → {1 . . . cNT (α)} constitutes a one-to-one correspondency function between the nonterminals in γ and α. A nonnegative weight w ≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f ) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word align"
D11-1108,D08-1021,1,0.937898,"l Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1 ) (1) • Extend the bilingual pivoting approach to paraphrase induction to produce rich syntactic paraphrases. Several subsequent"
D11-1108,P05-1033,0,0.851368,"rases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1 ) (1) • Extend the bilingual pivoting approach to paraphrase induction to produce rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash cat"
D11-1108,J07-2003,0,0.556159,"araphrase induction to produce rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not inclu"
D11-1108,D07-1008,0,0.150486,"ere the compression rate cr falls in the range 0.5 < cr ≤ 0.8. From these, we randomly select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. 6.4 Grammar Augmentations As we discussed in Section 5, the paraphrase grammar we induce is capable of representing a wide variety of transformations. However, the formalism and extraction method are not explicitly geared towards a compression application. For instance, the synchronous nature of our grammar does not allow us to perform deletions of constituents as done by Cohn and Lapata (2007)’s tree transducers. One way to extend the grammar’s capabilities towards the requirements of a given task is by injecting additional rules designed to capture appropriate operations. For the compression task, this could include adding rules to delete target-side nonterminals: JJ → JJ |ε This would render the grammar asynchronous and require adjustments to the decoding process. Alternatively, we can generate rules that specifically delete particular adjectives from the corpus: JJ → superfluous |ε . In our experiments we evaluate the latter approach by generating optional deletion rules for all"
D11-1108,C08-1018,0,0.256051,"anguage Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1 ) (1) • Extend the bilingual pivoting approach to paraphrase induction to produce rich syntactic paraphrases. Several subsequent efforts extended the bi"
D11-1108,C04-1051,0,0.198539,"radigms for syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. • Demonstrate our framework’s suitability for a variety of text-to-text generation tasks by obtaining state-of-the-art results on the example task of sentence compression. 2 Related Work Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. These include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 19"
D11-1108,P10-4002,1,0.175165,"rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical rules. Cohn and Lapat"
D11-1108,J93-1004,0,0.146258,"from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contribution"
D11-1108,N04-1035,0,0.279686,"e nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical rules. Cohn and Lapata (2008) used the GHKM extraction method (Galley et al., 2004), which is limited to constituent phrases and thus produces a reasonably small set of syntactic rules. Zhao et al. (2008b) added slots to bilingually extracted paraphrase patterns that were labeled with part-ofspeech tags, but not larger syntactic constituents. Before the shift to statistical natural language processing, paraphrasing was often treated as syntactic transformations or by parsing and then generating • Perform a thorough analysis of the types of paraphrases we obtain and discuss the paraphrastic transformations we are capable of capturing. • Describe how training paradigms for syn"
D11-1108,D10-1051,0,0.0136359,"cision estimate of B LEU with an additional “verbosity penalty” that is applied to compressions that fail to meet a given target compression rate ϕ. We rely on the B LEU brevity penalty to prevent the system from producing overly aggressive compressions. The scaling term λ determines how severely we penalize deviations from ϕ. In our experiments we use λ = 10. It is straightforward to find similar adaptations for other tasks. For text simplification, for instance, the penalty term can include a readability metric. For poetry generation we can analogously penalize outputs that break the meter (Greene et al., 2010). 6.3 Development Data In Section 4 we discussed phrasal probabilities. While these help quantify how good a paraphrase is in general, they do not make any statement on task-specific things such as the change in language complexity or text length. To make this information available to the decoder, we enhance our paraphrases with four compression-targeted features. We add the count features csrc and ctgt , indicating the number of words on either side of the rule as well as two difference features: cdcount = ctgt − csrc and the analogously computed difference in the average word To tune the par"
D11-1108,P07-1019,0,0.00898414,"inals, which enables us to promote more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: max p(d, e2 |e1 )). Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis CD NNS JJ twelve cartoons insulting CD C → hα1 , α2 , ∼, ϕ ~ i, d∈D(e2 ,e1 ) DT+NNP DT NNP the prophet mohammad 12 of the cartoons that are offensive to the prophet mohammad we create a paraphrase rule: eˆ2 ≈ yield (arg NP JJ NNS DT NNP DT+NNP NP NP VP NP Paraphrase Rule Foreign Pivot Phrase Lexical paraphrase: JJ → offensive |insulting JJ -&gt; beleidigend |offensive JJ -&gt; beleidigend |insulting Reduced relative clause: NP → NP that VP |NP VP NP -&gt; NP die VP |NP VP NP -&gt; NP die VP |NP that VP Pred. adjective copula deletion: VP → are JJ to NP |JJ NP VP → sind JJ für NP |are JJ to NP"
D11-1108,N06-1058,0,0.0649688,"1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of"
D11-1108,N03-1017,0,0.0243958,"ese include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and"
D11-1108,W03-1601,0,0.0356656,"the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and"
D11-1108,W98-1426,0,0.0720656,"reorder , fires if the rule swaps the order of two nonterminals, which enables us to promote more complex paraphrases that require structural reordering. Decoding With this, paraphrasing becomes an English-to-English translation problem which can be formulated similarly to Equation 5 as: max p(d, e2 |e1 )). Figure 3 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. Another advantage of using the decoder from statistical machine translation is that n-gram language models, which have been shown to be useful in natural language generation (Langkilde and Knight, 1998), are already well integrated (Huang and Chiang, 2007). 5 Analysis CD NNS JJ twelve cartoons insulting CD C → hα1 , α2 , ∼, ϕ ~ i, d∈D(e2 ,e1 ) DT+NNP DT NNP the prophet mohammad 12 of the cartoons that are offensive to the prophet mohammad we create a paraphrase rule: eˆ2 ≈ yield (arg NP JJ NNS DT NNP DT+NNP NP NP VP NP Paraphrase Rule Foreign Pivot Phrase Lexical paraphrase: JJ → offensive |insulting JJ -&gt; beleidigend |offensive JJ -&gt; beleidigend |insulting Reduced relative clause: NP → NP that VP |NP VP NP -&gt; NP die VP |NP VP NP -&gt; NP die VP |NP that VP Pred. adjective copula deletion: VP →"
D11-1108,W09-0424,1,0.517892,"ction to produce rich syntactic paraphrases. Several subsequent efforts extended the bilingual pivoting technique, many of which introduced elements of more contemporary syntax-based approaches to statistical machine translation. Madnani et al. (2007) extended the technique to hierarchical phrase-based machine translation (Chiang, 2005), which is formally a synchronous context-free grammar (SCFG) and thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical r"
D11-1108,W10-1718,1,0.81311,"toolkit (Venugopal and Zollmann, 2009). The grammars we extract tend to be extremely large. To keep their size manageable, we only consider translation rules that have been seen more than 3 times and whose translation probability exceeds 10−4 for pivot recombination. Additionally, we only retain the top 25 most likely paraphrases of each phrase, ranked by a uniformly weighted combination of phrasal and lexical paraphrase probabilities. We tuned the model parameters to our P R E´ CIS objective function, implemented in the Z-MERT toolkit (Zaidan, 2009). For decoding we used the Joshua decoder (Li et al., 2010). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 6.6 Evaluation To assess the output quality of the resulting sentence compression system, we compare it to two state-ofthe-art sentence compression systems. Specifically, we compare against our implementation of Clarke and Lapata (2008)’s compression model which uses a series of constraints in an integer linear programming (ILP) solver, and Cohn and Lapata (2007)’s tree transducer toolkit"
D11-1108,P06-1077,0,0.0471752,"≥ 0 is assigned to each rule, reflecting the likelihood of the rule. Rule Extraction Phrase-based approaches to statistical machine translation (and their successors) extract pairs of (e, f ) phrases from automatically word-aligned parallel sentences. Och (2003b) described various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) wordalignment models. These phrase extraction heuristics have been extended so that they extract synchronous grammar rules (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006). Most of these extraction methods require that one side of the parallel corpus be parsed. This is typically done automatically with a statistical parser. Figure 1 shows examples of rules obtained from a sentence pair. To extract a rule, we first choose a source side span f like das leck. Then we use phrase extraction techniques to find target spans e that are consistent with the word alignment (in this case the leak is consistent with our f ). The nonterminal symbol that is the left-hand side of the SCFG rule is then determined by the syntactic constituent that dominates e (in this case NP)."
D11-1108,J10-3003,0,0.435482,"raphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amou"
D11-1108,W07-0716,0,0.353926,"ing and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic"
D11-1108,E06-1038,0,0.0138678,"allows changes to the tree topology. Cohn and Lapata argue that this is a natural fit for sentence compression, since deletions introduce structural mismatches. We trained the T3 software2 on the 936 hfull, compressedi sentence pairs that comprise our development set. This is equivalent in size to the training corpora that Cohn and Lapata (2007) used (their training corpora ranged from 2 www.dcs.shef.ac.uk/people/T.Cohn/t3/ 882–1020 sentence pairs), and has the advantage of being in-domain with respect to our test set. Both these systems reported results outperforming previous systems such as McDonald (2006). To showcase the value of the adaptations discussed above, we also compare variants of our paraphrase-based compression systems: using Hiero instead of syntax, using syntax with or without compression features, using an augmented grammar with optional deletion rules. We solicit human judgments of the compressions along two five-point scales: grammaticality and meaning. Judges are instructed to decide how much the meaning from a reference translation is retained in the compressed sentence, with a score of 5 indicating that all of the important information is present, and 1 being that the compr"
D11-1108,P79-1016,0,0.845325,"illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing no"
D11-1108,P04-1083,0,0.0345897,"se of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lap"
D11-1108,C88-2088,0,0.562127,"nolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 20"
D11-1108,C82-1038,0,0.678767,"al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase"
D11-1108,W11-1611,1,0.464043,"Missing"
D11-1108,P03-1021,0,0.472333,"g (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the"
D11-1108,W06-3112,0,0.0538655,"Missing"
D11-1108,N03-1024,0,0.109714,"ints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A variety of different types of corpora (and semantic equivalence cues) have been used to automatically induce paraphrase collections for English (Madnani and Dorr, 2010). Perhaps the most natural type of corpus for this task is a monolingual parallel text, which allows sentential paraphrases to be extracted since the sentence pairs in such corpora are perfect paraphrases of each other (Barzilay and McKeown, 2001; Pang et al., 2003). While rich syntactic paraphrases have been learned from monolingual parallel corpora, they suffer from very limited data availability and thus have poor coverage. Other methods obtain paraphrases from raw monolingual text by relying on distributional similarity (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). While vast amounts of data are readily available for these approaches, the distributional similarity signal they use is noisier than the sentence-level correspondency in parallel corpora and additionally suffers from problems such as mistaking cousin expressions or antonyms (such"
D11-1108,P02-1040,0,0.0951856,"ohn and Lapata, 2008) or whether they include arbitrary phrases, including non-constituent phrases (Zollmann and Venugopal, 2006; Callison-Burch, 2008). We adopt the extraction for all phrases, including non-constituents, since it allows us to cover a much greater set of phrases, both in translation and paraphrasing. Feature Functions Rather than assigning a single weight w, we define a set of feature functions ϕ ~ = {ϕ1 ...ϕN } that are combined in a log-linear model: w=− N X λi log ϕi . (4) i=1 The weights ~λ of these feature functions are set to maximize some objective function like B LEU (Papineni et al., 2002) using a procedure called minimum error rate training (MERT), owing to Och (2003a). MERT iteratively adjusts the weights until the decoder produces output that best matches reference translations in a development set, according to the objective function. We will examine appropriate objective functions for text-to-text generation tasks in Section 6.2. Typical features used in statistical machine translation include phrase translation probabilities (calculated using maximum likelihood estimation over all phrase pairs enumerable in the parallel corpus), word-for-word lexical translation probabili"
D11-1108,W04-3219,0,0.110365,"≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be ext"
D11-1108,P05-1034,0,0.0285203,"al machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs i"
D11-1108,P02-1006,0,0.0251685,"raphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic"
D11-1108,P07-1059,0,0.0351686,"f sentence compression and achieve results competitive with state-of-the-art compression systems. 1 Introduction Paraphrases are alternative ways of expressing the same information (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’"
D11-1108,C96-2155,0,0.0278403,"a (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et"
D11-1108,W90-0102,0,0.41483,"nani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here synta"
D11-1108,W04-3206,0,0.0482543,"nd discuss the paraphrastic transformations we are capable of capturing. • Describe how training paradigms for syntactic/sentential paraphrase models should be tailored to different text-to-text generation tasks. • Demonstrate our framework’s suitability for a variety of text-to-text generation tasks by obtaining state-of-the-art results on the example task of sentence compression. 2 Related Work Madnani and Dorr (2010) survey a variety of datadriven paraphrasing techniques, categorizing them based on the type of data that they use. These include large monolingual texts (Lin and Pantel, 2001; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), comparable corpora (Barzilay and Lee, 2003; Dolan et al., 2004), monolingual parallel corpora (Barzilay and McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1"
D11-1108,J97-3002,0,0.246935,"zlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2"
D11-1108,P01-1067,0,0.0230038,"al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-linear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 200"
D11-1108,C02-1163,0,0.0325514,"McKeown, 2001; Pang et al., 2003), and bilingual parallel corpora (Bannard and Callison-Burch, 2005; Madnani et al., 2007; Zhao et al., 2008b). We focus on the latter type of data. Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005) who induced paraphrases using techniques from phrase-based statistical machine translation (Koehn et al., 2003). After extracting a bilingual 1169 f = X f ≈ X f p(e2 |f, e1 )p(f |e1 ) (2) p(e2 |f )p(f |e1 ). (3) from a semantic representation (McKeown, 1979; Muraki, 1982; Meteer and Shaked, 1988; Shemtov, 1996; Yamamoto, 2002). Indeed, some work generated paraphrases using (non-probabilistic) synchronous grammars (Shieber and Schabes, 1990; Dras, 1997; Dras, 1999; Kozlowski et al., 2003). After the rise of statistical machine translation, a number of its techniques were repurposed for paraphrasing. These include sentence alignment (Gale and Church, 1993; Barzilay and Elhadad, 2003), word alignment and noisy channel decoding (Brown et al., 1990; Quirk et al., 2004), phrase-based models (Koehn et al., 2003; Bannard and Callison-Burch, 2005), hierarchical phrase-based models (Chiang, 2005; Madnani et al., 2007), log-l"
D11-1108,P08-1116,0,0.54245,"expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1"
D11-1108,P08-1089,0,0.71213,"expressions or antonyms (such as {boy, girl } or {rise, fall }) for paraphrases. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168–1179, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step over foreign language phrases (Bannard and Callison-Burch, 2005). The coverage of paraphrase lexica extracted from bitexts has been shown to outperform that obtained from other sources (Zhao et al., 2008a). While there have been efforts pursuing the extraction of more powerful paraphrases (Madnani et al., 2007; Callison-Burch, 2008; Cohn and Lapata, 2008; Zhao et al., 2008b), it is not yet clear to what extent sentential paraphrases can be induced from bitexts. In this paper we: phrase table, English paraphrases are obtained by pivoting through foreign language phrases. Since many paraphrases can be extracted for a phrase, Bannard and Callison-Burch rank them using a paraphrase probability defined in terms of the translation model probabilities p(f |e) and p(e|f ): X p(e2 |e1 ) = p(e2 , f |e1"
D11-1108,P09-1094,0,0.0403077,"ear models and minimum error rate training (Och, 2003a; Madnani et al., 2007; Zhao et al., 2008a), and here syntaxbased machine translation (Wu, 1997; Yamada and Knight, 2001; Melamed, 2004; Quirk et al., 2005). Beyond cementing the ties between paraphrasing and syntax-based statistical machine translation, the novel contributions of our paper are (1) an in-depth analysis of the types of structural and sentential paraphrases that can be extracted with bilingual pivoting, (2) a discussion of how our English–English paraphrase grammar should be adapted to specific text-to-text generation tasks (Zhao et al., 2009) with (3) a concrete example of the adaptation procedure for the task of paraphrase-based sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008; Cohn and Lapata, 2009). 3 SCFGs in Translation The model we use in our paraphrasing approach is a syntactically informed synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by Chiang (2005). Formally, a probabilistic SCFG G is defined by specifying G = hN , TS , TT , R, Si, where N is a set of nonterminal symbols, TS and TT are the source and target lang"
D11-1108,N06-1057,0,0.0338613,"mation (Culicover, 1968). Automatically generating and detecting paraphrases is a crucial aspect of many NLP tasks. In multi-document summarization, paraphrase detection is used to collapse redundancies (Barzilay et al., 1999; Barzilay, 2003). Paraphrase generation can be used for query expansion in information retrieval and question answering systems (McKeown, 1979; Anick and Tipirneni, 1999; Ravichandran and Hovy, 2002; Riezler et al., 2007). Paraphrases allow for more flexible matching of system output against human references for tasks like machine translation and automatic summarization (Zhou et al., 2006; Kauchak and Barzilay, 2006; Madnani et al., 2007; Snover et al., 2010). 1168 Broadly, we can distinguish two forms of paraphrases: phrasal paraphrases denote a set of surface text forms with the same meaning: the committee’s second proposal the second proposal of the committee while syntactic paraphrases augment the surface forms by introducing nonterminals (or slots) that are annotated with syntactic constraints: the NP 1 ’s NP 2 the NP 2 of the NP 1 It is evident that the latter have a much higher potential for generalization and for capturing interesting paraphrastic transformations. A va"
D11-1108,W06-3119,0,0.275665,"nd thus can be thought of as a paraphrase grammar. The paraphrase grammar can paraphrase (or “decode”) input sentences using an SCFG decoder, like the Hiero, Joshua or cdec MT systems (Chiang, 2007; Li et al., 2009; Dyer et al., 2010). Like Hiero, Madnani’s model uses just one nonterminal X instead of linguistic nonterminals. Three additional efforts incorporated linguistic syntax. Callison-Burch (2008) introduced syntactic constraints by labeling all phrases and paraphrases (even non-constituent phrases) with CCGinspired slash categories (Steedman and Baldridge, 2011), an approach similar to Zollmann and Venugopal (2006)’s syntax-augmented machine translation (SAMT). Callison-Burch did not formally define a synchronous grammar, nor discuss decoding, since his presentation did not include hierarchical rules. Cohn and Lapata (2008) used the GHKM extraction method (Galley et al., 2004), which is limited to constituent phrases and thus produces a reasonably small set of syntactic rules. Zhao et al. (2008b) added slots to bilingually extracted paraphrase patterns that were labeled with part-ofspeech tags, but not larger syntactic constituents. Before the shift to statistical natural language processing, paraphrasi"
D11-1108,P97-1070,0,\N,Missing
D11-1108,C88-1016,0,\N,Missing
D11-1108,W10-0701,1,\N,Missing
D11-1108,P99-1039,0,\N,Missing
D11-1108,D08-1076,0,\N,Missing
D13-1056,2009.eamt-1.23,0,0.092581,"Missing"
D13-1056,D12-1032,0,0.0163741,"edness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves to another. Examples below show how similar is the name Bill associated with other names in log probability: Bill/Bill: -0.8 Bill/Billy: -5.2 Bill/William: -13.6 Bill/Mary: -18.6 Finally, one decision we made during feature design was not to use any parsing-based features, with a permissive assumption that the input might not be well-formed English, or even not complete sentences (such as fragmented snippets from web search). The “deepest” linguistic processing stays at the level of tagging and chunking, making the"
D13-1056,P11-1131,0,0.0343557,"Missing"
D13-1056,P06-1009,0,0.241342,"translation) rather than monolingual. Moreover, most work has considered token-based approaches over phrase-based.1 Here we seek to address this imbalance by proposing better phrase-based models for monolingual word alignment. ∗ Performed while faculty at Johns Hopkins University. In this paper we use the term token-based alignment for one-to-one alignment and phrase-based for non one-to-one alignment, and word alignment in general for both. 1 The token aligner jacana-align (Yao et al., 2013a) has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of Blunsom and Cohn (2006). It employs a Conditional Random Field (Lafferty et al., 2001) to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as “observation” and target tokens as “hidden states”. However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying model: a state can only span one token each time, making it unable to align multiple consecutive tokens (i.e. a phrase). We extend this model by introducing semiMarkov states for phrase-based alignment: a state can instead span multiple consecutive time steps, thus a"
D13-1056,J93-2003,0,0.0472056,"e. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-p"
D13-1056,W07-1427,0,0.102732,"Missing"
D13-1056,J08-4005,1,0.880184,"Missing"
D13-1056,P09-1053,0,0.2096,"Missing"
D13-1056,W11-2107,0,0.0285654,"Missing"
D13-1056,C04-1051,0,0.429672,"and phrase-only alignment (subscript p). it is another topic, we simply show in this section using just alignment scores in binary prediction problems. Specifically, we pick the tasks of recognizing textual entailment (RTE), paraphrase identification (PP), and question answering sentence ranking (QA) described in Heilman and Smith (2010): RTE: predicting whether a hypothesis can be inferred from the premise, with training data from RTE-1/2 and RTE-3 dev, and test from RTE-3 test. PP: predicting whether two sentences are paraphrases, with training and test data from the MSR Paraphrase Corpus (Dolan et al., 2004). QA: predicting whether a sentence contains the answer to the question, with training data from TREC-8 to TREC-12 and test data from TREC-13. For each aligned pair, we can compute a normalized decoding score. Following MacCartney et al. (2008), we select a threshold score and predict true if the decoding score is above this threshold. For the tasks of RTE and PP, we tuned this threshold w.r.t the maximal accuracy on the training set, then reported performance on the test set. For the task of QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked"
D13-1056,N13-1092,1,0.387664,"Missing"
D13-1056,N10-1112,0,0.0196559,"ue labels. It is only computed during training in the denominator because in the numerator cost(ay , ay ) = 0. Hamming cost is used in practice without learning the weights (i.e., uniform weights). The more inconsistence there is between ay and ˆ a, the more penalized is the decoding sequence ˆ a through the cost function. 3.2 1 6 7..14 Z(s, t) X Shops ...-... P exp( i,k λk fk (ai−1 , ai , s, t)) This assumes a first-order Conditional Random Field (Lafferty et al., 2001). Since the word alignment task is evaluated over F1 , instead of directly optimizing it, we choose a much easier objective (Gimpel and Smith, 2010) and add a cost function to the normalizing function Z(s, t) in the denominator: Z(s, t) = 0 Phrase-based Model The token-based model supports 1 : 1 alignment. We first extend it in the direction of ls : 1, where a target state spans ls words on the source side (ls source words align to 1 target word). Then we extend it in the direction of 1 : lt , where lt is the target phrase length a source word aligns to (1 source word aligns to lt target words). The final combined 592 15 shops Shops are closed up for now until March Figure 1: A semi-Markov phrase-based model example and the desired Viterb"
D13-1056,S13-1005,0,0.0116787,"s is the left hand side syntactic non-terminal symbol. We did not use the syntactic part (e.g., the N P of N N S ↔ the N N S of N P ) of PPDB as we did not make the assumption that the input sentence pairs were well-formed (and newswire-like) English, or 594 even of a language with a parser available. Also, for phrasal alignments, we ruled out those paraphrases spanning multiple syntactic structures, or of different syntactic structures (indicated as [X] in PPDB), for instance, and crazy ↔ , mad. Semantic Relatedness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves"
D13-1056,N10-1145,0,0.119622,"on Empirical Methods in Natural Language Processing, pages 590–600, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron lea"
D13-1056,N06-1014,0,0.0177325,"08) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just like how they were used in bilingual alignment). To combine the strengths of both semi-Markov models and discriminative training, we propose to use the semi-Markov Condi"
D13-1056,C08-1066,0,0.0315246,"odels (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Lia"
D13-1056,D08-1084,0,0.228637,"d aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can b"
D13-1056,W02-1018,0,0.0337279,"sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens."
D13-1056,P09-2073,0,0.0234686,"ttle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment o"
D13-1056,J03-1002,0,0.0465458,"Missing"
D13-1056,N03-1024,0,0.0311421,"nt of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences"
D13-1056,D12-1016,0,0.0332604,"rase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadan"
D13-1056,P11-2044,0,0.385701,"work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact a"
D13-1056,C12-2120,0,0.278948,"Missing"
D13-1056,C96-2141,0,0.435527,"essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2 http://code.google.com/p/jacana/ 591 bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions"
D13-1056,U06-1019,0,0.0376029,"Missing"
D13-1056,C10-1131,0,0.102753,"Missing"
D13-1056,D07-1003,0,0.114867,"Missing"
D13-1056,P13-2123,1,0.662726,"Missing"
D13-1056,N13-1106,1,0.881841,"Missing"
D13-1056,N03-1003,0,\N,Missing
D13-1056,U04-1000,0,\N,Missing
D16-1106,D13-1160,0,0.075922,"Missing"
D16-1106,chang-manning-2012-sutime,0,0.0296917,"us on the those events identified by the system which are relevant to the main fields in the GVDB schema.4 We map the arguments of these events onto the corresponding database fields, e.g. the agent of the event corresponds to the GVDB’s shooter name. Since the system identifies multiple such events per article, we count it as correct as long as one argument correctly matches the corresponding value in the GVDB (e.g. the system is correct as long as one extracted event has an agent which matches the GVDB’s shooter name for that article). In addition, we run the Stanford CoreNLP TimeEx system (Chang and Manning, 2012) over the articles in order to identify the time of the reported incident. We report the system’s performance using both exact match against the gold annotation (“strict”) as well as an approximate match, in which the system is correct if it is either a substring or a superstring of the gold annotation. E.g. if the victim name is Sean Bolton, the approximate metric will count both Bolton and Officer Sean Bolton as correct. While performance is high for certain structured types of information, like dates and times, fields like victim and shooter name are much less reliably identified. Furthermo"
D16-1106,D11-1142,0,0.0254719,"n, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 1 Although these technological achievements are profound, often times we as researchers apply them to somewhat trivial settings like learning about the latest Hollywood divorces (Wijaya et al., 2015) or learning silly facts about the world, like that hwhite suites, will never go out of, stylei (Fader et al., 2011). In this paper, we call the attention of the NLP community to one particularly good use case of our current technology, which could have profound policy implications: gun violence research. Introduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards unde"
D16-1106,P13-1008,1,0.794198,"mation Table 1: Current contents of the GVDB. Size and level of annotation is continually growing. See Forthcoming Extensions. Current Baselines To establish a baseline level of performance, we run an off-the-shelf information 3 See supplementary material for all extracted information and screenshots. Figure 2: Annotation interface associates structured information (e.g. the time of day when the shooting occurred) with a specific span of text in the article. extraction system on the 7,366 articles and measure precision and recall for identifying key information about the incidents. We use the Li et al. (2013) systems, which identifies a range of entities and events. We focus on the those events identified by the system which are relevant to the main fields in the GVDB schema.4 We map the arguments of these events onto the corresponding database fields, e.g. the agent of the event corresponds to the GVDB’s shooter name. Since the system identifies multiple such events per article, we count it as correct as long as one argument correctly matches the corresponding value in the GVDB (e.g. the system is correct as long as one extracted event has an agent which matches the GVDB’s shooter name for that a"
D16-1106,N10-1021,0,0.103628,"Missing"
D16-1106,W15-1205,0,0.017789,"s. The National Violent Death Registry System, arguably the most organized effort, receives data from only 16 states. Most large-scale epidemiological studies sample information from only 100 Emergency Departments. across the text of thousands of web pages. Replacing expensive, manual data entry with automated processing is exactly the type of problem that NLP is made to solve. In fact, the recent application of NLP tools to social science problems has generated a flurry of exciting and encouraging results. NLP has made novel contributions to the way scientists measure everything from income (Preoctiuc-Pietro et al., 2015b) to mental health (Preoctiuc-Pietro et al., 2015a; Schwartz et al., 2016; Choudhury et al., 2016), disease (Santillana et al., 2015; Ireland et al., 2015; Eichstaedt et al., 2015), and the quality of patient care (Nakhasi et al., 2016; Ranard et al., 2016). Text mining has promise for the study of gun violence, too (Bushman et al., 2016). However, most questions about gun violence are not easily answered using shallow analyses like topic models or word clusters. Epidemiologists want to know, for example, does gun ownership lead to increases in gun violence? Or, is there evidence of contagion"
D16-1106,N13-1008,0,0.0109243,"ssion as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers across the country, data abounds. We argue that this is the exac"
D16-1106,W09-2417,0,0.0216846,"roduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers"
D16-1106,N16-1008,0,0.0243649,"n language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is an ambitious goal, and the current state-of-the-art of language technology has made impressive strides towards understanding “who did what to whom, when, where, how, and why” (Kao and Poteet, 2007). Advances in NLP have enabled us to read news in real time (Petrovi´c et al., 2010), identify the key players (Ruppenhofer et al., 2009), recognize the relationships between them (Riedel et al., 2013), summarize the new information (Wang et al., 2016), update central databases Gun violence is an undeniable problem in the United States, but its causes are poorly understood, and attempts to reason about solutions are often marred by emotions and political bias. Research into the factors that cause and prevent gun violence is limited by the fact that data collection is expensive, and political agendas have all but eliminated funding on the topic. However, in the form of unstructured natural language published daily by newspapers across the country, data abounds. We argue that this is the exact type of information that NLP is designed to organ"
D16-1106,D15-1059,0,0.0129558,"t from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem. 1 Although these technological achievements are profound, often times we as researchers apply them to somewhat trivial settings like learning about the latest Hollywood divorces (Wijaya et al., 2015) or learning silly facts about the world, like that hwhite suites, will never go out of, stylei (Fader et al., 2011). In this paper, we call the attention of the NLP community to one particularly good use case of our current technology, which could have profound policy implications: gun violence research. Introduction The field of natural language processing often touts its mission as harnessing the information contained in human language: taking unstructured data in the form of speech and text, and transforming it into information that can be searched, categorized, and reasoned about. This is"
D16-1240,W12-5115,0,0.0465336,"Missing"
D16-1240,chang-manning-2012-sutime,0,0.0665372,"6) I planned to solve the problem tomorrow. We exploit this property to predict implicativeness– whether the truth of a verb’s complement can be inferred– by observing the verb’s usage in practice. 3 Method We hypothesize that, given a large corpus, we should be able to distinguish implicative verbs from nonimplicative verbs by observing how often the main verb tense agrees/disagrees with the tense of the complement clause. Unfortunately, verbs in infinitival complement clauses are not conjugated, and so are not necessarily marked for tense. We therefore use the Stanford Temporal Tagger (TT) (Chang and Manning, 2012) in order to identify time-referring expressions (e.g. tomorrow or last night) and resolve them to either past, present, or future tense. We find all sentences containing VB∗1 to VB2 constructions in the Annotated Gigaword corpus (Napoles et al., 2012). We run the the TT over all of the sentences in order to identify time-referring expressions. We only consider sentences in which a time-referring expression appears and is in a direct dependency relationship with the complement verb (VB2 ). We provide the TT with the document publication dates,3 which are used to resolve each time mention to a"
D16-1240,W07-1409,0,0.0270118,"e. For example, our method finds high tense agreement for choose to and be allowed to, which are often used to communicate, albeit indirectly, that their complements did in fact happen. To convince ourselves that treating such verbs as implicatives makes sense in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means"
D16-1240,W07-1401,0,0.0915829,"agreement yield more definitive judgments (true/false). Each bar represents aggregated judgements over approx. 20 verbs. Interestingly, tense agreement accurately models verbs that are not implicative by definition, but which nonetheless tend to behave implicatively in practice. For example, our method finds high tense agreement for choose to and be allowed to, which are often used to communicate, albeit indirectly, that their complements did in fact happen. To convince ourselves that treating such verbs as implicatives makes sense in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-c"
D16-1240,S12-1020,0,0.125807,"sts rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012). Another interesting extension concerns the role of tense in word representations. While currently, tense is rarely built directly into distributional representations of words (Mikolov et al., 2013; Pennington et al., 2014), our results suggest it may offer important insights into the semantics of individual words. We leave this question as a direction for future work. 6 Conclusion Differentiating between implicative and nonimplicative verbs is important for discriminating inferences that can and cannot be made in natural language. We have presented a data-driven method that captures the impl"
D16-1240,W09-3720,0,0.0237304,"nse in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012). Another interesting extension concerns the role of te"
D16-1240,W06-3907,0,0.47352,"as implicatives makes sense in practice, we manually look through 2228 the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method. Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives. 5 Discussion and Related Work Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data. Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled. The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems. The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012). Another interesting extensio"
D16-1240,W12-3018,0,0.0761681,"Missing"
D16-1240,D14-1162,0,0.0786456,"Missing"
D16-1240,P13-1162,0,0.154291,"ve verbs, like want, which do not permit any inference regarding their complements, and for which the truth of the complement is unaffected by negation in the main clause (Table 1). The method described in this paper aims to separate implicatives from non-implicatives (manage vs. want), rather than to differentiate between types implicatives (manage vs. fail). Making this implicative/non-implicative distinction is a necessary first step toward handling inferences involving embedded clauses, and one that, to date, has only been performed using manually-constructed word lists (MacCartney, 2009; Recasens et al., 2013). 2.1 Tense Constraints on Complement Clauses Karttunen (1971) observed that, in sentences involving implicatives, the tense of the main verb must necessarily match the tense of the complement clause. For example, (3), in which the main clause and the complement are both in the past tense, is acceptable but (4), in which the complement is in the future, is clearly not. For non-implicatives, however, tives, factives presuppose, rather than entail, their complements. E.g. both I was/was not glad to solve the problem entail I solved the problem. We do not address factives here, as factives rarely"
D17-1152,N06-1003,1,0.701386,"there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supe"
D17-1152,P11-2071,0,0.053725,"Missing"
D17-1152,P14-1079,0,0.0228582,"r low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix comple"
D17-1152,E14-1049,0,0.0756012,"nsla1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The mapping relies on seed translations which can be from existing dictiona"
D17-1152,P04-1064,0,0.0228215,"languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matri"
D17-1152,P16-1014,0,0.0134515,"coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with diff"
D17-1152,N13-1056,1,0.819043,"experiments on both low and high resource languages show the effectiveness of our model, outperforming the current stateof-the-art. • We make our code, datasets, and output translations publicly available.1 2 Related Work Bilingual Lexicon Induction Previous research has used different sources for estimating transla1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Simila"
D17-1152,J17-2001,1,0.854232,"y reported accuracies (Irvine and CallisonBurch, 2017) on test sets constructed from the same crowdsourced dictionaries (Pavlick et al., 2014)8 . The accuracies across languages appear to improve consistently with the amount of signals being input to the model. In the following experiments, we investigate how sensitive these improvements are with varying training size. In Figure 6, we show accuracies obtained by 7 Actual improvement per language depends on the coverage of the Wikipedia interlanguage links for that language 8 The comparison however, cannot be made apples-toapples since the way Irvine and Callison-Burch (2017) select test sets from the crowdsourced dictionaries maybe different and they do not release the test sets BPR NN kesadaran consciousness empathy awareness perceptions perception BPR WE kesadaran conscience awareness understanding consciousness acquaintance BPR WE with varying sizes of seed translation lexicons used to train its mapping. The results show that a seed lexicon size of 5K is enough across languages to achieve optimum performance. This finding is consistent with the finding of Vuli´c and Korhonen (2016) that accuracies peak at about 5K seed translations across all their models and"
D17-1152,D15-1015,0,0.252663,"Missing"
D17-1152,E12-1014,1,0.929334,"ata to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF"
D17-1152,P06-1103,0,0.411889,"an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF"
D17-1152,C12-1089,0,0.34901,"ata to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF"
D17-1152,N03-1017,0,0.052479,"f which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages. house EN koki Abstract Figure 1: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues. Introduction Machine translation (MT) models typically require large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to"
D17-1152,N15-1028,0,0.0613798,"Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The mapping relies on seed translations which can be from existing dictionaries or be reliably chosen from pseudo-bilingual corpora of comparable texts e.g.,"
D17-1152,W15-1521,0,0.0222246,"onolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The mapping relies on seed translations which can be from existing dictionaries or be reliably chosen from pseudo-bilingual corpora of compar"
D17-1152,P16-1009,0,0.0345859,"olingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages. house EN koki Abstract Figure 1: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues. Introduction Machine translation (MT) models typically require large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this p"
D17-1152,P16-1162,0,0.0205078,"olingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages. house EN koki Abstract Figure 1: Our framework allows us to use a diverse range of signals to learn translations, including incomplete bilingual dictionaries, information from related languages (like Indonesian loan words from Dutch shown here), word embeddings, and even visual similarity cues. Introduction Machine translation (MT) models typically require large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this p"
D17-1152,P15-2093,0,0.0438987,"ng space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matrix using MF with a Bayesian Personalized Ranking (BPR) objective (Rendle et al."
D17-1152,P95-1050,0,0.544339,"uire large, sentence-aligned bilingual texts to learn good translation models (Wu et al., 2016; Sennrich et al., 2016a; Koehn et al., 2003). However, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word- or subword- levels (Sennrich et al., 2016b) can be inaccurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be “out-of-vocabulary” words encountered at run-time. The Bilingual Lexicon Induction (BLI) task (Rapp, 1995), which learns word translations from monolingual or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating tr"
D17-1152,P16-1157,0,0.139163,"and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Method"
D17-1152,N16-1103,0,0.062977,"been shown to outperform traditional supervised methods in the presence of positive-only data (Riedel et al., 2013), which is true in our case since we only observe positive translations. (2) BPR is easily extendable to incorporate additional signals for inferring missing values in the matrix (He and McAuley, 2016). Since observed translations may be sparse, i.e. the “cold start” problem in the matrix completion task, incorporating additional signals of translation equivalence estimated on monolingual corpora is useful. (3) BPR is also shown to be effective for multilingual transfer learning (Verga et al., 2016). For low resource source languages, there may be related, higher resource languages from which we can project available translations (e.g., translations of loan words) to the target language (Figure 1). We conduct large scale experiments to learn translations from both low and high resource languages to English and achieve state-of-the-art performance on these languages. Our main contributions are as follows: • We introduce a MF framework that learns translations by integrating diverse bilingual and monolingual signals of translation, each potentially noisy/incomplete. • The framework is easi"
D17-1152,N13-1008,0,0.0217984,"2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matrix using MF with a Bayesian Personalized Ranking (BPR) objective (Rendle et al., 2009). We select BPR for a number of reasons: (1) BPR has been shown to outperform traditional supervised methods in the presence of positive-only data (Riedel et al., 2013), which is true in our case since we only observe positive translations. (2) BPR is easily extendable to incorporate additional signals for inferring missing values in the matrix (He and McAuley, 2016). Since observed translations may be sparse, i.e. the “cold start” problem in the matrix completion task, incorporating additional signals of translation equivalence estimated on monolingual corpora is useful. (3) BPR is also shown to be effective for multilingual transfer learning (Verga et al., 2016). For low resource source languages, there may be related, higher resource languages from which"
D17-1152,N15-1118,0,0.0449847,"Missing"
D17-1152,W02-2026,0,0.639555,"l or comparable corpora, is an attempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and reduce the need for parallel data to learn good translation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, coverage, and translation quality (Gulcehre et al., 2016; Callison-Burch et al., 2006; Daum´e and Jagarlamudi, 2011). Previous research has explored different sources for estimating translation equivalence from monolingual corpora (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and CallisonBurch, 2013, 2017). These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and CallisonBurch, 2016). More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We u"
D17-1152,P16-2031,0,0.0888149,"Missing"
D17-1152,P16-1024,0,0.138033,"Missing"
D17-1152,D13-1168,0,0.0615986,"Missing"
D17-1152,P15-2118,0,0.110968,"Missing"
D17-1152,P13-1084,0,0.0298198,"ximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision (Upadhyay et al., 2016). We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of information (Fan et al., 2014; Rockt¨aschel et al., 2015). MF is also shown to result in good crosslingual representations for tasks such as alignment (Goutte et al., 2004), QA (Zhou et al., 2013), and cross-lingual word embeddings (Shi et al., 2015). 1452 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1452–1463 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning translations as a matrix completion problem. Starting from some observed translations (e.g., from existing bilingual dictionaries,) we infer missing translations in the matrix using MF with a Bayesi"
D17-1152,D13-1141,0,0.050486,"n Previous research has used different sources for estimating transla1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic similarities between words are used to measure their translation equivalence (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Irvine and Callison-Burch, 2013, 2017). With the increasing popularity of word embeddings, many recent works approximate similarities between words in different languages by constructing a shared bilingual embedding space (Klementiev et al., 2012b; Zou et al., 2013; Vuli´c and Moens, 2013; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chandar A P et al., 2014; Gouws et al., 2015; Luong et al., 2015; Lu et al., 2015; Upadhyay et al., 2016). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar representations. Similarities between words can then be measured in the shared space. One approach to induce this shared space is to learn a mapping function between the languages’ monolingual semantic spaces (Mikolov et al., 2013a; Dinu et al., 2014). The"
D17-2007,N13-1092,1,0.779387,"Missing"
D17-2007,Q14-1035,0,0.0284059,"ed by KnowYourNyms? players (with frequency counts). standing applications and may provide useful insights for psycholinguistics research. Go to www.know-your-nyms.com to play KnowYourNyms?. 2 Related Work Several games with a purpose (GWAPs) have been developed for gathering linguistic annotations for building resources and training systems (Chamberlain et al., 2013). Lafourcade (2007) and Fort et al. (2014) developed games for defining semantic relations and dependency relations in French. Chamberlain et al. (2008) created Phrase Detectives to annotate and validate things like co-reference. Jurgens and Navigli (2014) recently proposed using video games to link Word37 Proceedings of the 2017 EMNLP System Demonstrations, pages 37–42 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics in their appropriate distribution. Once completed, another round begins. The rounds are short (5-20 seconds, depending on the relation type), which makes the game fun and easy to play in short periods of time. 4 System Implementation 4.1 The web application was built with the Django framework, using Python for all backend and database interaction and standard JavaScript, HTML, and CSS for"
D17-2007,S15-1021,0,0.0144274,"he scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations to play. All are selected by default. System Overview KnowYourNyms? is modeled after GWAPs like th"
D17-2007,L16-1722,0,0.0134093,"his example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations to play. All are selected by default. System Overview KnowYour"
D17-2007,W15-4208,0,0.0223658,"three main views. Figure 1: This example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations to play. All are selected by defa"
D17-2007,W16-5310,0,0.0570328,"erver. The application has multiple components that are important to the user experience, which are separated into three main views. Figure 1: This example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, anto"
D17-2007,W11-2501,0,0.0197132,"xperience, which are separated into three main views. Figure 1: This example scoring page shows the scores for the player’s words, and the top answers. Net senses to images and perform word sense disambiguation. KnowYourNyms? gathers high quality semantic relationships between English words to increase the coverage of resources like WordNet and assign a taxonomic structure to the Paraphrase Database (Ganitkevitch et al., 2013). Additionally, it provides rich data for training relation detection systems like LexNET (Shwartz and Dagan, 2016), up to now trained on small training datasets (BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), ROOT9 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015)). 3 Architecture Welcome Screen This screen gives information about the purpose of the game, what semantic relationships are, how to play, and a little about our team. When a user is signed in this screen displays some statistics about the player, including number of completed rounds, total score, and average score per round. Four checkboxes are displayed, one for each playable semantic relationship type (synonyms, antonyms, hyponyms, and meronyms). These allow the user to select which relations"
D17-2007,J17-4004,0,\N,Missing
D18-1202,J84-3009,0,0.153249,"Missing"
D18-1202,P18-1128,0,0.0192846,"* {quick} < {fast} < {speedy}* †† : p ≤ .01 †: p ≤ .05 Table 2: Pairwise relation prediction and global ranking results for each score type in isolation, and for the bestscoring combinations of 2 and 3 score types on each dataset. For the global ranking accuracy and average τb results, we denote with the † symbol scores for metrics incorporating paraphrase-based evidence that significantly out-perform both scorepat and scoresocal under the paired Student’s t-test, using the Anderson-Darling test to confirm that scores conform to a normal distribution (Fisher, 1935; Anderson and Darling, 1954; Dror et al., 2018). Example output is also given, with correct rankings starred. or positive correlation, and a value of 0 indicating no correlation between predicted and gold rankings. We report τb as a weighted average over scales in each dataset, where weights correspond to the number of adjective pairs in each scale. Spearman’s rho (ρ). We report the Spearman’s ρ rank correlation coefficient between predicted (rP (J)) and gold-standard (rG (J)) ranking permutations. For each dataset, we calculate this metric just once by treating each adjective in a particular scale as a single data point, and calculating a"
D18-1202,N13-1092,1,0.86228,"Missing"
D18-1202,P93-1023,0,0.785283,"xtracting sets of same-attribute adjectives from WordNet ‘dumbbells’ – consisting of two direct antonyms at the poles and satellites of synonymous/related adjectives incident to each antonym (Gross and Miller, 1990) – and ordering them by intensity. The annotations, however, are not in WordNet as of its latest version (3.1). Work on adjective intensity generally focuses on two related tasks: clustering adjectives based on the attributes they modify, and ranking sameattribute adjectives by intensity. With respect to the former, common approaches involve clustering adjectives by their contexts (Hatzivassiloglou and McKeown, 1993; Shivade et al., 2015). We do not focus on the clustering task in this paper, but concentrate on the ranking task. Approaches to the task of ranking scalar adjectives by their intensity mostly fall under the paradigms of pattern-based or lexicon-based approaches. Pattern-based approaches work by extracting lexical (Sheinman and Tokunaga, 2009; de Melo and Bansal, 2013; Sheinman et al., 2013) or syntactic (Shivade et al., 2015) patterns indicative of an intensity relationship from large corpora. For example, the patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjec"
D18-1202,C92-2082,0,0.163772,"approaches rely particularly pleased quite limited rather odd so silly completely mad ↔ ↔ ↔ ↔ ↔ ecstatic restricted crazy dumb crazy Figure 1: Examples of paraphrases from PPDB of the form RB JJu ↔ JJv which can be used to infer pairwise intensity relationships (JJu < JJv ). on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest. We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases wi"
D18-1202,D13-1169,0,0.230759,"Missing"
D18-1202,P10-1018,0,0.34145,"Missing"
D18-1202,Q13-1023,0,0.647048,"Missing"
D18-1202,P15-2070,1,0.904063,"Missing"
D18-1202,R15-1071,0,0.0652019,"adjectives ju and jv in JJG RAPH provides evidence about the relative intensity relationship between them. However, it has just been noted that JJG RAPH is noisy, containing both contradictory/cyclic edges and adverbs that are not uniformly intensifying. Rather than try to eliminate cycles, or manually annotate each adverb with a weight corresponding to its intensity and polarity 4 Other Intensity Evidence Our experiments compare the proposed paraphrase approach with existing pattern- and lexicon-based approaches. 4.1 Figure 3: A subgraph of JJG RAPH, depicting its directed graph structure. (Ruppenhofer et al., 2015; Taboada et al., 2011), we aim to learn these weights automatically in the process of predicting pairwise intensity. Given adjective pair (ju , jv ), we build a classifier that outputs a score from 0 to 1 indicating the predicted likelihood that ju < jv . Its binary features correspond to adverb edges from ju to jv and from jv to ju in JJG RAPH. The feature space includes only adverbs from R that appear at least 10 times in JJG RAPH, resulting in features for m = 259 unique adverbs in each direction (i.e. from ju to jv and vice versa) for 2m = 518 binary features total. Note that while all ad"
D18-1202,E14-4023,0,0.0822145,"paraphrases from PPDB of the form RB JJu ↔ JJv which can be used to infer pairwise intensity relationships (JJu < JJv ). on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest. We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases with approximately similar meaning, such as really great ↔ phenomenal. Adjectival paraphrases can be exploited to uncover intensity relationships. A paraphrase pair of t"
D18-1202,D15-1300,0,0.0141514,"patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjective less intense than Y. Lexicon-based approaches are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a 1 www.paraphrase.org lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011). Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant"
D18-1202,N15-1051,0,0.413677,"Missing"
D18-1202,J11-2001,0,0.101837,"hes are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a 1 www.paraphrase.org lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011). Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant than are moderate adverbs like very. Unlike Collex, which requires predetermined sets of ‘end-of-scale’ and ‘normal’ adverbial modi"
D18-1202,L16-1424,0,0.507498,"ng a full scale (e.g. freezing to sweltering), or a half scale (warm to sweltering); all three test sets group adjectives into half scales. The three datasets are described here, and their characteristics are given in Table 1. deMelo (de Melo and Bansal, 2013)4 . 87 adjective 4 http://demelo.org/gdm/intensity/ sets are extracted from WordNet ‘dumbbell’ structures (Gross and Miller, 1990), and partitioned into half-scale sets based on their pattern-based evidence in the Google N-Grams corpus (Brants and Franz, 2009). Sets are manually annotated for intensity relations (<, >, and =). Wilkinson (Wilkinson and Oates, 2016). Twelve adjective sets are generated by presenting crowd workers with small seed sets (e.g. huge, small, microscopic), and eliciting similar adjectives. Sets are automatically cleaned for consistency, and then annotated for intensity by crowd workers. While the original dataset contains full scales, we manually sub-divide these into 21 half-scales for use in this study. Details on the modification from full- to half-scales are in the Supplemental Material. Crowd. We also crowdsourced a new set of adjective scales with high coverage of the PPDB vocabulary. In a three-step process, we first ask"
D18-2021,P18-1239,1,0.882792,"Missing"
D18-2021,W14-1618,0,0.0378344,"eried and will be positioned in the vector space close to other OOV words based on their string similarity: from pymagnitude import ∗ vectors = Magnitude (&quot;w2v. magnitude &quot;) k = vectors . query (&quot;king&quot;) q = vectors . query (&quot; queen &quot;) vectors . similarity (k,q) # 0.6510958 Magnitude queries return almost instantly and are memory efficient. It uses lazy loading directly from disk, instead of having to load the entire model into memory. Additionally, Magnitude supports nearest neighbors operations, finding all words that are closer to a key than another key, and analogy solving (optionally with Levy and Goldberg (2014)’s 3CosMul function): &quot;uber&quot; in vectors # True &quot; uberx &quot; in vectors # False &quot; uberxl &quot; in vectors # False vectors . query (&quot; uberx &quot;) # Returns : [ 0.0507 , − 0.0708, ...] vectors . query (&quot; uberxl &quot;) # Returns : [ 0.0473 , − 0.08237 , ...] vectors . similarity (&quot; uberx &quot;, &quot; uberxl &quot;) # Returns : 0.955 vectors . most similar (k, topn =5) #[(‘ king ’, 1.0) , (‘ kings ’, 0.71) , # (‘ queen ’, 0.65) , (‘ monarch ’, 0.64) , # (‘ crown prince ’, 0.62)] vectors . most similar (q, topn =5) #[(‘ queen ’, 1.0) , (‘ queens ’, 0.74) , #(‘ princess ’, 0.71) , (‘king ’, 0.65) , # (’ monarch ’, 0.64)] vecto"
D18-2021,D14-1162,0,0.101877,"ntation8 for more information about conversion configuration options. Other matching nuances We employ other techniques when computing the string similarity metric, such as shrinking repeated character sequences of three or more to two (hiiiiiiii → hii), ranking strings of a similar length higher, and ranking strings that share the same first or last character higher for shorter words. 6 File Format To provide efficiency at runtime, Magnitude uses a custom “.magnitude” file format instead of “.bin”, “.txt”, “.vec”, or “.hdf5” that word2vec, GloVe, fastText, and ELMo use (Mikolov et al., 2013; Pennington et al., 2014; Joulin et al., 2016; Peters et al., 2018). The “.magnitude” file is a SQLite database file. There are 3 variants of the file format: Light, Medium, Heavy. Heavy models have the largest file size but support all of the Magnitude library’s features. Medium models support all features except approximate similarity search. Light models do not support approximate similarity searches or interpolated OOV lookups, but they still support basic OOV lookups. See Figure 1 for more information about the structure and layout of the “.magnitude” format. Quantization The converter utility accepts a -p <PREC"
D18-2021,N18-1202,0,0.0336746,"configuration options. Other matching nuances We employ other techniques when computing the string similarity metric, such as shrinking repeated character sequences of three or more to two (hiiiiiiii → hii), ranking strings of a similar length higher, and ranking strings that share the same first or last character higher for shorter words. 6 File Format To provide efficiency at runtime, Magnitude uses a custom “.magnitude” file format instead of “.bin”, “.txt”, “.vec”, or “.hdf5” that word2vec, GloVe, fastText, and ELMo use (Mikolov et al., 2013; Pennington et al., 2014; Joulin et al., 2016; Peters et al., 2018). The “.magnitude” file is a SQLite database file. There are 3 variants of the file format: Light, Medium, Heavy. Heavy models have the largest file size but support all of the Magnitude library’s features. Medium models support all features except approximate similarity search. Light models do not support approximate similarity searches or interpolated OOV lookups, but they still support basic OOV lookups. See Figure 1 for more information about the structure and layout of the “.magnitude” format. Quantization The converter utility accepts a -p <PRECISION&gt; flag to specify the decimal precisio"
E06-1032,P04-1079,0,0.00865044,"rominent factors contribute to Bleu’s crudeness: • Synonyms and paraphrases are only handled if they are in the set of multiple reference translations. • The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty. was being led to the |calm as he was | would take |carry him |seemed quite | when |taken would receive the same Bleu score as the hypothesis translation from Table 1, even though human judges would assign it a much lower score. This problem is made worse by the fact that Bleu equally weights all items in the reference sentences (Babych and Hartley, 2004). Therefore omitting content-bearing lexical items does • The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. Each of these failures contributes to an increased amount of inappropriately indistinguishable translations in the analysis presented above. Given that Bleu can theoretically assign equal scoring to translations of obvious different quality, it is logical that a higher Bleu score may not 252 Fluency How do you judge the fluency of this translation? 5 = Flawless English 4 = Good English 3 = Non-native English 2 ="
E06-1032,W05-0909,0,0.108748,"to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical a"
E06-1032,P05-1048,0,0.0875749,"Missing"
E06-1032,2003.mtsummit-papers.6,0,0.0277887,"Missing"
E06-1032,2003.mtsummit-papers.9,0,0.335611,"ric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements. If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is. In this paper we give a number of counterexamples for Bleu’s correlation with human judgments. We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it"
E06-1032,koen-2004-pharaoh,0,0.026394,"and seeing whether Bleu cor0.4 0.42 0.44 0.46 0.48 0.5 0.52 Bleu Score Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training 1 corpus, and selected one that was trained with 64 of the data, which had a Bleu score that was close to, but still higher than that for the rule-based system. We then performed a manual evaluation where we had three judges assign fluency and adequacy ratings for the English translations of 300 French sentences for each of the three systems. The"
E06-1032,2005.mtsummit-papers.11,1,0.0613044,"tput which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu cor0.4 0.42 0.44 0.46 0.48 0.5 0.52 Bleu Score Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems. We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems. We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004). We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems’ output. We then built a number of SMT systems with various portions of the training 1 corpus, and selected one that was trained with 64 of the data, wh"
E06-1032,N03-1020,0,0.24596,"to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation"
E06-1032,N03-2021,0,0.459458,"s to an arithmetic average, and calculating the brevity penalty in a slightly different manner. Hovy and Ravichandra (2003) suggested increasing Bleu’s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu’s n-gram matches. Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization."
E06-1032,P02-1038,0,0.108484,"n that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models’ feature functions (Och and Ney, 2002). This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another. For instance, when measuring correlation using Pearson’s we get a very low correlation of R2 = 0.14 when the outlier in Figure 2 is included, but a strong R2 = 0.87 when it is excluded. Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742. Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph. We investigated"
E06-1032,P03-1021,0,0.33672,"s of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003). However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee gen"
E06-1032,P02-1040,0,0.129827,"We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002). The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003). Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations. Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation. All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation q"
E06-1032,P04-1078,0,0.0207213,"F as a way of placing greater emphasis on content-bearing words and phrases. Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does. Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match. Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization. Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation. Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do. We have shown that Bleu’s rather coarse model of allowable variation in translation can mean that an improved Bleu scor"
E06-1032,N04-1021,0,\N,Missing
E12-1014,D09-1109,0,0.0491141,"(log(n/nk ) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other Spanish phrases. We calculate the temporal similari"
E12-1014,D11-1029,0,0.0205152,"gnments across the two phrases. Because individual words are more frequent than multiword phrases, the accuracy of clex , tlex , and wlex tends to be higher than their phrasal equivalents (this is similar to the effect observed in Figure 2). Orthographic / phonetic similarity. The final lexical similarity feature that we incorporate is o(f, e), which measures the orthographic similarity between words in a phrase pair. Etymologically related words often retain similar spelling across languages with the same writing system, and low string edit distance sometimes signals translation equivalency. Berg-Kirkpatrick and Klein (2011) present methods for learning correspondences between the alphabets of two languages. We can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetically similar. Transliterations can be generated for tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabil"
E12-1014,C88-1016,0,0.0456416,"Missing"
E12-1014,J93-2003,0,0.0204204,"mate the monolingual parameters in Section 3. This approach allows us to replace expensive/rare bilingual parallel training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reorde"
E12-1014,W10-1703,1,0.657033,"ver the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4 Specifcially, news-test2008 plus news-syscomb2009 for dev and newstest2009 for test. • Next, we estimate the features from truly monolingual corpora. To estimate the contextual and temporal similarity features, we use the Spanish and English Gigaword corpora.5 These corpora are substantially larger than the Europarl corpora, providing 27x as much Spanish and 67x as much English for contextual similarity, and 6x as many paired dates for temporal similarity. Topical similarity is estimated using Spanish and English Wikipedia articles that are paired with interlanguage links. To project"
E12-1014,2006.amta-papers.3,0,0.0362064,"Missing"
E12-1014,P08-2040,0,0.0237341,"been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-e"
E12-1014,P05-1033,0,0.0992244,"ining data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the"
E12-1014,P11-2071,0,0.397556,"Missing"
E12-1014,P98-1069,0,0.436554,"e can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning trans"
E12-1014,W09-1117,1,0.878209,"res a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical machine translation. Given the obvious benefits of not having to rely on scarce bilingual parallel training data, it is surprising that bilingual lexicon induction has not been used for SMT before now. There are several open questions"
E12-1014,W01-1409,0,0.0135462,"Missing"
E12-1014,P08-1088,0,0.735932,"ual corpus, and (4) outputting the English words with the highest similarity as the most likely translations. 40 2.2 Bilingual lexicon induction for SMT 0 100 200 300 400 500 Top 1 Top 10 600 Corpus Frequency Figure 2: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists. nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al. (2008)). Although previous work reported high translation accuracy, it may be misleading to extrapolate the results to SMT, where it is necessary to translate a much larger set of words and phrases, including many low frequency items. In a preliminary study, we plotted the accuracy of translations against the frequency of the source words in the monolingual corpus. Figure 2 shows the result for translations induced using contextual similarity (defined in Section 3.1). Unsurprisingly, frequent terms have a substantially better chance of being paired with a correct translation, with words that only oc"
E12-1014,H05-1061,0,0.0096068,"ena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Concl"
E12-1014,P06-1103,1,0.859199,"tor as follows: wk = nf,k × (log(n/nk ) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other Spanish phrases. We calcul"
E12-1014,P97-1017,0,0.0974624,"e), which measures the orthographic similarity between words in a phrase pair. Etymologically related words often retain similar spelling across languages with the same writing system, and low string edit distance sometimes signals translation equivalency. Berg-Kirkpatrick and Klein (2011) present methods for learning correspondences between the alphabets of two languages. We can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetically similar. Transliterations can be generated for tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabilities φ and lexical weighting probabilities w. Our seven similarity scores are not the only ones that could be incorporated into the translation model. Various other similarity scores can be computed depending on the available monolingual data and its associated metadata (see, e.g. Schafer and Yarowsky (2002)). 3.3 Reordering The re"
E12-1014,W02-0902,0,0.891337,"ernatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical machine translation. Given the obvious benefits of not having to rely on scarce bilingual parallel traini"
E12-1014,N03-1017,0,0.0174139,"replace expensive/rare bilingual parallel training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reordering probabilities from the phrasebased models are estimated from bilingu"
E12-1014,J82-2005,0,0.784423,"Missing"
E12-1014,2005.mtsummit-papers.11,0,0.0139923,"re our method against the normal bilingual training procedure. We expect bilingual training to result in higher translation quality because it is a more direct method for learning translation probabilities. We systematically remove different parameters from the standard phrase-based model, and then replace them with our monolingual equivalents. Our goal is to recover as much of the loss as possible for each of the deleted bilingual components. The standard phrase-based model that we use as our top-line is the Moses system (Koehn et al., 2007) trained over the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4 Specifcially, ne"
E12-1014,W11-2132,0,0.0284881,"Missing"
E12-1014,D09-1092,0,0.202334,"normalized topic signatures. In our experiments, we use interlingual links between Wikipedia articles to estimate topic similarity. We treat each linked article pair as a topic and collect counts for each phrase across all articles in its corresponding language. Thus, the size of a phrase topic signature is the number of article pairs with interlingual links in Wikipedia, and each component contains the number of times the phrase appears in (the appropriate side of) the corresponding pair. Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al., 2009), but it is scalable to full bilingual lexicon induction. 3.2 Lexical similarity features In addition to the three phrase similarity features used in our model – c(f, e), t(f, e) and w(f, e) – we include four additional lexical similarity features for each of phrase pair. The first three lexical features clex (f, e), tlex (f, e) and wlex (f, e) are the lexical equivalents of the phrase-level contextual, temporal and wikipedia topic similarity scores. They score the similarity of individual words within the phrases. To compute these lexical similarity features, we average similarity scores over"
E12-1014,P06-1011,0,0.119033,"parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obt"
E12-1014,J03-1002,0,0.00369842,"and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f, e) is orientated with the preceding phrase pair"
E12-1014,J04-4002,0,0.0468358,"he preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse counts, lexical weighting FFs are used"
E12-1014,P03-1021,0,0.00834943,"bilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse counts, lexical weighting FFs are used to smooth. Aver• Other features. Other typical features are n-gram language model scores and a phrase penalty, which governs whether to use fewer longer phrases or more shorter phrases. These are not bilingually estimated, so we can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated"
E12-1014,P95-1050,0,0.908394,"able features. 1 Introduction The parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (Brown et al., 1993). However, these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building a good translation system (Germann, 2001). We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. We then introduce a • In Section 2.2 we analyze the challenges of using bilingual lexicon induction for statistical MT (performance on low frequency items, and moving from words to phrases). • In Sections 3.1 and 3.2 we use multiple cues present in monolingual data to estimate lexical and phrasal translation scores. • In Section 3.3 we propose a novel algorithm for estimating phrase reordering features from monolingual texts. •"
E12-1014,P99-1067,0,0.629355,"be used to replace bilingual estimation of phrase- and lexical-translation probabilities, making a significant step towards SMT without parallel corpora. 3 Monolingual Parameter Estimation We use bilingual lexicon induction methods to estimate the parameters of a phrase-based translation model from monolingual data. Instead of scores estimated from bilingual parallel data, we make use of cues present in monolingual data to provide multiple orthogonal estimates of similarity between a pair of phrases. 3.1 Phrasal similarity features Contextual similarity. We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. More formally, assume that (s1 , s2 , . . . sN ) and (t1 , t2 , . . . tM ) are (arbitrarily indexed) source and target vocabularies, respectively. A source phrase f is represented with an Figure 4: Temporal histograms of the English phrase terrorist, its Spanish translation terrorista, and riqueza (wealth) collected from monolingual texts spanning a 13 year period. While the correct translation has a good temporal match, the non-translation riqueza has a distinctly different signature. N - and target phrase e with an M"
E12-1014,E09-1003,0,0.0187282,"Missing"
E12-1014,P11-1002,0,0.295311,"mated scores capture some novel information not contained in the standard feature set. 6 Additional Related Work Carbonell et al. (2006) described a data-driven MT system that used no parallel text. It produced translation lattices using a bilingual dictionary and scored them using an n-gram language model. Their method has no notion of translation similarity aside from a bilingual dictionary. Similarly, S´anchez-Cartagena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corp"
E12-1014,2011.mtsummit-papers.64,0,0.0247298,"Missing"
E12-1014,W02-2026,1,0.822051,"h minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical mac"
E12-1014,2009.mtsummit-posters.17,0,0.0111132,"ader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover"
E12-1014,2008.iwslt-papers.6,0,0.00839041,"method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize"
E12-1014,N10-1063,0,0.0235881,"a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new language"
E12-1014,N04-4026,0,0.0201637,"asebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f, e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via m"
E12-1014,W03-1001,0,0.0241578,"rientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse counts, lexical we"
E12-1014,C10-1124,0,0.0123608,"and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluat"
E12-1014,P03-1041,0,0.0248361,"phrase pair (f, e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse co"
E12-1014,C98-1066,0,\N,Missing
E12-1014,H05-1021,0,\N,Missing
E12-1014,P07-2045,1,\N,Missing
E12-1014,D08-1076,0,\N,Missing
E14-1021,N10-1033,0,0.0516865,"Missing"
E14-1021,N04-1035,0,0.391652,"es, which may have contain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S VP VP VP VP VP VP S S some people propose to impeach him , while others want him to resign . and want expect some him to step down step d"
E14-1021,2010.iwslt-papers.1,0,0.0175925,"als (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S VP VP VP VP VP VP S S some people propose to impeach him , while others want him to resign . and want expect some him to step down step down to step down want to impeach him want VP want to impeach PRP VBP him to step down PRP to step down while propose want some people him to resign resign to resign propose"
E14-1021,W09-0437,0,0.027626,"Missing"
E14-1021,P06-1121,0,0.052512,"ntain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S VP VP VP VP VP VP S S some people propose to impeach him , while others want him to resign . and want expect some him to step down step down to step down want"
E14-1021,P05-1074,1,0.542729,"s, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraphrase data. In the syntactic setting we allowed labeling of rules with either constituent labels or CCG-style slashed categories. The size of the extracted grammars is shown in Table 2. We also used version 0.2 of the SCFG-based paraphrase collection known as the ParaPhrase DataBase or PPDB (Ganitkevitch et al., 2013). The PPDB paraphrases were extracted using the pivoting technique (Bannard and Callison-Burch, 2005) on bilingual parallel corpora containing over 42 million sentence pairs. The PPDB release includes a tool for pruning the grammar to a smaller size by retaining only high-precision paraphrases. We include PPDB grammars for several different pruning settings in our analysis. 5.2 Experimental setup We calculated our two metrics for each of the grammars listed in Table 2. To perform synchronous parsing, we used the Joshua decoder (Post et al., 2013), which includes an implementation of Dyer’s two-pass parsing algorithm (2010). After splitting the LDC data into 10 equal pieces, we trained paraphr"
E14-1021,D11-1108,1,0.918889,"Missing"
E14-1021,P01-1008,0,0.108705,"lish translations.1 1 LDC Catalog numbers LDC2002T01, LDC2005T05, LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17, and LDC2010T23. 195 Corpus LDC Multiple Translations Classic French Literature MSR Paraphrase Corpus ParaMetric sentence pairs 83,284 75,106 5,801 970 total words 2,254,707 682,978 219,492 21,944 Grammar LDC Hiero Lit. Hiero MSR Hiero ParaMetric Hiero LDC Syntax Lit. Syntax MSR Syntax ParaMetric Syntax PPDB-v0.2-small PPDB-v0.2-large PPDB-v0.2-xl Table 1: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Rules 52,784,462 3,288,546 2,456,513 584,944 23,978,477 715,154 406,115 317,772 1,292,224 9,456,356 46,592,161 Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). Grammar ParaMetric Syntax LDC Hiero Lit. Hiero MSR Hiero LDC Syntax Lit. Syntax MSR Syntax PPDB-v0.2-small PPDB-v0.2-large PPDB-v0.2-xl • The MSR Paraphrase corpus which consists of sentence pai"
E14-1021,C08-1013,1,0.956064,"aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates how well certain models are able to produce synchronous parses of sentence pairs drawn from monolingual parallel corpora. PARADIGM’s different metrics are explained in Section 4, but first we give background on synchronous parsing and synchronous grammars. for paraphrase evaluation (Callison-Burch et al., 2008; Cohn et al., 2008). The first of our two proposed metrics calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in a test set. The second measure enumerates paraphrase rules from a monolingual parallel corpus and calculates the overlap between this reference paraphrase collection, and the paraphrase resource being evaluated. 2 Related work and background The most closely related work is ParaMetric (Callison-Burch et al., 2008), which is a set of objective measures for evaluating the quality of phrase-based paraphrases. ParaMetric extracts a set of gold-s"
E14-1021,W12-3134,1,0.84264,"ility performance is not very different. This implies that paraphrases can be annotated with linguistic information without necessarily hurting their ability to explain particular sentence pairs. Contrast this result, with, for • We created a development set with sentence compressions by selecting 1000 pairs of sentences from the multiple translation corpus where two English translations of the same foreign sentences differed in each other by a length ratio of 0.67–0.75. • We decoded a test set of 1000 sentences using each of the grammars and its optimized 198 weights with the Joshua decoder (Ganitkevitch et al., 2012). The selected in the same fashion as the dev sentences, so each one had a human-created reference compression. BLEU P R E´ CIS P INC PARADIGMprecision PARADIGMrecall PARADIGMavg−len We conducted a human evaluation to judge the meaning and grammaticality of the sentence compressions derived from each paraphrase grammar. We presented workers on Mechanical Turk with the input sentence to the compression sentence (the long sentence), along with 5 shortened outputs from our compression systems. To ensure that workers were producing reliable judgments we also presented them with a positive control"
E14-1021,W10-1703,1,0.850532,"Missing"
E14-1021,N13-1092,1,0.865327,"Missing"
E14-1021,P11-1020,0,0.0528546,"Cartney et al. (2008)). Most methods produce a list of paraphrases for a given input phrase. So CallisonBurch et al. (2008) calculate two more generally applicable measures by comparing the paraphrases in an automatically extracted resource to gold standard paraphrases extracted via the alignments. These allow a lower-bound on precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques 2.1 Synchronous parsing with SCFGs Synchronous context-free grammars An SCFG (Lewis and Stearns, 1968; Aho and Ullman, 1972) is similar to a context-free grammar, except that it generates pairs of strings in correspondence. Each production rule in an SCFG rewrites a non-terminal symbol as a pair of phrases, which may have contain a mix of words and non-terminals symbols. The grammar i"
E14-1021,N12-1023,0,0.0286876,"Missing"
E14-1021,J07-2003,0,0.0544937,"t it generates pairs of strings in correspondence. Each production rule in an SCFG rewrites a non-terminal symbol as a pair of phrases, which may have contain a mix of words and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S"
E14-1021,D11-1125,0,0.0209659,"xamples, even though the absolute number of pairs they can reach is lower. 5.6 Correlation with human judgments • Each paraphrase grammar was augmented with an appropriate set of rule-level features that capture information pertinent to the task. In this case, the paraphrase rules were given two additional features that shows how the number of words and characters changed after applying the rule. • Similarly to how the weights of the models are set using minimum error rate training in statistical machine translation, the weights for each of the paraphrase grammars using the PRO tuning method (Hopkins and May, 2011). Effects of grammar size and choice of syntactic labels • Instead of optimizing to the BLEU metric, as is done in machine translation, we optimized to P R E´ CIS, a metric developed for sentence compression that adapts BLEU so that it includes a “verbosity penalty” (Ganitkevitch et al., 2011) to encourage the compression systems to produce shorter output. Table 2 shows that the PPDB-derived grammars are much larger than the syntactic models derived from other domains. It may seem surprising that they should perform worse, but adding more rules to the grammar just by varying non-terminal label"
E14-1021,D07-1008,0,0.025412,"asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous gramm"
E14-1021,W03-1608,0,0.116335,"Missing"
E14-1021,W13-2226,1,0.859041,"Missing"
E14-1021,N06-1058,0,0.0298029,"ved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase gra"
E14-1021,N03-1017,0,0.0563691,"Missing"
E14-1021,W04-3219,0,0.0473053,"002T01, LDC2005T05, LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17, and LDC2010T23. 195 Corpus LDC Multiple Translations Classic French Literature MSR Paraphrase Corpus ParaMetric sentence pairs 83,284 75,106 5,801 970 total words 2,254,707 682,978 219,492 21,944 Grammar LDC Hiero Lit. Hiero MSR Hiero ParaMetric Hiero LDC Syntax Lit. Syntax MSR Syntax ParaMetric Syntax PPDB-v0.2-small PPDB-v0.2-large PPDB-v0.2-xl Table 1: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008). Rules 52,784,462 3,288,546 2,456,513 584,944 23,978,477 715,154 406,115 317,772 1,292,224 9,456,356 46,592,161 Table 2: Size of various paraphrase grammars. • Classic French Literature that were translated by different translators, and which were compiled by Barzilay and McKeown (2001). Grammar ParaMetric Syntax LDC Hiero Lit. Hiero MSR Hiero LDC Syntax Lit. Syntax MSR Syntax PPDB-v0.2-small PPDB-v0.2-large PPDB-v0.2-xl • The MSR Paraphrase corpus which consists of sentence pairs drawn from comparable news articles"
E14-1021,P06-1096,0,0.0411158,"Missing"
E14-1021,N06-1014,0,0.0369993,"l rules that appeared only once from the ParaMetric grammar. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sources. We use this to extract the gold standard paraphrase grammar. The size of the data from each source is summarized in Table 1. For each dataset, after tokenizing and normalizing, we parsed one sentence in each English pair using the Berkeley constituency parser (Liang et al., 2006b). We then obtained word-level alignments, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraphrase data. In the syntactic setting we allowed labeling of rules with either constituent labels or CCG-style slashed categories. The size of the extracted grammars is shown in Table 2. We also used version 0.2 of the SCFG-based paraphrase collection known as the ParaPhrase DataBase or PPDB (Ganitkevitch et al., 2013). The PPDB paraphrases were"
E14-1021,P07-1058,0,0.0603894,"Missing"
E14-1021,W11-2160,1,0.908921,"aluated using subjective manual evaluation or through task-based evaluations. 192 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaMetric’s word-aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this case, PARADIGM evaluates how well certain models are able to produce synchronous parses of sentence pairs drawn from monolingual parallel corpora. PARADIGM’s different metrics are explained in Section 4, but first we give background on"
E14-1021,D10-1090,0,0.0185545,"gnments were used to calculate how well an automatic paraphrasing technique is able to align the paraphrases in a sentence pair. This measure is limited to a class of paraphrasing techniques that perform alignment (like MacCartney et al. (2008)). Most methods produce a list of paraphrases for a given input phrase. So CallisonBurch et al. (2008) calculate two more generally applicable measures by comparing the paraphrases in an automatically extracted resource to gold standard paraphrases extracted via the alignments. These allow a lower-bound on precision and relative recall to be calculated. Liu et al. (2010) introduce the PEM metric as an alternative to BLEU, since BLEU prefers identical paraphrases. PEM uses a second language as a pivot to judge semantic equivalence. This requires use of some bilingual data. Chen and Dolan (2011) suggest using BLEU together with their metric PINC, which uses n-grams to measure lexical difference between paraphrases. PARADIGM extends the ideas in ParaMetric from lexical and phrasal paraphrasing techniques 2.1 Synchronous parsing with SCFGs Synchronous context-free grammars An SCFG (Lewis and Stearns, 1968; Aho and Ullman, 1972) is similar to a context-free gramma"
E14-1021,D08-1084,0,0.397288,"Missing"
E14-1021,W12-3127,1,0.845441,"e in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S VP VP VP VP VP VP S S some people propose to impeach him , while others want him to resign . and want expect some him to step down step down to step down want to impeach him want VP want to impeach PRP VBP him to step down PRP to step down while propose want some people him to resign resign to resign propose to impeach him propo"
E14-1021,J97-3002,0,0.521349,"non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S VP VP VP VP VP VP S S some people propose to impeach him , while others want him to resign . and want expect some him to step down step down to step down want to impeach him want VP want to impeach PRP VBP him to step down PRP to step down while propose want some people him to resign resign to resign propose to impeach him propose VP propose to impeach PRP VB"
E14-1021,J10-3003,0,0.021639,"these evaluation metrics on paraphrase collections derived from three different data types: multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality. 1 Chris Callison-Burch University of Pennsylvania Introduction Paraphrases are useful in a wide range of natural language processing applications. A variety of data-driven approaches have been proposed to generate paraphrase resources (see Madnani and Dorr (2010) for a survey of these methods). Few objective metrics have been established to evaluate these resources. Instead, paraphrases are typically evaluated using subjective manual evaluation or through task-based evaluations. 192 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011)"
E14-1021,P08-1089,0,0.0211726,"proaches have been proposed to generate paraphrase resources (see Madnani and Dorr (2010) for a survey of these methods). Few objective metrics have been established to evaluate these resources. Instead, paraphrases are typically evaluated using subjective manual evaluation or through task-based evaluations. 192 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192–201, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics to paraphrasing techniques that also generate syntactic templates, such as Zhao et al. (2008), Cohn and Lapata (2009), Madnani (2010) and Ganitkevitch et al. (2011). Instead of extracting gold standard paraphrases using techniques from phrasebased machine translation, we use grammar extraction techniques (Weese et al., 2011) to extract gold standard paraphrase grammar rules from ParaMetric’s word-aligned sentential paraphrases. Using these rules, we calculate the overlap between a gold standard paraphrase grammar and an automatically generated paraphrase grammar. Moreover, like ParaMetric, PARADIGM is able to do further analysis on a restricted class of paraphrasing models. In this ca"
E14-1021,P09-1094,0,0.020857,"s whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchron"
E14-1021,N06-1057,0,0.0260435,"matical.” The results of these subjective evaluations are not easily reusable. Other researchers have evaluated their paraphrases through task-based evaluations. Lin and Pantel (2001) measured their potential impact on question-answering. Cohn and Lapata (2007) evaluate their applicability in the text-to-text generation task of sentence compression. Zhao et al. (2009) use them to perform sentence compression and simplification and to compute sentence similarity. Several researchers have demonstrated that paraphrases can improve machine translation evaluation (c.f. Kauchak and Barzilay (2006), Zhou et al. (2006), Madnani (2010) and Snover et al. (2010)). We introduce an automatic evaluation metric called PARADIGM, PARAphrase DIagnostics through Grammar Matching. This metric evaluates paraphrase collections that are represented using synchronous grammars. Synchronous treeadjoining grammars (STAGs), synchronous tree substitution grammars (STSGs), and synchronous context free grammars (SCFGs) are popular formalisms for representing paraphrase rules (Dras, 1997; Cohn and Lapata, 2007; Madnani, 2010; Ganitkevitch et al., 2011). We present two measures that evaluate these paraphrase grammars using gold sta"
E14-1021,W11-1611,1,0.913019,"Missing"
E14-1021,W06-3119,0,0.299878,"and non-terminals symbols. The grammar is synchronous because both phrases in the pair must have an identical set of non-terminals (though they can come in different orders), and corresponding non-terminals must be rewritten using the same rule. Much recent work in MT (and, by extension, paraphrasing approaches that use MT machinery) has been focused on choosing an appropriate set of non-terminal symbols. The Hiero model (Chiang, 2007) used a single non-terminal symbol X. Other approaches have read symbols from constituent parses of the training data (Galley et al., 2004; Galley et al., 2006; Zollmann and Venugopal, 2006). Labels based combinatory categorial grammar (Steedman and Baldridge, 2011) have also been used (Almaghout et al., 2010; Weese et al., 2012). Synchronous parsing Wu (1997) introduced a parsing algorithm using a variant of CKY. Dyer recently showed (2010) 193 S S VP VP S S VP VP VP NP NP VP some DT want VBP to impeach VB him PRP and CC others NNS expect VBP him PRP to step VB down PRT . . NP CC VBP VBP DT S VP VP VP VP VP VP S S some people propose to impeach him , while others want him to resign . and want expect some him to step down step down to step down want to impeach him want VP want to"
E14-1021,P00-1056,0,0.201571,"ses shows the percentage of ParaMetric rules that are present in the overlap. • The ParaMetric data which consists of 900 manually word-aligned sentence pairs collected by Cohn et al. (2008). 300 sentence pairs were drawn from each of the 3 above sources. We use this to extract the gold standard paraphrase grammar. The size of the data from each source is summarized in Table 1. For each dataset, after tokenizing and normalizing, we parsed one sentence in each English pair using the Berkeley constituency parser (Liang et al., 2006b). We then obtained word-level alignments, either using GIZA++ (Och and Ney, 2000) or, in the case of ParaMetric, using human annotations. We used the Thrax grammar extractor (Weese et al., 2011) to extract Hiero-style and syntactic SCFGs from the paraphrase data. In the syntactic setting we allowed labeling of rules with either constituent labels or CCG-style slashed categories. The size of the extracted grammars is shown in Table 2. We also used version 0.2 of the SCFG-based paraphrase collection known as the ParaPhrase DataBase or PPDB (Ganitkevitch et al., 2013). The PPDB paraphrases were extracted using the pivoting technique (Bannard and Callison-Burch, 2005) on bilin"
E14-1021,C04-1051,0,\N,Missing
E14-1021,P97-1070,0,\N,Missing
E14-1021,J08-4005,1,\N,Missing
E17-2016,N09-1003,0,0.0657359,"Missing"
E17-2016,P14-2134,0,0.0404042,"Missing"
E17-2016,P12-1015,0,0.167199,"Missing"
E17-2016,D10-1124,0,0.128412,"Missing"
E17-2016,D12-1039,0,0.0495714,"Missing"
E17-2016,C12-1064,0,0.0188054,"tagged with geo1 Under our G EO 30 word embeddings, the word love is closer to the context GooglePlaces:bar than to highway:residential. The relationship is inverted for the word hate. 99 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 99–104, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tokens in these tweets were normalized by converting to lowercase, replacing @-mentions, numbers, and URLs with special symbols, and applying the lexical normalization dictionary of Han et al. (2012a). To enrich our collected tweets with geospatial features, we used publicly-available geospatial data from OpenStreetMap and the Google Places API. OpenStreetMap (OSM) is a crowdsourced mapping initiative. Users provide surveyed data such as administrative boundaries, land use, and road networks in their local area. In addition to geographic coordinates, each shape in the data set includes tags describing its type and attributes, such as shop:convenience and building:retail for a convenience store. We downloaded metro extracts for our 20 cities in shapefile format. To maximize coverage, we s"
E17-2016,Q14-1007,1,0.794804,"tweets and re-train G EO 30, T EXT 5, and G EO 30+T EXT 5 vectors on the larger set. Table 3: Most similar contexts, based on cosine similarity of the associated G EO 30 context vectors. below the current state-of-the-art; this is to be expected given the relatively small size of our training corpus (approx. 400M tokens). 5 Geospatial context Similar to Irvine and Callison-Burch (2013), we use a supervised method to make a binary translation prediction for Turkish-English word pairs. We build a dataset of positive Turkish-English word pairs by all Turkish words in a TurkishEnglish dictionary (Pavlick et al., 2014) that appear in our vector vocabulary and do not translate to the same word in English (528 words in total). We add these words and their translations to our dataset as positive examples. Then, for each TurkTranslation Prediction Our intrinsic evaluation established that geospatial context provides semantic information about words, but it is weaker than information provided by textual context. So a natural question to ask is whether geospatial context can be useful in any setting. One potential strength of word embeddings trained using geospatial contexts is that the features are language-inde"
E17-2016,P13-4002,0,0.0304364,"Missing"
E17-2016,N15-1058,0,0.0341769,"Missing"
E17-2016,J15-4004,0,0.0504006,"Missing"
E17-2016,P11-1096,0,0.0606608,"Missing"
E17-2016,P12-1092,0,0.132142,"Missing"
E17-2016,N13-1056,1,0.792363,"one another in vector space. This type of model could be applicable in a low-resource language setting where large parallel texts are unavailable but geolocated text is. To test this hypothesis, we collect an additional 236k geolocated Turkish tweets and re-train G EO 30, T EXT 5, and G EO 30+T EXT 5 vectors on the larger set. Table 3: Most similar contexts, based on cosine similarity of the associated G EO 30 context vectors. below the current state-of-the-art; this is to be expected given the relatively small size of our training corpus (approx. 400M tokens). 5 Geospatial context Similar to Irvine and Callison-Burch (2013), we use a supervised method to make a binary translation prediction for Turkish-English word pairs. We build a dataset of positive Turkish-English word pairs by all Turkish words in a TurkishEnglish dictionary (Pavlick et al., 2014) that appear in our vector vocabulary and do not translate to the same word in English (528 words in total). We add these words and their translations to our dataset as positive examples. Then, for each TurkTranslation Prediction Our intrinsic evaluation established that geospatial context provides semantic information about words, but it is weaker than information"
E17-2016,P14-2050,0,0.346626,"absolute, physical location (like a geographic bounding box), we model language with respect to attributes describing a type of location (like amenity:movie theater or landuse:residential). This allows us to model the impact of geospatial context independently of language and region. We enrich a corpus of geolocated tweets with geospatial information describing the physical environment where they were posted. We use the geospatial contexts to train geo-word embeddings with the skip-gram with negative sampling (S KIP G RAM) model (Mikolov et al., 2013) as adapted to support arbitrary contexts (Levy and Goldberg, 2014). We then demonstrate the semantic value of geospatial context in two ways. First, using intrinsic methods of evaluation, we show that the resulting geo-word embeddings themselves encode information about semantic relatedness. Second, we present initial results suggesting that because the embeddings are trained with language-agnostic features, they give a potentially useful signal about bilingual translation pairs. There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts."
ganitkevitch-callison-burch-2014-multilingual,W10-1751,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,steinberger-etal-2006-jrc,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W04-3219,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,S12-1034,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W11-2504,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,C04-1051,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,J13-3001,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,C02-1056,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,J10-3003,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,D11-1009,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P07-1059,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,D08-1076,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,D08-1021,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W07-0716,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,N06-1057,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W13-2515,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P99-1071,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P08-1077,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,N03-1003,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,D11-1108,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P06-1055,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P05-1074,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,N03-1017,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P79-1016,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W11-1610,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P11-1020,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P08-1116,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W06-3119,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P13-1135,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,2005.mtsummit-papers.11,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,P14-1133,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,D13-1008,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W13-2226,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,N13-1092,1,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,J07-2003,0,\N,Missing
ganitkevitch-callison-burch-2014-multilingual,W11-2160,1,\N,Missing
irvine-etal-2014-american,P06-1043,0,\N,Missing
irvine-etal-2014-american,D10-1124,0,\N,Missing
irvine-etal-2014-american,P08-1076,0,\N,Missing
irvine-etal-2014-american,N13-1092,1,\N,Missing
irvine-etal-2014-american,D09-1079,0,\N,Missing
J08-4005,P06-1002,0,0.0102134,"the alignment or have an unaligned word on a boundary, respectively, indicated by a cross. where Ap and B p are the predicted and reference phrase pairs, respectively, and p the atom subscript denotes the subset of atomic phrase pairs, Aatom ⊆ Ap . As shown in Equation (3) we measure precision and recall between atomic phrase pairs and the full space of atomic and composite phrase pairs. This ensures that we do not multiply reward composite phrase pair combinations,11 while also not unduly penalizing non-matching phrase pairs which are composed of atomic phrase pairs in 11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore might multiply count phrase pairs. 604 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Table 2 Phrase pairs are speciﬁed by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs. ∗ This phrase pair is atomic in A but composite in B. Atomic phrase pairs they they discussed the aspects in detail in de"
J08-4005,P05-1074,1,0.755451,"marization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (ﬁghting, battle) or structural (last week’s ﬁghting, the battle last week), and are represented as words or ∗ School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ∗∗ Center for Speech and Language Processing, Johns Hopkin"
J08-4005,W03-1004,0,0.109887,"). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (ﬁghting, battle) or structural (last week’s ﬁghting, the battle last week), and are represented as words or ∗ School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ∗∗ Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218. E-mail: ccb@cs.jhu.edu. † School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission recei"
J08-4005,N03-1003,0,0.458229,"inburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for publication: 26 March 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 syntax trees—they all rely on some form of alignment for extracting paraphrase pairs. In its simplest form, the alignment can range over individual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a"
J08-4005,J93-2003,0,0.00937007,"Missing"
J08-4005,N06-1003,1,0.386521,"Missing"
J08-4005,D07-1008,1,0.626972,"Missing"
J08-4005,W06-1628,0,0.0184492,"Missing"
J08-4005,W04-3216,0,0.019874,"Missing"
J08-4005,C04-1051,0,0.725955,"Missing"
J08-4005,N06-2009,0,0.0282029,"Missing"
J08-4005,J07-3002,0,0.072039,"reference alignment B can be then computed using standard recall, precision, and F1 measures (Och and Ney 2003): Precision = |AS ∩ BP | |AS | Recall = |AP ∩ BS | |BS | F1 = 2 · Precision · Recall Precision + Recall (2) where the subscripts S and P denote sure and possible word alignments, respectively. Note that both precision and recall are asymmetric in that they compare sets of possible and sure alignments. This is designed to be maximally generous: sure predictions which are present in the reference as possibles are not penalized in precision, and the converse applies for recall. We adopt Fraser and Marcu (2007)’s deﬁnition of F1, an F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufﬁciently penalize unbalanced precision and recall.9 As our corpus is monolingual, in order to avoid artiﬁcial score inﬂation, we limit the precision and recall calculations to consider only pairs of non-identical words (and phrases, as discussed subsequently). To give an example, consider the sentence pairs in Figure 2, whose alignments have been produced by the two annotators A (left) and B (rig"
J08-4005,N03-1017,0,0.0310332,"Missing"
J08-4005,W05-0809,0,0.020932,"Missing"
J08-4005,W03-0301,0,0.0235874,"or instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In the following section we expl"
J08-4005,C00-2163,0,0.194676,"ons are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In"
J08-4005,P00-1056,0,0.595666,"ons are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In"
J08-4005,J03-1002,0,0.0916633,"were split into two distinct sets, each consisting of 300 sentences (100 per corpus), and were annotated by a single coder. Each coder annotated the same amount of data. In addition, we obtained a trial set of 50 sentences from the MTC corpus which was used for familiarizing our annotators with the paraphrase alignment task (this set does not form part of the corpus). In sum, we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly annotated. To speed up the annotation process, the data sources were ﬁrst aligned automatically and then hand-corrected. We used Giza++ (Och and Ney 2003), a publicly available 2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. 3 The corpus can be downloaded from http://www.isi.edu/∼knight/. 4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D920CD-47E3-85BC-A2F65CD28042/Details.aspx. 599 Computational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair−1) training ings of English translations as training instances. Thi"
J08-4005,W99-0604,0,0.180416,"Missing"
J08-4005,N03-1024,0,0.192731,"Missing"
J08-4005,W04-3219,0,0.44813,"Missing"
J08-4005,N06-1057,0,0.0232382,"rd alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBur"
J08-4005,P01-1008,0,\N,Missing
J08-4005,J08-4004,0,\N,Missing
J12-2006,P98-1013,0,0.198951,"Missing"
J12-2006,2010.amta-papers.7,1,0.863358,"s produced by the taggers described here. The resulting system signiﬁcantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu–English test set. This ﬁnding supports the hypothesis that both syntactic and semantic information can improve translation quality. 1. Introduction This article describes the resource- and system-building efforts of an 8-week Johns Hopkins Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT) (Baker et al. 2010a, 2010b, 2010c, 2010d). Speciﬁcally, we describe our modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that were built using the lexicon and annotation scheme. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). Two examples of MN tagging are shown in Figure 1. Note that modality and negation are uniﬁed into single MN tags (e.g., the “Able” modality tag is combin"
J12-2006,baker-etal-2010-modality,1,0.821878,"s produced by the taggers described here. The resulting system signiﬁcantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu–English test set. This ﬁnding supports the hypothesis that both syntactic and semantic information can improve translation quality. 1. Introduction This article describes the resource- and system-building efforts of an 8-week Johns Hopkins Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT) (Baker et al. 2010a, 2010b, 2010c, 2010d). Speciﬁcally, we describe our modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that were built using the lexicon and annotation scheme. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). Two examples of MN tagging are shown in Figure 1. Note that modality and negation are uniﬁed into single MN tags (e.g., the “Able” modality tag is combin"
J12-2006,P05-1033,0,0.0907455,"-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process by adding semantic notations onto the syntactic non-terminals produced by a Penn Treebank trained parser, thus making the categories more informative. In the parsing domain, the work of Petrov and Klein (2007) is related to the current work. In their work, rule splitting and rule merging are applied to reﬁne parse trees during machine learning. Hierarchical splitting leads to the creation of learned categor"
J12-2006,W09-3012,1,0.662316,"ators identify explicit certainty markers and also take into account Perspective, Focus, and Time. Focus separates certainty into facts and opinions, to include attitudes. In our scheme, Focus would be covered by want and belief modality. Also, separating focus and uncertainty can allow the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010) describe a scheme for automatic committed belief tagging. Committed belief indicates the writer believes the proposition. The authors use a previously annotated corpus of committed belief, non-committed belief, and not applicable (Diab et al. 2009), and derive features for machine learning from parse trees. The authors desire to combine their work with FactBank annotation. The CoNLL-2010 shared task (Farkas et al. 2010) was about the detection of cues for uncertainty and their scope. The task was described as “hedge detection,” that is, ﬁnding statements which do not or cannot be backed up with facts. Auxiliary verbs such as may, might, can, and so forth, are one type of hedge cue. The training data for the shared task included the BioScope corpus (Szarvas et al. 2008), which is manually annotated with negation and speculation cues and"
J12-2006,W10-3001,0,0.0447283,"Missing"
J12-2006,N06-1031,0,0.116492,"one manually by students who were provided a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities, modality, and negation automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added beneﬁt of the extracted translation rules is that they are capable of producing semantically tagged Urdu parses, despite the fact that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes that of Huang and Knight (2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making"
J12-2006,P03-1054,0,0.00282903,"ed by only an English parser and tagger. Related work in syntax-based MT includes that of Huang and Knight (2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic p"
J12-2006,W04-3250,0,0.0152888,"dually, each of these made modest improvements over the syntactically informed system alone. Grafting named entities onto the parse trees improved the Bleu score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simultaneously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax alone. This improvement was the largest improvement that we got from anything other than the move from linguistically naive models to syntactically informed models. We used bootstrap resampling to test whether the differences in Bleu scores were statistically signiﬁcant (Koehn 2004). All of the results were a signiﬁcant improvement over Hiero (at p ≤ 0.01). The difference between the syntactic system and the syntactic system with named entities is not signiﬁcant (p = 0.38). The differences between the 5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu words) and four reference translations per sentence. The BLEU score for these experiments is measured on uncased output. 432 Baker et al. Modality and Negation in SIMT syntactic system and the syntactic system with MN, and between the syntactic system and the syntactic system w"
J12-2006,P07-2045,1,0.0084265,"s in this article including the training, development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a split of the NIST08 Urdu–English test set, and the blind test set was NIST09. Urdu set training dev devtest test English lines tokens types tokens types 202k 981 883 1,792 1.7M 21k 22k 42k 56k 4k 4k 6k 1.7M 19k 19–20k 38–41k 51k 4k 4k 5k Table 1) results in signiﬁcantly degraded translation quality compared, for example, to an Arabic–English system that has more than 100 times the amount of training data. The output in Figure 2 was produced using Moses (Koehn et al. 2007), a state-ofthe-art phrase-based MT system that by default does not incorporate any linguistic information (e.g., syntax or morphology or transliteration knowledge). As a result, words that were not directly observed in the bilingual training data were untranslatable. Names, in particular, are problematic. For example, the lack of translation for Nagaland and Nagas induces multiple omissions throughout the translated text, thus producing several instances where the holder of a claim (or belief ) is missing. This is because out-ofvocabulary words are deleted from the Moses output. We use syntac"
J12-2006,W09-0424,1,0.73648,"elements are subsequently used in the translation rules that are extracted from the parallel corpus. The goal of adding them to the translation rules is to constrain the space of possible translations to more grammatical and more semantically coherent output. We explored whether including such semantic elements could improve translation output in the face of sparse training data and few source language annotations. Results were encouraging. Translation quality, as measured by the Bleu metric (Papineni et al. 2002), improved when the training process for the Joshua machine translation system (Li et al. 2009) used in the SCALE workshop included MN annotation. We were particularly interested in identifying modalities and negation because they can be used to characterize events in a variety of automated analytic processes. Modalities and negation can distinguish realized events from unrealized events, beliefs from certainties, and can distinguish positive and negative instances of entities and events. For example, the correct identiﬁcation and retention of negation in a particular language—such as a single instance of the word “not”—is very important for a correct representation of events and likewi"
J12-2006,J93-2004,0,0.0409281,"Missing"
J12-2006,A00-2030,1,0.384287,"Missing"
J12-2006,Y05-1014,0,0.102007,"rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and Karttunen 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modiﬁers. 415 Computational Linguistics Volume 38, Number 2 Earlier work describing the difﬁculty of correctly translating modality using ma´ chine translation includes Sigurd and Gawronska (1994) and Murata et al. (2005). Sigurd ´ and Gawronska (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to speciﬁcally handle modal constructions, whereas our modal annotation approach is a part of a full translation system. We now consider other literature, relating to tree-grafting and machine tra"
J12-2006,W06-3907,0,0.0592182,"Missing"
J12-2006,J05-1004,0,0.125993,"Missing"
J12-2006,P02-1040,0,0.088684,"ed to the source language (in our case, Urdu) during a process of syntactic alignment. These semantic elements are subsequently used in the translation rules that are extracted from the parallel corpus. The goal of adding them to the translation rules is to constrain the space of possible translations to more grammatical and more semantically coherent output. We explored whether including such semantic elements could improve translation output in the face of sparse training data and few source language annotations. Results were encouraging. Translation quality, as measured by the Bleu metric (Papineni et al. 2002), improved when the training process for the Joshua machine translation system (Li et al. 2009) used in the SCALE workshop included MN annotation. We were particularly interested in identifying modalities and negation because they can be used to characterize events in a variety of automated analytic processes. Modalities and negation can distinguish realized events from unrealized events, beliefs from certainties, and can distinguish positive and negative instances of entities and events. For example, the correct identiﬁcation and retention of negation in a particular language—such as a single"
J12-2006,C10-2117,0,0.0335943,"Missing"
J12-2006,prasad-etal-2008-penn,0,0.0139365,"al predicates. ¨ The Prague Dependency Treebank (Hajiˇc et al. 2001; Bohmov´ a, Cinkov´a, and Hajiˇcov´a 2005) (PDT) is a multi-level system of annotation for texts in Czech and other languages, with its roots in the Prague school of linguistics. Besides a morphological layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical layer includes functional relationships, dependency relations, and co-reference. The PDT also integrates propositional and extra-propositional meanings in a single annotation framework. The Penn Discourse Treebank (PDTB) (Webber et al. 2003; Prasad et al. 2008) annotates discourse connectives and their arguments over a portion of the Penn Treebank. Within this framework, senses are annotated for the discourse connectives in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is the Conditional tag, which includes hypothetical, general, unreal present, unreal past, factual present, and factual past arguments. The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for establishing the importance of attributing a belief or assertion expressed in text to its agent (equivalent to the notion of holder in our sc"
J12-2006,P08-1001,0,0.123143,"he rule extraction algorithm with augmented parse trees containing syntactic labels that have semantic annotations grafted onto them so that they additionally express semantic information. Our strategy for producing semantically grafted parse trees involves three steps: 1. The English sentences in the parallel training data are parsed with a syntactic parser. In our work, we used the lexicalized probabilistic context free grammar parser provided by Basis Technology Corporation. 2. The English sentences are MN-tagged by the system described herein and named-entity-tagged by the Phoenix tagger (Richman and Schone 2008). 3. The modality/negation and entity markers are grafted onto the syntactic parse trees using a tree-grafting procedure. The grafting procedure was implemented as part of the SIMT effort. Details are further spelled out in Section 7.2. Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that although we focus the discussion here on the modality and negation, our framework is general and we were able to incorporate other semantic elements (speciﬁcally, named entities) into the SIMT effort. Once the semantically grafted trees have been produced for the parallel corpus, t"
J12-2006,N07-2036,0,0.0114641,"tion is asserted to be real or not real). A major annotation effort for temporal and event expressions is the TimeML speciﬁcation language, which has been developed in the context of reasoning for question answering (Saur´ı, Verhagen, and Pustejovsky 2006). TimeML, which includes modality annotation on events, is the basis for creating the TimeBank and FactBank corpora (Pustejovsky et al. 2006; Saur´ı and Pustejovsky 2009). In FactBank, event mentions are marked with their degree of factuality. Recent work incorporating modality annotation includes work on detecting certainty and uncertainty. Rubin (2007) describes a scheme for ﬁve levels of certainty, referred to as Epistemic modality, in news texts. Annotators identify explicit certainty markers and also take into account Perspective, Focus, and Time. Focus separates certainty into facts and opinions, to include attitudes. In our scheme, Focus would be covered by want and belief modality. Also, separating focus and uncertainty can allow the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010) describe a scheme for automatic committed belief tagging. Committed belief indicates the writer believes the proposition. The a"
J12-2006,C94-1018,0,0.350259,"l. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and Karttunen 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modiﬁers. 415 Computational Linguistics Volume 38, Number 2 Earlier work describing the difﬁculty of correctly translating modality using ma´ chine translation includes Sigurd and Gawronska (1994) and Murata et al. (2005). Sigurd ´ and Gawronska (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to speciﬁcally handle modal constructions, whereas our modal annotation approach is a part of a full translation system. We now consider other literature, relating to tree"
J12-2006,P99-1039,0,0.0211303,"e 10 illustrates how modality tags are grafted onto a parse tree. Note that although we focus the discussion here on the modality and negation, our framework is general and we were able to incorporate other semantic elements (speciﬁcally, named entities) into the SIMT effort. Once the semantically grafted trees have been produced for the parallel corpus, the trees are presented, along with word alignments (produced by the Berkeley aligner), to the rule extraction software to extract synchronous grammar rules that are both 1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999). 428 Baker et al. Modality and Negation in SIMT syntactically and semantically informed. These grammar rules are used by the decoder to produce translations. In our experiments, we used the Joshua decoder (Li et al. 2009), the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special purpose-built tree-grafting software. Figure 11 shows example semantic rules that are used by the decoder. The verb phrase rules are augmented with modality and negation, taken from the semantic categories listed in Table 2. Because these get marked on the Urdu source as well as the English tran"
J12-2006,W08-0606,0,0.0640498,"d corpus of committed belief, non-committed belief, and not applicable (Diab et al. 2009), and derive features for machine learning from parse trees. The authors desire to combine their work with FactBank annotation. The CoNLL-2010 shared task (Farkas et al. 2010) was about the detection of cues for uncertainty and their scope. The task was described as “hedge detection,” that is, ﬁnding statements which do not or cannot be backed up with facts. Auxiliary verbs such as may, might, can, and so forth, are one type of hedge cue. The training data for the shared task included the BioScope corpus (Szarvas et al. 2008), which is manually annotated with negation and speculation cues and their scope, and paragraphs from Wikipedia possibly containing hedge information. Our scheme also identiﬁes cues in the form of triggers, but our desired outcome is to cover the full range of modalities and not just certainty and uncertainty. To identify scope, we use syntactic parse trees, as was allowed in the CoNLL task. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based r"
J12-2006,N07-1063,0,0.0476172,"Missing"
J12-2006,J10-2004,0,0.0138227,"is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process"
J12-2006,J09-3003,0,0.010836,"Missing"
J12-2006,W06-3119,0,0.00897083,"xternal information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process by adding semantic notations onto the syntactic non-terminals produced by a Penn Treebank trained parser, thus making the categories more informative. In the pa"
J12-2006,W08-2225,0,\N,Missing
J12-2006,J03-4002,0,\N,Missing
J12-2006,C98-1013,0,\N,Missing
J14-1006,W09-0807,0,0.210683,"Missing"
J14-1006,W10-0701,1,0.548435,"Missing"
J14-1006,E06-1047,0,0.0112994,"Missing"
J14-1006,P09-1080,0,0.0135625,"nition system (Novotney, Schwartz, and Khudanpur 2011). Identifying dialectal content can also aid in creating parallel data sets for machine translation, with a dialectal source side. A user might be interested in content of a specific dialect, or, conversely, in strictly non-dialectal content. This would be particularly relevant in fine-tuning and personalizing search engine results, and could allow for better user-targeted advertising. In the same vein, being able to recognize dialectal content in user-generated text could aid in characterizing communicants and their biographic attributes (Garera and Yarowsky 2009). Zaidan and Callison-Burch r Arabic Dialect Identification In the context of an application such as machine translation (MT), identifying dialectal content could be quite helpful. Most MT systems, when faced with OOV words, either discard the words or make an effort to transliterate them. If a segment is identified as being dialectal first, the MT system might instead attempt to find equivalent MSA words, which are presumably easier to process correctly (e.g., as in Salloum and Habash [2011] and, to some degree, Habash [2008]). Even for non-OOV words, identifying dialectal content before tran"
J14-1006,P08-2015,0,0.0112307,"Missing"
J14-1006,habash-etal-2012-conventional,0,0.187958,"Missing"
J14-1006,W12-2301,0,0.084875,"Missing"
J14-1006,P06-1086,0,0.0425338,"d the collected labels to train and evaluate automatic classifiers for dialect identification, and observed interesting linguistic aspects about the task and annotators’ behavior. Using an approach based on language model scoring, we develop classifiers that significantly outperform baselines that use large amounts of MSA data, and we approach the accuracy rates exhibited by human annotators. In addition to n-gram features, one could imagine benefiting from morphological features of the Arabic text, by incorporating analyses given by automatic analyzers such as BAMA (Buckwalter 2004), MAGEAD (Habash and Rambow 2006), ADAM (Salloum and Habash 2011), or CALIMA (Habash, Eskander, and Hawwari 2012). Although the difference between our presented approach and human annotators was found to be relatively small, incorporating additional linguistically motivated features might be pivotal in bridging that final gap. In future annotation efforts, we hope to solicit more detailed labels about dialectal content, such as specific annotation for why a certain sentence is dialectal and not MSA: Is it due to structural differences, dialectal terms, and so forth? We also hope to expand beyond the three dialects discussed i"
J14-1006,W11-2602,0,0.0164269,"and evaluate automatic classifiers for dialect identification, and observed interesting linguistic aspects about the task and annotators’ behavior. Using an approach based on language model scoring, we develop classifiers that significantly outperform baselines that use large amounts of MSA data, and we approach the accuracy rates exhibited by human annotators. In addition to n-gram features, one could imagine benefiting from morphological features of the Arabic text, by incorporating analyses given by automatic analyzers such as BAMA (Buckwalter 2004), MAGEAD (Habash and Rambow 2006), ADAM (Salloum and Habash 2011), or CALIMA (Habash, Eskander, and Hawwari 2012). Although the difference between our presented approach and human annotators was found to be relatively small, incorporating additional linguistically motivated features might be pivotal in bridging that final gap. In future annotation efforts, we hope to solicit more detailed labels about dialectal content, such as specific annotation for why a certain sentence is dialectal and not MSA: Is it due to structural differences, dialectal terms, and so forth? We also hope to expand beyond the three dialects discussed in this article, by including sou"
J14-1006,N07-1033,1,0.841657,"Missing"
J14-1006,N12-1006,1,\N,Missing
J14-1006,D08-1004,1,\N,Missing
J14-1006,P11-2007,1,\N,Missing
J17-2001,W09-3109,0,0.0222771,"gs. In Klementiev et al. (2012), we described a framework for estimating the parameters of machine translation without bilingual parallel corpora. Many of the monolingually estimated features that we used in that framework are the same as the features used here for bilingual lexicon induction. In that work, we performed oracle experiments where the translations were given by an existing phrase-table, and simply re-scored using the monolingually estimated signals of translation equivalence. 7.4 Extracting Parallel Data from Comparable Corpora Resnik and Smith (2003), Munteanu and Marcu (2005), Abdul-Rauf and Schwenk (2009a), Abdul-Rauf and Schwenk (2009b), and Smith, Quirk, and Toutanova (2010) identify parallel sentences in comparable corpora. Munteanu and Marcu (2006) identify parallel sub-sentential fragments using a probabilistic lexicon and information retrieval methods to identify similar document pairs and then use the same word translation probabilities to detect parallel fragments within the document pairs. They supplement existing parallel data with the new sentence and fragment pairs and evaluate endto-end SMT systems trained on the augmented parallel datasets. Quirk, Udupa, and Menezes (2007) also"
J17-2001,E09-1003,0,0.0223514,"gs. In Klementiev et al. (2012), we described a framework for estimating the parameters of machine translation without bilingual parallel corpora. Many of the monolingually estimated features that we used in that framework are the same as the features used here for bilingual lexicon induction. In that work, we performed oracle experiments where the translations were given by an existing phrase-table, and simply re-scored using the monolingually estimated signals of translation equivalence. 7.4 Extracting Parallel Data from Comparable Corpora Resnik and Smith (2003), Munteanu and Marcu (2005), Abdul-Rauf and Schwenk (2009a), Abdul-Rauf and Schwenk (2009b), and Smith, Quirk, and Toutanova (2010) identify parallel sentences in comparable corpora. Munteanu and Marcu (2006) identify parallel sub-sentential fragments using a probabilistic lexicon and information retrieval methods to identify similar document pairs and then use the same word translation probabilities to detect parallel fragments within the document pairs. They supplement existing parallel data with the new sentence and fragment pairs and evaluate endto-end SMT systems trained on the augmented parallel datasets. Quirk, Udupa, and Menezes (2007) also"
J17-2001,D09-1109,0,0.069531,"Missing"
J17-2001,D11-1029,0,0.0149008,"lajokull otunbajewa eruption cloud rubell dormancy dzv spatz centimes kleve reallocate frostrup roze minc bicyclists lgbt where ed is the standard Levenshtein edit distance between the two strings. This is straightforward for languages that use the same character set, but it is more complicated for languages that are written using different scripts. A variety of prior work has focused on the problem of learning mappings between character sets (e.g., Yamada and Knight 1999; Tao et al. 2006; Yoon, Kim, and Sproat 2007; Bergsma and Kondrak 2007; Li et al. 2009; Snyder, Barzilay, and Knight 2010; Berg-Kirkpatrick and Klein 2011). For non-Roman script languages, we transliterate words into the Roman script before measuring orthographic similarity with their candidate English translations. Following prior work (Virga and Khudanpur 2003; Irvine, Callison-Burch, and Klementiev 2010), we treat transliteration as a monotone character translation task and train models on the mined pairs of person names in foreign, non-Roman script languages and English. Our MT-based transliteration system can translate a single character as many characters, and it can translate multiple input characters into a single output character. Becau"
J17-2001,P07-1083,0,0.0358988,"onzi affects suspected fed combat arrested wawel volcanic ash spewed eyjafjallajokull otunbajewa eruption cloud rubell dormancy dzv spatz centimes kleve reallocate frostrup roze minc bicyclists lgbt where ed is the standard Levenshtein edit distance between the two strings. This is straightforward for languages that use the same character set, but it is more complicated for languages that are written using different scripts. A variety of prior work has focused on the problem of learning mappings between character sets (e.g., Yamada and Knight 1999; Tao et al. 2006; Yoon, Kim, and Sproat 2007; Bergsma and Kondrak 2007; Li et al. 2009; Snyder, Barzilay, and Knight 2010; Berg-Kirkpatrick and Klein 2011). For non-Roman script languages, we transliterate words into the Roman script before measuring orthographic similarity with their candidate English translations. Following prior work (Virga and Khudanpur 2003; Irvine, Callison-Burch, and Klementiev 2010), we treat transliteration as a monotone character translation task and train models on the mined pairs of person names in foreign, non-Roman script languages and English. Our MT-based transliteration system can translate a single character as many characters,"
J17-2001,J90-2002,0,0.75974,"tion Science Department, 3330 Walnut Street, Philadelphia, PA 19104. E-mail: ccb@upenn.edu. Submission received: 12 November 2014; revised version received: 14 March 2016; accepted for publication: 29 May 2016. doi:10.1162/COLI_a_00284 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction In natural language processing, translations are typically learned from parallel corpora, which are sentence-aligned bilingual texts (Brown et al. 1990). In contrast, bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. These monolingual corpora can range from being completely unrelated topics to being comparable corpora that contain related information (such as Wikipedia articles on the same subject, but written independently in two languages) but are not translations of each other. Being able to learn translations from monolingual text is potentially very useful for machine translation (MT). For many language pairs, we often only have access to small bilingual resources. When a mac"
J17-2001,P11-2071,0,0.0332079,"tion and dictionary expansion methods could be used to supplement parallel data used for estimating word alignments and scored phrase tables. The most obvious way to integrate lexicon induction output into the SMT pipeline would be to induce translations for out-of-vocabulary and rare words. That is, if a word in our test set does not have a translation in the phrase table, we could induce one for it. Although most work on bilingual lexicon induction is motivated by the idea that outputs could be integrated into end-to-end SMT, until recently such an extrinsic evaluation was rarely performed. Daumé and Jagarlamudi (2011) use CCA and both contextual and orthographic features to induce translations. Razmara et al. (2013) construct a graph using source-language monolingual text and identify translations for sourcelanguage OOV words by pivoting through paraphrases. In Irvine, Quirk, and Daumé (2013), we presented a method for expanding an initial translation dictionary estimated from old-domain parallel corpora by matching marginal probabilities over new-domain comparable corpora. Daumé and Jagarlamudi (2011), Razmara et al. (2013), and our prior work in Irvine, Quirk, and Daumé (2013) integrate translations into"
J17-2001,D12-1025,0,0.0129308,"anslations. Tamura, Watanabe, and Sumita (2012) utilize the classic notions of co-occurrence and contextual similarity but use graph-based label propagation to induce translations. 7.2 Other Approaches to Learning Translation of OOVs Approaching the problem from an information retrieval perspective, Zhang, Huang, and Vogel (2005) use a system based on cross-lingual query expansion to identify translations for OOV words. A new line of research has tried to use decipherment techniques (Knight 2013) to learn translations from monolingual corpora (Ravi and Knight 2011; Nuhn, Mauser, and Ney 2012; Dou and Knight 2012, 2013). This research line draws on previous decipherment work for solving simpler substitution/transposition ciphers, while recognizing that thinking of the foreign language as a “code” also requires customizing the decipherment algorithms so that they can deal with highly non-deterministic mappings and very large substitution tables. 304 Irvine and Callison-Burch A Comprehensive Analysis of Bilingual Lexicon Induction 7.3 Integration with Machine Translation Any bilingual lexicon induction and dictionary expansion methods could be used to supplement parallel data used for estimating word al"
J17-2001,D13-1173,0,0.0302654,"Missing"
J17-2001,W95-0114,0,0.170381,"2. Monolingual Signals of Translation Equivalence We frame bilingual lexicon induction as a binary classification problem; for a pair of source and target language words, we predict whether the two are translations of one another or not. For a given source language word, we score all target language candidates separately and then rank them. We use a variety of signals derived from source and target monolingual corpora as features and use supervision to estimate the strength of each. A diverse range of signals have been used for bilingual lexicon induction in past work, notably by Rapp (1995), Fung (1995), Schafer and Yarowsky (2002), Klementiev and Roth (2006), Klementiev et al. (2012), and others. In this section, we detail the signals of translation equivalence that we use as components in our discriminative model. 2.1 Contextual Similarity In a similar fashion to how vector space models can be used to compute the similarity between two words in one language by creating vectors that represent their co-occurrence patterns with other words (Turney and Pantel 2010), context vector representations can also be used to compare the similarity of words across two languages. 275 Computational Lingui"
J17-2001,P98-1069,0,0.393652,"d the mappings between the vector spaces of the two languages. Instead, after computing the two co-occurrence matrices for the two languages, Rapp (1995) iteratively randomly permutes the word order of the matrix for one of the languages and calculates the similarity between the two matrices. The permutation is optimal when the similarity between the matrices is maximal, which is when the ordered words in the two matrices are most likely to be translations of one another. Results are given for a set of 100 English and German word translation pairs. Later formulations of the problem, including Fung and Yee (1998) and Rapp (1999), used small seed dictionaries to project word-based context vectors from the vector space of one language into the vector space of the other language. That is, each position in contextual vector v corresponds to a word in the source vocabulary,2 and vectors v are computed for each source word in the test set. Fung and Yee (1998) calculate the ith position of word w’s context vector, vwi , as vwi = TFi,w · IDFi where TFi,w is the number of times i and w co-occur (in this case, defined as appearing in the same sentence), and: IDFi = log maxn + 1 fi where maxn is the maximum freq"
J17-2001,W09-1117,1,0.860844,"Missing"
J17-2001,P08-1088,0,0.157807,"nd the rank of its correct translation. Across all languages, we find a positive average correlation of 0.25, indicating that, as we expected, we tend to rank correct translations higher for more bursty words. This effect is significant to a p-value of 0.01 for all 24 languages. Comparing our results here with those in Section 5.2, we see that burstiness is a better predictor of ranking performance on a given word than frequency. 6. Comparison with a Sophisticated Generative Model We compare our discriminative bilingual lexicon induction approach with the popular generative model developed by Haghighi et al. (2008). Haghighi et al. present a canonical correlation analysis (CCA)–based approach to inducing bilingual lexicons. The generative model presented in that work first generates a set of one-to-one matchings, M, between pairs of source and target words. Then, a feature vector is generated for each matched word type, si and tj , from a “language-independent concept,” zi,j . Similar to our work, source and target words are represented by feature vectors characterizing their orthographies and their contexts in monolingual corpora. However, unlike our work, the generative model proposed in Haghighi et a"
J17-2001,W11-1209,0,0.0150722,"ocument pairs and then use the same word translation probabilities to detect parallel fragments within the document pairs. They supplement existing parallel data with the new sentence and fragment pairs and evaluate endto-end SMT systems trained on the augmented parallel datasets. Quirk, Udupa, and Menezes (2007) also seek to identify phrase translation pairs from comparable corpora, but that method requires a first-pass identification of promising comparable pairs of sentences from paired comparable documents. They then use a generative model to extract fragment translation pairs. Similarly, Hewavitharana and Vogel (2011) seek to identify phrase translation pairs from comparable corpora but require a first pass to identify a set of comparable sentences and then a second pass through the data to find the best phrasal alignment within each sentence pair. These efforts at using comparable corpora to expand parallel corpora are orthogonal to the approaches that we propose in this article. 8. Conclusions We have performed the most systematic analysis of bilingual lexicon induction to date. We analyze a set of 18 monolingually derived signals of translation equivalence, 305 Computational Linguistics Volume 43, Numbe"
J17-2001,N13-1056,1,0.800015,"Missing"
J17-2001,2010.amta-papers.12,1,0.854225,"Missing"
J17-2001,W10-0717,1,0.8663,"Missing"
J17-2001,D13-1109,1,0.830597,"Missing"
J17-2001,E12-1014,1,0.918129,"exicon induction as a binary classification problem; for a pair of source and target language words, we predict whether the two are translations of one another or not. For a given source language word, we score all target language candidates separately and then rank them. We use a variety of signals derived from source and target monolingual corpora as features and use supervision to estimate the strength of each. A diverse range of signals have been used for bilingual lexicon induction in past work, notably by Rapp (1995), Fung (1995), Schafer and Yarowsky (2002), Klementiev and Roth (2006), Klementiev et al. (2012), and others. In this section, we detail the signals of translation equivalence that we use as components in our discriminative model. 2.1 Contextual Similarity In a similar fashion to how vector space models can be used to compute the similarity between two words in one language by creating vectors that represent their co-occurrence patterns with other words (Turney and Pantel 2010), context vector representations can also be used to compare the similarity of words across two languages. 275 Computational Linguistics Volume 43, Number 2 The earliest work in bilingual lexicon induction by Rapp"
J17-2001,P06-1103,0,0.10014,"d machine translation. Instead, it has been evaluated by holding out a portion of the bilingual dictionary and evaluating how well the algorithm learns the translations of the held-out words. To discover translated words across languages, past work has proposed a variety of monolingual distributional similarity metrics as signals of translation equivalence. These signals include contextual similarity, temporal similarity, and orthographic similarity. Most prior work has used unsupervised methods (like rank combination) to aggregate these types of orthogonal signals (Schafer and Yarowsky 2002; Klementiev and Roth 2006). Surprisingly, no past research has used supervised approaches to combine diverse monolingually derived signals for bilingual lexicon induction. The field of machine learning has shown repeatedly that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp 1995), and that same dictionary may be used for estimatin"
J17-2001,P13-5003,0,0.028028,"tilingual dictionary. Laws et al. (2010) use graph-based models to represent linguistic relations and induce translations. Tamura, Watanabe, and Sumita (2012) utilize the classic notions of co-occurrence and contextual similarity but use graph-based label propagation to induce translations. 7.2 Other Approaches to Learning Translation of OOVs Approaching the problem from an information retrieval perspective, Zhang, Huang, and Vogel (2005) use a system based on cross-lingual query expansion to identify translations for OOV words. A new line of research has tried to use decipherment techniques (Knight 2013) to learn translations from monolingual corpora (Ravi and Knight 2011; Nuhn, Mauser, and Ney 2012; Dou and Knight 2012, 2013). This research line draws on previous decipherment work for solving simpler substitution/transposition ciphers, while recognizing that thinking of the foreign language as a “code” also requires customizing the decipherment algorithms so that they can deal with highly non-deterministic mappings and very large substitution tables. 304 Irvine and Callison-Burch A Comprehensive Analysis of Bilingual Lexicon Induction 7.3 Integration with Machine Translation Any bilingual le"
J17-2001,W02-0902,0,0.189413,"and Yarowsky (2002) exploit the idea that word translations tend to co-occur in time across languages, and Schafer (2006) uses this and a diverse set of other similarity measures to bootstrap a small seed bilingual dictionary and induce full dictionaries for low-resource languages. Schafer combines the different signals, and weights their contribution in an ad hoc manual fashion, rather than setting them empirically by applying machine learning algorithms. Klementiev and Roth (2006) also use the temporal cue to train a phonetic similarity model for associating Named Entities across languages. Koehn and Knight (2002) use similarity in spelling as another kind of cue that a pair of words may be translations of one another. Other work has used dependency relations in place of adjacent words to define context (Garera, Callison-Burch, and Yarowsky 2009; Andrade, Matsuzaki, and Tsujii 2012). Recent work has used graph-based models to induce translations. Mausam et al. (2010) use freely available online dictionaries and inference over translation graphs to compile a very large, multilingual dictionary. Laws et al. (2010) use graph-based models to represent linguistic relations and induce translations. Tamura, W"
J17-2001,C10-2070,0,0.0145202,"to train a phonetic similarity model for associating Named Entities across languages. Koehn and Knight (2002) use similarity in spelling as another kind of cue that a pair of words may be translations of one another. Other work has used dependency relations in place of adjacent words to define context (Garera, Callison-Burch, and Yarowsky 2009; Andrade, Matsuzaki, and Tsujii 2012). Recent work has used graph-based models to induce translations. Mausam et al. (2010) use freely available online dictionaries and inference over translation graphs to compile a very large, multilingual dictionary. Laws et al. (2010) use graph-based models to represent linguistic relations and induce translations. Tamura, Watanabe, and Sumita (2012) utilize the classic notions of co-occurrence and contextual similarity but use graph-based label propagation to induce translations. 7.2 Other Approaches to Learning Translation of OOVs Approaching the problem from an information retrieval perspective, Zhang, Huang, and Vogel (2005) use a system based on cross-lingual query expansion to identify translations for OOV words. A new line of research has tried to use decipherment techniques (Knight 2013) to learn translations from"
J17-2001,P06-1011,0,0.0356415,"the monolingually estimated features that we used in that framework are the same as the features used here for bilingual lexicon induction. In that work, we performed oracle experiments where the translations were given by an existing phrase-table, and simply re-scored using the monolingually estimated signals of translation equivalence. 7.4 Extracting Parallel Data from Comparable Corpora Resnik and Smith (2003), Munteanu and Marcu (2005), Abdul-Rauf and Schwenk (2009a), Abdul-Rauf and Schwenk (2009b), and Smith, Quirk, and Toutanova (2010) identify parallel sentences in comparable corpora. Munteanu and Marcu (2006) identify parallel sub-sentential fragments using a probabilistic lexicon and information retrieval methods to identify similar document pairs and then use the same word translation probabilities to detect parallel fragments within the document pairs. They supplement existing parallel data with the new sentence and fragment pairs and evaluate endto-end SMT systems trained on the augmented parallel datasets. Quirk, Udupa, and Menezes (2007) also seek to identify phrase translation pairs from comparable corpora, but that method requires a first-pass identification of promising comparable pairs o"
J17-2001,J05-4003,0,0.0453486,"in domain adaptation settings. In Klementiev et al. (2012), we described a framework for estimating the parameters of machine translation without bilingual parallel corpora. Many of the monolingually estimated features that we used in that framework are the same as the features used here for bilingual lexicon induction. In that work, we performed oracle experiments where the translations were given by an existing phrase-table, and simply re-scored using the monolingually estimated signals of translation equivalence. 7.4 Extracting Parallel Data from Comparable Corpora Resnik and Smith (2003), Munteanu and Marcu (2005), Abdul-Rauf and Schwenk (2009a), Abdul-Rauf and Schwenk (2009b), and Smith, Quirk, and Toutanova (2010) identify parallel sentences in comparable corpora. Munteanu and Marcu (2006) identify parallel sub-sentential fragments using a probabilistic lexicon and information retrieval methods to identify similar document pairs and then use the same word translation probabilities to detect parallel fragments within the document pairs. They supplement existing parallel data with the new sentence and fragment pairs and evaluate endto-end SMT systems trained on the augmented parallel datasets. Quirk, U"
J17-2001,P12-1017,0,0.0279562,"Missing"
J17-2001,P02-1038,0,0.235184,"ignals include contextual similarity, temporal similarity, and orthographic similarity. Most prior work has used unsupervised methods (like rank combination) to aggregate these types of orthogonal signals (Schafer and Yarowsky 2002; Klementiev and Roth 2006). Surprisingly, no past research has used supervised approaches to combine diverse monolingually derived signals for bilingual lexicon induction. The field of machine learning has shown repeatedly that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp 1995), and that same dictionary may be used for estimating the parameters of a model to combine monolingual signals. In this setting, bilingual lexicon induction is critical for translating source words that do not appear in the parallel data or dictionary. We make several contributions with this article.1 First, we present a discriminative model of bilingual lexicon induction that significantly outperforms previ"
J17-2001,2007.mtsummit-papers.50,0,0.0808506,"Missing"
J17-2001,P95-1050,0,0.846985,"signals (Schafer and Yarowsky 2002; Klementiev and Roth 2006). Surprisingly, no past research has used supervised approaches to combine diverse monolingually derived signals for bilingual lexicon induction. The field of machine learning has shown repeatedly that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp 1995), and that same dictionary may be used for estimating the parameters of a model to combine monolingual signals. In this setting, bilingual lexicon induction is critical for translating source words that do not appear in the parallel data or dictionary. We make several contributions with this article.1 First, we present a discriminative model of bilingual lexicon induction that significantly outperforms previous models. Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence. When feature weights are"
J17-2001,P99-1067,0,0.522881,"he vector spaces of the two languages. Instead, after computing the two co-occurrence matrices for the two languages, Rapp (1995) iteratively randomly permutes the word order of the matrix for one of the languages and calculates the similarity between the two matrices. The permutation is optimal when the similarity between the matrices is maximal, which is when the ordered words in the two matrices are most likely to be translations of one another. Results are given for a set of 100 English and German word translation pairs. Later formulations of the problem, including Fung and Yee (1998) and Rapp (1999), used small seed dictionaries to project word-based context vectors from the vector space of one language into the vector space of the other language. That is, each position in contextual vector v corresponds to a word in the source vocabulary,2 and vectors v are computed for each source word in the test set. Fung and Yee (1998) calculate the ith position of word w’s context vector, vwi , as vwi = TFi,w · IDFi where TFi,w is the number of times i and w co-occur (in this case, defined as appearing in the same sentence), and: IDFi = log maxn + 1 fi where maxn is the maximum frequency of any of"
J17-2001,P11-1002,0,0.00481249,"ls to represent linguistic relations and induce translations. Tamura, Watanabe, and Sumita (2012) utilize the classic notions of co-occurrence and contextual similarity but use graph-based label propagation to induce translations. 7.2 Other Approaches to Learning Translation of OOVs Approaching the problem from an information retrieval perspective, Zhang, Huang, and Vogel (2005) use a system based on cross-lingual query expansion to identify translations for OOV words. A new line of research has tried to use decipherment techniques (Knight 2013) to learn translations from monolingual corpora (Ravi and Knight 2011; Nuhn, Mauser, and Ney 2012; Dou and Knight 2012, 2013). This research line draws on previous decipherment work for solving simpler substitution/transposition ciphers, while recognizing that thinking of the foreign language as a “code” also requires customizing the decipherment algorithms so that they can deal with highly non-deterministic mappings and very large substitution tables. 304 Irvine and Callison-Burch A Comprehensive Analysis of Bilingual Lexicon Induction 7.3 Integration with Machine Translation Any bilingual lexicon induction and dictionary expansion methods could be used to sup"
J17-2001,P13-1109,0,0.031115,"Missing"
J17-2001,J03-3002,0,0.00954997,"l to improve performance in domain adaptation settings. In Klementiev et al. (2012), we described a framework for estimating the parameters of machine translation without bilingual parallel corpora. Many of the monolingually estimated features that we used in that framework are the same as the features used here for bilingual lexicon induction. In that work, we performed oracle experiments where the translations were given by an existing phrase-table, and simply re-scored using the monolingually estimated signals of translation equivalence. 7.4 Extracting Parallel Data from Comparable Corpora Resnik and Smith (2003), Munteanu and Marcu (2005), Abdul-Rauf and Schwenk (2009a), Abdul-Rauf and Schwenk (2009b), and Smith, Quirk, and Toutanova (2010) identify parallel sentences in comparable corpora. Munteanu and Marcu (2006) identify parallel sub-sentential fragments using a probabilistic lexicon and information retrieval methods to identify similar document pairs and then use the same word translation probabilities to detect parallel fragments within the document pairs. They supplement existing parallel data with the new sentence and fragment pairs and evaluate endto-end SMT systems trained on the augmented"
J17-2001,W02-2026,0,0.171737,"translations into end-to-end machine translation. Instead, it has been evaluated by holding out a portion of the bilingual dictionary and evaluating how well the algorithm learns the translations of the held-out words. To discover translated words across languages, past work has proposed a variety of monolingual distributional similarity metrics as signals of translation equivalence. These signals include contextual similarity, temporal similarity, and orthographic similarity. Most prior work has used unsupervised methods (like rank combination) to aggregate these types of orthogonal signals (Schafer and Yarowsky 2002; Klementiev and Roth 2006). Surprisingly, no past research has used supervised approaches to combine diverse monolingually derived signals for bilingual lexicon induction. The field of machine learning has shown repeatedly that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp 1995), and that same dictionar"
J17-2001,N10-1063,0,0.0138428,"Missing"
J17-2001,P10-1107,0,0.0523847,"Missing"
J17-2001,D12-1003,0,0.240446,"Missing"
J17-2001,W06-1630,0,0.0429112,"ffices bond occupied aer madoff declaration ponzi affects suspected fed combat arrested wawel volcanic ash spewed eyjafjallajokull otunbajewa eruption cloud rubell dormancy dzv spatz centimes kleve reallocate frostrup roze minc bicyclists lgbt where ed is the standard Levenshtein edit distance between the two strings. This is straightforward for languages that use the same character set, but it is more complicated for languages that are written using different scripts. A variety of prior work has focused on the problem of learning mappings between character sets (e.g., Yamada and Knight 1999; Tao et al. 2006; Yoon, Kim, and Sproat 2007; Bergsma and Kondrak 2007; Li et al. 2009; Snyder, Barzilay, and Knight 2010; Berg-Kirkpatrick and Klein 2011). For non-Roman script languages, we transliterate words into the Roman script before measuring orthographic similarity with their candidate English translations. Following prior work (Virga and Khudanpur 2003; Irvine, Callison-Burch, and Klementiev 2010), we treat transliteration as a monotone character translation task and train models on the mined pairs of person names in foreign, non-Roman script languages and English. Our MT-based transliteration syste"
J17-2001,W03-1508,0,0.0109916,"ward for languages that use the same character set, but it is more complicated for languages that are written using different scripts. A variety of prior work has focused on the problem of learning mappings between character sets (e.g., Yamada and Knight 1999; Tao et al. 2006; Yoon, Kim, and Sproat 2007; Bergsma and Kondrak 2007; Li et al. 2009; Snyder, Barzilay, and Knight 2010; Berg-Kirkpatrick and Klein 2011). For non-Roman script languages, we transliterate words into the Roman script before measuring orthographic similarity with their candidate English translations. Following prior work (Virga and Khudanpur 2003; Irvine, Callison-Burch, and Klementiev 2010), we treat transliteration as a monotone character translation task and train models on the mined pairs of person names in foreign, non-Roman script languages and English. Our MT-based transliteration system can translate a single character as many characters, and it can translate multiple input characters into a single output character. Because transliteration is strictly a monotone operation, we do not allow reordering in our models. Additionally, unlike in machine translation, our translation and language models can support very large n-gram siz"
J17-2001,W99-0906,0,0.0339754,"tigating convicted spy offices bond occupied aer madoff declaration ponzi affects suspected fed combat arrested wawel volcanic ash spewed eyjafjallajokull otunbajewa eruption cloud rubell dormancy dzv spatz centimes kleve reallocate frostrup roze minc bicyclists lgbt where ed is the standard Levenshtein edit distance between the two strings. This is straightforward for languages that use the same character set, but it is more complicated for languages that are written using different scripts. A variety of prior work has focused on the problem of learning mappings between character sets (e.g., Yamada and Knight 1999; Tao et al. 2006; Yoon, Kim, and Sproat 2007; Bergsma and Kondrak 2007; Li et al. 2009; Snyder, Barzilay, and Knight 2010; Berg-Kirkpatrick and Klein 2011). For non-Roman script languages, we transliterate words into the Roman script before measuring orthographic similarity with their candidate English translations. Following prior work (Virga and Khudanpur 2003; Irvine, Callison-Burch, and Klementiev 2010), we treat transliteration as a monotone character translation task and train models on the mined pairs of person names in foreign, non-Roman script languages and English. Our MT-based tran"
J17-2001,P07-1015,0,0.0594882,"Missing"
J17-2001,D09-1092,0,\N,Missing
J17-2001,C98-1066,0,\N,Missing
J17-2001,W09-3501,0,\N,Missing
J17-2001,Q14-1007,1,\N,Missing
N06-1003,H05-1085,0,0.0417204,"Missing"
N06-1003,E03-1076,1,0.247316,"more 23 Related Work Previous research on trying to overcome data sparsity issues in statistical machine translation has largely focused on introducing morphological analysis as a way of reducing the number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 7 Discussion cal machine translation to larger corpora and longer phrases. In Proceedings of ACL. In this paper we have shown that significant gains in coverage and transl"
N06-1003,N03-1017,1,0.047697,"Missing"
N06-1003,koen-2004-pharaoh,0,0.0150597,"ility, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 20 4.2 Translation with paraphrases We extracted all source language (Spanish and French) phrases up to length 10 from the test and development sets which did not have translations in phrase tables that were generated for the three training corpora. For each of these phrases we generated a list of para"
N06-1003,2005.mtsummit-papers.11,1,0.103114,"definition of the paraphrase probability to include multiple corpora, as follows: P p(e2 |e1 ) ≈ c∈C f in c p(f |e1 )p(e2 |f ) P |C| (3) where c is a parallel corpus from a set of parallel corpora C. Thus multiple corpora may be used by summing over all paraphrase probabilities calculated from a single corpus (as in Equation 1) and normalized by the number of parallel corpora. 4 Experimental Design We examined the application of paraphrases to deal with unknown phrases when translating from Spanish and French into English. We used the publicly available Europarl multilingual parallel corpus (Koehn, 2005) to create six training corpora for the two language pairs, and used the standard Europarl development and test sets. 4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probab"
N06-1003,W02-1018,0,0.0323214,"Missing"
N06-1003,J04-2003,0,0.064585,"Missing"
N06-1003,P02-1038,0,0.339431,"Missing"
N06-1003,J04-4002,0,0.120477,"Missing"
N06-1003,P03-1021,0,0.0206105,"For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default be"
N06-1003,P05-1074,1,0.629984,"slate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potential paraphrases of the original English phrase. Figure 2 illustrates how a German phrase can be used as a point of identification for English paraphrases in this way. The method defined in Bannard and CallisonBurch (2005) has several features that make it an ideal candid"
N06-1003,P01-1008,0,0.667567,"that we employ for dealing with unknown source language words is to substitute paraphrases of those words, and then translate the paraphrases. Table 1 gives examples of paraphrases and their translations. If we had learned a translation of garantizar we could translate it instead of encargarnos, and similarly for utilizado instead of usado. 3 Acquiring Paraphrases Paraphrases are alternative ways of expressing the same information within one language. The automatic generation of paraphrases has been the focus of a significant amount of research lately. Many methods for extracting paraphrases (Barzilay and McKeown, 2001; Pang et al., 2003) make use of monolingual parallel corpora, such as multiple translations of classic French novels into English, or the multiple reference translations used by many automatic evaluation metrics for machine translation. Bannard and Callison-Burch (2005) use bilingual parallel corpora to generate paraphrases. Paraphrases are identified by pivoting through phrases in another language. The foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate back to are treated as potent"
N06-1003,J93-2003,0,0.0092731,"nt approaches. 1 • Define a method for incorporating paraphrases of unseen source phrases into the statistical machine translation process. • Show that by translating paraphrases we achieve a marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. • Argue that while we observe an improvement in Bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve. Introduction As with many other statistical natural language processing tasks, statistical machine translation (Brown et al., 1993) produces high quality results when ample training data is available. This is problematic for so called “low density” language pairs which do not have very large parallel corpora. For example, when words occur infrequently in a parallel corpus parameter estimates for word-level alignments can be inaccurate, which can in turn lead to inaccurate phrase translations. Limited amounts of training data can further lead to a problem of low coverage in that many phrases encountered at run-time are not ob• Present an alternative methodology for targeted manual evaluation that may be useful in other res"
N06-1003,P02-1040,0,0.110282,"ed statistical machine translation system based on the log-linear formulation described in (Och and Ney, 2002) ˆ = arg max p(e|f ) e e = arg max e M X λm hm (e, f ) (4) (5) m=1 The baseline model had a total of eight feature functions, hm (e, f ): a language model probability, a phrase translation probability, a reverse phrase translation probability, lexical translation probability, a reverse lexical translation probability, a word penalty, a phrase penalty, and a distortion cost. To set the weights, λm , we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al., 2002) as the objective function. The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training on each of the three training corpora. We used the Pharaoh beamsearch decoder (Koehn, 2004) to produce the translations after all of the model parameters had been set. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. This is the default behavior for many systems, as noted in Section 2.1. 20 4.2 T"
N06-1003,J03-3002,0,0.0379518,"number of types observed in a training text. For example, Nissen and Ney (2004) apply morphological analyzers to English and German and are able to reduce the amount of training data needed to reach a certain level of translation quality. Goldwater and McClosky (2005) find that stemming Czech and using lemmas improves the word-to-word correspondences when training Czech-English alignment models. Koehn and Knight (2003) show how monolingual texts and parallel corpora can be used to figure out appropriate places to split German compounds. Still other approaches focus on ways of acquiring data. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Oard et al. (2003) describe various methods employed for quickly gathering resources to create a machine translation system for a language with no initial resources. 7 Discussion cal machine translation to larger corpora and longer phrases. In Proceedings of ACL. In this paper we have shown that significant gains in coverage and translation quality can be had by integrating paraphrases into statistical machine translation. In effect, paraphrases introduce some amount of generalization into statistical machine translation. Whereas b"
N06-1003,E06-1032,1,\N,Missing
N06-1003,N03-2026,1,\N,Missing
N06-1003,N03-1024,0,\N,Missing
N06-1003,P05-1032,1,\N,Missing
N10-1024,N03-2003,0,0.0397212,"Missing"
N10-1024,D09-1030,1,0.300441,"Missing"
N10-1024,cieri-etal-2004-fisher,0,0.0118515,"Missing"
N10-1024,H93-1023,0,0.163163,"Missing"
N10-1024,D08-1027,0,0.280299,"Missing"
N10-1024,D09-1006,1,0.721851,"Missing"
N10-1057,D09-1030,1,0.854875,"2.1 Amazon’s Mechanical Turk The high cost associated with hiring and training a human editor makes it difficult to imagine an alternative to automatic metrics. However, we propose soliciting edits from workers on Amazon’s Mechanical Turk (AMT). AMT is a virtual marketplace where “requesters” can post tasks to be completed by “workers” (aka Turkers) around the world. Two main advantages of AMT are the preexisting infrastructure, and the low cost of completing tasks, both in terms of time and money. Data collected over AMT has already been used in several papers such as Snow et al. (2008) and Callison-Burch (2009). When a requester creates a task to be completed over AMT, it is typical to have completed by more than one worker. The reason is that the use of AMT for data collection has an inherent problem with data quality. A requester has fewer tools at their disposal to ensure workers are doing the task properly (via training, feedback, etc) when compared to hiring annotators in the ‘real’ world. Those redundant annotations are therefore collected to increase the likelihood of at least one submission from a faithful (and competent) worker. 2.2 AMT for HTER The main idea it to mimic the real-world HTER"
N10-1057,2006.amta-papers.25,0,0.0540055,"ss options available to predict HTER, such as automatic metrics. We then discuss the possibility of relying on human annotators, and the inherent difficulty in training them, before discussing the concept of soliciting edits over AMT. We detail the task given to the workers and summarize the data that we collected, then show how we can combine their data to obtain significanly better rank predictions of documents. 2 Human-Targeted TER Translation edit rate (TER) measures the number of edits required to transform a hypothesis into an appropriate sentence in terms of grammaticality and meaning (Snover et al., 2006). While TER usually scores a hypothesis against an existing reference sentence, human-targeted TER scores a hypothesis against a post-edited version of itself. 369 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 369–372, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics While HTER has been shown to correlate quite well with human judgment of MT quality, it is quite challenging to obtain HTER scores for MT output, since this would require hiring and training human subjects to perform the editing task. The"
N10-1057,D08-1027,0,0.0702216,"Missing"
N10-1062,W09-0432,0,0.0129327,"are against the war and the occupation of Iraq by the United States and the United Kingdom, and we demand the immediate withdrawal of the occupying forces from this country . Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one. 5 Related Work 5.3 Streaming Language Models 5.1 Translation Model Domain Adaptation Our work is related to domain adaptation for translation models. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only prior work we are aware of is Liang"
N10-1062,D07-1090,0,0.0372992,"Missing"
N10-1062,J93-2003,0,0.0175172,"g an HMM-based alignment model where the probability of alignment aj is dependent only on the previous alignment at aj−1 (Vogel et al., 1996). We can write Pr(f , a |e) = |f | X Y p(aj |aj−1 , |e|) · p(fj |eaj ) a′ ∈a j=1 where we assume a first-order dependence on previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a first-order dependency. Although a series of more complex models are defined, IBM Models 2 to Model 6 (Brown et al., 1993; Och and Ney, 2003), researchers typically find that extracting phrase pairs or translation grammar rules using Model 1 and the HMM-based alignments results in equivalently high translation quality. Nevertheless, there is nothing in our approach which limits us to using just Model 1 and the HMM model. A high-level overview of the standard, batch EM algo"
N10-1062,P05-1032,1,0.260706,"990). Treating the entire corpus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Callison-Burch et al., 2005; Lopez, 2008). We need to delete items if we wish to maintain Static Test Points 2.5 input stream model coverage Delta in BLEU scores 2 Unbounded Test Points input stream model coverage epoch 1 epoch 2 1.5 1 0.5 Bounded Test Points 0 input stream 5 model coverage epoch 2 15 20 25 30 35 epochs sliding windows Figure 1: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed win"
N10-1062,J07-2003,0,0.0130749,"as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of 398 Ep 00 03 06 10 From–To 04/1996–12/2000 02/2002–09/2002 10/2003–03/2004 03/2006–09/2006 Sent Pairs 600k 70k 60k 73k Source/Target 15.0M/16.0M 1.9M/2.0M 1.6M/1.7M 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models. the target side parallel d"
N10-1062,W07-0733,0,0.0127905,"n is clear and well known: we are against the war and the occupation of Iraq by the United States and the United Kingdom, and we demand the immediate withdrawal of the occupying forces from this country . Figure 4: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one. 5 Related Work 5.3 Streaming Language Models 5.1 Translation Model Domain Adaptation Our work is related to domain adaptation for translation models. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Most techniques center around using mixtures of translation models. Once trained, these models generally never change. They therefore fall under the batch training regime. The focus of this work instead is on incremental retraining and also on supporting bounded memory consumption. Our experiments examine updating model parameters in a single domain over different periods in time. Naturally, we could also use domain adaptation techniques to further improve how we incorporate new samples. 5.2 Online EM for SMT For stepwise online EM for SMT models, the only pri"
N10-1062,D09-1079,1,0.816519,"eved an absolute improvement of +1.24 BLEU over the static baseline for the final test point. We get another absolute gain of +1.08 BLEU by allowing the LM coverage to adapt as well. Using an online, adaptive model gives a total gain of +2.32 BLEU over a static baseline that does not adapt. 4.6 Increasing LM Coverage A natural and interesting extension to the experiments above is to use the target side of the incoming stream to extend the LM coverage alongside the TM. 400 6 Although we batch retrain the LMs we could use an online LM that incorporates new vocabulary from the input stream as in Levenberg and Osborne (2009). Source: Die Kommission ist bereit, an der Schaffung eines solchen Rechtsrahmens unter Zugrundelegung von vier wesentlichen Prinzipien mitzuwirken. Reference: The commission is willing to cooperate in the creation of such a legal framework on the basis of four essential principles. Static: The commission is prepared, in the creation of a legal framework, taking account of four fundamental principles them. Online: The commission is prepared to participate in the creation of such a legal framework, based on four fundamental principles. Source: Unser Standpunkt ist klar und allseits bekannt: Wir"
N10-1062,W09-0424,1,0.738055,"ve were not limited to French-English, this time our parallel input stream was generated from the GermanEnglish language pair of Europarl with German as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of 398 Ep 00 03 06 10 From–To 04/1996–12/2000 02/2002–09/2002 10/2003–03/2004 03/2006–09/2006 Sent Pairs 600k 70k 60k 73k Source/Target 15.0M/16.0M 1.9M/2.0M 1.6M/1.7M 1.9M/2.0M Table 1: Date ranges, total sentence pairs, and source and target word counts encountered in"
N10-1062,N09-1069,0,0.380716,"count collection begins anew using the new distribution θˆt+1 . When we move to processing an incoming data stream, however, the batch EM algorithm’s requirement that all data be available for each iteration becomes impractical since we do not have access to all n examples at once. Instead we receive examples from the input stream incrementally. For this reason online EM algorithms have been developed to update the probability model θˆ incrementally without needing to store and iterate through all the unlabeled training data repeatedly. Various online EM algorithms have been investigated (see Liang and Klein (2009) for an overview) but our focus is on the stepwise online EM (sOEM) algorithm (Cappe and Moulines, 2009). Instead of iterating over the full set of training examples, sOEM stochastically approximates the batch E-step and incorporates the information from the newly available streaming observations in steps. Each step is called a mini-batch and is comprised of one or more new examples encountered in the stream. Unlike in batch EM, in sOEM the expected counts are retained between EM iterations and not cleared. 4 As the M-step can be computed in closed form we desig¯ nate it in this work as θ(S)."
N10-1062,C08-1064,0,0.0559607,"pus as a single string, a suffix array holds in lexicographical order (only) the starting index of each suffix of the string. After construction, since the corpus is now ordered, we can query the suffix array quickly using binary search to efficiently find all occurrences of a particular token or sequence of tokens. Then we can easily compute, on-the-fly, the statistics required such as translation probabilities for a given source phrase. Suffix arrays can also be compressed, which make them highly attractive structures for representing massive translation models (Callison-Burch et al., 2005; Lopez, 2008). We need to delete items if we wish to maintain Static Test Points 2.5 input stream model coverage Delta in BLEU scores 2 Unbounded Test Points input stream model coverage epoch 1 epoch 2 1.5 1 0.5 Bounded Test Points 0 input stream 5 model coverage epoch 2 15 20 25 30 35 epochs sliding windows Figure 1: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window. constant"
N10-1062,J03-1002,0,0.0144271,"previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a first-order dependency. Although a series of more complex models are defined, IBM Models 2 to Model 6 (Brown et al., 1993; Och and Ney, 2003), researchers typically find that extracting phrase pairs or translation grammar rules using Model 1 and the HMM-based alignments results in equivalently high translation quality. Nevertheless, there is nothing in our approach which limits us to using just Model 1 and the HMM model. A high-level overview of the standard, batch EM algorithm applied to HMM-based word alignment model is shown in Algorithm 1. 2.3 Stepwise EM for Word Alignments Application of sOEM to HMM and Model 1 based word aligning is straightforward. The process of collecting the counts over the expected conditional probabili"
N10-1062,P03-1021,0,0.0309733,"fixed sized sample of it). To ensure the recency results reported above were not limited to French-English, this time our parallel input stream was generated from the GermanEnglish language pair of Europarl with German as source and English again as target. For testing we held out a total of 22k sentences from 10 evenly spaced intervals in the input stream which divided the input stream into 10 epochs. Stream statistics for three example epochs are shown in Table 1. We held out 4.5k sentence pairs as development data to optimize the feature function weights using minimum error rate training (Och, 2003) and these weights were used by all models. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Note that nothing hinges on whether we used a syntax or a phrase-based system. We used a 5-gram, Kneser-Ney smoothed language model (LM) trained on the initial segment of 398 Ep 00 03 06 10 From–To 04/1996–12/2000 02/2002–09/2002 10/2003–03/2004 03/2006–09/2006 Sent Pairs 600k 70k 60k 73k Source/Target 15.0M/16.0M 1.9M/2.0M 1.6M/1.7M 1.9M/2.0M Table 1: Date ran"
N10-1062,2001.mtsummit-papers.68,0,0.0122748,"ted all 36 test points again using a new grammar for each document extracted from only the sentences contained in the epoch that was before it. To explicitly test the effect of recency 5 Available at http://www.statmt.org/europarl on the TM all other factors of the SMT pipeline remained constant including the language model and the feature weights. Hence, the only change from the static baseline to the epochs performance was the TM data which was based on recency. Note that at this stage we did not use any incremental retraining. Results are shown in Figure 2 as the differences in BLEU score (Papineni et al., 2001) between the baseline TM versus the translation models trained on material chronologically closer to the given test point. The consistently positive deltas in BLEU scores between the model that is never retrained and the models that are retrained show that we achieve a higher translation performance when using more upto-date TMs that incorporate recent sentence pairs. As the chronological distance between the initial, static model and the retrained models increases, we see ever-increasing differences in translation performance. This underlines the need to retrain translation models with timely"
N10-1062,C96-2141,0,0.151243,"slation to estimate word alignment probabilities between parallel sentences. From these alignments, bilingual rules or phrase pairs can be extracted. Given a set of parallel sentence examples, {F, E}, with F the set of source sentences and E the corresponding target sentences, we want to find the latent alignments a for a sentence pair (f , e) ∈ {F, E} that defines the most probable correspondence between words fj and ei such that aj = i. We can induce these alignments using an HMM-based alignment model where the probability of alignment aj is dependent only on the previous alignment at aj−1 (Vogel et al., 1996). We can write Pr(f , a |e) = |f | X Y p(aj |aj−1 , |e|) · p(fj |eaj ) a′ ∈a j=1 where we assume a first-order dependence on previously aligned positions. To find the most likely parameter weights for the translation and alignment probabilities for the HMM-based alignments, we employ the EM algorithm via dynamic programming. Since HMMs have multiple local minima, we seed the HMM-based model probabilities with a better than random guess using IBM Model 1 (Brown et al., 1993) as is standard. IBM Model 1 is of the same form as the HMM-based model except it uses a uniform distribution instead of a"
N10-1062,P02-1040,0,\N,Missing
N10-1062,2005.eamt-1.39,0,\N,Missing
N12-1006,W05-0909,0,0.0349848,"Missing"
N12-1006,W10-0701,1,0.675469,"Missing"
N12-1006,E06-1047,0,0.780602,"Missing"
N12-1006,N09-1025,0,0.0340312,"tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU optimization procedure (Devlin, 2009). The Dialectal Arabic side of our corpus consisted of 1.5M words (1.1M Levantine and 380k Egyptian). Table 2 gives statistics about the various train/tune/test splits we used in our experiments. Since the Egyptian set was so small, we split it only to training/test sets, opting not to have a tuning set. The MSA training data we used consisted of ArabicEnglish cor"
N12-1006,P05-1071,0,0.186427,"e, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmodified MSA segmenter, and there is higher variability in the written form of dialect compared to MSA. Given the significant, albeit smaller gain on dialectal inpu"
N12-1006,P06-1086,0,0.552091,"Missing"
N12-1006,N06-2013,0,0.0454305,"achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table"
N12-1006,N04-4015,0,0.0397882,"scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash"
N12-1006,J03-1002,0,0.00335263,"tal cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU opt"
N12-1006,P02-1040,0,0.104675,"Missing"
N12-1006,2006.amta-papers.21,0,0.538693,"Missing"
N12-1006,W11-2602,0,0.322886,"le 6). 4.5 Mapping from Dialectal Arabic to MSA Before Translating to English Given the large amount of linguistic resources that have been developed for MSA over the past years, and the extensive research that was conducted on machine translation from MSA to English and other languages, an obvious research question is whether Dialectal Arabic is best translated to English by first pivoting through MSA, rather than directly. The proximity of Dialectal Arabic to MSA makes the mapping in principle easier than general machine translation, and a number of researchers have explored this direction (Salloum and Habash, 2011). In this scenario, the dialectal source would first be automatically transformed to MSA, using either a rule-based or statistical mapping module. The Dialectal Arabic-English parallel corpus we created presents a unique opportunity to compare the MSA-pivoting approach against direct translation. First, we collected equivalent MSA data for the Levantine Web test and tuning sets, by asking Turkers to transform dialectal passages to valid and fluent MSA. Turkers were shown example transformations, and we encouraged fewer changes where applicable (e.g. morphological rather than lexical mapping),"
N12-1006,2010.amta-papers.5,0,0.238646,"Missing"
N12-1006,P08-1066,0,0.0165598,"creating our parallel corpus. The total cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a t"
N12-1006,2006.amta-papers.25,1,0.697297,"plex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmod"
N12-1006,P11-2007,1,0.80814,"nizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into 50 their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC. They demonstrated a variety of mechanisms that increase the translation quality of crowdsourced translations to near professional levels, with a total cost that is less than one tenth the cost of professional translation. Zaidan and Callison-Burch (2011b) created the Arabic Online Commentary (AOC) dataset, a 52Mword monolingual dataset rich in dialectal content. Over 100k sentences from the AOC were annotate"
N12-1006,P11-1122,1,0.553138,"Missing"
N12-1078,S07-1002,0,0.0152215,"en considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME , ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff"
N12-1078,W10-0731,0,0.0126843,"fter first recognizing the underlying obligation sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) s"
N12-1078,J08-4004,0,0.0165177,"ng 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME , ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where α ≥ 0.8 is considered to be reliable and 0.667 ≤ α < 0.8 allows for tentative conclusions. Two points emerge from Table 1: there were greater agreement rates for sense map than compare, and 3 Turkers were sufficient. 3 compare5 compare3 sense map5 sense map3 Experiment Design Data Selection We used two parallel corpora: the French-English 109 corpus (Callison-Burch et al., 2009) and the GALE Chinese-English corpus. 622 α-Turker 0.47 0.44 0.79 0.75 α-maj. 0.66 0.52 0.93 0.87 maj.-agr. 0.87 0.83 0.95 0.91 Table 1: MTurk result on testing Turker reliability. Krippendorff’s Alpha is used t"
N12-1078,P05-1074,1,0.816043,"signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are translated into the same French word and by further enforcing that the majority sense for these English words was projected as the sense for the French word. Bannard and Callison-Burch (2005) applied the idea that French phrases aligned to the same English phrase are paraphrases in a system that induces paraphrases by pivoting through aligned foreign phrases. Based on this, and other successful prior work, it seems neither of the assumptions must hold universally. Therefore we investigate how often we might expect one or the other to dominate: we sample polysemous words from wide-domain {French,Chinese}-English corpora, and use Amazon’s Mechanical Turk (MTurk) to annotate word sense on the English side. We calculate empirical probabilities based on counting over the competing poly"
N12-1078,E09-1013,0,0.0598667,"Missing"
N12-1078,P02-1033,0,0.0342769,"ticles in which a researcher claimed success. Under the polysemy assumption, Gale et al. (1992) used French translations as English sense indicators in the task of WSD. For instance, for the English word duty, the French translation droit was taken to signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are translated into the same French word and by further enforcing that the majority sense for these English words was projected as the sense for the French word. Bannard and Callison-Burch (2005) applied the idea that French phrases aligned to the same English phrase are paraphrases in a system that induces paraphrases by pivoting through aligned foreign phrases. Based on this, and other successful prior work, it seems neither of the assumptions must hold universally. Therefore we investigate how often we might expect one or t"
N12-1078,1992.tmi-1.9,0,0.582365,"d for both paraphrase induction and word sense disambiguation (WSD). Usually one of the following two assumptions is made for these tasks: 1. Polysemy If two different words in language A are aligned to the same word in language B, then the word in language B is polysemous. 2. Synonymy If two different words in language A are aligned to the same word in language B, then the two words in A are synonyms, and thus is not evidence of polysemy in B. Despite the alternate nature of these assumptions, both have associated articles in which a researcher claimed success. Under the polysemy assumption, Gale et al. (1992) used French translations as English sense indicators in the task of WSD. For instance, for the English word duty, the French translation droit was taken to signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are t"
N12-1078,N06-2015,0,0.0256115,"Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME , ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where α ≥ 0.8 is considered to be reliable and 0.667 ≤"
N12-1078,P11-2055,0,0.13284,"Missing"
N12-1078,W10-0703,0,0.017203,"g the underlying obligation sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the"
N12-1078,W11-0409,0,0.016078,"sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed"
N12-1078,D08-1027,0,0.0213405,"Missing"
N12-1078,W09-0401,1,\N,Missing
N13-1056,W10-0701,1,0.750232,"Missing"
N13-1056,P11-2071,0,0.345504,"Missing"
N13-1056,D12-1025,0,0.525079,"ilarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals proposed in prior work. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) use some supervision to learn how to project contextual and orthographic features into a low-dimensional space, with the goal of representing words which are translations of one another as vectors which are close together in that space. However, both of those appr"
N13-1056,P98-1069,0,0.201564,"metrics derived from monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phr"
N13-1056,P08-1088,0,0.783156,"or the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed"
N13-1056,W10-0717,1,0.869611,"Missing"
N13-1056,2010.amta-papers.12,1,0.853582,"tokens cluding contextual and string similarity, and directly learn how to combine them. 3 Monolingual Data and Signals 3.1 Data Throughout our experiments, we seek to learn how to translate words in a given source language into English. Table 1 lists our languages of interest, along with the total amount of monolingual data that we use for each. We use web crawled timestamped news articles to estimate temporal similarity, Wikipedia pages which are inter-lingually linked to English pages to estimate topic similarity, and both datasets to estimate frequency and contextual similarity. Following Irvine et al. (2010), we use pairs of Wikipedia page titles to train a simple transliterator for languages written in a non-Roman script, which allows us to compute orthographic similarity for pairs of words in different scripts. 3.2 Signals Our definitions of orthographic, topic, temporal, and contextual similarity are taken from Klementiev et al. (2012), and the details of each may be found there. Here, we give briefly describe them and give our definition of a novel, frequency-based signal. Orthographic We measure orthographic similarity between a pair of words as the normalized1 edit distance between the two"
N13-1056,P06-1103,0,0.632535,"ributional properties across languages. Prior research has shown that contextual similarity (Rapp, 1995), temporal similarity (Schafer and Yarowsky, 2002), and topical information (Mimno et al., 2009) ⇤ Performed while faculty at Johns Hopkins University Chris Callison-Burch⇤ Computer and Information Science Dept. University of Pennsylvania are all good signals for learning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of machine learning has shown decisively that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney, 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp, 1995), and that same dictionary may be used for est"
N13-1056,E12-1014,1,0.785974,"al signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals"
N13-1056,W02-0902,0,0.94214,"phic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to t"
N13-1056,D09-1092,0,0.0747002,"ore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon f"
N13-1056,P12-1017,0,0.236935,"al lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals proposed in prior work. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) use some supervision to learn how to project contextual and orthographic features into a low-dimensional space, with the goal of representing words which are translations of one another as vectors which are close together in that space. However, both of those approaches focus on only two"
N13-1056,P02-1038,0,0.145194,"ning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of machine learning has shown decisively that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney, 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp, 1995), and that same dictionary may be used for estimating the parameters of a model to combine monolingual signals. Alternatively, in a low resource machine translation (MT) setting, it is reasonable to assume a small amount of parallel data from which a bilingual dictionary can be extracted for supervision. In this setting, bilingual lexicon induction is critical for translating source words which do not appea"
N13-1056,P95-1050,0,0.799215,"g similarity metrics derived from monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked t"
N13-1056,P99-1067,0,0.462435,"m monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based tr"
N13-1056,P11-1002,0,0.0364821,"l, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals proposed in prior work. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) use some supervision to learn how to project contextual and orthographic features into a low-dimensional space, with the goal of representing words which are translations of one another as vectors which are close together in that space. Howe"
N13-1056,W02-2026,0,0.86939,"l corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolin"
N13-1056,C98-1066,0,\N,Missing
N13-1092,P05-1074,1,0.55111,"riety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, jailed, locked up, taken into custody, and thrown into prison, along with a set of incorrect/noisy paraphrases that have different syntactic types or that are due to misalignments. For PPDB, we formula"
N13-1092,P11-1062,0,0.00591233,"and languages supported. Furthermore, we intend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource. Finally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012). Acknowledgements We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or end"
N13-1092,P08-1077,0,0.016869,"-sets that rely on dependency and constituency parses, part-of-speech tags, or lemmatization have been proposed in work such as by Church and Hanks (1991) and Lin and Pantel (2001). For instance, a phrase is described by the various syntactic relations such as: “what verbs have this phrase as the subject?”, or “what adjectives modify this phrase?”. Other work has used simpler n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?”. A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). For PPDB, we compute n-gram-based context signatures for the 200 million most frequent phrases in the Google n-gram corpus (Brants and Franz, 2006; Lin et al., 2010), and richer linguistic signatures for 175 million phrases in the Annotated Gigaword corpus (Napoles et al., 2012). Our features extend beyond those previously used in the work by Ganitkevitch et al. (2012). They are: 760 • Incoming and outgoing (wrt. the phrase) dependency link features, labeled with the corresponding lexical item, lemmata and POS. • Syntactic features for any constit"
N13-1092,W11-2504,1,0.357316,"Missing"
N13-1092,P05-1033,0,0.0143651,"me foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, jailed, locked up, taken into custody, and thrown into prison, along with a set of incorrect/noisy paraphrases that have different syntactic types or that are due to misalignments. For PPDB, we formulate our paraphrase collection as a weighted synchronous context-free grammar (SCFG) (Aho and Ullman, 1972; Chiang, 2005) 1 Freely available at http://paraphrase.org. 758 Proceedings of NAACL-HLT 2013, pages 758–764, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics ... 5 farmers were thrown into jail The paraphrase rules obtained using this method are capable of making well-formed generalizations of meaning-preserving rewrites in English. For instance, we extract the following example paraphrase, capturing the English possessive rule: in Ireland ... ... fünf Landwirte festgenommen , weil ... ... oder wurden festgenommen , gefoltert ... NP → the NP 1 of NNS 2 |the NNS2 ’s NP 1 ."
N13-1092,C08-1018,0,0.0578526,"utational Linguistics ... 5 farmers were thrown into jail The paraphrase rules obtained using this method are capable of making well-formed generalizations of meaning-preserving rewrites in English. For instance, we extract the following example paraphrase, capturing the English possessive rule: in Ireland ... ... fünf Landwirte festgenommen , weil ... ... oder wurden festgenommen , gefoltert ... NP → the NP 1 of NNS 2 |the NNS2 ’s NP 1 . ... or have been imprisoned , tortured ... Figure 1: Phrasal paraphrases are extracted via bilingual pivoting. with syntactic nonterminal labels, similar to Cohn and Lapata (2008) and Ganitkevitch et al. (2011). An SCFG rule has the form: def r = C → hf, e, ∼, ϕ ~ i, where the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } which are combined in a log-linear model (with weights ~λ) to compute the cost of applying r: cost(r) = − N X λi log ϕi . (1) i=1 To"
N13-1092,C04-1051,0,0.862531,"dition to the paraphrase collection itself, we provide tools to filter PPDB to only retain high precision paraphrases, scripts to limit the collection to phrasal or lexical paraphrases (synonyms), and software that enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus piv"
N13-1092,eisele-chen-2010-multiun,0,0.097034,"Missing"
N13-1092,D11-1108,1,0.723807,"Missing"
N13-1092,S12-1034,1,0.722511,"Missing"
N13-1092,kingsbury-palmer-2002-treebank,0,0.282654,"st S. This allows us to apply sophisticated paraphrases to the predicate while capturing its arguments in a generalized fashion. tated with distributional similarity scores based on lexical features collected from the Spanish portion of the multilingual release of the Google n-gram corpus (Brants and Franz, 2009), and the Spanish Gigaword corpus (Mendonca et al., 2009). Table 2 gives a breakdown of PPDB:Spa. 6 Analysis To estimate the usefulness of PPDB as a resource for tasks like semantic role labeling or parsing, we analyze its coverage of Propbank predicates and predicate-argument tuples (Kingsbury and Palmer, 2002). We use the Penn Treebank (Marcus et al., 1993) to map Propbank annotations to patterns which allow us to search PPDB:Eng for paraphrases that match the annotated predicate. Figure 3 illus50 Avg. Score 0 -30 Coverage 100 0.5 0.8 PP / Type Coverage 150 0 -25 -20 -15 -10 -5 0 5 140 120 100 0.6 80 0.4 60 40 0.2 20 3 1 -30 160 Relation Tokens Covered Paraphrases / Type Relation Types Covered Paraphrases / Type 1 1 0 -30 -25 -20 -15 -10 -5 0 0 -25 -20 -15 -10 -5 0 Pruning Threshold Pruning Threshold (a) PPDB:Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for v"
N13-1092,W07-0733,0,0.0117205,"Missing"
N13-1092,2005.mtsummit-papers.11,0,0.265202,"Missing"
N13-1092,J10-4005,0,0.00651194,"strings of terminal and nonterminal symbols. There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } which are combined in a log-linear model (with weights ~λ) to compute the cost of applying r: cost(r) = − N X λi log ϕi . (1) i=1 To create a syntactic paraphrase grammar we first extract a foreign-to-English translation grammar from a bilingual parallel corpus, using techniques from syntactic machine translation (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match: def r1 = C → hf, e1 , ∼1 , ϕ ~ 1i def r2 = C → hf, e2 , ∼2 , ϕ ~ 2 i, we pivot over f to create a paraphrase rule rp : def rp = C → he1 , e2 , ∼p , ϕ ~ p i, with a combined nonterminal correspondency function ∼p . Note that the common source side f implies that e1 and e2 share the same set of nonterminal symbols. 759 The paraphrase feature vector ϕ ~ p is computed from the translation feature vectors ϕ ~ 1 and ϕ ~ 2 by following the pivoting idea. For instance, we estimate the conditional paraphra"
N13-1092,lin-etal-2010-new,0,0.0125111,"nd constituency parses, part-of-speech tags, or lemmatization have been proposed in work such as by Church and Hanks (1991) and Lin and Pantel (2001). For instance, a phrase is described by the various syntactic relations such as: “what verbs have this phrase as the subject?”, or “what adjectives modify this phrase?”. Other work has used simpler n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?”. A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). For PPDB, we compute n-gram-based context signatures for the 200 million most frequent phrases in the Google n-gram corpus (Brants and Franz, 2006; Lin et al., 2010), and richer linguistic signatures for 175 million phrases in the Annotated Gigaword corpus (Napoles et al., 2012). Our features extend beyond those previously used in the work by Ganitkevitch et al. (2012). They are: 760 • Incoming and outgoing (wrt. the phrase) dependency link features, labeled with the corresponding lexical item, lemmata and POS. • Syntactic features for any constituents governing th"
N13-1092,J93-2004,0,0.0536204,"to the predicate while capturing its arguments in a generalized fashion. tated with distributional similarity scores based on lexical features collected from the Spanish portion of the multilingual release of the Google n-gram corpus (Brants and Franz, 2009), and the Spanish Gigaword corpus (Mendonca et al., 2009). Table 2 gives a breakdown of PPDB:Spa. 6 Analysis To estimate the usefulness of PPDB as a resource for tasks like semantic role labeling or parsing, we analyze its coverage of Propbank predicates and predicate-argument tuples (Kingsbury and Palmer, 2002). We use the Penn Treebank (Marcus et al., 1993) to map Propbank annotations to patterns which allow us to search PPDB:Eng for paraphrases that match the annotated predicate. Figure 3 illus50 Avg. Score 0 -30 Coverage 100 0.5 0.8 PP / Type Coverage 150 0 -25 -20 -15 -10 -5 0 5 140 120 100 0.6 80 0.4 60 40 0.2 20 3 1 -30 160 Relation Tokens Covered Paraphrases / Type Relation Types Covered Paraphrases / Type 1 1 0 -30 -25 -20 -15 -10 -5 0 0 -25 -20 -15 -10 -5 0 Pruning Threshold Pruning Threshold (a) PPDB:Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for varying pruning thresholds. (b) PPDB:Eng’s covera"
N13-1092,D12-1104,0,0.0522494,"tend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource. Finally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012). Acknowledgements We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government. Ref"
N13-1092,W12-3018,1,0.636441,"Missing"
N13-1092,W04-3219,0,0.206887,"rase collection itself, we provide tools to filter PPDB to only retain high precision paraphrases, scripts to limit the collection to phrasal or lexical paraphrases (synonyms), and software that enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract"
N13-1092,P07-1059,0,0.0790247,"enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, imprisoned, incarcerated, j"
N13-1092,P06-1101,0,0.015993,"ion’s coverage with regard to both data size and languages supported. Furthermore, we intend to improve paraphrase scoring by incorporating additional sources of information, as well as by better utilizing information present in the data, like domain or topic. We will also address points of refinement such as handling of phrase ambiguity, and effects specific to individual pivot languages. Our aim is for PPDB to be a continuously updated and improving resource. Finally, we will explore extensions to PPDB to include aspects of related large-scale resources such as lexical-semantic hierarchies (Snow et al., 2006), textual inference rules (Berant et al., 2011), relational patterns (Nakashole et al., 2012), and (lexical) conceptual networks (Navigli and Ponzetto, 2012). Acknowledgements We would like to thank Frank Ferraro for his Propbank processing tools. This material is based on research sponsored by the NSF under grant IIS-1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be inter"
N13-1092,P10-2043,1,0.223784,"Missing"
N13-1092,P08-1116,0,0.0561042,"ert ... NP → the NP 1 of NNS 2 |the NNS2 ’s NP 1 . ... or have been imprisoned , tortured ... Figure 1: Phrasal paraphrases are extracted via bilingual pivoting. with syntactic nonterminal labels, similar to Cohn and Lapata (2008) and Ganitkevitch et al. (2011). An SCFG rule has the form: def r = C → hf, e, ∼, ϕ ~ i, where the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-toone correspondence, ∼, between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. Following Zhao et al. (2008), each rule r is annotated with a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } which are combined in a log-linear model (with weights ~λ) to compute the cost of applying r: cost(r) = − N X λi log ϕi . (1) i=1 To create a syntactic paraphrase grammar we first extract a foreign-to-English translation grammar from a bilingual parallel corpus, using techniques from syntactic machine translation (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match: def r1 = C → hf, e1 , ∼1 , ϕ ~ 1i def r2 = C → hf, e2 , ∼2 , ϕ ~ 2 i, we pivot over f to creat"
N13-1092,N06-1057,0,0.00512664,", and software that enables users to extract paraphrases for languages other than English. 2 Introduction Paraphrases, i.e. differing textual realizations of the same meaning, have proven useful for a wide variety of natural language processing applications. Past paraphrase collections include automatically derived resources like DIRT (Lin and Pantel, 2001), the MSR paraphrase corpus and phrase table (Dolan et al., 2004; Quirk et al., 2004), among others. Although several groups have independently extracted paraphrases using Bannard and CallisonBurch (2005)’s bilingual pivoting technique (see Zhou et al. (2006), Riezler et al. (2007), Snover et al. (2010), among others), there has never been an official release of this resource. Extracting Paraphrases from Bitexts To extract paraphrases we follow Bannard and Callison-Burch (2005)’s bilingual pivoting method. The intuition is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. We can thus pivot over f and extract he1 , e2 i as a pair of paraphrases, as illustrated in Figure 1. The method extracts a diverse set of paraphrases. For thrown into jail, it extracts arrested, detained, impr"
N13-1092,J90-1003,0,\N,Missing
N13-1092,steinberger-etal-2006-jrc,0,\N,Missing
N13-1092,W09-0401,1,\N,Missing
N13-1106,P08-1081,0,0.00871982,"Missing"
N13-1106,N03-2010,0,0.00784703,"type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the type of answers that are mostly associated with w"
N13-1106,N10-1145,0,0.87532,"ake public (together with the software) to the community.3 Related prior work is interspersed throughout the paper. Feature distance renNoun renVerb renOther insN, insV, insPunc, insDet, insOtherPos delN, delV, ... ins{N,V,P}Mod insSub, insObj insOtherRel delNMod, ... renNMod, ... XEdits alignNodes, alignNum, alignN, alignV, alignProper Tree Edit Distance Model Tree Edit Distance (§2.1) models have been shown effective in a variety of applications, including textual entailment, paraphrase identification, answer ranking and information retrieval (Reis et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Augsten et al., 2010). We chose the variant proposed by Heilman and Smith (2010), inspired by its simplicity, generality, and effectiveness. Our approach differs from those authors in their reliance on a greedy search routine to make use of a complex tree kernel. With speed a consideration, we opted for the dynamic-programming solution of Zhang and Shasha (1989) (§2.1). We added new lexicalsemantic features §(2.2) to the model and then evaluated our implementation on the QASR task, showing strong results §(2.3). 859 # edits inserting a noun, verb, punctuation mark, determiner or other POS ty"
N13-1106,W01-1203,0,0.0196754,"ow questions, even though they do not indicate an answer type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most"
N13-1106,C02-1150,0,0.249239,"n though they do not indicate an answer type as clearly as how much does. Some extra features are designed for what/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the typ"
N13-1106,P02-1054,0,0.0315697,"Missing"
N13-1106,P05-1012,0,0.00925047,"against the TREC answer pattern (in the form of Perl regular expressions). If a sentence matched, then it was deemed a (noisy) positive example. Finally, TRAIN, DEV and TEST were manually corrected for errors. Those authors decided to limit candidate source sen861 tences to be no longer than 40 words.7 Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation. The data was processed by Wang et al. (2007) with the following tool chain: POS tags via MXPOST (Ratnaparkhi, 1996); parse trees via MSTParser (McDonald et al., 2005) with 12 coarsegrained dependency relation labels; and named entities via Identifinder (Bikel et al., 1999). Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in Table 3. Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.8 3 CRF with TED for Answer Extraction In this section we move from ranking source sentences, to the next QA stage: answer extraction. Given our competitive TED-based alignment model, the most obvious solution to extraction would be to report those spans aligned from"
N13-1106,P07-1098,0,0.0123631,"hat/which questions per required answer types. The question dependency tree is analyzed and the Lexical Answer Type (LAT) is extracted. The following are some examples of LAT for what questions: • color: what is Crips’ gang color? • animal: what kind of animal is an agouti? The extra LAT=? feature is also used with chunking features for what/which questions. There is significant prior work in building specialized templates or classifiers for labeling question types (Hermjakob, 2001; Li and Roth, 2002; Zhang and Lee, 2003; Hacioglu and Ward, 2003; Metzler and Croft, 2005; Blunsom et al., 2006; Moschitti et al., 2007). We designed our shallow question type features based on the intuitions of these prior work, with the goal of having a relatively compact approach that still extracts useful predictive signal. One possible drawback, however, is that if an LAT is not observed during training but shows up in testing, the sequence tagger would not know which answer type to associate with the question. In this case it falls back to the more general qword=? feature and will most likely pick the type of answers that are mostly associated with what questions in training. Edit script Our TED module produces an edit t"
N13-1106,W96-0213,0,0.0416997,"the task collection, which were then compared against the TREC answer pattern (in the form of Perl regular expressions). If a sentence matched, then it was deemed a (noisy) positive example. Finally, TRAIN, DEV and TEST were manually corrected for errors. Those authors decided to limit candidate source sen861 tences to be no longer than 40 words.7 Keeping with prior work, those questions with only positive or negative examples were removed, leaving 94 of the original 100 questions for evaluation. The data was processed by Wang et al. (2007) with the following tool chain: POS tags via MXPOST (Ratnaparkhi, 1996); parse trees via MSTParser (McDonald et al., 2005) with 12 coarsegrained dependency relation labels; and named entities via Identifinder (Bikel et al., 1999). Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported in Table 3. Our implementation gives state of the art performance, and is furthered improved by our inclusion of semantic features drawn from WordNet.8 3 CRF with TED for Answer Extraction In this section we move from ranking source sentences, to the next QA stage: answer extraction. Given our competitive TED-based alignment model, the most obvious solution to extr"
N13-1106,N03-2029,0,0.0254606,"n an aligned adjective. Alignment distance We observed that a candidate answer often appears close to an aligned word (i.e., answer tokens tend to be located “nearby” portions of text that align across the pair), especially in compound noun constructions, restrictive clauses, preposition phrases, etc. For instance, in the following pair, the answer Limp Bizkit comes from the leading compound noun: 863 • What is the name of Durst ’s group? • Limp Bizkit lead singer Fred Durst did a lot ... Past work has designed large numbers of specific templates aimed at these constructions (Soubbotin, 2001; Ravichandran et al., 2003; Clark et al., 2003; Sneiders, 2002). Here we use a single general feature that we expect to pick up much of this signal, without the significant feature engineering. Thus we incorporated a simple feature to roughly model this phenomenon. It is defined as the distance to the nearest aligned nonstop word in the original word order. In the above example, the only aligned nonstop word is Durst. Then this nearest alignment distance feature for the word Limp is: nearest dist to align(Limp):5 This is the only integer-valued feature. All other features are binary-valued. Note this feature does not s"
N13-1106,W06-3104,0,0.00950297,"ractice this gave stopwords “too much say” in guiding the overall edit sequence. The resultant system is fast in practice, processing 10,000 pre-parsed tree pairs per second on a contemporary machine.6 2.2 TED for Sentence Ranking The task of Question Answer Sentence Ranking (QASR) takes a question and a set of source sentences, returning a list sorted by the probability likelihood that each sentence contains an appropriate answer. Prior work in this includes that of: Punyakanok et al. (2004), based on mapping syntactic dependency trees; Wang et al. (2007) utilizing Quasi-Synchronous Grammar (Smith and Eisner, 2006); Heilman and Smith (2010) using TED; and Shima et al. (2008), Ding et al. (2008) and Wang and Manning (2010), who each employed a CRF in various ways. Wang et al. (2007) made their dataset public, which we use here for system validation. To date, models based on TED have shown the best performance for this task. Our implementation follows Heilman and Smith (2010), with the addition of 15 new features beyond their original 33 (see Table 1). Based on results 6 In later tasks, feature extraction and decoding will slow down the system, but the final system was still able to process 200 pairs per"
N13-1106,C10-1131,0,0.743788,"Missing"
N13-1106,D07-1003,0,0.439691,"erences, tagging or parsing errors. 860 tent terms. In practice this gave stopwords “too much say” in guiding the overall edit sequence. The resultant system is fast in practice, processing 10,000 pre-parsed tree pairs per second on a contemporary machine.6 2.2 TED for Sentence Ranking The task of Question Answer Sentence Ranking (QASR) takes a question and a set of source sentences, returning a list sorted by the probability likelihood that each sentence contains an appropriate answer. Prior work in this includes that of: Punyakanok et al. (2004), based on mapping syntactic dependency trees; Wang et al. (2007) utilizing Quasi-Synchronous Grammar (Smith and Eisner, 2006); Heilman and Smith (2010) using TED; and Shima et al. (2008), Ding et al. (2008) and Wang and Manning (2010), who each employed a CRF in various ways. Wang et al. (2007) made their dataset public, which we use here for system validation. To date, models based on TED have shown the best performance for this task. Our implementation follows Heilman and Smith (2010), with the addition of 15 new features beyond their original 33 (see Table 1). Based on results 6 In later tasks, feature extraction and decoding will slow down the system,"
N15-1072,W10-0710,0,0.0588893,"Missing"
N15-1072,W10-0701,1,0.766567,"s, translations produced via crowdousrcing may be low quality. Previous work has addressed this problem, showing that non-professional translators hired on Amazon Mechanical Turk (MTurk) can achieve professional-level quality, by soliciting multiple translations of each source sentence and then choosing the best translation (Zaidan and CallisonBurch, 2011). Introduction Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost. Many NLP researchers have started creating speech and language data through crowdsourcing (for example, Snow et al. (2008), Callison-Burch and Dredze (2010) and others). One NLP application that has been the focus of crowdsourced data collection is statistical machine translation (SMT) In this paper we focus on a different aspect of crowdsourcing than Zaidan and Callison-Burch (2011). We attempt to achieve the same high quality while minimizing the associated costs. We propose two complementary methods: (1) We reduce the number of translations that we solicit for each source sentence. Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one. We do so by building"
N15-1072,N10-1024,1,0.844983,"performance can be identified and blocked. Lin et al. (2014) examined the relationship between worker accuracy and budget in the context of using crowdsourcing to train a machine learning classifier. They show that if the goal is to train a classifier on the labels, that the properties of the classifier will determine whether it is better to re-label data (resulting in higher quality labels) or get more single labeled items (of lower quality). They showed that classifiers with weak inductive bias benefit more from relabeling, and that relabeling is more important when worker accuracy is low. Novotney and Callison-Burch (2010) showed a similar result for training an automatic speech recognition (ASR) system. When creating training data for an ASR system, given a fixed budget, their system’s accuracy was higher when it is trained on more low quality transcription data compared to when it was trained on fewer high quality transcriptions. 8 Conclusion In this paper, we propose two mechanisms to optimize cost: a translation reducing method and a translator reducing method. They have different applicable scenarios for large corpus construction. The translation reducing method works if there exists a specific requirement"
N15-1072,P03-1021,0,0.229126,"Missing"
N15-1072,P02-1040,0,0.103316,"Missing"
N15-1072,W13-2323,0,0.0121257,"l from noisy labels. We cannot always get high-quality labeled data from crowdsourcing, but we can still ensure that a model trained on the data is accurate by redundantly labeling the data. Sheng et al. (2008) proposed a framework for repeated-labeling that resolves the uncertainty in labeling via majority voting. The experimental results show that a model’s accuracy is improved even if labels in its training data are noisy and imperfect. As long as the integrated quality (the probability of the integrated labeling being correct) is higher than 0.5, repeated labeling benefits model training. Passonneau and Carpenter (2013) created a Bayesian model of annotation. They applied it to the problem of word sense annotation. Passonneau and Carpenter (2013) also proposed an approach to detect and avoid spam workers. They measured the performance of worker by comparing worker’s labels to the current majority labels. Workers with bad performance can be identified and blocked. Lin et al. (2014) examined the relationship between worker accuracy and budget in the context of using crowdsourcing to train a machine learning classifier. They show that if the goal is to train a classifier on the labels, that the properties of th"
N15-1072,W12-3152,1,0.872422,"Missing"
N15-1072,D08-1027,0,0.034917,"Missing"
N15-1072,P11-1122,1,0.903411,"soliciting multiple translations of each source sentence and then choosing the best translation (Zaidan and CallisonBurch, 2011). Introduction Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost. Many NLP researchers have started creating speech and language data through crowdsourcing (for example, Snow et al. (2008), Callison-Burch and Dredze (2010) and others). One NLP application that has been the focus of crowdsourced data collection is statistical machine translation (SMT) In this paper we focus on a different aspect of crowdsourcing than Zaidan and Callison-Burch (2011). We attempt to achieve the same high quality while minimizing the associated costs. We propose two complementary methods: (1) We reduce the number of translations that we solicit for each source sentence. Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one. We do so by building models to distinguish between 705 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 705–713, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Ling"
N15-1072,N12-1006,1,0.90294,"Missing"
N15-1072,N13-1069,0,0.0475018,"Missing"
N16-1172,2010.iwslt-papers.2,0,0.109031,"raphrase set P , it constructs a graph G = (V, E) where vertices V = {pi ∈ P } are words in the paraphrase set and edges connect words that share foreign word alignments in a bilingual parallel corpus. The edges of the graph are weighted based on their contextual similarity (computed over a monolingual corpus). In order to partition the graph into clusters, edges in the initial graph G with contextual similarity below a threshold T 0 are deleted. The connected components in the resulting graph G0 are taken as the sense clusters. The threshold is dynamically tuned using an iterative procedure (Apidianaki and He, 2010). As evaluated against reference clusters derived from SEMEVAL 2007 Lexical Substitution gold data (McCarthy and Navigli, 2007), their method, which we call SEMCLUST, outperformed simple most-frequent-sense, one-sense-per-paraphrase, and random baselines. Apidianaki et al. (2014)’s work corroborated the existence of sense distinctions in the paraphrase sets, and highlighted the need for further work to organize them by sense. In this paper, we improve on their method using more advanced clustering algorithms, and by systematically exploring a wider range of similarity measures. 3 Graph Cluster"
N16-1172,apidianaki-etal-2014-semantic,0,0.661342,"ches to WSI aims to discover the senses of a word by clustering the monolingual contexts in which it appears (Navigli, 2009). Another uncovers a word’s senses by clustering its foreign alignments from parallel corpora (Diab, 2003). A more recent family of approaches to WSI represents a word as a feature vector of its substitutable words, i.e. paraphrases (Melamud et al., 2015; Yatbaz et al., 2012). In this paper we take inspiration from each of these families of approaches, and we explore them when measuring word similarity in sense clustering. The work most closely related to ours is that of Apidianaki et al. (2014), who used a simple graphbased approach to cluster pivot paraphrases on the basis of contextual similarity and shared foreign alignments. Their method represents paraphrases as nodes in a graph and connects each pair of words sharing one or more foreign alignments with an edge weighted by contextual similarity. Concretely, for paraphrase set P , it constructs a graph G = (V, E) where vertices V = {pi ∈ P } are words in the paraphrase set and edges connect words that share foreign word alignments in a bilingual parallel corpus. The edges of the graph are weighted based on their contextual simil"
N16-1172,P05-1074,1,0.838418,"phrases of the noun bug would yield a single list of paraphrases that includes insect, glitch, beetle, error, microbe, wire, cockroach, malfunction, microphone, mosquito, virus, tracker, pest, informer, snitch, parasite, bacterium, fault, mistake, failure and many others. The goal of this work is to group these paraphrases into clusters that denote the distinct senses of the input word or phrase, as shown in Figure 1. We develop a method for clustering the paraphrases from the Paraphrase Database (PPDB). PPDB contains over 100 million paraphrases generated using the bilingual pivoting method (Bannard and Callison-Burch, 2005), which posits that two English words are potential paraphrases of each other if they share one or more foreign translations. We apply two clustering algorithms, Hierarchical Graph Factorization Clustering (Yu et al., 2005; Sun and Korhonen, 2011) and Self-Tuning Spectral Clustering (Ng et al., 2001; Zelnik-Manor and Perona, 2004), and systematically explore different ways of defining the similarity matrix that they use as input. We exploit a variety of features from PPDB to cluster its paraphrases by sense, including its im1463 Proceedings of NAACL-HLT 2016, pages 1463–1472, c San Diego, Cali"
N16-1172,D08-1021,1,0.752611,"Missing"
N16-1172,C04-1051,0,0.0829291,"that it probabilistically assigns each paraphrase to a cluster at each level of the hierarchy. If some pi has high probability in multiple clusters, we can assign pi to all of them (Figure 3c). 3.2 Spectral Clustering The second clustering algorithm that we use is SelfTuning Spectral Clustering (Zelnik-Manor and Perona, 2004). Like HGFC, spectral clustering takes an adjacency matrix W as input, but the similarities end there. Whereas HGFC produces a hierarchical clustering, spectral clustering produces a flat clustering with k clusters, with k specified at runtime. The Zelnik-Manor and Perona (2004)’s selftuning method is based on Ng et al. (2001)’s spectral clustering algorithm, which computes a normalized Laplacian matrix L from the input W , and executes K-means on the largest k eigenvectors of L. Intuitively, the largest k eigenvectors of L should align with the k senses in our paraphrase set. 4 Similarity Measures Each of our clustering algorithms take as input an adjacency matrix W where the entries wij correspond to some measure of similarity between words i and j. For the paraphrases in Figure 1, W is a 20x20 matrix that specifies the similarity of every pair of paraphrases like"
N16-1172,1992.tmi-1.9,0,0.442265,"f the k th element of vip equals P P DB2.0 Score(i, k). We can then calculate the cosine similarity or Jensen-Shannon divergence between vectors: simP P DB.cos (i, j) = cos(vip , vjp ) 1466 simP P DB.js (i, j) = 1 − JS(vip , vjp ) where JS(vip , vjp ) is calculated assuming that the paraphrase probability distribution for word i is given by its normalized word-paraphrase vector vip . 4.3 Similarity of Foreign Word Alignments When an English word is aligned to several foreign words, sometimes those different translations indicate a different word sense (Yao et al., 2012). Using this intuition, Gale et al. (1992) trained an English WSD system on a bilingual corpus, using the different French translations as labels for the English word senses. For instance, given the English word duty, the French translation droit was a proxy for its tax sense and devoir for its obligation sense. PPDB is derived from bilingual coropra. We recover the aligned foreign words and their associated translation probabilities that underly each PPDB entry. For each English word in our dataset, we get each foreign word that it aligns to in the Spanish and Chinese bilingual parallel corpora used by Ganitkevitch and Callison-Burch"
N16-1172,ganitkevitch-callison-burch-2014-multilingual,1,0.818605,"s intuition, Gale et al. (1992) trained an English WSD system on a bilingual corpus, using the different French translations as labels for the English word senses. For instance, given the English word duty, the French translation droit was a proxy for its tax sense and devoir for its obligation sense. PPDB is derived from bilingual coropra. We recover the aligned foreign words and their associated translation probabilities that underly each PPDB entry. For each English word in our dataset, we get each foreign word that it aligns to in the Spanish and Chinese bilingual parallel corpora used by Ganitkevitch and Callison-Burch (2014). We use this to define a novel foreign word alignment similarity metric, simT RAN S (i, j) for two English paraphrases i and j. This is calculated as the cosine similarity of the word-alignment vectors via and vja where each feature in v a is a foreign word to which i or j aligns, a is the translation probabiland the value of entry vif ity p(f |i). simT RAN S (i, j) = 4.4 cos(via , vja ) Monolingual Distributional Similarity Lastly, we populate the adjacency with a distributional similarity measure based on WORD 2 VEC (Mikolov et al., 2013). Each paraphrase i in our data set is represented as"
N16-1172,N13-1092,1,0.92448,"Missing"
N16-1172,S07-1009,0,0.18319,"nect words that share foreign word alignments in a bilingual parallel corpus. The edges of the graph are weighted based on their contextual similarity (computed over a monolingual corpus). In order to partition the graph into clusters, edges in the initial graph G with contextual similarity below a threshold T 0 are deleted. The connected components in the resulting graph G0 are taken as the sense clusters. The threshold is dynamically tuned using an iterative procedure (Apidianaki and He, 2010). As evaluated against reference clusters derived from SEMEVAL 2007 Lexical Substitution gold data (McCarthy and Navigli, 2007), their method, which we call SEMCLUST, outperformed simple most-frequent-sense, one-sense-per-paraphrase, and random baselines. Apidianaki et al. (2014)’s work corroborated the existence of sense distinctions in the paraphrase sets, and highlighted the need for further work to organize them by sense. In this paper, we improve on their method using more advanced clustering algorithms, and by systematically exploring a wider range of similarity measures. 3 Graph Clustering Algorithms To partition paraphrases by sense, we use two advanced graph clustering methods rather than using Apidianaki et"
N16-1172,N15-1050,0,0.0373991,"Missing"
N16-1172,P15-2070,1,0.905761,"Missing"
N16-1172,P15-1146,1,0.892575,"Missing"
N16-1172,P15-2067,1,0.899375,"Missing"
N16-1172,2003.mtsummit-papers.42,0,0.256045,"Missing"
N16-1172,D07-1043,0,0.0923965,". Both were used in the 2010 SemEval Word Sense Induction Task (Manandhar et al., 2010) and by Apidianaki et al. (2014). We give our results in terms of weighted average performance on these metrics, where the score for each individual paraphrase set is weighted by the number of reference clusters for that query word. Paired F-Score frames the clustering problem as a classification task (Manandhar et al., 2010). It genF (K)∩F (S) , F (S) and F = 2·P ·R P +R . V-Measure assesses the quality of a clustering solution against reference clusters in terms of clustering homogeneity and completeness (Rosenberg and Hirschberg, 2007). Homogeneity describes the extent to which each cluster is composed of paraphrases belonging to the same reference cluster, and completeness refers to the extent to which points in a reference cluster are assigned to a single cluster. Both are defined in terms of conditional entropy. VMeasure is the harmonic mean of homogeneity h and completeness c; V-Measure = 2·h·c h+c . 7.3 Baselines 0.5 One Cluster per Paraphrase (1 C 1 PAR) assigns each paraphrase pi ∈ P to its own cluster. By definition, the homogeneity of 1 C 1 PAR clustering is 1. Random (RAND) For each query term’s paraphrase set, we"
N16-1172,D11-1095,0,0.152484,"nd many others. The goal of this work is to group these paraphrases into clusters that denote the distinct senses of the input word or phrase, as shown in Figure 1. We develop a method for clustering the paraphrases from the Paraphrase Database (PPDB). PPDB contains over 100 million paraphrases generated using the bilingual pivoting method (Bannard and Callison-Burch, 2005), which posits that two English words are potential paraphrases of each other if they share one or more foreign translations. We apply two clustering algorithms, Hierarchical Graph Factorization Clustering (Yu et al., 2005; Sun and Korhonen, 2011) and Self-Tuning Spectral Clustering (Ng et al., 2001; Zelnik-Manor and Perona, 2004), and systematically explore different ways of defining the similarity matrix that they use as input. We exploit a variety of features from PPDB to cluster its paraphrases by sense, including its im1463 Proceedings of NAACL-HLT 2016, pages 1463–1472, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics parasite microbe beetle insect bacterium pest mosquito cockroach bug (n) virus error glitch malfunction snitch fault informer wire failure mistake tracker microphone Figure 2"
N16-1172,N12-1078,1,0.898774,"Missing"
N16-1172,D12-1086,0,0.0212671,"Missing"
N16-1172,S10-1011,0,\N,Missing
N16-3013,W03-1004,0,0.0785871,"tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993)), “simple” English (the Britannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We systematically varied the weights of the Gigaword LM and the PPDB 2.0 score features and selected values that yielded the best output as judged by the authors. The parameters selected for the generic language packs are weightlm = 10 and weightppdb2 = 15, with all other weights are set to zero. Example output is shown in Table 1. 4 User customization The language packs include configuration files with pre-determined weights that can be used on their own or as a jumping-off point for custom configurations. There are weights for eac"
N16-3013,ganitkevitch-callison-burch-2014-multilingual,1,0.907711,"Missing"
N16-3013,D11-1108,1,0.889367,"Missing"
N16-3013,W12-3134,1,0.836518,"1 The components of the language packs are described below. 1 http://joshua-decoder.com/ language-packs/paraphrase/ 62 Proceedings of NAACL-HLT 2016 (Demonstrations), pages 62–66, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Grammar Our approach to sentential paraphrasing is analogous to machine translation. As a translation grammar, we use PPDB 2.0, which contains 170-million lexical, phrasal, and syntactic paraphrases (Pavlick et al., 2015). Each language pack contains a PPDB grammar that has been packed into a binary form for faster computation (Ganitkevitch et al., 2012), and users can select which size grammar to use. The rules present in each grammar are determined by the PPDB 2.0 score, which indicates the paraphrase quality (as given by a supervised regression model) and correlates strongly with human judgments of paraphrase appropriateness (Pavlick et al., 2015). Grammars of different sizes are created by changing the paraphrase score thresholds; larger grammars therefore contain a wider diversity of paraphrases, but with lower confidences. Features Each paraphrase in PPDB 2.0 contains 44 features, described in Ganitkevitch and CallisonBurch (2014) and P"
N16-3013,N13-1092,1,0.877691,"Missing"
N16-3013,J10-3003,0,0.0799258,"e available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ phrase grammar for sentence compression (Ganitkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various other MT approaches have been used for generating sentence simplifications, however none of these used a general-purpose paraphrase grammar (Narayan and Gardent, 2014; Wubben et al., 2012, among others). Another application of sentential paraphrases is to expand multiple reference sets for machine translation (Madnani and Dorr, 2010). PPDB has been used for many tasks, including recognizing textual entailment, question generation, and measuring semantic similarity. These language packs were inspired by the foreign language packs released with Joshua 6 (Post et al., 2015). 6 We have presented a black box for generating sentential paraphrases: PPDB language packs. The language packs include everything necessary for generation, so that they can be downloaded and invoked with a single command. This toolkit can be used for Related work Previous work has applied machine translation techniques to monolingual sentence rewriting t"
N16-3013,J93-2004,0,0.053127,"d in Xu et al. (2016), and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993)), “simple” English (the Britannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We systematically varied the weights of the Gigaword LM and the PPDB 2.0 score features and selected values that yielded the best output as judged by the authors. The parameters selected for the generic language packs are weightlm = 10 and weightppdb2 = 15, with all other weights are set to zero. Example output is shown in Table 1. 4 User customization The language packs include configuration files with pre-determined weights that can be used on their"
N16-3013,P14-1041,0,0.0213724,"file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the configuration file, are available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ phrase grammar for sentence compression (Ganitkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various other MT approaches have been used for generating sentence simplifications, however none of these used a general-purpose paraphrase grammar (Narayan and Gardent, 2014; Wubben et al., 2012, among others). Another application of sentential paraphrases is to expand multiple reference sets for machine translation (Madnani and Dorr, 2010). PPDB has been used for many tasks, including recognizing textual entailment, question generation, and measuring semantic similarity. These language packs were inspired by the foreign language packs released with Joshua 6 (Post et al., 2015). 6 We have presented a black box for generating sentential paraphrases: PPDB language packs. The language packs include everything necessary for generation, so that they can be downloaded"
N16-3013,P15-2070,1,0.915574,"Missing"
N16-3013,2006.amta-papers.25,0,0.0549992,"udes an input text box (one sentence Figure 1: A screen shot of the web tool. The number to the right of each output sentence is the TER. at a time), and slider bars to change the weights of any of the features used for decoding. Since this model has not been manually evaluated, we favor precision over recall and maintain a relatively conservative level of paraphrasing. The user is shown the top 10 outputs, as ranked by the sentence score. For each output sentence, we report the Translation Edit Rate (TER), which is the number of changes needed to transform the output sentence into the input (Snover et al., 2006). This tool can be used to demonstrate and test a model or to hand-tune the model in order to determine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the configuration file, are available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ phrase grammar for sentence compression (Ganitkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various oth"
N16-3013,P12-1107,0,0.0647131,"Missing"
N16-3013,Q15-1021,1,0.83388,"distinguished only 63 by the different weight vectors, and are selected by point the Joshua invocation script to the corresponding configuration file. 3.1 Tuned models We include two models that were tuned for (1) sentence compression and (2) simplification. The compression model is based on the work of Ganitkevitch et al. (2011), and uses the same features, tuning data, and objective function, P R E´ CIS. The simplification model is described in Xu et al. (2016), and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993)), “simple” English (the Britannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We sys"
N16-3013,Q16-1029,1,0.937774,"ere are tuned models for (1) sentence compression, (2) text simplification, and (3) a general-purpose model with handtuned weights. These models are distinguished only 63 by the different weight vectors, and are selected by point the Joshua invocation script to the corresponding configuration file. 3.1 Tuned models We include two models that were tuned for (1) sentence compression and (2) simplification. The compression model is based on the work of Ganitkevitch et al. (2011), and uses the same features, tuning data, and objective function, P R E´ CIS. The simplification model is described in Xu et al. (2016), and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993))"
N18-1019,P13-3021,0,0.0220536,"llel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformations involved in text simplification. Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Søgaard, 2013).1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking. To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their de"
N18-1019,C14-1188,0,0.0144012,"hine Translation system extended to support phrase deletion, and Wubben et al. (2012) augment a phrase-based system with a re-ranking heuristic. Woodsend and Lapata (2011) view simplification as a monolingual text generation task. They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input. In contrast to these methods, Narayan and Gardent (2016)’s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformat"
N18-1019,E14-1057,0,0.0577141,"Missing"
N18-1019,D16-1215,1,0.926775,"re lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sense inventories or to directly perform in-context substitution. In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step. Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015). Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases available in the 2 Datasets for system training and evaluation have been made available in the SemEval 2016 Complex Word Identification task (Paetzold and Specia, 2016) but present several issues that make system comparison problematic. We explain the drawbacks of the proposed datasets that led to their exclusion from this work in Section 5. 1 For a detailed overview of syntactic simplification works, see (Shardlow, 2014). 208 Annotators Prevalence Example Words 0 0.617 heard, sat, feet, shops, town 1 0.118"
N18-1019,P14-5010,0,0.0119287,"four different reading levels. Xu et al. (2015) also aligned sentences from these texts, extracting 141,582 complex/simple sentence pairs. We use the Newsela corpus to create a goldstandard dataset of complex and simple words for training and testing our models. We do this by hiring crowdsourced annotators through Amazon Mechanical Turk, and asking them to identify complex words in the context of given texts. We randomly select 200 texts from the Newsela corpus, and take the first 200 tokens from each to be labeled by nine annotators. We preprocess the texts using the Stanford CoreNLP suite (Manning et al., 2014) for tokenization, lemmatization, part-of-speech (POS) tagging, and named entity recognition. The annotators are instructed to label at least 10 complex words they deem worth sim3.2 Methods Following Shardlow (2013a), we use a Support Vector Machine classifier. We also conduct experiments with a Random Forest Classifier. Shardlow (2013a) identified several features that help to determine whether or not a word is complex, including word length, number of syllables, word frequency, number of unique WordNet synsets, and number of WordNet synonyms. Shardlow used word frequencies extracted from SUB"
N18-1019,P05-1074,1,0.635779,"elines, we consider candidate substitutions from three datasets. The first is WordNet (Miller, 1995), a lexical network encoding manually identified semantic relationships between words, such as synonymy, hypernymy and hyponymy. This resource has been widely used in substitution tasks (McCarthy and Navigli, 2007). We also use paraphrases extracted from the Paraphrase Database (PPDB) and the Simple Paraphrase Database (SimplePPDB). PPDB is a collection of more than 100 million English paraphrase pairs (Ganitkevitch et al., 2013). These pairs were extracted using a bilingual pivoting technique (Bannard and Callison-Burch, 2005), which assumes that two English phrases that translate to the same foreign phrase have the same meaning. PPDB was updated by Pavlick et al. (2015) to assign labels stating the precise entailment relationship between paraphrase pairs (e.g. forward/backward entailment), and new confidence scores (PPDB 2.0 scores) reflecting the strength of paraphrase relations. SimplePPDB is a subset of PPDB which contains 4.5 million simplification rules, linking a complex word or phrase with a simpler paraphrase with the same meaning. Simplification rules come with both a PPDB 2.0 score and a simplification c"
N18-1019,S07-1009,0,0.294471,"uce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context. This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sense inventories or to directly perform in-context substitution. In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step. Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015). Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases available in the 2 Datasets for system training and evaluation have been made avail"
N18-1019,P11-2087,0,0.0221431,"been shown that frequent words increase a text’s readability (Devlin and Tait, 1998; Kauchak, 2013). Models that include a semantic processing step for substitute selection aim to ensure that the selected substitutes express the correct meaning of words in specific contexts. WSD is often carried out by selecting the correct synset (i.e. set of synonyms describing a sense) for a target word in WordNet (Miller, 1995) and retrieving the synonyms describing that sense. Thomas and Anderson (2012) use WordNet’s tree structure (hypernymy relations) to reduce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context. This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sen"
N18-1019,W15-1501,0,0.256079,"o contextunaware models. 1 Figure 1: An example sentence with complex words identified by our classifier, and their substitutes proposed by the embedding-based substitution model. previous work, our classifier takes into account both lexical and context features. We extract candidate substitutes for the identified complex words from SimplePPDB (Pavlick and Callison-Burch, 2016), a database of 4.5 million English simplification rules linking English complex words to simpler paraphrases. We select the substitutes that best fit each context using a word embeddingbased lexical substitution model (Melamud et al., 2015). An example sentence, along with the complex words identified by our model and the proposed replacements, is shown in Figure 1. We show that our complex word identification classifier and substitution model improve over several baselines which exploit other types of information and do not account for context. Our approach proposes highly accurate substitutes that are simpler than the target words and preserve the meaning of the corresponding sentences. Introduction Automated text simplification is the process that involves transforming a complex text into one with the same meaning, but can be"
N18-1019,W17-1914,1,0.903861,"1 For a detailed overview of syntactic simplification works, see (Shardlow, 2014). 208 Annotators Prevalence Example Words 0 0.617 heard, sat, feet, shops, town 1 0.118 protests, pump, trial 2 0.062 sentenced, fraction, primary 3 0.047 measures, involved, elite 4 0.035 fore, pact, collapsed 5 0.031 slew, enrolled, widespread 6 0.029 edible, seize, dwindled 7 0.023 perilous, activist, remorse 8 0.023 vintners, adherents, amassed 9 0.015 abdicate, detained, liaison Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to select the ones that are adequate in specific contexts. In the same line, Cocos et al. (2017) used a word embedding-based substitution model (Melamud et al., 2015) for ranking PPDB paraphrases in context. We extend this work and adapt the Melamud et al. (2015) model to the simplification setting by using candidate paraphrases extracted from the Simple PPDB resource (Pavlick and Callison-Burch, 2016), a subset of the PPDB that contains complex words and phrases, and their simpler counterparts that can be used for incontext simplification. 3 Table 1: Examples of words identified as difficult to understand within a text by n annotators, where 0 ≤ n ≤ 9. Column 2 (Prevalence) shows the pr"
N18-1019,W11-1601,0,0.131133,"simplification, and sentence splitting. In this paper, we focus on lexical simplification, the task of replacing difficult words in a text with words that are easier to understand. Lexical simplification involves two main processes: identifying complex words within a text, and suggesting simpler paraphrases for these words that preserve their meaning in this context. To identify complex words, we train a model on data manually annotated for complexity. Unlike 2 Related Work Prior approaches to text simplification have addressed the task as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). The proposed models are trained on aligned sentences extracted from Wikipedia and Simple Wikipedia, a corpus that 207 Proceedings of NAACL-HLT 2018, pages 207–217 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion. Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution. Coster and Kauchak (2011) employ a phrase-based Machine Translatio"
N18-1019,W12-3018,0,0.0217572,"Missing"
N18-1019,N13-1092,1,0.904854,"Missing"
N18-1019,W16-6620,0,0.0544251,"The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input. In contrast to these methods, Narayan and Gardent (2016)’s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformations involved in text simplification. Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice an"
N18-1019,P13-1151,0,0.0188796,"ures, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used. Ranking is often addressed in terms of word frequency in a large corpus since it has been shown that frequent words increase a text’s readability (Devlin and Tait, 1998; Kauchak, 2013). Models that include a semantic processing step for substitute selection aim to ensure that the selected substitutes express the correct meaning of words in specific contexts. WSD is often carried out by selecting the correct synset (i.e. set of synonyms describing a sense) for a target word in WordNet (Miller, 1995) and retrieving the synonyms describing that sense. Thomas and Anderson (2012) use WordNet’s tree structure (hypernymy relations) to reduce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rul"
N18-1019,P16-2024,1,0.940117,"ct complex words with higher accuracy than other commonly used methods, and propose good simplification substitutes in context. They also highlight the limited contribution of context features for CWI, which nonetheless improve simplification compared to contextunaware models. 1 Figure 1: An example sentence with complex words identified by our classifier, and their substitutes proposed by the embedding-based substitution model. previous work, our classifier takes into account both lexical and context features. We extract candidate substitutes for the identified complex words from SimplePPDB (Pavlick and Callison-Burch, 2016), a database of 4.5 million English simplification rules linking English complex words to simpler paraphrases. We select the substitutes that best fit each context using a word embeddingbased lexical substitution model (Melamud et al., 2015). An example sentence, along with the complex words identified by our model and the proposed replacements, is shown in Figure 1. We show that our complex word identification classifier and substitution model improve over several baselines which exploit other types of information and do not account for context. Our approach proposes highly accurate substitut"
N18-1019,D11-1038,0,0.0387281,"us that 207 Proceedings of NAACL-HLT 2018, pages 207–217 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion. Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution. Coster and Kauchak (2011) employ a phrase-based Machine Translation system extended to support phrase deletion, and Wubben et al. (2012) augment a phrase-based system with a re-ranking heuristic. Woodsend and Lapata (2011) view simplification as a monolingual text generation task. They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zha"
N18-1019,P15-2070,1,0.920774,"Missing"
N18-1019,P12-1107,0,0.0775015,"Missing"
N18-1019,Q15-1021,1,0.934259,"Missing"
N18-1019,D17-1062,0,0.0421641,"11) view simplification as a monolingual text generation task. They propose a model based on a quasi-synchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. The grammar is also induced from a parallel Wikipedia corpus, and an integer linear programming model selects the most appropriate simplification from the space of possible rewrites generated by the grammar. The hybrid model of Angrosh et al. (2014) combines a synchronous grammar extracted from the same parallel corpus with a set of hand crafted syntactic simplification rules. In recent work, Zhang and Lapata (2017) propose a reinforcement learning-based text simplification model which jointly models simplicity, grammaticality, and semantic fidelity to the input. In contrast to these methods, Narayan and Gardent (2016)’s sentence simplification approach does not need a parallel corpus for training, but rather uses a deep semantic representation as input for simplification. The above-mentioned systems support the full range of transformations involved in text simplification. Other works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical"
N18-1019,P13-3015,0,0.288672,"works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Søgaard, 2013).1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking. To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used. Ranking is often addressed in terms of word frequency in a large corpus since it has b"
N18-1019,C10-1152,0,0.10565,"ication, syntactic simplification, and sentence splitting. In this paper, we focus on lexical simplification, the task of replacing difficult words in a text with words that are easier to understand. Lexical simplification involves two main processes: identifying complex words within a text, and suggesting simpler paraphrases for these words that preserve their meaning in this context. To identify complex words, we train a model on data manually annotated for complexity. Unlike 2 Related Work Prior approaches to text simplification have addressed the task as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). The proposed models are trained on aligned sentences extracted from Wikipedia and Simple Wikipedia, a corpus that 207 Proceedings of NAACL-HLT 2018, pages 207–217 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics contains instances of transformation operations needed for simplification such as rewording, reordering, insertion and deletion. Zhu et al. (2010) propose to use a tree-based translation model which covers splitting, dropping, reordering and substitution. Coster and Kauchak (2011) employ a phras"
N18-1019,W13-2908,0,0.44462,"works address specific subtasks, such as syntactic or lexical simplification, which involve identifying grammatical or lexical complexities in a text and rewriting these using simpler words and structures. Syntactic simplification might involve operations such as sentence splitting, rewriting of sentences including passive voice and anaphora resolution (Chandrasekar and Srinivas, 1997; Klerke and Søgaard, 2013).1 Lexical simplification involves complex word identification, substitute generation, context-based substitute selection and simplicity ranking. To identify the words to be simplified, Shardlow (2013a) proposes to use a Support Vector Machine (SVM) that exploits several lexical features, such as frequency, character and syllable length. Our approach also uses a SVM classifier for identifying complex words, but complements this set of features with context-related features that have not been exploited in previous work.2 In the lexical simplification subtask, existing methods differ in their decision to include a word sense disambiguation (WSD) step for substitute selection and in the ranking method used. Ranking is often addressed in terms of word frequency in a large corpus since it has b"
N18-1019,S12-1046,0,0.0205931,"nd Anderson (2012) use WordNet’s tree structure (hypernymy relations) to reduce the size of the vocabulary in a document. Biran et al. (2011) perform disambiguation in an unsupervised manner. They learn simplification rules from comparable corpora and apply them to new sentences using vector-based context similarity measures to select words that are the most likely candidates for substitution in a given context. This process does not involve an explicit WSD step, and simplification is addressed as a context-aware lexical substitution task. The SemEval 2012 English Lexical Simplification task (Specia et al., 2012) also addresses simplification as lexical substitution (McCarthy and Navigli, 2007), allowing systems to use external sense inventories or to directly perform in-context substitution. In our work, we opt for an approach which addresses lexical substitution in a direct way and does not include an explicit disambiguation step. Lexical substitution systems perform substitute ranking in context using vector-space models (Thater et al., 2011; Kremer et al., 2014; Melamud et al., 2015). Recently, Apidianaki (2016) showed that a syntax-based substitution model can successfully filter the paraphrases"
N18-1019,Q14-1018,0,0.0257101,". The intuition for including context-specific features is that if a target word is surrounded by simple words, a reader is likely better able to understand the meaning of the target word, which would thus not need it simplified. To evaluate the performance of our lexical simplification model, we create a test set from the Newsela corpus. We extract lexical simplification rules from these parallel sentences using two methods. First, we find sentence pairs with only one lexical replacement and use these word pairs as simplification instances. Next, we use a monolingual word alignment software (Sultan et al., 2014) to extract all non-identical aligned word pairs. We only consider word pairs corresponding to different lemmas (i.e. words with different base forms). From this process, we collect a test set of 14,436 word pairs. 4 4.2 4.1 Lexical Simplification In-context Ranking and Substitution To accurately replace words in texts with simpler paraphrases and ensure the generated sentences preserve the meaning of the original, we need to take into account the surrounding context. To do this, we adapt the word embedding-based lexical substitution model of Melamud et al. (2015) to the simplification task. V"
N18-1019,I11-1127,0,0.0561484,"Missing"
N18-1030,E17-1056,0,0.0162954,"omic Organization: Select a subset of ˆ the predicted edges, R P ⊆ R, that produces a high sum of scores, r∈Rˆ s(rij ), subject to structural constraints. The final output is the ˆ ˆ graph G(E, R). Structural constraints dictate what can be considered a valid or invalid combination of edges in a taxonomic graph (Do and Roth, 2010). Two structural constraints frequently imposed are that the final graph be a DAG, or that the final graph be a tree/forest.1 Examples of algorithms that produce DAG structures are the longest-path algorithm of Kozareva and Hovy (2010), the ContrastMedium approach of Faralli et al. (2017), and the random cycle-breaking method used in (Panchenko et al., 2016) and Faralli et al. (2015). We experiment with a variation of the last one here, which we call N O C YC. To produce tree-structured taxonomies, most researchers (including us) use algorithms for finding the maximally-weighted rooted tree spanning a directed graph (DMST). Examples of prior work following this approach are Navigli et al. (2011) and Bansal et al. (2014). Another dimension along which taxonomy organization approaches differ is whether they explicitly require the set of chosen relational inˆ to be fully transiti"
N18-1030,P14-1098,0,0.036834,"Missing"
N18-1030,W11-2501,0,0.0144971,"th human judgements of paraphrase quality (Pavlick et al., 2015). We extract sets of entities from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appea"
N18-1030,N13-1092,1,0.720357,"Missing"
N18-1030,J15-2003,0,0.0185141,"insect) is seˆ and (insect IS - A organism) is lected as part of R, General Framework for Taxonomy Induction The problem of taxonomy induction can be summarized via three core sub-tasks. While all systems that build taxonomies automatically must ad1 WLOG, the tree and forest constraints are identical, as a dummy root node can be attached to the root of each component in a forest to produce a tree. 324 ˆ then (beetle IS - A organism) selected as part of R, must also be selected. Two methods that impose such transitivity constraints are the M AX T RANS G RAPH and M AX T RANS F OREST methods of Berant et al. (2015), both of which we experiment with here. A final consideration when choosing a taxonomy organization algorithm is whether the method should enable the consolidation of synonyms into a single taxonomic entity. Synonym sets, or synsets, are present as nodes in the WordNet graph (Miller, 1995). Potential advantages to using synonym sets, rather than individual terms, as nodes include the ability to model polysemy (horse means one thing when grouped with its synonym cavalry and another entirely when grouped with sawhorse), and the ability to be more precise in defining relations. A few early taxon"
N18-1030,C92-2082,0,0.59337,"Missing"
N18-1030,J12-1003,0,0.0200049,"in 3b. Finally, its transitive reduction is in 3c. Because flipping edges in the result produces a tree rooted at organism, the graph in 3a is called forest-reducible. The purpose of modifying scores this way is efficiency; M AX T RANS G RAPH solves its optimization on each connected component of the graph independently, where components are constructed by considering only positively-weighted edges in the graph. Increasing λ increases sparsity and decreases runtime. The objective of the ILP is to maximize the weights of selected relations, while requiring that the graph respects transitivity. Berant et al. (2012) proved this problem is NP-hard and provided an ILP formulation for it as follows. Let xij be a binary variable that indicates whether edge (ei , ej ) ˆ is in the subset of selected edges, R. The algorithm works by adding a dummy root node eROOT to E, and an edge from eROOT to every other node ei in the graph. We then use Chu-Liu-Edmonds to find the directed tree rooted at eROOT that spans all nodes in E and has the maximal sum of scores. Note that until now we have considered edges in taxonomy graphs to point from hyponyms to hypernyms; in this case we must switch the order, so that the spann"
N18-1030,D10-1108,0,0.0524198,"Missing"
N18-1030,N15-1098,0,0.0537651,"Missing"
N18-1030,S15-2151,0,0.0217095,"pairwise relation prediction as a common initialization; we then (b) feed the resulting graphs as identical input to six taxonomic organization algorithms. We evaluate the impact of varied structural constraints between algorithms. Introduction Many words and phrases fit within a natural semantic hierarchy: a mobile is a type of telephone, which in turn is a communications device and an object. Taxonomies, which encode this knowledge, are important resources for natural language understanding systems. There is ongoing interest in developing methods to build taxonomic resources automatically (Bordea et al., 2015, 2016). Although several widelyused general ontologies (e.g. WordNet (Miller, 1995)) and domain-specific ontologies (e.g. Unified Medical Language System (UMLS) (Bodenreider, 2004)) exist, these resources are handcrafted and therefore expensive to update or expand. Automatic taxonomy induction enables the construction of taxonomic resources at scale in new languages and domains. Further, there is evidence that it is useful to build dynamic or contextspecific taxonomies extemporaneously for some applications (Do and Roth, 2010). 323 Proceedings of NAACL-HLT 2018, pages 323–333 c New Orleans, L"
N18-1030,S16-1168,0,0.0294462,"Missing"
N18-1030,C02-1144,0,0.0786533,"when choosing a taxonomy organization algorithm is whether the method should enable the consolidation of synonyms into a single taxonomic entity. Synonym sets, or synsets, are present as nodes in the WordNet graph (Miller, 1995). Potential advantages to using synonym sets, rather than individual terms, as nodes include the ability to model polysemy (horse means one thing when grouped with its synonym cavalry and another entirely when grouped with sawhorse), and the ability to be more precise in defining relations. A few early taxonomy induction approaches incorporated synonym clustering (e.g. Lin and Pantel (2002) and Pantel and Ravichandran (2004)). The two transitive algorithms that we analyze here, M AX T RANS G RAPH and M AX T RANS F OREST, also consolidate equivalent terms into a single node. 3 3.1 The no-cycles method, which we abbreviate as N O C YC, is a simple method for constructing a DAG with high score from a set of predicted relational edges. It is not transitive. The algorithm works as follows. From the set R of all predicted hypernym relations, we first filter out of the graph G(E, R) any edges with score s(rij ) less than a tunable threshold τ . Next, we break any cycles by finding stro"
N18-1030,D10-1107,0,0.0817601,"in developing methods to build taxonomic resources automatically (Bordea et al., 2015, 2016). Although several widelyused general ontologies (e.g. WordNet (Miller, 1995)) and domain-specific ontologies (e.g. Unified Medical Language System (UMLS) (Bodenreider, 2004)) exist, these resources are handcrafted and therefore expensive to update or expand. Automatic taxonomy induction enables the construction of taxonomic resources at scale in new languages and domains. Further, there is evidence that it is useful to build dynamic or contextspecific taxonomies extemporaneously for some applications (Do and Roth, 2010). 323 Proceedings of NAACL-HLT 2018, pages 323–333 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dress each of these tasks, the sequence and manner in which they are addressed varies. In the most straightforward case, the core tasks are viewed as orthogonal and carried out sequentially. They are: Taxonomy induction involves three sub-tasks: entity extraction, relation prediction, and taxonomic organization. In many cases these subtasks are undertaken sequentially to build a taxonomy from the ground up. While many works directly compare methods for r"
N18-1030,D12-1104,0,0.0846298,"Missing"
N18-1030,E17-1007,0,0.012244,"New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dress each of these tasks, the sequence and manner in which they are addressed varies. In the most straightforward case, the core tasks are viewed as orthogonal and carried out sequentially. They are: Taxonomy induction involves three sub-tasks: entity extraction, relation prediction, and taxonomic organization. In many cases these subtasks are undertaken sequentially to build a taxonomy from the ground up. While many works directly compare methods for relation prediction (e.g. Turney and Mohammad (2015), Shwartz et al. (2017) and others), none directly compare methods for the final taxonomic organization step with varying constraints. Each paper that proposes a taxonomic organization method starts with its own set of predicted relations, making it impossible to determine – even with benchmark datasets – the extent to which improvements in identifying ground-truth relations are due to (a) better relation prediction, or (b) better taxonomic organization. In this work, we present an empirical applesto-apples comparison of six algorithms for unsupervised taxonomic organization. The algorithms vary along three axes: wh"
N18-1030,S15-1021,0,0.0128219,"the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appear in any of the local taxonomies. We train HypeNET using our 76K-pair test set, and provide the resu"
N18-1030,S16-1206,0,0.0461464,"Missing"
N18-1030,N04-1041,0,0.100186,"rganization algorithm is whether the method should enable the consolidation of synonyms into a single taxonomic entity. Synonym sets, or synsets, are present as nodes in the WordNet graph (Miller, 1995). Potential advantages to using synonym sets, rather than individual terms, as nodes include the ability to model polysemy (horse means one thing when grouped with its synonym cavalry and another entirely when grouped with sawhorse), and the ability to be more precise in defining relations. A few early taxonomy induction approaches incorporated synonym clustering (e.g. Lin and Pantel (2002) and Pantel and Ravichandran (2004)). The two transitive algorithms that we analyze here, M AX T RANS G RAPH and M AX T RANS F OREST, also consolidate equivalent terms into a single node. 3 3.1 The no-cycles method, which we abbreviate as N O C YC, is a simple method for constructing a DAG with high score from a set of predicted relational edges. It is not transitive. The algorithm works as follows. From the set R of all predicted hypernym relations, we first filter out of the graph G(E, R) any edges with score s(rij ) less than a tunable threshold τ . Next, we break any cycles by finding strongly connected components (SCC) in"
N18-1030,Q15-1025,0,0.0259428,"Missing"
N18-1030,P15-2070,1,0.861283,"Missing"
N18-1030,C14-1097,0,0.0305415,"Missing"
N18-1030,L16-1722,0,0.0121738,"quality (Pavlick et al., 2015). We extract sets of entities from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appear in any of the local taxonomi"
N18-1030,W15-4208,0,0.0129284,"We extract sets of entities from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; 327 proaches to hypernym prediction. It uses a recurrent neural network to represent the set of observed dependency paths connecting an input word pair, and concatenates this representation with distributional word embeddings to produce a set of features for predicting hypernymy. We create a dataset of noun pairs for training and evaluating the HypeNET model. It combines noun pairs from four benchmark relation prediction datasets (BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016), EVALution (Santus et al., 2015), and K&H+N (Necsulescu et al., 2015)) with a set of related and unrelated noun pairs extracted from PPDB. Since each of these is a multi-class dataset, we binarize the data by labeling noun pairs with a hypernym relation as positive instances, and all others as negative. The combined benchmark+PPDB training set contains 76,152 noun pairs with a 1:4 hypernym:nonhypernym ratio, and the evaluation set contains 29,051 pairs. We ensure lexical separation from our taxonomy induction dataset; no terms in the classifier training set appear in any of the local taxonomies. We train HypeNET using our 76"
N18-1030,P16-1226,0,0.0298547,"Missing"
N18-2077,W16-2302,0,0.0245056,"Missing"
N18-2077,D16-1025,0,0.024631,"Missing"
N18-2077,N10-1031,0,0.0239866,"etrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical su"
N18-2077,N12-1017,0,0.408623,"large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type"
N18-2077,P15-2070,1,0.921182,"Missing"
N18-2077,W16-2339,0,0.0316299,"Missing"
N18-2077,N13-1092,1,0.918735,"Missing"
N18-2077,2006.amta-papers.25,0,0.407861,"selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems. 1 Damon undermines richness variability belittles pluralism diminishes divergence in cinematography cinema film movie Figure 1: An English reference sentence enriched with substitutes selected by the embedding-based lexical substitution model. metric with automatically generated lattices (hereafter HyTERA). We show that HyTERA strongly correlates with HyTER with hand-crafted lattices, and approximates the hTER score (Snover et al., 2006) as measured using post-edits made by human annotators. Furthermore, we generate lattices for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent tran"
N18-2077,H05-1021,0,0.0419773,"TERA to hTER scores. In Section § 5.2, we explore whether H y TERA can reliably predict human translation quality scores from the WMT16 Metrics Shared Task. The vectors s and t are word embeddings of the substitute and target generated by the skip-gram with negative sampling model (Mikolov et al., 2013b,a).4 The context C is the set of context embeddings for words appearing within a fixedwidth window of the target t in a sentence (we use 1 The code is available at https://bitbucket. org/gwisniewski/hytera/ 2 Note that as permutations of interest can be compactly encoded in a fine-state graph (Kumar and Byrne, 2005), the MOVE operation can be easily considered in our code by applying the substitutions to the permutation lattice rather than to the sentence. 3 PPDB paraphrases come into packages of different sizes (going from S to XXXL): small packages contain highprecision paraphrases while larger ones have high coverage. All are available from paraphrase.org 4 For the moment, we focus on individual content words. In future work, we plan to also annotate longer text segments in the references with multi-word PPDB paraphrases. 5 In the original implementation, Melamud et al. (2015) use syntactic dependenci"
N18-2077,W09-0441,0,0.0362055,"l substitution method described in Section 3 to each of the four references associated with a sentence, and considering the union of the resulting lattices. We report results for two kinds of lattices: lattices encoding all lexical substitutes available for a word in PPDB (allPars) and lattices of substitutes with PPDBSc&gt;2.3 (allParsFiltered) and AddCosSc≥0. As expected, the allPars lattices are much larger than the manual and the filtered lattices (cf. Table 1). In all our experiments, all corpora are down-cased and tokenized using standard Moses scripts. hTER scores are computed using TERp (Snover et al., 2009). 5.2 WMT Metrics Evaluation In our second set of experiments, we explore the ability of HyTERA to predict direct human judgments at the sentence level using the setting of the WMT16 Metrics Shared Task (Bojar et al., 2016). We measure the correlation between adSentence Level Evaluation Table 2 reports the correlation between HyTER, HyTERA and hTER at the sentence level. We also include as a baseline the correlation with the sentence-level B LEU 9 More precisely: S B LEU = 1 · 4 p where p is the i 4 ∑i=1 i number of i-grams that appears both in the reference and in the hypothesis divided by th"
N18-2077,W12-3129,0,0.0244022,"orm the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model ("
N18-2077,W15-3050,0,0.0514759,"Missing"
N18-2077,W15-1501,0,0.178423,". The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We propose to use an embedding-based lexical substitution model (Melamud et al., 2015) for building this type of reference networks and test, for the first time, the 2 The Original HyTER Metric The HyTER metric (Dreyer and Marcu, 2012) computes the similarity between a translation hypothesis and a reference lattice that compactly encodes millions of meaning-equivalent translations. Formally HyTER is defined as: H y TER (x, Y ) = arg min y∈Y LS(x, y) len(y) (1) where Y is a set of references that can be encoded as a finite state automaton such as the one represented in Figure 1, x is a translation hypothesis and LS is the standard Levenshtein distance, defined as the minimum num"
N18-2077,W15-3053,0,0.053028,"Missing"
N18-2077,W12-3018,0,0.0656812,"Missing"
N18-2077,P02-1040,0,0.115047,"for standard datasets from a recent WMT Metrics Shared Task and perform the first evaluation of H y TER on large and noisier datasets. The results show that it still remains an interesting solution for MT evaluation, but highlight its limits when used to evaluate recent MT systems that make far less errors of lexical choice than older systems. Introduction Human translators and MT systems can produce multiple plausible translations for input texts. To reward meaning-equivalent but lexically divergent translations, MT evaluation metrics exploit synonyms and paraphrases, or multiple references (Papineni et al., 2002; Doddington, 2002; Denkowski and Lavie, 2010; Lo et al., 2012). The H y TER metric (Dreyer and Marcu, 2012) relies on massive reference networks encoding an exponential number of correct translations for parts of a given sentence, proposed by human annotators. The manually built networks attempt to encode the set of all correct translations for a sentence, and H y TER rewards high quality hypotheses by measuring their minimum edit distance to the set of possible translations. HyTER spurred a lot of enthusiasm but the need for human annotations heavily reduced its applicability to new data. We"
N19-1053,J17-3005,0,0.0239768,"a subset of the data with low rater–rater agreement ρ (see Appendix A.2). In certain steps, we use an information retrieval (IR) system2 to generate the best candidates for the task at hand. Feng and Hirst (2011) classified an input into one of the argument schemes. Habernal and Gurevych (2017) provided a large corpus annotated with argument units. Cabrio and Villata (2018) provide a thorough survey the recent work in this direction. A few other works studied other aspects of argumentative structures (Cabrio and Villata, 2012; Khatib et al., 2016; Lippi and Torroni, 2016; Zhang et al., 2017; Stab and Gurevych, 2017). A few recent works use a similar conceptual design that involves a claim, perspectives and evidence.These works are either too small due to the high cost of construction (Aharoni et al., 2014) or too noisy because of the way they are crawled from online resources (Wachsmuth et al., 2017; Hua and Wang, 2017). Our work makes use of both online content and of crowdsourcing, in order to construct a sizable and high-quality dataset. 4 4.1 Step 1: The initial data collection. We start by crawling the content of a few notable debating websites: idebate.com, debatewise.org, procon.org. This yields ∼"
N19-1053,S16-1003,0,0.124324,"Missing"
N19-1053,W15-4631,0,0.0217366,"laim. The problem has gained significant attention in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; for example, Palau and"
N19-1053,N18-1074,0,0.0775384,"Missing"
N19-1053,W14-2105,0,0.0482376,"ing phrases that support or oppose a given claim. The problem has gained significant attention in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures f"
N19-1053,C10-1099,1,0.654772,"xhaustive spectrum of ideas with respect to a claim, presenting a small but diverse set of perspectives could be an important step towards addressing the selection bias problem. Moreover, it would be impractical to develop an exhaustive pool of evidence for all perspectives, from a diverse set of credible sources. We are not attempting to do that. We aim at formulating the core NLP problems, and developing a dataset that will facilitate studying these problems from the NLP angle, realizing that using the outcomes of this research in practice requires addressing issues such as trustworthiness (Pasternack and Roth, 2010, 2013) and possibly others. Inherently, our objective requires understanding the relations between perspectives and claims, the nuances in the meaning of various perspectives in the context of claims, and relations between perspectives and evidence. This, we argue, can be done with a diverse enough, but not exhaustive, dataset. And it can be done without attending to the legitimacy and credibility of sources contributing evidence, an important problem but orthogonal to the one studied here. Figure 2: Depiction of a few claims, their perspectives and evidences from P ERSPECTRUM. The supporting"
N19-1053,W14-2508,0,0.447756,"urch, 2005). Stance classification of perspectives: a system is supposed to assess the stances of the perspectives with respect to the given claim (supporting, opposing, etc.) (Hasan and Ng, 2014). Substantiating the perspectives: a system is expected to find valid evidence paragraph(s) in support of each perspective. Conceptually, this is similar to the well-studied problem of textual entailment (Dagan et al., 2013) except that here the en3 Related Work Claim verification. The task of fact verification or fact-checking focuses on the assessment of the truthfulness of a claim, given evidence (Vlachos and Riedel, 2014; Mitra and Gilbert, 2015; Samadi et al., 2016; Wang, 2017; Nakov et al., 2018; Hanselowski et al., 2018; Karimi et al., 2018; Alhindi et al., 2018). These tasks are highly related to the task of textual-entailment that has been extensively studied in the field (Bentivogli et al., 2008; Dagan et al., 2013; Khot et al., 2018). Some recent work study jointly the problem of identifying evidence and verifying that it supports the claim (Yin and Roth, 2018). Our problem structure encompasses the fact verification problem (as verification of perspectives from evidence; Figure 1). Stance classificati"
N19-1053,D15-1050,0,0.0316394,"t or oppose a given claim. The problem has gained significant attention in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; f"
N19-1053,W17-5106,0,0.346926,"gumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; for example, Palau and Moens (2009); Khatib et al. (2016); Ajjour et al. (2017) studied elements of arguments and the internal relations between them. 544 Dataset Stance Classification Evidence Verification Human Verified Open Domain P ERSPECTRUM (this work) FEVER (Thorne et al., 2018) (Wachsmuth et al., 2017) LIAR (Wang, 2017) (Vlachos and Riedel, 2014) (Hasan and Ng, 2014) 3 7 3 7 7 3 3 3 3 3 3 7 3 3 7 3 3 3 3 3 3 3 3 7 Table 1: Comparison of P ERSPECTRUM to a few notable datasets in the field. make sure that the workers are responding objectively to the tasks (as opposed to using their personal opinions or preferences). The screen-shots of the annotation interfaces for each step are included in the Appendix (Section A.3). In the steps outlined below, we filter out a subset of the data with low rater–rater agreement ρ (see Appendix A.2). In certain steps, we use an information retrieval (IR) syst"
N19-1053,P17-2067,0,0.476969,"to assess the stances of the perspectives with respect to the given claim (supporting, opposing, etc.) (Hasan and Ng, 2014). Substantiating the perspectives: a system is expected to find valid evidence paragraph(s) in support of each perspective. Conceptually, this is similar to the well-studied problem of textual entailment (Dagan et al., 2013) except that here the en3 Related Work Claim verification. The task of fact verification or fact-checking focuses on the assessment of the truthfulness of a claim, given evidence (Vlachos and Riedel, 2014; Mitra and Gilbert, 2015; Samadi et al., 2016; Wang, 2017; Nakov et al., 2018; Hanselowski et al., 2018; Karimi et al., 2018; Alhindi et al., 2018). These tasks are highly related to the task of textual-entailment that has been extensively studied in the field (Bentivogli et al., 2008; Dagan et al., 2013; Khot et al., 2018). Some recent work study jointly the problem of identifying evidence and verifying that it supports the claim (Yin and Roth, 2018). Our problem structure encompasses the fact verification problem (as verification of perspectives from evidence; Figure 1). Stance classification. Stance classification aims at detecting phrases that s"
N19-1053,E17-2088,0,0.125359,"ntion in the recent years; to note a few important ones, Hasan and Ng (2014) create a dataset of dataset text snippets, annotated with “reasons” (similar to perspectives in this work) and stances (whether they support or oppose the claim). Unlike this work, our pool of the relevant “reasons” is not restricted. Ferreira and Vlachos (2016) create a dataset of rumors (claims) coupled with news headlines and their stances. There are a few other works that fall in this category (Boltuˇzi´c and ˇ Snajder, 2014; Park and Cardie, 2014; Rinott et al., 2015; Swanson et al., 2015; Mohammad et al., 2016; Sobhani et al., 2017; Bar-Haim et al., 2017). Our approach here is closely related to existing work in this direction, as stance classification is part of the problem studied here. Argumentation. There is a rich literature on formalizing argumentative structures from free text. There are a few theoretical works that lay the ground work to characterizing units of arguments and argument-inducing inference (Teufel et al., 1999; Toulmin, 2003; Freeman, 2011). Others have studied the problem of extracting argumentative structures from free-form text; for example, Palau and Moens (2009); Khatib et al. (2016); Ajjour et"
N19-1317,C96-2183,0,0.667434,"of a simplification generated by a standard Seq2Seq model vs. our model. Introduction Automatic text simplification aims to reduce the complexity of texts and preserve their meaning, making their content more accessible to a broader audience (Saggion, 2017). This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state"
N19-1317,W11-1601,0,0.0511063,"matic and human evaluation settings, and show that the generated simple sentences are shorter and simpler, while remaining competitive with respect to fluency and adequacy. We also include a detailed error analysis to explain where the model currently falls short and provide suggestions for addressing these issues. 2 Related Work Text simplification has often been addressed as a monolingual translation process, which generates a simplified version of a complex text. Zhu et al. (2010) employ a tree-based translation model and consider sentence splitting, deletion, reordering, and substitution. Coster and Kauchak (2011) use a Phrase-Based Machine Translation (PBMT) system with support for deleting phrases, while Wubben et al. (2012) extend a PBMT system with a reranking heuristic (PBMT-R). Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017). In recent"
N19-1317,P18-1082,0,0.0392393,"erate relatively long sentences, our model is able to generate shorter and simpler sentences, while remaining competitive regarding humanevaluated fluency and adequacy. Finally, we provide a qualitative analysis of where our different contributions improve performance, the effect of length on human-evaluated meaning preservation, and the current shortcomings of our model as insights for future research. Generating diverse outputs from Seq2Seq models could be used in a variety of NLP tasks, such as chatbots (Shao et al., 2017), image captioning (Vijayakumar et al., 2018), and story generation (Fan et al., 2018). In addition, the proposed techniques can also be extremely helpful in leveled and personalized text simplification, where the goal is to generate different sentences based on who is requesting the simplification. Acknowledgments We would like to thank the anonymous reviewers for their helpful feedback on this work. We would also like to thank Devanshu Jain, Shyam Upadhyay, and Dan Roth for their feedback on the postdecoding aspect of this work, as well as Anne Cocos and Daphne Ippolito for their insightful comments during proofreading. This material is based in part on research sponsored by"
N19-1317,W11-2123,0,0.0443189,"3.3 MSE 3.72 1.13 Table 2: Pearson Correlation and Overall Mean Squared Error (MSE) for the sentence-level complexity prediction model (CNN), compared to a length-based baseline. SCE ← CE · w return SCE 0 Correlation 0.503 0.650 Reranking Diverse Candidates Generating diverse sentences is helpful only if we are able to effectively rerank them in a way that promotes simpler sentences while preserving fluency and adequacy. To do this, we propose three • Fluency (fi ): We calculate the perplexity based on a 5-gram language model trained on English Gigaword v.5 (Parker et al., 2011) using KenLM (Heafield, 2011). • Adequacy (ai ): We generate Paragraph Vector representations (Le and Mikolov, 2014) for the input sentence and each candidate and calculate the cosine similarity. • Simplicity (si ): We develop a sentence complexity prediction model to predict the overall complexity of each sentence we generate. To calculate sentence complexity, we modify a Convolutional Neural Network (CNN) for sentence classification (Kim, 2014) to make continuous predictions. We use aligned sentences from the Newsela corpus (Xu et al., 2015) as training data, labeling each with the complexity level from which it came.6"
N19-1317,E17-3017,0,0.160826,"duction Automatic text simplification aims to reduce the complexity of texts and preserve their meaning, making their content more accessible to a broader audience (Saggion, 2017). This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine trans"
N19-1317,D14-1181,0,0.00379492,"To do this, we propose three • Fluency (fi ): We calculate the perplexity based on a 5-gram language model trained on English Gigaword v.5 (Parker et al., 2011) using KenLM (Heafield, 2011). • Adequacy (ai ): We generate Paragraph Vector representations (Le and Mikolov, 2014) for the input sentence and each candidate and calculate the cosine similarity. • Simplicity (si ): We develop a sentence complexity prediction model to predict the overall complexity of each sentence we generate. To calculate sentence complexity, we modify a Convolutional Neural Network (CNN) for sentence classification (Kim, 2014) to make continuous predictions. We use aligned sentences from the Newsela corpus (Xu et al., 2015) as training data, labeling each with the complexity level from which it came.6 As with the word complexity prediction model, we report MSE and Pearson correlation on a held-out test set in Table 2.7 We normalize each individual score between 0 and 1, and calculate a final score as follows: scorei = βf fi + βa ai + βs si (2) We tune these weights (β) on our validation data during experimentation to find the most appropriate combinations of reranking metrics. Examples of improvements resulting fro"
N19-1317,W11-1611,1,0.672136,"Missing"
N19-1317,P14-1041,0,0.398723,"relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement learning (Zhang and Lapata,"
N19-1317,N18-1019,1,0.841356,"ion of Seq2Seq models, please see (Sutskever et al., 2014). 3138 3 Model Frequency Baseline Length Baseline LinReg Seq2Seq Approach 3.1 Complexity-Weighted Loss Function Standard Seq2Seq models use cross entropy as the loss function at training time. This only takes into account how similar our generated tokens are to those in the reference simple sentence, and not the complexity of said tokens. Therefore, we first develop a model to predict word complexities, and incorporate these into a custom loss function. 3.1.1 Word Complexity Prediction Extending the complex word identification model of Kriz et al. (2018), we train a linear regression model using length, number of syllables, and word frequency; we also include Word2Vec embeddings (Mikolov et al., 2013). To collect data for this task, we consider the Newsela corpus, a collection of 1,840 news articles written by professional editors at 5 reading levels (Xu et al., 2015).3 We extract word counts in each of the five levels; in this dataset, we denote 4 as the original complex document, 3 as the least simplified re-write, and 0 as the most simplified re-write. We propose using Algorithm 1 to obtain the complexity label for each word w, where lw re"
N19-1317,P17-2014,0,0.175761,"inyals and Le, 2015), summarization (Nallapati et al., 2016), etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017).2 Nisioi et al. (2017) was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search. Vu et al. (2018) extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models. There are two main Seq2Seq models we will compare to in this work, along with the statistical model from Narayan and Gardent (2014). Zhang and Lapata (2017) proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a rein"
N19-1317,P02-1040,0,0.104313,"nt. Model Hybrid DRESS DMASS S2S S2S-Loss S2S-FA S2S-Cluster-FA S2S-Diverse-FA S2S-All-FAS S2S-All-FA SARI 33.27 36.00 34.35 36.32 36.03 36.47 37.22 35.36 36.30 37.11 Model Complex Hybrid DRESS DMASS S2S S2S-Loss S2S-FA Oracle – – – – – 54.01 50.36 52.65 50.40 50.40 S2S-Cluster-FA S2S-Diverse-FA S2S-All-FAS S2S-All-FA Reference Table 4: Comparison of our models to baselines and state-of-the-art models using SARI. We also include oracle SARI scores (Oracle), given a perfect reranker. S2S-All-FA is significantly better than the DMASS and Hybrid baselines using a student t-test (p < 0.05). BLEU (Papineni et al., 2002) for evaluation; even though it correlates better with fluency than SARI, Sulem et al. (2018) recently showed that BLEU often negatively correlates with simplicity on the task of sentence splitting. We also calculate oracle SARI, where appropriate, to show the score we could achieve if we had a perfect reranking model. Our results are reported in Table 4. Our best models outperform previous state-ofthe-art systems, as measured by SARI. Table 4 also shows that, when used separately, reranking and clustering result in improvements on this metric. Our loss and diverse beam search methods have mor"
N19-1317,D15-1166,0,0.0218597,"a PBMT system with a reranking heuristic (PBMT-R). Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017). In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), conversation agents (Vinyals and Le, 2015), summarization (Nallapati et al., 2016), etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks ("
N19-1317,P14-5010,0,0.00953077,"Missing"
N19-1317,K16-1028,0,0.0363035,"ropose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform competitively with neural models on automatic metrics, though it is outperformed using human judgments (Zhang and Lapata, 2017). In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), conversation agents (Vinyals and Le, 2015), summarization (Nallapati et al., 2016), etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence x to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017).2 Nisioi et al. (2017) was the first major application of Seq2"
N19-1317,D14-1162,0,0.0829513,"its are the same as Zhang and Lapata (2017). We preprocess our data by tokenizing and replacing named entities using CoreNLP (Manning et al., 2014). 4.2 Training Details For our experiments, we use Sockeye, an open source Seq2Seq framework built on Apache MXNet (Hieber et al., 2017; Chen et al., 2015). In this model, we use LSTMs with attention for both our encoder and decoder models with 256 hidden units, and two hidden layers. We attempt to match the hyperparameters described in Zhang and Lapata (2017) as closely as possible; as such, we use 300-dimensional pretrained GloVe word embeddings (Pennington et al., 2014), and Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. We ran our models for 30 epochs.8 During training, we use our complexityweighted loss function, with α = 2; for our baseline models, we use cross-entropy loss. At inference time, where appropriate, we set the beam size b = 100, and the similarity penalty δ = 1.0. After inference, we set the number of clusters to 20, and we compare two separate reranking weightings: one which uses fluency, adequacy, and simplicity (FAS), where βf = βa = βs = 13 ; and one which uses only fluency and adequacy (FA), where βf = βa = 12 and βs"
N19-1317,D17-1235,0,0.0276333,"e standard metric for simplification. More importantly, while other previous models generate relatively long sentences, our model is able to generate shorter and simpler sentences, while remaining competitive regarding humanevaluated fluency and adequacy. Finally, we provide a qualitative analysis of where our different contributions improve performance, the effect of length on human-evaluated meaning preservation, and the current shortcomings of our model as insights for future research. Generating diverse outputs from Seq2Seq models could be used in a variety of NLP tasks, such as chatbots (Shao et al., 2017), image captioning (Vijayakumar et al., 2018), and story generation (Fan et al., 2018). In addition, the proposed techniques can also be extremely helpful in leveled and personalized text simplification, where the goal is to generate different sentences based on who is requesting the simplification. Acknowledgments We would like to thank the anonymous reviewers for their helpful feedback on this work. We would also like to thank Devanshu Jain, Shyam Upadhyay, and Dan Roth for their feedback on the postdecoding aspect of this work, as well as Anne Cocos and Daphne Ippolito for their insightful"
N19-1317,2006.amta-papers.25,0,0.24737,"Missing"
N19-1317,W16-3411,0,0.240817,"age learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is th"
N19-1317,D18-1081,0,0.158457,"Missing"
N19-1317,P08-1040,0,0.0941426,"ir meaning, making their content more accessible to a broader audience (Saggion, 2017). This process can benefit people with reading disabilities, foreign language learners and young children, and can assist non-experts exploring a new field. Text simplification has gained wide interest in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limita"
N19-1317,Q15-1021,1,0.956109,"ose in the reference simple sentence, and not the complexity of said tokens. Therefore, we first develop a model to predict word complexities, and incorporate these into a custom loss function. 3.1.1 Word Complexity Prediction Extending the complex word identification model of Kriz et al. (2018), we train a linear regression model using length, number of syllables, and word frequency; we also include Word2Vec embeddings (Mikolov et al., 2013). To collect data for this task, we consider the Newsela corpus, a collection of 1,840 news articles written by professional editors at 5 reading levels (Xu et al., 2015).3 We extract word counts in each of the five levels; in this dataset, we denote 4 as the original complex document, 3 as the least simplified re-write, and 0 as the most simplified re-write. We propose using Algorithm 1 to obtain the complexity label for each word w, where lw represents the level given to the word, and cwi represents the number of times that word occurs in level i. Algorithm 1 Word Complexity Data Collection 1: procedure DATA C OLLECTION 2: lw ← 4 3: for i ∈ {3, 0} do 4: if cwi ≥ 0.7 ∗ cwi+1 then 5: if cwi ≥ 0.4 ∗ cw4 then 6: lw ← i return lw Here, we initially label the word"
N19-1317,Q16-1029,1,0.870827,"utions, reranking using fluency, adequacy, and simplicity (FAS weights). Finally, S2S-All-FA integrates all modifications we propose, and reranks using FA weights. 5 Results In this section, we compare the baseline models and various configurations of our model with both standard automatic simplification metrics and a human evaluation. We show qualitative examples where each of our contributions improves the generated simplification in Table 3. 5.1 Automatic Evaluation Following previous work (Zhang and Lapata, 2017; Zhao et al., 2018), we use SARI as our main automatic metric for evaluation (Xu et al., 2016).11 Specifically, SARI calculates how often a generated sentence correctly keeps, inserts, and deletes n-grams from the complex sentence, using the reference simple standard as the gold-standard, where 1 ≤ n ≤ 4. Note that we do not use 9 For Hybrid and DRESS, we use the generated outputs provided in Zhang and Lapata (2017). We made a significant effort to rerun the code for DRESS, but were unable to do so. 10 For DMASS, we ran the authors’ code on our data splits from Newsela, in collaboration with the first author to ensure an accurate comparison. 11 To calculate SARI, we use the original sc"
N19-1317,D17-1062,0,0.387635,"and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement learning (Zhang and Lapata, 2017) and memory augmentation (Zhao et al., 2018), but these systems often still produce outputs that are longer than the reference sentences. To avoid this problem, we propose to extend the generic Seq2Seq framework at both training and inference time by encouraging the model to choose simpler content words, and by effectively choosing an output based on a large set of can3137 Proceedings of NAACL-HLT 2019, pages 3137–3147 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics didate simplifications. The main contributions of this paper can be summarized as"
N19-1317,D18-1355,0,0.549236,"l approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement learning (Zhang and Lapata, 2017) and memory augmentation (Zhao et al., 2018), but these systems often still produce outputs that are longer than the reference sentences. To avoid this problem, we propose to extend the generic Seq2Seq framework at both training and inference time by encouraging the model to choose simpler content words, and by effectively choosing an output based on a large set of can3137 Proceedings of NAACL-HLT 2019, pages 3137–3147 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics didate simplifications. The main contributions of this paper can be summarized as follows: • We propose a custom loss functio"
N19-1317,C10-1152,0,0.452302,"about the largest benefit for the simplification system. We compare our model to several state-of-the-art systems in both an automatic and human evaluation settings, and show that the generated simple sentences are shorter and simpler, while remaining competitive with respect to fluency and adequacy. We also include a detailed error analysis to explain where the model currently falls short and provide suggestions for addressing these issues. 2 Related Work Text simplification has often been addressed as a monolingual translation process, which generates a simplified version of a complex text. Zhu et al. (2010) employ a tree-based translation model and consider sentence splitting, deletion, reordering, and substitution. Coster and Kauchak (2011) use a Phrase-Based Machine Translation (PBMT) system with support for deleting phrases, while Wubben et al. (2012) extend a PBMT system with a reranking heuristic (PBMT-R). Woodsend and Lapata (2011) propose a model based on a quasisynchronous grammar, a formalism able to capture structural mismatches and complex rewrite operations. Narayan and Gardent (2014) combine a sentence splitting and deletion model with PBMT-R. This model has been shown to perform co"
N19-1317,N18-2013,0,0.118978,"o a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence y. The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. P (y|x). Other works have since extended this framework to include attention mechanisms (Luong et al., 2015) and transformer networks (Vaswani et al., 2017).2 Nisioi et al. (2017) was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search. Vu et al. (2018) extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models. There are two main Seq2Seq models we will compare to in this work, along with the statistical model from Narayan and Gardent (2014). Zhang and Lapata (2017) proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a reinforcement learning framework at training time to reward the model for producing sentences that score high on fluency, adequacy, and simplicity. This work showed stateof"
N19-1317,D11-1038,0,0.515178,"in recent years due to its relevance for NLP tasks. Simplifying text during preprocessing can improve the performance of syntactic parsers (Chandrasekar et al., 1996) and 1 Our code is available in our fork of Sockeye (Hieber et al., 2017) at https://github.com/rekriz11/sockeye-recipes. semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014), and can improve the grammaticality (fluency) and meaning preservaˇ tion (adequacy) of translation output (Stajner and Popovic, 2016). Most text simplification work has approached the task as a monolingual machine translation problem (Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Once viewed as such, a natural approach is to use sequence-to-sequence (Seq2Seq) models, which have shown state-ofthe-art performance on a variety of NLP tasks, including machine translation (Vaswani et al., 2017) and dialogue systems (Vinyals and Le, 2015). One of the main limitations in applying standard Seq2Seq models to simplification is that these models tend to copy directly from the original complex sentence too often, as this is the most common operation in simplification. Several recent efforts have attempted to alleviate this problem using reinforcement"
N19-1317,P12-1107,0,0.565081,"Missing"
N19-4011,P17-4012,0,0.0255253,"s test statistical significance. IRT is the basis for almost all psychometric studies (Embretson and Reise, 2013). We follow the work of Otani et al. (2016), who used head-to-head pairwise testing to compare machine translation systems. However, we further this work by also examining the discriminative power of prompts. For instance “my name is david . what is my name ?” from the NCM evaluation dataset has been shown to have low discriminative power, Selection of Baselines We seek to establish reasonable public baselines for Seq2Seq-based chatbots. All models trained by us use the OpenNMT-py (Klein et al., 2017) Seq2Seq implementation with its default parameters: two layers of LSTMs with 512 hidden neurons for the bidirectional encoder and the unidirectional decoder. We trained models on three standard datasets: OpenSubtitles, SubTle, and Twitter, and plan to introduce baselines trained on other datasets. The number of baseline methods will continue to grow. We plan to add an information retrieval baseline, the hierarchical encoder-decoder model (Serban et al., 2017), and several other baselines from ParlAI. Conclusion and Future Work ChatEval is a framework for systematic evaluation of chatbots. The"
N19-4011,W18-5028,0,0.0449826,"ion of chatbots in a standardized way, and (2) a web portal for accessing model code, trained parameters, and evaluation results, which grows with participation. In addition, ChatEval includes newly created and curated evaluation datasets with both human annotated and automated baselines. RankME5 (Novikova et al., 2018) is an evaluation system for natural language generation. While RankME could be adapted for chatbot evaluation, this would require significant modification of the source code. Furthermore, RankME is only a crowdsourcing framework, which is more narrow than ChatEval. DialCrowd6 (Lee et al., 2018) is a tool for the easy creation of human evaluation tasks for conversational agents. Finally, Kaggle7 is another important venue for competitions, which allows for multiple test beds. However, none of these tools and websites offer a unified solution to public baselines, evaluation sets, and an integrated A/B model testing framework. In many ways, the goal of ChatEval is similar to Appraise: an Open-Source Toolkit for Manual Evaluation of MT Output (Federmann, 2012). Just as Appraise is integrated with WMT, ChatEval should also be used in shared tasks in dialog competitions. Related Work The"
N19-4011,D18-1431,0,0.0316226,"amazon.com/ alexaprize 2 http://convai.io/ 3 http://workshop.colips.org/wochat/ 4 https://parl.ai 5 https://github.com/jeknov/RankME https://dialrc.org/dialcrowd.html 7 https://www.kaggle.com 6 61 Overfitting One important feature of ChatEval is the ease of adding new evaluation datasets. In order to assure that researchers are not overfitting to any evaluation set, the ChatEval team will take top performing models and also apply them to other datasets. New evaluation datasets can be added upon request from the ChatEval team. We plan to add both the prompts as well as the model responses from Baheti et al. (2018) as well as Li et al. (2019). Finally, we have added the ability to interact with baseline models using FlowAI (Wubben, 2018).10 the ChatEval website. The profile consists of the URLs and description provided by the researcher, the responses of the model to each prompt in the evaluation set, and a visualization of the results of human and automatic evaluation. Response Comparison To facilitate the qualitative comparison of models, we offer a response comparison interface where users can see all the prompts in a particular evaluation set and the responses generated by each model. Evaluation Dat"
N19-4011,D17-2014,0,0.0308827,"king for absolute assessments of quality yields less discriminative results than soliciting direct comparisons of quality. In the dataset introduced for the ConvAI2 competition, nearly all the proposed algorithms were evaluated to be within one standard deviation of each other (Zhang et al., 2018). Therefore, for our human evaluation task, we ask humans to directly compare the responses of two models given the previous utterances in the conversation. Both Facebook and Amazon have developed evaluation systems that allow humans to converse with (and then rate) a chatbot (Venkatesh et al., 2018; Miller et al., 2017). Facebook’s ParlAI 4 is the most comparable system for a unified framework for sharing, training, and evaluating chatbots; however, ChatEval is different in that it entirely focuses on the evaluation and warehousing of models. Our infrastructure takes as input text files containing model responses and does not require any code base integration. The ChatEval web interface consists of four primary pages. Aside from the overview page, there is a model submission form, a page for viewing the profile of any submitted model, and a page for comparing the responses of multiple models. Model Submissio"
N19-4011,N18-2012,0,0.107522,"Missing"
N19-4011,P13-1166,0,0.0333177,"l's profile on proﬁle on ChatEval SETC Evaluation Toolkit Automatic Evaluation Introduction Figure 1: Flow of information in ChatEval. A researcher submits information about her model, including its responses to prompts in a standard evaluation set. Automatic evaluation as well as human evaluation are conducted, then the results are posted publicly on the ChatEval website. Reproducibility and model assessment for opendomain dialog systems is challenging, as many small variations in the training setup or evaluation technique can result in significant differences in perceived model performance (Fokkens et al., 2013). While reproducibility is problematic for NLP in general, this is especially true for dialog systems due to the lack of automatic metrics. In addition, as the field has grown, it has become increasingly fragmented in human evaluation methodologies. Papers often focus on novel methods, but insufficient attention is paid to ensuring that datasets and evaluation remain consistent and reproducible. For example, while human evaluation of chatbot quality is extremely common, few papers publish the set of prompts used for this evaluation, and almost no papers release their learned model parameters."
N19-4011,P18-1205,0,0.196042,"Appraise is integrated with WMT, ChatEval should also be used in shared tasks in dialog competitions. Related Work The ChatEval Web Interface Prize,1 ConvAI2 Competitions such as the Alexa and WOCHAT,3 rank submitted chatbots by having humans converse with them and then rate the quality of the conversation. However, asking for absolute assessments of quality yields less discriminative results than soliciting direct comparisons of quality. In the dataset introduced for the ConvAI2 competition, nearly all the proposed algorithms were evaluated to be within one standard deviation of each other (Zhang et al., 2018). Therefore, for our human evaluation task, we ask humans to directly compare the responses of two models given the previous utterances in the conversation. Both Facebook and Amazon have developed evaluation systems that allow humans to converse with (and then rate) a chatbot (Venkatesh et al., 2018; Miller et al., 2017). Facebook’s ParlAI 4 is the most comparable system for a unified framework for sharing, training, and evaluating chatbots; however, ChatEval is different in that it entirely focuses on the evaluation and warehousing of models. Our infrastructure takes as input text files conta"
P04-1023,J93-2003,0,0.0355453,"importance of amplifying the contribution of word-aligned data during parameter estimation. This paper shows that word-level alignments improve the parameter estimates for translation models, which in turn results in improved statistical translation for languages that do not have large sentence-aligned parallel corpora. 2 Parameter Estimation Using Sentence-Aligned Corpora The task of statistical machine translation is to choose the source sentence, e, that is the most probable translation of a given sentence, f , in a foreign language. Rather than choosing e∗ that directly maximizes p(e|f ), Brown et al. (1993) apply Bayes’ rule and select the source sentence: e∗ = arg max p(e)p(f |e). e (1) In this equation p(e) is a language model probability and is p(f |e) a translation model probability. A series of increasingly sophisticated translation models, referred to as the IBM Models, was defined in Brown et al. (1993). The translation model, p(f |e) defined as a marginal probability obtained by summing over word-level alignments, a, between the source and target sentences: X p(f |e) = p(f , a|e). (2) a While word-level alignments are a crucial component of the IBM models, the model parameters are genera"
P04-1023,W01-1409,0,0.0135292,"of labeled and unlabeled data. 7 Discussion and Future Work In this paper we show with the appropriate modification of EM significant improvement gains can be had through labeling word alignments in a bilingual corpus. Because of this significantly less data is required to achieve a low alignment error rate or high Bleu score. This holds even when using noisy word alignments such as our automatically created set. One should take our research into account when trying to efficiently create a statistical machine translation system for a language pair for which a parallel corpus is not available. Germann (2001) describes the cost of building a Tamil-English parallel corpus from scratch, and finds that using professional translations is prohibitively high. In our experience it is quicker to manually word-align translated sentence pairs than to translate a sentence, and word-level alignment can be done by someone who might not be fluent enough to produce translations. It might therefore be possible to achieve a higher performance at a fraction of the cost by hiring a nonprofessional produce word-alignments after a limited set of sentences have been translated. We plan to investigate whether it is feas"
P04-1023,N03-1017,0,0.0757788,"act that using word-aligned data in estimating the parameters for machine translation leads to better alignments is predictable. A more significant result is whether it leads to improved translation quality. In order to test that our improved parameter estimates lead to better translation quality, we used a state-of-the-art phrase-based decoder to translate a held out set of German sentences into English. The phrase-based decoder extracts phrases from the word alignments produced by GIZA++, and computes translation probabilities based on the frequency of one phrase being aligned with another (Koehn et al., 2003). We trained a language model Ratio 0.1 0.2 0.3 0.5 0.7 0.9 AER when λ = Standard MLE 11.73 10.89 10.23 8.65 8.29 7.78 when λ = .9 9.40 8.66 8.13 8.19 8.03 7.78 Table 5: The effect of weighting word-aligned data more heavily that its proportion in the training data (corpus size 16000 sentence pairs) using the 34,000 English sentences from the training set. Table 4 shows that using word-aligned data leads to better translation quality than using sentencealigned data. Particularly, significantly less data is needed to achieve a high Bleu score when using word alignments. Training on a corpus of"
P04-1023,W03-0301,0,0.0551552,"Missing"
P04-1023,J03-1002,0,0.0361955,"the word aligned data. The example alignments were held out sentence pairs that were aligned after training on 500 sentence pairs. The alignments produced when the training on word-aligned data are dramatically better than when training on sentence-aligned data. We contrasted these improvements with the improvements that are to be had from incorporating a bilingual dictionary into the estimation process. For this experiment we allowed a bilingual dictionary to constrain which words can act as translations of each other during the initial estimates of translation probabilities (as described in Och and Ney (2003)). As can be seen in Table 3, using a dictionary reduces the AER when compared to using GIZA++ without a dictionary, but not as dramatically as integrating the word-alignments. We further tried combining a dictionary with our word-alignments but found that the dictionary results in only very minimal improvements over using word-alignments alone. 1. Verifying that the use of word-aligned data has an impact on the quality of alignments predicted by the IBM Models, and comparing the quality increase to that gained by using a bilingual dictionary in the estimation stage. 2. Evaluating whether impr"
P04-1023,2001.mtsummit-papers.68,0,0.0186586,"Missing"
P04-1023,J03-3002,0,0.00588674,"on probabilistic translation models (Brown et al., 1993) are generally trained using sentence-aligned parallel corpora. For many language pairs these exist in abundant quantities. However for new domains or uncommon language pairs extensive parallel corpora are often hard to come by. Two factors could increase the performance of statistical machine translation for new language pairs and domains: a reduction in the cost of creating new training data, and the development of more efficient methods for exploiting existing training data. Approaches such as harvesting parallel corpora from the web (Resnik and Smith, 2003) address the creation of data. We take the second, complementary approach. We address the problem of efficiently exploiting existing parallel corpora by adding explicit word-level alignments between a number of the sentence pairs in the training corpus. We modify the standard parameter estimation procedure for IBM Models and HMM variants so that they can exploit these additional wordlevel alignments. Our approach uses both word- and sentence-level alignments for training material. 1. Describe how the parameter estimation framework of Brown et al. (1993) can be adapted to incorporate word-level"
P04-1023,P02-1040,0,\N,Missing
P05-1032,J93-2003,0,0.0128684,", June 2005. 2005 Association for Computational Linguistics length 1 2 3 4 5 6 7 8 9 10 num uniq (mil) .88 16.5 42.6 58.7 65.0 66.4 65.8 64.3 62.2 59.9 average # translations 8.322 1.733 1.182 1.065 1.035 1.022 1.015 1.012 1.010 1.010 avg trans length 1.37 2.35 3.44 4.58 5.75 6.91 8.07 9.23 10.4 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3"
P05-1032,N03-1017,0,0.0312701,"hu• Motivate the problem with storing enumerated phrases in a table by examining the memory requirements of the method for the NIST data set • Detail the advantages of using long phrases in SMT, and examine their potential coverage • Describe a suffix array-based data structure which allows for the retrieval of translations of arbitrarily long phrases, and show that it requires far less memory than a table • Calculate the computational complexity and average time for retrieving phrases and show how this can be sped up by orders of magnitude with no loss in translation accuracy 2 Related Work Koehn et al. (2003) compare a number of different approaches to phrase-based statistical machine 255 Proceedings of the 43rd Annual Meeting of the ACL, pages 255–262, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics length 1 2 3 4 5 6 7 8 9 10 num uniq (mil) .88 16.5 42.6 58.7 65.0 66.4 65.8 64.3 62.2 59.9 average # translations 8.322 1.733 1.182 1.065 1.035 1.022 1.015 1.012 1.010 1.010 avg trans length 1.37 2.35 3.44 4.58 5.75 6.91 8.07 9.23 10.4 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. translation including the joint probability phrasebased model (M"
P05-1032,koen-2004-pharaoh,0,0.0547244,"regularly produced by other human activity. For some language pairs very large sets of training data are now available. The publications of the European Union and United Nations provide gigbytes of data between various language pairs which can be easily mined using a web crawler. The Linguistics Data Consortium provides an excellent set of off the shelf Arabic-English and Chinese-English parallel corpora for the annual NIST machine translation evaluation exercises. The size of the NIST training data presents a problem for phrase-based statistical machine translation. Decoders such as Pharaoh (Koehn, 2004) primarily use lookup tables for the storage of phrases and their translations. Since retrieving longer segments of hu• Motivate the problem with storing enumerated phrases in a table by examining the memory requirements of the method for the NIST data set • Detail the advantages of using long phrases in SMT, and examine their potential coverage • Describe a suffix array-based data structure which allows for the retrieval of translations of arbitrarily long phrases, and show that it requires far less memory than a table • Calculate the computational complexity and average time for retrieving p"
P05-1032,W02-1018,0,0.0285929,") compare a number of different approaches to phrase-based statistical machine 255 Proceedings of the 43rd Annual Meeting of the ACL, pages 255–262, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics length 1 2 3 4 5 6 7 8 9 10 num uniq (mil) .88 16.5 42.6 58.7 65.0 66.4 65.8 64.3 62.2 59.9 average # translations 8.322 1.733 1.182 1.065 1.035 1.022 1.015 1.012 1.010 1.010 avg trans length 1.37 2.35 3.44 4.58 5.75 6.91 8.07 9.23 10.4 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (200"
P05-1032,J04-4002,0,0.00836157,"achine 255 Proceedings of the 43rd Annual Meeting of the ACL, pages 255–262, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics length 1 2 3 4 5 6 7 8 9 10 num uniq (mil) .88 16.5 42.6 58.7 65.0 66.4 65.8 64.3 62.2 59.9 average # translations 8.322 1.733 1.182 1.065 1.035 1.022 1.015 1.012 1.010 1.010 avg trans length 1.37 2.35 3.44 4.58 5.75 6.91 8.07 9.23 10.4 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growin"
P05-1032,P02-1040,0,0.110776,"termine the effect of different levels of sampling, we compare the translation quality against cumulative retrieval time for calculating the phrase translation probabilities for all subphrases in an evaluation set. We translated a held out set of 430 German sentences with 50 words or less into English. The test sentences were drawn from the 01/17/00 proceedings of the Europarl corpus. The remainder of the corpus (1 million sentences) was used as training data to calculate the phrase translation probabilities. We calculated the translation quality using Bleu’s modified n-gram precision metric (Papineni et al., 2002) for n-grams of up to length four. The framework that we used to calculate the translation probabilities was similar to that detailed in Koehn et al. (2003). That is: ˆ = arg max p(eI1 |f1I ) e eI1 = arg max pLM (eI1 ) ∗ eI1 I Y (6) (7) i=1 Where pLM is a language model probability and d is a distortion probability which penalizes movement. Table 6 gives a comparison of the translation quality under different levels of sampling. While the ac261 quality .290 .289 .291 .289 .288 .288 .288 Table 6: A comparison of retrieval times and translation quality when the number of translations is capped a"
P05-1032,W03-1001,0,0.0173349,"15 1.012 1.010 1.010 avg trans length 1.37 2.35 3.44 4.58 5.75 6.91 8.07 9.23 10.4 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 Scaling to Long Phrases Table 1 gives statistics about the Arabic-English parallel corpus used in the NIST large data track. The corpus contains 3.75 million sentence pairs, and has 127 million words in English"
P05-1032,P03-1041,0,0.00957709,"lity phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 Scaling to Long Phrases Table 1 gives statistics about the Arabic-English parallel corpus used in the NIST large data track. The corpus contains 3.75 million sentence pairs, and has 127 million words in English, and 106 million words in Arabic. The table shows the number of unique Arabic phrases, and gives the average number of translations into English and their average length. Table 2 gives estimates of the size of"
P05-1032,2003.mtsummit-papers.53,0,0.0206887,"u and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 Scaling to Long Phrases Table 1 gives statistics about the Arabic-English parallel corpus used in the NIST large data track. The corpus contains 3.75 million sentence pairs, and has 127 million words in English, and 106 million words in Arabic. The table shows the number of unique Arabic phrases, and gives the average number of translations into English and their average length. Table 2 gives estimates of the size of the lookup tables neede"
P05-1032,J01-1001,0,0.0869974,"suffix array, which is identical in length to the corpus. Figure 2 shows the final state of the suffix array, which is as a list of the indices of words in the corpus that corresponds to an alphabetically sorted list of the suffixes. The advantages of this representation are that it is compact and easily searchable. The total size of the suffix array is a constant amount of memory. Typically it is stored as an array of integers where the array is the same length as the corpus. Because it is organized alphabetically, any phrase can be quickly located within it using a binary search algorithm. Yamamoto and Church (2001) show how to use suffix arrays to calculate a number of statistics that are interesting in natural language processing applications. They demonstrate how to calculate term fre258 quency / inverse document frequency (tf / idf) for all n-grams in very large corpora, as well as how to use these frequencies to calculate n-grams with high mutual information and residual inverse document frequency. Here we show how to apply suffix arrays to parallel corpora to calculate phrase translation probabilities. 4.1 Applied to parallel corpora In order to adapt suffix arrays to be useful for statistical mach"
P05-1074,W03-1608,0,0.276602,"ration the production of paraphrases allows for the creation of more varied and fluent text (Iordanskaja et al., 1991). In multidocument summarization the identification of paraphrases allows information repeated across documents to be condensed (McKeown et al., 2002). In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al., 2003). In question answering, discovering paraphrased answers may provide additional evidence that an answer is correct (Ibrahim et al., 2003). In this paper we introduce a novel method for extracting paraphrases that uses bilingual parallel corpora. Past work (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Ibrahim et al., 2003) has examined the use of monolingual parallel corpora for paraphrase extraction. Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English, and data created for machine translation evaluation methods such as Bleu (Papineni et al., 2002) which use multiple reference translations. While the results reported for these me"
P05-1074,N03-1017,0,0.045721,"lel corpora. Small data sets mean a limited number of paraphrases can be extracted. Furthermore, the narrow range of text genres available for monolingual parallel corpora limits the range of contexts in which the paraphrases can be used. Instead of relying on scarce monolingual parallel data, our method utilizes the abundance of bilingual parallel data that is available. This allows us to create a much larger inventory of phrases that is applicable to a wider range of texts. Our method for identifying paraphrases is an extension of recent work in phrase-based statistical machine translation (Koehn et al., 2003). The essence of our method is to align phrases in a bilingual parallel corpus, and equate different English phrases that are aligned with the same phrase in the other language. This assumption of similar mean597 Proceedings of the 43rd Annual Meeting of the ACL, pages 597–604, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics Emma burst into tears and he tried to comfort her, saying things to make her smile. Emma cried, and he tried to console her, adorning his words with puns. Figure 1: Using a monolingal parallel corpus to extract paraphrases ing when multiple phrases m"
P05-1074,N03-1003,0,0.818061,"of paraphrases allows information repeated across documents to be condensed (McKeown et al., 2002). In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al., 2003). In question answering, discovering paraphrased answers may provide additional evidence that an answer is correct (Ibrahim et al., 2003). In this paper we introduce a novel method for extracting paraphrases that uses bilingual parallel corpora. Past work (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Ibrahim et al., 2003) has examined the use of monolingual parallel corpora for paraphrase extraction. Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English, and data created for machine translation evaluation methods such as Bleu (Papineni et al., 2002) which use multiple reference translations. While the results reported for these methods are impressive, their usefulness is limited by the scarcity of monolingual parallel corpora. Small data sets mean a limited number of paraphrases can be extracted."
P05-1074,koen-2004-pharaoh,0,0.00932232,"Instead, we use phrases in the other language as pivots: we look at what foreign language phrases the English translates to, find all occurrences of those foreign phrases, and then look back at what other English phrases they translate to. We treat the other 598 Thus Brown et al. decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the words in the sentences. More recent approaches to statistical translation calculate the translation probability using larger blocks of aligned text. Koehn (2004), Tillmann (2003), and Vogel et al. (2003) describe various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) models. We use the heuristic for phrase alignment described in Och and Ney (2003) which aligns phrases by incrementally building longer phrases from words and phrases which have adjacent alignment points.1 1 Note that while we induce the translations of phrases from what is im more, the übrigen wir sind ist cost dynamic is completely under control die diesbezügliche kostenentwicklung völlig unter kontrolle es"
P05-1074,W02-1018,0,0.016832,"wardly using maximum likelihood estimation by counting how often the phrases e and f were aligned in the parallel corpus: p(e|f ) = count(e, f ) e count(e, f ) P (3) Note that the paraphrase probability defined in Equation 2 returns the single best paraphrase, eˆ2 , irrespective of the context in which e1 appears. Since the best paraphrase may vary depending on information about the sentence that e1 appears in, we extend the paraphrase probability to include that sentence S: eˆ2 = arg max p(e2 |e1 , S) e2 6=e1 (4) word-level alignments in this paper, direct estimation of phrasal translations (Marcu and Wong, 2002) would also suffice for extracting paraphrases from bilingual corpora. 599 a million, as far as possible, at work, big business, carbon dioxide, central america, close to, concentrate on, crystal clear, do justice to, driving force, first half, for the first time, global warming, great care, green light, hard core, horn of africa, last resort, long ago, long run, military action, military force, moment of truth, new world, noise pollution, not to mention, nuclear power, on average, only too, other than, pick up, president clinton, public transport, quest for, red cross, red tape, socialist par"
P05-1074,J03-1002,0,0.0346299,"Thus Brown et al. decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the words in the sentences. More recent approaches to statistical translation calculate the translation probability using larger blocks of aligned text. Koehn (2004), Tillmann (2003), and Vogel et al. (2003) describe various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) models. We use the heuristic for phrase alignment described in Och and Ney (2003) which aligns phrases by incrementally building longer phrases from words and phrases which have adjacent alignment points.1 1 Note that while we induce the translations of phrases from what is im more, the übrigen wir sind ist cost dynamic is completely under control die diesbezügliche kostenentwicklung völlig unter kontrolle es we owe relevant den it steuerzahlern to the schuldig die kosten taxpayers to keep the costs unter kontrolle zu haben in check Figure 2: Using a bilingual parallel corpus to extract paraphrases 2.2 Assigning probabilities We define a paraphrase probability p(e2 |e1 ) i"
P05-1074,P01-1008,0,0.700926,"arization the identification of paraphrases allows information repeated across documents to be condensed (McKeown et al., 2002). In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al., 2003). In question answering, discovering paraphrased answers may provide additional evidence that an answer is correct (Ibrahim et al., 2003). In this paper we introduce a novel method for extracting paraphrases that uses bilingual parallel corpora. Past work (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Ibrahim et al., 2003) has examined the use of monolingual parallel corpora for paraphrase extraction. Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English, and data created for machine translation evaluation methods such as Bleu (Papineni et al., 2002) which use multiple reference translations. While the results reported for these methods are impressive, their usefulness is limited by the scarcity of monolingual parallel corpora. Small data sets mean a limited number of paraph"
P05-1074,N03-1024,0,0.116912,"e alternative ways of conveying the same information. Paraphrases are useful in a number of NLP applications. In natural language generation the production of paraphrases allows for the creation of more varied and fluent text (Iordanskaja et al., 1991). In multidocument summarization the identification of paraphrases allows information repeated across documents to be condensed (McKeown et al., 2002). In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al., 2003). In question answering, discovering paraphrased answers may provide additional evidence that an answer is correct (Ibrahim et al., 2003). In this paper we introduce a novel method for extracting paraphrases that uses bilingual parallel corpora. Past work (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Ibrahim et al., 2003) has examined the use of monolingual parallel corpora for paraphrase extraction. Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English, and data created for machine translation e"
P05-1074,J93-2003,0,0.0257593,"ly extracts more than one possible paraphrase for each phrase. We assign a probability to each of the possible paraphrases. This is a mechanism for ranking paraphrases, which can be utilized when we come to select the correct paraphrase for a given context . Section 2.2 explains how we calculate the probability of a paraphrase. 2.1 Aligning phrase pairs We use phrase alignments in a parallel corpus as pivots between English paraphrases. We find these alignments using recent phrase-based approaches to statistical machine translation. The original formulation of statistical machine translation (Brown et al., 1993) was defined as a word-based operation. The probability that a foreign sentence is the translation of an English sentence is calculated by summing over the probabilities of all possible word-level alignments, a, between the sentences: X p(f |e) = p(f , a|e) a Much previous work on extracting paraphrases (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003) has focused on finding identifying contexts within aligned monolingual sentences from which divergent text can be extracted, and treated as paraphrases. Barzilay and McKeown (2001) gives the example shown in Figure 1 of how"
P05-1074,P02-1040,0,0.0990409,"ring paraphrased answers may provide additional evidence that an answer is correct (Ibrahim et al., 2003). In this paper we introduce a novel method for extracting paraphrases that uses bilingual parallel corpora. Past work (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Ibrahim et al., 2003) has examined the use of monolingual parallel corpora for paraphrase extraction. Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English, and data created for machine translation evaluation methods such as Bleu (Papineni et al., 2002) which use multiple reference translations. While the results reported for these methods are impressive, their usefulness is limited by the scarcity of monolingual parallel corpora. Small data sets mean a limited number of paraphrases can be extracted. Furthermore, the narrow range of text genres available for monolingual parallel corpora limits the range of contexts in which the paraphrases can be used. Instead of relying on scarce monolingual parallel data, our method utilizes the abundance of bilingual parallel data that is available. This allows us to create a much larger inventory of phra"
P05-1074,P04-1023,1,0.38483,"Missing"
P05-1074,P02-1033,0,0.0194839,"are aligned with the same phrase in the other language. This assumption of similar mean597 Proceedings of the 43rd Annual Meeting of the ACL, pages 597–604, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics Emma burst into tears and he tried to comfort her, saying things to make her smile. Emma cried, and he tried to console her, adorning his words with puns. Figure 1: Using a monolingal parallel corpus to extract paraphrases ing when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language (we return to this point in Section 4.4). The remainder of this paper is as follows: Section 2 contrasts our method for extracting paraphrases with the monolingual case, and describes how we rank the extracted paraphrases with a probability assignment. Section 3 describes our experimental setup and includes information about how phrases were selected, how we manually aligned parts of the bilingual corpus, and how we evaluated the paraphrases. Section 4 gives the results of our evalu"
P05-1074,W03-1001,0,0.0509502,"se phrases in the other language as pivots: we look at what foreign language phrases the English translates to, find all occurrences of those foreign phrases, and then look back at what other English phrases they translate to. We treat the other 598 Thus Brown et al. decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the words in the sentences. More recent approaches to statistical translation calculate the translation probability using larger blocks of aligned text. Koehn (2004), Tillmann (2003), and Vogel et al. (2003) describe various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) models. We use the heuristic for phrase alignment described in Och and Ney (2003) which aligns phrases by incrementally building longer phrases from words and phrases which have adjacent alignment points.1 1 Note that while we induce the translations of phrases from what is im more, the übrigen wir sind ist cost dynamic is completely under control die diesbezügliche kostenentwicklung völlig unter kontrolle es we owe relevant"
P05-1074,2003.mtsummit-papers.53,0,0.0805808,"er language as pivots: we look at what foreign language phrases the English translates to, find all occurrences of those foreign phrases, and then look back at what other English phrases they translate to. We treat the other 598 Thus Brown et al. decompose the problem of determining whether a sentence is a good translation of another into the problem of determining whether there is a sensible mapping between the words in the sentences. More recent approaches to statistical translation calculate the translation probability using larger blocks of aligned text. Koehn (2004), Tillmann (2003), and Vogel et al. (2003) describe various heuristics for extracting phrase alignments from the Viterbi word-level alignments that are estimated using Brown et al. (1993) models. We use the heuristic for phrase alignment described in Och and Ney (2003) which aligns phrases by incrementally building longer phrases from words and phrases which have adjacent alignment points.1 1 Note that while we induce the translations of phrases from what is im more, the übrigen wir sind ist cost dynamic is completely under control die diesbezügliche kostenentwicklung völlig unter kontrolle es we owe relevant den it steuerzahlern to t"
P07-2045,N03-2002,0,0.152204,"nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input."
P07-2045,koen-2004-pharaoh,0,0.148177,"to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb"
P07-2045,D07-1091,1,0.158367,"Missing"
P07-2045,N03-1017,1,0.161374,"informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu 2 Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co"
P07-2045,P03-1021,0,0.176468,"ent data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz"
P07-2045,J03-1002,0,0.164868,"L 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre"
P07-2045,P02-1040,0,0.148118,"d language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo"
P07-2045,N07-1062,1,0.152186,"up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more"
P07-2045,D08-1076,0,\N,Missing
P07-2045,2006.iwslt-evaluation.8,1,\N,Missing
P09-4007,P05-1032,1,0.805694,"ssor architectures and distributed computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2 ; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optim"
P09-4007,J07-2003,0,0.827653,"lation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was suppo"
P09-4007,P03-2041,0,0.0474557,"ize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). 2 The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable as the SRILM native bridge code. 3 The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. 26 • Variational Decoding: spurious ambiguity causes the probability of an output string among to be split among many derivations. The goodness of a string is measured by the total probability of its derivations, which means that finding the best output string is computatio"
P09-4007,P06-1121,0,0.410414,"grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Resear"
P09-4007,W05-1506,0,0.0961856,"t this demonstration description paper. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, c Suntec, Singapore, 3 August 2009. 2009 ACL and AFNLP System google lium dcu joshua uka limsi uedin rwth cmu-statxfer BLEU-4 31.14 26.89 26.86 26.52 25.96 25.51 25.44 24.89 23.65 • Pruning: We incorporate beam- and cubepruning (Chiang, 2007) to make decoding feasible for large SCFGs. • k-best extraction: Given a source sentence, the chart-parsing algorithm produces a hypergraph representing an exponential number of derivation hypotheses. We implement the extraction algorithm of Huang and Chiang (2005) to extract the k most likely derivations from the hypergraph. Table 1: BLEU scores for top primary systems on the WMT-09 French-English Task from CallisonBurch et al. (2009), who also provide human evaluation results. 2.1 • Oracle Extraction: Even within the large set of translations represented by a hypergraph, some desired translations (e.g. the references) may not be contained due to pruning or inherent modeling deficiency. We implement an efficient dynamic programming algorithm (Li and Khudanpur, 2009) for finding the oracle translations, which are most similar to the desired translations"
P09-4007,P09-1067,1,0.853412,"tand-alone tool that does not rely on the rest of the toolkit. Scalability: Joshua, especially the decoder, is scalable to large models and data sets. For example, the parsing and pruning algorithms are implemented with dynamic programming strategies and efficient data structures. We also utilize suffixarray grammar extraction, parallel/distributed decoding, and bloom filter language models. Joshua offers state-of-the-art quality, having been ranked 4th out of 16 systems in the FrenchEnglish task of the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. We describe Joshua (Li et al., 2009a)1 , an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interest"
P09-4007,P07-2045,1,0.0131579,"lkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No. HR0011-06-2-0001 and the National Science Foundation under grants No. 0713448 and 0840112. The views and findings are the authors’ alone. 1 Please cite Li et al. (2009a) if you use Joshua in your research, and not this demonstration description paper. 25 Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, c Suntec, Singapore, 3 August 2009. 2009 ACL and AFNLP System google lium dcu joshua uka limsi uedi"
P09-4007,P06-1077,0,0.259566,"rsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the Defense Advanced Research Projects Agency’s GA"
P09-4007,D07-1104,0,0.0157455,"d computing (Li and Khudanpur, 2008). • Language Models: We implement three local n-gram language models: a straightforward implementation of the n-gram scoring function in Java, capable of reading standard ARPA backoff n-gram models; a native code bridge that allows the decoder to use the SRILM toolkit to read and score ngrams2 ; and finally a Bloom Filter implementation following Talbot and Osborne (2007). • Suffix-array Grammar Extraction: Grammars extracted from large training corpora are often far too large to fit into available memory. Instead, we follow Callison-Burch et al. (2005) and Lopez (2007), and use a source language suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using th"
P09-4007,W08-0402,1,0.798672,"Missing"
P09-4007,P03-1021,0,0.0153466,"guage suffix array to extract only rules that will actually be used in translating a particular test set. Direct access to the suffix array is incorporated into the decoder, allowing rule extraction to be performed for each input sentence individually, but it can also be executed as a standalone pre-processing step. • Minimum Error Rate Training: Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measured by an automatic evaluation metric, such as BLEU. The optimization consists of a series of line-optimizations using the efficient method of Och (2003). More details on the MERT method and the implementation can be found in Zaidan (2009).3 • Grammar formalism: Our decoder assumes a probabilistic synchronous contextfree grammar (SCFG). It handles SCFGs of the kind extracted by Hiero (Chiang, 2007), but is easily extensible to more general SCFGs (as in Galley et al. (2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). 2 The first implementation allows users to easily try the Joshua toolkit without installing SRILM. However, users should note that the basic Java LM implementation is not as scalable"
P09-4007,P05-1034,0,0.0603092,"hronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit. 1 Joshua Toolkit Introduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a Java-based generalpurpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. ∗ This research was supported in part by the D"
P09-4007,N09-2003,1,0.88482,"Missing"
P09-4007,P07-1065,0,0.0575308,"Missing"
P09-4007,W09-0424,1,0.2272,"tand-alone tool that does not rely on the rest of the toolkit. Scalability: Joshua, especially the decoder, is scalable to large models and data sets. For example, the parsing and pruning algorithms are implemented with dynamic programming strategies and efficient data structures. We also utilize suffixarray grammar extraction, parallel/distributed decoding, and bloom filter language models. Joshua offers state-of-the-art quality, having been ranked 4th out of 16 systems in the FrenchEnglish task of the 2009 WMT evaluation, both in automatic (Table 1) and human evaluation. We describe Joshua (Li et al., 2009a)1 , an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit’s features to potential users, whether they be newcomers to the field or power users interest"
P09-4007,W09-0401,1,\N,Missing
P09-4007,D08-1076,0,\N,Missing
P10-1088,P01-1005,0,0.0135121,"ts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the problem of diminishing returns that we discussed in Section 1. The second area of work that is related to ours is previous work on AL that is cost-conscious. The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal a"
P10-1088,W10-0733,1,0.58534,"Missing"
P10-1088,W08-0620,1,0.821917,"in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the"
P10-1088,W09-1107,1,0.649941,"Missing"
P10-1088,E06-1032,1,0.658696,"Missing"
P10-1088,J07-2003,0,0.0145546,". All experiments in this paper evaluate on a genre-balanced split of the NIST2008 Urdu-English test set. In addition, the language pack contains an Urdu-English dictionary consisting of ≈ 114000 entries. In all the experiments, we use the dictionary at every iteration of training. This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009). We hereafter refer to these systems as jHier and jSyntax, respectively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: Figure 2: The VG sentence selection algorithm do not occur at all in our so-far labeled data. We call an n-gram “covered” if it occurs at least once in our so-far labeled data. VG has a preference for covering frequent n-grams before"
P10-1088,W05-0619,0,0.195875,"Missing"
P10-1088,P00-1016,0,0.0182088,"hey recognize the problem of diminishing returns that we discussed in Section 1. The second area of work that is related to ours is previous work on AL that is cost-conscious. The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal annotation cost. An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions. The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the"
P10-1088,P09-1021,0,0.391667,"s doesn’t even begin until much higher performance has already been achieved with a period of diminishing returns firmly established. Figure 1 shows the learning curves for two state of the art statistical machine translation (SMT) systems for Urdu-English translation. Observe how the learning curves rise rapidly at first but then a trend of diminishing returns occurs: put simply, the curves flatten. This paper investigates whether we can buck the trend of diminishing returns, and if so, how we can do it effectively. Active learning (AL) has been applied to SMT recently (Haffari et al., 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1. In contrast, we are interested in applying AL when a large amount of data already exists as is the case for many important lanuage pairs. We develop an AL algorithm that focuses on keeping annotation costs (measured by time in seconds) low. It succeeds in doing this by only soliciting translations for parts of sentences. We show that this gets a savings in human annotation time above and beyond what the reduction i"
P10-1088,N04-1012,0,0.0254325,"nscious. The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal annotation cost. An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. King et al. (2004) design a robot that tests gene functions. The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. Unlike our situation for SMT, their costs are all known beforehand because they are simply the cost of materials to conduct the experiments, which are already known to the robot. Hachey et al. (2005) showed that selectively sampled examples for an NER task took long"
P10-1088,N09-1047,0,0.492433,"of our main experiments doesn’t even begin until much higher performance has already been achieved with a period of diminishing returns firmly established. Figure 1 shows the learning curves for two state of the art statistical machine translation (SMT) systems for Urdu-English translation. Observe how the learning curves rise rapidly at first but then a trend of diminishing returns occurs: put simply, the curves flatten. This paper investigates whether we can buck the trend of diminishing returns, and if so, how we can do it effectively. Active learning (AL) has been applied to SMT recently (Haffari et al., 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1. In contrast, we are interested in applying AL when a large amount of data already exists as is the case for many important lanuage pairs. We develop an AL algorithm that focuses on keeping annotation costs (measured by time in seconds) low. It succeeds in doing this by only soliciting translations for parts of sentences. We show that this gets a savings in human annotation time above and"
P10-1088,P02-1040,0,0.108104,"ively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: Figure 2: The VG sentence selection algorithm do not occur at all in our so-far labeled data. We call an n-gram “covered” if it occurs at least once in our so-far labeled data. VG has a preference for covering frequent n-grams before covering infrequent n-grams. The VG method is depicted in Figure 2. Figure 3 shows the learning curves for both jHier and jSyntax for VG selection and random selection. The y-axis measures BLEU score (Papineni et al., 2002),which is a fast automatic way of measuring translation quality that has been shown to correlate with human judgments and is perhaps the most widely used metric in the MT community. The x-axis measures the number of sentence translation pairs in the training data. The VG curves are cut off at the point at which the stopping criterion in Section 3.3 is met. From Figure 3 it might appear that VG selection is better than random selection, achieving higher-performing systems with fewer translations in the labeled data. However, it is important to take care when measuring annotation costs (especial"
P10-1088,W00-1306,0,0.0772571,"beyond what the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augme"
P10-1088,P02-1064,0,0.0825959,"t the reduction in words would indicate. Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set"
P10-1088,D08-1027,0,0.0208766,"Missing"
P10-1088,P09-1117,0,0.0385684,"Missing"
P10-1088,W09-0424,1,0.828466,"pack contains an Urdu-English dictionary consisting of ≈ 114000 entries. In all the experiments, we use the dictionary at every iteration of training. This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009). We hereafter refer to these systems as jHier and jSyntax, respectively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: Figure 2: The VG sentence selection algorithm do not occur at all in our so-far labeled data. We call an n-gram “covered” if it occurs at least once in our so-far labeled data. VG has a preference for covering frequent n-grams before covering infrequent n-grams. The VG method is depicted in Figure 2. Figure 3 shows the learning curves for both jHier and jSyntax for"
P10-1088,P10-2068,0,0.0204293,". Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the problem of diminishing returns that we discussed in Section 1. The second area of"
P10-1088,P10-1157,0,0.0176608,"tokens for an NER task. Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see, e.g., (Hwa, 2000; Sassano, 2002; Bloodgood and Vijay-Shanker, 2008; Bloodgood and VijayShanker, 2009b; Mairesse et al., 2010; Vickrey et al., 2010)). The current paper is most highly related to previous work falling into three main areas: use of AL when large corpora already exist; cost-focused AL; and AL for SMT. In a sense, the work of Banko and Brill (2001) is closely related to ours. Though their focus is mainly on investigating the performance of learning methods on giant corpora many orders of magnitude larger than previously used, they do lay out how AL might be useful to apply to acquire data to augment a large set cheaply because they recognize the problem of diminishing returns that we discussed in Sectio"
P10-1088,W06-3119,0,0.0321401,"nre-balanced split of the NIST2008 Urdu-English test set. In addition, the language pack contains an Urdu-English dictionary consisting of ≈ 114000 entries. In all the experiments, we use the dictionary at every iteration of training. This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al., 2009). We hereafter refer to these systems as jHier and jSyntax, respectively. We will now present results of experiments with different methods for growing MT training data. The results are organized into three areas of investigations: Figure 2: The VG sentence selection algorithm do not occur at all in our so-far labeled data. We call an n-gram “covered” if it occurs at least once in our so-far labeled data. VG has a preference for covering frequent n-grams before covering infrequent n-grams. The VG method is depicted in Figur"
P10-1088,ambati-etal-2010-active,0,\N,Missing
P10-1088,P08-2017,0,\N,Missing
P10-1088,N09-2035,1,\N,Missing
P11-1063,W07-0702,0,0.0512813,"ains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree s"
P11-1063,D07-1090,0,0.0202207,"n top-down or bottom-up parsing. We provided a rigorous formal definition of incremental syntactic languages models, and detailed what steps are necessary to incorporate such LMs into phrase-based decoding. We integrated an incremental syntactic language model into Moses. The translation quality significantly improved on a constrained task, and the perplexity improvements suggest that interpolating between n-gram and syntactic LMs may hold promise on larger data sets. The use of very large n-gram language models is typically a key ingredient in the best-performing machine translation systems (Brants et al., 2007). Our n-gram model trained only on WSJ is admittedly small. Our future work seeks to incorporate largescale n-gram language models in conjunction with incremental syntactic language models. The added decoding time cost of our syntactic language model is very high. By increasing the beam size and distortion limit of the baseline system, future work may examine whether a baseline system with comparable runtimes can achieve comparable translation quality. A more efficient implementation of the HHMM parser would speed decoding and make more extensive and conclusive translation experiments possible"
P11-1063,J90-2002,0,0.613811,"Missing"
P11-1063,2003.mtsummit-papers.6,0,0.536633,"approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast t"
P11-1063,P98-1035,0,0.45749,"and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing"
P11-1063,P08-1009,0,0.00775866,"on with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. 3 Parser as Syntactic Language Model in Phrase-Based Translation Parsing is the task of selecting the representation τˆ (typically a tree) that best models the structure of ... ... ÀËÂÃÄÅÆ ÀËÂÍÄÅÆ hsi president president Friday τ˜13 τ˜23 ÊÁÂÃÄÅÆ ÊËÂÃÄÅÆ ÊËÌÃÄÅÆ hsi that that president τ˜12 τ˜22 Obama met τ˜32 ÀÁÂÃÄÅÆ ÊÁÂÃÄÅÆ ÊËÂÃÄÅÆ ÊËÌÃÄÅÆ hsi hsi the the president president meets τ˜0 τ˜"
P11-1063,P05-1033,0,0.0800322,"l refer to incremental processing which proceeds from the beginning of a sentence as left-to-right. 620 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics statistical MT that can run in linear-time (§4) • Integration with Moses (§5) along with empirical results for perplexity and significant translation score improvement on a constrained UrduEnglish task (§6) 2 Related Work Neither phrase-based (Koehn et al., 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examin"
P11-1063,P10-1146,0,0.0152744,"porated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language"
P11-1063,P05-1063,0,0.182058,"ree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translat"
P11-1063,W06-1628,0,0.0338022,"Missing"
P11-1063,D09-1076,0,0.0101563,"extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we inc"
P11-1063,D07-1079,0,0.0114627,"arch has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translat"
P11-1063,P05-1067,0,0.0218033,"e-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to la"
P11-1063,P81-1022,0,0.795817,"set of trees under consideration to τ˜t , that subset of analyses or partial analyses that remains after any pruning is performed. An incremental syntactic language model can then be defined by a probability mass function (Equation 5) and a transition function δ (Equation 6). The role of δ is explained in §3.3 below. Any parser which implements these two functions can serve as a syntactic language model. Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After 622 P(e1 ...et ) ≈ P(˜ τ t) = X P(e1 ...et |τ )P(τ ) (5) τ ∈˜ τt δ(et , τ˜t−1 ) → τ˜t (6) 3.2 Decoding in phrase-based translation S Given a source language input sentence f , a trained source-to-target translation model, and a target language model, the task of translation is to find the maximally probable translation e ˆ using a linear combination of j feature functions h weighted according to tuned parameters λ (Och and Ney, 2002). X e ˆ = argmax exp( λj hj (e, f )) e DT VP NN VP The president VB PP NP meet"
P11-1063,P03-2041,0,0.0165951,"2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree"
P11-1063,P09-1087,0,0.0804127,"yntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in conc"
P11-1063,N04-1035,0,0.0604707,"ps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work"
P11-1063,P06-1121,0,0.0188307,"may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using synta"
P11-1063,N10-1127,0,0.0204658,"a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. 3 Parser as Syntactic Language Model in Phrase-Based Translation Parsing is the task of selecting the representation τˆ (typically a tree) that best models the structure of ... ... ÀËÂÃÄÅÆ ÀËÂÍÄÅÆ hsi president president Friday τ˜13 τ˜23 ÊÁÂÃÄÅÆ ÊË"
P11-1063,P03-1011,0,0.0126599,"s contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information t"
P11-1063,N04-1014,0,0.00964682,"e of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-b"
P11-1063,P07-1037,0,0.0905615,"Missing"
P11-1063,W04-0305,0,0.0296463,"cal choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal"
P11-1063,D10-1027,0,0.0260813,"to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarc"
P11-1063,P10-1110,0,0.0398287,"grammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal definition of an incremental parser for 1 Whil"
P11-1063,2006.amta-papers.8,0,0.0280808,"any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollman"
P11-1063,N03-1017,0,0.0181292,"for 1 While not all languages are written left-to-right, we will refer to incremental processing which proceeds from the beginning of a sentence as left-to-right. 620 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics statistical MT that can run in linear-time (§4) • Integration with Moses (§5) along with empirical results for perplexity and significant translation score improvement on a constrained UrduEnglish task (§6) 2 Related Work Neither phrase-based (Koehn et al., 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to imp"
P11-1063,P07-2045,1,0.00717479,"-gram WSJ 4-gram WSJ 5-gram WSJ HHMM Interpolated WSJ 5-gram + HHMM In-domain WSJ 23 ppl 1973.57 349.18 262.04 244.12 232.08 384.66 Out-of-domain ur-en dev ppl 3581.72 1312.61 1264.47 1261.37 1261.90 529.41 209.13 225.48 258.35 312.28 222.39 123.10 174.88 321.05 Giga 5-gram Interp. Giga 5-gr + WSJ HHMM Interp. Giga 5-gr + WSJ 5-gram Figure 7: Average per-word perplexity values. HHMM was run with beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. ppl = b −logb P(e1 ...eT ) T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when tr"
P11-1063,J10-4005,0,0.00767483,"l, the task of translation is to find the maximally probable translation e ˆ using a linear combination of j feature functions h weighted according to tuned parameters λ (Och and Ney, 2002). X e ˆ = argmax exp( λj hj (e, f )) e DT VP NN VP The president VB PP NP meets DT NN IN NP on Friday the board (7) Figure 2: Sample binarized phrase structure tree. j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is also maintained at each node in the translation lattice. The search space is further trimmed with hypothesis recombination, which collapses lattice nodes that share a common coverage vector and n-gram state. 3.3 NP S S/NP NP S/PP IN Friday S/VP VP NP VP/NN NP/NN NN DT president The on NN VP/NP DT board VB the meets Figure 3: Sample binarized phrase structure tree after application of right-corner transform. Incorporati"
P11-1063,P06-1077,0,0.011446,"hrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation gramma"
P11-1063,P07-1089,0,0.0174076,"ation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonte"
P11-1063,P09-1063,0,0.0305554,"Missing"
P11-1063,J93-2004,0,0.037355,"all in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. ppl = b −logb P(e1 ...eT ) T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets4 are found by interpolating 4 In-domain is WSJ Section 23. Out-of-domain are the English reference translations of the dev section , set aside in (Baker et al., 2009) for parameter tuning, of the NIST Open MT 2008 Urdu-English task. Sentence length 10 20 30 40 Moses 0.21 0.53 0.85 1.13 +HHMM beam=50 533 1193 1746 2095 +HHMM beam=2000 1143 2562 3749 4588 Moses LM(s) n-gram only HHMM + n-gram Figure 8: Mean"
P11-1063,P04-1083,0,0.00963529,") which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine tr"
P11-1063,D08-1022,0,0.0146806,"g translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substanti"
P11-1063,P08-1023,0,0.0238383,"hether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can d"
P11-1063,2006.amta-papers.15,0,0.0202505,"ove translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a stan"
P11-1063,P02-1038,0,0.127444,"he beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After 622 P(e1 ...et ) ≈ P(˜ τ t) = X P(e1 ...et |τ )P(τ ) (5) τ ∈˜ τt δ(et , τ˜t−1 ) → τ˜t (6) 3.2 Decoding in phrase-based translation S Given a source language input sentence f , a trained source-to-target translation model, and a target language model, the task of translation is to find the maximally probable translation e ˆ using a linear combination of j feature functions h weighted according to tuned parameters λ (Och and Ney, 2002). X e ˆ = argmax exp( λj hj (e, f )) e DT VP NN VP The president VB PP NP meets DT NN IN NP on Friday the board (7) Figure 2: Sample binarized phrase structure tree. j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is"
P11-1063,2008.amta-papers.16,0,0.0493922,"f n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-ba"
P11-1063,P05-1034,0,0.0411963,"els: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621"
P11-1063,J01-2004,0,0.258771,"terms of lexical choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation"
P11-1063,J10-1001,1,0.917078,"till often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal definition of an incre"
P11-1063,N09-1039,1,0.931192,"f rt = r⊥ : PθS-T-W,d (st |st−1 rt ) (14) def PθR,d (rtd |rtd+1 sdt−1 sd−1 t−1 ) =  if crtd+1 6= xt : Jrtd = r⊥ K (15) if crtd+1 = xt : PθR-R,d (rtd |sdt−1 sd−1 t−1 ) These HHMM right-corner parsing operations are then defined in terms of branch- and depth-specific PCFG probabilities θG-R,d and θG-L,d : 3 3 (13) where r⊥ is a null state resulting from the failure of an incomplete constituent to complete, and constants are defined for the edge conditions of s0t and rtD+1 . Figure 5 illustrates this model in action. These pushdown automaton operations are then refined for right-corner parsing (Schuler, 2009), distinguishing active transitions (model θS-T-A,d , in which an incomplete constituent is completed, but not reduced, and then immediately expanded to a Model probabilities are also defined in terms of leftprogeny probability distribution EθG-RL∗,d which is itself defined in terms of PCFG probabilities: X 0 def EθG-RL∗,d (cη → cη0 ...) = PθG-R,d (cη → cη0 cη1 ) (16) cη1 k def EθG-RL∗,d (cη → cη0k 0 ...) = X k−1 EθG-RL∗,d (cη → cη0k ...) cη0k · X PθG-L,d (cη0k → cη0k 0 cη0k 1 ) ∗ def + def EθG-RL∗,d (cη → cηι ...) = ∞ X k EθG-RL∗,d (cη → cηι ...) (18) k=0 ∗ EθG-RL∗,d (cη → cηι ...) = EθG-RL∗,"
P11-1063,P08-1066,0,0.0549959,"tituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 200"
P11-1063,C90-3045,0,0.410186,"improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009)."
P11-1063,W04-3312,0,0.0139479,"nd such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translat"
P11-1063,J97-3002,0,0.0534401,"tructure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in"
P11-1063,P01-1067,0,0.07917,"chniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic"
P11-1063,2007.mtsummit-papers.71,0,0.0231704,"ty. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic)"
P11-1063,W06-3119,0,0.0318199,"., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in 621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1"
P11-1063,C90-3001,0,\N,Missing
P11-1063,C00-2092,0,\N,Missing
P11-1063,C04-1015,0,\N,Missing
P11-1063,C98-1035,0,\N,Missing
P11-1063,N04-1021,0,\N,Missing
P11-1063,W90-0102,0,\N,Missing
P11-1122,W10-0710,0,0.131924,"ed the possibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch (2010) solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems. That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel (2010), focusing on the design of the translation HIT, and by Irvine and Klementiev (2010), who created translation lexicons between English and 42 rare languages. Resnik et al. (2010) explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source language paraphrased the corresponding source portion. The paraphrased source would then be retranslated to produce a different translation, hopefully more coherent than the original. 8 Conclusio"
P11-1122,W10-0733,1,0.701823,"probabilistic model to filter labels from non-experts, in the context of an image labeling task. Their system generatively models image difficulty, as well as noisy, even adversarial, annotators. They apply their method to simulated labels rather than real-life labels. Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. One such method was to collect reference translations to score MT output. It was only a pilot study (50 sentences in each of several languages), but it showed the possibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch (2010) solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems. That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel (2010), focusing on the design of the translation HIT, and by Irvine and Klementiev (2010), who created translation"
P11-1122,W10-0701,1,0.220397,"ch method was to collect reference translations to score MT output. It was only a pilot study (50 sentences in each of several languages), but it showed the possibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch (2010) solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems. That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel (2010), focusing on the design of the translation HIT, and by Irvine and Klementiev (2010), who created translation lexicons between English and 42 rare languages. Resnik et al. (2010) explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source language paraphrased the corresponding source portion."
P11-1122,D09-1030,1,0.567986,"8) were among the first to use MTurk to obtain data for several NLP tasks, such as textual entailment and word sense disambiguation. Their approach, based on majority voting, had a component for annotator bias correction. They showed that for such tasks, a few nonexpert labels usually suffice. Whitehill et al. (2009) proposed a probabilistic model to filter labels from non-experts, in the context of an image labeling task. Their system generatively models image difficulty, as well as noisy, even adversarial, annotators. They apply their method to simulated labels rather than real-life labels. Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. One such method was to collect reference translations to score MT output. It was only a pilot study (50 sentences in each of several languages), but it showed the possibility of obtaining high-quality translations from non-professionals. As a followup, Bloodgood and Callison-Burch (2010) solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT system"
P11-1122,2010.amta-workshop.0,0,0.147592,"Missing"
P11-1122,P98-1069,0,0.0218535,"large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We us"
P11-1122,W01-1409,0,0.0123399,"Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We use Amazon’s Mechanical Turk to hire a large group of nonprofessional translators, and have them recreate an Urdu–English evaluation set at a fraction of the cost of professional translators. The original dataset already has professionally-pro"
P11-1122,P08-1088,0,0.019117,"tively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We use Amazon’s Mechanical Turk to hire a large group of"
P11-1122,W10-0717,0,0.12627,"s. As a followup, Bloodgood and Callison-Burch (2010) solicited a single translation of the NIST Urdu-to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems. That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel (2010), focusing on the design of the translation HIT, and by Irvine and Klementiev (2010), who created translation lexicons between English and 42 rare languages. Resnik et al. (2010) explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source language paraphrased the corresponding source portion. The paraphrased source would then be retranslated to produce a different translation, hopefully more coherent than the original. 8 Conclusion and Future Work We have demonstrated that it is possible to obtain high-quality tr"
P11-1122,W10-1718,1,0.798563,"Missing"
P11-1122,J05-4003,0,0.0203516,"orpora. SMT owes its existence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like 1220 Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of h"
P11-1122,J04-2003,0,0.0180945,"n shown to produce state-of-the-art results for language pairs like 1220 Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exc"
P11-1122,P03-1021,0,0.0452857,"Missing"
P11-1122,P02-1040,0,0.092734,"a to compute another worker-specific feature. Namely, we can evaluate the competency of each worker by scoring their translations against the reference translations. We then use that feature for every translation given by that worker. The intuition is that workers known to produce good translations are likely to continue to produce good translations, and the opposite is likely true as well. 4.4 Evaluation Strategy To measure the quality of the translations, we make use of the existing professional translations. Since we have four professional translation sets, we can calculate the BLEU score (Papineni et al., 2002) for one professional translator P1 using the other three P2,3,4 as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. We can see how a translation set T (chosen by our model) compares to this range by calculating T ’s BLEU scores against the same four sets of three reference translations. We will evaluate different strategies for selecting such a set T , and see how much each improves on the BLEU score, compared to randomly picking from among the Turker translations. W"
P11-1122,P95-1050,0,0.226375,"ta. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via"
P11-1122,J03-3002,0,0.014303,"tence-aligned parallel corpora. SMT owes its existence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like 1220 Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (200"
P11-1122,D10-1013,0,0.004832,"to-English dataset we used. Their evaluation was similar to our correlation experiments, examining how well the collected translations agreed with the professional translations when evaluating three MT systems. That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Two relevant papers from that workshop were by Ambati and Vogel (2010), focusing on the design of the translation HIT, and by Irvine and Klementiev (2010), who created translation lexicons between English and 42 rare languages. Resnik et al. (2010) explore a very interesting way of creating translations on MTurk, relying only on monolingual speakers. Speakers of the target language iteratively identified problems in machine translation output, and speakers of the source language paraphrased the corresponding source portion. The paraphrased source would then be retranslated to produce a different translation, hopefully more coherent than the original. 8 Conclusion and Future Work We have demonstrated that it is possible to obtain high-quality translations from non-professional translators, and that the cost is an order of magnitude cheap"
P11-1122,W02-2026,0,0.0572621,"allel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a TamilEnglish corpus at $0.36/word. At that rate, translating enough data to build even a small parallel corpus like the LDC’s 1.5 million word Urdu–English corpus would exceed half a million dollars. In this paper we examine the idea of creating low cost translations via crowdscouring. We use Amazon’s Mechanical Turk t"
P11-1122,N10-1063,0,0.0275778,"ence to data like the Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like 1220 Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional t"
P11-1122,2006.amta-papers.25,0,0.0303984,"nt, choose from the four translations the one that scores highest against the reference sentence. The second oracle operates on the worker level: for each source segment, choose from the four translations the one provided by the worker whose translations (over all sentences) score the highest. The two oracles achieve BLEU scores of 43.75 and 40.64, respectively – well within the range of professional translators. We examined two voting-inspired methods, since taking a majority vote usually works well when dealing with MTurk data. The first selects the translation with the minimum average TER (Snover et al., 2006) against the other three translations, since that would be a ‘consensus’ translation. The second method selects the translation that received the best average rank, using the rank labels assigned by other Turkers (see 3.3). These approaches achieve BLEU scores of 34.41 and 36.64, respectively. The main set of experiments evaluated the features from 4.1 and 4.3. We applied our approach using each of the four feature types: sentence features, Turker features, rank features, and the caliBLEU 45 40 35 30 25 42.38 26.91 28.13 43.75 40.64 34.41 36.64 34.95 35.79 37.14 37.82 39.06 Reference (ave.) Jo"
P11-1122,D08-1027,0,0.145584,"Missing"
P11-1122,C10-1124,0,0.00701275,"e Canadian Hansards (which by law must be published in both French and English). SMT can be applied to any language pair for which there is sufficient data, and it has been shown to produce state-of-the-art results for language pairs like 1220 Arabic–English, where there is ample data. However, large bilingual parallel corpora exist for relatively few languages pairs. There are various options for creating new training resources for new language pairs. These include harvesting the web for translations or comparable corpora (Resnik and Smith, 2003; Munteanu and Marcu, 2005; Smith et al., 2010; Uszkoreit et al., 2010), improving SMT models so that they are better suited to the low resource setting (Al-Onaizan et al., 2002; Probst et al., 2002; Oard et al., 2003; Niessen and Ney, 2004), or designing models that are capable of learning translations from monolingual corpora (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Haghighi et al., 2008). Relatively little consideration is given to the idea of simply hiring translators to create parallel data, because it would seem to be prohibitively expensive. For example, Germann (2001) estimated the cost of hiring professional translators to create a Ta"
P11-1122,N03-2026,0,\N,Missing
P11-1122,C98-1066,0,\N,Missing
P11-1122,2010.amta-workshop.3,0,\N,Missing
P11-2007,W09-0807,0,0.158255,"Missing"
P11-2007,W10-0717,0,0.0565228,"Missing"
P11-2007,P11-1122,1,0.0436284,"Missing"
P13-1135,W10-1703,1,0.322705,"Missing"
P13-1135,W11-2103,1,0.788665,"BLEU scores for several language pairs before and after adding the mined parallel data to systems trained on data from WMT data. WMT 11 Baseline +Web Data WMT 12 Baseline +Web Data FR-EN 30.96 31.24 FR-EN 29.88 30.08 EN-FR 30.69 31.17 EN-FR 28.50 28.76 Corpus EN-FR EN-ES EN-DE News Commentary 2.99M 50.3M 316M 668M 121M 3.43M 49.2M 281M 68.8M 3.39M 47.9M 88.4M Europarl United Nations FR-EN Gigaword CommonCrawl Table 9: BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Gigaword (Callison-Burch et al., 2011). For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al. (2010), in order to determine how the effect of such data compares with the effect of webmined data. The baseline system was trained using only the Europarl corpus (Koehn, 2005) as parallel data, and all experiments use the same language model trained on the target sides of Europarl, the English side of all linked SpanishEnglish Wikipedia articles, and the English side of the mined CommonCrawl data. We use a 5gram language model and tune using"
P13-1135,W12-3102,1,0.806207,"Missing"
P13-1135,J93-1004,0,0.134564,", 2003): 1. Candidate pair selection: Retrieve candidate document pairs from the CommonCrawl corpus. 2. Structural Filtering: (a) Convert the HTML of each document 2 3 commoncrawl.org http://aws.amazon.com/s3/pricing/ into a sequence of start tags, end tags, and text chunks. (b) Align the linearized HTML of candidate document pairs. (c) Decide whether to accept or reject each pair based on features of the alignment. 3. Segmentation: For each text chunk, perform sentence and word segmentation. 4. Sentence Alignment: For each aligned pair of text chunks, perform the sentence alignment method of Gale and Church (1993). 5. Sentence Filtering: Remove sentences that appear to be boilerplate. Candidate Pair Selection We adopt a strategy similar to that of Resnik and Smith (2003) for finding candidate parallel documents, adapted to the parallel architecture of Map-Reduce. The mapper operates on each website entry in the CommonCrawl data. It scans the URL string for some indicator of its language. Specifically, we check for: 1. Two/three letter language codes (ISO-639). 2. Language names in English and in the language of origin. If either is present in a URL and surrounded by non-alphanumeric characters, the URL"
P13-1135,P07-2045,1,0.011242,"Missing"
P13-1135,2005.mtsummit-papers.11,1,0.613567,"oehn3 pkoehn@inf.ed.ac.uk Chris Callison-Burch1,2,5 ccb@cs.jhu.edu ∗ Adam Lopez1,2 alopez@cs.jhu.edu Department of Computer Science, Johns Hopkins University Human Language Technology Center of Excellence, Johns Hopkins University 3 School of Informatics, University of Edinburgh 4 Institute of Computational Linguistics, University of Zurich 5 Computer and Information Science Department, University of Pennsylvania 1 2 Abstract are readily available, ordering in the hundreds of millions of words for Chinese-English and ArabicEnglish, and in tens of millions of words for many European languages (Koehn, 2005). In each case, much of this data consists of government and news text. However, for most language pairs and domains there is little to no curated parallel data available. Hence discovery of parallel data is an important first step for translation between most of the world’s languages. Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl host"
P13-1135,W02-0109,0,0.0188601,"Missing"
P13-1135,P12-3005,0,0.0302301,"Missing"
P13-1135,J05-4003,0,0.0147224,"pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 1 The Web is an important source of parallel text. Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010)— it is common to find document pairs that are direct translations of one another. This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns H"
P13-1135,P11-1122,1,0.252203,"Missing"
P13-1135,P03-1021,0,0.0312913,"5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely tokens. 4.1 News Domain Translation Our first set of experiments are based on systems built for the 2012 Workshop on Statistical Machine Translation (WMT) (Callison-Burch et al., 2012) using all available parallel and monolingual data for that task, aside from the French-English Gigaword. In these experiments, we use 5-gram language models when the target language is English or German, and 4-gram language models for French and Spanish. We tune model weights using minimum error rate training (MERT; Och, 2003) on the WMT 2008 test data. The results are given in Table 8. For all language pairs and both test sets (WMT 2011 and WMT 2012), we show an improvement of around 0.5 BLEU. We also included the French-English Gigaword in separate experiments given in Table 9, and Table 10 compares the sizes of the datasets used. These results show that even on top of a different, larger parallel corpus mined from the web, adding CommonCrawl data still yields an improvement. 4.2 Open Domain Translation A substantial appeal of web-mined parallel data is that it might be suitable to translation of domains other th"
P13-1135,J03-3002,0,0.249669,"trap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jrs026/CommonCrawlMiner Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines (Nie et al., 1999; Chen and Nie, 2000; Resnik, 1999; Resnik and Smith, 2003). However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. For most researchers, this is prohibitively expensive. As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted 1374 Proceedings of the 5"
P13-1135,P99-1068,0,0.0228645,"utset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jrs026/CommonCrawlMiner Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines (Nie et al., 1999; Chen and Nie, 2000; Resnik, 1999; Resnik and Smith, 2003). However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. For most researchers, this is prohibitively expensive. As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted"
P13-1135,N10-1063,1,0.944364,"enres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 1 The Web is an important source of parallel text. Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010)— it is common to find document pairs that are direct translations of one another. This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jr"
P13-1135,N12-1079,0,0.020081,"Missing"
P13-1135,C10-1124,0,0.0223887,"Missing"
P13-1135,D11-1126,0,0.0714544,"Missing"
P13-1135,P06-4018,0,\N,Missing
P13-2012,D12-1032,1,0.765713,"g synonym, hypernym, hyponym, meronym, and holonym edges and bucket the length. String Transducer To represent similarity between arguments that are names, we use a stochastic edit distance model. This stochastic string-tostring transducer has latent “edit” and “no edit” regions where the latent regions allow the model to assign high probability to contiguous regions of edits (or no edits), which are typical between variations of person names. In an edit region, parameters govern the relative probability of insertion, deletion, substitution, and copy operations. We use the transducer model of Andrews et al. (2012). Since in-domain name pairs were not available, we picked 10,000 entities at random from Wikipedia to estimate the transducer parameters. The entity labels were used as weak supervision during EM, as in Andrews et al. (2012). For a pair of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. treat as independent experts. For each of these rule probabilities (experts), we find all rules that match t"
P13-2012,W99-0201,0,0.415464,"ry of entity disambiguation both within and across documents. While most information extraction work focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as “elect” or nominalizations such as “election”. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks 2 PARMA (Predicate ARguMent Aligner) is a pipelined system with a wide variety of features used to align predicates and arguments in two documents. Predicates are represented as mention spans and arguments are represented as coreference chains (sets of mention spans) provided by in-document coreference resolution systems such as included in the"
P13-2012,P98-1013,0,0.0851903,"of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. treat as independent experts. For each of these rule probabilities (experts), we find all rules that match the head tokens of a given alignment and have a feature for the max and harmonic mean of the log probabilities of the resulting rule set. FrameNet FrameNet is a lexical database based on Charles Fillmore’s Frame Semantics (Fillmore, 1976; Baker et al., 1998). The database (and the theory) is organized around semantic frames that can be thought of as descriptions of events. Frames crucially include specification of the participants, or Frame Elements, in the event. The Destroying frame, for instance, includes frame elements Destroyer or Cause Undergoer. Frames are related to other frames through inheritance and perspectivization. For instance the frames Commerce buy and Commerce sell (with respective lexical realizations “buy” and “sell”) are both perspectives of Commerce goods-transfer (no lexical realizations) which inherits from Transfer (with"
P13-2012,P10-1143,0,0.142587,"Missing"
P13-2012,N13-1106,1,0.858341,"Missing"
P13-2012,J08-4005,1,0.795859,"ions we perform can vary the “relatedness” of the two documents in terms of the predicates and arguments that they talk about. This reflects our expectation of real world data, where we do not expect perfect overlap in predicates and arguments between a source and target document, as you would in translation data. Lastly, we prune any document pairs that have more than 80 predicates or arguments or have a Jaccard index on bags of lemmas greater than 0.5, to give us a dataset of 328 document pairs. Metric We use precision, recall, and F1. For the RF dataset, we follow Roth and Frank (2012) and Cohn et al. (2008) and evaluate on a version of F1 that considers SURE and POSSIBLE links, which are available in the RF data. Given an alignment to be scored A and a reference alignment B which contains SURE and POSSIBLE links, Bs and Bp respectively, precision and recall are: 8 LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01, LDC2003T18, and LDC2005T05 P = 66 |A ∩ Bp | |A| R= |A ∩ Bs | |Bs | (1) EECB lemma RF lemma Roth and Frank MTC lemma PARMA PARMA PARMA F1 63.5 74.3 48.3 54.8 57.6 42.1 59.2 P 84.8 80.5 40.3 59.7 52.4 51.3 73.4 R 50.8 69.0 60.3 50.7 64.0 35.7 49.6 task di"
P13-2012,P07-1033,0,0.164696,"Missing"
P13-2012,C10-1032,1,0.882206,"Missing"
P13-2012,N13-1092,1,0.853348,"Missing"
P13-2012,P11-1095,0,0.0328258,"Missing"
P13-2012,D12-1045,0,0.367258,"ses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as “elect” or nominalizations such as “election”. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks 2 PARMA (Predicate ARguMent Aligner) is a pipelined system with a wide variety of features used to align predicates and arguments in two documents. Predicates are represented as mention spans and arguments are represented as coreference chains (sets of mention spans) provided by in-document coreference resolution systems such as included in the Stanford NLP toolkit. Results indicated that the chains are of sufficient quality so as not to li"
P13-2012,W12-3018,1,0.876778,"Missing"
P13-2012,S12-1030,0,0.165538,"jamin Van Durme, Mark Dredze, Nicholas Andrews, Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder, Jonathan Weese, Tan Xu† , and Xuchen Yao Human Language Technology Center of Excellence Johns Hopkins University, Baltimore, Maryland USA †University of Maryland, College Park, Maryland USA Abstract to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argument alignment. As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA). We demonstrate the effectiveness of this approach by achieving state of the art performance on the data of Roth and Frank despite having little relevant training data. We then show that while the “lemma match” heuris"
P13-2012,C98-1013,0,\N,Missing
P13-2123,W07-1427,0,0.175196,"Missing"
P13-2123,N10-1044,0,0.0384138,"Missing"
P13-2123,P09-2073,0,0.0414792,"in the task of tagging mail addresses, a feature of “5 consecutive digits” is highly indicative of a POSTCODE. However, in the alignment model, it does not make sense to design features based on a hard-coded state, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈["
P13-2123,J03-1002,0,0.0065229,"te, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈[1,M ] ∈ [0, N ]. We specify that when am = 0, source word st is aligned to a NULL state, i.e., deleted. This models a many-to-one alignment from source to target. Multiple source words can be aligned to the same targ"
P13-2123,D12-1016,0,0.0743357,"tagging mail addresses, a feature of “5 consecutive digits” is highly indicative of a POSTCODE. However, in the alignment model, it does not make sense to design features based on a hard-coded state, say, a feature of “source word lemma matching target word lemma” fires for state index 6. To avoid this data sparsity problem, all features are defined implicitly with respect to the state. For instance: yields the lowest tree edit distance. Other tree or graph matching work for alignment includes that of (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Chambers et al., 2007; Mehdad, 2009; Roth and Frank, 2012). Finally, feature and model design in monolingual alignment is often inspired by bilingual work, including distortion modeling, phrasal alignment, syntactic constraints, etc (Och and Ney, 2003; DeNero and Klein, 2007; Bansal et al., 2011). 3 The Alignment Model 3.1 Model Design Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). Given a source sentence s of length M , and a target sentence t of length N , the alignment from s to t is a sequence of target word indices a, where am∈[1,M ] ∈ [0, N ]. We spe"
P13-2123,P11-2044,0,0.496604,"ilizes arbitrary features (to make use of word similarity measure and lexical resources) and exploits deeper sentence structures (especially in the case of major languages where robust parsers are available). In this setting the balance between precision and speed becomes an issue: while we might leverage an extensive NLP pipeline for a ∗ 2 Related Work The MANLI aligner (MacCartney et al., 2008) was first proposed to align premise and hypothesis sentences for the task of natural language inference. It applies perceptron learning and handles phrase-based alignment of arbitrary phrase lengths. Thadani and McKeown (2011) optimized this model by decoding via Integer Linear Programming (ILP). Benefiting from modern ILP solvers, this led to an order-of-magnitude speedup. With extra syntactic constraints added, the exact alignment match rate for whole sentence pairs was also significantly improved. Besides the above supervised methods, indirect supervision has also been explored. Among them, Wang and Manning (2010) extended the work of McCallum et al. (2005) and modeled alignment as latent variables. Heilman and Smith (2010) used tree kernels to search for the alignment that 1 Performed while faculty at Johns Hop"
P13-2123,C10-1131,0,0.0630786,"Missing"
P13-2123,N10-1145,0,\N,Missing
P13-2123,W08-1301,0,\N,Missing
P13-2123,D08-1084,0,\N,Missing
P13-2123,P11-1131,0,\N,Missing
P13-2123,P07-1003,0,\N,Missing
P13-2123,J08-4005,1,\N,Missing
P13-2123,N10-1112,0,\N,Missing
P13-2123,P06-1009,0,\N,Missing
P14-1107,E09-1003,0,0.0183789,"T gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook loc"
P14-1107,W10-0710,0,0.0144795,"expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 201"
P14-1107,ambati-etal-2010-active,0,0.0223859,"h, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional tra"
P14-1107,P10-1088,1,0.845564,"or instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant transla"
P14-1107,W10-0701,1,0.794839,"k, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian lan"
P14-1107,D09-1030,1,0.817253,"tations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati"
P14-1107,W01-1409,0,0.0602304,"become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages using volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013). Rather than relying on volunte"
P14-1107,N10-1078,0,0.0308235,"lems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1 A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to"
P14-1107,W10-1718,1,0.892503,"Missing"
P14-1107,Q13-1028,0,0.0230426,"Missing"
P14-1107,J05-4003,0,0.051185,"e, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professi"
P14-1107,P02-1040,0,0.100739,"n ti and tj . ( #col (eij ∈ ET ) I(ti , tj ) = , (9) 0 otherwise Then the adjacency matrix N is then defined as (10) 5 (12) Evaluation We are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turkers. We want to test two versions of our proposed collaborative co-ranking method: 1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations. Metric Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score (Papineni et al., 2002) for one professional translator (P1) using the other three (P2,3,4) as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations. Therefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets. This allows us to compare the BLEU"
P14-1107,W12-3152,1,0.885996,"range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 wor"
P14-1107,J03-3002,0,0.0131709,"actice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term"
P14-1107,N10-1063,0,0.0140024,"eneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site i"
P14-1107,P13-1135,1,0.835643,"for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opene"
P14-1107,2006.amta-papers.25,0,0.0296192,"om the candidates produced by the collaboration of translator/post-editor pairs. The third oracle operates at the worker level: for each source segment, we choose from the translations the one provided by the worker whose translations (over all sentences) score the highest on average. The fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations. These oracle methods represent ideal solutions under our scenario. We also examine two voting-inspired methods. The first method selects the translation with the minimum average TER (Snover et al., 2006) against the other translations; intuitively, this would represent the “consensus” translation. The second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER. Results A summary of our results in given in Table 2. As expected, random selection yields bad performance, with a BLEU score of 30.52. The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and"
P14-1107,D08-1027,0,0.0720958,"Missing"
P14-1107,C10-1124,0,0.0139175,"tate-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, cr"
P14-1107,D11-1040,1,0.792595,"n, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GT C ). Edges in GT C connect author pairs (nodes in GT ) to the candidate that they produced (nodes in GC ). Together, GT , GC , and GT C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT , VC , ET , EC , ET C ). G is divided into three subgraphs, GT , GC , and GT C . GC = (VC , EC ) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT , ET ) is a weighted undirect"
P14-1107,P12-1054,1,0.828,"of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GT C ). Edges in GT C connect author pairs (nodes in GT ) to the candidate that they produced (nodes in GC ). Together, GT , GC , and GT C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT , VC , ET , EC , ET C ). G is divided into three subgraphs, GT , GC , and GT C . GC = (VC , EC ) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT , ET ) is"
P14-1107,P11-1122,1,0.917573,"Missing"
P14-1107,N12-1006,1,0.850492,"aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch,"
P14-1107,N13-1069,0,0.0366214,"Missing"
P14-1107,Q14-1007,1,\N,Missing
P15-1146,P05-1074,1,0.5637,"DB, annotated on MTurk as described in Section 5 1514 Lexical Distributional Paraphrase Translation Path WordNet We use the lemmas, POS tags, and phrase lengths of p1 and p2 , the substrings shared by p1 and p2 , and the Levenstein, Jaccard, and Hamming distances between p1 and p2 . Given a dependency context vectors for p1 and p2 , we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and CallisonBurch (2014) for a complete description of all of the features included with PPDB. We include the number of foreign language “pivots” (translations) shared by p1 and p2 for each of 24 languages used in the construction of PPDB, as a fraction of the total number of translations observed for each of p1 and p2 . We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which are observed between p1 and p2 in the Annotated Gigaword corpus (Napoles et al., 2012). We include binary features indicating whether Word"
P15-1146,D07-1017,0,0.0891599,"country/patriotic drive/vehicle family/home basketball/court playing/toy islamic/jihad delay/time Unrelated girl/play found/party profit/year man/talk car/family holiday/series green/tennis sunday/tour city/south back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves"
P15-1146,S14-2114,1,0.90105,"Missing"
P15-1146,W08-2222,1,0.588578,"Missing"
P15-1146,S14-2141,0,0.0747714,"dependent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphr"
P15-1146,W04-3205,0,0.0861134,"tempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, ma"
P15-1146,P11-1062,0,0.0709412,"Missing"
P15-1146,S13-2045,0,0.00626621,"ringboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference usin"
P15-1146,W09-0215,0,0.0129149,"ve of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). W"
P15-1146,C04-1051,0,0.466255,"sk of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extr"
P15-1146,ganitkevitch-callison-burch-2014-multilingual,1,0.922495,"Missing"
P15-1146,S14-2055,0,0.0262925,"trieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase tabl"
P15-1146,P98-2127,0,0.382648,"⇠ Cosine Similarity shades/the shade yard/backyard each other/man picture/drawing practice/target Monolingual (symmetric) ¬ large/small ⌘ few/several ¬ different/same ¬ other/same ¬ put/take Monolingual (asymmetric) A boy/little boy A man/two men A child/three children ⌘ is playing/play A side/both sides ⌘ A ⌘ ⌘ ⌘ Bilingual dad/father some kid/child a lot of/many female/woman male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. in X and in Y separate X from Y to X and/or to Y from X to Y more/less X than Y ate levels of agreement (Fleiss’s  = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification Table 4: Top paths associated with the ¬ class. We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and e"
P15-1146,W07-1431,0,0.153459,"wing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this t"
P15-1146,J10-3003,0,0.048805,"Missing"
P15-1146,N13-1092,1,0.191264,"Missing"
P15-1146,S14-2001,0,0.0556365,"Missing"
P15-1146,W07-1401,0,0.0157195,"RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a represent"
P15-1146,W12-3018,1,0.892473,"Missing"
P15-1146,D09-1122,0,0.082205,"Missing"
P15-1146,C92-2082,0,0.478737,"ources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 15"
P15-1146,P06-1101,0,0.0656433,"4), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly"
P15-1146,C08-1107,0,0.0134612,"of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-"
P15-1146,W04-3206,0,0.0162431,"Missing"
P15-1146,C04-1146,0,0.0100585,"es examples of some of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as describe"
P15-1146,Q14-1034,1,0.68632,"Missing"
P15-1146,S14-2044,0,0.0221701,"tion, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dola"
P15-1146,W07-1409,0,\N,Missing
P15-1146,C98-2122,0,\N,Missing
P15-1146,S14-2004,0,\N,Missing
P15-1146,J13-3001,0,\N,Missing
P15-2010,D08-1021,1,0.774015,"phrases. We address the problem of customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like bi"
P15-2010,P13-2121,0,0.0208555,"m the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is appropriate given the domain-specific context. We consider a paraphrase rule to be g"
P15-2010,J10-3003,0,0.0554926,"paraphrase extraction techniques learn paraphrases for a mix of senses that work well in general. But in specific domains, paraphrasing should be sensitive to specialized language use. domain: the verb treat is used in expressions like treat you to dinner in conversational domains versus treat an infection in biology. This domain shift changes the acceptability of its paraphrases. We address the problem of customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms at"
P15-2010,W07-0716,0,0.0611515,"Missing"
P15-2010,D09-1074,0,0.0542275,"Missing"
P15-2010,apidianaki-etal-2014-semantic,0,0.075967,"customizing paraphrase models to specific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change dep"
P15-2010,P10-2041,0,0.527199,"data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. ∗ Incubated by the Allen Institute for Artificial Intelligence. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2.1 according to each language model (LM). That is, for a sentence si , we compute Paraphrase extraction Paraphrases can be extracted via bilingual pivoting. Intuitively, if two"
P15-2010,D11-1033,0,0.0380065,"2 provides examples of paraphrases extracted using our domain-specific AUC 39.5 40.8 40.8 41.2 41.9 42.3 43.7 ∆absolute – +1.3 +1.3 +1.7 +2.4 +2.8 +4.2 ∆relative – +3.3 +3.3 +4.3 +6.1 +7.1 +10.6 Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set weights discriminatively based on sentence-le"
P15-2010,N15-1023,1,0.825856,"ns. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on 1. We sort sentences in the training corpus ba"
P15-2010,P05-1074,1,0.900281,"ge depending on 1. We sort sentences in the training corpus based on how well they represent the target domain, and then extract paraphrases from a subsample of the most domain-like data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. ∗ Incubated by the Allen Institute for Artificial Intelligence. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2.1 ac"
P15-2010,tiedemann-2012-parallel,0,0.0169721,"advantage of producing the full set of paraphrases that can be extracted from the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is a"
P15-2010,C12-1177,0,0.0191268,"ific target domains. We explore the following ideas: Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on 1. We sort sentenc"
P15-2010,I08-2088,0,0.0804447,"Missing"
P15-2010,steinberger-etal-2006-jrc,0,\N,Missing
P15-2010,D10-1044,0,\N,Missing
P15-2010,W09-0401,1,\N,Missing
P15-2010,N13-1092,1,\N,Missing
P15-2010,J13-3001,0,\N,Missing
P15-2067,W07-1424,0,0.157149,"Missing"
P15-2067,ferrandez-etal-2010-aligning,0,0.118792,"Missing"
P15-2067,P13-2130,0,0.15027,"Missing"
P15-2067,N13-1092,1,0.691494,"Missing"
P15-2067,P10-2045,0,0.033468,"Missing"
P15-2067,J02-3001,0,0.237857,"Missing"
P15-2067,P98-1013,0,0.13149,"Missing"
P15-2067,W10-0735,0,0.029492,"Missing"
P15-2067,P13-2121,0,0.0542521,"Missing"
P15-2067,P14-1136,0,0.035095,"Missing"
P15-2067,D08-1021,1,0.826454,"Missing"
P15-2067,W10-0907,0,0.0542799,"Missing"
P15-2067,N03-2022,0,0.0541534,"Missing"
P15-2067,P11-1144,0,0.0227914,"Missing"
P15-2067,C10-2107,0,0.0853446,"Missing"
P15-2067,N12-1086,0,0.098033,"Missing"
P15-2067,D08-1048,0,0.205318,"Missing"
P15-2067,W14-2901,1,0.897335,"Missing"
P15-2067,D07-1002,0,0.049493,"Missing"
P15-2067,C98-1013,0,\N,Missing
P15-2070,D11-1108,1,0.522536,"Missing"
P15-2070,N13-1092,1,0.806581,"Missing"
P15-2070,S13-1005,0,0.0116965,"udes finegrained entailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphr"
P15-2070,D13-1090,0,0.0129039,"ntailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable"
P15-2070,P05-1074,1,0.656316,"zontal axis) for four ways of automatically ranking the paraphrases: p(e2 |e1 ) (far left), PPDB 1.0’s heuristic ranking method (middle left), word2vec similarity (middle right), and our supervised model for PPDB 2.0 (far right). Our rankings achieve the highest correlation with human judgements with a Spearman’s ρ of 0.71. Upon publication of this paper, we will release PPDB 2.0 along with a set of 26K phrase pairs annotated with human similarity judgments. 2 + + + + + + Improved rankings of paraphrases The notion of ranking paraphrases goes back to the original method that PPDB is based on. Bannard and Callison-Burch (2005) introduced the bilingual pivoting method, which extracts incarcerated as a potential paraphrase of put in prison since they are both aligned to festgenommen in different sentence pairs in an English-German bitext. Since incarcerated aligns to many foreign words (in many languages) the list of potential paraphrases is long. Paraphrases vary in quality since the alignments are automatically produced and noisy. In order to rank the paraphrases, Bannard and CallisonBurch (2005) define a paraphrase probability in terms of the translation model probabilities p(f |e) and p(e|f ): p(e2 |e1 ) ≈ X p(e2"
P15-2070,D11-1009,0,0.0153625,"straining to types which appear in PPDB), and collected human judgments for their full list of paraphrases. 2 https://code.google.com/p/word2vec/ For phrases, we use the vector of the rarest word as an approximation of the vector for the phrase. 3 427 1. 2. 3. 4. 5. achieves a 9-12 point improvement in MRR over the PPDB 1.0 rankings. Similarly, it improves AP by 7-9 points. 3 Other Additions Entailment relations 4 the final analysis the last the finish the final part the last part Related Work The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) and Malakasiotis and Androutsopoulos (2011). Zhao et al. (2008) improved Bannard and Callison-Burch (2005)’s paraphrase probability by converting it into log-linear model inspired by machine translation, allowing them to incorporate a variety of features. Malakasiotis and Androutsopoulos (2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extra"
P15-2070,S14-2141,0,0.0183407,"s than PPDB 1.0’s heuristic rankings. Each paraphrase pair in the database now also includes finegrained entailment relations, word embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between c"
P15-2070,N15-1023,1,0.665525,"2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraphrases of the end, sorted from most complex to most simple using these scores. These classifications could be useful for natural language generation tasks like text simplification (Xu et al., 2015). A complete evaluation of these scores is given in Pavlick and Nenkova (2015). 3.3 11. 12. 13. 14. 15. used to measure word and phrase similarity, possibly to improve paraphrasing. Multiview Latent Semantic Analysis (MVLSA) is a state-of-the-art method for modeling word similarities. MVLSA can incorporate an arbitrary number of data views, such as monolingual signals, bilingual signals, and even signals from other embeddings. PPDB 2.0 contains new similarity features based on MVLSA embeddings for all phrases. A complete discussion is given in Rastogi et al. (2015). Although we typically think of paraphrases as equivalent or as bidirectionally entailing, a substantial f"
P15-2070,P11-1062,0,0.0101222,"milar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatic"
P15-2070,P15-1146,1,0.863748,"Missing"
P15-2070,D07-1017,0,0.0315597,"(2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of"
P15-2070,D14-1162,0,0.112672,"Missing"
P15-2070,S14-2114,0,0.0184637,"Missing"
P15-2070,N15-1058,1,0.0846646,"Missing"
P15-2070,Q14-1018,0,0.0256743,"embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes"
P15-2070,S14-2039,0,0.013621,"embedding similarities, and style annotations. 1 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Introduction The Paraphrase Database (PPDB) is a collection of over 100 million paraphrases that was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes"
P15-2070,C12-1177,0,0.00936657,"ntences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et al. (2011) which reranked bilingually-extracted paraphrases using monolingual distributional similarities, but did not use a supervised model. Work that is relevant to our classification of semantic entailment types to each paraphrase, includes learning directionality of inference rules (Bhagat et al., 2007; Berant et al., 2011) and learning hypernyms rather than paraphrases (Snow et al., 2004). Our style annotations are related to Xu et al. (2012)’s efforts at learning stylistic paraphrases. Our word embeddings additions to the paraphrase database are related to many current projects on that topic, including projects that attempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraph"
P15-2070,Q15-1021,1,0.0693675,"ttempt to customize embeddings to lexical resources (Faruqui et al., 2015). However, the Rastogi et al. (2015) embeddings included here were shown to be state-of-the art in Style scores Some of the variation within paraphrase sets can be attributed to stylistic variations of language. We automatically induce style information on each rule in PPDB for two dimensions– complexity and formality. Table 2 shows some paraphrases of the end, sorted from most complex to most simple using these scores. These classifications could be useful for natural language generation tasks like text simplification (Xu et al., 2015). A complete evaluation of these scores is given in Pavlick and Nenkova (2015). 3.3 11. 12. 13. 14. 15. used to measure word and phrase similarity, possibly to improve paraphrasing. Multiview Latent Semantic Analysis (MVLSA) is a state-of-the-art method for modeling word similarities. MVLSA can incorporate an arbitrary number of data views, such as monolingual signals, bilingual signals, and even signals from other embeddings. PPDB 2.0 contains new similarity features based on MVLSA embeddings for all phrases. A complete discussion is given in Rastogi et al. (2015). Although we typically think"
P15-2070,D13-1056,1,0.547488,"Missing"
P15-2070,P14-2089,0,0.046308,"was automatically constructed by Ganitkevitch et al. (2013). Although it is relatively new, it has been adopted by a large number of researchers, who have demonstrated that it is useful for a variety of natural language processing tasks. It has been used for recognizing textual entailment (Beltagy et al., 2014; Bjerva et al., 2014), measuring the semantic similarity of texts (Han et al., 2013; Ji and Eisenstein, 2013; Sultan et al., 2014b), monolingual alignment (Yao et al., 2013; Sultan et al., 2014a), natural language generation (Ganitkevitch et al., 2011), and improved lexical embeddings (Yu and Dredze, 2014; Rastogi et al., 2015; Faruqui et al., 2015). For any given input phrase to PPDB, there are often dozens or hundreds of possible paraphrases. There are several interesting research questions that arise because of the number and variety of paraphrases in PPDB. How can we distinguish between correct and incorrect paraphrases? Within the paraphrase sets, are all of the paraphrases truly substitutable or do they sometimes exhibit other types of relationships (like directional entailment)? When the paraphrases share the same meaning, are there stylistic reasons why we should choose one versus anot"
P15-2070,P08-1116,0,0.0365443,"pes from Wikipedia (constraining to types which appear in PPDB), and collected human judgments for their full list of paraphrases. 2 https://code.google.com/p/word2vec/ For phrases, we use the vector of the rarest word as an approximation of the vector for the phrase. 3 427 1. 2. 3. 4. 5. achieves a 9-12 point improvement in MRR over the PPDB 1.0 rankings. Similarly, it improves AP by 7-9 points. 3 Other Additions Entailment relations 4 the final analysis the last the finish the final part the last part Related Work The most closely related work to our supervised re-ranking of PPDB is work by Zhao et al. (2008) and Malakasiotis and Androutsopoulos (2011). Zhao et al. (2008) improved Bannard and Callison-Burch (2005)’s paraphrase probability by converting it into log-linear model inspired by machine translation, allowing them to incorporate a variety of features. Malakasiotis and Androutsopoulos (2011) developed a similar model trained on human judgements. Both efforts apply their model to natural language generation by paraphrasing full sentences. We apply our model to the sub-sentential paraphrases directly, in order to improve the quality of the Paraphrase Database. Also related is work by Chan et"
P15-2070,N15-1184,0,\N,Missing
P15-2070,W11-2504,1,\N,Missing
P15-2070,D08-1021,1,\N,Missing
P16-1204,W06-1805,0,0.175972,"ther it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with comp"
P16-1204,W07-1430,0,0.196159,"or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We"
P16-1204,D14-1059,0,0.0246266,"DEL(red, 5) a1 girl2 in3 a4 dress5 We say that the entailment relation that holds between x and e(x) is generated by the edit e. In the above example, we would say that e generates a forward entailment (@) since a girl in a red dress entails a girl in a dress. 3 Natural Logic Entailment Relations Natural logic (MacCartney, 2009) is a formalism that describes entailment relationships between natural language strings, rather than operating over mathematical formulae. Natural logic enables both light-weight representation and robust inference, and is an increasingly popular choice for NLU tasks (Angeli and Manning, 2014; Bowman et al., 2015b; Pavlick et al., 2015). There are seven “basic entailment relations” described by natural logic, five of which we explore here.1 These five relations, as they might hold between an AN and the head N, are summarized in Figure 1. The forward entailment relation is the restrictive case, in which the AN (brown dog) is a subset of (and thus entails) the N (dog) but the N does not entail the AN (dog does not entail brown dog). The symmetric reverse entailment can also occur, in which the N is a subset of the set denoted by the AN. An example of this is the AN possible solution"
P16-1204,P15-1034,0,0.014064,"Missing"
P16-1204,D10-1115,0,0.0125819,"nt perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We have investigated the problem of adjective-noun composition, specifically in relation to the task of RTE. AN composition is capable of producing a range of natural logic entailment relationship, at odds with commonly-used heuristics which treat all adjectives a restrictive. We have shown that predicting these e"
P16-1204,E12-1004,0,0.0148644,"bine. Adjectival modification is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce eith"
P16-1204,S14-2114,0,0.0577574,"Missing"
P16-1204,W13-0104,0,0.184772,"Missing"
P16-1204,D15-1075,0,0.625266,"uistic phenomena like implicature (3) have yet to be explicitly included in RTE tasks, commonsense inferences like those in (4) (from the SICK dataset) have become a common part of NLU tasks like RTE, question answering, and image labeling. chased by a bear and are running for their lives! Example (4) is just one of many RTE problems which rely on intuition rather than strict logical inference. Transformation-based RTE. There have been an enormous range of approaches to automatic RTE– from those based on theorem proving (Bjerva et al., 2014) to those based on vector space models of semantics (Bowman et al., 2015a). Transformation-based RTE systems attempt to solve the RTE problem by identifying a sequence of atomic edits (MacCartney, 2009) which can be applied, one by one, in order to transform p into h. Each edit can be associated with some entailment relation. Then, the entailment relation that holds between p and h overall is a function of the entailment relations associated with each atomic edit. This approach is appealing in that it breaks potentially complex p/h pairs into a series of bite-sized pieces. Transformation-based RTE is widely used, not only in rule-based approaches (MacCartney and M"
P16-1204,W15-0705,0,0.0151887,"ly artificial contexts, as we believe this will result in a greater variety of entailment relations and will avoid systematically biasing our judgements toward entailments. Second, we use the most frequent AN pairs, as these will better represent the types of ANs that NLU systems are likely to encounter in practice. We look at four different corpora capturing four different genres: Annotated Gigaword (Napoles et al., 2012) (News), image captions (Young et al., 2014) (Image Captions), the Internet Argument Corpus (Walker et al., 2012) (Forums), and the prose fiction subset of GutenTag dataset (Brooke et al., 2015) (Literature). From each corpus, we select the 100 nouns which occur with the largest number of unique adjectives. Then, for each noun, we take the 10 adjectives with which the noun occurs most often. For each AN, we choose 3 contexts2 in which the N appears unmodified, and generate p/h pairs by inserting the A into each. We collect 3 judgements for each p/h pair. Since this task is subjective, and we want to focus our analysis on clean instances on which human agreement is high, we remove pairs for which one or more of the annotators chose the “does not make sense” option and pairs for which"
P16-1204,W04-3205,0,0.0490343,", 2014) includes a suite of RTE systems, including baseline systems as well as featurerich supervised systems which provide state-of-the-art performance on the RTE3 datasets (Giampiccolo et al., 2007). We test two systems from Excitement: the simple Maximum Entropy (MaxEnt) model which uses a suite of dense, similarity-based features (e.g. word overlap, cosine similarity), and the more sophisticated Maximum Entropy model (MaxEnt+LR) which uses the same similarity-based features but additionally incorporates features from external lexical resources such as WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). We also train a standard unigram model (BOW). Transformation-based. The Excitement platform also includes a transformation-based RTE system called BIUTEE (Stern and Dagan, 2012). The BIUTEE system derives a sequence of edits that can be used to transform the premise into the hypothesis. These edits are represented using feature vectors, and the system searches over edit sequences for the lowest cost “proof” of either entailment or non-entailment. The feature weights are set by logistic regression during training. Deep learning. Bowman et al. (2015a) recently reported very promising results u"
P16-1204,W07-1401,0,0.0279932,"2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce either a binary (YES/NO) or trinary (ENTAILMENT/CONTRADICTION/UNKNOWN) output. The type of knowledge tested in the RTE task has shifted in recent years. While older datasets mostly captured logical reasoning (Cooper et al., 1996) and lexical knowledge (Giampiccolo et al., 2007) (see Examples (1) and (2) in Table 1), the recent datasets have become increasingly reliant on common-sense knowledge of scenes and events (Marelli et al., 2014). In Example (4) in Table 1, for which the gold label is ENTAILMENT , it is perfectly reasonable to assume the dogs are playing. However, this is not necessarily true that running entails playing– maybe the dogs are being 2164 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2164–2173, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) FraCas (2) RTE2"
P16-1204,W10-2805,0,0.0149709,"t al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We have investigated the problem of adjective-noun composition, specifically in relation to the task of RTE. AN composition is capable of producing a range of natural logic entailment relationship, at odds with commonly-used heuristics which treat all adjectives a restrictive. We have shown that predicting these entailment relat"
P16-1204,D11-1050,0,0.0139231,"n NLP, has explored different classes of adjectives (e.g. privative, intensional) as they relate to entailment (Kamp and Partee, 1995; Partee, 2007; Boleda et al., 2013; Nayak et al., 2014). In general, prior studies have focused on modeling properties of the adjectives alone, ignoring the context-dependent nature of AN/N entailments– i.e. in prior work little is always restrictive, whether it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed"
P16-1204,P14-5008,0,0.0201818,"Missing"
P16-1204,marelli-etal-2014-sick,0,0.0138331,"NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce either a binary (YES/NO) or trinary (ENTAILMENT/CONTRADICTION/UNKNOWN) output. The type of knowledge tested in the RTE task has shifted in recent years. While older datasets mostly captured logical reasoning (Cooper et al., 1996) and lexical knowledge (Giampiccolo et al., 2007) (see Examples (1) and (2) in Table 1), the recent datasets have become increasingly reliant on common-sense knowledge of scenes and events (Marelli et al., 2014). In Example (4) in Table 1, for which the gold label is ENTAILMENT , it is perfectly reasonable to assume the dogs are playing. However, this is not necessarily true that running entails playing– maybe the dogs are being 2164 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2164–2173, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) FraCas (2) RTE2 (3) NA (4) SICK p h p h p h p h No delegate finished the report on time. Some Scandinavian delegate finished the report on time. Trade between China and India is"
P16-1204,W14-4724,0,0.0548291,"(2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 2010; Boleda et al., 2013). However, work on distributional AN composition has so far remained out-of-context, and has mostly been evaluated in terms of overall “similarity” rather than directly addressing the entailment properties associated with composition. 9 Conclusion We have investigated the"
P16-1204,W12-3018,0,0.0156506,"Missing"
P16-1204,P15-1146,1,0.572693,"Missing"
P16-1204,C08-1066,0,0.0382672,"ce requires understanding not only the meanings of the individual words, but also understanding how those meanings combine. Adjectival modification is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis"
P16-1204,W13-0509,0,0.0950159,"to human levels, indicating that the systems fail even to memorize the most-likely class for each adjective in training. 8 Related Work Past work, both in linguistics and in NLP, has explored different classes of adjectives (e.g. privative, intensional) as they relate to entailment (Kamp and Partee, 1995; Partee, 2007; Boleda et al., 2013; Nayak et al., 2014). In general, prior studies have focused on modeling properties of the adjectives alone, ignoring the context-dependent nature of AN/N entailments– i.e. in prior work little is always restrictive, whether it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae"
P16-1204,P12-3013,0,0.374545,"only the meanings of the individual words, but also understanding how those meanings combine. Adjectival modification is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human readi"
P16-1204,W14-5418,0,0.0225433,"2007; Boleda et al., 2013; Nayak et al., 2014). In general, prior studies have focused on modeling properties of the adjectives alone, ignoring the context-dependent nature of AN/N entailments– i.e. in prior work little is always restrictive, whether it is modifying baby or control. Pustejovsky (2013) offer a preliminary analysis of the contextual complexities surrounding adjective inference, which reinforces many of the observations we have made here. Hartung and Frank (2011) analyze adjectives in terms of the properties they modify but don’t address them from an entailment perspective. Tien Nguyen et al. (2014) look at the 2170 adjectives in the restricted domain of computer vision. Other past work has employed first-order logic and other formal representations of adjectives in order to provide compositional entailment predictions (Amoia and Gardent, 2006; Amoia and Gardent, 2007; McCrae et al., 2014). Although theoretically appealing, such rigid logics are unlikely to provide the flexibility needed to handle the type of common-sense inferences we have discussed here. Distributional representations provide much greater flexibility in terms of representation (Baroni and Zamparelli, 2010; Guevara, 201"
P16-1204,walker-etal-2012-corpus,0,0.0180227,"e above pilot experiments, we proceed with our study as follows. First, we use only artificial contexts, as we believe this will result in a greater variety of entailment relations and will avoid systematically biasing our judgements toward entailments. Second, we use the most frequent AN pairs, as these will better represent the types of ANs that NLU systems are likely to encounter in practice. We look at four different corpora capturing four different genres: Annotated Gigaword (Napoles et al., 2012) (News), image captions (Young et al., 2014) (Image Captions), the Internet Argument Corpus (Walker et al., 2012) (Forums), and the prose fiction subset of GutenTag dataset (Brooke et al., 2015) (Literature). From each corpus, we select the 100 nouns which occur with the largest number of unique adjectives. Then, for each noun, we take the 10 adjectives with which the noun occurs most often. For each AN, we choose 3 contexts2 in which the N appears unmodified, and generate p/h pairs by inserting the A into each. We collect 3 judgements for each p/h pair. Since this task is subjective, and we want to focus our analysis on clean instances on which human agreement is high, we remove pairs for which one or m"
P16-1204,Q14-1006,0,0.583785,"fication is one of the most basic types of composition in natural language. Most existing work in NLU makes a simplifying assumption that adjectives tend to be restrictive– i.e. adding an adjective modifier limits the set of things to which the noun phrase can refer. For example, the set of little dogs is a subset of the set of dogs, and we cannot in general say that dog entails little dog. This assumption has been exploited by high-performing RTE systems (MacCartney and Manning, 2008; Stern and Dagan, 2012), as well as used as the basis for learning new entailment rules (Baroni et al., 2012; Young et al., 2014). However, this simplified view of adjectival modification often breaks down in practice. Consider the question of whether laugh entails bitter laugh in the follow• We benchmark several state-of-the-art RTE systems on this task. 2 Recognizing Textual Entailment The task of recognizing textual entailment (RTE) (Dagan et al., 2006) is commonly used to evaluate the state-of-the-art of automatic NLU. The RTE task is: given two utterances, a premise (p) and a hypothesis (h), would a human reading p typically infer that h is most likely true? Systems are expected to produce either a binary (YES/NO)"
P16-2024,E99-1042,0,0.561558,"Missing"
P16-2024,W11-1601,0,0.0591354,"and which match the syntactic category of the target. On average, Simple PPDB proposes 8.8 such candidate simplifications per target. Comparison to existing methods. Our baselines include three existing methods for generating lists of candidates that were proposed in prior work. The methods we test for generating lists of candidate paraphrases for a given target are: the WordNetGenerator, which pulls synonyms from WordNet (Devlin and Tait, 1998; Carroll et al., 1999), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal Wikipedia (Coster and Kauchak, 2011a), and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glavaˇs and ˇ Stajner, 2015) (we use the pre-trained Word2Vec VSM (Mikolov et al., 2013)). For each generated list, we follow Horn et al. (2014)’s supervised SVM Rank approach to rank the candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (C"
P16-2024,P11-2117,0,0.0242177,"and which match the syntactic category of the target. On average, Simple PPDB proposes 8.8 such candidate simplifications per target. Comparison to existing methods. Our baselines include three existing methods for generating lists of candidates that were proposed in prior work. The methods we test for generating lists of candidate paraphrases for a given target are: the WordNetGenerator, which pulls synonyms from WordNet (Devlin and Tait, 1998; Carroll et al., 1999), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal Wikipedia (Coster and Kauchak, 2011a), and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glavaˇs and ˇ Stajner, 2015) (we use the pre-trained Word2Vec VSM (Mikolov et al., 2013)). For each generated list, we follow Horn et al. (2014)’s supervised SVM Rank approach to rank the candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (C"
P16-2024,N13-1092,1,0.427348,"Missing"
P16-2024,P15-2011,0,0.139424,"Missing"
P16-2024,C04-1129,0,0.0311292,"e Paraphrase Database containing 4.5 million simplifying paraphrase rules. The large scale of Simple PPDB will support research into increasingly advanced methods for text simplification. Motivation Language is complex, and the process of reading and understanding language is difficult for many groups of people. The goal of text simplification is to rewrite text in order to make it easier to understand, for example, by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajne"
P16-2024,P14-2075,0,0.101407,"tes that were proposed in prior work. The methods we test for generating lists of candidate paraphrases for a given target are: the WordNetGenerator, which pulls synonyms from WordNet (Devlin and Tait, 1998; Carroll et al., 1999), the KauchakGenerator, which generates candidates based on automatic alignments between Simple Wikipedia and normal Wikipedia (Coster and Kauchak, 2011a), and the GlavasGenerator, which generates candidates from nearby phrases in vector space (Glavaˇs and ˇ Stajner, 2015) (we use the pre-trained Word2Vec VSM (Mikolov et al., 2013)). For each generated list, we follow Horn et al. (2014)’s supervised SVM Rank approach to rank the candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (Coster and Kauchak, 2011b). We omit the language modeling features since our evaluation does not consider the context in which the substitution is to be applied. All of these methods (the three generation methods and the ranker) are imple"
P16-2024,S12-1046,0,0.0133657,"by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajner, 2015; Paetzold, 2015), existing models have been byand-large limited to single words. Often, how1 http://www.seas.upenn.edu/˜nlp/ resources/simple-ppdb.tgz 143 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 143–148, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2 Identifying Simplification Rules 2.1 scored, the average huma"
P16-2024,W10-0406,0,0.0253281,"araphrase rules. The large scale of Simple PPDB will support research into increasingly advanced methods for text simplification. Motivation Language is complex, and the process of reading and understanding language is difficult for many groups of people. The goal of text simplification is to rewrite text in order to make it easier to understand, for example, by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajner, 2015; Paetzold, 2015), existing models have been byand"
P16-2024,P12-1107,0,0.0348609,"Missing"
P16-2024,P15-4015,0,0.0614715,"e candidates for simplicity. We reimplement the main features of their model: namely, word frequencies according to the Google NGrams corpus (Brants and Franz, 2006) and the Simple Wikipedia corpus, and the alignment probabilities according to automatic word alignments between Wikipedia and Simple Wikipedia sentences (Coster and Kauchak, 2011b). We omit the language modeling features since our evaluation does not consider the context in which the substitution is to be applied. All of these methods (the three generation methods and the ranker) are implemented as part of the LEXenstein toolkit (Paetzold and Specia, 2015). We use the LEXenstein implementations for the results reported here, using off-the-shelf configurations and treating each method as a black box. Table 2: Accuracy on 10-fold cross-validation, and precision for identifying simplifying rules. Folds are constructed so that train and test vocabularies are disjoint. 5 points higher than the strongest baseline, a supervised model which uses only word embeddings as features. 2.3 Simple PPDB We run the trained model described above over all 7.5 million paraphrase rules. From the predictions, we construct Simple PPDB: a list of 4.5 million simplifyin"
P16-2024,Q15-1021,1,0.526854,"i, for each feature f , we include f (e1 ), f (e2 ) and f (e1 ) − f (e2 ).3 We also include the cosine similarity of the averaged word embeddings and the PPDB paraphrase quality score as features. We train a multi-class logistic regression model4 to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more complex output, or 3) non-sense output. Data. We collect our training data in two phases. First, we sample 1,000 phrases from the vocabulary of the PPDB. We limit ourselves to words which also appear at least once in the Newsela corpus for text simplifcation (Xu et al., 2015), in order to ensure that we focus our model on the types of words for which the final resource is most likely to be applied. For each of these 1,000 words/phrases, we sample up to 10 candidate paraphrases from PPDB, stratified evenly across paraphrase quality scores. We ask workers on Amazon Mechanical Turk to rate each of the chosen paraphrase rules on a scale from 1 to 5 to indicate how well the paraphrase preserves the meaning of the original phrase. We use the same annotation design used in Pavlick et al. (2015). We have 5 workers judge each pair, omitting workers who do not provide corre"
P16-2024,N15-2002,0,0.0616377,"Missing"
P16-2024,Q16-1029,1,0.554725,"will support research into increasingly advanced methods for text simplification. Motivation Language is complex, and the process of reading and understanding language is difficult for many groups of people. The goal of text simplification is to rewrite text in order to make it easier to understand, for example, by children (De Belder and Moens, 2010), language learners (Petersen and Ostendorf, 2007), people with disabilities (Rello et al., 2013; Evans et al., 2014), and even by machines (Siddharthan et al., 2004). Automatic text simplification (Napoles and Dredze, 2010; Wubben et al., 2012; Xu et al., 2016) has the potential to dramatically increase access to information by making written documents available at all reading levels. Full text simplification involves many steps, including grammatical restructuring and summarization (Feng, 2008). One of the most basic subtasks is lexical simplification (Specia et al., 2012)– replacing complicated words and phrases with simpler paraphrases. While there is active research in the area of lexical simplification (Coster ˇ and Kauchak, 2011a; Glavaˇs and Stajner, 2015; Paetzold, 2015), existing models have been byand-large limited to single words. Often,"
P16-2024,N15-1023,1,0.593985,"Missing"
P16-2024,P15-2070,1,0.867766,"Missing"
P16-2024,W14-1215,0,\N,Missing
P18-1239,W16-3210,0,0.0491398,"Missing"
P18-1239,D15-1021,0,0.0169179,"Instead, we show a gradual degradation in performance as words become more abstract. Their dataset is restricted to six high-resource languages and a small vocabulary of 557 English words. In contrast, we present results for over 260,000 English words and 32 foreign languages. Recent research in the NLP and computer vision communities has been enabled by large collections of images associated with words or longer texts. Object recognition has seen dramatic gains in part due to the ImageNet database (Deng et al., 2009), which contains 500-1000 images associated with 80,000 synsets in WordNet. Ferraro et al. (2015) surveys existing corpora that are used in vision and language research. Other NLP+Vision tasks that have been enabled by the availability of large datasets include caption generation for images, action recognition in videos, visual question answering, and others. Most existing work on multilingual NLP+Vision relies on having a corpus of images manually annotated with captions in several languages, as in the Multi30K dataset (Elliott et al., 2016). Several works have proposed using image features to improve sentence level translations or to translate image captions (Gella et al., 2017; Hitschl"
P18-1239,D15-1070,0,0.0278063,"nd language research. Other NLP+Vision tasks that have been enabled by the availability of large datasets include caption generation for images, action recognition in videos, visual question answering, and others. Most existing work on multilingual NLP+Vision relies on having a corpus of images manually annotated with captions in several languages, as in the Multi30K dataset (Elliott et al., 2016). Several works have proposed using image features to improve sentence level translations or to translate image captions (Gella et al., 2017; Hitschler and Riezler, 2016; Miyazaki and Shimizu, 2016). Funaki and Nakayama (2015) show that automatically scraped data from websites in English and Japanese can be used to effectively perform zero-shot learning for the task of cross-lingual document retrieval. Since collecting multilingual annotations is difficult at a large-scale or for low-resource languages, our approach relies only on data scraped automatically from the web. 3 Corpus Construction We present a new dataset for image-based word translation that is more expansive than any previous ones, encompassing all parts-of-speech, the gamut of abstract to concrete, and both low- and highresource languages. 3.1 Dictio"
P18-1239,P98-1069,0,0.385353,"Missing"
P18-1239,D17-1303,0,0.0757877,"dNet. Ferraro et al. (2015) surveys existing corpora that are used in vision and language research. Other NLP+Vision tasks that have been enabled by the availability of large datasets include caption generation for images, action recognition in videos, visual question answering, and others. Most existing work on multilingual NLP+Vision relies on having a corpus of images manually annotated with captions in several languages, as in the Multi30K dataset (Elliott et al., 2016). Several works have proposed using image features to improve sentence level translations or to translate image captions (Gella et al., 2017; Hitschler and Riezler, 2016; Miyazaki and Shimizu, 2016). Funaki and Nakayama (2015) show that automatically scraped data from websites in English and Japanese can be used to effectively perform zero-shot learning for the task of cross-lingual document retrieval. Since collecting multilingual annotations is difficult at a large-scale or for low-resource languages, our approach relies only on data scraped automatically from the web. 3 Corpus Construction We present a new dataset for image-based word translation that is more expansive than any previous ones, encompassing all parts-of-speech, t"
P18-1239,P08-1088,0,0.0489847,"s the largest of its kind and should be a standard for future work in learning translations from images. The dataset may facilitate research into multilingual, multimodal models, and translation of low-resource languages. 2 Related Work The task of learning translations without sentencealigned bilingual parallel texts is often called bilingual lexicon induction (Rapp, 1999; Fung and Yee, 1998). Most work in bilingual lexicon induction has focused on text-based methods. Some researchers have used similar spellings across related languages to find potential translations (Koehn and Knight, 2002; Haghighi et al., 2008). Others have exploited temporal similarity of word frequencies to induce translation pairs (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Irvine and Callison-Burch (2017) provide a systematic study of different text-based features used for bilingual lexicon induction. Recent work has focused on building joint distributional word embedding spaces for multiple languages, leveraging a range of levels of language supervision from bilingual dictionaries to comparable texts (Vuli´c and Korhonen, 2016; Wijaya et al., 2017). The most closely related work to ours is research into bilingual l"
P18-1239,P16-1227,0,0.0248086,"(2015) surveys existing corpora that are used in vision and language research. Other NLP+Vision tasks that have been enabled by the availability of large datasets include caption generation for images, action recognition in videos, visual question answering, and others. Most existing work on multilingual NLP+Vision relies on having a corpus of images manually annotated with captions in several languages, as in the Multi30K dataset (Elliott et al., 2016). Several works have proposed using image features to improve sentence level translations or to translate image captions (Gella et al., 2017; Hitschler and Riezler, 2016; Miyazaki and Shimizu, 2016). Funaki and Nakayama (2015) show that automatically scraped data from websites in English and Japanese can be used to effectively perform zero-shot learning for the task of cross-lingual document retrieval. Since collecting multilingual annotations is difficult at a large-scale or for low-resource languages, our approach relies only on data scraped automatically from the web. 3 Corpus Construction We present a new dataset for image-based word translation that is more expansive than any previous ones, encompassing all parts-of-speech, the gamut of abstract to concr"
P18-1239,J17-2001,1,0.851721,"models, and translation of low-resource languages. 2 Related Work The task of learning translations without sentencealigned bilingual parallel texts is often called bilingual lexicon induction (Rapp, 1999; Fung and Yee, 1998). Most work in bilingual lexicon induction has focused on text-based methods. Some researchers have used similar spellings across related languages to find potential translations (Koehn and Knight, 2002; Haghighi et al., 2008). Others have exploited temporal similarity of word frequencies to induce translation pairs (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Irvine and Callison-Burch (2017) provide a systematic study of different text-based features used for bilingual lexicon induction. Recent work has focused on building joint distributional word embedding spaces for multiple languages, leveraging a range of levels of language supervision from bilingual dictionaries to comparable texts (Vuli´c and Korhonen, 2016; Wijaya et al., 2017). The most closely related work to ours is research into bilingual lexicon induction using image similarity by Bergsma and Van Durme (2011) and Kiela et al. (2015). Their work differs from ours in that they focused more narrowly on the translation o"
P18-1239,P14-2135,0,0.225906,"olinguistics and cognitive psychology literature. Concrete words directly reference a sense experience (Paivio et al., 1968), while abstract words can denote ideas, emotions, feelings, qualities or other abstract or intangible concepts. Concreteness ratings are closely correlated with imagery ratings, defined as the ease with which a word arouses a mental image (Gilhooly and Logie, 1980; Friendly et al., 1982). Intuitively, concrete words are easier to represent visually, so a measure of a word’s concreteness ought to be able to predict the effectiveness of using images to translate the word. Kiela et al. (2014) defines an unsupervised method called image dispersion that approximates a word’s concreteness by taking the average pairwise cosine distance of a set of image representations of the word. Kiela et al. (2015) show that image dispersion helps predict the usefulness of image representations for translation. In this paper, we introduce novel supervised approaches for predicting word concreteness from image and textual features. We make use of a dataset created by Brysbaert et al. (2014) containing human evaluations of concreteness for 39,954 English words. Concurrently with our work, Hartmann an"
P18-1239,D15-1015,0,0.305535,"Missing"
P18-1239,P06-1103,0,0.13398,"Missing"
P18-1239,W02-0902,0,0.0637204,"is our dataset, which is the largest of its kind and should be a standard for future work in learning translations from images. The dataset may facilitate research into multilingual, multimodal models, and translation of low-resource languages. 2 Related Work The task of learning translations without sentencealigned bilingual parallel texts is often called bilingual lexicon induction (Rapp, 1999; Fung and Yee, 1998). Most work in bilingual lexicon induction has focused on text-based methods. Some researchers have used similar spellings across related languages to find potential translations (Koehn and Knight, 2002; Haghighi et al., 2008). Others have exploited temporal similarity of word frequencies to induce translation pairs (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Irvine and Callison-Burch (2017) provide a systematic study of different text-based features used for bilingual lexicon induction. Recent work has focused on building joint distributional word embedding spaces for multiple languages, leveraging a range of levels of language supervision from bilingual dictionaries to comparable texts (Vuli´c and Korhonen, 2016; Wijaya et al., 2017). The most closely related work to ours is r"
P18-1239,P16-1168,0,0.0153022,"ora that are used in vision and language research. Other NLP+Vision tasks that have been enabled by the availability of large datasets include caption generation for images, action recognition in videos, visual question answering, and others. Most existing work on multilingual NLP+Vision relies on having a corpus of images manually annotated with captions in several languages, as in the Multi30K dataset (Elliott et al., 2016). Several works have proposed using image features to improve sentence level translations or to translate image captions (Gella et al., 2017; Hitschler and Riezler, 2016; Miyazaki and Shimizu, 2016). Funaki and Nakayama (2015) show that automatically scraped data from websites in English and Japanese can be used to effectively perform zero-shot learning for the task of cross-lingual document retrieval. Since collecting multilingual annotations is difficult at a large-scale or for low-resource languages, our approach relies only on data scraped automatically from the web. 3 Corpus Construction We present a new dataset for image-based word translation that is more expansive than any previous ones, encompassing all parts-of-speech, the gamut of abstract to concrete, and both low- and highre"
P18-1239,petrov-etal-2012-universal,0,0.124649,"Missing"
P18-1239,P99-1067,0,0.452331,"Missing"
P18-1239,P16-2068,0,0.0514535,"l. (2014)’s dataset, which provides human judgments for about 40k words, each with a 1-5 abstractnessto-concreteness score, and scraped 100 images from English Google Image Search for each word. We then trained a two-layer perceptron with one hidden layer of 32 units, to predict word concreteness. The inputs to the network were the elementwise mean and standard deviation (concatenated into a 8094-dimensional vector)of the CNN features for each of the images corresponding to a word. To better assess this image-only approach, we also experimented with using the distributional word embeddings of Salle et al. (2016) as input. We used these 300-dimensional vectors either seperately or concatentated with the image-based features. Our final network was trained with a crossentropy loss, although an L2 loss performed nearly as well. We randomly selected 39,000 words as our training set. Results on the remaining held-out validation set are visualized in Figure 3. Although the concatenated image and word embedding features performed the best, we do not expect to have high-quality word embeddings for words in low-resource languages. Therefore, for the evaluation in Section 6.2, we used the imageembeddings-only m"
P18-1239,W02-2026,0,0.0739764,"t may facilitate research into multilingual, multimodal models, and translation of low-resource languages. 2 Related Work The task of learning translations without sentencealigned bilingual parallel texts is often called bilingual lexicon induction (Rapp, 1999; Fung and Yee, 1998). Most work in bilingual lexicon induction has focused on text-based methods. Some researchers have used similar spellings across related languages to find potential translations (Koehn and Knight, 2002; Haghighi et al., 2008). Others have exploited temporal similarity of word frequencies to induce translation pairs (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Irvine and Callison-Burch (2017) provide a systematic study of different text-based features used for bilingual lexicon induction. Recent work has focused on building joint distributional word embedding spaces for multiple languages, leveraging a range of levels of language supervision from bilingual dictionaries to comparable texts (Vuli´c and Korhonen, 2016; Wijaya et al., 2017). The most closely related work to ours is research into bilingual lexicon induction using image similarity by Bergsma and Van Durme (2011) and Kiela et al. (2015). Their work differs fro"
P18-1239,P16-1024,0,0.0538587,"Missing"
P18-1239,N13-1011,0,0.0200976,"´c and Korhonen, 2016; Wijaya et al., 2017). The most closely related work to ours is research into bilingual lexicon induction using image similarity by Bergsma and Van Durme (2011) and Kiela et al. (2015). Their work differs from ours in that they focused more narrowly on the translation of nouns for a limited number of high resource languages. Bergsma and Van Durme (2011) compiled datasets for Dutch, English, French, German, Italian, and Spanish by downloading 20 images for up to 500 concrete nouns in each of the foreign languages, and 20,000 English words. Another dataset was generated by Vulic and Moens (2013) who collected images for 1,000 words in Spanish, Italian, and Dutch, along with the English translations for each. Their dataset also consists of only nouns, but includes abstract nouns. Our corpus will allow researchers to explore image similarity for bilingual lexicon induction on a much wider range of languages and parts of speech, which is especially desirable given the potential utility of the method for improving translation between languages with little parallel text. The ability of images to usefully represent a word is strongly dependent on how concrete or abstract the word is. The t"
P18-1239,D17-1152,1,0.858726,"languages to find potential translations (Koehn and Knight, 2002; Haghighi et al., 2008). Others have exploited temporal similarity of word frequencies to induce translation pairs (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Irvine and Callison-Burch (2017) provide a systematic study of different text-based features used for bilingual lexicon induction. Recent work has focused on building joint distributional word embedding spaces for multiple languages, leveraging a range of levels of language supervision from bilingual dictionaries to comparable texts (Vuli´c and Korhonen, 2016; Wijaya et al., 2017). The most closely related work to ours is research into bilingual lexicon induction using image similarity by Bergsma and Van Durme (2011) and Kiela et al. (2015). Their work differs from ours in that they focused more narrowly on the translation of nouns for a limited number of high resource languages. Bergsma and Van Durme (2011) compiled datasets for Dutch, English, French, German, Italian, and Spanish by downloading 20 images for up to 500 concrete nouns in each of the foreign languages, and 20,000 English words. Another dataset was generated by Vulic and Moens (2013) who collected images"
P19-1365,D18-1431,0,0.0120596,"of 100 or generate 100 samples through random sampling, and then we select 10 from the 100, either through post-decoding clustering (PDC) or by taking the 10 candidates with highest likelihood. We examine these decoding strategies on two tasks: open ended dialog and image captioning. For each task, we evaluate both the quality and diversity of the 10 outputs from each strategy. 5.1 Open-ended Dialog Task In the dialog domain, we use an LSTM-based sequence-to-sequence (Seq2Seq) model implemented in the OpenNMT framework (Klein et al., 2017). We match the model architecture and training data of Baheti et al. (2018). The Seq2Seq model has four layers each in the encoder and decoder, with hidden size 1000, and was trained on a cleaned version of OpenSubtitles (Tiedemann, 2009) to predict the next utterance given the previous one. Evaluation is performed on 100 prompts from the Cornell Movie Dialog Corpus (DanescuNiculescu-Mizil and Lee, 2011). These prompts are a subset of the 1000 prompts used in Baheti et al. (2018), which were filtered using item response theory for discriminative power. We report perplexity (PpL), averaged over all the top 10 outputs for each example.5 Since the quality of open-ended"
P19-1365,W11-0609,0,0.0235575,"Missing"
P19-1365,D13-1128,0,0.0349248,"agnostic to model architecture and to the data type of the input, as long as the output of the model is a probability distribution over tokens in a sequence. Automatic Quality Evaluation An important part of this work is how to accurately measure not only the effect these methods have on candidate diversity, but also on the overall quality of the candidates. In choosing to report human scores and perplexity for the dialog domain, and SPICE for image captioning, we omitted some quality measures used in other papers. For image captioning, BLEU (Papineni et al., 2001), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), and CIDer (Vedantam et al., 2015) scores are often reported, but SPICE has been shown to have higher correlation with human judgments (Anderson et al., 2016). In the dialog domain, single-reference BLEU score (Papineni et al., 2001) is sometimes used to measure response quality, but it has been shown to have little correlation with human-judged quality (Liu et al., 2016). Therefore, most works in dialog systems use human evaluation as the ultimate measure of quality (Li et al., 2016a; Sedoc et al., 2018) 8 Conclusion In this work, we perform an analysis of posttraining decoding strategies th"
P19-1365,P18-1082,0,0.276597,"nsist of an encoder, which transforms some input x into a fixed-size latent representation, and a decoder which transforms these representations in order to output a conditional 1 Code can be found at https://github.com/ rekriz11/DeDiv. yˆt ∼ yt Choosing a temperature greater than one causes outputs to look increasingly more random, while bringing the temperature less than zero causes sequences to increasingly resemble greedy sampling. Recently, top-s random sampling has been proposed as an alternative to using temperature. Sampling is restricted to the s most likely tokens 3753 at each step (Fan et al., 2018; Radford et al., 2019). We find that top-s random sampling’s hardrestriction on generating low probability words is more effective at controlling the stochasticity of sampled sequences than sampling with temperature. Beam Search Beam search approximates finding the most likely sequence by performing breadth-first search over a restricted search space. At every step of decoding, the method keeps track of b partial hypotheses. The next set of partial hypotheses are chosen by expanding every path from the existing set of b hypotheses, and then choosing the b with the highest scores. Most commonl"
P19-1365,D13-1111,0,0.34201,"skever et al., 2014). However, for more open-ended tasks, beam search is ill-suited to generating a set of diverse candidate sequences; this is because candidates outputted from a large-scale beam search often only differ by punctuation and minor morphological variations (Li and Jurafsky, 2016). The term “diversity” has been defined in a variety of ways in the literature, with some using it as a synonym for sentence interestingness or unlikeliness (Hashimoto et al., 2019), and others considering it a measure of how different two or more sentences are from each other (Vijayakumar et al., 2016; Gimpel et al., 2013). We take the latter approach, and define diversity as the ability of a generative method to create a set of possible outputs that are each valid given the input, but vary as widely as possible in terms of word choice, topic, and meaning. There are a number of reasons why it is desirable to produce a set of diverse candidate outputs for a given input. For example, in collaborative story generation, the system makes suggestions to a user for what they should write next (Clark et al., 2018). In these settings, it would be beneficial to show the user multiple different ways to continue their stor"
P19-1365,D17-1210,0,0.0533209,"Missing"
P19-1365,N19-1169,0,0.0124419,"nslation found that beam search is an effective strategy to heuristically sample sufficiently likely sequences from these probabilistic models (Sutskever et al., 2014). However, for more open-ended tasks, beam search is ill-suited to generating a set of diverse candidate sequences; this is because candidates outputted from a large-scale beam search often only differ by punctuation and minor morphological variations (Li and Jurafsky, 2016). The term “diversity” has been defined in a variety of ways in the literature, with some using it as a synonym for sentence interestingness or unlikeliness (Hashimoto et al., 2019), and others considering it a measure of how different two or more sentences are from each other (Vijayakumar et al., 2016; Gimpel et al., 2013). We take the latter approach, and define diversity as the ability of a generative method to create a set of possible outputs that are each valid given the input, but vary as widely as possible in terms of word choice, topic, and meaning. There are a number of reasons why it is desirable to produce a set of diverse candidate outputs for a given input. For example, in collaborative story generation, the system makes suggestions to a user for what they s"
P19-1365,P17-4012,0,0.0374685,"ion, we show results from oversampling then filtering. We use a beam size of 100 or generate 100 samples through random sampling, and then we select 10 from the 100, either through post-decoding clustering (PDC) or by taking the 10 candidates with highest likelihood. We examine these decoding strategies on two tasks: open ended dialog and image captioning. For each task, we evaluate both the quality and diversity of the 10 outputs from each strategy. 5.1 Open-ended Dialog Task In the dialog domain, we use an LSTM-based sequence-to-sequence (Seq2Seq) model implemented in the OpenNMT framework (Klein et al., 2017). We match the model architecture and training data of Baheti et al. (2018). The Seq2Seq model has four layers each in the encoder and decoder, with hidden size 1000, and was trained on a cleaned version of OpenSubtitles (Tiedemann, 2009) to predict the next utterance given the previous one. Evaluation is performed on 100 prompts from the Cornell Movie Dialog Corpus (DanescuNiculescu-Mizil and Lee, 2011). These prompts are a subset of the 1000 prompts used in Baheti et al. (2018), which were filtered using item response theory for discriminative power. We report perplexity (PpL), averaged over"
P19-1365,N19-1317,1,0.824394,"from each cluster. Note that in the case any clusters have size less than cb , we then include the highest-ranked candidates not found after clustering. 3 We follow Tam et al. (2019) and used averaged GloVe word embeddings (Pennington et al., 2014). 4 Clustering Post-Decoding (PDC) In the previous section, we discuss several diversity-promoting methods that can be applied during the decoding process. However, it is also possible to encourage additional diversity posthoc. On the task of sentence simplification, after decoding using a large-scale diversity-promoting beam search (beam size 100), Kriz et al. (2019) then clustered similar sentences together to further increase the variety of simplifications from which to choose. Document embeddings generated via Paragraph Vector (Le and Mikolov, 2014) were used as the sentence embeddings with which to perform K-means. In this work, we extend this post-decoding clustering idea in three key ways. First, we make use of sentence-level embeddings which leverage the pre-trained language representations from the Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018).4 Second, after clustering, Kriz et al. (2019) took the sentence c"
P19-1365,N16-1014,0,0.533946,"Krause et al. (2017) show how a set of diverse sentence-length image captions can be transformed into an entire paragraph about the image. Lastly, in applica3752 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3752–3762 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tions that involve reranking candidate sequences, the reranking algorithms are more effective when the input sequences are diverse. Reranking diverse candidates has been shown to improve results in both open dialog and machine translation (Li et al., 2016a; Li and Jurafsky, 2016; Gimpel et al., 2013). Furthermore, in open-ended dialog, the use of reranking to personalize a model’s responses for each user is a promising research direction (Choudhary et al., 2017). With these sorts of applications in mind, a variety of alternatives and extensions to beam search have been proposed which seek to produce a set of diverse candidate responses instead of a single high likelihood one (Li et al., 2016a; Vijayakumar et al., 2016; Kulikov et al., 2018; Tam et al., 2019). Many of these approaches show marked improvement in diversity over standard beam sear"
P19-1365,W04-1013,0,0.0446228,"trait that they are agnostic to model architecture and to the data type of the input, as long as the output of the model is a probability distribution over tokens in a sequence. Automatic Quality Evaluation An important part of this work is how to accurately measure not only the effect these methods have on candidate diversity, but also on the overall quality of the candidates. In choosing to report human scores and perplexity for the dialog domain, and SPICE for image captioning, we omitted some quality measures used in other papers. For image captioning, BLEU (Papineni et al., 2001), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), and CIDer (Vedantam et al., 2015) scores are often reported, but SPICE has been shown to have higher correlation with human judgments (Anderson et al., 2016). In the dialog domain, single-reference BLEU score (Papineni et al., 2001) is sometimes used to measure response quality, but it has been shown to have little correlation with human-judged quality (Liu et al., 2016). Therefore, most works in dialog systems use human evaluation as the ultimate measure of quality (Li et al., 2016a; Sedoc et al., 2018) 8 Conclusion In this work, we perform an analysis of"
P19-1365,D16-1230,0,0.0642252,"Missing"
P19-1365,D15-1166,0,0.0167036,"gies for generating diverse outputs from conditional language models. We also show how diversity can be improved without sacrificing quality by oversampling additional candidates, then filtering to the desired number. 1 Figure 1: An image with the top five captions from standard beam search and from random sampling. Note the latter set is more diverse but lower quality. Introduction Conditional neural language models, which train a neural net to map from one sequence to another, have had enormous success in natural language processing tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), text summarization (Nallapati et al., 2016), and dialog systems (Vinyals and Le, 2015). These models output a probability distribution over the next token in the output sequence given the input and the previously predicted tokens. Since computing the overall most likely output sequence is intractable, early work in neural machine translation found that beam search is an effective strategy to heuristically sample sufficiently likely sequences from these probabilistic models (Sutskever et al., 2014). However, for more open-ended tasks, beam search is ill-suited to generating a set of diverse c"
P19-1365,2001.mtsummit-papers.68,0,0.0116339,"rategies we evaluate share the trait that they are agnostic to model architecture and to the data type of the input, as long as the output of the model is a probability distribution over tokens in a sequence. Automatic Quality Evaluation An important part of this work is how to accurately measure not only the effect these methods have on candidate diversity, but also on the overall quality of the candidates. In choosing to report human scores and perplexity for the dialog domain, and SPICE for image captioning, we omitted some quality measures used in other papers. For image captioning, BLEU (Papineni et al., 2001), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), and CIDer (Vedantam et al., 2015) scores are often reported, but SPICE has been shown to have higher correlation with human judgments (Anderson et al., 2016). In the dialog domain, single-reference BLEU score (Papineni et al., 2001) is sometimes used to measure response quality, but it has been shown to have little correlation with human-judged quality (Liu et al., 2016). Therefore, most works in dialog systems use human evaluation as the ultimate measure of quality (Li et al., 2016a; Sedoc et al., 2018) 8 Conclusion In this work, we perf"
P19-1365,D18-1428,0,0.0275428,"We also note that concurrent work has proposed nucleus sampling as an improvement to the sampling strategies discussed in this paper (Holtzman et al., 2019). Diversity Promotion During Training Several works have attempted to encourage diversity during training by replacing the standard loglikelihood loss with a diversity-promoting objective. Li et al. (2016a) introduces an objective that maximizes mutual information between the source and target. Zhang et al. (2018) uses an adversarial information maximization approach to encourage generated text to be simultaneously informative and diverse. Xu et al. (2018) also uses an adversarial loss; their loss function rewards fluent text and penalizes repetitive text. We do not evaluate on these methods as they tend to be taskspecific and difficult to implement. All of the diversity strategies we evaluate share the trait that they are agnostic to model architecture and to the data type of the input, as long as the output of the model is a probability distribution over tokens in a sequence. Automatic Quality Evaluation An important part of this work is how to accurately measure not only the effect these methods have on candidate diversity, but also on the o"
P19-1365,D14-1162,0,0.0839454,"et al. (2019) proposed a clustering-based beam search method to help condense and remove meaningless responses from chatbots. Specifically, at each decoding step t, this method initially considers the top 2∗b candidates. From there, each candidate sequence is embedded3 , and the embeddings are clustered into c clusters using K-means. Finally, we take the top cb candidates from each cluster. Note that in the case any clusters have size less than cb , we then include the highest-ranked candidates not found after clustering. 3 We follow Tam et al. (2019) and used averaged GloVe word embeddings (Pennington et al., 2014). 4 Clustering Post-Decoding (PDC) In the previous section, we discuss several diversity-promoting methods that can be applied during the decoding process. However, it is also possible to encourage additional diversity posthoc. On the task of sentence simplification, after decoding using a large-scale diversity-promoting beam search (beam size 100), Kriz et al. (2019) then clustered similar sentences together to further increase the variety of simplifications from which to choose. Document embeddings generated via Paragraph Vector (Le and Mikolov, 2014) were used as the sentence embeddings wit"
P19-1365,W18-6709,1,0.822555,"rs. For image captioning, BLEU (Papineni et al., 2001), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), and CIDer (Vedantam et al., 2015) scores are often reported, but SPICE has been shown to have higher correlation with human judgments (Anderson et al., 2016). In the dialog domain, single-reference BLEU score (Papineni et al., 2001) is sometimes used to measure response quality, but it has been shown to have little correlation with human-judged quality (Liu et al., 2016). Therefore, most works in dialog systems use human evaluation as the ultimate measure of quality (Li et al., 2016a; Sedoc et al., 2018) 8 Conclusion In this work, we perform an analysis of posttraining decoding strategies that attempt to promote diversity in conditional language models. We show how over-sampling outputs then filtering down to the desired number is an easy way to increase diversity. Due to the computational expense of running large beam searches, we recommend using random-sampling to over-sample. The relative effectiveness of the various decoding strategies differs for the two tasks we considered, which suggests that choice of optimal diverse decoding strategy is both task-specific and dependent on one’s toler"
P19-1365,P02-1040,0,\N,Missing
P19-3022,D14-1083,0,0.0542943,"Missing"
P19-3022,P17-2032,0,0.0288719,"y similar technologies. For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and return"
P19-3022,C18-1176,0,0.229818,"Missing"
P19-3022,N19-4014,0,0.0408059,"Missing"
P19-3022,N18-5005,0,0.202546,"es. For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments"
P19-3022,D18-1402,0,0.040554,"es. For instance, bing.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments"
P19-3022,N18-1074,0,0.0357591,"Missing"
P19-3022,N19-1053,1,0.928078,"sity of the perspectives presented in them or whether they are supported by evidence. While it might be impractical to show an exhaustive spectrum of views with respect to a claim, cherry-picking a small but diverse set of perspectives could be a tangible step towards addressing the limitations of the current systems. Inherently this objective requires the understanding of the relations between each perspective and claim, as well as the nuance in semantic meaning between perspectives under the context of the claim. This work presents a demo for the task of substantiated perspective discovery (Chen et al., 2019). Our system receives a claim and it is expected to present a diverse set of wellcorroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. A typical output of the system is shown in Figure 3. The input to the system is a claim: Social media (like facebook or twitter) have had very positive effects in our life style. There is no single, best way to respond to the claim, but rather there are many valid responses that form a spectrum of perspectives, each with a stance relati"
P19-3022,W17-5106,0,0.163366,"provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments retrieved from the web. This work takes the effort one step further by employing language understanding techniques. There is a rich line of work on using Wikipedia as source for argument mining or to assess the veracity of a claim (Thorne et al., 2018). For instance, FAKTA is a system that extracts relevant documents from Wikipedia, among other sources, to predict the factuality of an input claim (Nadeem et al., 2019"
P19-3022,N19-1423,0,0.00809579,"o ensure both quality and efficiency. The retrieval systems extract candidates (perspectives or evidence paragraphs) which are later evaluated by carefully designed classifiers. C4: Extraction of Supporting Evidence. This classifier decides whether a given document lends enough evidence for a given perspective to a claim. In training the classifiers for each of the tasks, 130 Figure 2: Overview of the system structure: given a query to the system, it extracts candidates from its internal knowledge imal set of perspectives with the DBSCAN clustering algorithm (Ester et al., 1996). we use BERT (Devlin et al., 2019) and we follow the same steps described in Chen et al. (2019). 2.3 Candidate Retrieval Algorithm 1: Minimal Perspective Extraction Input: claim c. Output: perspectives, their stances & evidence. Pˆ ←IR(c) // candidate perspectives P = {} foreach p ∈ Pˆ do // perspective relevance if C1(c, p) &gt; T 1 and abs(C2(c, p)) &gt; T 2 then e ← C2(c, p) ˆ ←IR(c, p) // candidate evidence E E = {} ˆ do foreach e ∈ E // evidence verification if C4(c, p, e) &gt; T 4 then E ← E ∪ {e}. end end P ← P ∪ {(p, s, E)}. end end P ← /* minimal perspectives after clustering with DBSCAN on the equivalence scores between any p"
P19-3022,P18-1023,0,0.025386,"ng.com has recently started a service that provides two different responses to a given argument (screenshot in Figure 4). Since there is no published work on this system, it is not clear what the underlying mechanism is. 3 There exist a number of online debate platforms that provide similar functionalities as our system: kialo.com, procon.org, idebate.org , among others. Such websites usually provide a wide range of debate topics and various arguments in response to each topic. These resources have been proven useful in a line of works in argumentation (Hua and Wang, 2017; Stab et al., 2018b; Wachsmuth et al., 2018), among many others. While they provide rich sources of information, their content is fairly limited in terms of either their topical coverage or data availability for academic research purposes. Related Work There are few related tools to this work. args.me is a platform that accepts natural language queries and returns links to the pages that contain relevant topics (Wachsmuth et al., 2017), which are split into supporting & opposing categories (screenshot in Figure 4). Similarly, ArgumentText (Stab et al., 2018a) takes a topic as input and returns pro/con arguments retrieved from the web. T"
Q13-1014,P05-1074,1,0.728369,"Missing"
Q13-1014,W08-0208,0,0.0265935,", and supervised learning, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a na¨ıve solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1 http://alopez.github.io/dreamt 166 gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align |grade Students could then run experiments within minutes of beginning the assig"
Q13-1014,W11-2101,0,0.0267628,"Missing"
Q13-1014,J93-2003,0,0.0664021,"nload/hansard/ 9 This invited the possibility of cheating, since alignments of the test data are publicly available on the web. We did not advertise this, but as an added safeguard we obfuscated the data by distributing the test sentences randomly throughout the file. 20 AER × 100 Listing 1 The default aligner in DREAMT: thresholding Dice’s coefficient. for (f, e) in bitext: for f_i in set(f): f_count[f_i] += 1 for e_j in set(e): fe_count[(f_i,e_j)] += 1 for e_j in set(e): e_count[e_j] += 1 30 40 50 60 due ays ays ays -2 d -4 d -6 d ays days -8 d 168 days We privately implemented IBM Model 1 (Brown et al., 1993) as the target algorithm for a passing grade. We ran it for five iterations with English as the target language and French as the source. Our implementation did not use null alignment or symmetrization—leaving out these common improvements offered students the possibility of discovering them independently, and thereby rewarded. -10 > align -n 1000 |grade By varying the number of input sentences and the threshold for an alignment, students could immediately see the effect of various parameters on alignment quality. days The default implementation enabled immediate experimentation. On receipt of"
Q13-1014,W09-0401,1,0.880881,"Missing"
Q13-1014,W11-2103,1,0.932739,"between the human ranking and an output ranking. The check program simply ensures that a submission contains a valid ranking. We were concerned about hill-climbing on the test data, so we modified the leaderboard to report new results only twice a day. This encouraged students to experiment on the development data before posting new submissions, while still providing intermittent feedback. We privately implemented a version of BLEU, which obtained a correlation of 38.6 with the human rankings, a modest improvement over the baseline of 34.0. Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphr"
Q13-1014,W12-3102,1,0.884682,"Missing"
Q13-1014,D11-1003,0,0.016178,"4). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. 11 We implemented a version of the Lagrangian relaxation algorithm of Chang and Collins (2011), but found it difficult to obtain tight (optimal) solutions without iteratively reintroducing all of the original constraints. We suspect this is due to the lack of a distortion penalty, which enforces a strong preference towards translations with little reordering. However, the solution found by this algorithm is only approximates the objective implied by Equation 2, which sums over alignments. 170 Many teams who implemented the standard stack decoding algorithm experimented heavily with its pruning parameters. The best submission used extremely wide beam settings in conjunction with a reimp"
Q13-1014,W00-0601,0,0.0575064,"5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and 174 validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alig"
Q13-1014,J07-2003,0,0.0183732,"improvement, and thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2 http://python.org Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4 Thanks to an anonymous reviewer for this turn of phrase. 3 This scheme provided strong incentive to continue experimentation beyond the target algorithm.5 For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the compe"
Q13-1014,P08-2007,0,0.0256888,"lement consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 p(e, a|f ) = Y hi,i0 ,j,j 0 i∈a 0 0 p(fii |ejj ) J+1 Y j=1 p(ej |ej−1 , ej−2 ) (1) To evaluate output, we compute the conditional probability of e as follows. p(e|f ) = X p(e, a|f ) (2) a Note that this formulation is different from the typical Viterbi objective of standard beam search decoders, which do not sum over all alignments, but approximate p(e|f ) by maxa p(e, a|f ). Though the computation in Equation 2 is intractable (DeNero and Klein, 2008), it can be computed in a few minutes via dynamic programming on reasonably short sentences. We ensured that our data met this criterion. The corpus-level probability is then the product of all sentence-level probabilities in the data. The model includes no distortion limit or distortion model, for two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it ac"
Q13-1014,P10-4002,1,0.928176,"now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each imple"
Q13-1014,W08-0212,0,0.0278916,"ting final grades. Each student rated aspects of the course on a five point Likert scale, from 1 (strongly disagree) to 5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, includi"
Q13-1014,J07-3002,0,0.0344561,"Missing"
Q13-1014,W12-3134,1,0.927189,"re excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each implementation consists of a na¨ıve baseline"
Q13-1014,P01-1030,0,0.330619,"Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding al"
Q13-1014,P07-1019,0,0.0264172,"nd thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2 http://python.org Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4 Thanks to an anonymous reviewer for this turn of phrase. 3 This scheme provided strong incentive to continue experimentation beyond the target algorithm.5 For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the competitive effort.6 This stra"
Q13-1014,D07-1031,0,0.0154999,"saw many other solutions, indicating that many truly experimented with the problem: • Implementing heuristic constraints to require alignment of proper names and punctuation. • Running the algorithm on stems rather than surface words. • Initializing the first iteration of Model 1 with parameters estimated on the observed alignments in the development data. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an"
Q13-1014,N06-1058,0,0.0115716,"performs the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge The fourth challenge was reran"
Q13-1014,W05-0104,0,0.0345053,"eria. Everyone who completed all four assignments placed in the top five at least once. 6 The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. 167 Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data"
Q13-1014,J99-4005,0,0.395409,"lass on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key te"
Q13-1014,N03-1017,0,0.0330225,"to note that AER does not necessarily correlate with downstream performance, particularly on the Hansards dataset (Fraser and Marcu, 2007). We used the conclusion of the assignment as an opportunity to emphasize this point. 4 The Decoding Challenge The second challenge was decoding: given a fixed translation model and a set of input sentences, students were challenged to produce translations with the highest model score. This challenge introduced the difficulties of combinatorial optimization under a deceptively simple setup: the model we provided was a simple phrase-based translation model (Koehn et al., 2003) consisting only of a phrase table and tri169 gram language model. Under this simple model, for a French sentence f of length I, English sentence e of length J, and alignment a where each element consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 p(e, a|f ) = Y hi,i0 ,j,j 0 i∈a 0 0 p(fii |ejj ) J+1 Y j=1 p(ej |ej−1 , ej−2 ) (1) To evaluate output, we compute the conditional probability of e as follows. p(e|f ) = X p(e, a|f ) (2) a Note that this formulation is different from the typica"
Q13-1014,P07-2045,1,0.0187571,"rsities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation,"
Q13-1014,koen-2004-pharaoh,0,0.0336528,"ntained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with the pseudocode in Koehn’s (2010) popular textbook (reproduced here as Algorithm 1). The second program, grade, computes the log-probability of a set of translations, as outline above. We privately implemented a simple stack decoder that searched over permutations of phrases, similar to Koehn (2004). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. 11 We implemented a version of the Lagrangian relaxation algorithm of Ch"
Q13-1014,J10-4005,0,0.199806,"remaining authors were students in the worked described here. This research was conducted while Chris Callison-Burch was at Johns Hopkins University. in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source cod"
Q13-1014,2012.iwslt-papers.5,0,0.0214715,"Missing"
Q13-1014,N04-1022,0,0.0453486,"rank order of the underlying translation system. Students discovered that simply returning the first can173 didate earned a very high score, and most of them quickly converged to this solution. Unfortunately, the high accuracy of this baseline left little room for additional competition. Nevertheless, we were encouraged that most students discovered this by accident while attempting other strategies to rerank the translations. • Experimentation with parameters of the PRO algorithm. • Substitution of alternative learning algorithms. • Implementation of a simplified minimum Bayes risk reranker (Kumar and Byrne, 2004). Over a baseline of 24.02, the latter approach obtained a BLEU of 27.08, nearly matching the score of 27.39 from the underlying system despite an impoverished feature set. 7 Pedagogical Outcomes Could our students have obtained similar results by running standard toolkits? Undoubtedly. However, our goal was for students to learn by doing: they obtained these results by implementing key MT algorithms, observing their behavior on real data, and improving them. This left them with much more insight into how MT systems actually work, and in this sense, DREAMT was a success. At the end of class, w"
Q13-1014,2007.tmi-papers.13,0,0.0443585,"serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with"
Q13-1014,N06-1014,0,0.0123873,"nt not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with t"
Q13-1014,C04-1072,0,0.0405183,"Missing"
Q13-1014,J10-3002,0,0.0284528,"Missing"
Q13-1014,D07-1104,1,0.803017,"t know the true solution.11 We also posted an oracle containing the most probable output for each sentence, selected from among all submissions received so far. The intent of this oracle was to provide a lower bound on the best possible output, giving students additional incentive to continue improving their systems. 4.1 Data 4.3 We chose 48 French sentences totaling 716 words from the Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the l"
Q13-1014,W12-3101,1,0.89068,"Missing"
Q13-1014,W08-0209,0,0.0250327,"ng, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a na¨ıve solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1 http://alopez.github.io/dreamt 166 gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align |grade Students could then run experiments within minutes of beginning the assignment. Three of the four cha"
Q13-1014,J00-2004,0,0.0633455,"Missing"
Q13-1014,W03-0301,0,0.0297199,"a We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with the data. The first, align, computes Dice’s coefficient (1945) for every pair of French and English words, then aligns every pair for which its value is above an adjustable threshold. Our implementation (most of 7 Among them, Jordan Boyd-Graber, John DeNero, Philipp Koehn, and Slav Petrov (personal communication). 8 http://www.is"
Q13-1014,P04-1066,0,0.0562929,"Missing"
Q13-1014,P00-1056,0,0.0804879,"signment would earn an A; and top three placement compensated for weaker grades in other course criteria. Everyone who completed all four assignments placed in the top five at least once. 6 The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. 167 Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enoug"
Q13-1014,J03-1002,0,0.00871586,"Missing"
Q13-1014,N04-1021,0,0.039354,"as a simple program that produced a vector of feature weights using pairwise ranking optimization (PRO; Hopkins and May, 2011), with a perceptron as the underlying learning algorithm. A second, rerank, takes a weight vector as input and reranks the sentences; both programs were designed to work with arbitrary numbers of features. The grade program computed the BLEU score on development data, while check ensured that a test submission is valid. Finally, we provided an oracle program, which computed a lower bound on the achievable BLEU score on the development data using a greedy approximation (Och et al., 2004). The leaderboard likewise displayed an oracle on test data. We did not assign a target algorithm, but left the assignment fully open-ended. 6.3 Reranking Challenge Outcome For each assignment, we made an effort to create room for competition above the target algorithm. However, we did not accomplish this in the reranking challenge: we had removed most of the features from the candidate translations, in hopes that students might reinvent some of them, but we left one highly predictive implicit feature in the data: the rank order of the underlying translation system. Students discovered that si"
Q13-1014,W06-3112,0,0.013171,"Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge T"
Q13-1014,P02-1040,0,0.0866501,"Missing"
Q13-1014,P09-1038,0,0.0198477,"or two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it actually makes the problem more difficult, since a simple distance-based distortion model prefers translations with fewer permutations; without it, the model may easily prefer any permutation of the target phrases, making even the Viterbi search problem exhibit its true NP-hardness (Knight, 1999a; Zaslavskiy et al., 2009). Since the goal was to find the translation with the highest probability, we did not provide a held-out test set; with access to both the input sentences and 10 For simplicity, this formula assumes that e is padded with two sentence-initial symbols and one sentence-final symbol, and ignores the probability of sentence segmentation, which we take to be uniform. the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the translation that maximizes the evaluation. Indeed, since the problem is int"
Q13-1014,W11-2105,0,\N,Missing
Q14-1007,W10-0710,0,0.0986571,"cedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 31 25 22 21 Malayalam Telugu Portuguese German Urdu Russian Gujarati Turkish Cebuano Swahili Thai Others 219 87 82 66 56 44 39 35 29 23 22 ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with ≤20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with ≥3 native languages. Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate (Ambati and Vogel, 2010; Bloodgood and CallisonBurch, 2010; Ambati, 2012). To contrast our research with previous work, the main contributions of this paper are: (1) a robust methodology for assessing the bilingual skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study. 3 Experimental Design The central task in this study was to investigate Mechanical Turk’s bilingual population. We accomplished this through self-reported surveys combined with a HIT to translate individual words for 100 languages. We eval"
Q14-1007,ambati-etal-2010-active,0,0.134405,"ual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100"
Q14-1007,P10-1088,1,0.849791,"Missing"
Q14-1007,W10-0701,1,0.73723,"Missing"
Q14-1007,W01-1409,0,0.50491,"ed from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a s"
Q14-1007,W11-2148,0,0.0270578,"ting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. 81 English Hindi Chinese Arabic French Tagalog Italian Hebrew Vietnamese Swedish Hungarian Lithuanian 689 149 86 74 63 54 43 38 34 26 23 21 Tamil Spanish Romanian Kannada Polish Marathi Bengali Dutch Macedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 31 25 22 21 Malayalam Telugu Portuguese German Urdu Russian Gujarati Turkish Cebuano Swahili Thai Others 219 87 82 66 56 44 39 35 29 23 22 ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown ar"
Q14-1007,W10-0717,1,0.775416,"ographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine tran"
Q14-1007,W10-0729,0,0.0260829,"Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al."
Q14-1007,D11-1143,0,0.0205911,"Missing"
Q14-1007,W10-0716,0,0.012553,"call (von Ahn, 2005; Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US wo"
Q14-1007,N10-1024,1,0.220696,"Missing"
Q14-1007,W12-3152,1,0.790157,"Some languages follow the pattern of having a smattering of assignments completed early, with the rate picking up later. Figure 6 gives the throughput of the full-sentence translation task for the six Indian languages. The fastest language was Malayalam, for which we collected half a million words of translations in just under a week. Table 4 gives the size of the data set that we created for each of these languages. Training SMT systems We trained statistical translation models from the parallel corpora that we created for the six Indian languages using the Joshua machine translation system (Post et al., 2012). Table 5 shows the translation performance when trained on the bitexts alone, and when incorporating the bilingual dictionaries created in our earlier HIT. The scores reflect the performance when tested on held out sentences from the training data. Adding the dic87 trained on bitexts alone 12.03 16.19 6.65 8.08 11.94 19.22 bitext + dictionaries 17.29 18.10 9.72 9.66 13.70 21.98 BLEU ∆ 5.26 1.91 3.07 1.58 1.76 2.76 Table 5: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four"
Q14-1007,W10-0721,0,0.0153205,"tation, where people can Ernkvist, 2011). When Amazon introduced MTurk, be treated as a function call (von Ahn, 2005; Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers"
Q14-1007,D08-1027,0,0.151761,"Missing"
Q14-1007,P11-1122,1,0.467909,"a, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in th"
Q14-1007,N12-1006,1,0.854548,", which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speak"
Q14-1007,N13-1069,0,0.0609685,"for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. 81 English Hindi Chinese Arabic French Tagalog Italian Hebrew Vietnamese Swedish Hungarian Lithuanian 689 149 86 74 63 54 43 38 34 26 23 21 Tamil Spanish Romanian Kannada Polish Marathi Bengali Dutch Macedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 3"
Q14-1007,W10-0707,0,\N,Missing
Q14-1034,S12-1051,0,0.0148988,"f Green Ryu The Clippers Candice Robert Woods Amber Reggie Miller filtered random 0.0 0.2 0.4 0.6 0.8 Percentage of Positive Judgements Figure 5: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. sio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on 971 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different topics. method is inspired by a typical problem in extractive summarization, that the salient sentences are likely redundant (paraphrases) and need to be removed in the output summaries. We employ the scoring method used in SumBasic (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007), a sim"
Q14-1034,J08-4004,0,0.0498405,"Missing"
Q14-1034,R13-1026,1,0.754437,"otation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the pair, specifying whether the two words have same or different POS tags and what the specific tags are. We use the Twitter Part-Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3” in tweets about basketball game, “RIP” in tweets about a celebrity’s death. We use G2 log-likelihoodratio statistic, which has been frequently used in NLP, as a measure of word associations (Dunning, 1993; Moore, 2004). The significant scores are computed for"
Q14-1034,P12-1056,0,0.0569989,"Missing"
Q14-1034,C04-1051,0,0.816209,"emantic similarity systems. We make this dataset available to the research community.2 2 Joint Word-Sentence Paraphrase Model We present a new latent variable model that jointly captures paraphrase relations between sentence pairs and word pairs. It is very different from previous approaches in that its primary design goal and motivation is targeted towards short, lexically diverse text on the social web. 2.1 At-least-one-anchor Assumption Much previous work on paraphrase identification has been developed and evaluated on a specific benchmark dataset, the Microsoft Research Paraphrase Corpus (Dolan et al., 2004), which is de2 The dataset and code are made available at: SemEval-2015 shared task http://alt.qcri.org/semeval2015/ task1/ and https://github.com/cocoxu/ twitterparaphrase/ Corpus News (Dolan and Brockett, 2005) Twitter (This Work) Examples ◦ Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier. ◦ With the scandal hanging over Stewart’s company, revenue in the first quarter of the year dropped 15 percent from the same period a year earlier. ◦ The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq."
Q14-1034,W10-0735,0,0.0293585,"Missing"
Q14-1034,I05-5002,1,0.535018,"Missing"
Q14-1034,P11-1020,1,0.373917,"2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking Mechanical Turk workers to caption the action in short video segments. Similarly, Burrows et al. (2012) asked crowdsourcing workers to rewrite selected excerpts from books. Ling et al. (2014) crowdsourced bilingual parallel text using Twitter as the source of data. In contrast, we design a simple crowdsourcing task requiring only binary judgements on sentences collected from Twitter. There are several advantages as compared to existing work: a) the corpus also covers a very diverse range of topics and linguistic expressions, especially colloq"
Q14-1034,W02-1001,0,0.012317,"-or function; that is, if there exists at least one j such that zj = 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ"
Q14-1034,P09-1053,0,0.877987,"ng to Twitter, trends are determined by an algorithm which 6 More information about Twitter’s APIs: https://dev. twitter.com/docs/api/1.1/overview 442 =4 turk =5 turk turk =3 =2 =1 turk turk turk =0 The resulting system M ULTI P-PE provides consistently better precision and recall over the LEXLATENT model, as shown on the right in Figure 3. The M ULTI P-PE system outperforms LEXLATENT significantly according to a paired ttest with ρ less than 0.05. Our proposed M UL TI P takes advantage of Twitter’s specific properties and provides complementary information to previous approaches. Previously, Das and Smith (2009) has also used a product of experts to combine a lexical and a syntax-based model together. expert=0 Figure 4: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are"
Q14-1034,H93-1035,0,0.218833,"Missing"
Q14-1034,W04-3208,0,0.0872468,"Missing"
Q14-1034,C04-1151,0,0.0139898,"Missing"
Q14-1034,P12-1091,0,0.20052,"al Linguistics. (a) (b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined system shows significa"
Q14-1034,P13-1024,0,0.0363338,"on of a strong baseline used by Das and Smith (2009). competitive performance on the MSR corpus. The second baseline is a state-of-the-art unsupervised method, Weighted Textual Matrix Factorization (WTMF),4 which is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The original model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). We enhance the model with 1.6 million sentences from Twitter as suggested by Guo et al. (2013). Ji and Eisenstein (2013) presented a state-ofthe-art ensemble system, which we call LEXDISCRIM.5 It directly combines both discriminativelytuned latent features and surface lexical features into a SVM classifier. Specifically, the latent representation of a pair of sentences v~1 and v~2 is converted into a feature vector, [v~1 + v~2 , |v~1 − v~2 |], by concatenating the element-wise sum v~1 + v~2 and absolute different |v~1 − v~2 |. We also introduce a new baseline, LEXLATENT, which is a simplified version of LEXDISCRIM and easy to reproduce. It uses the same method to combine latent feature"
Q14-1034,D12-1039,0,0.0269215,"r to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the pair, specifying whether the two words have same or different POS tags and what the specific tags are. We use the Twitter Part-Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3"
Q14-1034,P11-1055,0,0.709502,"= 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ ← 0 2: for i ← 1 to n do 3: extract all possible word pairs wi exp(θ · f (z"
Q14-1034,N06-2015,0,0.015456,"rent paraphrase identification approaches on Twitter data. *An enhanced version that uses additional 1.6 million sentences from Twitter. ** Reimplementation of a strong baseline used by Das and Smith (2009). competitive performance on the MSR corpus. The second baseline is a state-of-the-art unsupervised method, Weighted Textual Matrix Factorization (WTMF),4 which is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The original model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). We enhance the model with 1.6 million sentences from Twitter as suggested by Guo et al. (2013). Ji and Eisenstein (2013) presented a state-ofthe-art ensemble system, which we call LEXDISCRIM.5 It directly combines both discriminativelytuned latent features and surface lexical features into a SVM classifier. Specifically, the latent representation of a pair of sentences v~1 and v~2 is converted into a feature vector, [v~1 + v~2 , |v~1 − v~2 |], by concatenating the element-wise sum v~1 + v~2 and absolute different |v~1 − v~2 |. We also"
Q14-1034,D13-1090,1,0.286981,"(b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined system shows significant improvement over the s"
Q14-1034,P06-1096,0,0.0226525,"Missing"
Q14-1034,W14-3356,0,0.0220602,"at at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking Mechanical Turk workers to caption the action in short video segments. Similarly, Burrows et al. (2012) asked crowdsourcing workers to rewrite selected excerpts from books. Ling et al. (2014) crowdsourced bilingual parallel text using Twitter as the source of data. In contrast, we design a simple crowdsourcing task requiring only binary judgements on sentences collected from Twitter. There are several advantages as compared to existing work: a) the corpus also covers a very diverse range of topics and linguistic expressions, especially colloquial language, which is different from and thus complements previous paraphrase corpora; b) the paraphrase corpus collected contains a representative proportion of both negative and positive instances, while lack of good negative examples was"
Q14-1034,D08-1084,0,0.0999499,"set zτ∗ = 1 and zj∗ = arg maxx∈0,1 φ(x, wj ; θ) for all j 6= τ ; for a negative example, we set z∗i = 0. The time complexity of both inferences for one sentence pair is O(|W (s)|2 ), where |W (s)|2 is the number of word pairs. In practice, we use online learning instead of optimizing the full objective. The detailed learning algorithm is presented in Figure 2. Following Hoffmann et al. (2011), we use 50 iterations in the experiments. 2.4 Feature Design 3 At the word-level, our discriminative model allows use of arbitrary features that are similar to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tag"
Q14-1034,J10-3003,0,0.0844497,"l−m e arms that have the highest estimated reward until reaching the maximum l = 10 annotations for any topic to insure data diversity. We tune the parameters m to be 1 and  to be between 0.35 ∼ 0.55 through simulation experiments, by artificially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by 444 Related Work Automatic Paraphrase Identification has been widely studied (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of various techniques.8 Many recent high-performance approaches use system combination (Das and Smith, 2009; Madnani et al., 2012; Ji and Eisenstein, 2013). For example, Madnani et al. (2012) combines multiple sophisticated machine translation metrics using a metaclassifier. An earlier attempt on Twitter data is that of Xu et al. (2013b). They limited the search space to only the tweets that explicitly mention a same date and a same named entity, however there remain a considerable amount of mislabels in their data.9 Zanzotto et al. (2011) also experim"
Q14-1034,N12-1019,0,0.30475,"to be between 0.35 ∼ 0.55 through simulation experiments, by artificially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by 444 Related Work Automatic Paraphrase Identification has been widely studied (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of various techniques.8 Many recent high-performance approaches use system combination (Das and Smith, 2009; Madnani et al., 2012; Ji and Eisenstein, 2013). For example, Madnani et al. (2012) combines multiple sophisticated machine translation metrics using a metaclassifier. An earlier attempt on Twitter data is that of Xu et al. (2013b). They limited the search space to only the tweets that explicitly mention a same date and a same named entity, however there remain a considerable amount of mislabels in their data.9 Zanzotto et al. (2011) also experimented with SVM tree kernel methods on Twitter data. Departing from the previous work, we propose a latent variable model to jointly infer the correspondence between words"
Q14-1034,W04-3243,0,0.0161665,"Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3” in tweets about basketball game, “RIP” in tweets about a celebrity’s death. We use G2 log-likelihoodratio statistic, which has been frequently used in NLP, as a measure of word associations (Dunning, 1993; Moore, 2004). The significant scores are computed for each trend on an average of about 1500 sentences and converted to binary features for every word pair, indicating whether the two words are both significant or not. Our topical features are novel and were not used in previous work. Following Riedel et al. (2010) and Hoffmann et al. (2011), we also incorporate conjunction features into our system for better accuracy, namely Word+POS, Word+Topical and Word+POS+Topical features. 3 https://github.com/knowitall/morpha 439 Experiments Data It is nontrivial to gather a gold-standard dataset of naturally occur"
Q14-1034,N12-1034,0,0.334376,"Missing"
Q14-1034,Q13-1030,1,0.648609,"ata is released by Xu et al. (2013b) at: https:// github.com/cocoxu/twitterparaphrase/ 2011; Yao et al., 2013a,b), but different in that the paraphrase task requires additional sentence alignment modeling with no word alignment data. Our approach is also inspired by Fung and Cheung’s (2004a; 2004b) work on bootstrapping bilingual parallel sentence and word translations from comparable corpora. Multiple Instance Learning (Dietterich et al., 1997) has been used by different research groups in the field of information extraction (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking"
Q14-1034,I05-5011,0,0.0125198,"and Yangfeng Ji4 1 University of Pennsylvania, Philadelphia, PA, USA {xwe, ccb}@cis.upenn.edu 2 The Ohio State University, Columbus, OH, USA ritter.1492@osu.edu 3 Microsoft Research, Redmond, WA, USA billdol@microsoft.com 4 Georgia Institute of Technology, Atlanta, GA, USA jiyfeng@gatech.edu Abstract (e.g. oscar nom’d doc ↔ Oscar-nominated documentary). In this paper, we investigate the task of determining whether two tweets are paraphrases. Previous work has exploited a pair of shared named entities to locate semantically equivalent patterns from related news articles (Shinyama et al., 2002; Sekine, 2005; Zhang and Weld, 2013). But short sentences in Twitter do not often mention two named entities (Ritter et al., 2012) and require nontrivial generalization from named entities to other words. For example, consider the following two sentences about basketball player Brook Lopez from Twitter: We present M ULTI P (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent vari"
Q14-1034,D12-1042,0,0.031759,"te_of_ the_art) 9 The data is released by Xu et al. (2013b) at: https:// github.com/cocoxu/twitterparaphrase/ 2011; Yao et al., 2013a,b), but different in that the paraphrase task requires additional sentence alignment modeling with no word alignment data. Our approach is also inspired by Fung and Cheung’s (2004a; 2004b) work on bootstrapping bilingual parallel sentence and word translations from comparable corpora. Multiple Instance Learning (Dietterich et al., 1997) has been used by different research groups in the field of information extraction (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraph"
Q14-1034,P11-2044,0,0.0133162,"maxx∈0,1 φ(x, wj ; θ) for all j 6= τ ; for a negative example, we set z∗i = 0. The time complexity of both inferences for one sentence pair is O(|W (s)|2 ), where |W (s)|2 is the number of word pairs. In practice, we use online learning instead of optimizing the full objective. The detailed learning algorithm is presented in Figure 2. Following Hoffmann et al. (2011), we use 50 iterations in the experiments. 2.4 Feature Design 3 At the word-level, our discriminative model allows use of arbitrary features that are similar to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the p"
Q14-1034,U06-1019,0,0.813586,"on for Computational Linguistics. (a) (b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined sy"
Q14-1034,D13-1008,0,0.0679257,"a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one word pair (we call it"
Q14-1034,P13-2117,1,0.909424,"us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one"
Q14-1034,W13-2515,1,0.806798,"us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one"
Q14-1034,P13-2123,1,0.512784,"Missing"
Q14-1034,D13-1056,1,0.747986,"Missing"
Q14-1034,D11-1061,0,0.281688,"3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one word pair (we call it an anchor and highlight with underscores; the words in the anchor pair need not be identical) that is indicative of sentential paraphra"
Q14-1034,D07-1071,0,0.0109552,"sts at least one j such that zj = 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ ← 0 2: for i ← 1 to n do 3: extract all possible wo"
Q14-1034,D13-1183,0,0.0609342,"i4 1 University of Pennsylvania, Philadelphia, PA, USA {xwe, ccb}@cis.upenn.edu 2 The Ohio State University, Columbus, OH, USA ritter.1492@osu.edu 3 Microsoft Research, Redmond, WA, USA billdol@microsoft.com 4 Georgia Institute of Technology, Atlanta, GA, USA jiyfeng@gatech.edu Abstract (e.g. oscar nom’d doc ↔ Oscar-nominated documentary). In this paper, we investigate the task of determining whether two tweets are paraphrases. Previous work has exploited a pair of shared named entities to locate semantically equivalent patterns from related news articles (Shinyama et al., 2002; Sekine, 2005; Zhang and Weld, 2013). But short sentences in Twitter do not often mention two named entities (Ritter et al., 2012) and require nontrivial generalization from named entities to other words. For example, consider the following two sentences about basketball player Brook Lopez from Twitter: We present M ULTI P (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we ac"
Q14-1034,N03-1031,0,\N,Missing
Q14-1034,W10-0711,0,\N,Missing
Q14-1034,J13-3001,0,\N,Missing
Q15-1021,C14-1188,0,0.0964198,"rasing. Although the inadequacy of text simplification evaluations has been discussed before (Siddharthan, 2014), we focus on these two common deficiencies and suggest two future directions. 4.1 Targeting specific audiences Simplification has many subtleties, since what constitutes simplification for one type of user may not be appropriate for another. Many researchers have studied simplification in the context of different audiences. However, most recent automatic simplification systems are developed and evaluated with little consideration of target reader population. There is one attempt by Angrosh et al. (2014) who evaluate their system by asking non-native speakers comprehension questions. They conducted an English vocabulary size test to categorize the users into different levels of language skills. The Newsela corpus allows us to target children at different grade levels. From the application point of view, making knowledge accessible to all children is an important yet challenging part of education (Scarton et al., 2010; Moraes et al., 2014). From the technical point of view, reading grade level is a clearly defined objective for both simplification systems and human annotators. Once there is a"
Q15-1021,I11-1053,0,0.017708,"s in the Parallel Wikipedia Simplification (PWKP) corpus. ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset. It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research. We are not the only researchers to notice problems with Simple Wikipedia. There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments. Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refoc"
Q15-1021,J03-4003,0,0.0443548,"sional simplifications. Wikipedia has good coverage on certain words, such as “approximately”, because of its large volume. 3.4 Log-odds-ratio analysis of syntax patterns We can also reveal the syntax patterns that are most strongly associated with simple text versus complex text using the log-odds-ratio technique. Table 9 shows syntax patterns that represent “parent node (head word) → children node(s)&quot; structures from a constituency parse tree. To extract theses patterns we parsed our corpus with the Stanford Parser (Klein and Manning, 2002) and applied its built-in head word identifier from Collins (2003). Both the Newsela and Wikipedia corpora exhibit syntactic differences that are intuitive and interesting. However, as with word frequency (Table 8), 290 complex syntactic patterns are retained more often in Wikipedia’s simplifications than in Newsela’s. In order to show interesting syntax patterns in the Wikipedia parallel data for Table 9, we first had to discard 3613 sentences in PWKP that contain both &quot;is a commune&quot; and &quot;France&quot;. As the word-level analysis in Tables 6 and 7 hints, there is an exceeding number of sentences about communes in France in the PWKP corpus, such as the sentence pa"
Q15-1021,P11-2117,0,0.0386432,"P) corpus. ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset. It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research. We are not the only researchers to notice problems with Simple Wikipedia. There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments. Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in the future (Eisenstein, 2013"
Q15-1021,J08-1001,0,0.0123409,"te sentence from a stub geographic article and its deterministic simplification. The influence of this template sentence is more overwhelming in the syntax-level analysis than in the word-level analysis —- about 1/3 of the top 30 syntax patterns would be related to these sentence pairs if they were not discarded. tions. However, previous research that uses SimpleNormal Wikipedia largely focuses on sentence-level transformation, without taking large discourse structure into account. 3.5 Document-level compression There are few publicly accessible document-level parallel simplification corpora (Barzilay and Lapata, 2008). The Newsela corpus will enable more research on document-level simplification, such as anaphora choice (Siddharthan and Copestake, 2002), content selection (Woodsend and Lapata, 2011b), and discourse relation preservation (Siddharthan, 2003). Simple Wikipedia is rarely used to study document-level simplification. Woodsend and Lapata (2011b) developed a model that simplifies Wikipedia articles while selecting their most important content. However, they could only use Simple Wikipedia in very limited ways. They noted that Simple Wikipedia is “less mature” with many articles that are just “stub"
Q15-1021,N13-1037,0,0.0273424,"nd Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in the future (Eisenstein, 2013). We hope to 284 inspire the creation of higher quality simplification datasets, and to encourage researchers to think critically about existing resources and evaluation methods. We believe this will lead to breakthroughs in text simplification research. 2 Simple Wikipedia is not that simple The Parallel Wikipedia Simplification (PWKP) corpus (Zhu et al., 2010) contains approximately 108,000 automatically aligned sentence pairs from cross-linked articles between Simple and Normal English Wikipedia. It has become a benchmark dataset for simplification largely because of its size and availabilit"
Q15-1021,C08-1013,1,0.833475,"Missing"
Q15-1021,W07-1007,0,0.085537,"Missing"
Q15-1021,E99-1042,0,0.429301,"Missing"
Q15-1021,C96-2183,0,0.887897,"Missing"
Q15-1021,P11-1020,0,0.0231991,"r output tends to result in lower adequacy judgements (Napoles et al., 2011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introduced a new, high-quality corpus of professionally simplified news articles, Newsela, as an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequac"
Q15-1021,C12-1034,0,0.186369,"Missing"
Q15-1021,D11-1108,1,0.391062,"Missing"
Q15-1021,gerber-hovy-1998-improving,0,0.175719,"Missing"
Q15-1021,N15-1022,0,0.103388,"ntence is still complex. The main causes of non-simplifications and partial-simplifications in the parallel Wikipedia corpus include: 1) The Simple Wikipedia was created by volunteer contributors with no specific objective; 2) Very rarely are the simple articles complete re-writes of the regular articles in Wikipedia (Coster and Kauchak, 2011), which makes automatic sentence alignment errors worse; 3) As an encyclopedia, Wikipedia contains many difficult sentences with complex terminology. The difficulty of sentence alignment between Normal-Simple Wikipedia is highlighted by a recent study by Hwang et al. (2015) that achieves state-of-the-art performance of 0.712 maximum F1 score (over the precisionrecall curve) by combining Wiktionary-based and dependency-parse-based sentence similarities. And in fact, even the simple side of the PWKP corpus contains an extensive English vocabulary of 78,009 unique words. 6,669 of these words do not exist in the normal side (Table 2). Below is a sentence from an article entitled “Photolithography&quot; in Simple Wikipedia: Microphototolithography is the use of photolithography to transfer geometric shapes on a photomask to the surface of a semiconductor wafer for making"
Q15-1021,W03-1602,0,0.0442639,"Missing"
Q15-1021,P13-1151,0,0.104475,"les while selecting their most important content. However, they could only use Simple Wikipedia in very limited ways. They noted that Simple Wikipedia is “less mature” with many articles that are just “stubs, comprising a single paragraph of just one or two sentences”. We quantify their observation in Figure 2, plotting the documentlevel compression ratio of Simple vs. Normal Wikipedia articles. The compression ratio is the ratio of the number of characters between each simple-complex article pair. In the plot, we use all 60 thousand article pairs from the Simple-Normal Wikipedia collected by Kauchak (2013) in May 2011. The overall compression ratio is skewed towards almost 0. For comparison, we also plot the ratio between the simplest version (Simp-4) and the original version (Original) of the news articles in the Newsela corpus. The Newsela corpus has a much more reasonable compression ratio and is therefore likely to be more suitable for studying documentlevel simplification. 3.6 Analysis of discourse connectives Although discourse is known to affect readability, the relation between discourse and text simplification is still under-studied with the use of statistical methods (Williams et al.,"
Q15-1021,Q13-1028,0,0.0106967,"human annotators. Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems. Newsela provides human simplification at different grade levels and reading comprehension quizzes alongside each article. In addition, readability is widely studied and can be automatically estimated (Kincaid et al., 1975; Pitler and Nenkova, 2008; Petersen and Ostendorf, 2009). Although existing readability metrics assume text is well-formed, they can potentially be used in combination with text quality metrics (Post, 2011; Louis and Nenkova, 2013) to evaluate simplifications. They can also be used to aid humans in the creation of reference simplifications. 4.2 Evaluating sub-tasks separately It is widely accepted that sentence simplification involves three different elements: splitting, deletion and paraphrasing (Feng, 2008; Narayan and Gardent, 2014). Splitting breaks a long sentence into a few short sentences to achieve better readability. Deletion reduces the complexity by removing unimportant parts of a sentence. Paraphrasing rewrites text into a simpler version via reordering, substitution and occasionally expansion. Most state-of"
Q15-1021,C10-1089,0,0.0180268,"Missing"
Q15-1021,W11-1611,1,0.810656,"Missing"
Q15-1021,P14-1041,0,0.675874,"s to think critically about existing resources and evaluation methods. We believe this will lead to breakthroughs in text simplification research. 2 Simple Wikipedia is not that simple The Parallel Wikipedia Simplification (PWKP) corpus (Zhu et al., 2010) contains approximately 108,000 automatically aligned sentence pairs from cross-linked articles between Simple and Normal English Wikipedia. It has become a benchmark dataset for simplification largely because of its size and availability, and because follow-up papers (Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014) often compare with Zhu et al.’s system outputs to demonstrate further improvements. The large quantity of parallel text from Wikipedia made it possible to build simplification systems using statistical machine translation (SMT) technology. But after the initial success of these firstgeneration systems, we started to suffer from the inadequacy of the parallel Wikipedia simplification datasets. There is scattered evidence in the literature. Bach et al. (2011) mentioned they have attempted to use parallel Wikipedia data, but opted to construc"
Q15-1021,D08-1020,0,0.0128282,"ortant yet challenging part of education (Scarton et al., 2010; Moraes et al., 2014). From the technical point of view, reading grade level is a clearly defined objective for both simplification systems and human annotators. Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems. Newsela provides human simplification at different grade levels and reading comprehension quizzes alongside each article. In addition, readability is widely studied and can be automatically estimated (Kincaid et al., 1975; Pitler and Nenkova, 2008; Petersen and Ostendorf, 2009). Although existing readability metrics assume text is well-formed, they can potentially be used in combination with text quality metrics (Post, 2011; Louis and Nenkova, 2013) to evaluate simplifications. They can also be used to aid humans in the creation of reference simplifications. 4.2 Evaluating sub-tasks separately It is widely accepted that sentence simplification involves three different elements: splitting, deletion and paraphrasing (Feng, 2008; Narayan and Gardent, 2014). Splitting breaks a long sentence into a few short sentences to achieve better read"
Q15-1021,P11-2038,0,0.0229599,"systems and human annotators. Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems. Newsela provides human simplification at different grade levels and reading comprehension quizzes alongside each article. In addition, readability is widely studied and can be automatically estimated (Kincaid et al., 1975; Pitler and Nenkova, 2008; Petersen and Ostendorf, 2009). Although existing readability metrics assume text is well-formed, they can potentially be used in combination with text quality metrics (Post, 2011; Louis and Nenkova, 2013) to evaluate simplifications. They can also be used to aid humans in the creation of reference simplifications. 4.2 Evaluating sub-tasks separately It is widely accepted that sentence simplification involves three different elements: splitting, deletion and paraphrasing (Feng, 2008; Narayan and Gardent, 2014). Splitting breaks a long sentence into a few short sentences to achieve better readability. Deletion reduces the complexity by removing unimportant parts of a sentence. Paraphrasing rewrites text into a simpler version via reordering, substitution and occasionall"
Q15-1021,N03-1026,0,0.0144108,"ria (grammaticality, simplicity and adequacy) do not explain which components in a system are good or bad. More importantly, deletion may be unfairly penalized since shorter output tends to result in lower adequacy judgements (Napoles et al., 2011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introd"
Q15-1021,N10-2011,0,0.0498743,"Missing"
Q15-1021,W03-2314,0,0.317258,"be related to these sentence pairs if they were not discarded. tions. However, previous research that uses SimpleNormal Wikipedia largely focuses on sentence-level transformation, without taking large discourse structure into account. 3.5 Document-level compression There are few publicly accessible document-level parallel simplification corpora (Barzilay and Lapata, 2008). The Newsela corpus will enable more research on document-level simplification, such as anaphora choice (Siddharthan and Copestake, 2002), content selection (Woodsend and Lapata, 2011b), and discourse relation preservation (Siddharthan, 2003). Simple Wikipedia is rarely used to study document-level simplification. Woodsend and Lapata (2011b) developed a model that simplifies Wikipedia articles while selecting their most important content. However, they could only use Simple Wikipedia in very limited ways. They noted that Simple Wikipedia is “less mature” with many articles that are just “stubs, comprising a single paragraph of just one or two sentences”. We quantify their observation in Figure 2, plotting the documentlevel compression ratio of Simple vs. Normal Wikipedia articles. The compression ratio is the ratio of the number o"
Q15-1021,E14-1076,0,0.0745086,"Missing"
Q15-1021,N10-1144,0,0.272647,"ntitative-comparative approach to study the quality of simplification data resources. 1 Introduction The goal of text simplification is to rewrite complex text into simpler language that is easier to understand. Research into this topic has many potential practical applications. For instance, it can provide reading aids for people with disabilities (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), low-literacy (Watanabe et al., 2009; De Belder and Moens, 2010), non-native backgrounds (Petersen and Ostendorf, 2007; Allen, 2009) or non-expert knowledge (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Text simplification may also help improve the performance of many natural language processing (NLP) tasks, such as parsing (Chandrasekar et al., 1996), summarization (Siddharthan et al., 2004; Klebanov et al., 2004; Vanderwende et al., 2007; Xu and Grishman, 2009), semantic role labeling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Gerber and Hovy, 1998; Chen et al., 2012), by transforming long, complex sentences into ones that are more easily processed. The Parallel Wikipedia Simplification (PWKP) corpus prepared by Zhu et al. (2010), has b"
Q15-1021,C04-1129,0,0.0857544,"Missing"
Q15-1021,P08-1040,0,0.445463,"Missing"
Q15-1021,E14-1021,1,0.818695,"011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introduced a new, high-quality corpus of professionally simplified news articles, Newsela, as an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequacies in comparison. We further discussed problems with current simplific"
Q15-1021,W03-2317,0,0.0777258,"y Kauchak (2013) in May 2011. The overall compression ratio is skewed towards almost 0. For comparison, we also plot the ratio between the simplest version (Simp-4) and the original version (Original) of the news articles in the Newsela corpus. The Newsela corpus has a much more reasonable compression ratio and is therefore likely to be more suitable for studying documentlevel simplification. 3.6 Analysis of discourse connectives Although discourse is known to affect readability, the relation between discourse and text simplification is still under-studied with the use of statistical methods (Williams et al., 2003; Siddharthan, 2006; Siddharthan and Katsos, 2010). Text simplification often involves splitting one sentence into multiple sentences, which is likely to require discourse-level changes such as introducing explicit rhetorical rela291 Figure 3: A radar chart that visualizes the odds ratio (radius axis) of discourse connectives in simple side vs. complex side. An odds ratio larger than 1 indicates the word is more likely to occur in the simplified text than in the complex text, and vice versa. Simple cue words (in the shaded region), except “hence”, are more likely to be added during Newsela’s s"
Q15-1021,D11-1038,0,0.222754,"ikipedia Simplification (PWKP) corpus. ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset. It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research. We are not the only researchers to notice problems with Simple Wikipedia. There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments. Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in t"
Q15-1021,P12-1107,0,0.336299,"Missing"
Q15-1021,W09-2809,1,0.844138,"Missing"
Q15-1021,C12-1177,1,0.0368874,"ents (Napoles et al., 2011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introduced a new, high-quality corpus of professionally simplified news articles, Newsela, as an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequacies in comparison. We further discussed prob"
Q15-1021,W13-2515,1,0.397575,"Missing"
Q15-1021,C10-1152,0,0.482887,"arthan and Katsos, 2010). Text simplification may also help improve the performance of many natural language processing (NLP) tasks, such as parsing (Chandrasekar et al., 1996), summarization (Siddharthan et al., 2004; Klebanov et al., 2004; Vanderwende et al., 2007; Xu and Grishman, 2009), semantic role labeling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Gerber and Hovy, 1998; Chen et al., 2012), by transforming long, complex sentences into ones that are more easily processed. The Parallel Wikipedia Simplification (PWKP) corpus prepared by Zhu et al. (2010), has become the benchmark dataset for training and evaluating automatic text simplification systems. An associated test set of 100 sentences from Wikipedia has been used for comparing the state-of-the-art approaches. The collection of simple-complex parallel sentences sparked a major advance for machine translationbased approaches to simplification. However, we will show that this dataset is deficient and should be considered obsolete. In this opinion paper, we argue that Wikipedia as a simplification data resource is suboptimal for several reasons: 1) It is prone to automatic sentence alignm"
Q15-1021,W08-1105,0,\N,Missing
Q15-1021,W14-4409,0,\N,Missing
Q16-1029,W14-1214,0,0.104671,"Missing"
Q16-1029,C14-1188,0,0.155152,"s dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent,"
Q16-1029,P05-1074,1,0.147414,"ber of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns."
Q16-1029,P11-2087,0,0.0097729,"r knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification q"
Q16-1029,E99-1042,0,0.71526,"development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and"
Q16-1029,W11-2504,1,0.904086,"Missing"
Q16-1029,C96-2183,0,0.904046,"ions Poperation and recalls Roperation : SARI = d1 Fadd + d2 Fkeep + d3 Pdel (7) where d1 = d2 = d3 = 1/3 and Poperation = Roperation 1 k 1 = k Foperation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal"
Q16-1029,P12-1098,0,0.134153,"text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important"
Q16-1029,P11-1020,0,0.0443074,"y word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one cruc"
Q16-1029,C12-1034,0,0.303799,"text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important"
Q16-1029,J07-2003,0,0.0340451,"patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns. The key differences between the paraphrase rules from PPDB and the transformations learned by the naive application of SMT to the Normal-Simple Wikipedia parallel corpus, are that the PPDB pa"
Q16-1029,P06-1048,0,0.042514,"), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they ha"
Q16-1029,W11-1601,0,0.212736,"includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing proble"
Q16-1029,P11-2117,0,0.0757764,"includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing proble"
Q16-1029,N12-1067,0,0.0258186,"ain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statis"
Q16-1029,W14-1215,0,0.106191,", which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; C"
Q16-1029,N15-1060,0,0.0389548,"data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objectiv"
Q16-1029,D15-1042,0,0.0371305,"accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it"
Q16-1029,W08-1105,0,0.0147252,"et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifyin"
Q16-1029,P15-2073,0,0.0139864,"be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives."
Q16-1029,W12-3134,1,0.938317,"ctions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which governs whether the system prefers fewer longer phrases or a greater number of shorter phrases), and a language model probability. Together these features are what the model uses to distinguish between good and bad translations. For monolingual translation tasks, previous research suggests that features like paraphrase probability and distributional similarity are potentially helpful in picking out good paraphrases (Chan et al., 2011) and for text-to-text generation (Ganitkevitch et al., 2012b). While these two features quantify how good a paraphrase rule is in general, they do not indicate how good the rule is for a specific task, like simplification. For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add 406 9 new features for simplification purposes:5 length in characters, length in words, number of syllables, language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word with the maximum number of syllables on each side and the difference between th"
Q16-1029,S12-1034,1,0.867442,"Missing"
Q16-1029,N13-1092,1,0.395478,"Missing"
Q16-1029,D11-1125,0,0.0478686,"on that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previous work on text simplification has only used BLEU for tuning, which is insufficient as we show empirically in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references. Unlike machine translation metrics which do not compare against the (foreign) input sentence, it is necessary to compare simplification system outputs against the inputs to as"
Q16-1029,P14-2075,0,0.0134708,"ttempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previou"
Q16-1029,N15-1022,0,0.0688657,"n or sentence splitting could be applied as pre- or post-processing steps. 2 Background Xu et al. (2015) laid out a series of problems that are present in current text simplification research, and argued that we should deviate from the previous state-of-the-art benchmarking setup. First, the Simple English Wikipedia data has dominated simplification research since 2010 (Zhu et al., 2010; Siddharthan, 2014), and is used together with Standard English Wikipedia to create parallel text to train MT-based simplification systems. However, recent studies (Xu et al., 2015; Amancio and ˇ Specia, 2014; Hwang et al., 2015; Stajner et al., 2015) showed that the parallel Wikipedia simplification corpus contains a large proportion of inadequate (not much simpler) or inaccurate (not aligned or only partially aligned) simplifications. It is one of the leading reasons that existing simplification systems struggle to generate simplifying paraphrases and leave the input sentences unchanged (Wubben 1 Our code and data are made available at: https:// github.com/cocoxu/simplification/ 402 et al., 2012). Previously researchers attempted some quick fixes by adding phrasal deletion rules (Coster and Kauchak, 2011a) or reran"
Q16-1029,W03-1602,0,0.0887002,"ation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is ac"
Q16-1029,P02-1028,0,0.0887695,"operation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that"
Q16-1029,P13-1151,0,0.30462,"c task, like simplification. For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add 406 9 new features for simplification purposes:5 length in characters, length in words, number of syllables, language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word with the maximum number of syllables on each side and the difference between the two sides, when it is applicable. We use language models built from the Gigaword corpus and the Simple Wikipedia corpus collected by Kauchak (2013). We also use a list of 3000 most common US English words compiled by Paul and Bernice Noll.6 3.4 Creating Multiple References Like with machine translation, where there are many equally good translations, in simplification there may be several ways of simplifying a sentence. Most previous work on text simplification only uses a single reference simplification, often from the Simple Wikipedia. This is undesirable since the Simple Wikipedia contains a large proportion of inadequate or inaccurate simplifications (Xu et al., 2015) . In this study, we collect multiple human reference simplificatio"
Q16-1029,W10-1754,0,0.0250509,"ression, in which compression of word and sentence lengths can be more straightforwardly implemented in features and the objective function in the SMT framework. We want to stress that sentence simplification is not a simple extension of sentence compression, but is a much more complicated task, primarily because high-quality data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a gi"
Q16-1029,C10-1089,0,0.0709952,"slation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion remo"
Q16-1029,P15-2097,1,0.8131,"because high-quality data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simpli"
Q16-1029,P14-1041,0,0.733939,"language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b;"
Q16-1029,P03-1021,0,0.0352014,"exical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previous work on text simplification has only used BLEU for tuning, which is insufficient as we show empirically in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references. Unlike machine translation metrics which do not compare against the (foreign) input sentence, it is necessary to compare simplification system o"
Q16-1029,P15-1146,1,0.25619,"ules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than"
Q16-1029,P16-2024,1,0.58114,"stral, old, age-old, archeological, former, antiquated, longstanding, archaic, centuries-old, and so on. However, there is nothing inherent in the rule extraction process to say which of the PPDB paraphrases are simplifications. In this paper, we model the task by incorporating rich features into each rule and let SMT advances in decoding and optimization determine how well a rule simplifies an input phrase. An alternative way of using PPDB for simplification would be to simply discard any of its rules which did not result in a simplified output, possibly using a simple supervised classifier (Pavlick and Callison-Burch, 2016). 3.3 Simplification-specific Features for Paraphrase Rules Designing good features is an essential aspect of modeling. For each input sentence i and its candidate output sentence j, a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } are combined with a weight vector w ~ in a linear model to obtain a single score hw~ : hw~ (i, j) = w ~ ·ϕ ~ (i, j) (8) In SMT, typical feature functions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which governs whether the system prefers fewer longer phrases or a greater number of shorter phrases"
Q16-1029,W14-1210,0,0.0624395,"sentences, and randomly split them into 2000 sentences for tuning, 350 for evaluation. Many crowdsourcing workers were able to provide simplifications of good quality and diversity (see Table 2 5 We release the data with details for each feature. http://www.manythings.org/vocabulary/ lists/l/noll-about.php 6 for an example and Table 4 for the manual quality evaluation). Having multiple references allows us to develop automatic metrics similar to BLEU to take advantage of the variation across many people’s simplifications. We leave more in-depth investigations on crowdsourcing simplification (Pellow and Eskenazi, 2014a,b) for future work. 3.5 Tuning Parameters Like in statistical machine translation, we set the weights of the linear model w ~ in the Equation (8) so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000 sentence development set. We use the pairwise ranking optimization (PRO) algorithm (Hopkins and May, 2011) implemented in the open-source Joshua toolkit (Ganitkevitch et al., 2012a; Post et al., 2013) for tuning. Specifically, we train the system to distinguish a good candidate output j from a bad candidate j 0 , measured by an objective function o"
Q16-1029,W13-2226,1,0.83823,"to take advantage of the variation across many people’s simplifications. We leave more in-depth investigations on crowdsourcing simplification (Pellow and Eskenazi, 2014a,b) for future work. 3.5 Tuning Parameters Like in statistical machine translation, we set the weights of the linear model w ~ in the Equation (8) so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000 sentence development set. We use the pairwise ranking optimization (PRO) algorithm (Hopkins and May, 2011) implemented in the open-source Joshua toolkit (Ganitkevitch et al., 2012a; Post et al., 2013) for tuning. Specifically, we train the system to distinguish a good candidate output j from a bad candidate j 0 , measured by an objective function o (Section 3.1), for an input sentence i: o(i, j) >o(i, j 0 ) ⇐⇒ hw~ (i, j) > hw~ (i, j 0 ) ⇐⇒ hw~ (i, j) − hw~ (i, j 0 ) > 0 ⇐⇒ w ~ ·ϕ ~ (i, j) − w ~ ·ϕ ~ (i, j 0 ) > 0 (9) ⇐⇒ w ~ · (~ ϕ(i, j) − ϕ ~ (i, j 0 )) > 0 Thus, the optimization reduces to a binary classification problem. Each training instance is the difference vector ϕ ~ (i, j) − ϕ ~ (i, j 0 )) of a pair of candidates, and its training label is positive or negative depending on whether"
Q16-1029,D15-1044,0,0.0729444,"Missing"
Q16-1029,E14-1076,0,0.0314472,"text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bi"
Q16-1029,N10-1144,0,0.0389537,"first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 20"
Q16-1029,C04-1129,0,0.0352311,"f statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sente"
Q16-1029,S12-1046,0,0.0151082,"Missing"
Q16-1029,P15-2135,0,0.164802,"Missing"
Q16-1029,W14-1201,0,0.576387,"rules, which is important given the fact that Simple Wikipedia and the newly released Newsela simplification corpus (Xu et al., 2015) are only available for English. Second, previous evaluation used in the simplification literature is uninformative and not comparable across models due to the complications between the three different operations of paraphrasing, deletion, and splitting. This, combined with the unreliable quality of Simple Wikipedia as a gold reference for evaluation, has been the bottleneck for developing automatic metrics. There exist only a few studˇ ies (Wubben et al., 2012; Stajner et al., 2014) on automatic simplification evaluation using existing MT metrics which show limited correlation with human assessments. In this paper, we restrict ourselves to lexical simplification, where we believe MT-derived evaluation metrics can best be deployed. Our newly proposed metric is the first automatic metric that shows reasonable correlation with human evaluation on the text simplification task. We also introduce multiple references to make automatic evaluation feasible. The most related work to ours is that of Ganitkevitch et al. (2013) on sentence compression, in which compression of word an"
Q16-1029,N10-1056,0,0.0281531,"Missing"
Q16-1029,P12-2008,0,0.533776,"also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic eva"
Q16-1029,C10-1152,0,0.796098,"ave been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing problem: given an input text, rewrite it subject to the constraints that the output should be simpler than the input, while preserving as much meaning of the input as possible, and maintaining the well-formedness of the text. Going beyond previous work, we make di401 Transactions of the Association for Computational Linguistics, vol. 4,"
Q16-1029,P08-1040,0,0.495345,"d3 Pdel (7) where d1 = d2 = d3 = 1/3 and Poperation = Roperation 1 k 1 = k Foperation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pair"
Q16-1029,W11-2160,1,0.839407,"can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns. The key differences between the paraphrase rules from PPDB and the transformations learned by the naive application of SMT to the Normal-Simple Wikipedia parallel corpus, are that the PPDB paraphrases are much mo"
Q16-1029,D11-1038,0,0.156758,"ly studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing problem: given an input text, rewrite it subject to the constraints that the output should be simpler than the input, while preserving as much meaning of the input as possible, and maintaining the well-formedness of the text. Going beyond previous work, we make di401 Transactions of the Association for Computational Linguistics, vol. 4, pp. 401–415, 2016. Action E"
Q16-1029,P12-1107,0,0.19806,"Missing"
Q16-1029,Q15-1021,1,0.646143,"ct modifications to four key components in the SMT pipeline:1 1) two novel simplification-specific tunable metrics; 2) large-scale paraphrase rules automatically derived from bilingual parallel corpora, which are more naturally and abundantly available than manually simplified texts; 3) rich rule-level simplification features; and 4) multiple reference simplifications collected via crowdsourcing for tuning and evaluation. In particular, we report the first study that shows promising correlations of automatic metrics with human evaluation. Our work answers the call made in a recent TACL paper (Xu et al., 2015) to address problems in current simplification research — we amend human evaluation criteria, develop automatic metrics, and generate an improved multiple reference dataset. Our work is primarily focused on lexical simplification (rewriting words or phrases with simpler versions), and to a lesser extent on syntactic rewrite rules that simplify the input. It largely ignores the important subtasks of sentence splitting and deletion. Our focus on lexical simplification does not affect the generality of the presented work, since deletion or sentence splitting could be applied as pre- or post-proce"
Q16-1029,C12-1177,1,0.164975,"mar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to"
Q16-1029,W06-3119,0,\N,Missing
Q16-1029,N15-1072,1,\N,Missing
Q19-1045,E09-1010,0,0.0480923,"Missing"
Q19-1045,P05-1074,1,0.635236,"ce of example usages of English words having a particular meaning. Rather than assume a rigid inventory of possible senses for each word, PSTS is grounded in the idea that the many fine-grained meanings of a word are instantiated by its paraphrases. For example, the word bug has different meanings corresponding to its paraphrases fly, error, and microbe, and PSTS includes sentences where bug takes on each of these meanings (Figure 1). Overall, the resource contains up to 10,000 sentences for each of roughly 3 million English lexical and phrasal paraphrases from the Paraphrase Database (PPDB) (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013; Pavlick et al., 2015). PSTS was compiled by automatically extracting sentences from the English side of bilingual parallel corpora using a technique inspired by bilingual pivoting (Bannard and Callison-Burch, 2005). For instance, to find a sentence containing bug where it means fly, we select English sentences where bug is translated to the French mouche, Spanish mosca, or one of the other foreign words that bug shares as a translation with fly. Qualitative analysis of the sentences in PSTS indicates that this is a noisy process, so we implement and compare two met"
Q19-1045,P91-1034,0,0.754564,"Missing"
Q19-1045,W11-2504,1,0.872267,"Missing"
Q19-1045,E12-1085,0,0.0274363,"Missing"
Q19-1045,W17-1914,1,0.849458,"and Navigli, 2007, 2009). For example, finding valid substitutes for bug in There are plenty of places to plant a bug in her office might include microphone or listening device but not glitch. The tasks of sense tagging and LexSub are closely related, since valid substitutes for a polysemous word must adhere to the correct meaning in each instance. Indeed, early LexSub systems explicitly included sense disambiguation as part of their pipeline (McCarthy and Navigli, 2007), and later studies have shown that performing sense disambiguation can improve the results of LexSub models and vice versa (Cocos et al., 2017; Alagi´c et al., 2018). We adopt an off-the-shelf LexSub model called CONTEXT2VEC (Melamud et al., 2016) as an unsupervised sentence ranking model. CONTEXT2VEC learns word and context embeddings using a bidirectional long short-term memory model such that words and their appropriate contexts have high cosine similarity. In order to apply CONTEXT2VEC to ranking sentence-paraphrase instances, we calculate the cosine similarity between the paraphrase’s CONTEXT2VEC word embedding and the context of the target word in the sentence, using a pre-trained model.5 The resulting score is hereafter refer"
Q19-1045,W06-2911,0,0.0339081,"Missing"
Q19-1045,P91-1048,0,0.152203,"Missing"
Q19-1045,P11-2055,0,0.0573582,"Missing"
Q19-1045,J94-4003,0,0.346223,"Missing"
Q19-1045,N19-1423,0,0.0257597,"Missing"
Q19-1045,S07-1009,0,0.127338,"Missing"
Q19-1045,P02-1033,0,0.0628272,"Missing"
Q19-1045,N15-1050,0,0.0896488,"paraphrases, rather than in foreign translations. This means that our method does not require any manual mapping from foreign translations to an English sense inventory. It also enables us to generate sense-tagged examples using bitext over multiple pivot languages, without having to resolve sense mapping between languages. There is a close relationship between sense tagging and paraphrasing. Some research efforts assume that words have a discrete sense inventory, and they represent each word sense as a set or cluster of paraphrases (Miller, 1995; Cocos and Callison-Burch, 2016). Other work (Melamud et al., 2015a), including in lexical substitution (McCarthy and Navigli, 2007, 2009), represents the contextualized meaning of a word instance by the set of paraphrases that could be substituted for it. This paper takes the view that assuming a discrete underlying sense inventory can be too rigid for many applications; humans have notoriously low agreement in manual sense-tagging tasks (Cinkov´a et al., 2012), and the appropriate sense granularity varies by setting. Instead, we assume a ‘‘one paraphrase per fine-grained meaning’’ model in this paper as a generalizable approach to word sense modeling. In P"
Q19-1045,S01-1001,0,0.295833,"Missing"
Q19-1045,K16-1006,0,0.0812946,"where it means fly, we select English sentences where bug is translated to the French mouche, Spanish mosca, or one of the other foreign words that bug shares as a translation with fly. Qualitative analysis of the sentences in PSTS indicates that this is a noisy process, so we implement and compare two methods for ranking sentences by the degree to which they are ‘‘characteristic’’ of their associated paraphrase meaning. When used to rank PSTS sentences, a supervised regression model trained to correlate with human judgments of sentence quality, and an unsupervised lexical substitution model (Melamud et al., 2016) lead to, Many natural language processing tasks require discriminating the particular meaning of a word in context, but building corpora for developing sense-aware models can be a challenge. We present a large resource of example usages for words having a particular meaning, called Paraphrase-Sense-Tagged Sentences (PSTS). Built on the premise that a word’s paraphrases instantiate its fine-grained meanings (i.e., bug has different meanings corresponding to its paraphrases fly and microbe) the resource contains up to 10,000 sentences for each of 3 million target-paraphrase pairs where the targ"
Q19-1045,1992.tmi-1.9,0,0.45859,"Missing"
Q19-1045,W15-1501,0,0.169446,"paraphrases, rather than in foreign translations. This means that our method does not require any manual mapping from foreign translations to an English sense inventory. It also enables us to generate sense-tagged examples using bitext over multiple pivot languages, without having to resolve sense mapping between languages. There is a close relationship between sense tagging and paraphrasing. Some research efforts assume that words have a discrete sense inventory, and they represent each word sense as a set or cluster of paraphrases (Miller, 1995; Cocos and Callison-Burch, 2016). Other work (Melamud et al., 2015a), including in lexical substitution (McCarthy and Navigli, 2007, 2009), represents the contextualized meaning of a word instance by the set of paraphrases that could be substituted for it. This paper takes the view that assuming a discrete underlying sense inventory can be too rigid for many applications; humans have notoriously low agreement in manual sense-tagging tasks (Cinkov´a et al., 2012), and the appropriate sense granularity varies by setting. Instead, we assume a ‘‘one paraphrase per fine-grained meaning’’ model in this paper as a generalizable approach to word sense modeling. In P"
Q19-1045,N13-1092,1,0.874008,"Missing"
Q19-1045,W04-0807,0,0.231126,"Missing"
Q19-1045,H94-1046,0,0.447556,"Missing"
Q19-1045,W12-3018,0,0.0478382,"Missing"
Q19-1045,W14-0132,0,0.0347516,"Missing"
Q19-1045,P15-1173,0,0.0641822,"Missing"
Q19-1045,S16-2013,0,0.0227244,"he sense-specific contexts in PSTS to generate training instances automatically. 5 Hypernym Prediction in Context Finally, we aim to demonstrate that PSTS can be used to automatically construct a training dataset for the task of predicting hypernymy in context, without relying on manually annotated resources or a pre-trained word sense disambiguation model. Most work on hypernym prediction has been done out of context: The input to the task is a pair of terms like (table, furniture), and the model predicts whether the second term is a hypernym of the first (in this case, it is). However, both Shwartz and Dagan (2016) and Vyas and Carpuat (2017) point out that hypernymy between two terms depends on their context. For example, the table mentioned in ‘‘He set the glass down on the table’’ is indeed a type of furniture, but in ‘‘Results are reported in table 3.1’’ it is not. This is the motivation for studying the task of predicting hypernymy within a given context, where the input to the problem is a pair of sentences each containing a target word, and the task is to predict whether a hypernym relationship holds between the two targets. Example task instances are in Table 5. Previous work on this task has re"
Q19-1045,S17-1004,0,0.049916,"the target x in the given sentence. They include the mean cosine similarity between word embeddings4 for paraphrase y and tokens within a twoword context window of x in sentence sx ; the cosine similarity between context-masked embed4 For computing all contextual features, we used 300dimensional skip-gram embeddings (Mikolov et al., 2013) trained on the Annotated Gigaword corpus (Napoles et al., 2012). Mean contextual similarity  w∈W cos(vy ,vw ) f (y, sx ) = |W | AddCos (Melamud et al., 2015b)  |W |·cos(vx ,vy )+ w∈W cos(vy ,vw ) f (x, y, sx ) = 2·|W | Context-masked embedding similarity (Vyas and Carpuat, 2017) f (x, y, sx ) = cos(vx,mask , vy,mask ) vx,mask = [vx  vWmin ; vx  vWmax ; vx  vWmean ] Table 3: Contextual features used for sentence quality prediction, given paraphrase pair x↔y and sentence sx ∈ S x,y . W contains words within a two-token context window of x in sx . vx is the word embedding for x. vW are vectors composed of the column-wise min/max/mean of embeddings for w ∈ W . The  symbol denotes element-wise multiplication. dings for x and y in sx (Vyas and Carpuat, 2017), and the AddCos lexical substitution metric where y is the substitute, x is the target, and the context is extr"
Q19-1045,P03-1058,0,0.26669,"Missing"
Q19-1045,P15-2070,1,0.897041,"Missing"
Q19-1045,P10-4014,0,0.0613392,"Missing"
S12-1034,P05-1074,1,0.922837,"Missing"
S12-1034,P99-1071,0,0.149569,"ompression, achieving stateof-the-art quality. 1 Introduction A wide variety of applications in natural language processing can be cast in terms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distribution"
S12-1034,P08-1077,0,0.666827,"lexical items in its context, such as: “for what verbs do we see with the phrase as the subject?”, or “what adjectives modify the phrase?”. However, when moving to vast text collections or collapsed representations of large text corpora, linguistic annotations can become impractically expensive to produce. A straightforward and widely used solution is to fall back onto lexical n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?” A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). 2.3 Other Related Work Recently, Chan et al. (2011) presented an initial investigation into combining phrasal paraphrases obtained through bilingual pivoting with monolingual distributional information. Their work investigated a reranking approach and evaluated their method via a substitution task, showing that the two sources of information are complementary and can yield improvements in paraphrase quality when combined. 3 Incorporating Distributional Similarity In order to incorporate distributional similarity information into the paraphrasing s"
S12-1034,D08-1021,1,0.944103,"Missing"
S12-1034,W11-2504,1,0.872905,"Missing"
S12-1034,P05-1033,0,0.0829457,"realizations, matching the lexicalized portions of the rule and generalizing over the nonterminals. Background Approaches to paraphrase extraction differ based on their underlying data source. In Section 2.1 we outline pivot-based paraphrase extraction from bilingual data, while the contextual features used to determine closeness in meaning in monolingual approaches is described in Section 2.2. 2.1 ⌃ h Paraphrase Extraction via Pivoting Following Ganitkevitch et al. (2011), we formulate our paraphrases as a syntactically annotated synchronous context-free grammar (SCFG) (Aho and Ullman, 1972; Chiang, 2005). An SCFG rule has the form: r = C → hf, e, ∼, ϕ ~ i, where the left-hand side of the rule, C, is a nonterminal and the right-hand sides f and e are strings of terminal and nonterminal symbols. There is a one-to-one correspondency between the nonterminals in f and e: each nonterminal symbol in f has to also appear in e. The function ∼ captures this bijective mapping between the nonterminals. Drawing on machine translation terminology, we refer to f as the source and e as the target side of the rule. Each rule is annotated with a feature vector of feature functions ϕ ~ = {ϕ1 ...ϕN } that, using"
S12-1034,C08-1018,0,0.0748158,"rm of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ). This generalizes over distributional similarity for contiguous phra"
S12-1034,D11-1108,1,0.892318,"Missing"
S12-1034,W12-3134,1,0.928703,"VP NP NP DT+NNP CD NNS JJ twelve cartoons insulting NNP DT the prophet mohammad 12 of the cartoons that are offensive to the prophet mohammad CD NNS DT JJ NNP DT+NNP NP NP VP NP Figure 3: An example of a synchronous paraphrastic derivation, here a sentence compression. Shaded words are deleted in the indicated rule applications. Figure 2 illustrates syntax-constrained pivoting and feature aggregation over multiple foreign language translations for a paraphrase pattern. After the SCFG has been extracted, it can be used within standard machine translation machinery, such as the Joshua decoder (Ganitkevitch et al., 2012). Figure 3 shows an example for a synchronous paraphrastic derivation produced as a result of applying our paraphrase grammar in the decoding process. The approach outlined relies on aligned bilingual texts to identify phrases and patterns that are equivalent in meaning. When extracting paraphrases from monolingual text, we have to rely on an entirely different set of semantic cues and features. 2.2 Monolingual Distributional Similarity Methods based on monolingual text corpora measure the similarity of phrases based on contextual features. To describe a phrase e, we define a set of features t"
S12-1034,D11-1125,0,0.0390613,"s that aim to better describe a rule’s compressive power: on top of the word count features wcount src and wcount tgt and the word count difference feature wcount diff , we add character based count and difference features ccount src , ccount tgt , and ccount diff , as well as logwcount tgt compression ratio features word cr = log wcount src ccount tgt and the analogously defined char cr = log ccount src . For model tuning and decoding we used the Joshua machine translation system (Ganitkevitch et al., 2012). The model weights are estimated using an implementation of the PRO tuning algorithm (Hopkins and May, 2011), with P R E´ CIS as our objective function (Ganitkevitch et al., 2011). The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a Kneser-Ney discounted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002). 260 VBG amod the long-term IN TO DT ⇣ ⌘ ~ syntax the long-term = sig ⇣ investment NN JJ lex-R-investment pos-L-IN-TO lex-L-on-to pos-L-TO lex-L-to dep-det-R-investment pos-R-NN dep-amod-R-investment dep-det-R-NN dep-amod-R-NN syn-gov-NP syn-miss-L-NN ⇣ Figure 6: An example of the syntactic featureset. The phrase “the lo"
S12-1034,2005.mtsummit-papers.11,0,0.0609411,"r falls in the range 0.5 &lt; cr ≤ 0.8. From these, we select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)’s state-of-the-art compression model which uses a series of constraints in an integer linear programming (ILP) solver. 4.2 Baseline Paraphrase Grammar We extract our paraphrase grammar from the French–English portion of the Europarl corpus (version 5) (Koehn, 2005). The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively. The paraphrase grammar is produced using the Hadoop-based Thrax Left Right .. VP PP 25 achieve the long-term the long-term goals 23 det the long-term plans 97 43 revise the long-term holding on to 64 confirmed the long-term the long-term investment 10 .. ⇣ ⌘ ~ ngram the long-term = sig ⇣ NP L-achieve = 25 R-plans = 97 L-revise = 43 R-goals = 23 L-confirmed = 64 R-investment = 10 ⇣ Figure 5: An example of the n-gram feature extractio"
S12-1034,J10-4005,0,0.0317159,"e and e as the target side of the rule. Each rule is annotated with a feature vector of feature functions ϕ ~ = {ϕ1 ...ϕN } that, using a corresponding weight vector ~λ, are combined in a loglinear model to compute the cost of applying r: cost(r) = − N X λi log ϕi . To extract paraphrases we follow the intuition that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning, as illustrated in Figure 1.1 First, we use standard machine translation methods to extract a foreign-to-English translation grammar from a bilingual parallel corpus (Koehn, 2010). Then, for each pair of translation rules where the left-hand side C and foreign string f match: r1 = C → hf, e1 , ∼1 , ϕ ~ 1i r2 = C → hf, e2 , ∼2 , ϕ ~ 2 i, we pivot over f to create a paraphrase rule rp : rp = C → he1 , e2 , ∼p , ϕ ~ p i, with a combined nonterminal correspondency function ∼p . Note that the common source side f implies that e1 and e2 share the same set of nonterminal symbols. The paraphrase feature vector ϕ ~ p is computed from the translation feature vectors ϕ ~ 1 and ϕ ~ 2 by following the pivoting idea. For instance, we estimate the conditional paraphrase probability p"
S12-1034,N06-1014,0,0.0135792,"From these, we select 936 sentences for the development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)’s state-of-the-art compression model which uses a series of constraints in an integer linear programming (ILP) solver. 4.2 Baseline Paraphrase Grammar We extract our paraphrase grammar from the French–English portion of the Europarl corpus (version 5) (Koehn, 2005). The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively. The paraphrase grammar is produced using the Hadoop-based Thrax Left Right .. VP PP 25 achieve the long-term the long-term goals 23 det the long-term plans 97 43 revise the long-term holding on to 64 confirmed the long-term the long-term investment 10 .. ⇣ ⌘ ~ ngram the long-term = sig ⇣ NP L-achieve = 25 R-plans = 97 L-revise = 43 R-goals = 23 L-confirmed = 64 R-investment = 10 ⇣ Figure 5: An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term"
S12-1034,lin-etal-2010-new,0,0.169001,"uch as: “for what verbs do we see with the phrase as the subject?”, or “what adjectives modify the phrase?”. However, when moving to vast text collections or collapsed representations of large text corpora, linguistic annotations can become impractically expensive to produce. A straightforward and widely used solution is to fall back onto lexical n-gram features, e.g. “what words or bigrams have we seen to the left of this phrase?” A substantial body of work has focussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bhagat and Ravichandran, 2008; Lin et al., 2010; Van Durme and Lall, 2010). 2.3 Other Related Work Recently, Chan et al. (2011) presented an initial investigation into combining phrasal paraphrases obtained through bilingual pivoting with monolingual distributional information. Their work investigated a reranking approach and evaluated their method via a substitution task, showing that the two sources of information are complementary and can yield improvements in paraphrase quality when combined. 3 Incorporating Distributional Similarity In order to incorporate distributional similarity information into the paraphrasing system, we need to"
S12-1034,P79-1016,0,0.663091,"be cast in terms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case o"
S12-1034,W11-1610,1,0.91607,"Missing"
S12-1034,W12-3018,1,0.806725,"Missing"
S12-1034,N07-1051,0,0.0131708,"development set, as well as 560 sentences for a test set that we use to gauge the performance of our system. We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)’s state-of-the-art compression model which uses a series of constraints in an integer linear programming (ILP) solver. 4.2 Baseline Paraphrase Grammar We extract our paraphrase grammar from the French–English portion of the Europarl corpus (version 5) (Koehn, 2005). The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively. The paraphrase grammar is produced using the Hadoop-based Thrax Left Right .. VP PP 25 achieve the long-term the long-term goals 23 det the long-term plans 97 43 revise the long-term holding on to 64 confirmed the long-term the long-term investment 10 .. ⇣ ⌘ ~ ngram the long-term = sig ⇣ NP L-achieve = 25 R-plans = 97 L-revise = 43 R-goals = 23 L-confirmed = 64 R-investment = 10 ⇣ Figure 5: An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term” is seen preceded by “revise” (43 times) and fol"
S12-1034,P02-1006,0,0.0114452,"ms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ). This generalizes over"
S12-1034,P05-1077,0,0.0223875,"d constituent labels for the phrase. The latter are split in governing constituent and missing constituent (with directionality). Figure 6 illustrates the syntax model’s feature extraction for an example phrase occurrence. Using this method we extract distributional signatures for over 12 million 1-to-4-gram phrases. 4.3.3 Locality Sensitive Hashing Collecting distributional signatures for a large number of phrases quickly leads to unmanageably large datasets. Storing the syntax model’s 12 million signatures in a compressed readable format, for instance, requires over 20GB of disk space. Like Ravichandran et al. (2005) and Bhagat and Ravichandran (2008), we rely on locality sensitive hashing (LSH) to make the use of these large collections practical. In order to avoid explicitly computing the feature vectors, which can be memory intensive for frequent phrases, we chose the online LSH variant described by Van Durme and Lall (2010), as implemented in the Jerboa toolkit (Van Durme, 2012). This method, based on the earlier work of Indyk and 261 Motwani (1998) and Charikar (2002), approximates the cosine similarity between two feature vectors based on the Hamming distance in a dimensionalityreduced bitwise repre"
S12-1034,P07-1059,0,0.0799536,"ications in natural language processing can be cast in terms of text-to-text generation. Given input in the form of natural language, a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim("
S12-1034,P10-2043,1,0.863591,"Missing"
S12-1034,P12-1107,0,0.0554392,"Missing"
S12-1034,N12-1078,1,0.848712,"Missing"
S12-1034,P08-1089,0,0.0575905,"Missing"
S12-1034,P09-1094,0,0.0225826,"a text-to-text generation system produces natural language output that is subject to a set of constraints. Compression systems, for instance, produce shorter sentences. Paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et al., 1999; Barzilay, 2003), query expansion (Anick and Tipirneni, 1999; Riezler et al., 2007), question answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and Lapata, 2008; Zhao et al., 2009), and simplification (Wubben et al., 2012). Paraphrase collections for text-to-text generation have been extracted from a variety of different corpora. Several approaches rely on bilingual paral• We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features estimated from bilingual data. • We define distributional similarity for paraphrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP , a JJ case of NP ). This generalizes over distributional similarity for contiguous phrases. • We compare di"
S12-1034,J90-1003,0,\N,Missing
S15-1015,P12-1042,1,0.906269,"Missing"
S15-1015,D10-1111,0,0.0321599,"discussion fora, there has been a significant increase in documented political and ideological discussions. Automatically predicting the perspective or stance of users in such media is a challenging research 137 Chris Callison-Burch University of Pennsylvania Philadelphia, PA ccb@cis.upenn.edu problem that has a wide variety of applications including recommendation systems, targeted advertising, political polling, product reviews and even predicting possible future events. Ideology refers to the beliefs that influence an individual’s goals, expectations and views of the world (Van Dijk, 1998; Ahmed and Xing, 2010). The ideological perspective of a person is often expressed in his/her choice of discussed topics. People with opposing perspectives will choose to make different topics more salient. (Entman, 1993). From a social-science viewpoint, the notion of “perspective” is related to the concept of “framing”. Framing involves making some topics (or some aspects of the discussed topics) more prominent in order to promote the views and interpretations of the writer (communicator). The communicator makes these framing decisions either consciously or unconsciously (Entman, 1993). These decisions are often"
S15-1015,C12-1003,0,0.039367,"Missing"
S15-1015,P12-2013,1,0.889844,"Missing"
S15-1015,P12-1091,1,0.793103,"“support#v#1” whose Synset is support#v#1’, back_up#v#1. 4.3 Latent Semantics The next set of features relies on “Latent Semantics” which maps text from a high-dimensional space such as unigrams to a low-dimensional one such as topics. Most of these models assign a semantic profile to each given sentence (or document) by considering the observed words and assuming that each given document has a distribution over “K” topics. We apply (1) Latent Dirichlet Allocation (LDA) (Blei et al., 2003) as implemented in MALLET toolkit (McCallum, 2002), and (2) Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to each post. In addition to observed words, WTMF also models missing ones namely explicitly modeling what the post is not about. WTMF defines missing words as the whole vocabulary of the training data minus the ones observed in the given document. 4.3.1 Number of Topics We vary the number of topics (K) between 100 and 500 (with a step-size of 100) and use the best “K” for each dataset. We define the best K, for each of LDA and WTMF, as the one that yields the best cross-validation results when combined with unigram features. The best K value for LDA is 400 for PCC and Abortion, 500 for Creat"
S15-1015,C12-2045,0,0.252418,"Missing"
S15-1015,P13-2142,0,0.302685,"Missing"
S15-1015,P10-2047,0,0.412334,"Missing"
S15-1015,W06-2915,0,0.668037,"Missing"
S15-1015,P05-3019,0,0.0119828,"hows the size of the training and test data in the ANES and Ideological-Debates datasets. 141 4.1 Approach Preprocessing We apply basic preprocessing to the text by separating punctuation and numbers from words. All punctuation and numbers are then ignored when training the classifier for all of our systems including the unigram baseline. The intuition behind this is that punctuation and numbers do not capture the perspective of a person but rather the writing style. Moreover, by ignoring them, we avoid overfitting the training data. 4.2 Word Sense Disambiguation (WSD) We use WN-Sense-Relate (Patwardhan et al., 2005) to perform word sense disambiguation. SenseRelate uses WordNet (Miller, 1995) to tag each word with the part-of-speech and sense-id. The only parts of speech that are handled by WN-Sense-Relate are adjectives (a), adverbs (r), verbs (v) and nouns (n). In addition to the part-of-speech and sense-id, WNSense-Relate also identifies and tags compounds. The word sense tagging process can be either contextual or can rely on the most frequent sense. We experiment with both variants. 4.2.1 Contextual WSD (WSD-CXT) In this variant of WSD, in addition to tagging compounds, we contextually disambiguate"
S15-1015,W10-0214,0,0.0791204,"0 106 7,142 159 88 84 67 2,734 2,160 2,842 1,588 24 18 14 15 Table 5: Statistics of the training and test sets for both the ANES and the four domains of the IdeologicalDebates datasets. debt.”, we replace “They” with “Republicans”. 4 3.2 Our goal is to determine whether semantic features help in identifying a person’s ideological perspective as determined by his/her answer to the PCC constrained question in the “ANES” dataset and his/her stance towards the ideological-topics discussed in the “Ideological-Debates” dataset independently. Ideological Debates Dataset This dataset was collected by Somasundaran and Wiebe (2010) . It contains debate posts from six domains; (a) Abortion, (b) Creationism, (c) GayRights, (d) Gun-Rights, (e) Healthcare and (f) Existence of God. Each domain represents an ideological topic with two possible perspectives, pro and against. Similar to the work of (Somasundaran and Wiebe, 2010), we use the first four domains to evaluate our approach. Table 2 shows the class distribution in each of these four domains while table 4 lists some sample posts. It should be noted that our results are not comparable to those obtained by (Somasundaran and Wiebe, 2010), since they used a subset of the p"
S15-1015,W10-0723,0,0.199649,"Missing"
S15-2001,S12-1051,0,0.130172,"Figure 2: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. 4.3 Annotation Quality We remove problematic annotators by checking their Cohen’s Kappa agreement (Artstein and Poesio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on the test dataset of 972 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different topics. Although the two scales of expert and crowdsourcing annotation are defined differently, their Pearson correlation coefficient reaches 0.735 (twotailed significance 0.001). Figure 1 shows a heatmap representing the detailed overlap between the two annotations. It suggests that the graded s"
S15-2001,J08-4004,0,0.0181162,"010): https://github.com/brendano/tweetmotif Trending Topics U.S. Facebook Dwight Howard GWB Netflix Ronaldo Dortmund Momma Dee Morning Huck Klay Milwaukee Harvick Jeff Green Ryu The Clippers Candice Robert Woods Amber Reggie Miller filtered random 0.0 0.2 0.4 0.6 0.8 Percentage of Positive Judgements Figure 2: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. 4.3 Annotation Quality We remove problematic annotators by checking their Cohen’s Kappa agreement (Artstein and Poesio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on the test dataset of 972 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different t"
S15-2001,S15-2004,0,0.0261708,"adopts typical machine learning classifiers and uses a variety of features, such as surface text, semantic level, textual entailment, word distributional representations by deep learning methods. FBK-HLT (Ngoc Phuoc An Vo and Popescu, 2015): This team uses supervised learning model with different features for the 2 runs, such as n-gram overlap, word alignment and edit distance. Hassy: This team uses a bag-of-embeddings approach via supervised learning. Two sentences are first embedded into a vector space, and then the system computes the dot-product of the two sentence embeddings. HLTC-HKUST (Bertero and Fung, 2015): This team uses supervised classification with a standard two-layer neural network classifier. The features used include translation metrics, lexical, syntactic and semantic similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, whil"
S15-2001,S14-2085,0,0.0857023,"Missing"
S15-2001,S15-2010,0,0.0246542,"Gram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and 8 RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP) for predicting semantic similarity where indicators of translatability are used as features (Bic¸ici and Way, 2014) and instance selection for RTM is performed with FDA5 (Bic¸ici and Yuret, 2014). RTM works as follows: FDA5 → MTPP → ML training → predict. Rob (van der Goot and van Noord, 2015): This system is inspired by a state-of-the-art semantic relatedness prediction system by Bjerva et al. (2014). It combines features from different parses with lexical and compositional dis"
S15-2001,S14-2114,0,0.0383579,"Missing"
S15-2001,D12-1050,0,0.0203303,"arch shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http"
S15-2001,P11-1020,0,0.104844,"Missing"
S15-2001,P09-1053,0,0.834216,"op ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has"
S15-2001,R13-1026,0,0.018233,"Missing"
S15-2001,C04-1051,1,0.79413,"Missing"
S15-2001,S15-2011,0,0.319055,"Figure 4: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC. AJ: This team utilizes TERp and BLEU – automatic evaluation metrics for Machine Translation. The system uses a logistic regression model and performs threshold selection. AMRITACEN: This team uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hi"
S15-2001,S15-2012,0,0.0178189,"Missing"
S15-2001,P12-1091,0,0.400672,"mmary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 1–11, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Paraphrase? yes yes yes no no debatable debatable Sentence 1 Ezekiel Ansah wearing 3D glasses wout the lens Marriage equality law pass"
S15-2001,C14-1047,0,0.0465864,"m uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM and logistic reThere are in total 19 teams participated: 6 Rank PI SS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 8 1 3 4 5 9 10 11 12 2 15 16 17 late 18 6 7 13 late Team Run Human Upperbound ASOBEK 01 svckernel ASOBEK 02 linearsvm MITRE 01 ikr ECNU 02 nnfeats FBK-HLT 01 voted TKLBLIIR 02 gs0105"
S15-2001,S13-1005,0,0.0243588,"d edit distance. Hassy: This team uses a bag-of-embeddings approach via supervised learning. Two sentences are first embedded into a vector space, and then the system computes the dot-product of the two sentence embeddings. HLTC-HKUST (Bertero and Fung, 2015): This team uses supervised classification with a standard two-layer neural network classifier. The features used include translation metrics, lexical, syntactic and semantic similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between"
S15-2001,N06-2015,0,0.0323369,"ith (2009). It uses simple n-gram (also in stemmed form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). It uses 0.5 as cutoff in the binary paraphrase identification task. 6 Systems and Results A total of 18 teams participated in the PI task (required), 13 of which also submitted to the SS task (optional). Every team submitted 2 runs except one (up to 2 were are allowed). 6.1 Evaluation Results Table 3 shows the evaluation results. We use the F1score and Pearson correlation as the primary evaluation metric for the PI and SS task respectively. The results are very exciting that most systems outperformed the two strong baselines we chose,"
S15-2001,D13-1090,0,0.100953,"witter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http://aclweb.org/aclwiki/index.php? title=Paraphras"
S15-2001,S15-2013,0,0.0114643,"rategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC. AJ: This team utilizes TERp and BLEU – automatic evaluation metrics for Machine Translation. The system uses a logistic regression model and performs threshold selection. AMRITACEN: This team uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM an"
S15-2001,J10-3003,0,0.0530457,"model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach &gt;0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available.1 1 Introduction The ability to identify paraphrases, i.e. alternative expressions of the same (or similar) meaning, and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detect"
S15-2001,N12-1019,0,0.281154,"echniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http://aclweb.org/aclwiki/"
S15-2001,W06-1603,0,0.0987618,"ominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexic"
S15-2001,D11-1141,0,0.00931635,"This causes two problems: a) it is expensive to obtain paraphrases via manual annotation; b) non-expert annotators tend to loosen the criteria and are more likely to make false positive errors. To address these challenges, we design a simple annotation task and introduce two selection mechanisms to select sentences which are more likely to be paraphrases, while preserving diversity and representativeness. 3 The tokenizer was developed by O’Connor et al. (2010): https://github.com/brendano/tweetmotif 4 The POS tagger was developed by Derczynski et al. (2013) and the NER tagger was developed by Ritter et al. (2011): https://github.com/aritter/twitter_nlp 3 =5 turk =4 turk =3 turk =2 turk turk Figure 1: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are nonparaphrases; 4,5 a"
S15-2001,S15-2009,0,0.0191271,"lap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM and logistic reThere are in total 19 teams participated: 6 Rank PI SS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 8 1 3 4 5 9 10 11 12 2 15 16 17 late 18 6 7 13 late Team Run Human Upperbound ASOBEK 01 svckernel ASOBEK 02 linearsvm MITRE 01 ikr ECNU 02 nnfeats FBK-HLT 01 voted TKLBLIIR 02 gs0105 MITRE 02 bieber HLTC-HKUST 02 run2 HLTC-HKUST 01 run1 ECNU 01 mlfeats AJ 01 first DEPTH 02 modelx23 CDTDS 01 simple CDTDS 02 simplews DEPTH 01 modelh22 FBK-HLT 02 multilayer ROB 01 all EBIQUITY 01 run TKLBLIIR 01 gsc054 EBIQUITY 02 run BASELINE logistic reg. COLUMBIA 02 ormf  HA"
S15-2001,S15-2007,0,0.0719415,"Missing"
S15-2001,U06-1019,0,0.299872,"ry; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used"
S15-2001,D13-1008,0,0.0670419,"useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent s"
S15-2001,Q14-1034,1,0.728134,"ar task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are nonparaphrases; 4,5 are paraphrases. Medium-scored cases (2 for crowdsourcing; 3 for expert annotation) are discarded in the system evaluation of the PI sub-task. 4.1 4 =1 expert=0 =0 In this shared task, we use the Twitter Paraphrase Corpus that we first presented in (Xu, 2014) and (Xu et al., 2014). Table 2 shows the basic statistics of the corpus. The sentences are preprocessed with tokenization,3 POS and named entity tags.4 The training and development set consists of 17,790 sentence pairs posted between April 24th and May 3rd, 2013 from 500+ trending topics featured on Twitter (excluding hashtags). The training and development set is a random split. Each sentence pair is annotated by 5 different crowdsourcing workers. For the test set, we obtain both crowdsourced and expert labels on 972 sentence pairs from 20 randomly sampled Twitter trending topics between May 13th and June 10th, 2"
S15-2001,W13-2515,1,0.744453,"larity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2"
S15-2001,D11-1061,0,0.174929,"meaning, and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam"
S15-2001,S15-2002,0,0.0446902,"similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and 8 RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP) for predicting semantic similarity where indicators of translatability are used as features (Bic¸ici and Way, 2014) and instance selection for RTM"
S15-2001,N12-1034,0,\N,Missing
S15-2001,S15-2006,0,\N,Missing
S16-2014,W06-1805,0,0.822311,"as led to the generalization that the deletion of non-subsective adjectives tends to result in contradictory utterances: Moussaoui is a would-be hijacker entails that it is not the case that Moussaoui is a hijacker. This generalization has prompted normative rules for the treatment of such adjectives in various NLP tasks. In information extraction, it is assumed that systems cannot extract useful rules from sentences containing non-subsective modifiers (Angeli et al., 2015), and in RTE, it is assumed that systems should uniformly penalize insertions and deletions of non-subsective adjectives (Amoia and Gardent, 2006). 1. (a) U.S. District Judge Leonie Brinkema accepted would-be hijacker Zacarias Moussaoui’s guilty pleas . . . (b) Moussaoui participated in the Sept. 11 attacks. 114 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 114–119, Berlin, Germany, August 11-12, 2016. Privative (e.g. fake) N AN Plain Non-subsective (e.g. alleged) N AN Subsective (e.g. red) AN Figure 1: Three main classes of adjectives. If their entailment behavior is consistent with their theoretical definitions, we would expect our annotations (Section 3) to produce the insertion ("
S16-2014,W07-1430,0,0.70578,"d and Related Work Classes of Adjectives. Adjectives are commonly classified taxonomically as either subsective or non-subsective (Kamp and Partee, 1995). Subsective adjectives are adjectives which pick out a subset of the set denoted by the unmodified noun; that is, AN ⊂ N1 . For non-subsective adjectives, in contrast, the AN cannot be guaranteed to be a subset of N. For example, clever is subsective, and so a clever thief is always a thief. However, While the hierarchical classification of adjectives described above is widely accepted and often applied in NLP tasks (Amoia and Gardent, 2006; Amoia and Gardent, 2007; Boleda et al., 2012; McCrae et al., 2014), it is not undisputed. Some linguists take the position that in fact privative ad1 We use the notation N and AN to refer both the the natural language expression itself (e.g. red car) as well as its denotation, e.g. {x|x is a red car}. 115 jectives are simply another type of subsective adjective (Partee, 2003; McNally and Boleda, 2004; Abdullah and Frost, 2005; Partee, 2007). Advocates of this theory argue that the denotation of the noun should be expanded to include both the properties captured by the privative adjectives as well as those captured b"
S16-2014,P15-1034,0,0.130864,"er is not a hijacker. The observation that adjective-nouns (ANs) involving non-subsective adjectives do not entail the underlying nouns (Ns) has led to the generalization that the deletion of non-subsective adjectives tends to result in contradictory utterances: Moussaoui is a would-be hijacker entails that it is not the case that Moussaoui is a hijacker. This generalization has prompted normative rules for the treatment of such adjectives in various NLP tasks. In information extraction, it is assumed that systems cannot extract useful rules from sentences containing non-subsective modifiers (Angeli et al., 2015), and in RTE, it is assumed that systems should uniformly penalize insertions and deletions of non-subsective adjectives (Amoia and Gardent, 2006). 1. (a) U.S. District Judge Leonie Brinkema accepted would-be hijacker Zacarias Moussaoui’s guilty pleas . . . (b) Moussaoui participated in the Sept. 11 attacks. 114 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 114–119, Berlin, Germany, August 11-12, 2016. Privative (e.g. fake) N AN Plain Non-subsective (e.g. alleged) N AN Subsective (e.g. red) AN Figure 1: Three main classes of adjectives. If"
S16-2014,W12-3018,0,0.0278809,"Missing"
S16-2014,W15-0103,0,0.0313077,"meaning (e.g. the functional features of the noun) without modifying the extension of the noun (Del Pinal, 2015). Such an analysis can explain how we can understand a fake gun as having many, but not all, of the properties of a gun. Several other studies abandon the attempt to organize adjectives taxonomically, and instead focus on the properties of the modified noun. Nayak et al. (2014) categorize non-subsective adjectives in terms of the proportion of properties that are shared between the N and the AN and Pustejovsky (2013) focus on syntactic cues about exactly which properties are shared. Bakhshandh and Allen (2015) analyze adjectives by observing that, e.g., red modifies color while tall modifies size. In Section 5, we discuss the potential benefits of pursuing these property-based analyses in relation to our experimental findings. through the Annotated Gigaword corpus (Napoles et al., 2012) for occurrences of each adjective in the list, restricting to cases in which the adjective appears as an adjective modifier of (is in an amod dependency relation with) a common noun (NN). For each adjective, we choose 10 sentences such that the adjective modifies a different noun in each. As a control, we take a sma"
S16-2014,D12-1112,0,0.0303058,"s of Adjectives. Adjectives are commonly classified taxonomically as either subsective or non-subsective (Kamp and Partee, 1995). Subsective adjectives are adjectives which pick out a subset of the set denoted by the unmodified noun; that is, AN ⊂ N1 . For non-subsective adjectives, in contrast, the AN cannot be guaranteed to be a subset of N. For example, clever is subsective, and so a clever thief is always a thief. However, While the hierarchical classification of adjectives described above is widely accepted and often applied in NLP tasks (Amoia and Gardent, 2006; Amoia and Gardent, 2007; Boleda et al., 2012; McCrae et al., 2014), it is not undisputed. Some linguists take the position that in fact privative ad1 We use the notation N and AN to refer both the the natural language expression itself (e.g. red car) as well as its denotation, e.g. {x|x is a red car}. 115 jectives are simply another type of subsective adjective (Partee, 2003; McNally and Boleda, 2004; Abdullah and Frost, 2005; Partee, 2007). Advocates of this theory argue that the denotation of the noun should be expanded to include both the properties captured by the privative adjectives as well as those captured by the subsective adje"
S16-2014,P16-1204,1,0.889631,"Missing"
S16-2014,W13-0509,0,0.0169333,"ve a “dual semantic structure” and that non-subsective adjectives modify part of this meaning (e.g. the functional features of the noun) without modifying the extension of the noun (Del Pinal, 2015). Such an analysis can explain how we can understand a fake gun as having many, but not all, of the properties of a gun. Several other studies abandon the attempt to organize adjectives taxonomically, and instead focus on the properties of the modified noun. Nayak et al. (2014) categorize non-subsective adjectives in terms of the proportion of properties that are shared between the N and the AN and Pustejovsky (2013) focus on syntactic cues about exactly which properties are shared. Bakhshandh and Allen (2015) analyze adjectives by observing that, e.g., red modifies color while tall modifies size. In Section 5, we discuss the potential benefits of pursuing these property-based analyses in relation to our experimental findings. through the Annotated Gigaword corpus (Napoles et al., 2012) for occurrences of each adjective in the list, restricting to cases in which the adjective appears as an adjective modifier of (is in an amod dependency relation with) a common noun (NN). For each adjective, we choose 10 s"
S16-2014,W07-1401,0,0.119521,"an instance of an adjective-noun phrase is an instance of the noun: a red car is a car and a successful senator is a senator. In contrast, adjective-noun phrases involving non-subsective adjectives, such as imaginary and former (Table 1), denote a set that is disjoint from the denotation of the nouns they modify: an imaginary car is not a car and a former senator is not a senator. Understanding whether or not adjectives are subsective is critical in any task involving natural language inference. For example, consider the below sentence pair from the Recognizing Textual Entailment (RTE) task (Giampiccolo et al., 2007): In this example, recognizing that 1(a) does not entail 1(b) hinges on understanding that a would-be hijacker is not a hijacker. The observation that adjective-nouns (ANs) involving non-subsective adjectives do not entail the underlying nouns (Ns) has led to the generalization that the deletion of non-subsective adjectives tends to result in contradictory utterances: Moussaoui is a would-be hijacker entails that it is not the case that Moussaoui is a hijacker. This generalization has prompted normative rules for the treatment of such adjectives in various NLP tasks. In information extraction,"
S16-2014,W14-4724,0,0.116189,"ctives are commonly classified taxonomically as either subsective or non-subsective (Kamp and Partee, 1995). Subsective adjectives are adjectives which pick out a subset of the set denoted by the unmodified noun; that is, AN ⊂ N1 . For non-subsective adjectives, in contrast, the AN cannot be guaranteed to be a subset of N. For example, clever is subsective, and so a clever thief is always a thief. However, While the hierarchical classification of adjectives described above is widely accepted and often applied in NLP tasks (Amoia and Gardent, 2006; Amoia and Gardent, 2007; Boleda et al., 2012; McCrae et al., 2014), it is not undisputed. Some linguists take the position that in fact privative ad1 We use the notation N and AN to refer both the the natural language expression itself (e.g. red car) as well as its denotation, e.g. {x|x is a red car}. 115 jectives are simply another type of subsective adjective (Partee, 2003; McNally and Boleda, 2004; Abdullah and Frost, 2005; Partee, 2007). Advocates of this theory argue that the denotation of the noun should be expanded to include both the properties captured by the privative adjectives as well as those captured by the subsective adjectives. This expanded"
S17-1002,chrupala-etal-2008-learning,0,0.0948476,"Missing"
S17-1002,N13-1092,1,0.877486,"Missing"
S17-1002,P82-1020,0,0.820605,"Missing"
S17-1002,P05-1074,1,0.629065,"from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b) in the paraphrasebased method as training data for our neural network model. The main contributions of this paper are: tion can provide substantial complementary information to the distributional signal for distinguishing between different semantic relations. While antonymy applies to expressions that represent contrasting meanings, paraphrases are phrases expressing the same meaning, which usually occur in similar textual contexts (Barzilay and McKeown, 2001) or have common translations in other languages (Bannard and Callison-Burch, 2005). Specifically, if two words or phrases are paraphrases, they are unlikely to be antonyms of each other. Our first approach to antonym detection exploits this fact and uses paraphrases for detecting and generating antonyms (The dementors caught Sirius Black/ Black could not escape the dementors). We start by focusing on phrase pairs that are most salient for deriving antonyms. Our assumption is that phrases (or words) containing negating words (or prefixes) are more helpful for identifying opposing relationships between term-pairs. For example, from the paraphrase pair (caught/not escape), we"
S17-1002,P01-1008,0,0.208388,"prior path-based methods on this task. We used the antonym pairs extracted from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b) in the paraphrasebased method as training data for our neural network model. The main contributions of this paper are: tion can provide substantial complementary information to the distributional signal for distinguishing between different semantic relations. While antonymy applies to expressions that represent contrasting meanings, paraphrases are phrases expressing the same meaning, which usually occur in similar textual contexts (Barzilay and McKeown, 2001) or have common translations in other languages (Bannard and Callison-Burch, 2005). Specifically, if two words or phrases are paraphrases, they are unlikely to be antonyms of each other. Our first approach to antonym detection exploits this fact and uses paraphrases for detecting and generating antonyms (The dementors caught Sirius Black/ Black could not escape the dementors). We start by focusing on phrase pairs that are most salient for deriving antonyms. Our assumption is that phrases (or words) containing negating words (or prefixes) are more helpful for identifying opposing relationships"
S17-1002,E17-1008,0,0.521346,"e terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speakers consider antonyms, they have limited coverage. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered"
S17-1002,P15-1146,1,0.899335,"Missing"
S17-1002,P15-2070,1,0.888814,"Missing"
S17-1002,D14-1162,0,0.0859183,"n sources are used to recognize semantic relations: pathbased and distributional. Path-based methods consider the joint occurrences of the two terms in a given sentence and use the dependency paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speaker"
S17-1002,P14-2086,0,0.268465,"Missing"
S17-1002,K15-1026,0,0.209936,"other relationships has proven to be difficult. Approaches to antonym detection have exploited distributional vector representations relying on the distributional hypothesis of semantic similarity (Harris, 1954; Firth, 1957) that words co-occurring in similar contexts tend to be semantically close. Two main information sources are used to recognize semantic relations: pathbased and distributional. Path-based methods consider the joint occurrences of the two terms in a given sentence and use the dependency paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying anto"
S17-1002,W16-5310,1,0.91968,"irs from paraphrases in the PPDB, the largest paraphrase resource currently available. • We demonstrate improvements to an integrated path-based and distributional model, showing that our morphology-aware neural network model, AntNET, performs better than state-of-the-art methods for antonym detection. Our second method is inspired by the recent success of deep learning methods for relation detection. Shwartz et al. (2016) proposed an integrated path-based and distributional model to improve hypernymy detection between term-pairs, and later extended it to classify multiple semantic relations (Shwartz and Dagan, 2016) (LexNET). Although LexNET was the best performing system in the semantic relation classification task of the CogALex 2016 shared task, the model performed poorly on synonyms and antonyms compared to other relations. The path-based component is weak in recognizing synonyms, which do not tend to co-occur, and the distributional information caused confusion between synonyms and antonyms, since both tend to occur in the same contexts. We propose AntNET, a novel extension of LexNET that integrates information about negating prefixes as a new morphological pattern feature and is able to distinguish"
S17-1002,P16-1226,1,0.906259,"paths that connect the terms as features (Hearst, 1992; Roth and Schulte im Walde, 2014; Schwartz et al., 2015). For distinguishing antonyms from other relations, Lin et al. (2003) proposed to use antonym patterns (such as either X or Y and from X to Y ). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) which provide a distributional representation for each term. Recently, combined path-based and distributional methods for relation detection have also been proposed (Shwartz et al., 2016; Nguyen et al., 2017). They showed that a good path representaIntroduction Identifying antonymy and expressions with contrasting meanings is valuable for NLP systems which go beyond recognizing semantic relatedness and require to identify specific semantic relations. While manually created semantic taxonomies, like WordNet (Fellbaum, 1998), define antonymy relations between some word pairs that native speakers consider antonyms, they have limited coverage. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that ar"
S17-1002,C92-2082,0,\N,Missing
S17-1009,D16-1215,1,0.895014,"Missing"
S17-1009,apidianaki-etal-2014-semantic,1,0.901008,"Missing"
S17-1009,W15-1501,0,0.0379515,"Missing"
S17-1009,W04-0807,0,0.0983441,"Missing"
S17-1009,W17-1914,1,0.830874,"Missing"
S17-1009,N16-1172,1,0.851795,"the extended synset s+i p . We calculate features that correspond to the average and maximum PPDB scores bewteen wp and lemmas in s+i p : There has been considerable research directed at expanding WordNet’s coverage either by integrating WordNet with additional semantic resources, as in Navigli and Ponzetto (2012), or by automatically adding new words and senses. In the second case, there have been several efforts specifically focused on hyponym/hypernym detection and attachment (Snow et al., 2006; Shwartz et al., 2016). There is also previous work aimed at adding semantic structure to PPDB. Cocos and Callison-Burch (2016) clustered paraphrases by word sense, effectively forming synsets within PPDB. By mapping individual paraphrases to WordNet synsets, our work could be used in coordination with these previous results in order to extend WordNet relations to the automaticallyinduced PPDB sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representati"
S17-1009,I05-5002,0,0.167329,"Missing"
S17-1009,W12-3018,0,0.0503996,"Missing"
S17-1009,N16-1163,0,0.0165124,", w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the cosine similarity between the word’s word2vec vector and the synset’s NA"
S17-1009,P16-1191,0,0.0209427,"sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the cosine similarity between t"
S17-1009,P15-1146,1,0.93301,"1998) is one of the most important resources for natural language processing research. Despite its utility, WordNet1 is manually compiled and therefore relatively small. It contains roughly 155k words, which does not approach web scale, and very few informal or colloquial words, domain-specific terms, new word uses, or named entities. Researchers have compiled several larger, automatically-generated thesaurus-like resources (Lin and Pantel, 2001; Dolan and Brockett, 2005; Navigli and Ponzetto, 2012; Vila et al., 2015). One of these is the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015b). With over 100 million paraphrase pairs, PPDB dwarfs WordNet in size but it lacks WordNet’s semantic structure. Paraphrases for a given word are indistinguishable by sense, and PPDB’s only inherent semantic relational information is predicted entailment relations between word types (Pavlick et al., 2015a). Several earlier studies attempted to incorporate se1 Our overall objective in this work is to map PPDB paraphrases for a target word to the WordNet synsets of the target. This work has two parts. In the first part (Section 4), we train and evaluate a binary lemma-synset membership classif"
S17-1009,N13-1092,1,0.832294,"Missing"
S17-1009,P15-1010,0,0.0305768,"tomaticallyinduced PPDB sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the"
S17-1009,P15-2070,1,0.905439,"Missing"
S17-1009,N15-1070,0,0.0208517,"= max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which the word and lemmas from the synset tend to appear within similar contexts. Word embeddings are real-valued vector representations of words that capture contextual information from a large corpus. Comparing the embeddings of two words is a common method for estimating their semantic similarity and relatedness. Embeddings can also be constructed to represent word senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016; Jauhar et al., 2015; Ettinger et al., 2016). CamachoCollados et al. (2016) developed compositional vector representations of WordNet noun senses – called NASARI embedded vectors – that are computed as the weighted average of the embeddings for words in each synset. They share the same embedding space as a publicly available2 set of 300-dimensional word2vec embeddings covering 300 million words (hereafter referred to as the word2vec embeddings) (Mikolov et al., 2013a,b). We calculate a distributional similarity feature for each word-synset pair by simply taking the cosine similarity between the word’s word2vec ve"
S17-1009,D16-1234,0,0.0374014,"Missing"
S17-1009,P16-1226,0,0.0495732,"Missing"
S17-1009,P06-1101,0,0.0453391,"o synset sip as follows. We call the set of all lemmas belonging to sip and any of its hypernym or hyponym synsets the extended synset s+i p . We calculate features that correspond to the average and maximum PPDB scores bewteen wp and lemmas in s+i p : There has been considerable research directed at expanding WordNet’s coverage either by integrating WordNet with additional semantic resources, as in Navigli and Ponzetto (2012), or by automatically adding new words and senses. In the second case, there have been several efforts specifically focused on hyponym/hypernym detection and attachment (Snow et al., 2006; Shwartz et al., 2016). There is also previous work aimed at adding semantic structure to PPDB. Cocos and Callison-Burch (2016) clustered paraphrases by word sense, effectively forming synsets within PPDB. By mapping individual paraphrases to WordNet synsets, our work could be used in coordination with these previous results in order to extend WordNet relations to the automaticallyinduced PPDB sense clusters. 3 xppdb.max = max P P DBScore(wp , w0 ) w0 ∈s+i p P xppdb.avg = P P DBScore(wp , w0 ) |s+i p | Distributional Similarity Our distributional similarity feature encodes the extent to which"
S17-1009,vasilescu-etal-2004-evaluating,0,0.0635538,"h˚ To compute the lexical substitutability score between a word wp and synset sip , we first retrieve example sentences e ∈ E containing t in sense sip from BabelNet v3.0 (Navigli and Ponzetto, 2012). Then, for each example e, we compute the AddCos lexical substitutability between wp and the target word in context Ce . We compute two types of this feature: The average AddCos score over all synset examples, and the maximum AddCos score over all synset examples. Lesk Similarity Among the information contained in WordNet for each synset is its definition, or gloss. The simplified Lesk algorithm (Vasilescu et al., 2004) identifies the most likely sense of a target word in context by measuring the overlap between the given context and the definition of each target sense. We use a slightly modified version of the algorithm to compute features that measure the overlap between the PPDB paraphrases for the target and the gloss of a synset. For calculating these Lesk-based features, we find synset glosses from WordNet 3.0 and from BabelNet v3.0 (Navigli and Ponzetto, 2012). First, we find D, the set of content words of the gloss for synset sip , by taking all nouns, verbs, adjectives, and adverbs that appear withi"
W03-0310,H94-1028,0,0.0728098,"Missing"
W03-0310,J93-2003,0,0.00831638,"Missing"
W03-0310,J93-2004,0,0.0296624,"Missing"
W03-0310,P00-1056,0,0.0237332,"ng source strings in six languages a greater reduction in word error rate was achieved. Our work is similar in spirit, although instead of using multi-source translation at the time of translation, we integrate it into the training stage. Whereas Och and Ney use multiple source strings to improve the quality of one translation only, our co-training method attempts to improve the accuracy of all translation models by bootstrapping more training data from multiple source documents. 3.1 Software The software that we used to train the statistical models and to produce the translations was GIZA++ (Och and Ney, 2000), the CMU-Cambridge Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), and the ISI ReWrite Decoder. The sizes of the language models used in each experiment were fixed throughout, in order to ensure that any gains that were made were not due to the trivial reason of the language model improving (which could be done by building a larger monolingual corpus of the target language). The experiments that we conducted used GIZA++ to produce IBM Model 4 translation models. It should be observed, however, that our co-training algorithm is entirely general and may be applied to any formulation o"
W03-0310,2001.mtsummit-papers.46,0,0.0882293,"through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. 3 Experimental Results In order to conduct co-training experiments we first needed to assemble appropriate corpora. The corpus used in our experiments was assembled from the data used in the (Och and Ney, 2001) multiple source translation paper. The data was gathered from the Bulletin of the European Union which is published on the Internet in the eleven official languages of the European Union. We used a subset of the data to create a multi-lingual corpus, aligning sentences between French, Spanish, German, Italian and Portuguese (Simard, 1999). Additionally we created bilingual corpora between English and each of the five languages using sentences that were not included in the multi-lingual corpus. Och and Ney (2001) used the data to find a translation that was most probable given multiple source"
W03-0310,W99-0604,0,0.0602328,"g data for a Spanish translation model contained the masculine form of a adjective, but not the feminine. Because languages vary in how they use morphology (some languages have grammatical gender whereas others don’t) one language’s translation model might have the translation of a particular word form whereas another’s would not. Thus co-training can increase the inventory of word forms and reduce the problem that morphology poses to simple statistical translation models. • improved word order – A significant source of errors in statistical machine translation is the word reordering problem (Och et al., 1999). The word order between related languages is often similar while word order between distant language may differ significantly. By including more examples through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. 3 Experimental Results In order to con"
W03-0310,2001.mtsummit-papers.68,0,0.0175181,"28.5 28 27.5 corpora for its training data. Coaching of German The performance of translation models was evaluated using a held-out set of 1,000 sentences in each language, with reference translations into English. Each translation model was used to produce translation of these sentences and the machine translations were compared to the reference human translations using word error rate (WER). The results are reported in terms of increasing accuracy, rather than decreasing error. We define accuracy as 100 minus WER. Other evaluation metrics such as position independent WER or the Bleu method (Papineni et al., 2001) could have been used. While WER may not be the best measure of translation quality, it is sufficient to track performance improvements in the following experiments. 3.3 27 10000 Evaluation 20000 25000 30000 Training Corpus Size (number of sentence pairs) 35000 40000 45.2 45 Co-training Table 1 gives the result of co-training using the most accurate translation from the candidate translations produced by five translation models. Each translation model was initially trained on bilingual corpora consisting of around 20,000 human translated sentences. These translation models were used to transla"
W03-0310,W01-0501,0,0.0422181,"Missing"
W03-0310,resnik-1998-parallel,0,0.0767729,"Missing"
W03-0310,W02-1013,0,0.0244848,"Missing"
W03-0310,P02-1040,0,\N,Missing
W03-0310,P02-1046,0,\N,Missing
W06-3123,J93-2003,0,0.0104668,"translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search 154 However, considering all possible phrases and all their possible alignments va"
W06-3123,P05-1066,1,0.176316,"hill-climbing are retained. Only a very small proportion of the alignment space can be searched and this reduces the chances of finding optimum parameters. The small number of alignments visited would lead to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 4.3 German-English submission We also submitted a German-English system using the standard approach to phrase extraction. The purpose of this submission was to validate the syntactic reordering method that we previously proposed (Collins et al., 2005). We parse the German training and test corpus and reorder it according to a set of manually devised rules. Then, we use our phrase-based system with standard phraseextraction, lexicalized reordering, lexical scoring, 5-gram LM, and the Pharaoh decoder. On the development test set, the syntactic reordering improved performance from 26.86 to 27.70. The best submission in last year’s shared task achieved a score of 24.77 on this set. 5 Conclusion We presented the first attempt at creating a systematic framework which uses word alignment constraints to guide phrase-based EM training. This shows c"
W06-3123,N03-1017,1,0.177164,"idence word alignments for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a"
W06-3123,W02-1018,0,0.2878,"the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. 1 for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation i"
W06-3123,P97-1063,0,0.0262337,"ssible alignments C, each of which is defined as the product of the probability of all individual concepts: p(F, E) = X Y p(&lt; ei , f i &gt;) (1) C∈C &lt;ei ,f i &gt;∈C The model is trained by initializing the translation table using Stirling numbers of the second kind to efficiently estimate p(&lt; ei , f i &gt;) by calculating the proportion of alignments which contain p(&lt; ei , f i &gt;) compared to the total number of alignments in the sentence (Marcu and Wong, 2002). EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). The highest probability phrase pairs are iteratively selected until all phrases are are linked. Then hill-climbing is performed by searching once for each iteration for all merges, splits, moves and swaps that improve the probability of the initial phrasal alignment. Fractional counts are collected for all alignments visited. Training the IBM models is computationally challenging, but the joint model is much more demanding. Considering all possible segmentations of phrases and all their possible alignments vastly increases the number of possible alignments that can be formed between two sent"
W06-3123,P02-1038,0,0.0182161,"er corpora. After the initialization phase of the training, all phrase pairs with counts less 156 No. Concepts BLEU Time(min) Unconstrained 6,178k 19.93 299 Constrained 1,457k 22.13 169 Table 1. The impact of constraining the joint model trained on 10,000 sentences of the German-English Europarl corpora and tested with the Europarl test set used in Koehn et al. (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. The model is also parallelized in order to speed up training. The translation models are included within a log-linear model (Och and Ney, 2002) which allows a weighted combination of features functions. For the comparison of the basic systems in Table 2 only three features were used for both the joint and the standard model: p(e|f ), p(f |e) and the language model, and they were given equal weights. The results in Table 2 show that the joint model is capable of training on large data sets, with a reasonable performance compared to the standard model. However, here it seems that the standard model has a slight advantage. This is almost certainly related to the fact that the joint model results in a much smaller phrase table. Pruning e"
W06-3123,2003.mtsummit-papers.53,0,0.0236842,"ts for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion"
W06-3123,koen-2004-pharaoh,0,\N,Missing
W06-3123,P02-1040,0,\N,Missing
W06-3123,P04-1023,1,\N,Missing
W06-3123,J04-4002,0,\N,Missing
W06-3123,W06-3105,0,\N,Missing
W06-3123,W06-3114,1,\N,Missing
W06-3123,2005.mtsummit-papers.11,1,\N,Missing
W06-3123,2005.iwslt-1.8,1,\N,Missing
W06-3123,P00-1056,0,\N,Missing
W06-3123,P03-1021,0,\N,Missing
W07-0718,W07-0735,0,0.066797,"Missing"
W07-0718,E06-1032,1,0.501959,"ve the time. 3 The judgment data along with all system translations are available at http://www.statmt.org/wmt07/ 142 4 Automatic evaluation The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality. Bleu was used exclusively since it is the most widely used metric in the field and has been shown to correlate with human judgments of translation quality in many instances (Doddington, 2002; Coughlin, 2003; Przybocki, 2004). However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006). The results of last year’s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006). We used the manual evaluation data as a means of testing the correlation of a range of automatic metrics in addition to Bleu. In total we used eleven different automatic evaluation measures to rank the shared task submissions. They are: • Meteor (Banerjee and Lavie, 2005)—Meteor measures precision and recall of unigrams when comparing a hypothesis translation Language Pair English-German German-English English-Spanish Span"
W07-0718,J96-2004,0,0.0978688,"—the inter-annotator agreement was barely considered fair, and the intra-annotator agreement was only moderate. Even when we reassessed fluency and adequacy as relative ranks the agreements increased only minimally. 6 Meta-evaluation In addition to evaluating the translation quality of the shared task entries, we also performed a “metaevaluation” of our evaluation methodologies. 6.1 Inter- and Intra-annotator agreement We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996). It is defined as P (A) − P (E) K= 1 − P (E) where P (A) is the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance. We define chance agreement for fluency and adequacy as 51 , since they are based on five point scales, and for ranking as 13 145 0.02 These timing figures are promising because they indicate that the tasks which the annotators were the most reliable on (constituent ranking and sentence ranking) were also much quicker to complete than the ones that they were unreliable on (assigning fluency and adequacy scores). This"
W07-0718,koen-2004-pharaoh,0,\N,Missing
W07-0718,H05-1100,0,\N,Missing
W07-0718,W07-0721,0,\N,Missing
W07-0718,W07-0731,0,\N,Missing
W07-0718,W07-0728,0,\N,Missing
W07-0718,W07-0738,0,\N,Missing
W07-0718,W07-0727,0,\N,Missing
W07-0718,N03-2021,0,\N,Missing
W07-0718,P05-1038,0,\N,Missing
W07-0718,P02-1040,0,\N,Missing
W07-0718,W05-0820,1,\N,Missing
W07-0718,W06-1610,0,\N,Missing
W07-0718,D07-1091,1,\N,Missing
W07-0718,W07-0730,0,\N,Missing
W07-0718,J04-4002,0,\N,Missing
W07-0718,W05-0909,0,\N,Missing
W07-0718,W07-0734,0,\N,Missing
W07-0718,2005.iwslt-1.1,0,\N,Missing
W07-0718,W07-0733,1,\N,Missing
W07-0718,W07-0707,0,\N,Missing
W07-0718,W07-0725,0,\N,Missing
W07-0718,W07-0723,0,\N,Missing
W07-0718,P05-1074,1,\N,Missing
W07-0718,W07-0732,1,\N,Missing
W07-0718,C04-1046,0,\N,Missing
W07-0718,W07-0724,0,\N,Missing
W07-0718,P05-1039,0,\N,Missing
W07-0718,N07-1006,0,\N,Missing
W07-0718,W06-3114,1,\N,Missing
W07-0718,N03-1017,1,\N,Missing
W07-0718,J03-1002,0,\N,Missing
W07-0718,P06-2003,0,\N,Missing
W07-0718,W07-0722,0,\N,Missing
W07-0718,2005.mtsummit-papers.11,1,\N,Missing
W07-0718,2006.iwslt-evaluation.1,0,\N,Missing
W07-0718,2003.mtsummit-papers.9,0,\N,Missing
W07-0718,W07-0726,0,\N,Missing
W07-0718,W07-0729,0,\N,Missing
W08-0309,W08-0312,0,0.0979584,"MT 3 f r .717 .708 .706 .704 .702 .699 .699 .695 .678 .674 .661 .654 .652 .638 .637 .633 .628 .627 .624 .616 .615 .615 .612 SAAR f r SAAR - C de RBMT 4 de CUED es RBMT 3 de CMU - SMTes UCB es LIMSI es RBMT 6 de RBMT 5 de LIMSI de LIU de SAAR de CMU - STATXFR f r UMD cz BBN - COMBO de UEDIN de MORPHOLOGIC hu DCU cz UEDIN - COMBO de UEDIN cz CMU - STATXFER de UEDIN hu .584 .574 .573 .572 .552 .548 .547 .537 .509 .493 .469 .447 .445 .444 .429 .407 .402 .387 .380 .327 .293 .280 .188 some of the allowable variation in translation. We use a single reference translation in our experiments. • Meteor (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. A number of variants are investigated here: meteor-baseline and meteorranking are optimized for correlation with adequacy and ranking judgments respectively. mbleu and mter are Bleu and TER computed using the flexible matching used in Meteor. Table 7: The average number of times that each system was judged to be better than or equal to all other systems in the sentence ranking task for the All-English condition. The subscript indicates t"
W08-0309,P05-1038,0,0.00937556,"le judge the translations of those syntactic phrases. In order to draw judges’ attention to these regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not"
W08-0309,W08-0314,0,0.0198808,"Missing"
W08-0309,W08-0321,0,0.0147788,"Missing"
W08-0309,W08-0316,0,0.0218948,"Missing"
W08-0309,W08-0319,0,0.0287247,"Missing"
W08-0309,E06-1032,1,0.371813,"ranslations are acceptable for each sentence in our test corpus. When we change our system and want to evaluate it, we do not need to manually evaluate those segments that match against the database, and could instead have people evaluate only those phrasal translations which are new. Accumulating these judgments over time would give a very reliable idea of what alternative translations were allowable. This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006). A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks. 8 Conclusions Similar to previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. One important aspect in which this year’s shared task differed from previous years was the introduction of an additional newswire test set that was different in nature to the training data. We 85 also added ne"
W08-0309,W07-0718,1,0.713541,"air that did not include English, but was not manually evaluated since it attracted minimal participation. • System combination – Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation. Three additional groups were invited to apply their system combination algorithms to all systems. Introduction This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007). There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics. There were a number of differences between this year’s workshop and last year’s workshop: • Test set selection – Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of • Refined manual evaluation – Because last year’s study indicated that fluency and adequacy judgments were slow and unreliable, we dropped them from manual evalua"
W08-0309,J96-2004,0,0.0556395,"roving the process of manual evaluation. 7.1 P (E) .333 .333 .333 .5 .5 K .367 .506 .517 .642 .649 Table 12: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation Table 11: The percent of time that each automatic metric was consistent with human judgments for translations into other languages 7 P (A) .578 .671 .678 .821 .825 Inter- and Intra-annotator agreement We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996). It is defined as P (A) − P (E) K= 1 − P (E) 83 Evaluation type Sentence ranking Constituent ranking Constituent (w/identicals) Yes/No judgments Yes/No (w/identicals) P (A) .691 .825 .832 .928 .930 P (E) .333 .333 .333 .5 .5 K .537 .737 .748 .855 .861 Table 13: Kappa coefficient values for intraannotator agreement for the different types of manual evaluation where P (A) is the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance. We define chance agreement for ranking tasks as 13 since there are three possible outcomes when ranking"
W08-0309,W08-0310,0,0.0252447,"Missing"
W08-0309,P05-1039,0,0.00794095,"ns of those syntactic phrases. In order to draw judges’ attention to these regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly corre"
W08-0309,W08-0327,1,0.775819,"Missing"
W08-0309,W08-0331,0,0.117642,"asure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: • Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples. The metric uses features derived from targetside language models and machine-generated translations (svm-pseudo-ref) as well as reference human translations (svm-human-ref). • Duh (2008) similarly used support vector machines to predict an ordering over a set of 5 We provide the scores assigned to each system by these metrics in Appendix A. Measuring system-level correlation To measure the correlation of the automatic metrics with the human judgments of translation quality at the system-level we used Spearman’s rank correlation coefficient ρ. We converted the raw scores assigned each system into ranks. We assigned a ranking to the systems for each of the three types of manual evaluation based on: • The percent of time that the sentences it produced were judged to be better th"
W08-0309,W07-0729,0,0.0315049,"Missing"
W08-0309,W08-0328,0,0.0942343,"s submitted to shared translation task. We designated the translations of the Europarl set as the development data for combination techniques which weight each system.3 CMU combined the French-English systems, BBN combined the French-English and German-English systems, and Edinburgh submitted combinations for the French-English and GermanEnglish systems as well as a multi-source system combination which combined all systems which translated from any language pair into English for the News test set. The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008). Saarland graciously provided the output of these systems, which we manually evaluated alongside all other entries. For more on the participating systems, please refer to the respective system descriptions in the proceedings of the workshop. 3 Human evaluation As with last year’s workshop, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores. It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality. Therefore, rather than select an official automatic evaluation metric like the NIST Machine"
W08-0309,W08-0332,0,0.47939,"ning how well various automatic metrics correlate with human judgments. In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments. The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words. They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translation"
W08-0309,W08-0302,0,0.0296793,"Missing"
W08-0309,P05-3026,0,0.0948555,"Missing"
W08-0309,W08-0315,0,0.0199873,"Missing"
W08-0309,W06-3114,1,0.562417,"s our first language pair that did not include English, but was not manually evaluated since it attracted minimal participation. • System combination – Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation. Three additional groups were invited to apply their system combination algorithms to all systems. Introduction This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007). There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics. There were a number of differences between this year’s workshop and last year’s workshop: • Test set selection – Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of • Refined manual evaluation – Because last year’s study indicated that fluency and adequacy judgments were slow and unreliable, we d"
W08-0309,W07-0733,1,0.572842,"a Europarl test set. The Europarl test data was again drawn from the transcripts of EU parliamentary proceedings from the fourth quarter of 2000, which is excluded from the Europarl training data. Our rationale behind investing a considerable sum to create the News test set was that we believe that it more accurately represents the quality of systems’ translations than when we simply hold out a portion of the training data as the test set, as with the Europarl set. For instance, statistical systems are heavily optimized to their training data, and do not perform as well on out-of-domain data (Koehn and Schroeder, 2007). Having both the News test set and the Europarl test set allows us to contrast the performance of systems on in-domain and out-of-domain data, and provides a fairer comparison between systems trained on the Europarl corpus and systems that were developed without it. 2.2 Provided materials To lower the barrier of entry for newcomers to the field, we provided a complete baseline MT system, along with data resources. We provided: • • • • sentence-aligned training corpora language model data development and dev-test sets Moses open source toolkit for phrase-based statistical translation (Koehn et"
W08-0309,N03-1017,1,0.00762077,"ions. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase. We noted this in the instructions to judges: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed). Grade only the highlighted part of each translation. Please note that segments are selected automatically, and they should be taken as an approximate guide. They might include extra words that are not in the actual alignment, or miss wor"
W08-0309,W08-0329,0,0.0334792,"Missing"
W08-0309,W08-0320,0,0.0114639,"Missing"
W08-0309,W08-0313,0,0.036043,"Missing"
W08-0309,W08-0323,0,0.0290179,"Missing"
W08-0309,W08-0317,0,0.0278407,"Missing"
W08-0309,W08-0311,0,0.034369,"Missing"
W08-0309,W08-0326,0,0.025716,"Missing"
W08-0309,J03-1002,0,0.00405346,"regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase. We noted this in the inst"
W08-0309,P03-1021,0,0.0375741,"aluate those segments that match against the database, and could instead have people evaluate only those phrasal translations which are new. Accumulating these judgments over time would give a very reliable idea of what alternative translations were allowable. This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006). A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks. 8 Conclusions Similar to previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. One important aspect in which this year’s shared task differed from previous years was the introduction of an additional newswire test set that was different in nature to the training data. We 85 also added new language pairs to our evaluation: Hungarian-English and German-Spanish. As in previous years we were pleased to notice an inc"
W08-0309,P02-1040,0,0.12021,"nt systems. In particular, it is especially useful for validating the automatic metrics which are frequently used by the machine translation research community. We continued the shared task which we debuted last year, by examining how well various automatic metrics correlate with human judgments. In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments. The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than"
W08-0309,W07-0707,0,0.0613323,"The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words. They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: • Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples. The metric use"
W08-0309,P07-1040,0,0.0268048,"at indicated which language the system was originally translating from. This entry was part of ongoing research in multi-lingual, multisource translation. Since there was no official multilingual system combination track, this entry should be viewed only as a contrastive data point. Table 4: Summary results for the sentence ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task. was the University of Edinburgh’s system combination entry. It uses a technique similar to Rosti et al. (2007) to perform system combination. Like the other system combination entrants, it was tuned on the Europarl test set and tested on the News test set, using systems that submitted entries to both tasks. BBN - COMBO CMU - COMBO CMU - GIMPEL CMU - SMT CMU - STATXFER CU - BOJAR CU - TECTOMT CUED CUED - CONTR DCU LIMSI LIU LIUM - SYSTRAN LIUM - SYS - CONTR MORPHOLOGIC PC - TRANSLATOR RBMT 2 RBMT 3 RBMT 4 RBMT 5 RBMT 6 SAAR SAAR - CONTR SYSTRAN UCB UCL UEDIN UEDIN - COMBO UMD UPC UW XEROX Czech-English Commentary Czech-English News English-Czech Commentary English-Czech News English-French Europarl Eng"
W08-0309,2004.tmi-1.8,0,\N,Missing
W08-0309,W08-0322,0,\N,Missing
W08-0309,W08-0318,1,\N,Missing
W08-0309,W08-0324,0,\N,Missing
W08-0309,W08-0325,0,\N,Missing
W08-0309,P07-1111,0,\N,Missing
W08-0309,P07-1038,0,\N,Missing
W08-0309,W08-0330,0,\N,Missing
W08-0309,2005.eamt-1.20,0,\N,Missing
W08-0309,D08-1076,0,\N,Missing
W08-2006,C04-1046,0,0.0141334,", j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC (i, j) is a function of the commute time between vertex i and vertex j. 4 Graph construction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we consider uniform weights on all edges as our main aim is to illustrate the different random walk measures rather than fine tune the gr"
W08-2006,J06-1003,0,0.119811,"sures • Propose a new similarity measure based on commute time. • An improvement to the above measure by eliminating noisy features via singular value decomposition. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 41 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41–48 Manchester, August 2008 3 Problem setting problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the human judgements using the Spearm"
W08-2006,P07-1054,0,0.0237167,"Time Given a graph with the transition probability matrix P as defined above, the hitting time between vertices i and j, denoted as h(i, j), is defined as the expected number of steps taken by a random walker to first encounter vertex j starting from vertex i. This can be recursively defined as follows: X  pik h(k, j) if i 6= j  1+ h(i, j) = k : wik > 0  0 if i = j (1) 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be derived as follows: 4 The Jensen-Shannon divergence between two distributions p and q is defined as D(p k a) + D(q k a), where D(. k .) is the Kullback-Liebler divergence and a = (p + q)/2. Note that unlike KL-divergence this measure is symmetric. See (Lin,"
W08-2006,D07-1061,0,0.274246,"choice of m depends on Armed with the stationary distribution vectors for vertices i and j, we define pagerank similarity either as the cosine of the stationary distribution vectors or the reciprocal Jensen-Shannon (JS) divergence4 between them. Table 3. shows results on the Miller-Charles data. We use α = 0.1, the best value on this data. Observe that these results are Method Pagerank JS-Divergence Pagerank Cosine Spearman correlation 0.379 0.393 Table 3: Similarity via pagerank (α = 0.1). better than the best bounded walk result. We further note that our results are different from that of (Hughes and Ramage, 2007) as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce. Hence for simplicity we stuck to a simpler graph generation process. Nevertheless, the result in Table 3. is still useful as we are interested in the performance of the various spectral similarity measures rather than achieving the best performance on the lexical relatedness task. The graphs we use in all methods are identical making comparisons across methods possible. Figure 2: Effect of m in Bounded walk the amount of computation available. A reasonably l"
W08-2006,N06-1058,0,0.0211135,"of its hyponyms – For each word sense, add edges to all of its hypernyms recursively. • simP (i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC (i, j) is a function of the commute time between vertex i and vertex j. 4 Graph construction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we consider uniform weights on all edges as our m"
W08-2006,P05-3019,0,0.0147224,"– For each word sense, add edges to all of its hyponyms – For each word sense, add edges to all of its hypernyms recursively. • simP (i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC (i, j) is a function of the commute time between vertex i and vertex j. 4 Graph construction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we con"
W08-2006,N04-3012,0,0.0193334,"rious random walk based measures • Propose a new similarity measure based on commute time. • An improvement to the above measure by eliminating noisy features via singular value decomposition. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 41 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41–48 Manchester, August 2008 3 Problem setting problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the h"
W08-2006,P93-1024,0,0.265483,"nstruction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we consider uniform weights on all edges as our main aim is to illustrate the different random walk measures rather than fine tune the graph construction process. 6 Shortest path based measure The most obvious measure of distance in a graph is the shortest path between the vertices which is defined as the minimum number of"
W09-0401,W08-0312,0,0.0796143,"calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation. We use a single reference translation in our experiments. • ULC (Gim´enez and M`arquez, 2008)—ULC is an arithmetic mean over other automatic metrics. The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations. The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008). • Meteor (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new"
W09-0401,1998.amta-tutorials.1,0,0.0723526,"Missing"
W09-0401,W09-0426,0,0.0294171,"Missing"
W09-0401,P07-1038,0,0.0331701,"Missing"
W09-0401,W09-0417,0,0.033793,"Missing"
W09-0401,W09-0411,0,0.034886,"Missing"
W09-0401,W07-0718,1,0.791479,"Missing"
W09-0401,W09-0420,0,0.0243178,"Missing"
W09-0401,W08-0309,1,0.578575,"est sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation. All of the data, translations, and human judgments produced for our workshop are publicly available.1 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008). There were three shared tasks this year: a translation task between English and five other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance on each of these shared task was determined after a comprehensive human evaluation. There were a number of differences between this year’s workshop and last year’s workshop: 2 Overview of the shared translation and system combination tasks The workshop examined translation between English and five other"
W09-0401,W09-0408,0,0.0416758,"Missing"
W09-0401,W09-0416,0,0.030911,"Missing"
W09-0401,W09-0406,0,0.0789703,"Missing"
W09-0401,W09-0419,1,0.738361,"Missing"
W09-0401,W09-0413,0,0.0338062,"Missing"
W09-0401,W09-0428,0,0.0340245,"Missing"
W09-0401,2004.tmi-1.8,0,0.00591015,"score to the higher ranked system). We divided this by the total number of pairwise comparisons to get a percentage. Because the systems generally assign real numbers as scores, we excluded pairs that the human annotators ranked as ties. Average terp ter bleusp4114 bleusp bleu bleu (cased) bleu-ter/2 wcd6p4er nist (cased) nist wpF wpbleu en-cz (5 systems) Because the sentence-level judgments collected in the manual evaluation are relative judgments rather than absolute judgments, it is not possible for us to measure correlation at the sentencelevel in the same way that previous work has done (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b). en-es (11 systems) Measuring sentence-level consistency en-fr (16 systems) 5.2 Average where di is the difference between the rank for systemi and n is the number of systems. The possible values of ρ range between 1 (where all systems are ranked in the same order) and −1 (where the systems are ranked in the reverse order). Thus an automatic evaluation metric with a higher absolute value for ρ is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute ρ. hu-en (6 systems) 6 d2i"
W09-0401,W09-0404,0,0.0128365,"-S (Lin and Och, 2004), with tuned n-gram weights, and bleusp, with constant weights. wcd6p4er is an error measure and bleusp is a quality score. In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating the automatic evaluation metrics. Last year, NIST began running a similar “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). In this year’s shared task we evaluated a number of different automatic metrics: • RTE (Pado et al., 2009)—The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation. Its predictions are based on a regression model over a feature set adapted from an entailment systems. The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures. • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in"
W09-0401,P02-1040,0,0.123715,"ing a similar “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). In this year’s shared task we evaluated a number of different automatic metrics: • RTE (Pado et al., 2009)—The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation. Its predictions are based on a regression model over a feature set adapted from an entailment systems. The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures. • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation. We use a single reference translation in our experiments. • ULC (Gim´enez and M`arquez, 2008)—ULC is an arithmetic mean over other automatic metrics. The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations. The ULC metric had the strongest correlati"
W09-0401,W09-0407,0,0.0551118,"Missing"
W09-0401,W09-0418,0,0.024243,"Missing"
W09-0401,W09-0424,1,0.0806642,"Missing"
W09-0401,W09-0402,0,0.0228572,"dNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new version which introduces a number of different features, and (Bleu − TER)/2, a combination of Bleu and Translation Edit Rate. • wpF and wpBleu (Popovic and Ney, 2009) These metrics are based on words and part of speech sequences. wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams. wpBLEU is a combnination of the normal Blue score and a part of speech-based Bleu score. • SemPOS (Kos and Bojar, 2009) – the SemPOS metric computes overlapping words, as defined in (Gim´enez and M`arquez, 2007), with respect to their semantic part of speech. Moreover, it does not use the surface representation of words but their underlying forms obtained from the TectoMT framework. • MaxSim (Chan and Ng, 2008)—MaxSim calculates"
W09-0401,E09-1082,1,0.692929,"ing the secondary system submissions. Baseline system To lower the barrier of entry for newcomers to the field, we provided Moses, an open source toolkit for phrase-based statistical translation (Koehn et al., 2007). The performance of this baseline system is similar to the best submissions in last year’s shared task. Twelve participating groups used the Moses toolkit for the development of their system. 2.4 System combination In addition to soliciting system combination entries for each of the language pairs, we treated system combination as a way of doing multi-source translation, following Schroeder et al. (2009). For the multi-source system combination task, we provided all 46 primary system submissions from any language into English, along with an additional 32 secondary systems. Table 2 lists the six participants in the system combination task. Submitted systems We received submissions from 22 groups from 20 institutions, as listed in Table 1, a similar turnout to last year’s shared task. Of the 20 groups that participated with regular system submissions in last year’s shared task, 12 groups returned this year. A major hurdle for many was a DARPA/GALE evaluation that occurred at the same time as th"
W09-0401,W09-0423,0,0.0383252,"Missing"
W09-0401,2006.amta-papers.25,0,0.223011,"Missing"
W09-0401,W09-0441,0,0.0797267,"or (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new version which introduces a number of different features, and (Bleu − TER)/2, a combination of Bleu and Translation Edit Rate. • wpF and wpBleu (Popovic and Ney, 2009) These metrics are based on words and part of speech sequences. wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams. wpBLEU is a combnination of the normal Blue score and a part of speech-based Bleu score. • SemPOS (Kos and Bojar, 2009) – the SemPOS metric computes overlapping words, as defined in (Gim´enez and M`arquez, 2007), with respect to their semantic part of speech."
W09-0401,D07-1049,0,0.0154831,"systems evaluated in this workshop, since they were trained using only the provided materials. In addition to cleaning the sentence-aligned parallel corpus we also de-duplicated the corpus, removing all sentence pairs that occured more than once in the parallel corpus. Many of the documents gathered in our web crawl were duplicates or near duplicates, and a lot of the text is repeated, as with web site navigation. We further eliminated sentence pairs that varied from previous sentences by only numbers, which helped eliminate template web pages such as expense reports. We used a Bloom Filter (Talbot and Osborne, 2007) to do de-duplication, so it may have discarded more sentence pairs than strictly necessary. After deduplication, the parallel corpus contained 28 million sentence pairs with 0.8 billion French words and 0.7 billion English words. 2.5 In total, we received 87 primary system submissions along with 42 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we received on last year’s system combination task, we provided two additional resources to participants: Monolingual news corpora We have crawled the news sources that wer"
W09-0401,moore-2002-fast,0,\N,Missing
W09-0401,W09-0422,0,\N,Missing
W09-0401,W09-0421,0,\N,Missing
W09-0401,W07-0738,0,\N,Missing
W09-0401,W09-0427,0,\N,Missing
W09-0401,W08-0332,0,\N,Missing
W09-0401,W09-0425,0,\N,Missing
W09-0401,W09-0429,1,\N,Missing
W09-0401,P04-1077,0,\N,Missing
W09-0401,W09-0410,0,\N,Missing
W09-0401,W09-0415,0,\N,Missing
W09-0401,P07-1111,0,\N,Missing
W09-0401,W09-0412,0,\N,Missing
W09-0401,W06-3114,1,\N,Missing
W09-0401,W09-0414,0,\N,Missing
W09-0401,W10-1720,0,\N,Missing
W09-0401,W09-0405,0,\N,Missing
W09-0401,W09-0409,0,\N,Missing
W09-0424,P05-1032,1,0.752623,"s also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f ) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all rules from the training set. The current code requires suffix array rule extraction to be run as a pre-processing step to extract the rules needed to translate a particular test set. However, we are currently extending the decoder to directly access the suffix array. This will allow the decoder at runtime to efficiently extract exactly those rules needed to tra"
W09-0424,W08-0309,1,0.277708,"lable as a standalone application, Z-MERT, that can be used with other MT systems. (Software and documentation at: http://cs.jhu.edu/ ˜ozaidan/zmert.) 137 System Joshua Baseline Minimum Bayes Risk Rescoring Deterministic Annealing Variational Decoding of 21.2 million English sentences with half a billion words. We used SRILM to train a 5-gram language model using a vocabulary containing the 500,000 most frequent words in this corpus. Note that we did not use the English side of the parallel corpus as language model training data. To tune the system parameters we used News Test Set from WMT08 (Callison-Burch et al., 2008), which consists of 2,051 sentence pairs with 43 thousand English words and 46 thousand French words. This is in-domain data that was gathered from the same news sources as the WMT09 test set. 3.2 BLEU-4 25.92 26.16 25.98 26.52 Table 1: The uncased BLEU scores on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation. of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In t"
W09-0424,D07-1104,0,0.412715,"e. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of"
W09-0424,J07-2003,0,0.947277,"ovided as an abstract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and"
W09-0424,moore-2002-fast,0,0.0291243,"d the Olympic Committee. The crawl gathered approximately 40 million files, consisting of over 1TB of data. We converted pdf, doc, html, asp, php, etc. files into text, and preserved the directory structure of the web crawl. We wrote set of simple heuristics to transform French URLs onto English URLs, and considered matching documents to be translations of each other. This yielded 2 million French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section"
W09-0424,P03-2041,0,0.0958636,"For our submission, we used k = 20, which resulted in 1.5 million (out of 23 million) sentence pairs being selected for use as training data. There were 30,037,600 English words and 30,083,927 French words in the subsampled training corpus. 2.2 2.3 Decoding Algorithms2 Grammar formalism: Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Currently, it only handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs (e.g., (Galley et al., 2006)) and closely related formalisms like synchronous tree substitution grammars (Eisner, 2003). Chart parsing: Given a source sentence to decode, the decoder generates a one-best or k-best translations using a CKY algorithm. Specifically, the decoding algorithm maintains a chart, which contains an array of cells. Each cell in turn maintains a list of proven items. The parsing process starts with the axioms, and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translat"
W09-0424,J03-1002,0,0.00743813,"es on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation. of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. More details will be provided in Li et al. (2009b). In this system, we have used both deterministic annealing (for training) and variational decoding (for decoding). Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objecti"
W09-0424,P06-1121,0,0.562847,"ize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms des"
W09-0424,P03-1021,0,0.692054,"other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two direct"
W09-0424,P05-1034,0,0.103948,"stract class to minimize the work necessary for new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the es"
W09-0424,W05-1506,0,0.084474,"ls on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivations from the hypergraph. Parallel and distributed decoding: We also implement parallel decoding and a distributed language model by exploiting multi-core and multi-processor architectures and distributed computing techniques. More details on these two features are provided by Li and Khudanpur (2008b). 2.4 updates, each reflecting a greedy selection of the dimension giving the most gain. Each iteration also optimizes several random “intermediate initial” points in addition to the one surviving from the previous iteration, as an approximation to performing"
W09-0424,P06-2101,0,0.0477005,"raining (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness 4 C"
W09-0424,N03-1017,0,0.0293738,"oaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the toolkit are at http://cs.jhu. edu/˜ccb/joshua. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135–139, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 135 In such tasks, feature calculation is also very expensive in terms of time required; huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability p(f |e) and reverse translation probability p(e|f ) (Koehn et al., 2003). Since the extraction steps must be re-run if any change is made to the input training data, the time required can be a major hindrance to researchers, especially those investigating the effects of tokenization or word segmentation. To alleviate these issues, we extract only a subset of all available rules. Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences. This results in a vastly smaller rule set than techniques which extract all ru"
W09-0424,P07-2045,1,0.0364964,"well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 1 The toolkit can be downloaded at http://www. sourceforge.net/projects/joshua, and the instructions in using the to"
W09-0424,P07-1065,0,0.0441818,"distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring function in Java. This Java implementation is able to read the standard ARPA backoff n-gram models, and thus the decoder can be used independently from the SRILM toolkit.3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n-grams. This native implementation is more scalable than the basic Java LM implementation. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). 2.5 WMT-09 Translation Task Results Minimum Error Rate Training Johsua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu. The optimization consists of a series of line-optimizations along the dimensions corresponding to the parameters. The search across a dimension uses the efficient method of Och (2003). Each iteration of our MERT implementation consists of multiple weight 3 This feature allows users to easily try the Joshua toolkit without installing the SRILM toolkit and compiling the n"
W09-0424,2005.mtsummit-papers.11,0,0.0944271,"llion French documents paired with their English equivalents. We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002). We filtered and de-duplcated the resulting parallel corpus. After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words. We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words. Our translation model was trained on these corpora using the subsampling descried in Section 2.1. For language model training, we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09. This data consisted Language Models In addition to the distributed LM mentioned above, we implement three local n-gram language models. Specifically, we first provide a straightforward implementation of the n-gram scoring func"
W09-0424,N04-1022,0,0.0715322,"r training) and variational decoding (for decoding). Translation Scores The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumar and Byrne, 2004). We re-score the top 300 translations to minimize expected loss under the Bleu metric. Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Variational Decoding: Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct deri"
W09-0424,2008.amta-papers.12,1,0.948747,"dan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD + Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany ? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.1 We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieve"
W09-0424,W08-0402,1,0.946992,"dan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD + Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany ? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract 2008b). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a). We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.1 We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieve"
W09-0424,W09-0424,1,0.108172,", and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. 2 More details on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivati"
W09-0424,P09-1067,1,0.881207,", and proceeds by applying the inference rules repeatedly to prove new items until proving a goal item. Whenever the parser proves a new item, it adds the item to the appropriate chart cell. The item also maintains backSuffix-array Grammar Extraction Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values. In real translation tasks, the grammars extracted from large training corpora are often far too large to fit into available memory. 2 More details on the decoding algorithms are provided in (Li et al., 2009a). 136 pointers to antecedent items, which are used for k-best extraction. Pruning: Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large target-language vocabularies. In our decoder, we incorporate two pruning techniques: beam and cube pruning (Chiang, 2007). Hypergraphs and k-best extraction: For each source-language sentence, the chart-parsing algorithm produces a hypergraph, which represents an exponential set of likely derivation hypotheses. Using the k-best extraction algorithm (Huang and Chiang, 2005), we extract the k most likely derivati"
W09-0424,P06-1077,0,0.253036,"new extensions. End-to-end Cohesion: There are many components to a machine translation pipeline. One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements. This leads to a large investment in scripts to convert formats and connect the different components, and often leads to untenable and non-portable projects as well as hinderIntroduction Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al. (2006), and Liu et al. (2006)) has made remarkable progress in the last few years. However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source. This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007)"
W09-0424,D08-1076,0,\N,Missing
W09-1117,W06-2920,0,0.0111348,"tive model from monolingual corpora and a seed lexicon. All of the aforementioned work defines context similarity in terms of the adjacent words over a window of some arbitary size (usually 2 to 4 words), as initially proposed by Rapp (1999). We show that the model for surrounding context can be improved by using dependency information rather than strictly relying on adjacent words, based on the success of dependency trees for monolingual clustering and disambiguation tasks (Lin and Pantel, 2002; Pado and Lapata, 2007) and the recent developments in multilingual dependency parsing literature (Buchholz and Marsi, 2006; Nivre et al., 2007). We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types, rather than just nouns. While the straightforward application of context-based model gives a lower overall accuracy than nouns alone, we show how learning a mapping of part-of-speech tagsets between the source and target language can result in comparable performance to that of noun translation. 3 Translation by Context Vector Projection This section details how translations are discovered from monolingual corpora through conte"
W09-1117,C02-1011,0,0.0213356,"Missing"
W09-1117,J07-2002,0,0.0141048,"text. Haghighi et al., (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. All of the aforementioned work defines context similarity in terms of the adjacent words over a window of some arbitary size (usually 2 to 4 words), as initially proposed by Rapp (1999). We show that the model for surrounding context can be improved by using dependency information rather than strictly relying on adjacent words, based on the success of dependency trees for monolingual clustering and disambiguation tasks (Lin and Pantel, 2002; Pado and Lapata, 2007) and the recent developments in multilingual dependency parsing literature (Buchholz and Marsi, 2006; Nivre et al., 2007). We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types, rather than just nouns. While the straightforward application of context-based model gives a lower overall accuracy than nouns alone, we show how learning a mapping of part-of-speech tagsets between the source and target language can result in comparable performance to that of noun translation. 3 Translation by Context Vector Pr"
W09-1117,J07-2003,0,0.021105,"Missing"
W09-1117,P99-1067,0,0.83134,"h Garera, Chris Callison-Burch, David Yarowsky Department of Computer Science, Johns Hopkins University Baltimore MD, USA {ngarera,ccb,yarowsky}@cs.jhu.edu Abstract researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by exploiting the cross-language evidence of closely related “bridge” languages that have more resources (Mann and Yarowsky, 2001). This paper investigates new ways of learning translations from monolingual corpora. We extend the Rapp (1999) model of context vector projection using a seed lexicon. It is based on the intuition that translations will have similar lexical context, even in unrelated corpora. For example, in order to translate the word “airplane”, the algorithm builds a context vector which might contain terms such as “passengers”, “runway”, “airport”, etc. and words in target language that have their translations (obtained via seed lexicon) in surrounding context can be considered as likely translations. We extend the basic approach by formulating a context model that uses dependency trees. The use of dependencies ha"
W09-1117,P98-1069,0,0.448521,"Missing"
W09-1117,P08-1088,0,0.665773,"the board, and better performance than statistical translation models on Top-10 accuracy for noun translation when trained on identical data. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129–137, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). 2 Related Work The literature on translation lexicon induction for low-density languages falls in to two broad categories: 1) Effectively utilizing similarity between languages by choosing a high-resource “bridge” language for translation (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002) and 2) Extracting noisy clues (such as similar context) from monolingual corpora with help of a seed lexicon (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002, Haghighi et al., 2008). The latter category is more relevant to this work and is explained in detail below. The idea of words with"
W09-1117,W02-0902,0,0.498206,"ne context models across the board, and better performance than statistical translation models on Top-10 accuracy for noun translation when trained on identical data. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129–137, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). 2 Related Work The literature on translation lexicon induction for low-density languages falls in to two broad categories: 1) Effectively utilizing similarity between languages by choosing a high-resource “bridge” language for translation (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002) and 2) Extracting noisy clues (such as similar context) from monolingual corpora with help of a seed lexicon (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002, Haghighi et al., 2008). The latter category is more relevant to this work and is explained in detail below."
W09-1117,N03-1017,0,0.011337,"Missing"
W09-1117,P07-2045,1,0.00989501,"glish and English-to-Spanish) using their sum as the final translation score. We contrasted the accuracy of the above methods, which use monolingual corpora, with a statistical camino Depposn Cntxt Model Adjbow Cntxt Model way 0.124 intentions 0.22 solution 0.097 way 0.21 steps 0.094 idea 0.20 path 0.093 thing 0.20 debate 0.085 faith 0.18 account 0.082 steps 0.17 means 0.080 example 0.17 work 0.079 news 0.16 approach 0.074 work 0.16 issue 0.073 attitude 0.15 model trained on bilingual parallel corpora. We refer to that model as Mosesen-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007). 4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus"
W09-1117,2005.mtsummit-papers.11,0,0.0135649,"4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). The fact that our two monolingual corpora are taken from a parallel corpus ensures that the assumption that similar contexts are a good indicator of translation holds. This assumption underlies in all work of translation lexicon induction from comparable monolingual corpora, and here we strongly bias toward that assumption. Despite the bias, the comparison of different context models holds, since all models are trained on the same data. 4.2 Evaluation Criterion The models were evaluated in terms of exact-match translation accuracy of the 1000 most frequent nouns in a English-Spanish dictiona"
W09-1117,N01-1020,1,0.834359,"proving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences Nikesh Garera, Chris Callison-Burch, David Yarowsky Department of Computer Science, Johns Hopkins University Baltimore MD, USA {ngarera,ccb,yarowsky}@cs.jhu.edu Abstract researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by exploiting the cross-language evidence of closely related “bridge” languages that have more resources (Mann and Yarowsky, 2001). This paper investigates new ways of learning translations from monolingual corpora. We extend the Rapp (1999) model of context vector projection using a seed lexicon. It is based on the intuition that translations will have similar lexical context, even in unrelated corpora. For example, in order to translate the word “airplane”, the algorithm builds a context vector which might contain terms such as “passengers”, “runway”, “airport”, etc. and words in target language that have their translations (obtained via seed lexicon) in surrounding context can be considered as likely translations. We"
W09-1117,J93-2004,0,0.0431806,"le 0.17 work 0.079 news 0.16 approach 0.074 work 0.16 issue 0.073 attitude 0.15 model trained on bilingual parallel corpora. We refer to that model as Mosesen-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007). 4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). The fact that our two monolingual corpora are taken from a parallel corpus ensures that the assumption that similar contexts are a good indicator of translation holds. This assumption underlies in all work of translation lexicon induction from comparable monolingual corpora, and here we strongly bias toward that assumption. Despite the bias, the comparison of differ"
W09-1117,H05-1066,0,0.00505969,"4 idea 0.20 path 0.093 thing 0.20 debate 0.085 faith 0.18 account 0.082 steps 0.17 means 0.080 example 0.17 work 0.079 news 0.16 approach 0.074 work 0.16 issue 0.073 attitude 0.15 model trained on bilingual parallel corpora. We refer to that model as Mosesen-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007). 4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). The fact that our two monolingual corpora are taken from a parallel corpus ensures that the assumption that similar contexts are a good indicator of translation holds. This assumption underlies in all work of translation lexicon induction from comparable monolingual cor"
W09-1117,W02-2026,1,0.863378,"ssociation for Computational Linguistics We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). 2 Related Work The literature on translation lexicon induction for low-density languages falls in to two broad categories: 1) Effectively utilizing similarity between languages by choosing a high-resource “bridge” language for translation (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002) and 2) Extracting noisy clues (such as similar context) from monolingual corpora with help of a seed lexicon (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002, Haghighi et al., 2008). The latter category is more relevant to this work and is explained in detail below. The idea of words with similar meaning having similar contexts in the same language comes from the Distributional Hypothesis (Harris, 1985) and Rapp (1999) was the first to propose using context of a given word as a clue to its translation. Given a German word with an unknown translation, a German context vector is"
W09-1117,C98-1066,0,\N,Missing
W09-1117,D07-1096,0,\N,Missing
W10-0701,W10-0731,0,0.0359833,"Missing"
W10-0701,W10-0710,0,0.167215,"rformance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasi"
W10-0701,2009.iwslt-evaluation.16,0,0.0223923,"lable for many users. Learning across multiple annotations may improve systems (Dredze et al., 2009). Additionally, even with efforts to clean up MTurk annotations, we can expect an increase in noisy examples in data. This will push for new more robust learning algorithms that are less sensitive to noise. If we increase the size of the data ten-fold but also increase the noise, can learning still be successful? Another learning area of great interest is active learning, which has long relied on simulated user experiments. New work evaluated active learning methods with real users using MTurk (Baker et al., 2009; Ambati et al., 2010; Hsueh et al., 2009; ?). Finally, the composition of complex data set annotations from simple user inputs can transform the method by which we learn complex outputs. Current approaches expect examples of labels that exactly match the expectation of the system. Can we instead provide lower level simpler user annotations and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideas come from unexpected places. Major transformations in the field have come from crea"
W10-0701,P07-1056,1,0.0643466,"data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is made when it can be demonstrated against previous approaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessible data. More than a new source for cheap data, MTurk is a source for new types of dat"
W10-0701,P10-1088,1,0.0566792,"nds on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for hig"
W10-0701,W10-0733,1,0.257767,"nds on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for hig"
W10-0701,W09-2416,0,0.0225701,"Missing"
W10-0701,W10-0735,0,0.0184231,"Missing"
W10-0701,D09-1030,1,0.324286,"ld standard data, or by stating what controls you used and what criteria you used to block bad Turkers. Finally, whenever possible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced effo"
W10-0701,W10-0720,0,0.0173225,"Missing"
W10-0701,W10-0709,0,0.0268793,"Missing"
W10-0701,W10-0711,0,0.0259705,"Missing"
W10-0701,W10-0708,0,0.0576436,"Missing"
W10-0701,W10-0704,0,0.0105147,"Translation (MT). MT is a data hungry task that relies on huge corpora of parallel texts between two languages. Performance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 trans"
W10-0701,W10-0722,0,0.0950833,"Missing"
W10-0701,W10-0724,0,0.0386468,"Missing"
W10-0701,W10-0732,1,0.52444,"s on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8 http://www.ischool.utexas.edu/˜cse2010/ call.htm 6.5 Information Extraction Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation types. The goal of relation extraction is to identify relations between entities or terms in a sentence, such as born in or religion. Gormley et al. (2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisg"
W10-0701,W10-0727,0,0.020468,"t automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al. (2010) evaluated the political bias of blog posts. Annotators labeled 1000 sentences to determine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval Large scale evaluations requiring significant human labor for evaluation have a long history in the information retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negative results on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8 http://www.ischool.utexas.edu/˜cse2010/ call.htm 6.5 Information Extraction Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation typ"
W10-0701,W10-0705,0,0.0183737,"ided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments. In the area of natural language generation, Heilman and Smith (2010) explored the potential of MTurk for ranking of computer generated questions about provided texts. These questions can be used to test reading comprehension and understanding. 60 Wikipedia articles were selected, for each of which 20 questions were generated. Turkers provided 5 ratings for each of the 1,200 questions, creating a significant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the K NEXT knowledge extr"
W10-0701,W10-0714,0,0.0071973,"ing specific demographics in data creation. Beyond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of data type and its impact on machine learning algorithms. With professional annotators, great time and care are paid to annotation guidelines and annotator training. These are difficult tasks with MTurk, which favors simple intuitive annotations"
W10-0701,W09-1904,0,0.0392972,"nical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose"
W10-0701,W10-0717,0,0.069042,"e for new types of data. Several of the papers in this workshop collected information about the annotators in addition to their annotations. This creates potential for studying how different user demographics understand language and allow for tar8 geting specific demographics in data creation. Beyond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of"
W10-0701,W10-0702,0,0.0297313,"Missing"
W10-0701,kaisser-lowe-2008-creating,0,0.00454559,"edu/uid/turkit/ inter-annotator agreement of the Turkers against experts on small amounts of gold standard data, or by stating what controls you used and what criteria you used to block bad Turkers. Finally, whenever possible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Cal"
W10-0701,W10-0726,0,0.0108165,"nowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, 6 allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required"
W10-0701,W10-0729,0,0.207466,"rated. Turkers provided 5 ratings for each of the 1,200 questions, creating a significant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the K NEXT knowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator"
W10-0701,W10-0712,0,0.0346725,"Missing"
W10-0701,W10-0730,0,0.0101152,"ntailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams expl"
W10-0701,W10-0716,0,0.138372,"k (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). B"
W10-0701,W10-0718,0,0.0345404,"Missing"
W10-0701,W10-0719,0,0.107949,"Missing"
W10-0701,W10-0734,0,0.0442928,"ing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7 http://sites.google.com/site/ amtworkshop2010/ recognizing textual entailment (RTE). They submitted 600 text segments and asked Turkers to identify facts and counter-facts (unsupported facts and contradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments. In the area of natural language generation, Heilman and Smith (2010) explored the potential of MTurk for ranking of computer generated questions"
W10-0701,N10-1024,1,0.504587,"scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows resear"
W10-0701,W10-0706,1,0.771093,"scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows resear"
W10-0701,W02-1011,0,0.0149006,"l exists for a change in focus in a number of ways. In natural language processing, data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is made when it can be demonstrated against previous approaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessib"
W10-0701,W10-0703,0,0.0141545,"demonstrates the potential for MTurk’s impact on the creation and curation of speech and language corpora. 6.1 Traditional NLP Tasks An established core set of computational linguistic tasks have received considerable attention in the natural language processing community. These include knowledge extraction, textual entailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portraye"
W10-0701,W09-3607,0,0.0112431,"’s authors is a strong advocate of such a position while the other disagrees, perhaps because he himself works on unsupervised methods. Certainly, we can agree that the potential exists for a change in focus in a number of ways. In natural language processing, data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is m"
W10-0701,W10-0721,0,0.0104382,"Missing"
W10-0701,W03-0419,0,0.0250001,"Missing"
W10-0701,D08-1027,0,0.23451,"Missing"
W10-0701,W10-0707,0,0.0217939,"r, 6 allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required Turkers to record audio files and upload them. An additional HIT was used to evaluate the quality of the narrations. A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing (Tong et al., 2010). Turkers were asked to submit handwritten shopping lists (large vocabulary) or weather descriptions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and a translation. The team collected 18 images per language, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias Two papers investigated the topics of sentiment, polarity and bias. Mellebeek et al. (2010) used several methods to obtain polarity scores for Spanish sentences expressing opinions about automative topics. They evaluated three HITs for collecting such da"
W10-0701,W10-0725,1,0.613046,"tivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7 http://sites.google.com/site/ amtworkshop2010/ recognizing textual entailment (RTE). They submitted 600 text segments and asked Turkers to identify facts and counter-facts (unsupported facts and contradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-"
W10-0701,W10-0723,0,0.00787248,"s (large vocabulary) or weather descriptions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and a translation. The team collected 18 images per language, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias Two papers investigated the topics of sentiment, polarity and bias. Mellebeek et al. (2010) used several methods to obtain polarity scores for Spanish sentences expressing opinions about automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al. (2010) evaluated the political bias of blog posts. Annotators labeled 1000 sentences to determine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval Large scale evaluations requiring significant human labor for evaluation have a long history in the information retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negat"
W10-0701,W10-0728,0,0.0517069,"(2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisgen-Yildiz et al. (2010) explored medical named entity recognition. They selected 100 clinical trial announcements from ClinicalTrials.gov. 4 annotators for each of the 100 announcements identified 3 types of medical entities: medical conditions, medications, and laboratory test. 6.6 Machine Translation The most popular shared task topic was Machine Translation (MT). MT is a data hungry task that relies on huge corpora of parallel texts between two languages. Performance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally"
W10-0701,D09-1006,1,0.422687,"nd Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk. 6 Shared Task The workshop included a shared task in which participa"
W10-0701,W10-0715,0,0.0129314,"Missing"
W10-0701,ambati-etal-2010-active,0,\N,Missing
W10-0701,J93-2004,0,\N,Missing
W10-0701,mcgraw-etal-2010-collecting,0,\N,Missing
W10-0706,D09-1030,1,0.943348,"annotations for low cost. Emerging best practices suggest designing short, simple tasks that require little amount of upfront effort to most effectively use Mechanical Turk’s labor pool. Suitable tasks are best limited to those easily accomplished in ‘short bites’ requiring little context switching. For instance, most annotation tasks in prior work (Snow et al., 2008) required selection from an enumerated list, allowing for easy automated quality control and data collection. More recent work to collect speech transcription (Novotney and Callison-Burch, 2010) or paral41 lel text translations (Callison-Burch, 2009) demonstrated that Turkers can provide useful free-form annotation. In this paper, we extend open ended collection even further by eliciting narrations of English Wikipedia articles. To vet prospective narrators, we use qualitative qualifications by aggregating the opinions of other Turkers on narrative style, thus avoiding quantification of qualitative tasks. The Spoken Wikipedia Project1 aims to increase the accessibility of Wikipedia by recording articles for use by blind or illiterate users. Since 2008, over 1600 English articles covering topics from art to technology have been narrated by"
W10-0706,N10-1024,1,0.923857,"the NLP community leaves no doubt that non-experts can provide useful annotations for low cost. Emerging best practices suggest designing short, simple tasks that require little amount of upfront effort to most effectively use Mechanical Turk’s labor pool. Suitable tasks are best limited to those easily accomplished in ‘short bites’ requiring little context switching. For instance, most annotation tasks in prior work (Snow et al., 2008) required selection from an enumerated list, allowing for easy automated quality control and data collection. More recent work to collect speech transcription (Novotney and Callison-Burch, 2010) or paral41 lel text translations (Callison-Burch, 2009) demonstrated that Turkers can provide useful free-form annotation. In this paper, we extend open ended collection even further by eliciting narrations of English Wikipedia articles. To vet prospective narrators, we use qualitative qualifications by aggregating the opinions of other Turkers on narrative style, thus avoiding quantification of qualitative tasks. The Spoken Wikipedia Project1 aims to increase the accessibility of Wikipedia by recording articles for use by blind or illiterate users. Since 2008, over 1600 English articles cove"
W10-0706,D08-1027,0,0.218417,"Missing"
W10-0725,W05-1209,0,0.019039,"wers. Therefore, the research questions we could ask are, 1. Are these hypotheses really those ones people interested in? 2. Are hypotheses different if we construct them in other ways? 2 Related Work The early related research was done by Cooper et al. (1996), where they manually construct a textbookstyle corpus aiming at different semantic phenomena involved in inference. However, the dataset is not large enough to train a robust machine-learningbased RTE system. The recent research from the RTE community focused on acquiring large quantities of textual entailment pairs from news headlines (Burger and Ferro, 2005) and negative examples from sequential sentences with transitional discourse connectives (Hickl et al., 2006). Although the quality of the data collected were quite good, most of the positive examples are similar to summarization and the negative examples are more like a comparison/contrast between two sentences instead of a contradiction. Those data are the real sentences used in news articles, but the way of obtaining them is not necessarily the (only) best way to 163 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 163–167,"
W10-0725,P05-1045,0,0.00693946,"e example as a guide along with the instructions. 4 Experiments and Results The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., Total Extracted NEs Facts 244 Counter-Facts 121 Generated Hypotheses Facts 790 Counter-Facts 203 Average (per Text) 1.19 1.11 3.85 1.86 Table 1: The statistics of the (valid) data we collect. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively. 2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al., 2005). In all, it contains 600 T-H pairs, and we use the texts to generate facts and counter-facts and hypotheses as references. We put our task online through CrowdFlower2 , and on average, we pay one cent for each (counter-)fact to the Turkers. CrowdFlower can help with finding trustful Turkers and the data were collected within a few hours. To get a sense of the quality of the data we collect, we mainly focus on analyzing the following three aspects: 1) the statistics of the datasets themselves; 2) the comparison between the data we collect and the original RTE dataset; and 3) the comparison bet"
W10-0725,D08-1027,0,0.23399,"Missing"
W10-0725,wang-sporleder-2010-constructing,1,\N,Missing
W10-0725,W07-1401,0,\N,Missing
W10-0733,W08-0309,1,0.489482,"Missing"
W10-0733,D09-1030,1,0.686279,"Missing"
W10-0733,J07-2003,0,0.0191143,"Missing"
W10-0733,N04-1035,0,0.021508,"Missing"
W10-0733,P06-1121,0,0.0122351,"Missing"
W10-0733,W09-0424,1,0.860446,"Missing"
W10-0733,D08-1027,0,0.0281345,"Missing"
W10-0733,W06-3119,0,0.023346,"Missing"
W10-0733,W09-0401,1,\N,Missing
W10-0733,N09-2035,1,\N,Missing
W10-1703,W10-0701,1,0.75236,"Missing"
W10-1703,W10-1706,0,0.0242402,"Missing"
W10-1703,W10-1714,0,0.0249619,"Missing"
W10-1703,W10-1709,0,0.0199718,"Missing"
W10-1703,W06-3114,1,0.791425,"ion – This year we excluded Google translations from the systems used in system combination. In last year’s evaluation, the large margin between Google and many of the other systems meant that it was hard to improve on when combining systems. This year, the system combinations perform better than their component systems more often than last year. Introduction This paper presents the results of the shared tasks of the joint Workshop on statistical Machine Translation (WMT) and Metrics for MAchine TRanslation (MetricsMATR), which was held at ACL 2010. This builds on four previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009), and one previous MetricsMATR meeting (Przybocki et al., 2008). There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The • Fewer rule-based systems – This year there were fewer rule-based systems submitted. In past years, University of Saarland compiled a large set of outputs from rule"
W10-1703,W10-1710,0,0.0338164,"Missing"
W10-1703,P07-2045,1,0.00952672,"chine translation. As with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. 2 train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). Overview of the shared translation and system combination tasks 2.4 Test data The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 2.5 System combination In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedb"
W10-1703,W10-1722,0,0.0405706,"Missing"
W10-1703,W09-0424,1,0.519668,"s with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. 2 train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). Overview of the shared translation and system combination tasks 2.4 Test data The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 2.5 System combination In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we receiv"
W10-1703,W10-1723,0,0.0273443,"Missing"
W10-1703,W10-1718,1,0.767992,"Missing"
W10-1703,W10-1748,0,0.0371589,"Missing"
W10-1703,W10-1754,0,0.0648908,"Missing"
W10-1703,W10-1731,0,0.0201809,"Missing"
W10-1703,W10-1725,0,0.0253357,"Missing"
W10-1703,W10-1732,0,0.0225271,"Missing"
W10-1703,W10-1733,0,0.0311783,"Missing"
W10-1703,W10-1726,0,0.0139847,"Missing"
W10-1703,W10-1727,0,0.0169981,"Missing"
W10-1703,W10-1728,0,0.0374247,"Missing"
W10-1703,W10-1729,0,0.0349316,"Missing"
W10-1703,W10-1716,0,\N,Missing
W10-1703,W10-1715,1,\N,Missing
W10-1703,W10-1751,0,\N,Missing
W10-1703,W10-1755,0,\N,Missing
W10-1703,W10-1721,0,\N,Missing
W10-1703,W10-1719,0,\N,Missing
W10-1703,W10-1746,0,\N,Missing
W10-1703,W10-1750,0,\N,Missing
W10-1703,W10-1708,0,\N,Missing
W10-1703,W10-1717,0,\N,Missing
W10-1703,W09-0401,1,\N,Missing
W10-1703,W10-1724,0,\N,Missing
W10-1703,W10-1745,0,\N,Missing
W10-1703,W10-1711,0,\N,Missing
W10-1703,2010.iwslt-evaluation.22,0,\N,Missing
W10-1703,D09-1030,1,\N,Missing
W10-1703,W07-0718,1,\N,Missing
W10-1703,W10-1743,0,\N,Missing
W10-1703,W10-1749,0,\N,Missing
W10-1703,W10-1744,0,\N,Missing
W10-1703,W08-0309,1,\N,Missing
W10-1703,W10-1713,0,\N,Missing
W10-1703,W09-0426,0,\N,Missing
W10-1703,W10-1704,0,\N,Missing
W10-1703,W10-1720,0,\N,Missing
W10-1703,W10-1747,0,\N,Missing
W10-1703,W10-1712,0,\N,Missing
W10-1703,W10-1741,0,\N,Missing
W10-1703,W10-1753,0,\N,Missing
W10-1703,W10-1740,0,\N,Missing
W10-1703,W10-1730,0,\N,Missing
W10-1703,W10-1705,0,\N,Missing
W10-1703,W10-1742,0,\N,Missing
W10-1703,W10-1752,0,\N,Missing
W10-1718,N09-1025,0,0.0465037,"Missing"
W10-1718,J07-2003,0,0.2479,"Missing"
W10-1718,P08-1115,0,0.0560574,"Missing"
W10-1718,J99-4004,0,0.114206,"Missing"
W10-1718,D09-1005,1,0.852808,"Missing"
W10-1718,W08-0402,1,0.900437,"Missing"
W10-1718,N09-2003,1,0.885137,"Missing"
W10-1718,W09-0424,1,0.907315,"Missing"
W10-1718,P09-1067,1,0.884344,"Missing"
W10-1718,D07-1104,0,0.0414902,"Missing"
W10-1718,P03-1021,0,0.11921,"Missing"
W10-1718,2006.amta-papers.25,0,0.0712727,"Missing"
W10-1718,W06-3119,0,0.208519,"Missing"
W10-1718,N04-1035,0,\N,Missing
W10-1718,W10-1726,1,\N,Missing
W10-1718,P02-1001,0,\N,Missing
W11-0505,W02-0109,0,0.0289966,"hierarchical clustering as future work, but allow multiple memberships. In addition to clustering using Wikipedia’s inter-page hyperlink structure, we experimented with two families of clustering algorithms pertaining to topic models: the K-means clustering vector space model and the latent Dirichlet allocation (LDA) probabilistic topic model. We used the Mallet software (McCallum, 2002) to run these topic models. We retrieve the latest revision of each article on the day that WikiTopics selected it. We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). Normalization, tokenization, and stop words removal were performed, but no stemming was performed. The unigram (bag-of-words) model was used and the number 5 http://code.pediapress.com/wiki/wiki/mwlib Test set Human-1 Human-2 Human-3 ConComp OneHop K-means tf K-means tf-idf LDA # Clusters 48.6 50.0 53.8 31.8 45.2 50 50 44.8 B3 F-score 0.70 ± 0.08 0.71 ± 0.11 0.74 ± 0.10 0.42 ± 0.18 0.58 ± 0.17 0.52 ± 0.04 0.58 ± 0.09 0.43 ± 0.08 Airbus A320 family Super Bowl XLIII Air Force One Arizona Cardinals Chesley Sullenberger Super Bowl US Airways Flight 1549 Kurt Warner 2009 flu pandemic by country S"
W11-0505,N10-1021,0,0.0837913,"Missing"
W11-0505,D07-1047,0,0.0246484,"llow new events in newswire, and to detect the first story about a new event (Allan et al., 1998). Allan et al. (2000) evaluated a variety of vector space clustering schemes, where the best settings from those experiments were then used in our work. This was followed recently by Petrovi´c et al. (2010), who took an approximate approach to first story detection, as applied to Twitter in an on-line streaming setting. Such a system might provide additional information to WikiTopics by helping to identify and describe current events that have yet to be explicitly described in a Wikipedia article. Svore et al. (2007) explored enhancing single-document summariation using news query logs, which may also be applicable to WikiTopics. Wikipedia’s inter-article links have been utilized to construct a topic ontology (Syed et al., 2008), word segmentation corpora (Gabay et al., 2008), or to compute semantic relatedness (Milne and Witten, 2008). In our work, we found the link structure to be as useful to cluster topically related articles as well as the article text. In future work, the text and the link structure will be combined as Chaudhuri et al. (2009) explored multi-view hierarchical clustering for Wikipedia"
W11-0505,P10-1058,0,0.0196557,"rming web crawls, article text extraction, clustering, classification, summarization, and web page generation. The system processes a constant stream of newswire documents. In contrast, WikiTopics analyzes a static set of articles. Hierarchical clustering like three-level clustering of Newsblaster (Hatzivassiloglou et al., 2000) could be applied to WikiTopics to organize current events hierarchically. Summarizing multiple sentences that are extracted from the articles in the same cluster would provide a comprehensive description about the current event. Integer linear programmingbased models (Woodsend and Lapata, 2010) may prove to be useful to generate summaries while global constraints like length, grammar, and coverage are met. The problem of Topic Detection and Tracking (TDT) is to identify and follow new events in newswire, and to detect the first story about a new event (Allan et al., 1998). Allan et al. (2000) evaluated a variety of vector space clustering schemes, where the best settings from those experiments were then used in our work. This was followed recently by Petrovi´c et al. (2010), who took an approximate approach to first story detection, as applied to Twitter in an on-line streaming sett"
W11-0505,P06-4018,0,\N,Missing
W11-1208,P05-1074,1,0.881787,"ion, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills. 53 Given the available parallel corpora from the MT community, there are studies focusing on extracting paraphrases from bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). The way they do is to treat one language as an pivot and equate two phrases in the other languages as paraphrases if they share a common pivot phrase. Paraphrase extraction draws on phrase pair extraction from the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases."
W11-1208,N03-1003,0,0.17802,"SMT phrase tables e.g., Munteanu and Marcu (2006) Table 1: Previous work in paraphrase acquisition and machine translation. 2 Related Work Roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level. Table 1 gives one example in each category. Paraphrase acquisition is mostly done at the sentence-level, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004), which is not straightforward to be used as a resource for other NLP applications. Quirk et al. (2004) adopted the MT approach to “translate” one sentence into a paraphrased one. As for the corpora, Barzilay and McKeown (2001) took different English translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001"
W11-1208,P01-1008,0,0.844413,"ther) small set of candidate words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extraction, i.e., from bilingual corpora to monolingual corpora. 2. We construct a"
W11-1208,P99-1071,0,0.444385,"Missing"
W11-1208,W10-4217,0,0.0478864,"ws/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted, which are quite successful in namedentity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills. 53 Given the available parallel corpora from the MT community, there are studies focusing on extracting paraphrases from bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). The way they do is to treat one language as an pivot and equate two phrases in the other languages as paraphrases if they share a common pivot phrase. Paraphrase extraction draws on phrase pair extraction from the translation literature. Since parall"
W11-1208,D08-1021,1,0.938988,"rase “parallel” corpora. Furthermore, in MT, certain words can be translated into a (rather) small set of candidate words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to"
W11-1208,E09-1025,1,0.867989,", Barzilay and McKeown (2001) took different English translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted, which are quite successful in namedentity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills. 53 Given the available parallel corpora from the MT community, there are studies focusing on extracting paraphrases from bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). The way they do is to treat one language as an pivot and equate"
W11-1208,I05-5002,0,0.0489442,"in Table 5). An alternative way is to follow the linguistic definition of a phrase, e.g. noun phrase (NP), verb phrase (VP), etc. In this case, we need to use (at least) a chunker to preprocess the text and obtain the proper boundary of each fragment and we used the OpenNLP chunker. We finalize our paraphrase collection by filtering out identical fragment pairs, subsumed fragment pairs (one fragment is fully contained in the other), and fragment having only one word. Apart from sentence pairs collected from the comparable corpora, we also did experiments on the existing MSR paraphrase corpus (Dolan and Brockett, 2005), which is a collection of manually annotated sentential paraphrases. The evaluation on both collections is done by the MTurk. Each task contains 8 pairs of fragments to be evaluated, plus one positive control using identical fragment pairs, and one negative control using a pair of random fragments. All the fragments are shown with the corresponding sentences from where they are extracted5 . The question being asked is 5 We thought about evaluating pairs of isolated fragments, 57 “How are the two highlighted phrases related?”, and the possible answers are, “These phrases refer to the same thin"
W11-1208,C04-1051,0,0.834894,"tment Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extraction, i.e., from bilingual corpora to monolingual corpora. 2. We construct a large collection of paraphrase fragments from monolingual comparable corpora"
W11-1208,W04-3208,0,0.0235461,"the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments"
W11-1208,C04-1151,0,0.0254662,"the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments"
W11-1208,P98-1069,0,0.103069,"aphrase extraction draws on phrase pair extraction from the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in"
W11-1208,W03-1608,0,0.152754,"words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extraction, i.e., from bilingual corpora to monolingual corpora. 2. We construct a large collection of pa"
W11-1208,N06-1014,0,0.0333808,"sentence pairs to feed our fragment extraction method. 3.3 Fragment Pair Extraction The basic procedure is to 1) establish alignments between words or n-grams and 2) extract target paraphrase fragments. For the first step, we use two approaches. One is to change the common substring alignment problem from string to word sequence and we extend the longest common substring (LCS) extraction algorithm to multiple common n-grams. An alternative way is to use a normal word aligner (widely used as the first step in MT systems) to accomplish the job. For our experiments, we use the BerkeleyAligner4 (Liang et al., 2006) by feeding it a dictionary of pairs of identical words along with the paired sentences. We can also combine these two methods by performing the LCS alignment first and adding additional word alignments from the aligner. These form the three configurations of our system (Table 2). Following Munteanu and Marcu (2006), we use both positive and negative lexical associations for the alignment. The positive association measures 4 http://code.google.com/p/ berkeleyaligner/ 56 how likely one word will be aligned to another (value from 0 to 1); and the negative associations indicates how unlikely an a"
W11-1208,D09-1040,1,0.931897,"Missing"
W11-1208,J05-4003,0,0.0292508,"ra have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. Th"
W11-1208,P06-1011,0,0.492896,"ng and Using Comparable Corpora, pages 52–60, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Corpora Sentence level Paraphrase acquisition Parallel e.g., Barzilay and McKeown (2001) Monolingual Comparable e.g., Quirk et al. (2004) Bilingual Parallel N/A Statistical machine translation Most SMT systems Parallel Bilingual Comparable e.g., Fung and Lo (1998) Sub-sentential level This paper e.g., Shinyama et al. (2002) & This paper e.g., Bannard and Callison-Burch (2005) SMT phrase tables e.g., Munteanu and Marcu (2006) Table 1: Previous work in paraphrase acquisition and machine translation. 2 Related Work Roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are parallel or comparable, and whether the output is at the sentence level or at the sub-sentential level. Table 1 gives one example in each category. Paraphrase acquisition is mostly done at the sentence-level, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004), which"
W11-1208,W04-3219,0,0.22536,"mparable corpora and achieve similar quality from a manually-checked paraphrase corpus. 3. We evaluate both intermediate and final results of the paraphrase collection, using the crowdsourcing technique, which is effective, fast, and cheap. 52 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 52–60, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Corpora Sentence level Paraphrase acquisition Parallel e.g., Barzilay and McKeown (2001) Monolingual Comparable e.g., Quirk et al. (2004) Bilingual Parallel N/A Statistical machine translation Most SMT systems Parallel Bilingual Comparable e.g., Fung and Lo (1998) Sub-sentential level This paper e.g., Shinyama et al. (2002) & This paper e.g., Bannard and Callison-Burch (2005) SMT phrase tables e.g., Munteanu and Marcu (2006) Table 1: Previous work in paraphrase acquisition and machine translation. 2 Related Work Roughly speaking, there are three dimensions to characterize the previous work in paraphrase acquisition and machine translation, whether the data comes from monolingual or bilingual corpora, whether the corpora are par"
W11-1208,2007.mtsummit-papers.50,0,0.040726,"000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. They show that even in non-parallel corpora, useful parallel words or phrases can still be found and the size of such data is much larger than that of Corpora (Gigaword) Paraphrase Collection (MSR) Paraphrase Collecton (CCB) Document Pair Extraction Sentence Pair Extraction Fragment Pair Extraction Comparability N-Gram Overlapping Interchangeability <doc&gt; . .. in 1995 ... </doc&gt; <doc&gt; . .. Jan., 1995 ... </doc&gt; <sent&gt; NATO ... in 1995 ... </sent&gt; <sent&gt; In 1995, NATO ... </sent&gt; <frag&gt; the finance chief </frag&gt; Paraphrased Fr"
W11-1208,W03-1609,0,0.130246,"ce-level, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004), which is not straightforward to be used as a resource for other NLP applications. Quirk et al. (2004) adopted the MT approach to “translate” one sentence into a paraphrased one. As for the corpora, Barzilay and McKeown (2001) took different English translations of the same novels (i.e., monolingual parallel corpora), while the others experimented on multiple sources of the same news/events, i.e., monolingual comparable corpora. At the sub-sentential level, interchangeable patterns (Shinyama et al., 2002; Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted, which are quite successful in namedentity-centered tasks, like information extraction, while they are not generalized enough to be applied to other tasks or they have a rather small coverage, e.g. RTE (Dinu and Wang, 2009). To our best knowledge, there is few focused study on general paraphrase fragments extraction at the sub-sentential level, from comparable corpora. A recent study by Belz and Kow (2010) mainly aimed at natural language generation, which they performed a small scale experiment on a specific topic, i.e., British hills."
W11-1208,N10-1063,0,0.0263253,"guage concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. They show that even in non-parallel corpora, useful parallel"
W11-1208,D08-1027,0,0.0451073,"Missing"
W11-1208,E03-1050,0,0.0343029,"raction from the translation literature. Since parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (20"
W11-1208,I05-1023,0,0.0255674,"ays of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. As for the MT research, the standard statistical MT systems require large size of parallel corpora for training and then extract sub-sentential translation phrases. Apart from the limited parallel corpora, comparable corpora are non-parallel bilingual corpora whose documents convey the similar information are also widely considered by many researchers, e.g., (Fung and Lo, 1998; Koehn and Knight, 2000; Vogel, 2003; Fung and Cheung, 2004a; Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Wu and Fung, 2005). A recent study by Smith et al. (2010) extracted parallel sentences from comparable corpora to extend the existing resources. At the sub-sentential level, Munteanu and Marcu (2006) extracted sub-sentential translation pairs from comparable corpora based on the loglikelihood-ratio of word translation probability. They exploit the possibility of making use of reports within a limited time window, which are about the same event or having overlapping contents, but in different languages. Quirk et al. (2007) extracted fragments using a generative model of noisy translations. They show that even in"
W11-1208,P08-1089,0,0.585996,"a. Furthermore, in MT, certain words can be translated into a (rather) small set of candidate words in the Chris Callison-Burch Computer Science Department Johns Hopkins University 3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218, USA ccb@cs.jhu.edu target language; while in principle, each paraphrase can have infinite number of “target” expressions, which reflects the variety of each human language. A variety of paraphrase extraction approaches have been proposed recently, and they require different types of training data. Some require bilingual parallel corpora (Callison-Burch, 2008; Zhao et al., 2008), others require monolingual parallel corpora (Barzilay and McKeown, 2001; Ibrahim et al., 2003) or monolingual comparable corpora (Dolan et al., 2004). In this paper, we focus on extracting paraphrase fragments from monolingual corpora, because this is the most abundant source of data. Additionally, this would potentially allow us to extract paraphrases for a variety of languages that have monolingual corpora, but which do not have easily accessible parallel corpora. This paper makes the following contributions: 1. We adapt a translation fragment pair extraction method to paraphrase extractio"
W11-1208,C98-1066,0,\N,Missing
W11-1208,W03-1004,0,\N,Missing
W11-1208,W07-1401,0,\N,Missing
W11-1610,P05-1074,1,0.828271,"e, the NIST05 Arabic reference set has a mean compression rate of 0.92 with 4 references per set. sions having the same character-based compression rate may have different word-based compression rates. The advantage of a character-based substitution model is in choosing shorter words when possible, freeing space for more content words. Going by word length alone would exclude the many paraphrases with fewer characters than the original phrase and the same number of words (or more). 3.1 Paraphrase Acquisition To generate paraphrases for use in our experiments, we took the approach described by Bannard and Callison-Burch (2005), which extracts paraphrases from bilingual parallel corpora. Figure 1 illustrates the process. A phrase to be paraphrased, like thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 2003). Other occurrences of the foreign phrase in the parallel corpus may align to another English phrase like jailed. Following Bannard and Callison-Burch, we treated any English phrases that share a common foreign phrase as"
W11-1610,D08-1021,1,0.862367,"paraphrases. Thus, thrown into jail not only paraphrases as jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, taken into custody, and thrown into prison . Moreover, because the method relies on noisy and potentially inaccurate word alignments, it is prone to generate many bad paraphrases, such as maltreated, thrown, cases, custody, arrest, owners, and protection. To rank candidates, Bannard and Callison-Burch defined the paraphrase probability p(e2 |e1 ) based on the translation model probabilities p(e|f ) and p(f |e) from statistical machine translation. Following Callison-Burch (2008), we refine selection by requiring both the original phrase and paraphrase to be of the same syntactic type, which leads to more grammatical paraphrases. Although many excellent paraphrases are extracted from parallel corpora, many others are unsuitable and the translation score does not always accurately distinguish the two. Therefore, we re86 Paraphrase study in detail scrutinise consider keep learn study studied studying it in detail undertook Monlingual 1.00 0.94 0.90 0.83 0.57 0.42 0.28 0.16 0.06 Bilingual 0.70 0.08 0.20 0.03 0.10 0.07 0.01 0.05 0.06 Table 1: Candidate paraphrases for stu"
W11-1610,N10-1084,0,0.022851,"night and Marcu (2002), Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been ex"
W11-1610,C08-1018,0,0.507127,"Missing"
W11-1610,W03-0501,0,0.304101,"h compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with the surging popularity and utility of Twitter. 3 Sentence Tightening The distinction between tightening and compression can be illustrated by considering how much space needs to be preserved. In the case of microblogging, often a sentence has just a few too many characters and needs to be “tightened”. On the other hand, if a sentence is much longer than a desired length"
W11-1610,W08-1105,0,0.29131,"Missing"
W11-1610,N10-1131,0,0.266767,"Missing"
W11-1610,N07-1023,0,0.640033,"Missing"
W11-1610,W06-2907,0,0.177064,"r than extractive (Marsi et al., 2010). This is one sense in which paraphrastic compression can improve existing compression methodologies. 1 Compression rate is defined as the compression length over original length, so lower values indicate shorter sentences. While not currently the standard, character-based lengths have been considered before in compression, and we believe that it is relevant for current and future applications. Character lengths have been used for document summarization (DUC 2004, Over and Yen (2004)), summarizing for mobile devices (Corston-Oliver, 2001), and subtitling (Glickman et al., 2006). Although in the past strict word limits have been imposed for various documents, information transmitted electronically is often limited by the number of bytes, which directly relates to the number of characters. Mobile devices, SMS messages, and microblogging sites such as Twitter are increasingly important for quickly spreading information. In this context, it is important to consider characterbased constraints. We examine whether paraphrastic compression allows more information to be conveyed in the same number of characters as deletion-only compressions. For example, the length constrain"
W11-1610,N03-1017,0,0.00431456,"hrases with fewer characters than the original phrase and the same number of words (or more). 3.1 Paraphrase Acquisition To generate paraphrases for use in our experiments, we took the approach described by Bannard and Callison-Burch (2005), which extracts paraphrases from bilingual parallel corpora. Figure 1 illustrates the process. A phrase to be paraphrased, like thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 2003). Other occurrences of the foreign phrase in the parallel corpus may align to another English phrase like jailed. Following Bannard and Callison-Burch, we treated any English phrases that share a common foreign phrase as potential paraphrases of each other. As the original phrase occurs several times and aligns with many different foreign phrases, each of these may align to a variety of other English paraphrases. Thus, thrown into jail not only paraphrases as jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, taken into custody, and thrown into prison . Moreover, beca"
W11-1610,lin-etal-2010-new,0,0.016322,"etail undertook Monlingual 1.00 0.94 0.90 0.83 0.57 0.42 0.28 0.16 0.06 Bilingual 0.70 0.08 0.20 0.03 0.10 0.07 0.01 0.05 0.06 Table 1: Candidate paraphrases for study in detail with corresponding approximate cosine similarity (Monolingual) and translation model (Bilingual) scores. ranked our candidates based on monolingual distributional similarity, employing the method described by Van Durme and Lall (2010) to derive approximate cosine similarity scores over feature counts using single token, independent left and right contexts. Features were computed from the web-scale n-gram collection of Lin et al. (2010). As 5-grams are the highest order of n-gram in this collection, the allowable set of paraphrases have at most four words (which allows at least one word of context). To our knowledge this is the first time such techniques have been used in combination in order to derive higher quality paraphrase candidates. See Table 1 for an example. The monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora. It could be applied to other datadriven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Although it is particularly well s"
W11-1610,J10-3003,0,0.0202358,"s were computed from the web-scale n-gram collection of Lin et al. (2010). As 5-grams are the highest order of n-gram in this collection, the allowable set of paraphrases have at most four words (which allows at least one word of context). To our knowledge this is the first time such techniques have been used in combination in order to derive higher quality paraphrase candidates. See Table 1 for an example. The monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora. It could be applied to other datadriven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (Pereira et al., 1993; Lin and Pantel, 2001; Barzilay and Lee, 2003). In order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in Clarke and Lapata (2008). For each phrase and set of candidate paraphras"
W11-1610,W09-0604,0,0.0180963,"re recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with the surging popularity and utility of Twitter. 3 Sentence Tightening The distinction between tightening and compression can be illustrated by considering how much space needs to be preserved. In the case of microblogging, often a sentence has just a few too many characters and needs to be “tightened”. On the other hand, if a sentence is much longer than a desired length, more drastic compression is necessary. The firs"
W11-1610,E06-1038,0,0.613846,"Missing"
W11-1610,W11-1611,1,0.897263,"Missing"
W11-1610,D09-1041,0,0.227018,"Missing"
W11-1610,P93-1024,0,0.0224638,"used in combination in order to derive higher quality paraphrase candidates. See Table 1 for an example. The monolingual-filtering technique we describe is by no means limited to paraphrases extracted from bilingual corpora. It could be applied to other datadriven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Although it is particularly well suited to the bilingual extracted corpora, since the information that it adds is orthogonal to that model, it would presumably add less to paraphrasing techniques that already take advantage of monolingual distributional similarity (Pereira et al., 1993; Lin and Pantel, 2001; Barzilay and Lee, 2003). In order to evaluate the paraphrase candidates and scoring techniques, we randomly selected 1,000 paraphrase sets where the source phrase was present in the corpus described in Clarke and Lapata (2008). For each phrase and set of candidate paraphrases, we extracted all of the contexts from the corpus in which the source phrase appeared. Human judges were presented each sentence with the original phrase and the same sentences with each paraphrase candidate ... last week five farmers were thrown into jail in Ireland because they resisted ... ... l"
W11-1610,P05-1036,0,0.372896,"he original sentence length and compared these to compressions generated using just deletions. Manual evaluation found that the oracle-then-deletion compressions to preserve more meaning than deletion-only compressions at uniform compression rates. 2 Related work Most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., Galley and McKeown (2007), Knight and Marcu (2002), Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotate"
W11-1610,P10-2043,1,0.883009,"Missing"
W11-1610,W04-1015,0,0.0846973,"(Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with the surging popularity and utility of Twitter. 3 Sentence Tightening The distinction between tightening and compression can be illustrated by considering how much space needs to be preserved. In the case of microblogging, often a sentence has just a few too many characters and needs to be “tightened”. On the other hand, if a sentence is much longer than a desired length, more drastic compression i"
W11-1610,D10-1050,0,0.0844201,"nd compared these to compressions generated using just deletions. Manual evaluation found that the oracle-then-deletion compressions to preserve more meaning than deletion-only compressions at uniform compression rates. 2 Related work Most of the previous research on sentence compression focuses on deletion using syntactic information, (e.g., Galley and McKeown (2007), Knight and Marcu (2002), Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard pa"
W11-1610,P10-1096,0,0.036457,"Missing"
W11-1610,P09-1094,0,0.0208317,"Nomoto (2009), Galanis and Androutsopoulos (2010), Filippova and Strube (2008), McDonald (2006), Yamangil and Shieber (2010), Cohn and Lapata (2008), Cohn and Lapata (2009), Turner and Charniak (2005)). Woodsend et al. (2010) incorporate paraphrase rules into a deletion model. Previous work in subtitling has made oneword substitutions to decrease character length at high compression rates (Glickman et al., 2006). More recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not meaning preservation (Chang and Clark, 2010). Zhao et al. (2009) applied an adaptable paraphrasing pipeline to sentence 2 Taken from the main page of http://wsj.com, April 9, 2011. 85 compression, optimizing for F-measure over a manually annotated set of gold standard paraphrases. Sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation (Dorr et al., 2003; Vandeghinste and Pan, 2004; Marsi et al., 2009). Corston-Oliver (2001) deleted characters from words to shorten the character length of sentences. To our knowledge character-based compression has not been examined before with t"
W11-1610,N03-1003,0,\N,Missing
W11-1611,W00-1401,0,0.0115426,"is corpus (Knight and Marcu, 2000), which contains a small set of 1067 extracted sentences from article/abstract pairs, and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to c"
W11-1611,W06-1421,0,0.0219611,"ressions should be grammatical and retain important meaning, they must be evaluated along these two dimensions. Evaluation is a difficult problem for NLG, and many of the problems identified in this work are relevant for other generation tasks. Shared tasks are popular in many areas as a way to compare system performance in an unbiased manner. Unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as a part of DUC. The benefits of shared-task evaluation have been discussed before (e.g., Belz and Kilgarriff (2006) and Reiter and Belz (2006)), and they include comparing systems fairly under the same conditions. One difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hard to define. Automatic evaluation relies on a comparison to a single gold standard at a predetermined length, which greatly limits the types of compressions that can be fairly judged. As we will discuss in Section 2.1.1, automatic evaluation assumes that deletions are independent, considers only a single gold standard, and cannot handle compressions with paraphrasing. Like for most areas in NLG, hum"
W11-1611,E06-1032,1,0.734381,"eriving such corpora from existing corpora of multi-reference translations. The longest reference translation can be paired with the shortest reference to represent a long sentence and corresponding paraphrased goldstandard compression. Similar to machine translation or summarization, automatic translation of paraphrastic compressions would require multiple references to capture allowable variation, since there are often many equally valid ways of compressing an input. ROUGE or BLEU could be applied to a set of multiplereference compressions, although BLEU is not without its own shortcomings (Callison-Burch et al., 2006). One benefit of both ROUGE and BLEU is that they are based on n-gram recall and precision (respectively) instead of word-error rate, so reordering and word substitutions can be evaluated. Dorr et al. (2003) used BLEU for evaluation in the context of headline generation, which uses rewording and is related to sentence compression. Alternatively, manual evalation can be adapted from other NLG domains, such as the techniques described in the following section. 2.2 Manual Evaluation In order to determine semantic and syntactic suitability, manual evaluation is preferable over automatic techniques"
W11-1611,P06-1048,0,0.175001,"l set of 1067 extracted sentences from article/abstract pairs, and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 20"
W11-1611,D07-1001,0,0.461746,"inal sentence is preserved. Decisions are rated along a 5-point scale (LDC, 2005). Most compression systems consider sentences out of context (a few exceptions exist, e.g., Daum´e III and Marcu (2002), Martins and Smith (2009), and Lin (2003)). Contextual cues and discourse structure may not be a factor to consider if the sentences are generated for use out of context. An example of a context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries (Clarke and Lapata, 2007). This technique has been used before for evaluating summarization and text comprehension (Mani et al., 2002; Morris et al., 1992). 2.2.1 Pitfalls of Manual Evaluation Grammar judgments decrease when the compression is presented alongside the original sentence. Figure 1 shows that the mean grammar rating for the same compressions is on average about 0.3 points higher when the compression is judged in isolation. Researchers should be careful to state when grammar is judged on compressions lacking reference sentences. Another factor is the group of judges. Obviously different studies will rely o"
W11-1611,C08-1018,0,0.383827,"tems could instead report the quality of compressions at several different compression rates, as Nomoto (2008) did. Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation. Output length is controlled for evaluation in some other areas, notably DUC. Systems compress by deletion and not substitution. More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). Cohn and Lapata (2008), Woodsend et al. (2010), and Napoles et al. (2011)). For paraphrastic compressions, manual evaluation alone reliably determines the compression quality. Because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression. To apply automatic techniques to substitutionbased compression, one would need a gold-standard set of paraphrastic compressions. These are rare. Cohn and Lapata (2008) created an abstractive corpus, which contains word reordering and paraphrasing in addition to deletion. Unfortunately, this corpus"
W11-1611,P02-1057,0,0.212377,"Missing"
W11-1611,W03-0501,0,0.0521744,"andard compression. Similar to machine translation or summarization, automatic translation of paraphrastic compressions would require multiple references to capture allowable variation, since there are often many equally valid ways of compressing an input. ROUGE or BLEU could be applied to a set of multiplereference compressions, although BLEU is not without its own shortcomings (Callison-Burch et al., 2006). One benefit of both ROUGE and BLEU is that they are based on n-gram recall and precision (respectively) instead of word-error rate, so reordering and word substitutions can be evaluated. Dorr et al. (2003) used BLEU for evaluation in the context of headline generation, which uses rewording and is related to sentence compression. Alternatively, manual evalation can be adapted from other NLG domains, such as the techniques described in the following section. 2.2 Manual Evaluation In order to determine semantic and syntactic suitability, manual evaluation is preferable over automatic techniques whenever possible. The most widely practiced manual evaluation methodology was first used by Knight and Marcu (2002). Judges grade each compressed sentence against the original and make two separate decisio"
W11-1611,N10-1131,0,0.0550021,": • highlight the importance of comparing systems with similar compression rates, • argue that comparisons in many previous publications are invalid, • provide suggestions for unbiased evaluation. While many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task. 2 Current Practices Because it was developed in support of extractive summarization (Knight and Marcu, 2000), compression has mostly been framed as a deletion task (e.g., McDonald (2006), Galanis and Androutsopoulos (2010), Clarke and Lapata (2008), and Galley 91 Workshop on Monolingual Text-To-Text Generation, pages 91–97, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91–97, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Words Sentence 31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for transporting bombs and bomb parts from Montana to California and mailing them to victims . 17 Kaczynski faces charges naming him responsible for transporting bombs to California and maili"
W11-1611,N07-1023,0,0.466241,"Missing"
W11-1611,A00-1043,0,0.75034,"ns of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatical relations also depends on parser accuracy and the type of dependency relations used.1 1 For example, the RASP parser uses 16 grammatical depen92 2.1.1 Pitfalls of Automatic Evaluation Automatic evaluation operates under three often incorrect assumptions: Deletions are independent. The dependency structu"
W11-1611,W03-1101,0,0.030491,"ntactic suitability, manual evaluation is preferable over automatic techniques whenever possible. The most widely practiced manual evaluation methodology was first used by Knight and Marcu (2002). Judges grade each compressed sentence against the original and make two separate decisions: how grammatical 93 is the compression and how much of the meaning from the original sentence is preserved. Decisions are rated along a 5-point scale (LDC, 2005). Most compression systems consider sentences out of context (a few exceptions exist, e.g., Daum´e III and Marcu (2002), Martins and Smith (2009), and Lin (2003)). Contextual cues and discourse structure may not be a factor to consider if the sentences are generated for use out of context. An example of a context-aware approach considered the summaries formed by shortened sentences and evaluated the compression systems based on how well people could answer questions about the original document from the summaries (Clarke and Lapata, 2007). This technique has been used before for evaluating summarization and text comprehension (Mani et al., 2002; Morris et al., 1992). 2.2.1 Pitfalls of Manual Evaluation Grammar judgments decrease when the compression is"
W11-1611,W05-1612,0,0.0131887,"and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatical relations also depends on parser accuracy and the type of dependency relations used.1 1 For example, the RASP parser uses 16 grammatical depen92 2.1.1 Pitfalls of Automatic Evaluation Automatic evaluation operates under three often incorrect assumptions: Deletions are independent. The dependency structure of a sentence may be unaltered when dependent words are not deleted as a unit. Examples of words that should be treated as a sin"
W11-1611,W09-1801,0,0.0717532,"and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatica"
W11-1611,E06-1038,0,0.716021,". In this work we: • highlight the importance of comparing systems with similar compression rates, • argue that comparisons in many previous publications are invalid, • provide suggestions for unbiased evaluation. While many may find this discussion intuitive, these points are not addressed in much of the existing research, and therefore it is crucial to enumerate them in order to improve the scientific validity of the task. 2 Current Practices Because it was developed in support of extractive summarization (Knight and Marcu, 2000), compression has mostly been framed as a deletion task (e.g., McDonald (2006), Galanis and Androutsopoulos (2010), Clarke and Lapata (2008), and Galley 91 Workshop on Monolingual Text-To-Text Generation, pages 91–97, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91–97, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Words Sentence 31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for transporting bombs and bomb parts from Montana to California and mailing them to victims . 17 Kaczynski faces charges naming him responsible for transp"
W11-1611,W11-1610,1,0.896071,"Missing"
W11-1611,P08-1035,0,0.0177534,"compression. Automatic evaluation considers a single gold-standard compression. This ignores the possibility of different length compressions and equally good compressions of the same length, where multiple non-overlapping deletions are acceptable. For an example, see Table 1. Having multiple gold standards would provide references at different compression lengths and reflect different deletion choices (see Section 3). Since no large corpus with multiple gold standards exists to our knowledge, systems could instead report the quality of compressions at several different compression rates, as Nomoto (2008) did. Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation. Output length is controlled for evaluation in some other areas, notably DUC. Systems compress by deletion and not substitution. More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). Cohn and Lapata (2008), Woodsend et al. (2010), and Napoles et al. (2011)). For paraphrastic compressions, man"
W11-1611,D09-1041,0,0.0712496,"ammatically loss. 4 Mismatched Comparisons We have observed that a difference in compression rates as small as 5 percentage points can influence the quality ratings by as much as 0.1 points and conclude: systems must be compared using similar levels of compression. In particular, if system A’s output is higher quality, but longer than system B’s, then it is not necessarily the case that A is better than B. Conversely, if B has results at least as good as system A, one can claim that B is better, since B’s output is shorter. Here are some examples in the literature of mismatched comparisons: • Nomoto (2009) concluded their system significantly outperformed that of Cohn and Lapata (2008). However, the compression rate of their system ranged from 45 to 74, while the compression rate of Cohn and Lapata (2008) was 35. This claim is unverifiable without further comparison. • Clarke and Lapata (2007), when comparing against McDonald (2006), reported significantly better results at a 5-point higher compression rate. At first glance, this does not seem like a remarkable difference. However, Model C&L McD C&L McD Meaning 3.83 3.94 3.76∗ 3.50∗ Grammar 3.66 3.87 3.53∗ 3.17∗ 5 CompR 64.1 64.2 78.4∗ 68.5∗ Ta"
W11-1611,W06-1422,0,0.018026,"and retain important meaning, they must be evaluated along these two dimensions. Evaluation is a difficult problem for NLG, and many of the problems identified in this work are relevant for other generation tasks. Shared tasks are popular in many areas as a way to compare system performance in an unbiased manner. Unlike other tasks, such as machine translation, there is no shared-task evaluation for compression, even though some compression systems are indirectly evaluated as a part of DUC. The benefits of shared-task evaluation have been discussed before (e.g., Belz and Kilgarriff (2006) and Reiter and Belz (2006)), and they include comparing systems fairly under the same conditions. One difficulty in evaluating compression systems fairly is that an unbiased automatic metric is hard to define. Automatic evaluation relies on a comparison to a single gold standard at a predetermined length, which greatly limits the types of compressions that can be fairly judged. As we will discuss in Section 2.1.1, automatic evaluation assumes that deletions are independent, considers only a single gold standard, and cannot handle compressions with paraphrasing. Like for most areas in NLG, human evaluation is preferable"
W11-1611,N03-1026,0,0.467005,"s will be few and therefore gold-standard compressions must be manually annotated. The most popular corpora are the Ziff-Davis corpus (Knight and Marcu, 2000), which contains a small set of 1067 extracted sentences from article/abstract pairs, and the manually annotated Clarke and Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with de"
W11-1611,P06-2109,0,0.734019,"Lapata (2008) corpus, consisting of nearly 3000 sentences from news articles and broadcast news transcripts. These corpora contain one gold standard for each sentence. 2.1 Automatic Techniques One of the most widely used automatic metrics is the F1 measure over grammatical relations of the goldstandard compressions (Riezler et al., 2003). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al., 2000) for judging compression quality (Clarke and Lapata, 2006). F1 has also been used over unigrams (Martins and Smith, 2009) and bigrams (Unno et al., 2006). Unno et al. (2006) compared the F1 measures to BLEU scores (using the gold standard as a single reference) over varying compression rates, and found that BLEU behaves similarly to both F1 measures. A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). The only metric that has been shown to correlate with human judgments is F1 (Clarke and Lapata, 2006), but even this is not entirely reliable. F1 over grammatical relations also depends on pars"
W11-1611,D10-1050,0,0.0222828,"t the quality of compressions at several different compression rates, as Nomoto (2008) did. Alternatively, systems could evaluate compressions that are of a similar length as the gold standard compression, to fix a length for the purpose of evaluation. Output length is controlled for evaluation in some other areas, notably DUC. Systems compress by deletion and not substitution. More recent approaches to compression introduce reordering and paraphrase operations (e.g., dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). Cohn and Lapata (2008), Woodsend et al. (2010), and Napoles et al. (2011)). For paraphrastic compressions, manual evaluation alone reliably determines the compression quality. Because automatic evaluation metrics compare shortened sentences to extractive gold standards, they cannot be applied to paraphrastic compression. To apply automatic techniques to substitutionbased compression, one would need a gold-standard set of paraphrastic compressions. These are rare. Cohn and Lapata (2008) created an abstractive corpus, which contains word reordering and paraphrasing in addition to deletion. Unfortunately, this corpus is small (575 sentences)"
W11-2103,W11-2134,0,0.0605238,"Missing"
W11-2103,W11-2137,0,0.0520272,"Missing"
W11-2103,W11-2138,0,0.0321994,"Missing"
W11-2103,W07-0718,1,0.805048,"el to publish a paper about their experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation sys"
W11-2103,W08-0309,1,0.779572,"heir experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous worksh"
W11-2103,W09-0401,1,0.532823,"anslation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation met"
W11-2103,W10-1703,1,0.633993,"nse to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation metrics other than BLEU are more"
W11-2103,W11-2105,0,0.0708412,"Missing"
W11-2103,W11-2140,0,0.0631771,"Missing"
W11-2103,W11-2141,0,0.049057,"Missing"
W11-2103,W97-0409,0,0.103469,"o’s crowdsourcing translation efforts, the Microsoft Translator team developed a Haitian Creole statistical machine translation engine from scratch in a compressed timeframe (Lewis, 2010). Despite the impressive number of translations completed by volunteers, machine translation was viewed as a potentially useful tool for higher volume applications or to provide translations of English medical documents into Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank the"
W11-2103,2003.mtsummit-papers.37,0,0.0310815,"nto Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future 27 crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). 3.1 Haitian Creole Data For the WMT11 featured translation task, we anonymized the SMS Haitian Creole"
W11-2103,P03-1021,0,0.527543,"an Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future 27 crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). 3.1 Haitian Creole Data For the WMT11 featured translation task, we anonymized the SMS Haitian Creole"
W11-2103,P02-1040,0,0.09575,"ETEOR -1.3- RANK MT E R ATER , MT E R ATER -P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out of English TESLA - M .90 .95 .96 .94 TESLA - B .81 .90 .91 .87 MP F .72 .63 .87 .89 .78 .80 WMP F .72 .61 .87 .89 .77 .79 MP 4 IBM 1 -.76 -.91 -.71 -.61 .75 .74 ROSE .65 .41 .90 .86 .71 .73 .70 .72 BLEU .65 .44 .87 .86 AMBER- TI .56 .54 .88 .84 .70 .75 AMBER .56 .53 .87 ."
W11-2103,W11-2111,0,0.0968425,"th other MT evaluation metrics and heuristics that take the reference translations into account. Please refer to the proceedings for papers providing detailed descriptions of all of the metrics. Metric IDs AMBER , AMBER - NL , AMBER - IT F15, F15 G 3 METEOR -1.3- ADQ , METEOR -1.3- RANK MT E R ATER , MT E R ATER -P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out"
W11-2103,W11-2153,0,0.0551069,"Missing"
W11-2103,W11-2110,0,0.0542786,"Missing"
W11-2103,W11-2154,0,0.0525812,"Missing"
W11-2103,W11-2112,0,0.042178,"Missing"
W11-2103,W11-2158,0,0.072375,"Missing"
W11-2103,W11-2120,0,0.0448017,"Missing"
W11-2103,2006.amta-papers.25,0,0.129527,"-P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out of English TESLA - M .90 .95 .96 .94 TESLA - B .81 .90 .91 .87 MP F .72 .63 .87 .89 .78 .80 WMP F .72 .61 .87 .89 .77 .79 MP 4 IBM 1 -.76 -.91 -.71 -.61 .75 .74 ROSE .65 .41 .90 .86 .71 .73 .70 .72 BLEU .65 .44 .87 .86 AMBER- TI .56 .54 .88 .84 .70 .75 AMBER .56 .53 .87 .84 .70 .74 AMBER- NL .56 .45 .88 .83 .68"
W11-2103,W11-2113,0,0.0828194,"Missing"
W11-2103,W11-2159,0,0.0580674,"Missing"
W11-2103,W11-2155,0,\N,Missing
W11-2103,2010.amta-papers.7,1,\N,Missing
W11-2103,W11-2152,0,\N,Missing
W11-2103,W11-2150,0,\N,Missing
W11-2103,W10-1719,0,\N,Missing
W11-2103,W10-1718,1,\N,Missing
W11-2103,C08-1109,0,\N,Missing
W11-2103,W11-2144,0,\N,Missing
W11-2103,W11-2148,0,\N,Missing
W11-2103,D11-1035,0,\N,Missing
W11-2103,W11-2147,0,\N,Missing
W11-2103,W11-2161,0,\N,Missing
W11-2103,W11-2108,0,\N,Missing
W11-2103,2010.eamt-1.37,0,\N,Missing
W11-2103,W11-2143,0,\N,Missing
W11-2103,W11-2139,0,\N,Missing
W11-2103,P07-2045,1,\N,Missing
W11-2103,W09-0415,0,\N,Missing
W11-2103,W11-2142,0,\N,Missing
W11-2103,W10-1711,0,\N,Missing
W11-2103,2010.iwslt-evaluation.22,0,\N,Missing
W11-2103,2005.eamt-1.12,0,\N,Missing
W11-2103,W11-2146,0,\N,Missing
W11-2103,W11-2104,0,\N,Missing
W11-2103,W11-2109,0,\N,Missing
W11-2103,W06-3114,1,\N,Missing
W11-2103,W05-0904,0,\N,Missing
W11-2103,W11-2157,0,\N,Missing
W11-2103,W11-2156,0,\N,Missing
W11-2103,W11-2151,0,\N,Missing
W11-2103,W11-2136,0,\N,Missing
W11-2103,W11-2135,0,\N,Missing
W11-2103,W11-2118,0,\N,Missing
W11-2103,W11-2145,0,\N,Missing
W11-2103,W11-2163,0,\N,Missing
W11-2103,W11-2160,1,\N,Missing
W11-2103,W09-0407,0,\N,Missing
W11-2103,D08-1076,0,\N,Missing
W11-2103,W11-2149,0,\N,Missing
W11-2103,W11-2117,0,\N,Missing
W11-2103,W11-2107,0,\N,Missing
W11-2103,W11-2164,0,\N,Missing
W11-2103,W11-2121,0,\N,Missing
W11-2103,W11-2119,0,\N,Missing
W11-2103,W11-2106,0,\N,Missing
W11-2103,W11-2116,0,\N,Missing
W11-2160,J07-2003,0,0.622091,"ntire MT pipeline, from data preparation to evaluation. This script is built on top of a module called CachePipe. CachePipe is a simple wrapper around shell commands that uses SHA-1 hashes and explicitlyprovided lists of dependencies to determine whether a command needs to be run, saving time both in running and debugging machine translation pipelines. We present progress on Joshua, an opensource decoder for hierarchical and syntaxbased machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 2 Introduction Joshua is an open-source1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugop"
W11-2160,P10-1146,0,0.0355065,"Missing"
W11-2160,clark-lavie-2010-loonybin,0,0.0141231,"have scores for the final version of the paper. 5 LDC2009T13 4 482 4 CachePipe: Cached pipeline runs Machine translation pipelines involve the specification and execution of many different datasets, training procedures, and pre- and post-processing techniques that can have large effects on translation outcome, and which make direct comparisons between systems difficult. The complexity of managing these pipelines and experimental environments has led to a number of different experimental management systems, such as Experiment.perl,6 Joshua 2.0’s Makefile system (Li et al., 2010), and LoonyBin (Clark and Lavie, 2010). In addition to managing the pipeline, these scripts employ different techniques to avoid expensive recomputation by caching steps. 6 http://www.statmt.org/moses/?n= FactoredTraining.EMS S GLUE VP GLUE COMMA+SBAR+. PP DT+NP VBN NP the reactor type der reaktortyp ADJP will be wird zwar NN operated mit NP with betrieben uran , das JJ , which is nicht angereichert ist . JJ PP GLUE not enriched . VBN NN DT+NP uranium ADJP VP COMMA+SBAR+. GLUE S Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals and non-terminals. The unshaded terminals are"
W11-2160,P10-4002,1,0.121611,"s to describe this past year’s work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshua’s grammar ex1 http://github.com/joshua-decoder/joshua Thrax: grammar extraction In modern machine translation systems such as Joshua (Li et al., 2009) and cdec (Dyer et al., 2010), a translation model is represented as a synchronous context-free grammar (SCFG). Formally, an SCFG may be considered as a tuple (N, S, Tσ , Tτ , G) where N is a set of nonterminal symbols of the grammar, S ∈ N is the goal symbol, Tσ and Tτ are the source- and target-side terminal symbol vocabularies, respectively, and G is a set of production rules of the grammar. Each rule in G is of the form X → hα, γ, ∼i where X ∈ N is a nonterminal symbol, α is a sequence of symbols from N ∪ Tσ , γ is a sequence of 478 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478–484, c E"
W11-2160,N04-1035,0,0.158565,"Missing"
W11-2160,N03-1017,0,0.00301836,"a constant phrase penalty In addition to simple features, Thrax also implements map-reduce features. These are features that require comparing rules in a certain order. Thrax uses Hadoop to sort the rules efficiently and calculate these feature functions. Thrax implements the following map-reduce features: • Phrasal translation probabilities p(α|γ) and p(γ|α), calculated with relative frequency: p(α|γ) = C(α, γ) C(γ) (2) (and vice versa), where C(·) is the number of times a given event was extracted. • Lexical weighting plex (α|γ, A) and plex (γ|α, A). We calculate these weights as given in (Koehn et al., 2003): let A be the alignment between α and γ, so (i, j) ∈ A if and only if the ith word of α is aligned to the jth word of γ. Then we can define plex (γ|α) as n Y i=1 X 1 w(γj |αi ) |{j : (i, j) ∈ A}| (3) (i,j)∈A where αi is the ith word of α, γj is the jth word of γ, and w(y|x) is the relative frequency of seeing word y given x. • Rarity penalty, given by exp(1 − C(X → hα, γi)) (4) where again C(·) is a count of the number of times the rule was extracted. 481 The above features are all implemented and can be turned on or off with a keyword in the Thrax configuration file. It is easy to extend Thr"
W11-2160,N04-1022,0,0.0316682,"to build and test new models. the data. We also removed any sentences longer than 50 tokens (after tokenization). For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser (Petrov et al., 2006) with the provided Treebank-trained grammar. We tuned the model weights against the WMT08 test set (news-test2008) using ZMERT (Zaidan, 2009), an implementation of minimum error-rate training included with Joshua. We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Figure 2 shows an example derivation with an SAMT grammar. To re-case the 1-best test set output, we trained a true-case 5gram language model using the same LM training data as before, and used an SCFG translation model to translate from the lowercased to true-case output. The translation model used rules limited to five tokens in length, and contained no hierarchical rules. 3 Experiments We built systems for six language pairs for the WMT 2011 shared task: cz-en, en-cz, de-en, en-de, fr-en, and en-fr.3 For each language pair, we built both SAMT and hiero grammars.4 Table 3 contains the resu"
W11-2160,W09-0424,1,0.922719,"Missing"
W11-2160,W10-1718,1,0.929398,"s context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 2 Introduction Joshua is an open-source1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugopal, 2006). The main focus of this paper is to describe this past year’s work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshua’s grammar ex1 http://github.com/joshua-decoder/joshua T"
W11-2160,P09-1063,0,0.0262969,"Missing"
W11-2160,C08-1064,1,0.294204,"pus with word-level alignments. SAMT additionally requires that the target side of the corpus be parsed. There are several parameters that can make a significant difference in a grammar’s overall translation performance. Each of these parameters is easily adjustable in Thrax by changing its value in a configuration file. • maximum rule span • maximum span of consistent phrase pairs • maximum number of nonterminals • whether to allow unaligned words at the edges of consistent phrase pairs Chiang (2007) gives reasonable heuristic choices for these parameters when extracting a Hiero grammar, and Lopez (2008) confirms some of them (maximum rule span of 10, maximum number of sourceside symbols at 5, and maximum number of nonterminals at 2 per rule). ?) provided comparisons among phrase-based, hierarchical, and syntax-based models, but did not report extensive experimentation with the model parameterizations. When extracting Hiero- or SAMT-style grammars, the first Hadoop job in the Thrax workflow takes in a parallel corpus and produces a set of rules. But in fact Thrax’s extraction mechanism is more general than that; all it requires is a function that maps a string to a set of rules. This makes it"
W11-2160,P06-1055,0,0.0220837,"n run on Hadoop, as Thrax does. The Joshua and cdec extractors only extract Hiero grammars, and Zollmann and Venugopal’s extractor can only extract SAMT-style grammars. They are not designed to score arbitrary feature sets, either. Since variation in translation models and feature sets can have a significant effect on translation performance, we have developed Thrax in order to make it easy to build and test new models. the data. We also removed any sentences longer than 50 tokens (after tokenization). For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser (Petrov et al., 2006) with the provided Treebank-trained grammar. We tuned the model weights against the WMT08 test set (news-test2008) using ZMERT (Zaidan, 2009), an implementation of minimum error-rate training included with Joshua. We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Figure 2 shows an example derivation with an SAMT grammar. To re-case the 1-best test set output, we trained a true-case 5gram language model using the same LM training data as before, and used an SCFG t"
W11-2160,P99-1039,0,0.0362819,"Missing"
W11-2160,P08-1064,0,\N,Missing
W11-2160,C08-1144,0,\N,Missing
W11-2160,W06-3119,0,\N,Missing
W11-2504,P05-1074,1,0.911847,"ng Monolingual Distributional Similarity Tsz Ping Chan, Chris Callison-Burch and Benjamin Van Durme Center for Language and Speech Processing, and HLTCOE Johns Hopkins University Abstract put of a pivot-based bilingual paraphrase model. In this paper we investigate the strengths and weaknesses of scoring paraphrases using monolingual distributional similarity versus the bilingually calculated paraphrase probability. We show that monolingual cosine similarity calculated on large volumes of text ranks bilingually-extracted paraphrases better than the paraphrase probability originally defined by Bannard and Callison-Burch (2005). While our current implementation shows improvement mainly in grammaticality, other contextual features are expected to enhance the meaning preservation of paraphrases. We also show that monolingual scores can provide a reasonable threshold for picking out high precision paraphrases. This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods. Our e"
W11-2504,P08-1077,0,0.577352,"Missing"
W11-2504,D08-1021,1,0.924614,"translation probabilities from grams are no longer than 5 tokens by design), it was not feasible to parse, which led to the use of n-gram a statistical translation model. Anecdotally, this paraphrase probability some- contexts. Here we use adjacent unigrams. For each times seems unable to discriminate between good phrase x we wished to paraphrase, we extracted the and bad paraphrases, so some researchers disregard context vector of x from the n-gram collection as it and treat the extracted paraphrases as an unsorted such: every (n-gram, frequency) pair of the form: set (Snover et al., 2010). Callison-Burch (2008) (ax, f ), or (xb, f ), gave rise to the (feature, value) attempts to improve the ranking by limiting para- pair: (wi−1 =a, f ), or (wi+1 =b, f ), respectively. In order to scale to this size of a collection, we relied phrases to be the same syntactic type. We attempt to rerank the paraphrases using other on Locality Sensitive Hashing (LSH), as was done information. This is similar to the efforts of Zhao previously by Ravichandran et al. (2005) and Bhaet al. (2008), who made use of multiple resources to gat and Ravichandran (2008). To avoid computing derive feature functions and extract paraph"
W11-2504,N03-1017,0,0.0030838,"irk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction from Bitexts Bannard and Callison-Burch (2005) proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora. Figure 1 illustrates their paraphrase extraction process. The target phrase, e.g. thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 2003). Other occurrences of the foreign phrase in the parallel corpus may align to a distinct English phrase, such as jailed. As the original phrase occurs several times and aligns with many different foreign phrases, each of these may align to a variety of other English paraphrases. Thus, thrown into jail not only paraphrases as jailed, but also as arrested, detained, imprisoned, incarcerated, locked up, and so on. Bad paraphrases, such as maltreated, thrown, cases, custody, arrest, and protection, may also arise due to poor word alignment quality and other factors. 33 Proceedings of the GEMS 2011"
W11-2504,2005.mtsummit-papers.11,0,0.0206692,"paraphrases since, by construction, they do not share any foreign translation and hence their paraphrase scores are not defined. As expected from the drawbacks of monolingual-based statistics, willing and eager are assigned top scores by MonoDS, although good paraphrases such as somewhat reluctant and disinclined are also ranked highly. This illustrates how BiP complements the monolingual reranking technique by providing orthogonal information to address the issue of antonyms for MonoDS. 3.3 Implementation Details For BiP and SyntBiP, the French-English parallel text from the Europarl corpus (Koehn, 2005) was used to train the paraphrase model. The parallel corpus was extracted from proceedings of the European parliament with a total of about 1.3 million sentences and close to 97 million words in the English text. Word alignments were generated with the Berkeley aligner. For SyntBiP, the English side of the parallel corpus was parsed using the Stanford parser (Klein and Manning, 2003). The translation models were trained with Thrax, a grammar extractor for machine translation (Weese et al., 2011). Thrax extracts phrase pairs that are labeled with complex syntactic labels following Zollmann and"
W11-2504,lin-etal-2010-new,0,0.0231586,"ilingual parallel corpus to extract textually similar partly because they both often apparaphrases. pear as the object of the verb eat. While syntacBannard and Callison-Burch (2005) defined a tic contexts provide strong evidence of distributional paraphrase probability to rank these paraphrase can- preferences, it is computationally expensive to parse very large corpora, so it is also common to represent didates, as follows: context vectors with simpler representations like adeˆ2 = arg max p(e2 |e1 ) (1) jacent words and n-grams (Lapata and Keller, 2005; e2 6=e1 Bhagat and Ravichandran, 2008; Lin et al., 2010; X p(e2 |e1 ) = p(e2 , f |e1 ) (2) Van Durme and Lall, 2010). In these models, apf ple and orange might be judged similar because both X tend to be one word to the right of some, and one to = p(e2 |f, e1 )p(f |e1 ) (3) the left of juice. f X Here we calculate distributional similarity using a ≈ p(e2 |f )p(f |e1 ) (4) web-scale n-gram corpus (Brants and Franz, 2006; f Lin et al., 2010). Given both the size of the collecwhere p(e2 |e1 ) is the paraphrase probability, and tion, and that the n-grams are sub-sentential (the np(e|f ) and p(f |e) are translation probabilities from grams are no longe"
W11-2504,P97-1009,0,0.0814776,"assessment of paraphrase quality in terms of grammaticality, yet have minimal effects on meaning preservation of paraphrases. While we speculated that MonoDS would improve both meaning and grammar scoring for paraphrases, we found in the results that only grammaticality was improved from the monolingual approach. This is likely due to the choice of how context is represented, which in this case is only single neighboring words. A consideration for future work to enhance paraphrasal meaning preservation would be to explore other contextual representations, such as syntactic dependency parsing (Lin, 1997), mutual information between co-occurences of phrases Church and Hanks (1991), or increasing the number of neighboring words used in n-gram based representations. In future work we will make use of other complementary bilingual and monolingual knowledge sources by combining other features such as n-gram length, language model scores, etc. One approach would be to perform minimum error rate training similar to Zhao et al. (2008) in which linear weights of a feature function for a set of paraphrases candidate are trained iteratively to minimize the phrasalsubstitution-based error rate. Instead o"
W11-2504,J10-3003,0,0.0520831,"errors in bilingual pivotbased methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 2 1 Introduction 2.1 Paraphrasing is the rewording of a phrase such that meaning is preserved. Data-driven paraphrase acquisition techniques can be categorized by the type of data that they use (Madnani and Dorr, 2010). Monolingual paraphrasing techniques cluster phrases through statistical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the"
W11-2504,P08-1118,0,0.0278629,"Missing"
W11-2504,D08-1103,0,0.00578013,"he paraphrase was assigned a low score of 0.098 as compared to other paraphrase candidates with the correct syntactic type. Note that the SyntBiP produced significantly fewer paraphrase candidates, since its paraphrase candidates must be the same syntactic type as the original phrase. Identity paraphrases are excluded for the rest of the discussion in this paper. 3.2 Susceptibility to Antonyms Monolingual distributional similarity is widely known to conflate words with opposite meaning and has motivated a large body of prior work on antonym detection (Lin and Zhao, 2003; Lin and Pantel, 2001; Mohammad et al., 2008a; Mohammad et al., 2008b; Marneffe et al., 2008; Voorhees, 2008). In contrast, the antonyms of a phrase are rarely produced during pivoting of the BiP methods because they tend not to share the same foreign translations. Since the reranking framework proposed here begins with paraphrases acquired by the BiP methodology, MonoDS can considerably enhance the quality of ranking while sidestepping the antonym problem that arises from using MonoDS alone. To support this intuition, an example of a paraphrase list with inserted hand-selected phrases ranked by each reranking methods is shown in Table"
W11-2504,N03-1024,0,0.0723111,"rases through statistical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the same foreign text (Barzilay and McKeown, 2001; Pang et al., 2003; Quirk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction from Bitexts Bannard and Callison-Burch (2005) proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora. Figure 1 illustrates their paraphrase extraction process. The target phrase, e.g. thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translati"
W11-2504,I05-1011,0,0.178451,"obabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 2 1 Introduction 2.1 Paraphrasing is the rewording of a phrase such that meaning is preserved. Data-driven paraphrase acquisition techniques can be categorized by the type of data that they use (Madnani and Dorr, 2010). Monolingual paraphrasing techniques cluster phrases through statistical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the same foreign text (Barzilay and McKeown, 2001; Pang et al., 2003; Quirk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction f"
W11-2504,W04-3219,0,0.0233104,"stical characteristics such as dependency path similarities or distributional cooccurrence information (Lin and Pantel, 2001; Pasca and Dienes, 2005). Bilingual paraphrasing techniques use parallel corpora to extract potential paraphrases by grouping English phrases that share the same foreign translations (Bannard and CallisonBurch, 2005). Other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting paraphrases from multiple English translations of the same foreign text (Barzilay and McKeown, 2001; Pang et al., 2003; Quirk et al., 2004). We exploit both methodologies, applying a monolingually-derived similarity metric to the outRelated Work Paraphrase Extraction from Bitexts Bannard and Callison-Burch (2005) proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora. Figure 1 illustrates their paraphrase extraction process. The target phrase, e.g. thrown into jail, is found in a German-English parallel corpus. The corresponding foreign phrase (festgenommen) is identified using word alignment and phrase extraction techniques from phrase-based statistical machine translation (Koehn et al., 200"
W11-2504,P05-1077,0,0.0246964,"the n-gram collection as it and treat the extracted paraphrases as an unsorted such: every (n-gram, frequency) pair of the form: set (Snover et al., 2010). Callison-Burch (2008) (ax, f ), or (xb, f ), gave rise to the (feature, value) attempts to improve the ranking by limiting para- pair: (wi−1 =a, f ), or (wi+1 =b, f ), respectively. In order to scale to this size of a collection, we relied phrases to be the same syntactic type. We attempt to rerank the paraphrases using other on Locality Sensitive Hashing (LSH), as was done information. This is similar to the efforts of Zhao previously by Ravichandran et al. (2005) and Bhaet al. (2008), who made use of multiple resources to gat and Ravichandran (2008). To avoid computing derive feature functions and extract paraphrase ta- feature vectors explicitly, which can be a memory bles. The paraphrase that maximizes a log-linear intensive bottleneck, we employed the online LSH combination of various feature functions is then se- variant described by Van Durme and Lall (2010). This variant, based on the earlier work of Indyk lected as the optimal paraphrase. Feature weights in the model are optimized by minimizing a phrase and Motwani (1998) and Charikar (2002), a"
W11-2504,P07-1058,0,0.0236409,"us consists of at most 5-gram and each distributional similarity feature requires a single neighboring token, the LSH signatures are generated only for phrases that are 4-gram or less. Phrases that didn’t appear in the n-grams with at least one feature were discarded. 4 Human Evaluation The different paraphrase scoring methods were compared through a manual evaluation conducted on Amazon Mechanical Turk. A set of 100 test phrases were selected and for each test phrase, five distinct sentences were randomly sampled to capture the fact that paraphrases are valid in some contexts but not others (Szpektor et al., 2007). Judges evaluated the paraphrase quality through a substitution test: For each sampled sentence, the test phrase is substituted with automatically-generated paraphrases. The sentences and the phrases are drawn from the English side of the Europarl corpus. Judges indicated the amount of the original meaning preserved by the paraphrases and the grammaticality of the resulting sentences. They assigned two values to each sentence using the 5-point scales defined in CallisonBurch (2008). The 100 test phrases consisted of 25 unigrams, 25 bigrams, 25 trigrams and 25 4-grams. These 25 phrases were ra"
W11-2504,P10-2043,1,0.82626,"Missing"
W11-2504,P08-1008,0,0.0210868,"raphrase candidates with the correct syntactic type. Note that the SyntBiP produced significantly fewer paraphrase candidates, since its paraphrase candidates must be the same syntactic type as the original phrase. Identity paraphrases are excluded for the rest of the discussion in this paper. 3.2 Susceptibility to Antonyms Monolingual distributional similarity is widely known to conflate words with opposite meaning and has motivated a large body of prior work on antonym detection (Lin and Zhao, 2003; Lin and Pantel, 2001; Mohammad et al., 2008a; Mohammad et al., 2008b; Marneffe et al., 2008; Voorhees, 2008). In contrast, the antonyms of a phrase are rarely produced during pivoting of the BiP methods because they tend not to share the same foreign translations. Since the reranking framework proposed here begins with paraphrases acquired by the BiP methodology, MonoDS can considerably enhance the quality of ranking while sidestepping the antonym problem that arises from using MonoDS alone. To support this intuition, an example of a paraphrase list with inserted hand-selected phrases ranked by each reranking methods is shown in Table 21 . Hand-selected antonyms of reluctant are inserted into the pa"
W11-2504,W11-2160,1,0.0373664,".3 Implementation Details For BiP and SyntBiP, the French-English parallel text from the Europarl corpus (Koehn, 2005) was used to train the paraphrase model. The parallel corpus was extracted from proceedings of the European parliament with a total of about 1.3 million sentences and close to 97 million words in the English text. Word alignments were generated with the Berkeley aligner. For SyntBiP, the English side of the parallel corpus was parsed using the Stanford parser (Klein and Manning, 2003). The translation models were trained with Thrax, a grammar extractor for machine translation (Weese et al., 2011). Thrax extracts phrase pairs that are labeled with complex syntactic labels following Zollmann and Venugopal (2006). For MonoDS, the web-scale n-gram collection of Lin et al. (2010) was used to compute the monolingual distributional similarity features, using 512 bits per signature in the resultant LSH projection. Following Van Durme and Lall (2010), we implic1 Generating a paraphrase list by MonoDS alone requires building features for all phrases in the corpus, which is computationally impractical and hence, was not considered here. 36 itly represented the projection matrix with a pool of si"
W11-2504,P08-1116,0,0.0696498,"g words. A consideration for future work to enhance paraphrasal meaning preservation would be to explore other contextual representations, such as syntactic dependency parsing (Lin, 1997), mutual information between co-occurences of phrases Church and Hanks (1991), or increasing the number of neighboring words used in n-gram based representations. In future work we will make use of other complementary bilingual and monolingual knowledge sources by combining other features such as n-gram length, language model scores, etc. One approach would be to perform minimum error rate training similar to Zhao et al. (2008) in which linear weights of a feature function for a set of paraphrases candidate are trained iteratively to minimize the phrasalsubstitution-based error rate. Instead of phrasal substitution in Zhao’s method, quantitative measure of correlation with human judgment can be used as the objective function to be optimized during training. Other techniques such as SVM-rank (Joachims, 2002) may also be investigated for aggregating results from multiple ranked lists. 8 Acknowledgements Thanks to Courtney Napoles for advice regarding a pilot version of this work. Thanks to Jonathan Weese, Matt Post an"
W11-2504,W06-3119,0,0.0599526,"Koehn, 2005) was used to train the paraphrase model. The parallel corpus was extracted from proceedings of the European parliament with a total of about 1.3 million sentences and close to 97 million words in the English text. Word alignments were generated with the Berkeley aligner. For SyntBiP, the English side of the parallel corpus was parsed using the Stanford parser (Klein and Manning, 2003). The translation models were trained with Thrax, a grammar extractor for machine translation (Weese et al., 2011). Thrax extracts phrase pairs that are labeled with complex syntactic labels following Zollmann and Venugopal (2006). For MonoDS, the web-scale n-gram collection of Lin et al. (2010) was used to compute the monolingual distributional similarity features, using 512 bits per signature in the resultant LSH projection. Following Van Durme and Lall (2010), we implic1 Generating a paraphrase list by MonoDS alone requires building features for all phrases in the corpus, which is computationally impractical and hence, was not considered here. 36 itly represented the projection matrix with a pool of size 10,000. In order to expand the coverage of the candidates scored by the monolingual method, the LSH signatures ar"
W11-2504,J90-1003,0,\N,Missing
W11-2504,P01-1008,0,\N,Missing
W12-2109,P06-2005,0,0.0315501,"rsion for SMS, which we need to deromanize. Normalizing and deromanizing SMS messages would allow us to use existing MT engines, which are typically trained on well-formed sentences written in their native-script, in order to translate the messages. With this work, we use and release a corpus of 1 million (4, 195 annotated) anonymized text mesRelated Work There is a strong thread of research dedicated to normalizing Twitter and SMS informal English (Sproat et al., 2001). Choudhury et al. (2007) use a supervised English SMS dataset and build a characterlevel HMM to normalize individual tokens. Aw et al. (2006) model the same task using a statistical MT system, making the output context-sensitive at the cost of including a character-level analysis. More recently, Han and Baldwin (2011) use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. Beaufort et al. (2010) use a large amount of training data to supervise an FST-based French SMS normalizer. Li and Yarowsky (2008) present methods that take advantage of monolingual distributional similarities to identify the full form of abbreviated Chinese wor"
W12-2109,W10-0701,1,0.852786,"Missing"
W12-2109,P04-1021,0,0.07633,"Missing"
W12-2109,P11-1038,0,0.0205512,"sentences written in their native-script, in order to translate the messages. With this work, we use and release a corpus of 1 million (4, 195 annotated) anonymized text mesRelated Work There is a strong thread of research dedicated to normalizing Twitter and SMS informal English (Sproat et al., 2001). Choudhury et al. (2007) use a supervised English SMS dataset and build a characterlevel HMM to normalize individual tokens. Aw et al. (2006) model the same task using a statistical MT system, making the output context-sensitive at the cost of including a character-level analysis. More recently, Han and Baldwin (2011) use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. Beaufort et al. (2010) use a large amount of training data to supervise an FST-based French SMS normalizer. Li and Yarowsky (2008) present methods that take advantage of monolingual distributional similarities to identify the full form of abbreviated Chinese words. One challenge in working with SMS data is that public data is sparse (Chen and Kan, 2011). Transliteration is well-studied (Knight and Graehl, 1997; Haizhou et al., 2004; Li"
W12-2109,2010.amta-papers.12,1,0.898308,"Missing"
W12-2109,P97-1017,0,0.0600985,"el analysis. More recently, Han and Baldwin (2011) use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. Beaufort et al. (2010) use a large amount of training data to supervise an FST-based French SMS normalizer. Li and Yarowsky (2008) present methods that take advantage of monolingual distributional similarities to identify the full form of abbreviated Chinese words. One challenge in working with SMS data is that public data is sparse (Chen and Kan, 2011). Transliteration is well-studied (Knight and Graehl, 1997; Haizhou et al., 2004; Li et al., 2010) and is usually viewed as a subproblem of MT. With this work, we release a corpus of SMS messages and attempt to normalize Urdu SMS texts. Doing so involves the same challenges as normalizing English SMS texts and has the added complexity that we must also deromanize, a process similar to the transliteration task. 1 See http://www.cs.jhu.edu/˜anni/papers/ urduSMS/ for details about obtaining the corpus. 75 Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 75–78, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computat"
W12-2109,P08-1049,0,0.0143773,"t al., 2001). Choudhury et al. (2007) use a supervised English SMS dataset and build a characterlevel HMM to normalize individual tokens. Aw et al. (2006) model the same task using a statistical MT system, making the output context-sensitive at the cost of including a character-level analysis. More recently, Han and Baldwin (2011) use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. Beaufort et al. (2010) use a large amount of training data to supervise an FST-based French SMS normalizer. Li and Yarowsky (2008) present methods that take advantage of monolingual distributional similarities to identify the full form of abbreviated Chinese words. One challenge in working with SMS data is that public data is sparse (Chen and Kan, 2011). Transliteration is well-studied (Knight and Graehl, 1997; Haizhou et al., 2004; Li et al., 2010) and is usually viewed as a subproblem of MT. With this work, we release a corpus of SMS messages and attempt to normalize Urdu SMS texts. Doing so involves the same challenges as normalizing English SMS texts and has the added complexity that we must also deromanize, a proces"
W12-2109,W10-2401,0,\N,Missing
W12-2109,P10-1079,0,\N,Missing
W12-3102,W10-1703,1,0.557844,"Missing"
W12-3102,W11-2103,1,0.709065,"- ANNOTATOR AGREEMENT P (A) 0.567 0.576 0.595 0.598 0.540 0.504 0.568 0.519 0.568 0.601 P (A) 0.660 0.566 0.733 0.732 0.792 0.566 0.719 0.634 0.671 0.722 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.272 0.312 0.323 0.336 0.222 0.176 0.272 0.214 0.284 0.375 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.428 0.296 0.554 0.557 0.648 0.279 0.526 0.401 0.455 0.564 Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7). Agreement rates vary widely across languages. For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648. We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4. The agreement rates for this year were somewhat lower than last year. 3.3 Results of the Translation Task We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In ou"
W12-3102,D09-1030,1,0.149941,"Missing"
W12-3102,W12-3103,0,0.0434403,"Missing"
W12-3102,W11-2107,0,0.311039,"Missing"
W12-3102,W12-3131,0,0.0259292,"Missing"
W12-3102,W12-3133,0,0.0166865,"Missing"
W12-3102,W12-3134,1,0.0973948,"Missing"
W12-3102,W12-3135,0,0.0202653,"Missing"
W12-3102,W12-3111,0,0.0311487,"Missing"
W12-3102,W12-3136,0,0.0403021,"Missing"
W12-3102,W12-3112,0,0.176358,"Missing"
W12-3102,2011.eamt-1.32,0,0.0209858,"with only two linear equations. System “SDLLW SVM” uses a 20-feature set and an SVM epsilon regression model with radial basis function kernel with parameters C, gamma, and epsilon tuned on a development set (305 training instances). The model was trained with 10-fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting. The final model was selected based on its performance on the development set and the number of support vectors. UU (R, S): System “UU best” uses the 17 baseline features, plus 82 features from Hardmeier (2011) (with some redundancy and some overlap with baseline features), and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser. System “UU bltk” uses only the 17 baseline features plus constituency and dependency trees as above. The machine learning component in both cases is SVM regression (SVMlight software). For the ranking task, 29 the ranking induced by the regression output is used. The system uses polynomial kernels of degree 2 (UU best) and 3 (UU bltk) as well as two different types of"
W12-3102,W12-3137,0,0.0533752,"Missing"
W12-3102,W12-3106,0,0.0413119,"Missing"
W12-3102,W12-3145,0,0.0522389,"Missing"
W12-3102,W12-3146,0,0.0998404,"Missing"
W12-3102,W09-0415,0,0.0172168,"Missing"
W12-3102,W12-3150,1,0.188192,"Missing"
W12-3102,W12-3119,0,0.030516,"Missing"
W12-3102,W12-3151,0,0.046244,"Missing"
W12-3102,W12-3117,0,\N,Missing
W12-3102,W12-3138,0,\N,Missing
W12-3102,W12-3130,0,\N,Missing
W12-3102,W12-3148,0,\N,Missing
W12-3102,W12-3114,0,\N,Missing
W12-3102,W12-3107,0,\N,Missing
W12-3102,W12-3139,1,\N,Missing
W12-3102,W11-2108,0,\N,Missing
W12-3102,W09-0401,1,\N,Missing
W12-3102,W12-3140,0,\N,Missing
W12-3102,W12-3132,0,\N,Missing
W12-3102,W12-3109,0,\N,Missing
W12-3102,W10-1711,0,\N,Missing
W12-3102,2010.iwslt-evaluation.22,0,\N,Missing
W12-3102,W07-0718,1,\N,Missing
W12-3102,W06-3114,1,\N,Missing
W12-3102,2009.eamt-1.5,1,\N,Missing
W12-3102,W12-3144,0,\N,Missing
W12-3102,W11-2158,0,\N,Missing
W12-3102,W12-3110,1,\N,Missing
W12-3102,W08-0309,1,\N,Missing
W12-3102,W12-3105,0,\N,Missing
W12-3102,W12-3149,0,\N,Missing
W12-3102,2011.eamt-1.12,1,\N,Missing
W12-3102,W04-3250,1,\N,Missing
W12-3102,W12-3142,0,\N,Missing
W12-3102,W11-2113,0,\N,Missing
W12-3102,W12-3143,0,\N,Missing
W12-3102,W11-2145,0,\N,Missing
W12-3102,W12-3147,0,\N,Missing
W12-3102,W12-3101,0,\N,Missing
W12-3102,W12-3115,0,\N,Missing
W12-3102,W11-2101,0,\N,Missing
W12-3102,W12-3113,0,\N,Missing
W12-3102,W12-3104,0,\N,Missing
W12-3102,W12-3108,0,\N,Missing
W12-3102,W12-3118,1,\N,Missing
W12-3127,C69-0101,0,0.542942,"Missing"
W12-3127,W07-0702,0,0.0219272,"roaches including unsupervised clustering (Zollmann and Vogel, 2011), merging (Hanneman et al., 2011), and selection (Mylonakis and Sima’an, 2011) of labels derived from phrasestructure parse trees very much like those used by our baseline systems. What we find particularly attractive about CCG is that it naturally assigns linguistically-motivated labels to most spans of a sentence using a reasonably concise label set, possibility obviating the need for further refinement. Indeed, the analytical flexibility of CCG has motivated its increasing use in MT, from applications in language modeling (Birch et al., 2007; Hassan et al., 2007) to more recent proposals to incorporate it into phrase-based (Mehay, 2010) and hierarchical translation systems (Auli, 2009). Our new model builds on these past efforts, representing a more fully instantiated model of CCGbased translation. We have shown that the label scheme allows us to keep many more translation rules than labels based on phrase structure syntax, extracting almost as many rules as the SAMT model, but keeping the label set an order of magnitude smaller, which leads to more efficient translation. This simply scratches the surface of possible uses of CCG"
W12-3127,J07-2003,0,0.8251,"(Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu–English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast. 1 N → h maison ; house i Introduction The Hiero model of Chiang (2007) popularized the usage of synchronous context-free grammars (SCFGs) for machine translation. SCFGs model translation as a process of isomorphic syntactic derivation in the source and target language. But the Hiero model is formally, not linguistically syntactic. Its derivation trees use only a single non-terminal label X, carrying no linguistic information. Consider Rule 1. X → h maison ; house i (1) We can add syntactic information to the SCFG rules by parsing the parallel training data and projecting parse tree labels onto the spans they yield and But we quickly run into trouble: how should"
W12-3127,J07-4004,0,0.297028,"categorial grammar (CCG) is an extension of CG that includes more combinators (operations that can combine categories). Steedman and Baldridge (2011) give an excellent overview of CCG. As an example, suppose we want to analyze the sentence “They own properties in various cities and villages” using the lexicon shown in Table 1. We assign categories according to the lexicon, then combine the categories using function application and other combinators to get an analysis of S for the complete sentence. Figure 1 shows the derivation. As a practical matter, very efficient CCG parsers are available (Clark and Curran, 2007). As shown by Fowler and Penn (2010), in many cases CCG is context-free, making it an ideal fit for our problem. 2.1 Labels for phrases Consider the German–English phrase pair der große Mann – the tall man. It is easily labeled as an NP and included in the translation table. By contrast, der große– the tall, doesn’t typically correspond to a complete subtree in a phrase structure parse. Yet translating the tall is likely to be more useful than translating the tall man, since it is more general—it can be combined with any other noun translation. T hey own properties in various cities and villag"
W12-3127,D07-1079,0,0.0189088,"bel a rule that translates pour l’´etablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While these heuristics are effect"
W12-3127,N09-1026,0,0.0135355,"ponds to (possibly many) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence. It then produces an augmented set of items hA, i, j, u, vi, in which items of the first type are augmented with left and right language model states u and v. In each pass, the number of items is linear in the number of nonterminal symbols of the grammar. This observation has motivated work in grammar transformations that reduce the size of the nonterminal set, often resulting in substantial gains in parsing or translation speed (Song et al., 2008; DeNero et al., 2009; Xiao et al., 2009). More formally, the upper bound on parsing complexity is always at least linear in the size of the grammar constant G, where G is often loosely defined as a grammar constant; Iglesias et al. (2011) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al. (2010) provide a more fine-grained analysis of G, showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar. Though these are worst-case analyses, it should be clear t"
W12-3127,P10-1035,0,0.0493258,"ion of CG that includes more combinators (operations that can combine categories). Steedman and Baldridge (2011) give an excellent overview of CCG. As an example, suppose we want to analyze the sentence “They own properties in various cities and villages” using the lexicon shown in Table 1. We assign categories according to the lexicon, then combine the categories using function application and other combinators to get an analysis of S for the complete sentence. Figure 1 shows the derivation. As a practical matter, very efficient CCG parsers are available (Clark and Curran, 2007). As shown by Fowler and Penn (2010), in many cases CCG is context-free, making it an ideal fit for our problem. 2.1 Labels for phrases Consider the German–English phrase pair der große Mann – the tall man. It is easily labeled as an NP and included in the translation table. By contrast, der große– the tall, doesn’t typically correspond to a complete subtree in a phrase structure parse. Yet translating the tall is likely to be more useful than translating the tall man, since it is more general—it can be combined with any other noun translation. T hey own properties in various cities and villages NP (SN P )/N P NP (N P N P )/N"
W12-3127,W02-1039,0,0.0248982,"ecting parse tree labels onto the spans they yield and But we quickly run into trouble: how should we label a rule that translates pour l’´etablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT"
W12-3127,N04-1035,0,0.0855087,"des majorité la Pour PP , IN For For NP JJ NN most people most people Then we can assign syntactic labels to Rule 4 to produce PP → h Pour NP ; For NP i (5) , Figure 3: A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example. The focus of this paper is how to assign labels to the left-hand non-terminal X and to the nonterminal gaps on the right-hand side. We discuss five models below, of which two are novel CG-based labeling schemes. 3.2 The rules extracted by this scheme are very similar to those produced by GHKM (Galley et al., 2004), in particular resulting in the “composed rules” of Galley et al. (2006), though we use simpler heuristics for handling of unaligned words and scoring in order to bring the model in line with both Hiero and SAMT baselines. Under this scheme we throw away a lot of useful translation rules that don’t translate exact syntactic constituents. For example, we can’t label Baseline: Hiero X → h Pour la majorit´e des ; For most i Hiero (Chiang, 2007) uses the simplest labeling possible: there is only one non-terminal symbol, X, for all rules. Its advantage over phrase-based translation in its ability"
W12-3127,P06-1121,0,0.066652,"n we can assign syntactic labels to Rule 4 to produce PP → h Pour NP ; For NP i (5) , Figure 3: A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example. The focus of this paper is how to assign labels to the left-hand non-terminal X and to the nonterminal gaps on the right-hand side. We discuss five models below, of which two are novel CG-based labeling schemes. 3.2 The rules extracted by this scheme are very similar to those produced by GHKM (Galley et al., 2004), in particular resulting in the “composed rules” of Galley et al. (2006), though we use simpler heuristics for handling of unaligned words and scoring in order to bring the model in line with both Hiero and SAMT baselines. Under this scheme we throw away a lot of useful translation rules that don’t translate exact syntactic constituents. For example, we can’t label Baseline: Hiero X → h Pour la majorit´e des ; For most i Hiero (Chiang, 2007) uses the simplest labeling possible: there is only one non-terminal symbol, X, for all rules. Its advantage over phrase-based translation in its ability to model phrases with gaps in them, enabling phrases to reorder subphrase"
W12-3127,W11-1015,0,0.0126196,"aller models (Hiero, phrase structure syntax, and CCG 1-best derivations) are significantly faster than the two larger ones. However, even though the CCG parse chart model is almost 34 the size of SAMT in terms of number of rules, it doesn’t take 43 of the 5 LDC2009T13 229 Discussion and Future Work Finding an appropriate mechanism to inform phrasebased translation models and their hierarchical variants with linguistic syntax is a difficult problem that has attracted intense interest, with a variety of promising approaches including unsupervised clustering (Zollmann and Vogel, 2011), merging (Hanneman et al., 2011), and selection (Mylonakis and Sima’an, 2011) of labels derived from phrasestructure parse trees very much like those used by our baseline systems. What we find particularly attractive about CCG is that it naturally assigns linguistically-motivated labels to most spans of a sentence using a reasonably concise label set, possibility obviating the need for further refinement. Indeed, the analytical flexibility of CCG has motivated its increasing use in MT, from applications in language modeling (Birch et al., 2007; Hassan et al., 2007) to more recent proposals to incorporate it into phrase-based"
W12-3127,P07-1037,0,0.0558347,"Missing"
W12-3127,D11-1127,0,0.0254243,"Missing"
W12-3127,N03-1017,0,0.0972877,"le: how should we label a rule that translates pour l’´etablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While thes"
W12-3127,P07-2045,1,0.0106061,"n task, in which syntax-based systems have been quite effective (Baker et al., 2009; Zollmann et al., 2008). The training corpus was the National Institute of Standards and Technology Open Machine Translation 2009 Evaluation (NIST Open MT09). According to the MT09 Constrained Training Con228 ditions Resources list2 this data includes NIST Open MT08 Urdu Resources3 and the NIST Open MT08 Current Test Set Urdu–English4 . This gives us 202,019 parallel translations, for approximately 2 million words of training data. 5.2 Experimental design We used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used th"
W12-3127,P11-1065,0,0.112476,"Missing"
W12-3127,P00-1056,0,0.0124437,"lation 2009 Evaluation (NIST Open MT09). According to the MT09 Constrained Training Con228 ditions Resources list2 this data includes NIST Open MT08 Urdu Resources3 and the NIST Open MT08 Current Test Set Urdu–English4 . This gives us 202,019 parallel translations, for approximately 2 million words of training data. 5.2 Experimental design We used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used the same feature set in all the translation grammars. This includes, for each rule C → hf ; ei, relative-frequency estimates of the probabil2 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/MT09_Constrain"
W12-3127,P03-1021,0,0.0350882,"nly half the time of the SAMT model, thanks to the smaller rule label set. 6 Table 3: Results of translation experiments on Urdu–English. Higher BLEU scores are better. BLEU’s brevity penalty is reported in parentheses. ities p(f |A), p(f |e), p(f |e, A), p(e|A), p(e|f ), and p(e|f, A). The feature set also includes lexical weighting for rules as defined by Koehn et al. (2003) and various binary features as well as counters for the number of unaligned words in each rule. To train the feature weights we used the Z-MERT implementation (Zaidan, 2009) of the Minimum Error-Rate Training algorithm (Och, 2003). To decode the test sets, we used the Joshua machine translation decoder (Weese et al., 2011). The language model is a 5-gram LM trained on English GigaWord Fourth Edition.5 5.3 Evaluation criteria We measure machine translation performance using the BLEU metric (Papineni et al., 2002). We also report the translation time for the test set in seconds per sentence. These results are shown in Table 3. All of the syntactic labeling schemes show an improvement over the Hiero model. Indeed, they all fall in the range of approximately 27–28 BLEU. We can see that the 1-best derivation CCG model perfo"
W12-3127,P02-1040,0,0.0852201,"d p(e|f, A). The feature set also includes lexical weighting for rules as defined by Koehn et al. (2003) and various binary features as well as counters for the number of unaligned words in each rule. To train the feature weights we used the Z-MERT implementation (Zaidan, 2009) of the Minimum Error-Rate Training algorithm (Och, 2003). To decode the test sets, we used the Joshua machine translation decoder (Weese et al., 2011). The language model is a 5-gram LM trained on English GigaWord Fourth Edition.5 5.3 Evaluation criteria We measure machine translation performance using the BLEU metric (Papineni et al., 2002). We also report the translation time for the test set in seconds per sentence. These results are shown in Table 3. All of the syntactic labeling schemes show an improvement over the Hiero model. Indeed, they all fall in the range of approximately 27–28 BLEU. We can see that the 1-best derivation CCG model performs slightly better than the phrase structure model, and the CCG parse chart model performs a little better than that. SAMT has the highest BLEU score. The models with a larger number of rules perform better; this supports our assertion that we shouldn’t throw away too many rules. When"
W12-3127,N07-1051,0,0.0195356,"includes NIST Open MT08 Urdu Resources3 and the NIST Open MT08 Current Test Set Urdu–English4 . This gives us 202,019 parallel translations, for approximately 2 million words of training data. 5.2 Experimental design We used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used the same feature set in all the translation grammars. This includes, for each rule C → hf ; ei, relative-frequency estimates of the probabil2 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/MT09_ConstrainedResources.pdf 3 LDC2009E12 4 LDC2009E11 Model Hiero Syntax SAMT CCG derivations CCG parse chart BLEU 25.67 (0.9781) 27.06 (0.970"
W12-3127,D08-1018,0,0.0175781,"re each item corresponds to (possibly many) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence. It then produces an augmented set of items hA, i, j, u, vi, in which items of the first type are augmented with left and right language model states u and v. In each pass, the number of items is linear in the number of nonterminal symbols of the grammar. This observation has motivated work in grammar transformations that reduce the size of the nonterminal set, often resulting in substantial gains in parsing or translation speed (Song et al., 2008; DeNero et al., 2009; Xiao et al., 2009). More formally, the upper bound on parsing complexity is always at least linear in the size of the grammar constant G, where G is often loosely defined as a grammar constant; Iglesias et al. (2011) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al. (2010) provide a more fine-grained analysis of G, showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar. Though these are worst-case analyses,"
W12-3127,W03-1001,0,0.0159877,"rties in various cities and villages” using the lexicon from Table 1. Φ indicates a conjunction operation; > and &lt; are forward and backward function application, respectively. Extraction from parallel text To extract SCFG rules, we start with a heuristic to extract phrases from a word-aligned sentence pair 1 We could assign NP/N to the determiner the and N/N to the adjective tall, then combine those two categories using function composition to get a category NP/N for the two words together. 224 , Figure 2: A word-aligned sentence pair fragment, with a box indicating a consistent phrase pair. (Tillmann, 2003). Figure 2 shows a such a pair, with a consistent phrase pair inside the box. A phrase pair (f, e) is said to be consistent with the alignment if none of the words of f are aligned outside the phrase e, and vice versa – that is, there are no alignment points directly above, below, or to the sides of the box defined by f and e. Given a consistent phrase pair, we can immediately extract the rule X → hf, ei (3) as we would in a phrase-based MT system. However, whenever we find a consistent phrase pair that is a sub-phrase of another, we may extract a hierarchical rule by treating the inner phrase"
W12-3127,W11-2160,1,0.869162,"e used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used the same feature set in all the translation grammars. This includes, for each rule C → hf ; ei, relative-frequency estimates of the probabil2 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/MT09_ConstrainedResources.pdf 3 LDC2009E12 4 LDC2009E11 Model Hiero Syntax SAMT CCG derivations CCG parse chart BLEU 25.67 (0.9781) 27.06 (0.9703) 28.06 (0.9714) 27.3 (0.9770) 27.64 (0.9673) sec./sent. 0.05 3.04 63.48 5.24 33.6 time. In fact, it takes only half the time of the SAMT model, thanks to the smaller rule label set. 6 Table 3: Results of translat"
W12-3127,D09-1038,0,0.0154381,"ny) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence. It then produces an augmented set of items hA, i, j, u, vi, in which items of the first type are augmented with left and right language model states u and v. In each pass, the number of items is linear in the number of nonterminal symbols of the grammar. This observation has motivated work in grammar transformations that reduce the size of the nonterminal set, often resulting in substantial gains in parsing or translation speed (Song et al., 2008; DeNero et al., 2009; Xiao et al., 2009). More formally, the upper bound on parsing complexity is always at least linear in the size of the grammar constant G, where G is often loosely defined as a grammar constant; Iglesias et al. (2011) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al. (2010) provide a more fine-grained analysis of G, showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar. Though these are worst-case analyses, it should be clear that grammars with fe"
W12-3127,W06-3119,0,0.509168,"tablishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While these heuristics are effective (Zollmann et al., 2008), they inflate grammar size, hamper effective pa"
W12-3127,P11-1001,0,0.061644,"es to translation time, the three smaller models (Hiero, phrase structure syntax, and CCG 1-best derivations) are significantly faster than the two larger ones. However, even though the CCG parse chart model is almost 34 the size of SAMT in terms of number of rules, it doesn’t take 43 of the 5 LDC2009T13 229 Discussion and Future Work Finding an appropriate mechanism to inform phrasebased translation models and their hierarchical variants with linguistic syntax is a difficult problem that has attracted intense interest, with a variety of promising approaches including unsupervised clustering (Zollmann and Vogel, 2011), merging (Hanneman et al., 2011), and selection (Mylonakis and Sima’an, 2011) of labels derived from phrasestructure parse trees very much like those used by our baseline systems. What we find particularly attractive about CCG is that it naturally assigns linguistically-motivated labels to most spans of a sentence using a reasonably concise label set, possibility obviating the need for further refinement. Indeed, the analytical flexibility of CCG has motivated its increasing use in MT, from applications in language modeling (Birch et al., 2007; Hassan et al., 2007) to more recent proposals to"
W12-3127,C08-1144,0,0.0991498,"tax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While these heuristics are effective (Zollmann et al., 2008), they inflate grammar size, hamper effective parameter estimation due to feature sparsity, and slow translation speed. Our objective is to find a syntactic formalism that 222 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 222–231, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics enables us to label most translation rules without relying on heuristics. Ideally, the label should be small in order to improve feature estimation and reduce translation time. Furthering an insight that informs SAMT, we show that combinatory categorial gra"
W12-3127,2010.iwslt-papers.1,0,\N,Missing
W12-3134,P05-1074,1,0.106708,"9h18m 25h46m 28h10m Rules 223M 328M 654M 716M Table 4: Extraction times and grammar sizes for the SAMT grammars using the Europarl and News Commentary training data for each listed language pair. 4.2 Translation Grammars Time 4h41m 5h20m 16h47m 16h22m Paraphrase Extraction Recently English-to-English text generation tasks have seen renewed interest in the NLP community. Paraphrases are a key component in largescale state-of-the-art text-to-text generation systems. We present an extended version of Thrax that implements distributed, Hadoop-based paraphrase extraction via the pivoting approach (Bannard and Callison-Burch, 2005). Our toolkit is capable of extracting syntactically informed paraphrase grammars at scale. The paraphrase grammars obtained with Thrax have been shown to achieve state-of-theart results on text-to-text generation tasks (Ganitkevitch et al., 2011). For every supported translation feature, Thrax implements a corresponding pivoted feature for paraphrases. The pivoted features are set up to be aware of the prerequisite translation features they are derived from. This allows Thrax to automatically detect the needed translation features and spawn the corresponding map-reduce passes before the pivot"
W12-3134,N10-1033,0,0.0375622,"a. With appropriate pruning settings, we are able to obtain paraphrase grammars estimated over bitexts with more than 100 million words. 5 Additional New Features • With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002). • We modified Joshua so that it can be used as a parser to analyze pairs of sentences using a synchronous context-free grammar. We implemented the two-pass parsing algorithm of Dyer (2010). 6 Conclusion We present a new iteration of the Joshua machine translation toolkit. Our system has been extended towards efficiently supporting large-scale experiments 290 in parsing-based machine translation and text-to-text generation: Joshua 4.0 supports compactly represented large grammars with its packed grammars, as well as large language models via KenLM and BerkeleyLM.We include an implementation of PRO, allowing for stable and fast tuning of large feature sets, and extend our toolkit beyond pure translation applications by extending Thrax with a large-scale paraphrase extraction modu"
W12-3134,W06-3113,0,0.0178341,"pes, as well as an 8-bit quantizer. We chose 8 bit as a compromise between compression, value decoding speed and translaHiero (43M rules) Syntax (200M rules) Format Baseline Packed Baseline Packed Packed 8-bit Memory 13.6G 1.8G 99.5G 9.8G 5.8G Table 1: Decoding-time memory use for the packed grammar versus the standard grammar format. Even without lossy quantization the packed grammar representation yields significant savings in memory consumption. Adding 8-bit quantization for the realvalued features in the grammar reduces even large syntactic grammars to a manageable size. tion performance (Federico and Bertoldi, 2006). Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets. We quantize by mapping each feature value onto the weighted average of its bucket. Joshua allows for an easily per-feature specification of type. Quantizers can be share statistics across multiple features with similar value distributions. 2.2 Experiments We assess the packed grammar representation’s memory efficiency and impact on the decoding speed on the WMT12 French-English task. Table 1 shows a comparison of the memory needed to store our W"
W12-3134,D11-1108,1,0.323188,"Missing"
W12-3134,W11-2123,0,0.182249,"Using more diverse sets of nonterminal labels – which can significantly improve translation performance – further aggravates this problem. As a consequence, the space requirements for storing the grammar in memory during decoding quickly grow impractical. In some cases grammars may become too large to fit into the memory on a single machine. As an alternative to the commonly used trie structures based on hash maps, we propose a packed trie representation for SCFGs. The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011) – both language model implementations are now integrated with Joshua. 2.1 Packed Synchronous Tries For our grammar representation, we break the SCFG up into three distinct structures. As Figure 1 indicates, we store the grammar rules’ source sides {αi }, target sides {γi }, and feature data {~ ϕi } in separate formats of their own. Each of the structures is packed into a flat array, and can thus be quickly read into memory. All terminal and nonterminal symbols in the grammar are mapped to integer symbol id’s using a globally accessible vocabulary map. We will now de"
W12-3134,D11-1125,0,0.25698,"paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality. Additionally, we present Joshua’s implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. J-PRO, like Z-MERT, makes it easy to implement new metrics and comes with both a built-in perceptron classifier and out-of-the-box support for widely used binary classifiers such as MegaM and MaxEnt (Daum´e III and Marcu, 2006; Manning and Klein, 2003). We describe our implementation in Section 3, presenting experimental results on performance, classifier convergence, and tuning speed. Finally, we introduce the inclusion of bilingual pivoting-based paraphrase extraction into Thrax, Joshua’s grammar extractor. Thrax’s paraphrase extraction mode is simple t"
W12-3134,W09-0424,1,0.601865,"4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both ne"
W12-3134,W10-1718,1,0.942532,"mar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental result"
W12-3134,C10-2075,0,0.0606954,"mar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental result"
W12-3134,N03-5008,0,0.0335893,"and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality. Additionally, we present Joshua’s implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. J-PRO, like Z-MERT, makes it easy to implement new metrics and comes with both a built-in perceptron classifier and out-of-the-box support for widely used binary classifiers such as MegaM and MaxEnt (Daum´e III and Marcu, 2006; Manning and Klein, 2003). We describe our implementation in Section 3, presenting experimental results on performance, classifier convergence, and tuning speed. Finally, we introduce the inclusion of bilingual pivoting-based paraphrase extraction into Thrax, Joshua’s grammar extractor. Thrax’s paraphrase extraction mode is simple to use, and yields state-ofthe-art syntactically informed sentential paraphrases (Ganitkevitch et al., 2011). The full feature set of Thrax (Weese et al., 2011) is supported for paraphrase grammars. An easily configured feature-level pruning mechanism allows to keep the paraphrase grammar si"
W12-3134,P03-1021,0,0.00352708,"d format). Even though decoding speed is slightly slower with the packed grammars (an average of 5.3 seconds per sentence versus 4.2 for the baseline), the effective translation speed is more than twice that of the baseline (1004 seconds to complete decoding the 2489 sentences, versus 2551 seconds with the standard setup). 3 J-PRO: Pairwise Ranking Optimization in Joshua Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation. It is reported to be more stable than the popular MERT algorithm (Och, 2003) and is more scalable with regard to the number of features. PRO treats parameter tuning as an n-best list reranking problem, and the idea is similar to other pairwise ranking techniques like ranking SVM and IR SVMs (Li, 2011). The algorithm can be described thusly: Let h(c) = hw, Φ(c)i be the linear model score of a candidate translation c, in which Φ(c) is the feature vector of c and w is the parameter vector. Also let g(c) be the metric score of c (without loss of generality, we assume a higher score indicates a better translation). We aim to find a parameter vector w such that for a pair o"
W12-3134,P11-1027,0,0.0329495,"sets of nonterminal labels – which can significantly improve translation performance – further aggravates this problem. As a consequence, the space requirements for storing the grammar in memory during decoding quickly grow impractical. In some cases grammars may become too large to fit into the memory on a single machine. As an alternative to the commonly used trie structures based on hash maps, we propose a packed trie representation for SCFGs. The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011) – both language model implementations are now integrated with Joshua. 2.1 Packed Synchronous Tries For our grammar representation, we break the SCFG up into three distinct structures. As Figure 1 indicates, we store the grammar rules’ source sides {αi }, target sides {γi }, and feature data {~ ϕi } in separate formats of their own. Each of the structures is packed into a flat array, and can thus be quickly read into memory. All terminal and nonterminal symbols in the grammar are mapped to integer symbol id’s using a globally accessible vocabulary map. We will now describe the implementation d"
W12-3134,W11-2160,1,0.362052,"ension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality. Additionally, we present Joshua’s implementation of the pairwise ranking opti"
W12-3134,N07-1062,0,0.0137962,"so does the resulting translation grammar. Using more diverse sets of nonterminal labels – which can significantly improve translation performance – further aggravates this problem. As a consequence, the space requirements for storing the grammar in memory during decoding quickly grow impractical. In some cases grammars may become too large to fit into the memory on a single machine. As an alternative to the commonly used trie structures based on hash maps, we propose a packed trie representation for SCFGs. The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011) – both language model implementations are now integrated with Joshua. 2.1 Packed Synchronous Tries For our grammar representation, we break the SCFG up into three distinct structures. As Figure 1 indicates, we store the grammar rules’ source sides {αi }, target sides {γi }, and feature data {~ ϕi } in separate formats of their own. Each of the structures is packed into a flat array, and can thus be quickly read into memory. All terminal and nonterminal symbols in the grammar are mapped to integer symbol id’s using a globally ac"
W12-3134,J07-2003,0,\N,Missing
W12-3152,W10-0710,0,0.068614,"t guess, an X marks a false positive, and a • denotes a false negative. Hiero’s extraction heuristics yield 4 rules for the top alignment and 16 for the bottom. reasonable translation accuracy. Closely related to our work here is that of Novotney and Callison-Burch (2010), who showed that transcriptions for training speech recognition systems could be obtained from Mechanical Turk with near baseline recognition performance and at a significantly lower cost. They also showed that redundant annotation was not worthwhile, and suggested that money was better spent obtaining more data. Separately, Ambati and Vogel (2010) probed the MTurk worker pool for workers capable of translating a number of low-resource languages, including Hindi, Telugu, and Urdu, demonstrating that such workers could be found and quantifying acceptable 408 We have described the collection of six parallel corpora containing four-way redundant translations of the source-language text. The Indian languages of these corpora are low-resource and understudied, and exhibit markedly different linguistic properties compared to English. We performed baseline experiments quantifying the translation performance of a number of systems, investigated"
W12-3152,J07-2003,0,0.227477,"present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that convey information such as tense, person, number, gender, mood, and voice. Morphological complexity is a considerable hindrance at all stages of the MT pipeline, but particularly alignment, where inflectional variations mask patterns from alignment tools that treat words as atoms. 3 We use hierarchical to denote translation grammars that use only a single nonterminal (Chiang, 2007), in contrast to syntactic systems, which make use of linguistic annotations (Zollmann and Venugopal, 2006; Galley et al., 2006). 402 The source of the documents for our translation task for each of the languages in Table 1 was the set of the top-100 most-viewed documents from each language’s Wikipedia. These lists were obtained using page view statistics compiled from dammit.lt/ wikistats over a one year period. We did not apply any filtering for topic or content. Table 2 contains a manually categorized list of documents for Hindi, with some minimal annotations indicating how the documents re"
W12-3152,P11-2031,0,0.0160195,"on results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale language model built from English Gigaword, but, notably, found a drop of over a point in BLEU score. This points forward to s"
W12-3152,N04-1035,0,0.0124259,"able 1: Languages. L1 is the worldwide number of native speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morpho"
W12-3152,P06-1121,0,0.00996435,"is the worldwide number of native speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that c"
W12-3152,W01-1409,0,0.0421683,"ally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequent quality control HITs, like editing). The tradeoff for low-cost translations is increased variance in translation quality when compared to the more consistently-good professional translations. Figure 4 contains some hand-picked"
W12-3152,N03-1017,0,0.0236059,"g data (plus dictionary), selected in two ways: best (result of vote), and random. There is little difference, suggesting quality control may not be terribly important. We did not collect votes for Malayalam. misspelling japenese japans japenes japenies japeneses japeneese japense count 91 40 9 3 3 1 1 Table 8: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts. 5.2 pairs and data conditions. Figure 6 (top) contains an example of a particularly poor alignment produced by the default alignment heuristic, the grow-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, a"
W12-3152,P07-2045,1,0.0155163,"spelling japenese japans japenes japenies japeneses japeneese japense count 91 40 9 3 3 1 1 Table 8: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts. 5.2 pairs and data conditions. Figure 6 (top) contains an example of a particularly poor alignment produced by the default alignment heuristic, the grow-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, and found a consistent increase in BLEU score for the Hiero grammars, often putting them on par with the original SAMT results (Table 9). Manual analysis suggests that the Berkeley aligner produces fewer, more rea"
W12-3152,2005.mtsummit-papers.11,0,0.0437349,"ask, and likely desire to maximize their throughput (and thus their wage). Unlike Zaidan and CallisonBurch (2011), who embed controls containing source language sentences with known professional translations, we had no professionally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequ"
W12-3152,N04-1022,0,0.0140679,"data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale language model built from English Gigaword, but, notably, found a drop of over a point in BLEU score. This points forward to some of the difficulties encountered wit"
W12-3152,N06-1014,0,0.0128778,"w-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, and found a consistent increase in BLEU score for the Hiero grammars, often putting them on par with the original SAMT results (Table 9). Manual analysis suggests that the Berkeley aligner produces fewer, more reasonable-looking alignments than the Moses heuristics (Figure 6). This suggest a fruitful approaches in revisiting assumptions underlying alignment heuristics. 6 Related Work Alignments Inconsistent orthography fragments the training data, exacerbating problems already present due to morpohological richness. One place this is manifested is during alignment,"
W12-3152,N10-1024,1,0.728684,"(to increase quality) or translate more foreign sentences (to increase coverage). To test this, we constructed two smaller datasets, each making use of only one of the four translations of each source sentence: • Selected randomly • Selected by choosing the translation that received a plurality of the votes (§3.3), breaking ties randomly (best) We again included the dictionaries in the training data (where available). Table 7 contains results on the same test sets as before. These results do not clearly indicate that quality control through redundant translations are worth the extra expense. Novotney and Callison-Burch (2010) had a similar finding for crowdsourced transcriptions. 5 Further Analysis The previous section has shown that reasonable BLEU scores can be obtained from baseline translation systems built from these corpora. While translation quality is an issue (for example, very lit406 eral translations, etc), the previous section’s voted dataset experiments suggest this is not one of the most important issues to address. In this section, we undertake a manual analysis of the collected datasets to inform future work. There are a number of issues that arise due to non-Roman scripts, high-variance translatio"
W12-3152,P03-1021,0,0.0249439,". How well can we translate the test sets? 2. Do linguistically motivated translation models improve translation results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale languag"
W12-3152,W11-2160,1,0.493032,"ance. The experiments aim to address the following questions: 1. How well can we translate the test sets? 2. Do linguistically motivated translation models improve translation results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoo"
W12-3152,P11-1122,1,0.421888,"e m s s fu ov ll i fo e r ✓ ✓ ✓ X ✓ ✓ ✓ . pair Bengali Hindi GIZA++ Berkeley 15m 34m 12m 19m 28m 38m 27m 60m 27m 30m 46m 58m Malayalam Tamil Telugu Urdu BLEU 13.54 16.47 12.70 10.10 13.36 20.41 gain +0.82 +0.94 -1.02 +0.29 +0.90 +0.88 Table 9: Hiero translation results using Berkeley alignments instead of GIZA++ heuristics. The gain columns denotes improvements relative to the Hiero systems in Table 5. In many cases (bold gains), the BLEU scores are at or above even the SAMT models from that table. wages and collection rates. The techniques described here are similar to those அ&quot;# described in Zaidan and Callison-Burch (2011), who $மா&apos;( showed that crowdsourcing with appropriate quality controls could be used to produce professional-level )த+ translations for Urdu-English translation. This paெவ./0 per extends that work by applying their techniques to a larger set of Indian languages and scaling it to பட3 training-data-set sizes. ஆைச ✓ X X ✓ 7 Summary . Figure 6: A bad Tamil alignment produced with the grow-diag-and alignment combination heuristic (top); the Berkeley aligner is better (bottom). A ✓ is a correct guess, an X marks a false positive, and a • denotes a false negative. Hiero’s extraction heuristics yield"
W12-3152,N12-1006,1,0.732385,"taining source language sentences with known professional translations, we had no professionally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequent quality control HITs, like editing). The tradeoff for low-cost translations is increased variance in translation quality when compared to t"
W12-3152,W06-3119,0,0.0610624,"tive speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that convey information such as tense, person"
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-2226,D11-1125,0,0.266708,"rgument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et al., 2006a; Tillmann and Zhang, 2006; Chiang et al., 2008; Hopkins and May, 2011) has made it possible to tune large numbers of features in statistical machine translation systems, and open2 nlp.stanford.edu/˜mgalley/software/ stanford-ghkm-latest.tar.gz 3 Roughly, the number of consecutive nonterminals in a rule (Hopkins and Langmead, 2010). 2.3 Decoding 207 source implementations such as Cherry and Foster (2012) have made it easy. Joshua 5.0 has moved to a sparse feature representation internally. First, to clarify terminology, a feature as implemented in the decoder is actually a template that can introduce any number of actual features (in the standard machine learning"
W13-2226,D11-1033,0,0.0297921,"uned our systems with kbMIRA. For truecasing, we used a monolingual translation system built on the training data, and finally detokenized with simple heuristics. Other features Joshua 5.0 also includes many features designed to increase its usability. These include: • A TCP/IP server architecture, designed to handle multiple sets of translation requests while ensuring fairness in thread assignment both across and within these connections. 5 • Intelligent selection of translation and language model training data using crossentropy difference to rank training candidates (Moore and Lewis, 2010; Axelrod et al., 2011) (described in detail in Orland (2013)). The 5.0 release of Joshua is the result of a significant year-long research, engineering, and usability effort that we hope will be of service to the research community. User-friendly packages of Joshua are available from joshua-decoder. org, while developers are encouraged to participate via github.com/joshua-decoder/ joshua. Mailing lists, linked from the main Joshua page, are available for both. • A bundler for easy packaging of trained models with all of its dependencies. • A year’s worth of improvements to the Joshua pipeline, including many new fe"
W13-2226,2005.mtsummit-papers.11,0,0.0472789,"Missing"
W13-2226,W09-0424,1,0.885997,"Missing"
W13-2226,N12-1047,0,0.254536,"search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily spe"
W13-2226,W10-1718,1,0.902749,"Missing"
W13-2226,D08-1024,0,0.0148575,"and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et al., 2006a; Tillmann and Zhang, 2006; Chiang et al., 2008; Hopkins and May, 2011) has made it possible to tune large numbers of features in statistical machine translation systems, and open2 nlp.stanford.edu/˜mgalley/software/ stanford-ghkm-latest.tar.gz 3 Roughly, the number of consecutive nonterminals in a rule (Hopkins and Langmead, 2010). 2.3 Decoding 207 source implementations such as Cherry and Foster (2012) have made it easy. Joshua 5.0 has moved to a sparse feature representation internally. First, to clarify terminology, a feature as implemented in the decoder is actually a template that can introduce any number of actual features (in the s"
W13-2226,P05-1033,0,0.0585045,"th MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which general"
W13-2226,P06-1096,0,0.127287,"Missing"
W13-2226,J07-2003,0,0.0799434,"binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 200"
W13-2226,P09-2036,0,0.0202158,"steps in arbitrary acyclic dependency graphs (much like the U NIX make tool, but written with machine translation in mind). Joshua’s pipeline is more limited in that the basic pipeline skeleton is hard-coded, but reduced versatility covers many standard use cases and is arguably easier to use. The pipeline is parameterized in many ways, and all the options below are selectable with command-line switches. Pipeline documentation is available online. that the grammar first be converted to Chomsky Normal Form, thereby avoiding the complexities of explicit binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently ex"
W13-2226,N06-1014,0,0.0453991,"tion, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al.,"
W13-2226,P10-2041,0,0.0180757,"ta (newstest2011). We tuned our systems with kbMIRA. For truecasing, we used a monolingual translation system built on the training data, and finally detokenized with simple heuristics. Other features Joshua 5.0 also includes many features designed to increase its usability. These include: • A TCP/IP server architecture, designed to handle multiple sets of translation requests while ensuring fairness in thread assignment both across and within these connections. 5 • Intelligent selection of translation and language model training data using crossentropy difference to rank training candidates (Moore and Lewis, 2010; Axelrod et al., 2011) (described in detail in Orland (2013)). The 5.0 release of Joshua is the result of a significant year-long research, engineering, and usability effort that we hope will be of service to the research community. User-friendly packages of Joshua are available from joshua-decoder. org, while developers are encouraged to participate via github.com/joshua-decoder/ joshua. Mailing lists, linked from the main Joshua page, are available for both. • A bundler for easy packaging of trained models with all of its dependencies. • A year’s worth of improvements to the Joshua pipeline"
W13-2226,P06-1121,0,0.0308244,"collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization met"
W13-2226,W12-3018,0,0.0734936,"Missing"
W13-2226,W12-3134,1,0.899023,"Missing"
W13-2226,P00-1056,0,0.227105,"arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produ"
W13-2226,N13-1092,1,0.786588,"Missing"
W13-2226,P03-1021,0,0.264651,"g, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and S"
W13-2226,P13-2121,0,0.0388401,"Missing"
W13-2226,P11-1027,0,0.0212052,"ltering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 201"
W13-2226,W11-2123,0,0.0202034,"ope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation th"
W13-2226,P06-1055,0,0.0791186,"g et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et a"
W13-2226,D10-1063,0,0.0697106,"at the grammar first be converted to Chomsky Normal Form, thereby avoiding the complexities of explicit binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), Berk"
W13-2226,P06-1091,0,0.0258474,"done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et al., 2006a; Tillmann and Zhang, 2006; Chiang et al., 2008; Hopkins and May, 2011) has made it possible to tune large numbers of features in statistical machine translation systems, and open2 nlp.stanford.edu/˜mgalley/software/ stanford-ghkm-latest.tar.gz 3 Roughly, the number of consecutive nonterminals in a rule (Hopkins and Langmead, 2010). 2.3 Decoding 207 source implementations such as Cherry and Foster (2012) have made it easy. Joshua 5.0 has moved to a sparse feature representation internally. First, to clarify terminology, a feature as implemented in the decoder is actually a template that can introduce any number of actu"
W13-2226,W11-2160,1,0.923673,"Missing"
W13-2226,N06-1033,0,0.0332843,"rallel execution of steps in arbitrary acyclic dependency graphs (much like the U NIX make tool, but written with machine translation in mind). Joshua’s pipeline is more limited in that the basic pipeline skeleton is hard-coded, but reduced versatility covers many standard use cases and is arguably easier to use. The pipeline is parameterized in many ways, and all the options below are selectable with command-line switches. Pipeline documentation is available online. that the grammar first be converted to Chomsky Normal Form, thereby avoiding the complexities of explicit binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of"
W13-2226,W06-3119,0,0.0251012,"mentations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few"
W13-2233,P11-2031,0,0.0974083,"Missing"
W13-2233,P11-2071,0,0.028712,"Missing"
W13-2233,D12-1025,0,0.0415685,": Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu. 2 Language Previous Work Prior work shows that a variety of signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Monz and Dorr, 2005; Huang et al., 2005; Schafer, 2006; Klementiev and Roth, 2006; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Other work has used decipherment techniques to learn translations from monolingual and comparable data (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Daum´e and Jagarlamudi (2011) use contextual and string similarity to mine translations for OOV words in a high resource language domain adaptation for a machine translation setting. Unlike most other prior work on bilingual lexicon induction, Daum´e and Jagarlamudi (2011) use the translations in end-to-end SMT. More recently, Irvine and Callison-Burch (2013) combine a variety of the techniques for estimating word pair similarity using source and target language comparable corpora. That work shows that only a small amount of supervision is needed to learn how to effective"
W13-2233,P98-1069,0,0.0365831,"Missing"
W13-2233,P08-1088,0,0.131692,"5 77 44 25 Telugu 414 41 39 21 Bengali 240 7 37 18 Malayalam 263 151 6 3 Hindi 659 n/a 34 11 Urdu 616 116 23 6 and 1.7 BLEU points translating the following low resource languages into English: Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu. 2 Language Previous Work Prior work shows that a variety of signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Monz and Dorr, 2005; Huang et al., 2005; Schafer, 2006; Klementiev and Roth, 2006; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Other work has used decipherment techniques to learn translations from monolingual and comparable data (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Daum´e and Jagarlamudi (2011) use contextual and string similarity to mine translations for OOV words in a high resource language domain adaptation for a machine translation setting. Unlike most other prior work on bilingual lexicon induction, Daum´e and Jagarlamudi (2011) use the translations in end-to-end SMT. More recently, Irvine and Callison-Burch (2013) combine a variety of the"
W13-2233,H05-1061,0,0.0594528,"Missing"
W13-2233,N13-1056,1,0.888807,"er, 2006; Klementiev and Roth, 2006; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Other work has used decipherment techniques to learn translations from monolingual and comparable data (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Daum´e and Jagarlamudi (2011) use contextual and string similarity to mine translations for OOV words in a high resource language domain adaptation for a machine translation setting. Unlike most other prior work on bilingual lexicon induction, Daum´e and Jagarlamudi (2011) use the translations in end-to-end SMT. More recently, Irvine and Callison-Burch (2013) combine a variety of the techniques for estimating word pair similarity using source and target language comparable corpora. That work shows that only a small amount of supervision is needed to learn how to effectively combine similarity features into a single model for doing bilingual lexicon induction. In this work, because we assume access to a small amount of bilingual data, it is natural to take such a supervised approach to inducing new translations, and we directly apply that of Irvine and Callison-Burch (2013). Klementiev et al. (2012) use comparable corpora to score an existing Spani"
W13-2233,P06-1103,0,0.0138364,"Missing"
W13-2233,E09-1003,0,0.0170373,"ns, and we directly apply that of Irvine and Callison-Burch (2013). Klementiev et al. (2012) use comparable corpora to score an existing Spanish-English phrase table extracted from the Europarl corpus. In this work, we directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013). Such efforts are orthogonal and complementary to the approach that we take. Table 1: Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data). Language Tamil Telugu Bengali Malayalam Hindi Urdu Web Crawls 0.1 0.4 2.7 0.1 18.1 285 Wikipedia 4.4 8.6 3.3 3.7 6.4 2.5 Table 2: Millions of words of time-stampe"
W13-2233,E12-1014,1,0.894296,"lations in end-to-end SMT. More recently, Irvine and Callison-Burch (2013) combine a variety of the techniques for estimating word pair similarity using source and target language comparable corpora. That work shows that only a small amount of supervision is needed to learn how to effectively combine similarity features into a single model for doing bilingual lexicon induction. In this work, because we assume access to a small amount of bilingual data, it is natural to take such a supervised approach to inducing new translations, and we directly apply that of Irvine and Callison-Burch (2013). Klementiev et al. (2012) use comparable corpora to score an existing Spanish-English phrase table extracted from the Europarl corpus. In this work, we directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013"
W13-2233,W02-0902,0,0.109166,"Missing"
W13-2233,N12-1047,0,0.0496282,"Missing"
W13-2233,N03-1017,0,0.0482923,"work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English. 1 Introduction Standard statistical machine translation (SMT) models (Koehn et al., 2003) are trained using large, sentence-aligned parallel corpora. Unfortunately, parallel corpora are not always available in large enough quantities to train robust models (Kolachina et al., 2012). In this work, we consider the situation in which we have access to only a small amount of bitext for a given low resource language pair, and we wish to supplement an SMT model with additional translations and features estimated using comparable corpora in the source and target languages. Assuming access to a small amount ∗ 1 We consider low resource settings to be those with parallel datasets of fewer t"
W13-2233,W02-2026,0,0.0203882,"Missing"
W13-2233,P07-2045,1,0.010684,", orthographic similarity using normalized edit distance, and topic similarity based on inter-lingually linked Wikipedia pages. Our hope is that by adding a diverse set of similarity features to the phrase tables, our models will better distinguish between good and bad translation pairs, improving accuracy. 4 4.1 Experiments Experimental setup We use the data splits given by Post et al. (2012) and, following that work, include the dictionaries in the training data and report results on the devtest set using case-insensitive BLEU and four references. We use the Moses phrase-based MT framework (Koehn et al., 2007). For each language, we extract a phrase table with a phrase limit of seven. In order to make our results comparable to those of Post et al. (2012), we follow that work and use 4 GIZA++ intersection alignments over all training data. The Post et al. (2012) datasets are crowdsourced English translations of source Wikipedia text. Using Wikipedia as comparable corpora, we observe all OOVs at least once. 5 6 Because the words within a phrase pair are often reordered, phrase-level orthographic similarity is unreliable. 264 Language Top-1 Acc. Top-10 Acc. Tamil 4.5 10.2 Telugu 32.8 47.9 Bengali 17.9"
W13-2233,N10-1063,0,0.0170884,"of Irvine and Callison-Burch (2013). Klementiev et al. (2012) use comparable corpora to score an existing Spanish-English phrase table extracted from the Europarl corpus. In this work, we directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013). Such efforts are orthogonal and complementary to the approach that we take. Table 1: Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data). Language Tamil Telugu Bengali Malayalam Hindi Urdu Web Crawls 0.1 0.4 2.7 0.1 18.1 285 Wikipedia 4.4 8.6 3.3 3.7 6.4 2.5 Table 2: Millions of words of time-stamped web crawls and Wik"
W13-2233,P12-1003,0,0.0299845,"l lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English. 1 Introduction Standard statistical machine translation (SMT) models (Koehn et al., 2003) are trained using large, sentence-aligned parallel corpora. Unfortunately, parallel corpora are not always available in large enough quantities to train robust models (Kolachina et al., 2012). In this work, we consider the situation in which we have access to only a small amount of bitext for a given low resource language pair, and we wish to supplement an SMT model with additional translations and features estimated using comparable corpora in the source and target languages. Assuming access to a small amount ∗ 1 We consider low resource settings to be those with parallel datasets of fewer than 1 million words. Most standard MT datasets contain tens or hundreds of millions of words. 2 Estimating reordering probabilities over sparse data also leads to model inaccuracies; we do not"
W13-2233,P13-1135,1,0.232938,"tiev et al. (2012) use comparable corpora to score an existing Spanish-English phrase table extracted from the Europarl corpus. In this work, we directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013). Such efforts are orthogonal and complementary to the approach that we take. Table 1: Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data). Language Tamil Telugu Bengali Malayalam Hindi Urdu Web Crawls 0.1 0.4 2.7 0.1 18.1 285 Wikipedia 4.4 8.6 3.3 3.7 6.4 2.5 Table 2: Millions of words of time-stamped web crawls and Wikipedia text, by language. 3 Using Comparable"
W13-2233,D09-1092,0,0.0231419,"1 39 21 Bengali 240 7 37 18 Malayalam 263 151 6 3 Hindi 659 n/a 34 11 Urdu 616 116 23 6 and 1.7 BLEU points translating the following low resource languages into English: Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu. 2 Language Previous Work Prior work shows that a variety of signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Monz and Dorr, 2005; Huang et al., 2005; Schafer, 2006; Klementiev and Roth, 2006; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Other work has used decipherment techniques to learn translations from monolingual and comparable data (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Daum´e and Jagarlamudi (2011) use contextual and string similarity to mine translations for OOV words in a high resource language domain adaptation for a machine translation setting. Unlike most other prior work on bilingual lexicon induction, Daum´e and Jagarlamudi (2011) use the translations in end-to-end SMT. More recently, Irvine and Callison-Burch (2013) combine a variety of the techniques for estim"
W13-2233,C10-1124,0,0.0185135,"son-Burch (2013). Klementiev et al. (2012) use comparable corpora to score an existing Spanish-English phrase table extracted from the Europarl corpus. In this work, we directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013). Such efforts are orthogonal and complementary to the approach that we take. Table 1: Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data). Language Tamil Telugu Bengali Malayalam Hindi Urdu Web Crawls 0.1 0.4 2.7 0.1 18.1 285 Wikipedia 4.4 8.6 3.3 3.7 6.4 2.5 Table 2: Millions of words of time-stamped web crawls and Wikipedia text, by language"
W13-2233,P11-1122,1,0.842872,"Missing"
W13-2233,P06-1011,0,0.0135403,"to inducing new translations, and we directly apply that of Irvine and Callison-Burch (2013). Klementiev et al. (2012) use comparable corpora to score an existing Spanish-English phrase table extracted from the Europarl corpus. In this work, we directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013). Such efforts are orthogonal and complementary to the approach that we take. Table 1: Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data). Language Tamil Telugu Bengali Malayalam Hindi Urdu Web Crawls 0.1 0.4 2.7 0.1 18.1 285 Wikipedia 4.4 8.6 3.3 3.7 6.4 2.5 Table 2: Mi"
W13-2233,W06-3119,0,0.0134798,"Missing"
W13-2233,P12-1017,0,0.0203242,"li, Malayalam, Hindi, and Urdu. 2 Language Previous Work Prior work shows that a variety of signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Monz and Dorr, 2005; Huang et al., 2005; Schafer, 2006; Klementiev and Roth, 2006; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Other work has used decipherment techniques to learn translations from monolingual and comparable data (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Daum´e and Jagarlamudi (2011) use contextual and string similarity to mine translations for OOV words in a high resource language domain adaptation for a machine translation setting. Unlike most other prior work on bilingual lexicon induction, Daum´e and Jagarlamudi (2011) use the translations in end-to-end SMT. More recently, Irvine and Callison-Burch (2013) combine a variety of the techniques for estimating word pair similarity using source and target language comparable corpora. That work shows that only a small amount of supervision is needed to learn how to effectively combine similarit"
W13-2233,W12-3152,1,0.836751,"directly apply their technique for scoring an existing phrase table. However, unlike that work, our initial phrase tables are estimated from small parallel corpora for genuine low resource languages. Additionally, we include new translations discovered in comparable corpora. Other prior work has mined supplemental parallel data from comparable corpora (Munteanu and Marcu, 2006; AbduI-Rauf and Schwenk, 2009; Smith et al., 2010; Uszkoreit et al., 2010; Smith et al., 2013). Such efforts are orthogonal and complementary to the approach that we take. Table 1: Information about datasets released by Post et al. (2012): thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data). Language Tamil Telugu Bengali Malayalam Hindi Urdu Web Crawls 0.1 0.4 2.7 0.1 18.1 285 Wikipedia 4.4 8.6 3.3 3.7 6.4 2.5 Table 2: Millions of words of time-stamped web crawls and Wikipedia text, by language. 3 Using Comparable Corpora to Improve Accuracy and Coverage After describing our bilingual and comparable corpora, we briefly describe the techniques proposed by I"
W13-2233,P95-1050,0,0.756451,"Missing"
W13-2233,P99-1067,0,0.051317,"Missing"
W13-2233,C98-1066,0,\N,Missing
W13-2233,P11-1002,0,\N,Missing
W13-2233,P05-1033,0,\N,Missing
W14-1617,W13-2233,1,0.760192,"our approach in three parts. In Section 3.1, we begin by inducing translations for unknown unigrams. Then, in 3.2, we introduce our algorithm for composing phrase translations. In order to achieve a high recall in our set of hypothesis translations, we define compositionality more loosely than is typical. Finally, in 3.3, we use comparable corpora to prune the large set of hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in indu"
W14-1617,W10-1703,1,0.830808,"anish-English Europarl v5 parallel corpus (Koehn, 2005). For Hindi, we use the parallel corpora released by Post et al. (2012). Again, we randomly sample 2, 000 sentence pairs from the training corpus (about 39, 000 Hindi words). We expect that this amount of parallel text could be compiled for a single text domain and any pair of modern languages. Additionally, we use approximately 2, 500 and 1, 000 single-reference parallel sentences each for tuning and testing our Spanish and Hindi models, respectively. Spanish tuning and test sets are newswire articles taken from the 2010 WMT shared task (Callison-Burch et al., 2010).1 We use the Hindi development and testing splits released by Post et al. (2012). 4.1 Composing and Pruning Phrase Translations There are about 183 and 66 thousand unique bigrams and trigrams in the Spanish and Hindi tuning and test sets, respectively. However, many of these phrases do not demand new hypothesis translations. We do not translate those which contain numbers or punctuation. Additionally, for Spanish, we exclude names, which are typically translated identically between Spanish and English.3 We exclude phrases which are sequences of stop words only. Additionally, we exclude phrase"
W14-1617,N13-1056,1,0.864364,"our approach in three parts. In Section 3.1, we begin by inducing translations for unknown unigrams. Then, in 3.2, we introduce our algorithm for composing phrase translations. In order to achieve a high recall in our set of hypothesis translations, we define compositionality more loosely than is typical. Finally, in 3.3, we use comparable corpora to prune the large set of hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in indu"
W14-1617,N12-1047,0,0.0272562,"itional phrase pairs from those not in each phrase table as negative supervision. Again, we use Vowpal Wabbit for learning a log linear model to score any phrase pair. 4.3 Machine Translation We use GIZA++ to word align each training corpus. We use the Moses SMT framework (Koehn et al., 2007) and the standard phrase-based MT feature set, including phrase and lexical translation probabilities and a lexicalized reordering model. When we augment our models with new translations, we use the average reordering scores over all bilingually estimated phrase pairs. We tune all models using batch MIRA (Cherry and Foster, 2012). We average results over three tuning runs and use approximate randomization to measure statistical significance (Clark et al., 2011). For Spanish, we use a 5-gram language model trained on the English side of the complete Europarl corpus and for Hindi a 5-gram language model trained on the English side of the complete training corpus released by Post et al. (2012). We train our language models using SRILM with Kneser-Ney smoothing. Our baseline models use a phrase limit of three, and we augment them with translations of phrases up to length three in our experiments. 5 5 For both languages, w"
W14-1617,D07-1103,0,0.0282229,"our approach that we have seen for Spanish and Hindi suggests that it is worth pursuing such directions for other even less related and resourced language pairs. In addition to language pair, text genre and the degree of looseness or literalness of given parallel corpora may also affect the amount of phrase translation compositionality. 8 Related Work Phrase-based SMT models estimated over very large parallel corpora are expensive to store and process. Prior work has reduced the size of SMT phrase tables in order to improve efficiency without the loss of translation quality (He et al., 2009; Johnson et al., 2007; Zens et al., 2012). Typically, the goal of pruning is to identify and remove phrase pairs which are likely to be inaccurate, using either the scores and counts of a given pair itself or those relative to other phrase pairs. Our work, in contrast, focuses on low resource settings, where training data is limited and provides incomplete and unreliable scored phrase pairs. We begin by dramatically increasing the size of our SMT phrase table in order to expand its coverage and then use non-parallel data to rescore and filter the table. In the decipherment task, translation models are learned from"
W14-1617,P11-2031,0,0.0799018,"el to score any phrase pair. 4.3 Machine Translation We use GIZA++ to word align each training corpus. We use the Moses SMT framework (Koehn et al., 2007) and the standard phrase-based MT feature set, including phrase and lexical translation probabilities and a lexicalized reordering model. When we augment our models with new translations, we use the average reordering scores over all bilingually estimated phrase pairs. We tune all models using batch MIRA (Cherry and Foster, 2012). We average results over three tuning runs and use approximate randomization to measure statistical significance (Clark et al., 2011). For Spanish, we use a 5-gram language model trained on the English side of the complete Europarl corpus and for Hindi a 5-gram language model trained on the English side of the complete training corpus released by Post et al. (2012). We train our language models using SRILM with Kneser-Ney smoothing. Our baseline models use a phrase limit of three, and we augment them with translations of phrases up to length three in our experiments. 5 5 For both languages, we learn an alignment over our tuning and test sets and complete parallel training sets. 6 grow-diag-final 7 We use an indicator featur"
W14-1617,E12-1014,1,0.930498,"of at least θF reqT to the inverted index. Doing so eliminates improbable target language constructions early on, for example house handsome her or cute a house. 3.3 Pruning Phrase Pairs Using Scores Derived from Comparable Corpora We further prune the large, noisy set of hypothesized phrase translations before augmenting a seed translation model. To do so, we use a supervised setup very similar to that used for inducing unigram translations; we estimate a variety of signals that indicate translation equivalence, including temporal, topical, contextual, and string similarity. As we showed in Klementiev et al. (2012), such signals are effective for identifying phrase translations as well as unigram translations. We add ngram length, alignment, and unigram translation features to the set, listed in Appendix A. We learn a log-linear model for combining the features into a single score for predicting the quality of a given phrase pair. We extract training data from the seed translation model. We rank hypothesis translations for each source phrase using clas162 sification scores and keep the top-k. We found that using a score threshold sometimes improves precision. However, as experiments below show, the reca"
W14-1617,P11-2071,0,0.161516,"Missing"
W14-1617,W02-0902,0,0.217707,"prune the large set of hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in inducing a comprehensive set of phrase translations is that the number of both source and target phrases is immense. For example, there are about 83 million unique phrases up to length three in the English Wikipedia. Pairwise comparisons of two sets of 100 million phrases corresponds to 1 x 1016 . Thus, even if we limit the task to short phrases"
W14-1617,P08-1010,0,0.0207353,"additional experiments, we found that without the new features the same sets of hallucinated phrase pairs hurt performance slightly in comparison with the baseline augmented with unigram translations, and results don’t change as we vary k.10 Thus, the translation models are able to effectively use the higher recall sets of new phrase 9 The same set used for composing phrase translations. For all values of k between 1 and 100, without the new features, BLEU scores are about 13.70 for Spanish 10 167 focus on the low resource language pair setting, where a large training corpus is not available. Deng et al. (2008) work in a standard SMT setting but use a discriminative framework for extracting phrase pairs from parallel corpora. That approach yields a phrase table with higher precision and recall than the table extracted by standard world alignment based heuristics (Och and Ney, 2003; Koehn et al., 2003). The discriminative model combines features from word alignments and bilingual training data as well as information theoretic features estimated over monolingual data into a single log-linear model and then the phrase pairs are filtered using a threshold on model scores. The phrase pairs that it extrac"
W14-1617,N03-1017,0,0.191353,"nally feasible. In contrast to unigrams, the difficulty in inducing a comprehensive set of phrase translations is that the number of both source and target phrases is immense. For example, there are about 83 million unique phrases up to length three in the English Wikipedia. Pairwise comparisons of two sets of 100 million phrases corresponds to 1 x 1016 . Thus, even if we limit the task to short phrases, the number of pairwise phrase comparisons necessary to do an exhaustive search is infeasible. However, multi-word translation units have been shown to improve the quality of SMT dramatically (Koehn et al., 2003). Phrase translations allow translation models to memorize local context-dependent translations and reordering patterns. 3 3.1 Unigram Translations In any low resource setting, many word translations are likely to be unknown. Therefore, before moving to phrases, we use a bilingual lexicon induction technique to identify translations for unigrams. Specifically, because we assume a setting where we have some small amount of parallel data, we follow our prior work on supervised bilingual lexicon induction (Irvine and CallisonBurch, 2013b). We take examples of good translation pairs from our word"
W14-1617,D12-1025,0,0.157293,"lish and Hindi-English. While Spanish and English are very closely related, Hindi and English are less related. Our oracle experiments showed potential for composing phrase translations for both language pairs, and, indeed, in our experiments using hallucinated phrase translations we saw significant translation quality gains for both. We expect that improving the quality of induced unigram translations will yield even more performance gains. The vast majority of prior work on low resource MT has focused on Spanish-English (Haghighi et al., 2008; Klementiev et al., 2012; Ravi and Knight, 2011; Dou and Knight, 2012; Ravi, 2013; Dou and Knight, 2013). Although such experiments serve as important proofs of concept, we found it important to also experiment with a more Table 4: Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are averaged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest performing (bolded) baseli"
W14-1617,P07-2045,1,0.014277,"s with multi-word source strings, that appear at least three times in the training corpus, and that are composeable using baseline model unigram translations and induced dictionaries. Then, for each language pair, we use the 3, 000 that have the highest ppf |eq scores as positive supervision. We randomly sample 9, 000 compositional phrase pairs from those not in each phrase table as negative supervision. Again, we use Vowpal Wabbit for learning a log linear model to score any phrase pair. 4.3 Machine Translation We use GIZA++ to word align each training corpus. We use the Moses SMT framework (Koehn et al., 2007) and the standard phrase-based MT feature set, including phrase and lexical translation probabilities and a lexicalized reordering model. When we augment our models with new translations, we use the average reordering scores over all bilingually estimated phrase pairs. We tune all models using batch MIRA (Cherry and Foster, 2012). We average results over three tuning runs and use approximate randomization to measure statistical significance (Clark et al., 2011). For Spanish, we use a 5-gram language model trained on the English side of the complete Europarl corpus and for Hindi a 5-gram langua"
W14-1617,D13-1173,0,0.0192506,"ish and English are very closely related, Hindi and English are less related. Our oracle experiments showed potential for composing phrase translations for both language pairs, and, indeed, in our experiments using hallucinated phrase translations we saw significant translation quality gains for both. We expect that improving the quality of induced unigram translations will yield even more performance gains. The vast majority of prior work on low resource MT has focused on Spanish-English (Haghighi et al., 2008; Klementiev et al., 2012; Ravi and Knight, 2011; Dou and Knight, 2012; Ravi, 2013; Dou and Knight, 2013). Although such experiments serve as important proofs of concept, we found it important to also experiment with a more Table 4: Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are averaged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest performing (bolded) baseline for each language; * indicates s"
W14-1617,2005.mtsummit-papers.11,0,0.068655,"reshold. 4 pairs extracted from each parallel corpus as positive supervision and 9, 000 random word pairs as negative supervision. We use Vowpal Wabbit2 for learning. The top-5 induced translations for each source language word are used as both a baseline set of new translations (Section 6.3) and for composing phrase translations. Experimental Setup 4.2 In all of our experiments, we assume that we have access to only a small parallel corpus. For our Spanish experiments, we randomly sample 2, 000 sentence pairs (about 57, 000 Spanish words) from the Spanish-English Europarl v5 parallel corpus (Koehn, 2005). For Hindi, we use the parallel corpora released by Post et al. (2012). Again, we randomly sample 2, 000 sentence pairs from the training corpus (about 39, 000 Hindi words). We expect that this amount of parallel text could be compiled for a single text domain and any pair of modern languages. Additionally, we use approximately 2, 500 and 1, 000 single-reference parallel sentences each for tuning and testing our Spanish and Hindi models, respectively. Spanish tuning and test sets are newswire articles taken from the 2010 WMT shared task (Callison-Burch et al., 2010).1 We use the Hindi develop"
W14-1617,I08-1053,0,0.0192134,"reasing the size of our SMT phrase table in order to expand its coverage and then use non-parallel data to rescore and filter the table. In the decipherment task, translation models are learned from comparable corpora without any parallel text (Ravi and Knight, 2011; Dou and Knight, 2012; Ravi, 2013). In contrast, we begin with a small amount of parallel data and take a very different approach to learning translation models. In our prior work (Irvine and CallisonBurch, 2013b), we showed how effective even small amounts of bilingual data can be for learning translations from monolingual texts. Garera and Yarowsky (2008) pivot through bilingual dictionaries in several language pairs to compose translations for compound words. Zhang and Zong (2013) construct a set of new, additional phrase pairs for the task of domain adaptation for machine translation. That work uses two dictionaries to bootstrap a set of phrase pair translations: one probabilistic dictionary extracted from 2 million words of bitext and one manually created new-domain dictionary of 140, 000 word translations. Our approach to the construction of new phrase pairs is somewhat similar to Zhang and Zong (2013), but we don’t rely on a very large ma"
W14-1617,P12-1003,0,0.0316068,"Missing"
W14-1617,P08-1088,0,0.493658,"hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in inducing a comprehensive set of phrase translations is that the number of both source and target phrases is immense. For example, there are about 83 million unique phrases up to length three in the English Wikipedia. Pairwise comparisons of two sets of 100 million phrases corresponds to 1 x 1016 . Thus, even if we limit the task to short phrases, the number of pairwise"
W14-1617,P06-1011,0,0.0372997,"ormation theoretic features estimated over monolingual data into a single log-linear model and then the phrase pairs are filtered using a threshold on model scores. The phrase pairs that it extracts are limited to those that appear in pairs of sentences in the parallel training data. Our work takes a similar approach to that of Deng et al. (2008), however, unlike that work, we hallucinate phrase pairs that did not appear in training data in order to augment the original, bilingually extracted phrase table. Other prior work has used comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010). Such efforts are orthogonal to our approach. We use parallel corpora, when available, and hallucinates phrase translations without assuming any parallel text in our comparable corpora. truly low resource language pair. The success of our approach that we have seen for Spanish and Hindi suggests that it is worth pursuing such directions for other even less related and resourced language pairs. In addition to language pair, text genre and the degree of looseness or literalness of given parallel corpora may also affect the amount of phrase translation compositionality. 8 Re"
W14-1617,J03-1002,0,0.00548982,"ble to effectively use the higher recall sets of new phrase 9 The same set used for composing phrase translations. For all values of k between 1 and 100, without the new features, BLEU scores are about 13.70 for Spanish 10 167 focus on the low resource language pair setting, where a large training corpus is not available. Deng et al. (2008) work in a standard SMT setting but use a discriminative framework for extracting phrase pairs from parallel corpora. That approach yields a phrase table with higher precision and recall than the table extracted by standard world alignment based heuristics (Och and Ney, 2003; Koehn et al., 2003). The discriminative model combines features from word alignments and bilingual training data as well as information theoretic features estimated over monolingual data into a single log-linear model and then the phrase pairs are filtered using a threshold on model scores. The phrase pairs that it extracts are limited to those that appear in pairs of sentences in the parallel training data. Our work takes a similar approach to that of Deng et al. (2008), however, unlike that work, we hallucinate phrase pairs that did not appear in training data in order to augment the origi"
W14-1617,P02-1040,0,0.0909568,"Missing"
W14-1617,W12-3152,1,0.927835,"supervision and 9, 000 random word pairs as negative supervision. We use Vowpal Wabbit2 for learning. The top-5 induced translations for each source language word are used as both a baseline set of new translations (Section 6.3) and for composing phrase translations. Experimental Setup 4.2 In all of our experiments, we assume that we have access to only a small parallel corpus. For our Spanish experiments, we randomly sample 2, 000 sentence pairs (about 57, 000 Spanish words) from the Spanish-English Europarl v5 parallel corpus (Koehn, 2005). For Hindi, we use the parallel corpora released by Post et al. (2012). Again, we randomly sample 2, 000 sentence pairs from the training corpus (about 39, 000 Hindi words). We expect that this amount of parallel text could be compiled for a single text domain and any pair of modern languages. Additionally, we use approximately 2, 500 and 1, 000 single-reference parallel sentences each for tuning and testing our Spanish and Hindi models, respectively. Spanish tuning and test sets are newswire articles taken from the 2010 WMT shared task (Callison-Burch et al., 2010).1 We use the Hindi development and testing splits released by Post et al. (2012). 4.1 Composing a"
W14-1617,P95-1050,0,0.499946,"ly, in 3.3, we use comparable corpora to prune the large set of hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in inducing a comprehensive set of phrase translations is that the number of both source and target phrases is immense. For example, there are about 83 million unique phrases up to length three in the English Wikipedia. Pairwise comparisons of two sets of 100 million phrases corresponds to 1 x 101"
W14-1617,P11-1002,0,0.328452,"uage pairs, Spanish-English and Hindi-English. While Spanish and English are very closely related, Hindi and English are less related. Our oracle experiments showed potential for composing phrase translations for both language pairs, and, indeed, in our experiments using hallucinated phrase translations we saw significant translation quality gains for both. We expect that improving the quality of induced unigram translations will yield even more performance gains. The vast majority of prior work on low resource MT has focused on Spanish-English (Haghighi et al., 2008; Klementiev et al., 2012; Ravi and Knight, 2011; Dou and Knight, 2012; Ravi, 2013; Dou and Knight, 2013). Although such experiments serve as important proofs of concept, we found it important to also experiment with a more Table 4: Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are averaged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest perf"
W14-1617,P13-1036,0,0.0489117,". While Spanish and English are very closely related, Hindi and English are less related. Our oracle experiments showed potential for composing phrase translations for both language pairs, and, indeed, in our experiments using hallucinated phrase translations we saw significant translation quality gains for both. We expect that improving the quality of induced unigram translations will yield even more performance gains. The vast majority of prior work on low resource MT has focused on Spanish-English (Haghighi et al., 2008; Klementiev et al., 2012; Ravi and Knight, 2011; Dou and Knight, 2012; Ravi, 2013; Dou and Knight, 2013). Although such experiments serve as important proofs of concept, we found it important to also experiment with a more Table 4: Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are averaged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest performing (bolded) baseline for each"
W14-1617,P13-1109,0,0.0419922,"ction 3.1, we begin by inducing translations for unknown unigrams. Then, in 3.2, we introduce our algorithm for composing phrase translations. In order to achieve a high recall in our set of hypothesis translations, we define compositionality more loosely than is typical. Finally, in 3.3, we use comparable corpora to prune the large set of hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in inducing a comprehensive set"
W14-1617,W02-2026,0,0.11138,"we use comparable corpora to prune the large set of hypothesis translations for each source phrase. words, or out-of-vocabulary (OOV) words, have been the focus of previous work on integrating bilingual lexicon induction and machine translation (Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013a; Razmara et al., 2013). Bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures of words in the source language with distributional signatures representing target language words (Rapp, 1995; Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). If the source and target language each contain, for example, 100, 000 words, the number of pairwise comparisons is about 10 billion, which is significant but computationally feasible. In contrast to unigrams, the difficulty in inducing a comprehensive set of phrase translations is that the number of both source and target phrases is immense. For example, there are about 83 million unique phrases up to length three in the English Wikipedia. Pairwise comparisons of two sets of 100 million phrases corresponds to 1 x 1016 . Thus, even if we limit t"
W14-1617,N10-1063,0,0.0371917,"s estimated over monolingual data into a single log-linear model and then the phrase pairs are filtered using a threshold on model scores. The phrase pairs that it extracts are limited to those that appear in pairs of sentences in the parallel training data. Our work takes a similar approach to that of Deng et al. (2008), however, unlike that work, we hallucinate phrase pairs that did not appear in training data in order to augment the original, bilingually extracted phrase table. Other prior work has used comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010). Such efforts are orthogonal to our approach. We use parallel corpora, when available, and hallucinates phrase translations without assuming any parallel text in our comparable corpora. truly low resource language pair. The success of our approach that we have seen for Spanish and Hindi suggests that it is worth pursuing such directions for other even less related and resourced language pairs. In addition to language pair, text genre and the degree of looseness or literalness of given parallel corpora may also affect the amount of phrase translation compositionality. 8 Related Work Phrase-bas"
W14-1617,D12-1089,0,0.0204728,"have seen for Spanish and Hindi suggests that it is worth pursuing such directions for other even less related and resourced language pairs. In addition to language pair, text genre and the degree of looseness or literalness of given parallel corpora may also affect the amount of phrase translation compositionality. 8 Related Work Phrase-based SMT models estimated over very large parallel corpora are expensive to store and process. Prior work has reduced the size of SMT phrase tables in order to improve efficiency without the loss of translation quality (He et al., 2009; Johnson et al., 2007; Zens et al., 2012). Typically, the goal of pruning is to identify and remove phrase pairs which are likely to be inaccurate, using either the scores and counts of a given pair itself or those relative to other phrase pairs. Our work, in contrast, focuses on low resource settings, where training data is limited and provides incomplete and unreliable scored phrase pairs. We begin by dramatically increasing the size of our SMT phrase table in order to expand its coverage and then use non-parallel data to rescore and filter the table. In the decipherment task, translation models are learned from comparable corpora"
W14-1617,P13-1140,0,0.30509,"e. In the decipherment task, translation models are learned from comparable corpora without any parallel text (Ravi and Knight, 2011; Dou and Knight, 2012; Ravi, 2013). In contrast, we begin with a small amount of parallel data and take a very different approach to learning translation models. In our prior work (Irvine and CallisonBurch, 2013b), we showed how effective even small amounts of bilingual data can be for learning translations from monolingual texts. Garera and Yarowsky (2008) pivot through bilingual dictionaries in several language pairs to compose translations for compound words. Zhang and Zong (2013) construct a set of new, additional phrase pairs for the task of domain adaptation for machine translation. That work uses two dictionaries to bootstrap a set of phrase pair translations: one probabilistic dictionary extracted from 2 million words of bitext and one manually created new-domain dictionary of 140, 000 word translations. Our approach to the construction of new phrase pairs is somewhat similar to Zhang and Zong (2013), but we don’t rely on a very large manually generated dictionary. Additionally, we 9 Conclusions We showed that “hallucinating” phrasal translations can significantly"
W14-3357,E12-1016,0,0.0420539,"Missing"
W14-3357,P08-1088,0,0.0670661,"uality translations at test time due to the mismatch. In Irvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use her"
W14-3357,N12-1047,0,0.0489945,"the old-domain language model and, then, both the old-domain and the new-domain language models. Our first comparison system augments the standard feature set with the orthographic similarity feature, which is not based on comparable corpora. Our second comparison system uses both the orthographic feature and the contextual and topic similarity features estimated over a random set of comparable document pairs. The third system estimates contextual and topic similarity using new-domain-like comparable corpora. We tune our phrase table feature weights for each model separately using batch MIRA (Cherry and Foster, 2012) and new-domain tuning data. Results are averaged over three tuning runs, and we use the implementation of approximate randomization 5 Results Table 3 presents a summary of our results on the test set in each domain. Using only the old-domain language model, our baselines yield BLEU scores of 22.70 and 21.29 on the medical and science test sets, respectively. When we add the orthographic similarity feature, BLEU scores increase significantly, by about 0.4 on the medical data and 0.6 on science. Adding the contextual and topic features estimated over a random selection of comparable document pa"
W14-3357,N13-1056,1,0.907417,"est time due to the mismatch. In Irvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, w"
W14-3357,W07-0722,0,0.0423553,"e modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the"
W14-3357,P11-2031,0,0.0902697,"to concepts from the field of physics but also include computer science and chemistry 3 Or about 4 thousand lines each. The sentences in the medical domain text are much shorter than those in the science domain. 4 As of January 2014. 440 Science Diagnosis (artificial intelligence) Absorption spectroscopy Spectral line Chemical kinetics Mahalanobis distance Dynamic light scattering Amorphous solid Magnetic hyperthermia Photoelasticity Galaxy rotation curve Medical Pregabalin Cetuximab Fluconazole Calcitonin Pregnancy category Trazodone Rivaroxaban Spironolactone Anakinra Cladribine released by Clark et al. (2011) to measure the statistical significance of each feature-augmented model compared with the baseline model that uses the same language model(s). As noted in Section 3, the features that we estimate from comparable corpora may be zerovalued. We use our second tuning sets5 to tune a minimum threshold parameter for our new features. We measure performance in terms of BLEU score on the second tuning set as we vary the new feature threshold between 1e´07 and 0.5 for each domain. A threshold of 0.01, for example, means that we replace all feature with values less than 0.01 with 0.01. For both new-dom"
W14-3357,D13-1109,1,0.866456,"ch. In Irvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, w"
W14-3357,P11-2071,0,0.0865037,"Missing"
W14-3357,W07-0717,0,0.0655578,"lecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training"
W14-3357,E12-1014,1,0.896557,"pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, where we replaced bilingually estimated phrase table features with the new features and cited applications to low resource SMT. In this work we also focus on scoring a phrase table using comparable corpora. However, here we work in a domain adaptation setting and seek to augment, not replace, an existin"
W14-3357,W02-0902,0,0.100361,"omain, resulting in low quality translations at test time due to the mismatch. In Irvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic si"
W14-3357,P98-1069,0,0.35337,"nd target word and phrase in our phrase table using the source and target sides of our comparable corpus, respectively. We begin by collecting vectors of counts of words that appear in the context of each source and target phrase, ps and pt . We use a bag-of-words context consisting of the two words to the left and two words to 2 Similar to distributional similarity, which is typically defined monolingually. 438 the right of each occurrence of each phrase. Various means of computing the component values of context vectors from raw context frequency counts have been proposed (e.g. Rapp (1999), Fung and Yee (1998)). Following Fung and Yee (1998), we compute the value of the k-th component of ps ’s contextual vector, Cps , as follows: have interlingual French-English links. Specifically, we compute topical vectors by counting the number of occurrences of each word and phrase across Wikipedia pages. That is, for each source and target phrase, ps and pt , we collect M dimensional topic vectors, where M is the number of Wikipedia page pairs used (in our experiments, M is typically 5, 000). We use Wikipedia’s interlingual links to align the French and English topic vectors and normalize each topic vector by"
W14-3357,W07-0733,0,0.0997416,"explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus"
W14-3357,P07-2045,1,0.0112894,"Missing"
W14-3357,2011.iwslt-papers.5,0,0.0414625,"e from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain paralle"
W14-3357,P10-2041,0,0.0940992,"ent pairs, which we mine from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subs"
W14-3357,P95-1050,0,0.554462,"data but no training data in the medical domain, resulting in low quality translations at test time due to the mismatch. In Irvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The defi"
W14-3357,P99-1067,0,0.330016,"each source and target word and phrase in our phrase table using the source and target sides of our comparable corpus, respectively. We begin by collecting vectors of counts of words that appear in the context of each source and target phrase, ps and pt . We use a bag-of-words context consisting of the two words to the left and two words to 2 Similar to distributional similarity, which is typically defined monolingually. 438 the right of each occurrence of each phrase. Various means of computing the component values of context vectors from raw context frequency counts have been proposed (e.g. Rapp (1999), Fung and Yee (1998)). Following Fung and Yee (1998), we compute the value of the k-th component of ps ’s contextual vector, Cps , as follows: have interlingual French-English links. Specifically, we compute topical vectors by counting the number of occurrences of each word and phrase across Wikipedia pages. That is, for each source and target phrase, ps and pt , we collect M dimensional topic vectors, where M is the number of Wikipedia page pairs used (in our experiments, M is typically 5, 000). We use Wikipedia’s interlingual links to align the French and English topic vectors and normalize"
W14-3357,P13-1109,0,0.149845,"rvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, where we replaced biling"
W14-3357,W02-2026,0,0.208246,"raining data in the medical domain, resulting in low quality translations at test time due to the mismatch. In Irvine et al. (2013a), we introduced a taxonomy for classifying machine translation errors 1 Some prior work has referred to old-domain and newdomain corpora as out-of-domain and in-domain, respectively. 437 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics seen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexica"
W14-3357,E12-1055,0,0.0407456,", or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for un3 Phrase Table Scoring We begin with a scored phrase table estimated using our old-domain parallel training corpus. The phrase table contains about 201 million unique source phrases up to length seve"
W14-3357,C08-1125,0,0.114523,"se of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for un3 Phrase Table Scoring We begin with a scored phrase table estimated using our old-domain parallel training corpus. The phrase table contains about 201 million unique source phrases up to length seven and about 479 million total phrase pairs. We use Wikipedia as a source for comparable document pairs (details are given in Se"
W14-3357,D11-1033,0,\N,Missing
W14-3357,C98-1066,0,\N,Missing
W14-3357,P13-1140,0,\N,Missing
W14-3357,Q13-1035,1,\N,Missing
W14-3357,N13-1035,0,\N,Missing
W14-3357,W12-3154,0,\N,Missing
W15-0629,W12-5802,0,0.137912,"you to think carefully about how audience and purpose, as well as medium and genre, affect your choices as composers and reflect carefully on a new dimension of your topic. A polished essay that asserts an arguable thesis that is supported by research and sound reasoning. Table 1: Brief description of the assignments in the FWC, as provided by the syllabus. 2 Draft Intermed. Final Related Work While AES has traditionally been used for grading tests, there are some previous applications of AES in a non-testing environment. For example, Elliot et al. (2012) used AES to assist with placement and Chali and Hasan (2012) automatically graded essays written for an occupational therapy course by comparing them to the course material. Corpora for AES include English-language learner writing, specifically the First Certification Exam corpus (FCE), a portion of the Cambridge Learner Corpus consisting of 1,244 essays written for an English-language certification exam (Yannakoudakis et al., 2011), and the International Corpus of Learner English (ICLE), 6,085 essays written by university students across the world (Granger, 2003). The Kaggle ASAP–AES dataset has primarily native-English writing, with 22,000 short essa"
W15-0629,D13-1180,0,0.163681,"lude the Michigan Corpus of Upper-level Student Papers, with 829 academic papers that received an A grade, written by college seniors and graduate students across several disciplines (Mic, 2009). A separate corpus of freshman writing was collected at University of Michigan containing 3,500 ungraded pre-entrance essays (Gere and Aull, 2010). Methods previously used for AES include lin255 Tokens 840.3 938.5 Sentences 35.6 39.6 Paragraphs 5.2 5.7 Table 2: Average length of essays from the Fall 2011 semester. ear regression (Attali and Burstein, 2006), rank algorithms (Yannakoudakis et al., 2011; Chen and He, 2013), LSA (Pearson, 2010; Chali and Hasan, 2012), and Bayesian models (Rudner and Liang, 2002). Recent approaches focus on predicting specific aspect of the score by using targeted features such as coherence (McNamara et al., 2010; Yannakoudakis and Briscoe, 2012). Multi-task learning jointly models separate tasks in a single model using a shared representation. It has been used in NLP for tasks such as domain adaptation (Finkel and Manning, 2009), relation extraction (Jiang, 2009), and modeling annotator bias (Cohn and Specia, 2013). 3 Data The Freshman Writing Corpus (FWC) is a new corpus for AE"
W15-0629,P13-1004,0,0.058788,"Missing"
W15-0629,N09-1068,0,0.0252642,"Missing"
W15-0629,W10-0407,0,0.0154737,"tten by middle- and high-school students the United States (Shermis and Hamner, 2013). The FCE and Kaggle data were collected during examinations while the ICLE data was written during an exam or as part of a class assignment. Student writing collections not suitable for AES include the Michigan Corpus of Upper-level Student Papers, with 829 academic papers that received an A grade, written by college seniors and graduate students across several disciplines (Mic, 2009). A separate corpus of freshman writing was collected at University of Michigan containing 3,500 ungraded pre-entrance essays (Gere and Aull, 2010). Methods previously used for AES include lin255 Tokens 840.3 938.5 Sentences 35.6 39.6 Paragraphs 5.2 5.7 Table 2: Average length of essays from the Fall 2011 semester. ear regression (Attali and Burstein, 2006), rank algorithms (Yannakoudakis et al., 2011; Chen and He, 2013), LSA (Pearson, 2010; Chali and Hasan, 2012), and Bayesian models (Rudner and Liang, 2002). Recent approaches focus on predicting specific aspect of the score by using targeted features such as coherence (McNamara et al., 2010; Yannakoudakis and Briscoe, 2012). Multi-task learning jointly models separate tasks in a single"
W15-0629,P09-1114,0,0.0446824,"Missing"
W15-0629,P14-5010,0,0.0055941,"Missing"
W15-0629,C12-2096,0,0.0551916,"Missing"
W15-0629,W12-2004,0,0.0795095,"d at University of Michigan containing 3,500 ungraded pre-entrance essays (Gere and Aull, 2010). Methods previously used for AES include lin255 Tokens 840.3 938.5 Sentences 35.6 39.6 Paragraphs 5.2 5.7 Table 2: Average length of essays from the Fall 2011 semester. ear regression (Attali and Burstein, 2006), rank algorithms (Yannakoudakis et al., 2011; Chen and He, 2013), LSA (Pearson, 2010; Chali and Hasan, 2012), and Bayesian models (Rudner and Liang, 2002). Recent approaches focus on predicting specific aspect of the score by using targeted features such as coherence (McNamara et al., 2010; Yannakoudakis and Briscoe, 2012). Multi-task learning jointly models separate tasks in a single model using a shared representation. It has been used in NLP for tasks such as domain adaptation (Finkel and Manning, 2009), relation extraction (Jiang, 2009), and modeling annotator bias (Cohn and Specia, 2013). 3 Data The Freshman Writing Corpus (FWC) is a new corpus for AES that contains essays written by college students in a first-year writing program. The unique features of this corpus are multiple essay drafts, teacher grades on a detailed rubric, and teacher feedback. The FWC contains approximately 23,000 essays collected"
W15-0629,P11-1019,0,0.0304842,"ermed. Final Related Work While AES has traditionally been used for grading tests, there are some previous applications of AES in a non-testing environment. For example, Elliot et al. (2012) used AES to assist with placement and Chali and Hasan (2012) automatically graded essays written for an occupational therapy course by comparing them to the course material. Corpora for AES include English-language learner writing, specifically the First Certification Exam corpus (FCE), a portion of the Cambridge Learner Corpus consisting of 1,244 essays written for an English-language certification exam (Yannakoudakis et al., 2011), and the International Corpus of Learner English (ICLE), 6,085 essays written by university students across the world (Granger, 2003). The Kaggle ASAP–AES dataset has primarily native-English writing, with 22,000 short essays written by middle- and high-school students the United States (Shermis and Hamner, 2013). The FCE and Kaggle data were collected during examinations while the ICLE data was written during an exam or as part of a class assignment. Student writing collections not suitable for AES include the Michigan Corpus of Upper-level Student Papers, with 829 academic papers that recei"
W15-2614,P14-5010,0,0.0132468,"Missing"
W15-2614,D13-1170,0,0.00559072,"Missing"
W15-2614,W14-4907,0,0.0607018,"Missing"
W15-2614,D08-1027,0,0.167875,"Missing"
W15-2614,W09-1904,0,0.0255801,"collect crowdsourced annotations at scale (Khare et al., 2015; Wang et al., 2013). In some NLP problems, the annotation task requires some degree of common linguistic knowledge that most non-experts are assumed to have. By examining the accuracy of crowdsourced data and its usefulness in training models to perform common NLP tasks, previous research has shown that deficiencies in individual crowd worker accuracy can be overcome by taking consensus votes over multiple annotators or weighting the votes of annotators based on their overall performance (MacLean and Heer, 2013; Zhai et al., 2013; Hsueh et al., 2009; Snow et al., 2008). • The mastoid air cells are well-pneumatized. (mastoid) • Bilateral dysplastic vestibules and lateral semicircular canals. (semicircular canal) • The external auditory canal is patent. (EAC) Labeling some of these sentences might require a non-expert to do additional research. (e.g. Should a mastoid air cell be pneumatized? Does lateral describe the condition of the semicircular canal, or is lateral semicircular canal a compound noun?) In this work, we extend the study of crowdsourcing annotations to text-labeling tasks that require domain knowledge. Specifically, we exam"
W15-2614,H89-2078,0,0.612212,"Missing"
W17-1914,D16-1215,1,0.589816,"he rankings provided by two sense-agnostic, vector-based lexical substitution models. Lexical substitution requires systems to predict substitutes for target word instances that preserve their meaning in context (McCarthy and Navigli, 2007). We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts. In contrast, sense inventories with low substitutability might group words linked by different types of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further d"
W17-1914,N15-1059,0,0.0611884,"Missing"
W17-1914,N16-1172,1,0.914477,"Applications, pages 110–119, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics 2 2.1 Sentence In this world, one’s word is a promise. Silverplate: code word for the historic mission that would end World War II. I think she only heard the last words of my speech. A sense inventory for substitution Paraphrase substitutability The candidate substitutes used by our ranking models come from the Paraphrase Database (PPDB) XXL package (Ganitkevitch et al., 2013).1 Paraphrase relations in the PPDB are defined between words and phrases which might carry different senses. Cocos and Callison-Burch (2016) used a spectral clustering algorithm to cluster PPDB XXL into senses, but the clusters contain noisy paraphrases and paraphrases linked by different types of relations (e.g. hypernyms, antonyms) which are not always substitutable. We use a slightly modified version of their method to cluster paraphrases where both the number of clusters (senses) and their contents are optimized for substitutability. 2.2 Annotated Substitutes (Count) vow (1), utterance (1), tongue (1), speech (1) phrase (3), term (2), verbiage(1), utterance (1), signal (1), name (1), dictate (1), designation (1), decree (1) bi"
W17-1914,J13-3003,0,0.0210365,"pes of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense. Why bother to identify senses if even humans cannot agree upon their nature and number, and if simple word-embedding models yield good results without using any explicit sense representation? Word-based models are successful in various semantic tasks even though they conflate multiple word meanings into a single representation. Based on the hypothesis that capturing polysemy could further improve their performance, several works have focused on creating sen"
W17-1914,N15-1184,0,0.027754,"tances that preserve their meaning in context (McCarthy and Navigli, 2007). We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts. In contrast, sense inventories with low substitutability might group words linked by different types of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense. Why bother to identify senses if even humans cannot agree upon their nature and number, and if simple word-embedding models yield go"
W17-1914,N13-1092,1,0.871474,"with the clustered sense before learning embed110 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 110–119, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics 2 2.1 Sentence In this world, one’s word is a promise. Silverplate: code word for the historic mission that would end World War II. I think she only heard the last words of my speech. A sense inventory for substitution Paraphrase substitutability The candidate substitutes used by our ranking models come from the Paraphrase Database (PPDB) XXL package (Ganitkevitch et al., 2013).1 Paraphrase relations in the PPDB are defined between words and phrases which might carry different senses. Cocos and Callison-Burch (2016) used a spectral clustering algorithm to cluster PPDB XXL into senses, but the clusters contain noisy paraphrases and paraphrases linked by different types of relations (e.g. hypernyms, antonyms) which are not always substitutable. We use a slightly modified version of their method to cluster paraphrases where both the number of clusters (senses) and their contents are optimized for substitutability. 2.2 Annotated Substitutes (Count) vow (1), utterance (1"
W17-1914,P12-1092,0,0.14302,"Missing"
W17-1914,P15-1010,0,0.107934,"Missing"
W17-1914,N10-1013,0,0.105026,"Missing"
W17-1914,E14-1057,0,0.132413,"Missing"
W17-1914,I11-1127,0,0.218111,"Missing"
W17-1914,D15-1200,0,0.0216592,"uter and Information Science Department, University of Pennsylvania † LIMSI, CNRS, Universit´e Paris-Saclay, 91403 Orsay {acocos,marapi,ccb}@seas.upenn.edu Abstract dings (Reisinger and Mooney, 2010; Huang et al., 2012). Iacobacci et al. (2015) disambiguate the words in a corpus using a state-of-the-art WSD system and then produce continuous representations of word senses based on distributional information obtained from the annotated corpus. Moving from word to sense embeddings generally improves their performance in word and relational similarity tasks but is not beneficial in all settings. Li and Jurafsky (2015) show that although multisense embeddings give improved performance in tasks such as semantic similarity, semantic relation identification and part-of-speech tagging, they fail to help in others, like sentiment analysis and named entity extraction (Li and Jurafsky, 2015). The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for sense. We show that a filtering mechanism based on a sense inventory optimized for substitutability can improve the results of th"
W17-1914,de-marneffe-etal-2006-generating,0,0.0126458,"Missing"
W17-1914,S07-1009,0,0.212128,"Missing"
W17-1914,W15-1501,0,0.489592,"tution models. Lexical substitution requires systems to predict substitutes for target word instances that preserve their meaning in context (McCarthy and Navigli, 2007). We consider a sense inventory with high substitutability to be one which groups synonyms or paraphrases that are mutually-interchangeable in the same contexts. In contrast, sense inventories with low substitutability might group words linked by different types of relations. We carry out experiments with a syntactic vector-space model (Thater et al., 2011; Apidianaki, 2016) and a word-embedding model for lexical substitution (Melamud et al., 2015). Instead of using the senses to refine the vector representations as in (Faruqui et al., 2015), we use them to improve the lexical substitution rankings proposed by the models as a post-processing step. Our results show that senses can improve the performance of vector-space models in lexical substitution tasks. Introduction Word sense has always been difficult to define and pin down (Kilgarriff, 1997; Erk et al., 2013). Recent successes of embedding-based, sense-agnostic models in various semantic tasks cast further doubt on the usefulness of word sense. Why bother to identify senses if even"
W17-1914,W12-3018,0,0.200101,"Missing"
W17-1914,P15-2070,1,0.884108,"Missing"
W17-4405,P14-1073,1,0.859834,"gira rweba@scs.howard.edu Howard University Chris Callison-Burch ccb@cis.upenn.edu University of Pennsylvania Abstract and Dredze, 2011; Guo et al., 2013) and making crucial real-time decisions such as earthquake detection (Sakaki et al., 2010). Twitter is an informal forum that imposes a limit on the number of characters per tweet, hence, the vocabulary used to express tweets are diverse. This results in the prevalence of abbreviated or misspelled words in tweets and aliases used to represent named entities. Entity name variation poses a challenge to determining what or who a name refers to (Andrews et al., 2014); identifying name variations has been shown to help in different domains such as community question answering systems (Andy et al., 2016b,a) and automatic paraphrase acquisition (Shinyama et al., 2002). For example, given the following tweets that occurred in a 5-minute time period during an event: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of"
W17-4405,W16-4405,1,0.751642,"et al., 2013) and making crucial real-time decisions such as earthquake detection (Sakaki et al., 2010). Twitter is an informal forum that imposes a limit on the number of characters per tweet, hence, the vocabulary used to express tweets are diverse. This results in the prevalence of abbreviated or misspelled words in tweets and aliases used to represent named entities. Entity name variation poses a challenge to determining what or who a name refers to (Andrews et al., 2014); identifying name variations has been shown to help in different domains such as community question answering systems (Andy et al., 2016b,a) and automatic paraphrase acquisition (Shinyama et al., 2002). For example, given the following tweets that occurred in a 5-minute time period during an event: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an algorithm, EntitySpike, that identifies entities that s"
W17-4405,W16-3909,1,0.839797,"et al., 2013) and making crucial real-time decisions such as earthquake detection (Sakaki et al., 2010). Twitter is an informal forum that imposes a limit on the number of characters per tweet, hence, the vocabulary used to express tweets are diverse. This results in the prevalence of abbreviated or misspelled words in tweets and aliases used to represent named entities. Entity name variation poses a challenge to determining what or who a name refers to (Andrews et al., 2014); identifying name variations has been shown to help in different domains such as community question answering systems (Andy et al., 2016b,a) and automatic paraphrase acquisition (Shinyama et al., 2002). For example, given the following tweets that occurred in a 5-minute time period during an event: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an algorithm, EntitySpike, that identifies entities that s"
W17-4405,W16-6204,1,0.92601,"inition: Given a sequence of tweets and entity mentions, denoted by X =({e1 ,S1 },{e2 ,S2 },....{en ,Sn }), where ei represents a named entity that spikes in popularity in a given time period e.g. Julia Louis-Dreyfus, and Si represents the set of tweets that make reference to this named entity, ei , during this specified time period; the task is to create an alias list for each ei , if one exists. Background and Preliminaries In tweets collected during an on going event, there is a small window in time in which entities spike in popularity, though they have occurrences during the whole event (Dredze et al., 2016). Candidate Entity Identification: Following previous work in entity linking (Liu et al., 2013; Guo et al., 2013), we define an entity as a Wikipedia title page. An entity mention is a sequence of tokens in a tweet that can potentially link to an entity. The Grammy Awards show is mostly about famous people and so we focus only on entities belonging to the Person category. In order to construct a Wikipedia lexicon, we collect 1.5 million English Wikipedia title pages referring to Person named entities and extracted the backlinks (incoming links to the Wikipedia title page) from each of these Wi"
W17-4405,N13-1122,0,0.0340283,"represents a named entity that spikes in popularity in a given time period e.g. Julia Louis-Dreyfus, and Si represents the set of tweets that make reference to this named entity, ei , during this specified time period; the task is to create an alias list for each ei , if one exists. Background and Preliminaries In tweets collected during an on going event, there is a small window in time in which entities spike in popularity, though they have occurrences during the whole event (Dredze et al., 2016). Candidate Entity Identification: Following previous work in entity linking (Liu et al., 2013; Guo et al., 2013), we define an entity as a Wikipedia title page. An entity mention is a sequence of tokens in a tweet that can potentially link to an entity. The Grammy Awards show is mostly about famous people and so we focus only on entities belonging to the Person category. In order to construct a Wikipedia lexicon, we collect 1.5 million English Wikipedia title pages referring to Person named entities and extracted the backlinks (incoming links to the Wikipedia title page) from each of these Wikipedia title pages (we intend to make this dataset available to the research community). Given a set of tweets {"
W17-4405,P13-1128,0,0.0286279,"n ,Sn }), where ei represents a named entity that spikes in popularity in a given time period e.g. Julia Louis-Dreyfus, and Si represents the set of tweets that make reference to this named entity, ei , during this specified time period; the task is to create an alias list for each ei , if one exists. Background and Preliminaries In tweets collected during an on going event, there is a small window in time in which entities spike in popularity, though they have occurrences during the whole event (Dredze et al., 2016). Candidate Entity Identification: Following previous work in entity linking (Liu et al., 2013; Guo et al., 2013), we define an entity as a Wikipedia title page. An entity mention is a sequence of tokens in a tweet that can potentially link to an entity. The Grammy Awards show is mostly about famous people and so we focus only on entities belonging to the Person category. In order to construct a Wikipedia lexicon, we collect 1.5 million English Wikipedia title pages referring to Person named entities and extracted the backlinks (incoming links to the Wikipedia title page) from each of these Wikipedia title pages (we intend to make this dataset available to the research community). Give"
W17-5039,P06-1032,0,0.728794,"the-art neural MT system (Yuan and Briscoe, 2016), which uses twice the amount of training data, most of which is not publicly available. The analysis provided in this work will help improve future efforts in GEC, and can be used to inform approaches rooted in both neural and statistical MT. Introduction This work presents a systematic investigation for automatic grammatical error correction (GEC) inspired by machine translation (MT). The task of grammatical error correction can be viewed as a noisy channel model, and therefore a MT approach makes sense, and has been applied to the task since Brockett et al. (2006). Currently, the best GEC systems all use machine translation in some form, whether statistical MT (SMT) as a component of a larger pipeline (Rozovskaya and Roth, 2016) or neural MT (Yuan and Briscoe, 2016). These approaches make use of a great deal of resources, and in this work we propose a lighter-weight approach to GEC by methodically examining different aspects of the SMT pipeline, identifying and applying modifications tailored for GEC, introducing artificial data, and evaluating how each of these specializations contributes to the overall performance. Specifically, we demonstrate that •"
W17-5039,P17-1074,0,0.0216571,"anges made in the output. With errorcoded text, the performance by feature type can be examined with M2 , but this is not possible with GLEU or the un-coded JFLEG corpus. To investigate the types of changes systems make on a more granular level, we apply the feature extraction method described in Section 3.2 to quantify the morphological and lexical transformations. While we developed this method for scoring translation rules, it can work on any aligned text, and is similar to the forthcoming ERRANT toolkit, which is uses a rule-based framework for automatically categorizes grammatical edits (Bryant et al., 2017). We calculate the number of each of these transformations made by to the input by each system and the human references, determining significant differences with a paired t-test (p &lt; 0.05). Figure 1 contains the mean number of these transformations per sentence made by SMEC, YB16, and the human-corrected references, and Figure 2 shows the number of operations by part of speech. Even though the GLEU and M2 scores of the two systems are nearly identical, they are significantly different in all of the transformations in Figure 1, with SMEC having a higher edit distance from the original, but YB16"
W17-5039,W14-1702,0,0.100856,"Missing"
W17-5039,D16-1195,0,0.0116979,"tain error types (Rozovskaya et al., 2014). Performing less well, Wang et al. (2014) used factored SMT, representing words as factored units to more adeptly handle morphological changes. Shortly after the shared task, a system combining classifiers and SMT with no further customizations reported better performance than all competing systems (Susanto et al., 2014) The current leading GEC systems all use MT in some form, including hybrid approaches that use the output of error-type classifiers as MT input (Rozovskaya and Roth, 2016) or include a neural model of learner text as a feature in SMT (Chollampatt et al., 2016); phrase-based MT with sparse features tuned to a GEC metric (JunczysDowmunt and Grundkiewicz, 2016); and neural MT (Yuan and Briscoe, 2016). Three of these model have been evaluated on a separate test corpus and, while the PBMT system reported the highest scores on the CoNLL-14 test set, it was outperformed by the systems with neural components on the new test set (Napoles et al., 2017). 2.1 2.2 Evaluation GEC systems are automatically evaluated by comparing their output on sentences that have been manually annotated corpora. The Max-Match metric (M2 ) is the most widely used, and calculates"
W17-5039,D11-1108,1,0.879749,"Missing"
W17-5039,N12-1067,0,0.0173856,"features tuned to a GEC metric (JunczysDowmunt and Grundkiewicz, 2016); and neural MT (Yuan and Briscoe, 2016). Three of these model have been evaluated on a separate test corpus and, while the PBMT system reported the highest scores on the CoNLL-14 test set, it was outperformed by the systems with neural components on the new test set (Napoles et al., 2017). 2.1 2.2 Evaluation GEC systems are automatically evaluated by comparing their output on sentences that have been manually annotated corpora. The Max-Match metric (M2 ) is the most widely used, and calculates the F0.5 over phrasal edits (Dahlmeier and Ng, 2012). Napoles et al. (2015) proposed a GEC corpora There are two broad categories of parallel data for GEC. The first is error-coded text, in which annotators have coded spans of learner text containing an error, and which includes the NUS Cor1 Alignment mistakes may occur when sentences are split or joined, or when errors and corrections span multiple tokens, in which the automatic alignment within that span may err. 346 errors is amplified in sparse data due to the potentially infinite possible misspellings and large number of OOVs. Previous work has approached this issue by including spelling c"
W17-5039,D15-1052,0,0.0148434,") – TAG-inserted(rj ) • Substitutions – is-substituted(rj ) – TAG-substituted(rj ) – TAG-substituted-with-TAG(li , rj ) Morphological features: – inflection-change-same-lemma(li , rj ) – inflection-and-lemma-change(li , rj ) 3.3 Metric The decoder identifies the most probable derivation of an input sentence from the translation grammar. Derivations are scored by a combination of a language model score and weighted feature functions, and the weights are optimized to a specific metric during the tuning phase. Recent work has shown that MT metrics like BLEU are not sufficient for evaluating GEC (Grundkiewicz et al., 2015; Napoles et al., 2015) or tuning MT systems for GEC (Junczys-Dowmunt and Grundkiewicz, 3 More details about the features can be found at https: //github.com/cnap/smt-for-gec. 348 Rule 1. 2. 3. 4. Generate POS tags of the cased input sentence Label proper nouns in the input Align the cased input tokens with the output Capitalize the first alphanumeric character of the output sentence (if a letter). 5. For each pair of aligned tokens (li , rj ), capitalize rj if li is labeled a proper noun or rj is the token “i”. argued that → may argue that Alignment (, may), (argued, argue), (that, that) Fea"
W17-5039,W13-1703,0,0.0575588,"Missing"
W17-5039,W14-1703,0,0.312661,"epositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 2012 and 2013 shared tasks in GEC both targeted only certain error types (Dale et al., 2012; Ng et al., 2013), to which classification was appropriately suited. However, the goal of the 2014 CoNLL Shared Task was correcting all 28 types of grammatical errors, encouraging several MT-based approaches to GEC, (e.g., Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Two of the best CoNLL 2014 systems used MT as a black box, reranking output (Felice et al., 2014), and customizing the tuning algorithm and using lexical features (Junczys-Dowmunt and Grundkiewicz, 2014). The other leading system was classification-based and only targeted certain error types (Rozovskaya et al., 2014). Performing less well, Wang et al. (2014) used factored SMT, representing words as factored units to more adeptly handle morphological changes. Shortly after the shared task, a system combining classifiers and SMT with no further customizations reported better performance than a"
W17-5039,W12-2006,0,0.0430198,"Missing"
W17-5039,D16-1161,0,0.19834,"compared to the data available for bilingual MT, which commonly uses 100s of thousands or millions of aligned sentence pairs. We hypothesize that artificially generating transformation rules may overcome the limit imposed by lack of sufficiently large training data and improve performance. Particularly, the prevalence of spelling 2 347 https://pythonhosted.org/pyenchant/ – lemma-change-same-inflection(li , rj ) Spelling features: – not-in-dictionary(li ) – spelling-correction(li , rj ) Counts of spelling corrections are weighted by the probability of rj in an English Gigaword language model. Junczys-Dowmunt and Grundkiewicz (2016) used a large number of sparse features for a phrasebased MT system that achieved state of the art performance on the CoNLL-2014 test set. Unlike that work, which uses a potentially infinite amount of sparse features, we choose to use a discrete set of feature functions that are informed by this task. Our feature extraction relies on a variety of preexisting tools, including fast-align for word alignment (Dyer et al., 2013), trained over the parallel FCE, Lang-8, and NUCLE corpora; PyEnchant for detecting spelling changes; the Stanford POS tagger; the RASP morphological analyzer, morpha (Minne"
W17-5039,D14-1102,0,0.0419341,"L 2014 systems used MT as a black box, reranking output (Felice et al., 2014), and customizing the tuning algorithm and using lexical features (Junczys-Dowmunt and Grundkiewicz, 2014). The other leading system was classification-based and only targeted certain error types (Rozovskaya et al., 2014). Performing less well, Wang et al. (2014) used factored SMT, representing words as factored units to more adeptly handle morphological changes. Shortly after the shared task, a system combining classifiers and SMT with no further customizations reported better performance than all competing systems (Susanto et al., 2014) The current leading GEC systems all use MT in some form, including hybrid approaches that use the output of error-type classifiers as MT input (Rozovskaya and Roth, 2016) or include a neural model of learner text as a feature in SMT (Chollampatt et al., 2016); phrase-based MT with sparse features tuned to a GEC metric (JunczysDowmunt and Grundkiewicz, 2016); and neural MT (Yuan and Briscoe, 2016). Three of these model have been evaluated on a separate test corpus and, while the PBMT system reported the highest scores on the CoNLL-14 test set, it was outperformed by the systems with neural com"
W17-5039,P15-2097,1,0.919669,"metric (JunczysDowmunt and Grundkiewicz, 2016); and neural MT (Yuan and Briscoe, 2016). Three of these model have been evaluated on a separate test corpus and, while the PBMT system reported the highest scores on the CoNLL-14 test set, it was outperformed by the systems with neural components on the new test set (Napoles et al., 2017). 2.1 2.2 Evaluation GEC systems are automatically evaluated by comparing their output on sentences that have been manually annotated corpora. The Max-Match metric (M2 ) is the most widely used, and calculates the F0.5 over phrasal edits (Dahlmeier and Ng, 2012). Napoles et al. (2015) proposed a GEC corpora There are two broad categories of parallel data for GEC. The first is error-coded text, in which annotators have coded spans of learner text containing an error, and which includes the NUS Cor1 Alignment mistakes may occur when sentences are split or joined, or when errors and corrections span multiple tokens, in which the automatic alignment within that span may err. 346 errors is amplified in sparse data due to the potentially infinite possible misspellings and large number of OOVs. Previous work has approached this issue by including spelling correction as a step in"
W17-5039,E17-2037,1,0.849262,"ems all use MT in some form, including hybrid approaches that use the output of error-type classifiers as MT input (Rozovskaya and Roth, 2016) or include a neural model of learner text as a feature in SMT (Chollampatt et al., 2016); phrase-based MT with sparse features tuned to a GEC metric (JunczysDowmunt and Grundkiewicz, 2016); and neural MT (Yuan and Briscoe, 2016). Three of these model have been evaluated on a separate test corpus and, while the PBMT system reported the highest scores on the CoNLL-14 test set, it was outperformed by the systems with neural components on the new test set (Napoles et al., 2017). 2.1 2.2 Evaluation GEC systems are automatically evaluated by comparing their output on sentences that have been manually annotated corpora. The Max-Match metric (M2 ) is the most widely used, and calculates the F0.5 over phrasal edits (Dahlmeier and Ng, 2012). Napoles et al. (2015) proposed a GEC corpora There are two broad categories of parallel data for GEC. The first is error-coded text, in which annotators have coded spans of learner text containing an error, and which includes the NUS Cor1 Alignment mistakes may occur when sentences are split or joined, or when errors and corrections s"
W17-5039,P12-2039,0,0.017415,"larger than the error-coded corpora, and may contain more extensive rewrites. Additionally, corrections of sentences made without error coding are perceived to be more grammatical. Two corpora of this type are the Automatic Evaluation of Scientific Writing corpus, with more than 1 million sentences of scientific writing corrected by professional proofreaders (Daudaravicius et al., 2016), and the Lang-8 Corpus of Learner English, which contains 1 million sentence pairs scraped from an online forum for language learners, which were corrected by other members of the lang-8.com online community (Tajiri et al., 2012). Twice that many English sentence pairs can be extracted from version 2 of the Lang-8 Learner Corpora (Tomoya et al., 2011). We will include both types of corpora in our experiments in Section 4. Earlier approaches to grammatical error correction developed rule-based systems or classifiers targeting specific error types such as prepositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 201"
W17-5039,W13-3601,0,0.0470809,"rner Corpora (Tomoya et al., 2011). We will include both types of corpora in our experiments in Section 4. Earlier approaches to grammatical error correction developed rule-based systems or classifiers targeting specific error types such as prepositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 2012 and 2013 shared tasks in GEC both targeted only certain error types (Dale et al., 2012; Ng et al., 2013), to which classification was appropriately suited. However, the goal of the 2014 CoNLL Shared Task was correcting all 28 types of grammatical errors, encouraging several MT-based approaches to GEC, (e.g., Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Two of the best CoNLL 2014 systems used MT as a black box, reranking output (Felice et al., 2014), and customizing the tuning algorithm and using lexical features (Junczys-Dowmunt and Grundkiewicz, 2014). The other leading system was classification-based and only targeted certain error types (Rozovskaya et al., 2014). Performing l"
W17-5039,W08-1205,0,0.0375557,"rner English, which contains 1 million sentence pairs scraped from an online forum for language learners, which were corrected by other members of the lang-8.com online community (Tajiri et al., 2012). Twice that many English sentence pairs can be extracted from version 2 of the Lang-8 Learner Corpora (Tomoya et al., 2011). We will include both types of corpora in our experiments in Section 4. Earlier approaches to grammatical error correction developed rule-based systems or classifiers targeting specific error types such as prepositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 2012 and 2013 shared tasks in GEC both targeted only certain error types (Dale et al., 2012; Ng et al., 2013), to which classification was appropriately suited. However, the goal of the 2014 CoNLL Shared Task was correcting all 28 types of grammatical errors, encouraging several MT-based approaches to GEC, (e.g., Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Two of the best CoNLL 2014 systems used MT as a black"
W17-5039,I11-1017,0,0.0778109,"ithout error coding are perceived to be more grammatical. Two corpora of this type are the Automatic Evaluation of Scientific Writing corpus, with more than 1 million sentences of scientific writing corrected by professional proofreaders (Daudaravicius et al., 2016), and the Lang-8 Corpus of Learner English, which contains 1 million sentence pairs scraped from an online forum for language learners, which were corrected by other members of the lang-8.com online community (Tajiri et al., 2012). Twice that many English sentence pairs can be extracted from version 2 of the Lang-8 Learner Corpora (Tomoya et al., 2011). We will include both types of corpora in our experiments in Section 4. Earlier approaches to grammatical error correction developed rule-based systems or classifiers targeting specific error types such as prepositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 2012 and 2013 shared tasks in GEC both targeted only certain error types (Dale et al., 2012; Ng et al., 2013), to which classif"
W17-5039,P11-1094,0,0.011551,"line community (Tajiri et al., 2012). Twice that many English sentence pairs can be extracted from version 2 of the Lang-8 Learner Corpora (Tomoya et al., 2011). We will include both types of corpora in our experiments in Section 4. Earlier approaches to grammatical error correction developed rule-based systems or classifiers targeting specific error types such as prepositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 2012 and 2013 shared tasks in GEC both targeted only certain error types (Dale et al., 2012; Ng et al., 2013), to which classification was appropriately suited. However, the goal of the 2014 CoNLL Shared Task was correcting all 28 types of grammatical errors, encouraging several MT-based approaches to GEC, (e.g., Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Two of the best CoNLL 2014 systems used MT as a black box, reranking output (Felice et al., 2014), and customizing the tuning algorithm and using lexical features (Junczys-Dowmunt and Grundkiewicz, 2014). The"
W17-5039,N03-1033,0,0.00786625,"systems were evaluated on JFLEG, and the best system by both automatic metric and human evaluation is the neural MT system of Yuan and Briscoe (2016) (henceforth referred to as YB16). 3 publically → publicly Additionally, sparsity in morphological variations may arise in datasets. Wang et al. (2014) approached this issue with factored MT, which translates at the sub-word level. Instead, we also generate artificial translation rules representing morphological transformations using RASP’s morphological generator, morphg (Minnen et al., 2001). We perform POS tagging with the Stanford POS tagger (Toutanova et al., 2003) and create rules to switch the plurality of nouns (e.g., singular ↔ plural). For verbs, we generate rules that change that verb to every other inflected form, specifically the base form, third-person singular, past tense, past participle, and progressive tense (e.g., wake, wakes, woke, woken, waking). Generated words that did not appear in the PyEnchant dictionary were excluded. Customizing statistical machine translation Statistical MT contains various components, including the training data, feature functions, and an optimization metric. This section describes how we customized each of thes"
W17-5039,W14-1704,0,0.0126959,"million sentence pairs scraped from an online forum for language learners, which were corrected by other members of the lang-8.com online community (Tajiri et al., 2012). Twice that many English sentence pairs can be extracted from version 2 of the Lang-8 Learner Corpora (Tomoya et al., 2011). We will include both types of corpora in our experiments in Section 4. Earlier approaches to grammatical error correction developed rule-based systems or classifiers targeting specific error types such as prepositions or determiners, (e.g., Eeg-Olofsson and Knutsson, 2003; Tetreault and Chodorow, 2008; Rozovskaya et al., 2014), and few approaches were rooted in machine translation, though some exceptions exist (Brockett et al., 2006; Park and Levy, 2011, e.g.,). The 2012 and 2013 shared tasks in GEC both targeted only certain error types (Dale et al., 2012; Ng et al., 2013), to which classification was appropriately suited. However, the goal of the 2014 CoNLL Shared Task was correcting all 28 types of grammatical errors, encouraging several MT-based approaches to GEC, (e.g., Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Two of the best CoNLL 2014 systems used MT as a black box, reranking output (Fel"
W17-5039,W14-1711,0,0.0884858,"classification was appropriately suited. However, the goal of the 2014 CoNLL Shared Task was correcting all 28 types of grammatical errors, encouraging several MT-based approaches to GEC, (e.g., Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Two of the best CoNLL 2014 systems used MT as a black box, reranking output (Felice et al., 2014), and customizing the tuning algorithm and using lexical features (Junczys-Dowmunt and Grundkiewicz, 2014). The other leading system was classification-based and only targeted certain error types (Rozovskaya et al., 2014). Performing less well, Wang et al. (2014) used factored SMT, representing words as factored units to more adeptly handle morphological changes. Shortly after the shared task, a system combining classifiers and SMT with no further customizations reported better performance than all competing systems (Susanto et al., 2014) The current leading GEC systems all use MT in some form, including hybrid approaches that use the output of error-type classifiers as MT input (Rozovskaya and Roth, 2016) or include a neural model of learner text as a feature in SMT (Chollampatt et al., 2016); phrase-based MT with sparse features tuned to a GEC metri"
W17-5039,P16-1208,0,0.0499417,"undkiewicz, 2014). The other leading system was classification-based and only targeted certain error types (Rozovskaya et al., 2014). Performing less well, Wang et al. (2014) used factored SMT, representing words as factored units to more adeptly handle morphological changes. Shortly after the shared task, a system combining classifiers and SMT with no further customizations reported better performance than all competing systems (Susanto et al., 2014) The current leading GEC systems all use MT in some form, including hybrid approaches that use the output of error-type classifiers as MT input (Rozovskaya and Roth, 2016) or include a neural model of learner text as a feature in SMT (Chollampatt et al., 2016); phrase-based MT with sparse features tuned to a GEC metric (JunczysDowmunt and Grundkiewicz, 2016); and neural MT (Yuan and Briscoe, 2016). Three of these model have been evaluated on a separate test corpus and, while the PBMT system reported the highest scores on the CoNLL-14 test set, it was outperformed by the systems with neural components on the new test set (Napoles et al., 2017). 2.1 2.2 Evaluation GEC systems are automatically evaluated by comparing their output on sentences that have been manual"
W17-5039,W11-2160,1,0.81706,"ment between the input, output, and goldstandard references, and assumes only minimal, non-overlapping changes have been made. GLEU, on the other hand, measures n-gram overlap and therefore is better equipped to handle movement and changes to larger spans of text. 4 Experiments For our experiments, we use the Joshua 6 toolkit (Post et al., 2015). Tokenization is done with Joshua and token-level alignment with fast-align (Dyer et al., 2013). All text is lowercased, and we use a simple algorithm to recase the output (Table 2). We extract a hierarchical phrase-based translation model with Thrax (Weese et al., 2011) and perform parameter tuning with pairwise ranked optimization in Joshua. Our training data is from the Lang-8 corpus (Tomoya et al., 2011), which contains 1 million parallel sentences, and grammar is extracted from the 563k sentence pairs that contain corrections. Systems are tuned to the JFLEG tuning set (751 sentences) and evaluated on the JFLEG test set (747 sentences). We use an English Gigaword 5-gram language model. We evaluate performance with two metrics, GLEU and M2 , which have similar rankings and 4 349 Drawn from the CLC, which is not public. ror Correction (SMEC+morph ), scores"
W17-5039,Q16-1013,1,0.912597,"hlmeier et al., 2013), the Cambridge Learner Corpus (CLC; 1.9M pairs per Yuan and Briscoe (2016)) (Nicholls, 2003), and a subset of the CLC, the First Certificate in English (FCE; 34k pairs) (Yannakoudakis et al., 2011). MT systems are trained on parallel text, which can be extracted from error-coded corpora by applying the annotated corrections, resulting a clean corpus with nearly-perfect word and sentence alignments.1 These corpora are small by MT training standards and constrained by the coding approach, leading to minimal changes that may result in ungrammatical or awkward-sounding text (Sakaguchi et al., 2016). The second class of GEC corpora are parallel datasets, which contain the original text and a corrected version of the text, without explicitly coded error corrections. These corpora need to be aligned by sentences and tokens, and automatic alignment introduces noise. However, these datasets are cheaper to collect, significantly larger than the error-coded corpora, and may contain more extensive rewrites. Additionally, corrections of sentences made without error coding are perceived to be more grammatical. Two corpora of this type are the Automatic Evaluation of Scientific Writing corpus, wit"
W17-5039,Q16-1029,1,0.772594,"s. Each rule is of the form w=− N X λi log ϕi i=1 left-hand side (LHS) → right-hand side (RHS) In SMT, these features typically include a phrase penalty, lexical and phrase translation probabilities, a language model probability, binary indicators for purely lexical and monotonic rules, and counters of unaligned words and rule length. Previous work in other monolingual “translation” tasks has achieved success in using features tailored to that task, such as a measure of the relative lengths for sentence compression (Ganitkevitch et al., 2011) or lexical complexity for sentence simplification (Xu et al., 2016). For GEC, and has a feature vector, the weights of which are set to optimize an objective function, which in MT is metric like BLEU. A limiting factor on MTbased GEC is the available training data, which is small when compared to the data available for bilingual MT, which commonly uses 100s of thousands or millions of aligned sentence pairs. We hypothesize that artificially generating transformation rules may overcome the limit imposed by lack of sufficiently large training data and improve performance. Particularly, the prevalence of spelling 2 347 https://pythonhosted.org/pyenchant/ – lemma"
W17-5039,P11-1019,0,0.104035,"Missing"
W17-5039,N16-1042,0,0.508463,"the same part of speech as the original; and if it is a spelling correction. We additionally use these features to analyze the outputs generated by different systems and characterize their performance with the types of transformations it makes and how they compare to manually written corrections in addition to automatic metric evaluation. Our approach, Specialized Machine translation for Error Correction (SMEC), represents a single model that handles morphological changes, spelling corrections, and phrasal substitutions, and it rivals the performance of the state-of-the-art neural MT system (Yuan and Briscoe, 2016), which uses twice the amount of training data, most of which is not publicly available. The analysis provided in this work will help improve future efforts in GEC, and can be used to inform approaches rooted in both neural and statistical MT. Introduction This work presents a systematic investigation for automatic grammatical error correction (GEC) inspired by machine translation (MT). The task of grammatical error correction can be viewed as a noisy channel model, and therefore a MT approach makes sense, and has been applied to the task since Brockett et al. (2006). Currently, the best GEC s"
W17-5039,N13-1073,0,\N,Missing
W17-5039,W16-0506,1,\N,Missing
W18-6709,D16-1230,0,0.0666099,"Missing"
W18-6709,D17-2014,0,0.0159722,"king for absolute assessments of quality yields less discriminative results than soliciting direct comparisons of quality. In the dataset introduced for the ConvAI2 competition, nearly all the proposed algorithms were evaluated to be within one standard deviation of each other (Zhang et al., 2018). Therefore, for our human evaluation task, we ask humans to directly compare the responses of two models given the previous utterances in the conversation. Both Facebook and Amazon have developed evaluation systems that allow humans to converse with (and then rate) a chatbot (Venkatesh et al., 2018; Miller et al., 2017). Facebook’s ParlAI 4 is the most comparable system for a unified framework for sharing, training, and evaluating chatbots; however, ChatEval is different in that it entirely focuses on the evaluation and warehousing of models. Our infrastructure relies only on output text files, and does not require any code base integration . Response Comparison To facilitate qualitative comparison of models, we offer a response comparison interface where users can see all the prompts in a particular evaluation set, and the responses generated by each model. Evaluation Toolkit The ChatEval evaluation toolkit"
W19-0423,N18-2077,1,0.831184,"o. The Add measure (equation (1), hereafter called AddCos because of the Cosine function applied to the vector representations of words and contexts) estimates the substitutability of a candidate substitute s of the target word t in context C, where C corresponds to the set of the target word’s context elements in the sentence, and c corresponds to an individual context element. P cos(s, t) + c∈C cos(s, c) AddCos(t, s, C) = (1) |C |+ 1 The vectors used by the original method are syntax-based embeddings created with word2vecf (Levy and Goldberg, 2014). We use the lighter adaptation proposed by Apidianaki et al. (2018) which circumvents the need for syntactic analysis, and use 300-dimensional skip-gram word and context embeddings trained on the 4B words of the Annotated Gigaword corpus (Napoles et al., 2012). We apply the AddCos method to ELMo as well as to FC-ELMo embeddings. When using standard ELMo embeddings, the target and context word representations of a sentence are their corresponding ELMo vector, and the vector of a candidate substitute is obtained by substituting the target word by the candidate in the sentence, as described in Section 4.1. To adapt this to FC-ELMo embeddings, substitute represen"
W19-0423,P05-1074,1,0.601944,"ormation into our LexSub models, where senses are defined at the level of substitute paraphrases, can improve performance. For this purpose, we generate a dataset of “focused contexts” (hereafter abbreviated FC) for each target word which are specifically chosen to represent the specific sense that target word shares with each of its potential substitutes. The starting point for our focused contexts dataset is the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), a collection of over 80M English paraphrase pairs. PPDB was automatically built using the pivot method (Bannard and Callison-Burch, 2005), which discovers same-language paraphrases by ‘pivoting’ over bilingual parallel corpora. Specifically, if two English phrases such as “under control” and “in check” are each translated to the same German phrase “unter kontrolle” in some contexts, then this is taken as evidence that “under control” and “in check” have approximately similar meaning. Because PPDB was constructed using the pivot method, it follows that each paraphrase pair x ↔ y in PPDB has a set of shared foreign translations. This idea is core to the method for extracting substitute-focused sentences. The sentences for paraphr"
W19-0423,P16-1191,0,0.0223826,"stitution task and compare their performance to the contextsensitive models of Melamud et al. (2015) and Melamud et al. (2016). We also propose a way to tune the ELMo representations to the LexSub task, by using a dataset containing a high number of sentences for words in context that represent meanings close to that of their possible substitutes. 3 Substitute-focused Contexts Contextualized word embeddings for a given target word vary based on the sense of a target word instance. Unlike the variation in discrete sense-level embeddings (e.g. Iacobacci et al. (2015); Rothe and Sch¨utze (2015); Flekova and Gurevych (2016), and others), this variation is continuous. One of our experiments aims to see whether incorporating discrete fine-grained sense information into our LexSub models, where senses are defined at the level of substitute paraphrases, can improve performance. For this purpose, we generate a dataset of “focused contexts” (hereafter abbreviated FC) for each target word which are specifically chosen to represent the specific sense that target word shares with each of its potential substitutes. The starting point for our focused contexts dataset is the Paraphrase Database (PPDB) (Ganitkevitch et al.,"
W19-0423,N13-1092,1,0.863493,"Missing"
W19-0423,P15-1010,0,0.0239021,"e ELMo vectors for the first time to the lexical substitution task and compare their performance to the contextsensitive models of Melamud et al. (2015) and Melamud et al. (2016). We also propose a way to tune the ELMo representations to the LexSub task, by using a dataset containing a high number of sentences for words in context that represent meanings close to that of their possible substitutes. 3 Substitute-focused Contexts Contextualized word embeddings for a given target word vary based on the sense of a target word instance. Unlike the variation in discrete sense-level embeddings (e.g. Iacobacci et al. (2015); Rothe and Sch¨utze (2015); Flekova and Gurevych (2016), and others), this variation is continuous. One of our experiments aims to see whether incorporating discrete fine-grained sense information into our LexSub models, where senses are defined at the level of substitute paraphrases, can improve performance. For this purpose, we generate a dataset of “focused contexts” (hereafter abbreviated FC) for each target word which are specifically chosen to represent the specific sense that target word shares with each of its potential substitutes. The starting point for our focused contexts dataset"
W19-0423,E14-1057,0,0.269398,"Missing"
W19-0423,P14-2050,0,0.0459,"can yield a high score even if one of the elements in the sum is zero. The Add measure (equation (1), hereafter called AddCos because of the Cosine function applied to the vector representations of words and contexts) estimates the substitutability of a candidate substitute s of the target word t in context C, where C corresponds to the set of the target word’s context elements in the sentence, and c corresponds to an individual context element. P cos(s, t) + c∈C cos(s, c) AddCos(t, s, C) = (1) |C |+ 1 The vectors used by the original method are syntax-based embeddings created with word2vecf (Levy and Goldberg, 2014). We use the lighter adaptation proposed by Apidianaki et al. (2018) which circumvents the need for syntactic analysis, and use 300-dimensional skip-gram word and context embeddings trained on the 4B words of the Annotated Gigaword corpus (Napoles et al., 2012). We apply the AddCos method to ELMo as well as to FC-ELMo embeddings. When using standard ELMo embeddings, the target and context word representations of a sentence are their corresponding ELMo vector, and the vector of a candidate substitute is obtained by substituting the target word by the candidate in the sentence, as described in S"
W19-0423,S07-1009,0,0.536058,"esentations is explicitly modeled during training. 1 Introduction Contextualized word representations model complex characteristics of word usage, and give state-of-theart performance in a variety of NLP tasks involving syntactic and semantic processing. Each proposed model accounts for context in a different way depending on the underlying architecture, and might account for local or long-distance phenomena. In this work, we compare different word representations on the lexical substitution (LexSub) task, which involves proposing meaning-preserving substitutes for words in specific contexts (McCarthy and Navigli, 2007). The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed for a direct comparison of the contextualized representations built by different models. We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model (Melamud et al., 2015), learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network (Melamud et al., 2016), or use vectors that are learned functions of the"
W19-0423,K16-1006,0,0.107648,"substitutes for words in specific contexts (McCarthy and Navigli, 2007). The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed for a direct comparison of the contextualized representations built by different models. We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model (Melamud et al., 2015), learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network (Melamud et al., 2016), or use vectors that are learned functions of the internal states of a deep bidirectional language model (biLM) (Peters et al., 2018a). Additionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub task. Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding represe"
W19-0423,W15-1501,0,0.46586,"e compare different word representations on the lexical substitution (LexSub) task, which involves proposing meaning-preserving substitutes for words in specific contexts (McCarthy and Navigli, 2007). The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed for a direct comparison of the contextualized representations built by different models. We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model (Melamud et al., 2015), learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network (Melamud et al., 2016), or use vectors that are learned functions of the internal states of a deep bidirectional language model (biLM) (Peters et al., 2018a). Additionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub tas"
W19-0423,L18-1008,0,0.380285,"dditionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub task. Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding representations for measuring semantic similarity without directly accounting for context, such as Glove (Pennington et al., 2014) and FastText (Mikolov et al., 2018). The results of this study highlight the importance of the architecture used for model training in capturing information relevant for lexical substitution. We show that contextualized representations that Substitutes Sentences shoot (5) The panther fired at the bridge and hit a truck. While both he and the White House deny he was fired, Frum is so insistent on the fact that he quit on his own that it really makes you wonder. As a coach, we speak and listen with the intent of helping people surface, question and reframe assumptions. We hopped back onto the coach - now for the boulangerie! sack"
W19-0423,W12-3018,0,0.027377,"Missing"
W19-0423,P15-2070,1,0.90961,"Missing"
W19-0423,D14-1162,0,0.1156,"model (biLM) (Peters et al., 2018a). Additionally, we experiment with a way to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes. We explore the impact of this tuning on the LexSub task. Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding representations for measuring semantic similarity without directly accounting for context, such as Glove (Pennington et al., 2014) and FastText (Mikolov et al., 2018). The results of this study highlight the importance of the architecture used for model training in capturing information relevant for lexical substitution. We show that contextualized representations that Substitutes Sentences shoot (5) The panther fired at the bridge and hit a truck. While both he and the White House deny he was fired, Frum is so insistent on the fact that he quit on his own that it really makes you wonder. As a coach, we speak and listen with the intent of helping people surface, question and reframe assumptions. We hopped back onto the c"
W19-0423,N18-1202,0,0.141718,"Missing"
W19-0423,D18-1179,0,0.0528649,"Missing"
W19-0423,P15-1173,0,0.0489999,"Missing"
W19-0423,P10-1097,0,0.0706147,"Missing"
W19-2405,W17-3905,0,0.019808,"ter-aided story generation has been a source of interest since the early days of NLP. Classical AI algorithms relied on symbolic and logical planning and graph construction (Klein et al., 1973; Meehan, 1977; Turner, 1993; Riedl and Young, 2006). Statistical methods have also been proposed (McIntyre and Lapata, 2009; Li and Riedl, 2015; Gatt and Krahmer, 2018). Recently, the field has been influenced by the success of (conditional) neural language models (Bengio et al., 2003; Schwenk and Gauvain, 2004; Bahdanau et al., 2015; Nallapati et al., 2016). Story generation with neural models include (Chourdakis and Reiss, 2017; Peng et al., 2018; Radford et al., 2019). We build upon recent work that improves coherence in story generation by using hierarchical neural methods. These approaches allow reasoning at a higher level than words by considering a two-level hierarchy where a structuring representation conditions text generation. Martin et al. (2018) use sequences of events to structure generation while Jain et al. (2017) relies on sequences of short descriptions. Fan et al. (2018b) rely on writing prompts. Closer to our work, Clark et al. (2018a) condition on entity mentions. The training of these methods requ"
W19-2405,N18-1204,0,0.22575,"g span is predicted given the context and the target length. In the two stage method, words that should go in the span are predicted in inverse frequency order. For visualization, the left and right contexts have been truncated. Introduction Recent advances in language modeling have made considerable progress towards the automatic generation of fluent text (Jozefowicz et al., 2016; Baevski and Auli, 2019; Radford et al., 2019). This evolution has sparked the development of tools to assist human writers. For instance, Fan et al. (2018b) suggest generating short stories from high-level prompts, Clark et al. (2018b) study the interaction of human and language models for creative writing, and Peng et al. (2018) propose an interactive control of story lines. In addition, products such as Grammarly offer suggestions to improve grammar and wording (Hoover et al., 2015). Our work is concerned with story infilling. We envision this task as a step towards a suggestion tool to help writers interactively replace text spans. Text infilling, a form of cloze task (Taylor, 1953), involves removing sequences of words from text and asking for a replacement. Compared to traditional left-to-right language modeling, aut"
W19-2405,P09-1025,0,0.0366107,"theoretic sense (Sparck Jones, 1972; Shannon, 1948), i.e., it is easier to predict the presence of common words given nearby rare words than the opposite. Practically, predicting rare words first allows us to interrupt decoding after a fixed number of steps, then Related Work Automatic Story Generation Computer-aided story generation has been a source of interest since the early days of NLP. Classical AI algorithms relied on symbolic and logical planning and graph construction (Klein et al., 1973; Meehan, 1977; Turner, 1993; Riedl and Young, 2006). Statistical methods have also been proposed (McIntyre and Lapata, 2009; Li and Riedl, 2015; Gatt and Krahmer, 2018). Recently, the field has been influenced by the success of (conditional) neural language models (Bengio et al., 2003; Schwenk and Gauvain, 2004; Bahdanau et al., 2015; Nallapati et al., 2016). Story generation with neural models include (Chourdakis and Reiss, 2017; Peng et al., 2018; Radford et al., 2019). We build upon recent work that improves coherence in story generation by using hierarchical neural methods. These approaches allow reasoning at a higher level than words by considering a two-level hierarchy where a structuring representation cond"
W19-2405,W18-2706,1,0.646919,"w of the fountain, and it was Figure 1: In the one stage baseline, the missing span is predicted given the context and the target length. In the two stage method, words that should go in the span are predicted in inverse frequency order. For visualization, the left and right contexts have been truncated. Introduction Recent advances in language modeling have made considerable progress towards the automatic generation of fluent text (Jozefowicz et al., 2016; Baevski and Auli, 2019; Radford et al., 2019). This evolution has sparked the development of tools to assist human writers. For instance, Fan et al. (2018b) suggest generating short stories from high-level prompts, Clark et al. (2018b) study the interaction of human and language models for creative writing, and Peng et al. (2018) propose an interactive control of story lines. In addition, products such as Grammarly offer suggestions to improve grammar and wording (Hoover et al., 2015). Our work is concerned with story infilling. We envision this task as a step towards a suggestion tool to help writers interactively replace text spans. Text infilling, a form of cloze task (Taylor, 1953), involves removing sequences of words from text and asking"
W19-2405,P18-1082,0,0.332718,"w of the fountain, and it was Figure 1: In the one stage baseline, the missing span is predicted given the context and the target length. In the two stage method, words that should go in the span are predicted in inverse frequency order. For visualization, the left and right contexts have been truncated. Introduction Recent advances in language modeling have made considerable progress towards the automatic generation of fluent text (Jozefowicz et al., 2016; Baevski and Auli, 2019; Radford et al., 2019). This evolution has sparked the development of tools to assist human writers. For instance, Fan et al. (2018b) suggest generating short stories from high-level prompts, Clark et al. (2018b) study the interaction of human and language models for creative writing, and Peng et al. (2018) propose an interactive control of story lines. In addition, products such as Grammarly offer suggestions to improve grammar and wording (Hoover et al., 2015). Our work is concerned with story infilling. We envision this task as a step towards a suggestion tool to help writers interactively replace text spans. Text infilling, a form of cloze task (Taylor, 1953), involves removing sequences of words from text and asking"
W19-2405,K16-1028,0,0.03252,"ed number of steps, then Related Work Automatic Story Generation Computer-aided story generation has been a source of interest since the early days of NLP. Classical AI algorithms relied on symbolic and logical planning and graph construction (Klein et al., 1973; Meehan, 1977; Turner, 1993; Riedl and Young, 2006). Statistical methods have also been proposed (McIntyre and Lapata, 2009; Li and Riedl, 2015; Gatt and Krahmer, 2018). Recently, the field has been influenced by the success of (conditional) neural language models (Bengio et al., 2003; Schwenk and Gauvain, 2004; Bahdanau et al., 2015; Nallapati et al., 2016). Story generation with neural models include (Chourdakis and Reiss, 2017; Peng et al., 2018; Radford et al., 2019). We build upon recent work that improves coherence in story generation by using hierarchical neural methods. These approaches allow reasoning at a higher level than words by considering a two-level hierarchy where a structuring representation conditions text generation. Martin et al. (2018) use sequences of events to structure generation while Jain et al. (2017) relies on sequences of short descriptions. Fan et al. (2018b) rely on writing prompts. Closer to our work, Clark et al."
W19-2405,W18-1505,0,0.235276,"hould go in the span are predicted in inverse frequency order. For visualization, the left and right contexts have been truncated. Introduction Recent advances in language modeling have made considerable progress towards the automatic generation of fluent text (Jozefowicz et al., 2016; Baevski and Auli, 2019; Radford et al., 2019). This evolution has sparked the development of tools to assist human writers. For instance, Fan et al. (2018b) suggest generating short stories from high-level prompts, Clark et al. (2018b) study the interaction of human and language models for creative writing, and Peng et al. (2018) propose an interactive control of story lines. In addition, products such as Grammarly offer suggestions to improve grammar and wording (Hoover et al., 2015). Our work is concerned with story infilling. We envision this task as a step towards a suggestion tool to help writers interactively replace text spans. Text infilling, a form of cloze task (Taylor, 1953), involves removing sequences of words from text and asking for a replacement. Compared to traditional left-to-right language modeling, automatic infilling interacts well with human text revision processes, which are rarely purely left-t"
W19-2405,W18-1819,0,0.0710312,"Missing"
W19-3412,K16-1006,0,0.0118578,"ame and/or a nickname. For example, the nicknames for Sandor Clegane are Sandor and the hound. To identify the character(s) involved in a highlight from the tweets published during the highlight, we do the following: (1) given the highlight (section 3.1) we count the frequency of mentions of characters in tweets published during the highlight. We select the character with the most mentions. The intuition here is that the character with the most mentions in tweets published in each highlight played a major role in the sub-event that occurred during the highlight. 4 Our Model We use context2vec Melamud et al. (2016) to create a vector representation for the tweets in each highlight. Context2vec uses an unsupervised neural model, a bidirectional LSTM, to learn sentential context representations that result in comparable or better performances on tasks such as sentence completion and lexical substitution than popular context representation of averaged word embeddings. Context2vec learns sentential context representation around a target word by feeding one LSTM network with the sentence words around the target from left to right, and another from right to left. These left-to-right and rightto-left context w"
W19-3412,W04-3252,0,0.00912006,"les 2 and 3. Table 1 shows some of the summaries from our model for a highlight in both episodes 3 and 4. 6 Luhn: Derives a significance factor for each textual unit based occurrences and placements of frequent words within the unit (Luhn, 1958) Most Retweeted: We select the tweet with the most number of re-tweets in an highlight as a summary of the highlight. Baselines LexRank: Computes the importance of textual units using eigenvector centrality on a graph representation based on the similarity of the units (Erkan and Radev, 2004). TextRank: A graph-based extractive summarization algorithm (Mihalcea and Tarau, 2004). LSA: Constructs a terms-by-units matrix, and estimates the importance of the textual units based on SVD on the matrix (Gong and Liu, 2001) Rouge-1 38 14.4 10 15 10 10 Rouge-2 10 5 3 2 2 0 Rouge-L 31.5 12.3 10 11.7 10 10 Table 2: ROUGE-F1 scores on tweets from highlights in Episode 3 Algorithms Our Model Luhn Most Re-tweets TextRank LexRank LSA Rouge-1 33 16.5 10 10.5 10 11.3 Rouge-2 19.4 0 4 4 5 5 Rouge-L 25.2 13 10 10 10 10 Table 3: ROUGE-F1 scores on tweets from highlights in Episode 4 7 Conclusion and Future Work We proposed a model to summarize highlights of events from tweet streams rel"
W19-3412,K16-1028,0,0.0220247,"an event via hidden Markov models is proposed. Louis and Newman (2012) proposed an algorithm that aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster (Shen et al., 2013). In Nichols et al. (2012) an algorithm was proposed that uses the volume of tweets to identify subevents, then uses various weighting schemes to perform tweet selection. Li et al. (2017) proposed an algorithm for abstractive text summarization based on sequence-to-sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder. Nallapati et al. (2016) proposed a model using attentional endoder-decoder Recurrent Neural Network. Our algorithm is different from the previous work in that it identifies the character that had the most mentions in tweets published in a highlight and identifies the context in which this character was being discussed in this highlight; it then summarizes the highlight by selecting tweets that discuss this character in a similar context. highlights from this dataset and summarize these highlights. Each episode of GOTS7 lasted approximately an hour. We used the Twitter streaming API to collect time-stamped and tempor"
W19-3412,D17-1222,0,0.0154005,"algorithm was proposed based on the facility location problem. In Chakrabarti and Punera (2011) a summarization algorithm based on learning an underlying hidden state representation of an event via hidden Markov models is proposed. Louis and Newman (2012) proposed an algorithm that aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster (Shen et al., 2013). In Nichols et al. (2012) an algorithm was proposed that uses the volume of tweets to identify subevents, then uses various weighting schemes to perform tweet selection. Li et al. (2017) proposed an algorithm for abstractive text summarization based on sequence-to-sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder. Nallapati et al. (2016) proposed a model using attentional endoder-decoder Recurrent Neural Network. Our algorithm is different from the previous work in that it identifies the character that had the most mentions in tweets published in a highlight and identifies the context in which this character was being discussed in this highlight; it then summarizes the highlight by selecting tweets that discuss this character in a simila"
W19-3412,W11-0709,0,0.0200475,"ublished during the highlight and summarize the highlight by selecting the tweets that discuss this character in a similar context. 2 Related Work Some approaches to summarizing tweets related to an event adapt or modify summarization techniques that perform well with documents from news articles and apply these adaptations to tweets. In Sharifi et al. (2010a); Shen et al. 2 https://anietieandy.github.io (2013) a graph-based phrase reinforcement algorithm was proposed. In Sharifi et al. (2010b) a hybrid TF-IDF approach to extract one-or-multiplesentence summary for each topic was proposed. In Liu et al. (2011) an algorithm is proposed that explores a variety of text sources for summarizing twitter topics. In Harabagiu and Hickl (2011) an algorithm is proposed that synthesizes content from multiple microblog posts on the same topic and uses a generative model which induces event structures from the text and captures how users convey relevant content. In Marcus et al. (2011), a tool called ”Twitnfo” was proposed. This tool used the volume of tweets related to a topic to identify peaks and summarize these events by selecting tweets that contain a desired keyword or keywords, and selects frequent terms"
W19-3412,C12-2075,0,0.0176376,"rcus et al. (2011), a tool called ”Twitnfo” was proposed. This tool used the volume of tweets related to a topic to identify peaks and summarize these events by selecting tweets that contain a desired keyword or keywords, and selects frequent terms to provide an automated label for each peak. In Takamura et al. (2011); Shen et al. (2013) a summarization model based algorithm was proposed based on the facility location problem. In Chakrabarti and Punera (2011) a summarization algorithm based on learning an underlying hidden state representation of an event via hidden Markov models is proposed. Louis and Newman (2012) proposed an algorithm that aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster (Shen et al., 2013). In Nichols et al. (2012) an algorithm was proposed that uses the volume of tweets to identify subevents, then uses various weighting schemes to perform tweet selection. Li et al. (2017) proposed an algorithm for abstractive text summarization based on sequence-to-sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder. Nallapati et al. (2016) proposed a model using attentional endoder-dec"
W19-3412,N10-1100,0,0.042272,"ntify the highlights of pre-scheduled events from tweets streams related to the event and identify the character that had the most mentions in tweets published during the highlight. • Identify the context in which this character was being discussed in tweets published during the highlight and summarize the highlight by selecting the tweets that discuss this character in a similar context. 2 Related Work Some approaches to summarizing tweets related to an event adapt or modify summarization techniques that perform well with documents from news articles and apply these adaptations to tweets. In Sharifi et al. (2010a); Shen et al. 2 https://anietieandy.github.io (2013) a graph-based phrase reinforcement algorithm was proposed. In Sharifi et al. (2010b) a hybrid TF-IDF approach to extract one-or-multiplesentence summary for each topic was proposed. In Liu et al. (2011) an algorithm is proposed that explores a variety of text sources for summarizing twitter topics. In Harabagiu and Hickl (2011) an algorithm is proposed that synthesizes content from multiple microblog posts on the same topic and uses a generative model which induces event structures from the text and captures how users convey relevant conte"
W19-3412,N13-1135,0,0.0368533,"Missing"
