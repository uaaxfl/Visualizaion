2006.amta-papers.11,P04-1023,0,0.0239046,"nslation problem, we are given a source sentence, and the task is to output a target sentence which conveys the same meaning. In the process, the system produces a mapping between words of the source sentence and target sentence. In the alignment problem, we are given both sentences, and the task is simply to find this mapping. Alignment can therefore be thought of as a 90 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 90-99, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas Citation Koehn et al. (2003) 1 Callison-Burch et al. (2004) Callison-Burch et al. (2004) Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is"
2006.amta-papers.11,H05-1098,1,0.137604,"Alignments affect the quality of the lexical weighting feature. Here the correlation is direct: poor alignment will cause this feature to favor translation featuring word pairs which are not translations 4 Experiments All of our experiments were performed on ChineseEnglish translation in the news domain. The data we used in our experiments were divided into four parts. For phrase extraction and training of submodels, we used a large training set consisting of over 1 million sentences from various newswire corpora. This corpus is roughly the same as the one used for large-scale experiments by Chiang et al. (2005). We 6 An anonymous reviewer nicely summed up the relationship between word alignment and phrase extraction: “a couple currently if uneasily holding hands on the road to high-quality machine translation.” 93 supervised method. However, we believe that the GIZA++ alignments on this corpus are reasonably close to state-of-the-art. We refer to this alignment as Best. We wished to avoid confounding our study by considering alignments with vastly different profiles, such as recall-oriented alignments versus precision-oriented alignments. For our purposes, we were interested in the impact of the qua"
2006.amta-papers.11,P05-1033,0,0.0203216,"ences, the training data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system"
2006.amta-papers.11,W06-3105,0,0.0136155,"irst is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine translation is based on"
2006.amta-papers.11,P06-1002,0,0.142402,"aluation metrics for MT is longstanding and beyond the scope of this paper. The question of alignment metrics is actually closer to the problem at hand. Assuming that we have decided upon a satisfactory translation metric, one possible approach would be to optimize our alignment for different alignment metrics, in order to see which one best correlates with the final MT metric. A slightly different approach would be to create a parameterized alignment metric, and tune its parameters for MT output performance using logistic regression or similar techniques. Some of these issues are explored by Ayan and Dorr (2006) and Fraser and Marcu (2006). In this paper, we do not address the issues of specific metrics. Our experiments address the issue of alignment quality directly by using alignments whose qualitative rankings are consistent across all metrics. 3. The answer could be the obvious one: phrasebased translation is simply insensitive to the quality of the underlying alignment. It may simply be that the quality of word alignment links does not significantly impact the quality of the extracted phrase tables. The interaction between word alignments and phrase-based translation occurs in the learning step."
2006.amta-papers.11,H05-1009,0,0.0768425,"then the ith source word is aligned to the jth target word. Intrinsic evaluation is performed by comparison of the alignment set A with alignments created by human annotators. Annotations may contain two sets of links: the sure set S, containing only links about which all annotators are certain, and the probable set P, which includes all links in S as well as Because the heuristic approach depends on a word alignment, it is often assumed that the quality of the word alignment is critical to its success. A number of recent word alignment methods achieve impressive results on extrinsic metrics (Ayan et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). Often, it is implied that these improvements will propagate to a downstream translation system. However, several recent papers have reported that large gains in alignment accuracy often lead to, at best, minor gains in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show"
2006.amta-papers.11,H05-1012,0,0.066911,"task is to output a target sentence which conveys the same meaning. In the process, the system produces a mapping between words of the source sentence and target sentence. In the alignment problem, we are given both sentences, and the task is simply to find this mapping. Alignment can therefore be thought of as a 90 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 90-99, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas Citation Koehn et al. (2003) 1 Callison-Burch et al. (2004) Callison-Burch et al. (2004) Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is measured using the alignment error rate (AER) (Och and Ney"
2006.amta-papers.11,J96-1002,0,0.00904826,"Missing"
2006.amta-papers.11,N03-1017,0,0.572947,"urce and target sentences, the training data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a wor"
2006.amta-papers.11,W06-3123,0,0.00589422,"unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine translation is based on alignments between ph"
2006.amta-papers.11,koen-2004-pharaoh,0,0.349616,"properties and can be optimized directly for translation accuracy metrics, as has been shown for other NLP tasks, it is very slow (Taskar et al., 2004). Maximum likelihood probabilistic estimation is much faster, and for this reason it is very attractive for machine translation, where very large corpora are used, and efficiency is at a premium. To train the small number of log-linear feature weights, we use minimum error rate training (Och, 2003). The baseline translation model we consider in the following sections has eight features, following the example of the phrase-based Pharaoh system (Koehn, 2004). 1. A conditional phrase-to-phrase model that incorporates the probability of each phrase pair used in the derivation D (Equation 1). 2. The inverse conditional phrase-to-phrase probability model (Equation 2). 3. A lexical weighting feature (Equation 3). This feature operates over word alignments within phrase pairs. 4. The inverse lexical weighting (Equation 4). 5. A trigram language model feature. 6. A distortion count feature (Marcu and Wong, 2002; Koehn et al., 2003). 7. A feature counting the number of phrase pairs used in the translation. 8. A feature counting the number of target words"
2006.amta-papers.11,H01-1033,1,0.765317,"(AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair (F = f1 ... fI , E = e"
2006.amta-papers.11,W02-1018,0,0.170098,"to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine tra"
2006.amta-papers.11,H05-1095,0,0.0169738,"ining data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments ar"
2006.amta-papers.11,1996.amta-1.13,0,0.023964,"ent error rate (AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair"
2006.amta-papers.11,H05-1011,0,0.011763,"rget word. Intrinsic evaluation is performed by comparison of the alignment set A with alignments created by human annotators. Annotations may contain two sets of links: the sure set S, containing only links about which all annotators are certain, and the probable set P, which includes all links in S as well as Because the heuristic approach depends on a word alignment, it is often assumed that the quality of the word alignment is critical to its success. A number of recent word alignment methods achieve impressive results on extrinsic metrics (Ayan et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). Often, it is implied that these improvements will propagate to a downstream translation system. However, several recent papers have reported that large gains in alignment accuracy often lead to, at best, minor gains in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input align"
2006.amta-papers.11,W04-3207,0,0.0132889,"Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair (F = f1 ... fI , E = e1 ...eJ ) in which the source and target sentences contain I and J words, respectively. The alignment A"
2006.amta-papers.11,C00-2163,0,0.179259,"ukos (2005) 2 Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is measured using the alignment error rate (AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and"
2006.amta-papers.11,W04-3201,0,0.0199762,"iments are not necessarily comparable. AER is sensitive to annotation differences, and in particular to the presence or absence of probable links (Section 2). For a thorough explanation refer to Fraser and Marcu (2006). 4 Och and Ney (2000) report a similar relationship between AER and the word error rate metric for translation. 91 links that were uncertain (Och and Ney, 2000).5 Given the set of hypothesized alignment links A, we compute the standard metrics precision (P), recall (R), and alignment error rate (AER) as follows: found elsewhere in natural language processing (Ratnaparkhi, 1996; Taskar et al., 2004), translation models typically use a small feature space in which all features are active, and have non-integer values. These features are estimated using maximum likelihood methods. While the former approach has attractive properties and can be optimized directly for translation accuracy metrics, as has been shown for other NLP tasks, it is very slow (Taskar et al., 2004). Maximum likelihood probabilistic estimation is much faster, and for this reason it is very attractive for machine translation, where very large corpora are used, and efficiency is at a premium. To train the small number of"
2006.amta-papers.11,J03-1002,0,0.0674202,"in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input alignments were obtained using IBM Model 1 and IBM Model 4. The difference in performance of these two methods is known to be large; under similar conditions in a German-English evaluation the difference in AER was reported to be 9.3 absolute (Och and Ney, 2003). 2 The results in Ittycheriah and Roukos (2005) are reported in terms of alignment F-score. However, they point out that because their evaluation data for alignment contained only sure links (Section 2), we can obtain alignment error rate simply by subtracting the F-score from 1. We have done this here. 3 It should be noted that the AER numbers reported in these experiments are not necessarily comparable. AER is sensitive to annotation differences, and in particular to the presence or absence of probable links (Section 2). For a thorough explanation refer to Fraser and Marcu (2006). 4 Och and"
2006.amta-papers.11,H05-1010,0,0.0153172,"trinsic evaluation is performed by comparison of the alignment set A with alignments created by human annotators. Annotations may contain two sets of links: the sure set S, containing only links about which all annotators are certain, and the probable set P, which includes all links in S as well as Because the heuristic approach depends on a word alignment, it is often assumed that the quality of the word alignment is critical to its success. A number of recent word alignment methods achieve impressive results on extrinsic metrics (Ayan et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). Often, it is implied that these improvements will propagate to a downstream translation system. However, several recent papers have reported that large gains in alignment accuracy often lead to, at best, minor gains in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input alignments were obtained us"
2006.amta-papers.11,W99-0604,0,0.18997,"gnments between source and target sentences, the training data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we"
2006.amta-papers.11,2005.eamt-1.36,0,0.010191,"tease apart the effects of the two 8 Because the BLEU score is computed using aggregate statistics over the output, the locally best output for any given input sentence is not necessarily the one that results in the best overall BLEU score (indeed, due to BLEU’s use of a geometric average, most single sentences turn out to have a BLEU score of 0, which is not very useful even for determining the locally optimal sentence). Computing the choice of sentences which results in the best global BLEU is an intractable search problem, so we resorted to a greedy hill-climbing search (Och et al., 2004; Venugopal and Vogel, 2005). This works as follows: we first choose for each input sentence an output that maximizes a local non-zero approximation to BLEU. We then iterate over our input sentences and at each step choose a new output from the 1000-best list that optimizes the global BLEU score while holding all the other outputs constant. This is repeated until no further gains in BLEU score can be found. 95 Alignment Quality Best Slightly Degraded Moderately Degraded Highly Degraded BLEU score Dev Test .293 .272 .289 .268 .278 .262 .274 .252 Alignment Quality Best Slightly Degraded Moderately Degraded Highly Degraded"
2006.amta-papers.11,H01-1035,0,0.02398,"red using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair (F = f1 ... fI , E = e1 ...eJ ) in which the source and target sentences contain I"
2006.amta-papers.11,P03-1021,0,0.0377431,"h all features are active, and have non-integer values. These features are estimated using maximum likelihood methods. While the former approach has attractive properties and can be optimized directly for translation accuracy metrics, as has been shown for other NLP tasks, it is very slow (Taskar et al., 2004). Maximum likelihood probabilistic estimation is much faster, and for this reason it is very attractive for machine translation, where very large corpora are used, and efficiency is at a premium. To train the small number of log-linear feature weights, we use minimum error rate training (Och, 2003). The baseline translation model we consider in the following sections has eight features, following the example of the phrase-based Pharaoh system (Koehn, 2004). 1. A conditional phrase-to-phrase model that incorporates the probability of each phrase pair used in the derivation D (Equation 1). 2. The inverse conditional phrase-to-phrase probability model (Equation 2). 3. A lexical weighting feature (Equation 3). This feature operates over word alignments within phrase pairs. 4. The inverse lexical weighting (Equation 4). 5. A trigram language model feature. 6. A distortion count feature (Marc"
2006.amta-papers.11,I05-3011,0,0.0354292,"ignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine translation is based on alignments between phrases – sequences of words in the source and target sentences. The learning step in these systems often relies on alignments between words. It is often assumed that the quality of this word alignment is critical for translation. However, recent results sugge"
2006.amta-papers.11,P02-1040,0,0.105407,"(2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is measured using the alignment error rate (AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2"
2006.amta-papers.11,W96-0213,0,\N,Missing
2006.amta-papers.11,J93-2003,0,\N,Missing
2006.amta-papers.11,J07-3002,0,\N,Missing
2006.amta-papers.11,N04-1021,0,\N,Missing
2009.iwslt-papers.4,J07-2003,0,0.736251,"ervice of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based methods fail to capture to capture the essence of many language pairs [5]. One of the reasons is that reordering cannot always be reduced to the reordering of atom phrase units. Consider the mapping of the following sentence pair fragment: - 152 - 1 http://translate.google.com/ Proceedings of IWSLT 2009, Tokyo - Japan take the proposal into account ber¨ucksichtigt den Vorschlag The English phrasal verb take into account wraps around its object the proposal. Hierarchical phrasebased models [6] extend the notion of phrase mapping to allow rules such as take X1 into account k ber¨ucksichtigt X1 must explain X1 k muss X1 erkl¨aren either X1 or X2 k entweder X1 oder X2 Such translation rules may be formalized as a synchronous context free grammar, where the non-terminal X matches any constituent, and nonterminals with the same coindexes (e.g. X1 ) are recursively translated by a single rule. Such a formalism reflects one of the major insights of linguistics: Language is recursive and all modern theories of language use recursive structures. The research questions evolve around how to s"
2009.iwslt-papers.4,P06-1121,0,0.124726,"ce. • Store all extracted phrase pairs and rules for scoring. To provide one empirical fact to support this argument: The adaptation of the originally purely phrasebased training process in Moses to hierarchical and syntaxbased models took less than one month of work. Many syntax-based models relax the requirement that phrases have to correspond to syntactic constituents. For instance, in one of the best-performing models translation units may correspond to syntactic treelets (tree fragments), permitting reordering at a scope larger than that of a single constituent and its immediate children [7]. Also, spans that only match a sequence of constituents or incomplete constituents may be labeled with complex tags such as DET + ADJ or NP/N [8]. Note that these are manipulations of the syntax trees that do not change in any way the rule extraction method. There are many refinements to the the rule extraction method. Limits may be imposed to span sizes as well as number of words and non-terminals. Fractional counts may be for rules extracted from the same spans. Only minimal rules may be extracted to explain a sentence pair. Smoothing counts may be done using Good Turing discounting or othe"
2009.iwslt-papers.4,W06-3119,0,0.067609,"ally purely phrasebased training process in Moses to hierarchical and syntaxbased models took less than one month of work. Many syntax-based models relax the requirement that phrases have to correspond to syntactic constituents. For instance, in one of the best-performing models translation units may correspond to syntactic treelets (tree fragments), permitting reordering at a scope larger than that of a single constituent and its immediate children [7]. Also, spans that only match a sequence of constituents or incomplete constituents may be labeled with complex tags such as DET + ADJ or NP/N [8]. Note that these are manipulations of the syntax trees that do not change in any way the rule extraction method. There are many refinements to the the rule extraction method. Limits may be imposed to span sizes as well as number of words and non-terminals. Fractional counts may be for rules extracted from the same spans. Only minimal rules may be extracted to explain a sentence pair. Smoothing counts may be done using Good Turing discounting or other methods. - 153 - Proceedings of IWSLT 2009, Tokyo - Japan PHRASE - BASED HIERARCHICAL SYNTAX - BASED raw translated text raw translated text raw"
2009.iwslt-papers.4,W01-1812,0,0.0359255,"ons may also fan in to a node due to dynamic programming. A hypothesis, or state in the search graph, points back to its highest-probable path, but also alternative paths with lower probability. In practice, we store with each state information such as which foreign words have been covered so far, the partial translation constructed so far, and the model scores along with all underlying component scores. But this information may also be obtained by walking back the best possible path. In chart decoding the transitions may originate from multiple hypotheses. This can visualized as a hypergraph [9, 10], a generalization of a graph in which an edge (called a hyperedge) may originate from multiple nodes (called tail nodes). The nodes of the hypergraph - 155 - Proceedings of IWSLT 2009, Tokyo - Japan correspond to hypotheses, while the hyperedges correspond to rule applications. Just as in the graph case, we can extract a best hyperpath that corresponds to a single set of rule applications. Note that this is simply an extension of the case for phrase-based models, and indeed the graph generated by a phrase-based model is simply the special case of a hypergraph in which each hyperedge has only"
2009.iwslt-papers.4,P07-1019,0,0.587451,"ons may also fan in to a node due to dynamic programming. A hypothesis, or state in the search graph, points back to its highest-probable path, but also alternative paths with lower probability. In practice, we store with each state information such as which foreign words have been covered so far, the partial translation constructed so far, and the model scores along with all underlying component scores. But this information may also be obtained by walking back the best possible path. In chart decoding the transitions may originate from multiple hypotheses. This can visualized as a hypergraph [9, 10], a generalization of a graph in which an edge (called a hyperedge) may originate from multiple nodes (called tail nodes). The nodes of the hypergraph - 155 - Proceedings of IWSLT 2009, Tokyo - Japan correspond to hypotheses, while the hyperedges correspond to rule applications. Just as in the graph case, we can extract a best hyperpath that corresponds to a single set of rule applications. Note that this is simply an extension of the case for phrase-based models, and indeed the graph generated by a phrase-based model is simply the special case of a hypergraph in which each hyperedge has only"
2009.iwslt-papers.4,E09-1061,1,0.86948,"h in which each hyperedge has only one tail node. The virtue of the hypergraph view is that, even though our models have superficially quite different structures, their search spaces can all be represented in the same way, making them amenable to a variety of hypergraph algorithms [11]. These algorithms generalize familiar graph algorithms, which are simply special cases of their hypergraph generalizations. With this in mind, most statistical translation systems can be viewed as implementations of a very small number of generic algorithms, in which the main difference is a modelspecific logic [12]. 4.5. Stacks Viewing decoding as the task of finding the most probable path in a search graph or hypergraph, is one visualization of the problem. However, this graph is too large to efficiently construct even for relatively short sentences. We need to focus on the most promising part of the graph. To this end, we first group together comparable hypotheses in stacks, and then prune out the weaker ones. There are many ways to define the stacks. In sequential decoding, we group together hypotheses that cover the same number of input words. In chart decoding, we group together hypotheses that cov"
2009.iwslt-papers.4,J03-1005,0,0.0102483,"raph is too large to efficiently construct even for relatively short sentences. We need to focus on the most promising part of the graph. To this end, we first group together comparable hypotheses in stacks, and then prune out the weaker ones. There are many ways to define the stacks. In sequential decoding, we group together hypotheses that cover the same number of input words. In chart decoding, we group together hypotheses that cover the same input span. More fine-grained groupings are possible: in sequential decoding we could distinguish between hypotheses that cover different input words [13], and in chart decoding for models with target side syntax, we may keep different stacks for different target-side labels. However, we want to avoid having too many stacks, and such additional distinctions may also be enforced by diversity requirements during pruning [14]. We prune bad hypotheses based on their incremental score so far. When comparing hypotheses that cover different input words, we also include a future cost estimate for the remaining words. 4.6. Search Strategy The final decision of the decoding algorithm is: In which order do we generate the hypotheses? The incremental scori"
2009.iwslt-papers.4,J04-4002,0,0.0402213,"andide project [1]. However, occasionally words have to be inserted and deleted without clear lexical correspondence on the other side, and words do not always map one-to-one. As a consequence, the wordbased models proposed by IBM were burden with additional complexities such as word fertilities and NULL word generation. 2.1. Phrase-Based Models Over the last decade, word-based models have been all but abandoned (they still live on in word alignment methods), and replaced by an even simpler view of language. Phrase-based models view translation of small text chunks, again with some reordering [2, 3]. The complexities of many-to-many translation, insertion and deletion are hidden within the phrasal translation table. To give examples, phrase-based models may include rules such as assumes k geht davon aus, dass with regard to k bez¨uglich ¨ translation system k Ubersetzungssystem Implementations of such phrase-based models of translation have been shown to outperform all existing translation systems for some language pairs [4]. Currently most prominent is the online translation service of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based method"
2009.iwslt-papers.4,2008.iwslt-papers.8,0,0.0120533,"e the stacks. In sequential decoding, we group together hypotheses that cover the same number of input words. In chart decoding, we group together hypotheses that cover the same input span. More fine-grained groupings are possible: in sequential decoding we could distinguish between hypotheses that cover different input words [13], and in chart decoding for models with target side syntax, we may keep different stacks for different target-side labels. However, we want to avoid having too many stacks, and such additional distinctions may also be enforced by diversity requirements during pruning [14]. We prune bad hypotheses based on their incremental score so far. When comparing hypotheses that cover different input words, we also include a future cost estimate for the remaining words. 4.6. Search Strategy The final decision of the decoding algorithm is: In which order do we generate the hypotheses? The incremental scoring allows us to already compute fairly indicative scores for partial translation, so we broadly pursue a bottom-up decoding strategy, where we generate hypotheses of increasing input word coverage. This also allows efficient dynamic programming, since we generate all hypo"
2009.iwslt-papers.4,N03-1017,1,0.0173795,"andide project [1]. However, occasionally words have to be inserted and deleted without clear lexical correspondence on the other side, and words do not always map one-to-one. As a consequence, the wordbased models proposed by IBM were burden with additional complexities such as word fertilities and NULL word generation. 2.1. Phrase-Based Models Over the last decade, word-based models have been all but abandoned (they still live on in word alignment methods), and replaced by an even simpler view of language. Phrase-based models view translation of small text chunks, again with some reordering [2, 3]. The complexities of many-to-many translation, insertion and deletion are hidden within the phrasal translation table. To give examples, phrase-based models may include rules such as assumes k geht davon aus, dass with regard to k bez¨uglich ¨ translation system k Ubersetzungssystem Implementations of such phrase-based models of translation have been shown to outperform all existing translation systems for some language pairs [4]. Currently most prominent is the online translation service of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based method"
2009.iwslt-papers.4,W09-1114,1,0.818025,"ision Rule Finally, we have to pick one of the hypotheses that cover the entire input sentence to output a translation. Most commonly, this is the hypothesis with the best score, but that is not the only choice. There may be multiple ways to produce the same output. If our goal is to find the most probable translation given the input, then we should find all possible paths through the search graph that result in the same output and sum up their scores. Then, we output the translation with the highest score over all derivation. This is called max-translation decoding vs. maxderivation decoding [15]. But what if the best translation is an outlier? Given the uncertainty in all our models, we may prefer instead a different high-scoring translation that is most similar to the other high-scoring translations. This is the motivation for minimum Bayes risk decoding [16], which has been shown to often lead to better results. - 156 - Proceedings of IWSLT 2009, Tokyo - Japan Model phrase-based hierarchical target-syntax 5. Implementation Based on our observations about the deep similarities between many popular translation models, we have substantially extended the functionality of the Moses tool"
2009.iwslt-papers.4,W09-0401,1,0.68025,"n word alignment methods), and replaced by an even simpler view of language. Phrase-based models view translation of small text chunks, again with some reordering [2, 3]. The complexities of many-to-many translation, insertion and deletion are hidden within the phrasal translation table. To give examples, phrase-based models may include rules such as assumes k geht davon aus, dass with regard to k bez¨uglich ¨ translation system k Ubersetzungssystem Implementations of such phrase-based models of translation have been shown to outperform all existing translation systems for some language pairs [4]. Currently most prominent is the online translation service of Google that follows this approach.1 2.2. Hierarchical Phrase-Based Models However, phrase-based methods fail to capture to capture the essence of many language pairs [5]. One of the reasons is that reordering cannot always be reduced to the reordering of atom phrase units. Consider the mapping of the following sentence pair fragment: - 152 - 1 http://translate.google.com/ Proceedings of IWSLT 2009, Tokyo - Japan take the proposal into account ber¨ucksichtigt den Vorschlag The English phrasal verb take into account wraps around its"
2009.iwslt-papers.4,N04-1022,0,0.0211555,"our goal is to find the most probable translation given the input, then we should find all possible paths through the search graph that result in the same output and sum up their scores. Then, we output the translation with the highest score over all derivation. This is called max-translation decoding vs. maxderivation decoding [15]. But what if the best translation is an outlier? Given the uncertainty in all our models, we may prefer instead a different high-scoring translation that is most similar to the other high-scoring translations. This is the motivation for minimum Bayes risk decoding [16], which has been shown to often lead to better results. - 156 - Proceedings of IWSLT 2009, Tokyo - Japan Model phrase-based hierarchical target-syntax 5. Implementation Based on our observations about the deep similarities between many popular translation models, we have substantially extended the functionality of the Moses toolkit [17], which previously supported only phrase-based models. In particular, our implementation includes a chart decoder that can handle general synchronous contextfree grammars, including both hierarchical and syntaxbased grammars. Both phrase-based and hierarchical d"
2009.iwslt-papers.4,D07-1079,0,0.0381358,"ave substantially extended the functionality of the Moses toolkit [17], which previously supported only phrase-based models. In particular, our implementation includes a chart decoder that can handle general synchronous contextfree grammars, including both hierarchical and syntaxbased grammars. Both phrase-based and hierarchical decoders implement cube pruning [6, 10] and minimum Bayes risk decoding [16]. Our training implementation also includes rule extraction for hierarchical [6] and syntax-based translation. The syntax-based rule extractor produces rules similar to the “composed rules” of [18]. The source code is freely available.2 This allows us to take advantage of the mature Moses infrastructure by retaining much of the existing components. Also, the development of a hierarchical system alongside a phrase-based system allows us to more easily and fairly compare and contrast the models. Re-using and extending the existing Moses decoder reduces the amount of development required. As an illustration, the phrase-based decoder 24,000 lines of code. The more complex hierarchical and syntax extension added 10,000 lines to the codebase. Some components in a phrase-based and hierarchical"
2009.iwslt-papers.4,C04-1024,0,0.0499602,"cal, and a syntax-based model that uses syntax on the target side. We trained systems using the News Commentary training set that was released by WMT 20093 for English to German translation. See Table 1 for statistics on rule table sizes and BLEU scores for the news-dev2009b test set. Decoding for all the three models took about the same time, roughly 0.3 seconds per word. Decoding for hierarchical and syntax-based models is more complex, and we expect to achieve better results by tuning the search algorithm and using larger beam sizes. The syntax-based model uses the BitPar parser for German [19]. Note that recent work has shown that state-of-the-art performance requires improvements to word alignment [20] and data preparation, which were not done for these experiments. 3 http://mosesdecoder.svn.sourceforge.net - 157 - http://www.statmt.org/wmt09/ Proceedings of IWSLT 2009, Tokyo - Japan 7. Conclusions and Outlook ing success in machine translation,” in Proc. of EMNLP, 2008. Our experiments illustrate that the hierarchical and syntactic models in Moses achieve similar quality to the phrase-based model, even though their implementation is less mature. We expect that their performance w"
2009.iwslt-papers.4,W08-0306,0,0.0156471,"training set that was released by WMT 20093 for English to German translation. See Table 1 for statistics on rule table sizes and BLEU scores for the news-dev2009b test set. Decoding for all the three models took about the same time, roughly 0.3 seconds per word. Decoding for hierarchical and syntax-based models is more complex, and we expect to achieve better results by tuning the search algorithm and using larger beam sizes. The syntax-based model uses the BitPar parser for German [19]. Note that recent work has shown that state-of-the-art performance requires improvements to word alignment [20] and data preparation, which were not done for these experiments. 3 http://mosesdecoder.svn.sourceforge.net - 157 - http://www.statmt.org/wmt09/ Proceedings of IWSLT 2009, Tokyo - Japan 7. Conclusions and Outlook ing success in machine translation,” in Proc. of EMNLP, 2008. Our experiments illustrate that the hierarchical and syntactic models in Moses achieve similar quality to the phrase-based model, even though their implementation is less mature. We expect that their performance will continue to be improved by drawing on the substantial body of research in syntactic translation modeling ove"
2009.iwslt-papers.4,C08-1144,0,0.0345081,"Missing"
2009.iwslt-papers.4,D07-1078,0,0.0412003,"Missing"
2009.iwslt-papers.4,J93-2003,0,\N,Missing
2009.iwslt-papers.4,P07-2045,1,\N,Missing
2009.iwslt-papers.4,D08-1078,1,\N,Missing
2013.iwslt-papers.14,2005.iwslt-1.20,0,0.0595346,"er, and a host of factors that alter how an individual speaks (such as heartrate, stress, emotional state). Machine translation accuracy is affected by different factors, such as domain (e.g., newswire, medical, SMS, speech), register, and the typological differences between the languages. Because these technologies are imperfect themselves, their inaccuracies tend to multiply when they are chained together in the task of speech translation. Cross-lingual speech applications are typically built by combining speech recognition and machine translation systems, each trained on disparate datasets [1, 2]. The recognizer makes mistakes, passing text to the MT system with vastly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful f"
2013.iwslt-papers.14,D08-1027,0,0.0480385,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,D09-1030,1,0.309927,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,N10-1024,1,0.431154,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,N12-1006,1,0.287152,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,P11-1122,1,0.487136,"rily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceeded 25 words, it was split on the next utterance boundary. We pr"
2013.iwslt-papers.14,W12-3152,1,0.743004,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,2005.mtsummit-papers.11,0,0.0360278,"d practice, we took steps to deter wholesale use of automated translation services by our translators. • Utterances were presented as images rather than text; this prevented cutting and pasting into online translation services.5 • We obtained translations from Google Translate for the utterances before presenting them to workers. HITs which had a small edit distance from these translations were manually reviewed and rejected if they were too similar (in particular, if they contained many of the same errors). • We also included four consecutive short sentences from the Europarl parallel corpus [17] in each HIT. HITs which had low overlap with the reference translations of these sentences were manually reviewed and rejected if they were of low quality. We obtained four redundant translations of sixty randomly chosen conversations from the Fisher corpus. In total, 115 workers completed 2463 HITs, producing 46,324 utterance-level translations and a little less than half a million words. 2.3. Selection of Preferred Translators We then extended a strategy devised by [16] to select highquality translators from the first round of translations. We designed a second-pass HIT which was used to ra"
2013.iwslt-papers.14,W13-2226,1,0.868991,"performance of the MT system, and we report experiments varying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty •"
2013.iwslt-papers.14,J07-2003,0,0.0593645,"arying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Inter"
2013.iwslt-papers.14,P08-1115,0,0.0451524,"we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Interface Transcript 1-best Lattice Oracle Path Euro 41.8 24.3 32.1 LDC 58.7 35.4 37.1 46.2 ASR 54.6 34.7 35.9 44.3 LDC +ASR 58.7 35.5 36.8 46.3 Table 4: BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems. Training set Interface Tra"
2013.iwslt-papers.14,N12-1047,0,0.0637002,"isher Train, as described above. • ASR. An in-domain model trained on pairs of Spanish ASR outputs and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Trans"
2013.iwslt-papers.14,P02-1040,0,0.106207,"and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Transcript 1-best Lattice Reference 1-best → MT Lattice → MT 1-best → Google sí hablar de cuáles y cosas"
2020.acl-main.159,P19-1376,1,0.740346,"result from overapplication of the regular past tense (e.g. throw– throwed)—a type of error observed in human language learners as well—as opposed to the unattested forms produced by Rumelhart and McClelland’s model. K&C conclude that modern neural networks can learn human-like behavior for English past tense without recourse to explicit symbolic structure, and invite researchers to move beyond the ‘rules’ debate, asking instead whether the learner correctly generalizes to a range of novel inputs, and whether its errors (and other behavior) are human-like. This challenge was first taken up by Corkery et al. (2019), who showed that, on novel English-like words designed to elicit some irregular generalizations from humans, the ED model’s predictions do not closely match the human data. While these results suggest possible problems with the ED model, English may not be the best test case to fully understand these, since the sole regular inflectional class is also by far the most frequent. In contrast, many languages have multiple inflectional classes which can act ‘regular’ under various conditions (Seidenberg and Plaut, 2014; Clahsen, 2016). In this paper, we examine German number inflection, which has b"
2020.acl-main.159,P16-2090,0,0.146389,"Missing"
2020.acl-main.159,L16-1498,0,0.0255326,"Missing"
2020.acl-main.159,W18-1817,0,0.0582117,"Missing"
2020.acl-main.159,D19-1331,0,0.0590498,"Missing"
2020.acl-main.159,P15-2111,0,0.020628,"Missing"
2020.cmcl-1.8,P19-1376,1,0.883714,"f. Marcus et al. 1995). Based on the artificial language learning literature (e.g. Newport, 2016), we might expect speakers to display conditional probability matching on novel German nouns, such that the probability of a noun taking a certain plural inflection — in particular, the two highly frequent classes -e and -(e)n — depends upon its grammatical gender. Neural encoder-decoder (ED) models have recently been proposed for consideration as models of speaker cognition (Kirov and Cotterell, 2018). This has prompted investigation into the extent to which these models capture speaker behavior (Corkery et al., 2019; King et al., 2020; McCurdy et al., 2020). Earlier work suggests that neural models of German plural inflection are sensitive to grammatical gender: Goebel and Indefrey (2000) found that a simple recurrent network learned to favor -e plurals for masculine nouns, and -(e)n when the same nouns were presented as feminine gender. We hypothesize that neural models and adult speakers are equally capable of using the information available from grammatical gender to predict number inflection. We expect both to demonstrate similar gender-conditioned probability matching to the distribution shown in Fi"
2020.cmcl-1.8,2020.acl-main.159,1,0.904286,"ficial language learning literature (e.g. Newport, 2016), we might expect speakers to display conditional probability matching on novel German nouns, such that the probability of a noun taking a certain plural inflection — in particular, the two highly frequent classes -e and -(e)n — depends upon its grammatical gender. Neural encoder-decoder (ED) models have recently been proposed for consideration as models of speaker cognition (Kirov and Cotterell, 2018). This has prompted investigation into the extent to which these models capture speaker behavior (Corkery et al., 2019; King et al., 2020; McCurdy et al., 2020). Earlier work suggests that neural models of German plural inflection are sensitive to grammatical gender: Goebel and Indefrey (2000) found that a simple recurrent network learned to favor -e plurals for masculine nouns, and -(e)n when the same nouns were presented as feminine gender. We hypothesize that neural models and adult speakers are equally capable of using the information available from grammatical gender to predict number inflection. We expect both to demonstrate similar gender-conditioned probability matching to the distribution shown in Figure 1 (lower), resulting in a majority us"
2020.cmcl-1.8,P16-2090,0,0.0632739,"Missing"
2020.cmcl-1.8,2020.acl-main.597,0,0.0579472,"Missing"
2020.cmcl-1.8,2020.scil-1.58,0,0.026412,". Based on the artificial language learning literature (e.g. Newport, 2016), we might expect speakers to display conditional probability matching on novel German nouns, such that the probability of a noun taking a certain plural inflection — in particular, the two highly frequent classes -e and -(e)n — depends upon its grammatical gender. Neural encoder-decoder (ED) models have recently been proposed for consideration as models of speaker cognition (Kirov and Cotterell, 2018). This has prompted investigation into the extent to which these models capture speaker behavior (Corkery et al., 2019; King et al., 2020; McCurdy et al., 2020). Earlier work suggests that neural models of German plural inflection are sensitive to grammatical gender: Goebel and Indefrey (2000) found that a simple recurrent network learned to favor -e plurals for masculine nouns, and -(e)n when the same nouns were presented as feminine gender. We hypothesize that neural models and adult speakers are equally capable of using the information available from grammatical gender to predict number inflection. We expect both to demonstrate similar gender-conditioned probability matching to the distribution shown in Figure 1 (lower), res"
2020.findings-emnlp.252,P18-2003,0,0.0207798,"d. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed n-gram data, implying that they exploit linguistic structure in long-distance dependencies (Liu et al., 2018). Their internal representations appear to encode constituency (Blevins et al., 2018; Hupkes and Zuidema, 2018) and syntactic agreement (Lakretz et al., 2019; Gulordava et al., 2018). In this paper, we consider how such representations are learned, and what kind of inductive bias supports them. To understand how LSTMs exploit syntax, we use contextual decomposition (CD; Section 2.1), a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence (DI; Section 2.2), a measure of interaction between spans of words to produce the representation at a particular timestep. For exampl"
2020.findings-emnlp.252,N19-1419,0,0.161204,"20) even found that the recurrent inductive biases behind the LSTM’s success are so essential that distilling from them can improve the performance of fully attentional models. However, the reasons behind the LSTM’s effectiveness in language domains remain poorly understood. 1 As evidence of the ongoing popularity of LSTMs in NLP, a Google Scholar search restricted to aclweb.org since 2019 finds 191 citations to the original LSTM paper (Hochreiter and Schmidhuber, 1997) and 242 citations to the original Transformer paper (Vaswani et al., 2017). A Transformer can encode syntax using attention (Hewitt and Manning, 2019), and some LSTM variants explicitly encode syntax (Bowman et al., 2016; Dyer et al., 2016). So, the success of these models is partly explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language d"
2020.findings-emnlp.252,P16-1139,0,0.0189128,"are so essential that distilling from them can improve the performance of fully attentional models. However, the reasons behind the LSTM’s effectiveness in language domains remain poorly understood. 1 As evidence of the ongoing popularity of LSTMs in NLP, a Google Scholar search restricted to aclweb.org since 2019 finds 191 citations to the original LSTM paper (Hochreiter and Schmidhuber, 1997) and 242 citations to the original Transformer paper (Vaswani et al., 2017). A Transformer can encode syntax using attention (Hewitt and Manning, 2019), and some LSTM variants explicitly encode syntax (Bowman et al., 2016; Dyer et al., 2016). So, the success of these models is partly explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed n-gram data, implying that t"
2020.findings-emnlp.252,2020.acl-main.494,0,0.013249,"e timestamp at which to access their representations. However, in order to minimize information degradation over time, we access ht at the lowest timestep accommodating all spans in focus, t = max(idx(A), idx(B)). 2799 IQRt (A, B) = Figure 4: Mean DI (y-axis) between word pairs at varying syntactic distances (x-axis), stratified by whether the POS tags are closed or open class (line color) and by sequential distance (plot title). The y-axis ranges differ, but the scale is the same for all plots. Each mean is plotted only if there are at least 100 cases to average. Concurrently with this work, Chen et al. (2020) also developed a method of studying the interaction between words using Shapley-based techniques like CD. However, their method was based on an assumption of underlying hierarchical structure and therefore unsuitable for the experiments we are about to conduct. Their results nonetheless validate the relationship between feature interaction and syntactic structure. 3 English Language Experiments We now apply our measure of DI to a natural language setting to see how LSTMs employ bottom-up construction. In natural language, disentangling the meaning of individual words requires contextual infor"
2020.findings-emnlp.252,W19-4828,0,0.0221083,"le natural language exhibits the same cell state dynamics that make a memorized scaffold promote or inhibit long-range rule learning. Future work could test our findings on the learning process through carefully selected natural language, rather than synthetic, data. Our natural language results could lead to DI as a structural probe for testing syntax. Such a probe can be computed directly from an LSTM without learning additional parameters as required in other methods (Hewitt and Manning, 2019). In this way, it is similar to the probes that have been developed using attention distributions (Clark et al., 2019). By computing associations naturally through DI, we can even escape the need to augment models with attention just to permit analysis, as Kuncoro et al. (2017). Some effects on our natural language experiments may be due to the predictable nature of English syntax, which favors right-branching behavior. Future work could apply similar analysis to other languages with different grammatical word orders. 7 Conclusions Using our proposed tool of Decompositional Interdependence, we illustrate how information exchanged between words aligns roughly with syntactic structure, indicating LSTMs compose"
2020.findings-emnlp.252,P15-1033,0,0.0118623,"Bottom-up training is not a given and must be verified.5 However, if the hypothesis holds and training builds syntactic patterns hierarchically, it can lead to representations that are built hierarchically at inference time, reflecting linguistic structure, as we have seen. To test the idea of a compositional training process, we use synthetic data that controls for the consistency and frequency of longer-range relations. We find: with its child constituents and further associations are less dependent on each other. Similar behavior is the goal of RNNGs and other models which use stack LSTMs (Dyer et al., 2015), which ensure the words in a constituent will be highly interdependent in their shared representation because the constituent will be based on a dictionary lookup for its subtree structure. In an RNNG, this behavior is a result of bottom-up learning during training, when the composition operation combines existing tag subtrees into a new lookup key. Our next experiments will illustrate how LSTMs already learn bottom-up implicitly, because they are biased towards the top behavior in Figure 5 when a scaffolding environment is available. 4 1. LSTMs trained with familiar intervening spans have po"
2020.findings-emnlp.252,N16-1024,0,0.133718,"t distilling from them can improve the performance of fully attentional models. However, the reasons behind the LSTM’s effectiveness in language domains remain poorly understood. 1 As evidence of the ongoing popularity of LSTMs in NLP, a Google Scholar search restricted to aclweb.org since 2019 finds 191 citations to the original LSTM paper (Hochreiter and Schmidhuber, 1997) and 242 citations to the original Transformer paper (Vaswani et al., 2017). A Transformer can encode syntax using attention (Hewitt and Manning, 2019), and some LSTM variants explicitly encode syntax (Bowman et al., 2016; Dyer et al., 2016). So, the success of these models is partly explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed n-gram data, implying that they exploit linguist"
2020.findings-emnlp.252,N18-1108,0,0.0393615,"hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed n-gram data, implying that they exploit linguistic structure in long-distance dependencies (Liu et al., 2018). Their internal representations appear to encode constituency (Blevins et al., 2018; Hupkes and Zuidema, 2018) and syntactic agreement (Lakretz et al., 2019; Gulordava et al., 2018). In this paper, we consider how such representations are learned, and what kind of inductive bias supports them. To understand how LSTMs exploit syntax, we use contextual decomposition (CD; Section 2.1), a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence (DI; Section 2.2), a measure of interaction between spans of words to produce the representation at a particular timestep. For example, in the sentence “Socrates asked the student trick questions”, we might expect the hidden repres"
2020.findings-emnlp.252,K19-1001,0,0.150452,"aning of “cake” does not depend on “happily”, which modifies “eat” and is far on the syntactic tree from “cake”, but the meaning of “cake” should be more dependent on “slice”, which gives context for its part of speech and suggests that it is concrete.3 We will use the nonlinear interactions in contextual decomposition to analyze the DI between words alternately considered in focus. Generally, CD considers all nonlinear interactions between the relevant and irrelevant sets of ¯ the irrelevant contriwords to fall under βÓβ, bution, although other allocations of interactions have been proposed (Jumelet et al., 2019). DI uses these nonlinearities to discover how strongly a pair of spans are associated. A fully flat structure for building meaning could lead to a contextual representation that requires memorization of each word, breaking the simplifying assumption at the heart of CD that each word has an independent meaning to be incorporated into the sentence. Given two interacting sets of words to potentially designate as the β in focus, A, B such that A ∩ B = ∅, we use a measure of DI to quantify 3 In our natural language experiments, we focus on dependency relations, but the inductive bias we observe is"
2020.findings-emnlp.252,W19-4801,0,0.0230798,"advantage of some inherent trait of language (Liu et al., 2018) . The compositional training we have explored may be the mechanism behind this biased representational power. Synthetic data, meanwhile, has formed the basis for analyzing the inductive biases of neural networks and their capacity to learn compositional rules. Common synthetic datasets include the Dyck languages (Suzgun et al., 2019; Skachkova et al., 2018), SPk (Mahalunkar and Kelleher, 2019), synthetic variants of natural language (Ravfogel et al., 2019; Liu et al., 2018), and others (Mul and Zuidema, 2019; Liˇska et al., 2018; Korrel et al., 2019). Unlike these works, our synthetic task is not designed primarily to test the biases of the neural network or to improve its performance in a restricted setting, but to investigate the internal behavior of an LSTM in response to memorization. Investigations into learning dynamics like ours may offer insight into selecting training curricula. The application of a curriculum is based on the often unspoken assumption that the representation of a complex pattern can be reached more easily from a simpler pattern. However, we find that effectively representing shorter scaffolds actually makes a lan"
2020.findings-emnlp.252,W19-4800,0,0.214944,"Missing"
2020.findings-emnlp.252,E17-1117,0,0.0610955,"Missing"
2020.findings-emnlp.252,N19-1002,0,0.0688791,"ulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed n-gram data, implying that they exploit linguistic structure in long-distance dependencies (Liu et al., 2018). Their internal representations appear to encode constituency (Blevins et al., 2018; Hupkes and Zuidema, 2018) and syntactic agreement (Lakretz et al., 2019; Gulordava et al., 2018). In this paper, we consider how such representations are learned, and what kind of inductive bias supports them. To understand how LSTMs exploit syntax, we use contextual decomposition (CD; Section 2.1), a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence (DI; Section 2.2), a measure of interaction between spans of words to produce the representation at a particular timestep. For example, in the sentence “Socrates asked the student trick questions”, we might"
2020.findings-emnlp.252,W18-3024,0,0.0969896,"explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed n-gram data, implying that they exploit linguistic structure in long-distance dependencies (Liu et al., 2018). Their internal representations appear to encode constituency (Blevins et al., 2018; Hupkes and Zuidema, 2018) and syntactic agreement (Lakretz et al., 2019; Gulordava et al., 2018). In this paper, we consider how such representations are learned, and what kind of inductive bias supports them. To understand how LSTMs exploit syntax, we use contextual decomposition (CD; Section 2.1), a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence (DI; Section 2.2), a measure of interaction betwe"
2020.findings-emnlp.252,W19-3904,0,0.0146946,"ral networks. Saxe et al. (2019) explored how hierarchical ontologies are learned by following their tree structure in 2-layer 2804 feedforward networks. LSTMs also take advantage of some inherent trait of language (Liu et al., 2018) . The compositional training we have explored may be the mechanism behind this biased representational power. Synthetic data, meanwhile, has formed the basis for analyzing the inductive biases of neural networks and their capacity to learn compositional rules. Common synthetic datasets include the Dyck languages (Suzgun et al., 2019; Skachkova et al., 2018), SPk (Mahalunkar and Kelleher, 2019), synthetic variants of natural language (Ravfogel et al., 2019; Liu et al., 2018), and others (Mul and Zuidema, 2019; Liˇska et al., 2018; Korrel et al., 2019). Unlike these works, our synthetic task is not designed primarily to test the biases of the neural network or to improve its performance in a restricted setting, but to investigate the internal behavior of an LSTM in response to memorization. Investigations into learning dynamics like ours may offer insight into selecting training curricula. The application of a curriculum is based on the often unspoken assumption that the representati"
2020.findings-emnlp.252,N19-1356,0,0.102832,"ed for an LSTM to exhibit similarly compositional learning behavior by building longer constituents out of shorter ones during training, but we present evidence in favor of such learning dynamics. LSTMs have the theoretical capacity to encode a wide range of context-sensitive languages, but in practice their ability to learn such rules from data is limited (Weiss et al., 2018). Empirically, LSTMs encode the most recent noun as the subject of a verb by default, but they are still capable of learning to encode grammatical inflection from the first word in a sequence rather than the most recent (Ravfogel et al., 2019). Therefore, while inductive biases inherent to the model play a critical role in the ability of an LSTM to learn effectively, they are neither necessary nor sufficient in determining what the model can learn. Hierarchical linguistic structure may be learned from data alone, or be a natural product of the training process, with neither hypothesis a foregone conclusion. We provide a more precise lens on how LSTM training is itself compositional, beyond the properties of data. There is a limited literature on compositionality as an inductive bias of neural networks. Saxe et al. (2019) explored h"
2020.findings-emnlp.252,N19-1329,1,0.813864,"uence). 2. But in fact, the either/or rule is acquired faster with familiar constituents, as is clear even if the role of “either” is isolated (Figure 8). 3. The poor performance is instead connected to high interdependence between “either” and the intervening span (Figures 9 and 10). Synthetic Experiments Our next experiments use synthetic data to show how training is bottom-up. LSTM training sees long-range connections discovered after shortrange connections; in particular, document-level content topic information is encoded much later in training than local information like part of speech (Saphra and Lopez, 2019). These experiments explain such learning phases by showing that the training process is inherently compositional due to bottom-up learning.4 That is, not only are the shorter sequences learned first, but they form the basis for longer relations learned over them. For example, the model might learn to 4 Other phenomena contribute but are outside our current focus. First, long-range connections are less consistent (particularly in a right-branching language like English), and will thus take longer to learn (Appendix B. For example, the pattern of a determiner followed by a noun will appear very"
2020.findings-emnlp.252,W18-5425,0,0.0122741,"ty as an inductive bias of neural networks. Saxe et al. (2019) explored how hierarchical ontologies are learned by following their tree structure in 2-layer 2804 feedforward networks. LSTMs also take advantage of some inherent trait of language (Liu et al., 2018) . The compositional training we have explored may be the mechanism behind this biased representational power. Synthetic data, meanwhile, has formed the basis for analyzing the inductive biases of neural networks and their capacity to learn compositional rules. Common synthetic datasets include the Dyck languages (Suzgun et al., 2019; Skachkova et al., 2018), SPk (Mahalunkar and Kelleher, 2019), synthetic variants of natural language (Ravfogel et al., 2019; Liu et al., 2018), and others (Mul and Zuidema, 2019; Liˇska et al., 2018; Korrel et al., 2019). Unlike these works, our synthetic task is not designed primarily to test the biases of the neural network or to improve its performance in a restricted setting, but to investigate the internal behavior of an LSTM in response to memorization. Investigations into learning dynamics like ours may offer insight into selecting training curricula. The application of a curriculum is based on the often unsp"
2020.findings-emnlp.252,W19-3905,0,0.019321,"ure on compositionality as an inductive bias of neural networks. Saxe et al. (2019) explored how hierarchical ontologies are learned by following their tree structure in 2-layer 2804 feedforward networks. LSTMs also take advantage of some inherent trait of language (Liu et al., 2018) . The compositional training we have explored may be the mechanism behind this biased representational power. Synthetic data, meanwhile, has formed the basis for analyzing the inductive biases of neural networks and their capacity to learn compositional rules. Common synthetic datasets include the Dyck languages (Suzgun et al., 2019; Skachkova et al., 2018), SPk (Mahalunkar and Kelleher, 2019), synthetic variants of natural language (Ravfogel et al., 2019; Liu et al., 2018), and others (Mul and Zuidema, 2019; Liˇska et al., 2018; Korrel et al., 2019). Unlike these works, our synthetic task is not designed primarily to test the biases of the neural network or to improve its performance in a restricted setting, but to investigate the internal behavior of an LSTM in response to memorization. Investigations into learning dynamics like ours may offer insight into selecting training curricula. The application of a curriculum i"
2020.findings-emnlp.252,D18-1503,0,0.0397493,"Missing"
2020.findings-emnlp.252,P18-2117,0,0.0185523,"them (Lieven and Tomasello, 2008). LSTM models learn by backpropagation through time, which is unlikely to lead to the same inductive biases, the assumptions that define how the model generalizes from its training data. It may not be expected for an LSTM to exhibit similarly compositional learning behavior by building longer constituents out of shorter ones during training, but we present evidence in favor of such learning dynamics. LSTMs have the theoretical capacity to encode a wide range of context-sensitive languages, but in practice their ability to learn such rules from data is limited (Weiss et al., 2018). Empirically, LSTMs encode the most recent noun as the subject of a verb by default, but they are still capable of learning to encode grammatical inflection from the first word in a sequence rather than the most recent (Ravfogel et al., 2019). Therefore, while inductive biases inherent to the model play a critical role in the ability of an LSTM to learn effectively, they are neither necessary nor sufficient in determining what the model can learn. Hierarchical linguistic structure may be learned from data alone, or be a natural product of the training process, with neither hypothesis a forego"
2020.findings-emnlp.252,I17-2046,0,0.0137905,"igate the internal behavior of an LSTM in response to memorization. Investigations into learning dynamics like ours may offer insight into selecting training curricula. The application of a curriculum is based on the often unspoken assumption that the representation of a complex pattern can be reached more easily from a simpler pattern. However, we find that effectively representing shorter scaffolds actually makes a language model less effective at generalizing a long-range rule, as found by Zhang et al. (2018). This less generalizable representation is still learned faster, which may be why Zhang et al. (2017) found higher performance after one epoch. Our work suggests that measures of length, including syntactic depth, may be inappropriate bases for curriculum learning. 6 Future Work While we hope to isolate the role of long range dependencies through synthetic data, we must consider the possibility that the natural predictability of language data differs in relevant ways from the synthetic data, in which the scaffolds are predictable only through pure memorization. Because LSTM models take advantage of linguistic structure, we cannot be confident that predictable natural language exhibits the sam"
2020.findings-emnlp.252,silveira-etal-2014-gold,0,0.0502265,"Missing"
2021.acl-long.150,P19-1166,0,0.0187301,"guages (XWEAT; Lauscher and Glavas, 2019), and extended to operate on full sentences (May et al., 2019) and on contextual language models (Kurita et al., 2019). When WEAT is used as a metric, papers report the effect size of the subset of tests relevant to the task at hand, each separately. There are known issues with WEAT, such as sensitivity to corpus word frequency, and sensitivity 2 We count 34 papers from *CL and FAT* conferences since January 2020 that use WEAT or SEAT (May et al., 2019) in their methodology. 1927 to target and attribute wordlists, as found by Sedoc and Ungar (2019) and Ethayarajh et al. (2019). The latter proposes an alternative more theoretically robust metric, relational inner product association (RIPA), which uses the principal component of a gender subspace (determined via the method of Bolukbasi et al. (2016)) to directly measure how ”gendered” a word is. We have chosen to use the most common version of WEAT for this first empirical study, since it is most widely used. It would be interesting to test RIPA in the same way, if it were extended to more types of bias and more languages. But we note that all intrinsic metrics are sensitive to chosen wordlists, so this must be done"
2021.acl-long.150,W16-2506,0,0.0305759,"stics test whether the application performs differently on language related to different populations. Hence, research on debiasing embeddings relies crucially on a hypothesis that doing so will remove or reduce bias in downstream applications. However, we are aware of no prior research that confirms this hypothesis. This untested assumption leaves NLP bias research in a precarious position. Research into the semantics of word embeddings has already shown that intrinsic metrics (e.g. using analogies and semantic similarity, as in Hill et al., 2015) do not correlate well with extrinsic metrics (Faruqui et al., 2016). Research into the bias of word embeddings lacks the same type of systematic study, and thus as a field we are exposed to three large risks: 1) making misleading claims about the fairness of our systems, 2) concentrating our efforts on the wrong problem, and most importantly, 3) feeling a false sense of security that we are making more progress on the problem than we are. Our bias research can be rigorous and innovative, but unless we understand the limitations of metrics we use to evaluate it, it might have no impact. In this paper, we ask: Does the commonly used intrinsic metric for embeddi"
2021.acl-long.150,2020.findings-emnlp.301,0,0.0157932,"discuss this in more detail in Section 5). Despite recent widespread interest in contextual embeddings (e.g. BERT; Devlin et al., 2019), our experiments use these simpler contextless embed6 Wordlists used for bias-modification and configs for Attract-Repel are included in the codebase. 1929 dings because they are widely available in many toolkits and used in many real-world applications. Their design simplifies our experiments, whereas contextual embeddings would add significant complexity. However, we know that bias is still a problem for large contextual embeddings (Zhao et al., 2019, 2020; Gehman et al., 2020; Sheng et al., 2019), so our work remains important. If intrinsic and extrinsic measures do not correlate with simple embeddings, this result is unlikely to be changed by adding more architectural layers and configurable hyperparameters. 3.3 Downstream tasks We use three tasks that appear often in bias literature: Coreference resolution for English, hate speech detection for English, and hate speech detection for Spanish. To make the scenarios as realistic as possible, we use a common, easy-to-implement and high performing architecture for each task: the end-to-end coreference system of Lee e"
2021.acl-long.150,P19-1070,0,0.0191301,"pons.13 The second problem with XWEAT is that nouns on the wordlists for both abstract math and science concepts as well as abstract art concepts are almost entirely grammatically female. For instance, ciencia (science), geometr´ıa (geometry) are grammatically female, as are escultura (sculpture) and novela (novel). It is well established that for languages with grammatical gender, words that share a grammatical gender have embeddings that are closer together than words that do not (Gonen et al., 2019; McCurdy and Serbetci, 2017). So, when WEAT in English was translated into XWEAT in Spanish (Glavas et al., 2019), the terms were imbalanced with regard to grammatical gender, which makes the results misleading. We balance the lists, often replacing abstract nouns with corresponding adjectives which can take male or female form, e.g. cient´ıfico and cient´ıfica (scientific, male and female), such that we can use both versions to account for the effect of grammatical gender. Finally, we needed a metric to examine bias against migrants. Metrics for intrinsic bias must be targeted to the type of harm expected in the downstream application, and there is not an out-ofthe-box WEAT test for this. So we create a"
2021.acl-long.150,D17-1018,0,0.0261169,"2020; Sheng et al., 2019), so our work remains important. If intrinsic and extrinsic measures do not correlate with simple embeddings, this result is unlikely to be changed by adding more architectural layers and configurable hyperparameters. 3.3 Downstream tasks We use three tasks that appear often in bias literature: Coreference resolution for English, hate speech detection for English, and hate speech detection for Spanish. To make the scenarios as realistic as possible, we use a common, easy-to-implement and high performing architecture for each task: the end-to-end coreference system of Lee et al. (2017) and the the CNN of Kim (2014), which has been used in high-scoring systems in recent hate speech detection shared tasks (Basile et al., 2019). For each task, we feed pretrained embeddings to the model, frozen, and then train the model using the standard hyperparameters published for each model and task. 3.4 Languages We experiment on both English and Spanish. It is important to take a language with pervasive gender-marking (Spanish) into account, as previous work has shown that grammatical gender-marking has a strong effect on gender bias in embeddings (McCurdy and Serbetci, 2017; Gonen et al"
2021.acl-long.150,N19-1063,0,0.0255467,"1998) can be replicated via word embeddings measurements. There are thus 10 original tests chosen to replicate the tests presented to human subjects in IAT. The tests measure different kinds of biased associations, such as African-American names vs. White names with pleasant vs. unpleasant terms, and female terms vs. male terms with career vs. family words. WEAT was later repurposed as a predictor of bias in embedding spaces, via a somewhat muddy logical journey. It has since been translated into 6 other languages (XWEAT; Lauscher and Glavas, 2019), and extended to operate on full sentences (May et al., 2019) and on contextual language models (Kurita et al., 2019). When WEAT is used as a metric, papers report the effect size of the subset of tests relevant to the task at hand, each separately. There are known issues with WEAT, such as sensitivity to corpus word frequency, and sensitivity 2 We count 34 papers from *CL and FAT* conferences since January 2020 that use WEAT or SEAT (May et al., 2019) in their methodology. 1927 to target and attribute wordlists, as found by Sedoc and Ungar (2019) and Ethayarajh et al. (2019). The latter proposes an alternative more theoretically robust metric, relation"
2021.acl-long.150,Q17-1022,0,0.0688188,"Missing"
2021.acl-long.150,W19-3808,0,0.0175099,"translated into 6 other languages (XWEAT; Lauscher and Glavas, 2019), and extended to operate on full sentences (May et al., 2019) and on contextual language models (Kurita et al., 2019). When WEAT is used as a metric, papers report the effect size of the subset of tests relevant to the task at hand, each separately. There are known issues with WEAT, such as sensitivity to corpus word frequency, and sensitivity 2 We count 34 papers from *CL and FAT* conferences since January 2020 that use WEAT or SEAT (May et al., 2019) in their methodology. 1927 to target and attribute wordlists, as found by Sedoc and Ungar (2019) and Ethayarajh et al. (2019). The latter proposes an alternative more theoretically robust metric, relational inner product association (RIPA), which uses the principal component of a gender subspace (determined via the method of Bolukbasi et al. (2016)) to directly measure how ”gendered” a word is. We have chosen to use the most common version of WEAT for this first empirical study, since it is most widely used. It would be interesting to test RIPA in the same way, if it were extended to more types of bias and more languages. But we note that all intrinsic metrics are sensitive to chosen wor"
2021.acl-long.150,2020.emnlp-main.155,0,0.0222119,"f their work focuses on embeddings. While the latter may seem onerous, it may not be more so than exhaustively searching for a configuration where intrinsic bias metrics are predictive. This underscores the importance of making good downstream bias measures available, as either approach will require these. More datasets that are collected need to be annotated with subgroup demographic and identity information — there are very few available. More research needs to focus on creating good challenge sets to measure application bias. Additional research on more broad usage of unsupervised methods (Zhao and Chang, 2020) would also be valuable, though those also would benefit from subgroup identity annotation to make their results more interpretable. It is only when more of these things are readily available that we can see the true measure of the efficacy of our debiasing efforts. We do note a limitation of this study in that all downstream tasks are discriminative classification tasks. Bias in classification is more straightforward to measure, with well established metrics, but covers allocational harms (performance disparity), whereas the inclusion of generative models could better cover representational h"
2021.acl-long.150,N19-1064,0,0.02457,"Missing"
2021.acl-long.150,N18-2003,0,0.116589,"ed housekeeper’ vs. anti-stereotypical sentences, such as ‘He was a talented housekeeper’ or ‘She was a talented analyst’. We sub-sample and reduce the frequency of the pro-stereotypical collocations to debias, and sub-sample the anti-stereotypical conditions to overbias. As a postprocessing method for already trained embeddings, we use the Attract-Repel (Mrksic et al., 2017) algorithm. This algorithm was de4 There are additional embedding based debiasing methods used in practice, based on identifying and removing a gender subspace during training or as postprocessing (Bolukbasi et al., 2016; Zhao et al., 2018b). However, these methods do not change a word’s nearest neighbour clusters (Gonen and Goldberg, 2019), and so we would expect these debiasing methods to show superficial bias changes in WEAT without changing downstream bias. Both methods that we select modify the underlying word distribution and move many words in relation to each other. We verified this with tSNE visualisation as in Figure 1a following Gonen and Goldberg (2019) and find that our bias modification methods do change word clusters. 5 Stereotypes as defined by Zhao et al. (2018a) and by Caliskan et al. (2017), who use the U.S."
2021.acl-long.150,D18-1521,0,0.120536,"ed housekeeper’ vs. anti-stereotypical sentences, such as ‘He was a talented housekeeper’ or ‘She was a talented analyst’. We sub-sample and reduce the frequency of the pro-stereotypical collocations to debias, and sub-sample the anti-stereotypical conditions to overbias. As a postprocessing method for already trained embeddings, we use the Attract-Repel (Mrksic et al., 2017) algorithm. This algorithm was de4 There are additional embedding based debiasing methods used in practice, based on identifying and removing a gender subspace during training or as postprocessing (Bolukbasi et al., 2016; Zhao et al., 2018b). However, these methods do not change a word’s nearest neighbour clusters (Gonen and Goldberg, 2019), and so we would expect these debiasing methods to show superficial bias changes in WEAT without changing downstream bias. Both methods that we select modify the underlying word distribution and move many words in relation to each other. We verified this with tSNE visualisation as in Figure 1a following Gonen and Goldberg (2019) and find that our bias modification methods do change word clusters. 5 Stereotypes as defined by Zhao et al. (2018a) and by Caliskan et al. (2017), who use the U.S."
2021.acl-long.150,D19-1531,0,0.0174624,"the CNN of Kim (2014), which has been used in high-scoring systems in recent hate speech detection shared tasks (Basile et al., 2019). For each task, we feed pretrained embeddings to the model, frozen, and then train the model using the standard hyperparameters published for each model and task. 3.4 Languages We experiment on both English and Spanish. It is important to take a language with pervasive gender-marking (Spanish) into account, as previous work has shown that grammatical gender-marking has a strong effect on gender bias in embeddings (McCurdy and Serbetci, 2017; Gonen et al., 2019; Zhou et al., 2019). We use Spanish only for hate speech detection, because gender marking makes a challenge-set style coreference evaluation trivial to resolve and not a candidate for detection of gender bias.7 4 Datasets This fact is the premise behind the work of Stanovsky et al. (2019) who use the explicit marking in translation to reveal bias. WEAT & Bias modification wordlists Both WEAT and bias modification methods depend on seed wordlists.9 These wordlists are closely related to each other, and we match them by type of bias, such that we measure WEAT tests for gender bias with embeddings modified via gen"
2021.acl-long.150,D19-1339,0,0.0166136,"detail in Section 5). Despite recent widespread interest in contextual embeddings (e.g. BERT; Devlin et al., 2019), our experiments use these simpler contextless embed6 Wordlists used for bias-modification and configs for Attract-Repel are included in the codebase. 1929 dings because they are widely available in many toolkits and used in many real-world applications. Their design simplifies our experiments, whereas contextual embeddings would add significant complexity. However, we know that bias is still a problem for large contextual embeddings (Zhao et al., 2019, 2020; Gehman et al., 2020; Sheng et al., 2019), so our work remains important. If intrinsic and extrinsic measures do not correlate with simple embeddings, this result is unlikely to be changed by adding more architectural layers and configurable hyperparameters. 3.3 Downstream tasks We use three tasks that appear often in bias literature: Coreference resolution for English, hate speech detection for English, and hate speech detection for Spanish. To make the scenarios as realistic as possible, we use a common, easy-to-implement and high performing architecture for each task: the end-to-end coreference system of Lee et al. (2017) and the"
2021.acl-long.150,P19-1164,0,0.0141599,"ished for each model and task. 3.4 Languages We experiment on both English and Spanish. It is important to take a language with pervasive gender-marking (Spanish) into account, as previous work has shown that grammatical gender-marking has a strong effect on gender bias in embeddings (McCurdy and Serbetci, 2017; Gonen et al., 2019; Zhou et al., 2019). We use Spanish only for hate speech detection, because gender marking makes a challenge-set style coreference evaluation trivial to resolve and not a candidate for detection of gender bias.7 4 Datasets This fact is the premise behind the work of Stanovsky et al. (2019) who use the explicit marking in translation to reveal bias. WEAT & Bias modification wordlists Both WEAT and bias modification methods depend on seed wordlists.9 These wordlists are closely related to each other, and we match them by type of bias, such that we measure WEAT tests for gender bias with embeddings modified via gender bias wordlists (themselves derived from WEAT lists, as detailed below) and WEAT tests for migrant bias with embeddings modified for migrant bias. WEAT wordlists are standardised, and for English we use the three WEAT test wordlists (numbers 6,7,8) for gender.10 To ge"
2021.semeval-1.9,2021.semeval-1.154,0,0.0892195,"Missing"
2021.semeval-1.9,2021.semeval-1.153,0,0.0948692,"Missing"
2021.semeval-1.9,S17-2126,0,0.122134,"ogue-type jokes that are common on Twitter). Please see the appendix for a comprehensive list of accounts. Using the Twitter API, we crawled up to 2,000 tweets from each account, and removed retweets and texts containing links. We also removed tweets that contained references to US Politics, the pandemic, or TV show characters as topical humor can be difficult to understand once the event it is tied to has passed (Highfield, 2015). From an initial 76,542 texts, we were left with 8,000 tweets. From these, we removed hashtags that labelled the texts as humorous, e.g. #joke, and using Ekphrasis (Baziotis et al., 2017) we split up any remaining hashtags into their constituent words so as to make them less easy to differentiate from the Kaggle texts. 3.2 Annotation We recruited annotators from the Prolific2 platform. Participants were recruited based on their self-reported native English-speaker status, US citizenship, and membership of one of the following age groups: 18-25, 26-40, 41-55, 56-70. Each text was annotated by 5 members of each age group, giving a total of 20 annotations per text. Batches comprised 100 texts, and annotators answered the following questions: 1. Is the intention of this text to be"
2021.semeval-1.9,2020.emnlp-demos.2,0,0.0823706,"Missing"
2021.semeval-1.9,2021.semeval-1.35,0,0.095197,"Missing"
2021.semeval-1.9,S17-2004,0,0.0506411,"ers and 16,000 non-humorous texts, using a featuredriven approach. More recently, Zhang and Liu (2014) turned to online domains, by detecting humor on Twitter with a view to improving downstream tasks such as sentiment analysis and opinion 105 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 105–119 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Target mining. Workshops on humor detection have become more prominent with each shared task, and have attracted many new researchers to the field. SemEval 2017 (Potash et al., 2017) featured Hashtag Wars, a humor task with a unique data annotation procedure. This task featured tweets that had been submitted in response to a number of comedic hashtags released by a Comedy Central program. The top-10 response tweets were selected by the show’s producers and the winning tweet was selected by the show’s audience. Based on these labels, (top-10, winning tweet, and other) the sub-tasks required competitors to predict the labels, and to predict which text was funnier, given a pair tweets. The winning systems were split between feature-driven support vector machines (SVMs) and r"
2021.semeval-1.9,2021.semeval-1.173,0,0.0819124,"Missing"
2021.semeval-1.9,2021.semeval-1.161,0,0.0754419,"Missing"
2021.semeval-1.9,2021.semeval-1.163,0,0.0788196,"Missing"
2021.semeval-1.9,2021.semeval-1.158,0,0.04022,"The predictions were produced after Multi Sample Dropout was applied. This approach achieved third place in task 1a and first place in task 2. 6.2.2 Table 7: Results of the top performing systems for participants of task 1c (humor controversy), showing F1 and accuracy for the whole test set, and F1 for kaggle texts only and tweets only. Similarly, many teams experimented with single and multi-task learning setups, and multi-task models tended to be more successful across sub-tasks. Further improvements were achieved with domain adaptation strategies and adversarial training. 6.2.1 DeepBlueAI (Song et al., 2021) DeepBlueAI achieved high performance in subtasks 1a and 2. This team used stacked transformer models, which used the majority vote (in the case of classification) or the average prediction (for regression) from a RoBERTa and an ALBERT model. They optimized the performance of these PLMs with a number of techniques. First, they employed task-adaptive fine-tuning (Gururangan et al., 2020) by continuing pre-training on the text of the HaTeam DeepBlueAI mmmm HumorHunter abcbpc fdabek stevenhuahua megatron MagicPai ES-JUST SarcasmDet baseline (BERT) baseline (SVM) All 0.4120 0.4190 0.4230 0.4275 0."
2021.sigmorphon-1.9,Q17-1010,0,0.483342,"erivational morphology is an intriguing and contested issue within linguistics (e.g. Stump, 2005), and the question of how to model it computationally requires much more attention. 4.2 subword length of 2 characters, and used it to cluster words from the same cell rather than the same paradigm (e.g. clustering together English verbs in the third person singular such as “walks” and “jumps”). We attempted to follow this procedure, but it proved too difficult, as paradigm cell information was not explicitly included in the development data for this shared task. 3) We used the method described by Bojanowski et al. (2017) to identify important subwords within a word, in hopes of combining them with AG segmentations. However, the identified subwords did not consistently align with stem-adfix segementations as we had hoped, and did not seem to provide any additional benefit. Brown clustering Part of speech tags could provide latent structure as a higher-order grouping for paradigm clusters — for example, verbs would be expected to have paradigms more similar to other verbs than to nouns. Brown clusters (Brown et al., 1992) have been used for unsupervised induction of word classes approximating part of speech tag"
2021.sigmorphon-1.9,D13-1034,0,0.0478443,"Missing"
2021.sigmorphon-1.9,W13-2603,0,0.0730678,"Missing"
2021.sigmorphon-1.9,J92-4003,0,0.649613,"uded in the development data for this shared task. 3) We used the method described by Bojanowski et al. (2017) to identify important subwords within a word, in hopes of combining them with AG segmentations. However, the identified subwords did not consistently align with stem-adfix segementations as we had hoped, and did not seem to provide any additional benefit. Brown clustering Part of speech tags could provide latent structure as a higher-order grouping for paradigm clusters — for example, verbs would be expected to have paradigms more similar to other verbs than to nouns. Brown clusters (Brown et al., 1992) have been used for unsupervised induction of word classes approximating part of speech tags. We used a spectral clustering algorithm (Stratos et al., 2014) to learn Brown clusters, but they did not reliably correspond to part of speech categories on our development language data. Things that didn’t work 5 We attempted a number of unsupervised approaches beyond AG segmentations, with the goal of incorporating them during the clustering process; however, we could not consistently improve performance with any of them. It seems likely to us that these methods could still be used to improve AG-seg"
2021.sigmorphon-1.9,K18-3001,0,0.140003,"Missing"
2021.sigmorphon-1.9,2020.acl-main.598,0,0.0770183,"particular subtree σ is roughly proportional to the number of times X has previously expanded to σ. This leads to a “rich-get-richer” effect as more Introduction While the task of supervised morphological inflection has seen dramatic gains in accuracy over recent years (e.g. Cotterell et al., 2016, 2017, 2018; Vylomova et al., 2020), unsupervised morphological analysis remains an open challenge. This is evident in the results of the 2020 SIGMORPHON Shared Task 2 on Unsupervised Morphological Paradigm Completion, in which no submission consistently outperformed the baseline (Kann et al., 2020; Jin et al., 2020). The 2021 Shared Task 2 (Wiemerslage et al., 2021) focuses on a subproblem from the 2020 task: given raw text input, cluster tokens together based on membership in the same morphological paradigm. For example, given the sentence “My dog met some other dogs”, a successful system would assign “dog” and “dogs” to the same paradigm because they are two inflected forms of the same lemma “dog”, while each other word would occupy its own cluster. Furthermore, a successful system needs to cluster typologically diverse, morphologically rich languages such as Finnish and Navajo, with inflectional parad"
2021.sigmorphon-1.9,N09-1036,1,0.708419,"nd each grammar, resulting in 6 segmentations for each word. We then use frequency-based metrics frequently sampled subtrees gain higher probability over the segmentations to identify the language’s within the conditional adapted distribution. Given adfix direction, i.e. whether it is predominantly an AG specification, MCMC sampling can be used prefixing or suffixing, as described in Section 3.3. to infer values for the PCFG rule probabilities θ Finally, we iterate over the entire vocabulary and (Johnson et al., 2007a) and PYP hyperparameters apply frequency-based scores to generate paradigm (Johnson and Goldwater, 2009). clusters, as described in Section 3.4 . 2.2 Word walked jumping walking jump Segmentation walk-ed jump-ing walk-ing jump AGs for Morphological Analysis 3.2 The probabilistic parses generated by adaptor grammars can be used to segment sequences. In cases where the grammar specifies word structures, the segmentations may reflect morphological analyses. For example, an AG trained with the simple grammar shown in Table 1a may learn to cache “jump” and “walk” as Stem subtrees, and “ing” and “ed” as Suffix subtrees, ideally producing the target segmentations shown in Figure 1c. In practice, resear"
2021.sigmorphon-1.9,N07-1018,1,0.769271,"the related task of unsupervised morphological segmentation (Eskander et al., 2020). This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions. 1 Adaptor Grammars 2.1 Model Adaptor Grammars (AGs; Johnson et al., 2007b) are a class of nonparametric Bayesian probabilistic models which learn structured representations, or parses, of natural language input strings. An AG has two components: a Probabilistic Context-Free Grammar (PCFG) and one or more adaptors. The PCFG is a 5-tuple (N, W, R, S, θ) which specifies a base distribution over parse trees. Parse trees are generated top-down by expanding non-terminals N (including the start symbol S ∈ N ) to nonterminals N (excluding S) and terminals W , using the set of allowed expansion rules R with expansion probability θr for each rule r ∈ R. PCFGs have very stro"
2021.sigmorphon-1.9,2020.sigmorphon-1.3,0,0.152829,"of Cx returning a particular subtree σ is roughly proportional to the number of times X has previously expanded to σ. This leads to a “rich-get-richer” effect as more Introduction While the task of supervised morphological inflection has seen dramatic gains in accuracy over recent years (e.g. Cotterell et al., 2016, 2017, 2018; Vylomova et al., 2020), unsupervised morphological analysis remains an open challenge. This is evident in the results of the 2020 SIGMORPHON Shared Task 2 on Unsupervised Morphological Paradigm Completion, in which no submission consistently outperformed the baseline (Kann et al., 2020; Jin et al., 2020). The 2021 Shared Task 2 (Wiemerslage et al., 2021) focuses on a subproblem from the 2020 task: given raw text input, cluster tokens together based on membership in the same morphological paradigm. For example, given the sentence “My dog met some other dogs”, a successful system would assign “dog” and “dogs” to the same paradigm because they are two inflected forms of the same lemma “dog”, while each other word would occupy its own cluster. Furthermore, a successful system needs to cluster typologically diverse, morphologically rich languages such as Finnish and Navajo, with"
2021.sigmorphon-1.9,2020.lrec-1.879,0,0.32211,"l Analysis 3.2 The probabilistic parses generated by adaptor grammars can be used to segment sequences. In cases where the grammar specifies word structures, the segmentations may reflect morphological analyses. For example, an AG trained with the simple grammar shown in Table 1a may learn to cache “jump” and “walk” as Stem subtrees, and “ing” and “ed” as Suffix subtrees, ideally producing the target segmentations shown in Figure 1c. In practice, researchers have successfully applied AGs to the task of unsupervised morphological segmentation (Sirts and Goldwater, 2013; Eskander et al., 2016). Eskander et al. (2020) found that a languageindependent AG framework achieved state-of-theart results on 12 typologically distinct languages. 3 3.1 Grammar selection An adaptor grammar builds upon an initial PCFG specification, and many such grammars can be applied to model word structure. As a first step, we evaluate various grammar specifications on the development languages and select the grammars for our final model. To train the adaptor grammar representations, we use MorphAGram (Eskander et al., 2020), a framework which extends the adaptor grammar implementation of Johnson et al. (2007b). Eskander et al. (202"
2021.sigmorphon-1.9,Q13-1021,1,0.895391,"walk-ed jump-ing walk-ing jump AGs for Morphological Analysis 3.2 The probabilistic parses generated by adaptor grammars can be used to segment sequences. In cases where the grammar specifies word structures, the segmentations may reflect morphological analyses. For example, an AG trained with the simple grammar shown in Table 1a may learn to cache “jump” and “walk” as Stem subtrees, and “ing” and “ed” as Suffix subtrees, ideally producing the target segmentations shown in Figure 1c. In practice, researchers have successfully applied AGs to the task of unsupervised morphological segmentation (Sirts and Goldwater, 2013; Eskander et al., 2016). Eskander et al. (2020) found that a languageindependent AG framework achieved state-of-theart results on 12 typologically distinct languages. 3 3.1 Grammar selection An adaptor grammar builds upon an initial PCFG specification, and many such grammars can be applied to model word structure. As a first step, we evaluate various grammar specifications on the development languages and select the grammars for our final model. To train the adaptor grammar representations, we use MorphAGram (Eskander et al., 2020), a framework which extends the adaptor grammar implementation"
C08-1064,P06-1067,0,0.0180278,"to translate discontiguous phrases is important to modeling translation (Chiang, 2007; Simard et al., 2005; Quirk and Menezes, 2006), and it may be that this explains the results. However, there is another hypothesis. The model can also translate phrases in the form uX or Xu (a single contiguous unit and a gap). If it learns that uX often translates as Xu0 , then in addition to learning that u translates as u0 , it has also learned that u switches places with a neighboring phrase during translation. This is similar to lexicalized reordering in conventional phrase-based models (Tillman, 2004; Al-Onaizan and Papineni, 2006).6 If this is the real benefit of the hierarchical model, then the ability to translate discontiguous phrases may be irrelevant. To tease apart these claims, we make the following distinction. Rules in which both source and target phrases contain a single contiguous element— that is, in the form u, Xu, uX, or XuX— encode lexicalized reordering in hierarchical form. Rules representing the translation of discontiguous units—minimally uXv—encode translation knowledge that is strictly outside the purview of lexical reordering. We ran experiments varying both the number of contiguous subphrases and"
C08-1064,P06-1002,0,0.0547732,"Missing"
C08-1064,N06-1013,0,0.0271987,"Missing"
C08-1064,brown-2004-modified,0,0.0295567,"ve considerable resources at our disposal. To get around this problem, we use an algorithmic scaling technique that we call translation by pattern matching. In this approach, the training text and its word alignment reside in memory. We then translate as follows. for each input sentence do for each possible phrase in the sentence do Find its occurrences in the source text for each occurrence do Extract its aligned target phrase (if any) for each extracted phrase pair do Score using maximum likelihood Decode as usual using the scored rules A similar method is used in example-based translation (Brown, 2004). It was applied to phrase-based translation by Callison-Burch et al. (2005) and Zhang and Vogel (2005). The key point is that the complete translation model is never actually computed—rules and associated parameters are computed only as needed. Obviously, the ondemand computation must be very fast. If we can achieve this, then the model can in principle be arbitrarily large. Callison-Burch et al. (2005) and Zhang and Vogel (2005) give very similar recipes for application to phrase-based models. Fast lookup using pattern matching algorithms. The complexity of the na¨ıve algorithm to find all o"
C08-1064,P05-1032,0,0.755456,"s problem, we use an algorithmic scaling technique that we call translation by pattern matching. In this approach, the training text and its word alignment reside in memory. We then translate as follows. for each input sentence do for each possible phrase in the sentence do Find its occurrences in the source text for each occurrence do Extract its aligned target phrase (if any) for each extracted phrase pair do Score using maximum likelihood Decode as usual using the scored rules A similar method is used in example-based translation (Brown, 2004). It was applied to phrase-based translation by Callison-Burch et al. (2005) and Zhang and Vogel (2005). The key point is that the complete translation model is never actually computed—rules and associated parameters are computed only as needed. Obviously, the ondemand computation must be very fast. If we can achieve this, then the model can in principle be arbitrarily large. Callison-Burch et al. (2005) and Zhang and Vogel (2005) give very similar recipes for application to phrase-based models. Fast lookup using pattern matching algorithms. The complexity of the na¨ıve algorithm to find all occurrences of a source phrase in a training text T is linear in the length o"
C08-1064,D07-1007,0,0.00961866,"hing on a representation that is close in size to the information-theoretic minimum required by the data. Our approach is currently limited by the requirement for very fast parameter estimation. As we saw, this appears to prevent us from computing the target-to-source probabilities. It would also appear to limit our ability to use discriminative training methods, since these tend to be much slower than the analytical maximum likelihood estimate. Discriminative methods are desirable for feature-rich models that we would like to explore with pattern matching. For example, Chan et al. (2007) and Carpuat and Wu (2007) improve translation accuracy using discriminatively trained models with contextual features of source phrases. Their features are easy to obtain at runtime using our approach, which finds source phrases in context. However, to make their experiments tractable, they trained their discriminative models offline only for the specific phrases of the test set. Combining discriminative learning with our approach is an open problem. 6 Conclusion We showed that very large translation models present an interesting engineering challenge, and illustrated a solution to this challenge using pattern matchin"
C08-1064,P07-1005,0,0.0061906,"ports fast pattern matching on a representation that is close in size to the information-theoretic minimum required by the data. Our approach is currently limited by the requirement for very fast parameter estimation. As we saw, this appears to prevent us from computing the target-to-source probabilities. It would also appear to limit our ability to use discriminative training methods, since these tend to be much slower than the analytical maximum likelihood estimate. Discriminative methods are desirable for feature-rich models that we would like to explore with pattern matching. For example, Chan et al. (2007) and Carpuat and Wu (2007) improve translation accuracy using discriminatively trained models with contextual features of source phrases. Their features are easy to obtain at runtime using our approach, which finds source phrases in context. However, to make their experiments tractable, they trained their discriminative models offline only for the specific phrases of the test set. Combining discriminative learning with our approach is an open problem. 6 Conclusion We showed that very large translation models present an interesting engineering challenge, and illustrated a solution to this chall"
C08-1064,J07-2003,0,0.288377,"variants and show an improvement 1.4 BLEU on the NIST 2006 ChineseEnglish task. This opens the door for work on a variety of models that are much less constrained by computational limitations. 1 Introduction Translation model size is growing quickly due to the use of larger training corpora and more complex models. As an example of the growth in available training data, consider the curated Europarl corpus (Koehn, 2005), which more than doubled in size from 20 to 44 million words between 2003 and 2007.1 As an example of model complexity, consider the popular hierarchical phrase-based model of Chiang (2007), which can translate discontiguous phrases. Under the loosest interpretation of this capability, any subset of words in a sentence This research was conducted while I was at the University of Maryland. I thank David Chiang, Bonnie Dorr, Doug Oard, Philip Resnik, and the anonymous reviewers for comments, and especially Chris Dyer for many helpful discussions and for running the Moses experiments. This research was supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001 and by the EuroMatrix project funded by the European Commission (6th Fram"
C08-1064,D07-1079,0,0.0100735,"me). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Statistics from http://www.statmt.org/europarl. can be a phrase. Therefore, the number of rules that the model can learn is exponential in sentence length unless strict heuristics are used, which may limit the model’s effectiveness. Many other models translate discontiguous phrases, and the size of their extracted rulesets is such a pervasive problem that it is a recurring topic in the literature (Chiang, 2007; DeNeefe et al., 2007; Simard et al., 2005). Most decoder implementations assume that all model rules and parameters are known in advance. With very large models, computing all rules and parameters can be very slow. This is a bottleneck in experimental settings where we wish to explore many model variants, and therefore presents a real impediment to full exploration of their potential. We present a solution to this problem. To fully motivate the discussion, we give a concrete example of a very large model, which we generate using simple techniques that are known to improve translation accuracy. The model takes 77"
C08-1064,W08-0333,0,0.0114948,"modifications increase this to 34.5, a substantial improvement of 2.6 BLEU. 5 Related Work and Open Problems There are several other useful approaches to scaling translation models. Zens and Ney (2007) remove constraints imposed by the size of main memory by using an external data structure. Johnson et al. (2007) substantially reduce model size with a filtering method. However, neither of these approaches addresses the preprocessing bottleneck. To our knowledge, the strand of research initiated by Callison-Burch et al. (2005) and Zhang and Vogel (2005) and extended here is the first to do so. Dyer et al. (2008) address this bottleneck with a promising approach based on parallel processing, showing reductions in real time that are linear in the number of CPUs. However, they do not reduce the overall CPU time. Our techniques also benefit from parallel processing, but they reduce overall CPU time, thus comparing favorably even in this scenario.8 Moreover, our method works even with limited parallel processing. Although we saw success with this approach, there are some interesting open problems. As discussed in §4.2, there are tradeoffs in the form of slower decoding and increased memory usage. Decoding"
C08-1064,D07-1103,0,0.00694105,"e objective of this experiment is to ensure that our improvements are complementary to better language modeling, which often subsumes other improvements. The new baseline achieves a score of 31.9 on the NIST 2005 set, making it nearly the same as the state-of-the-art results reported by Chiang (2007). Our modifications increase this to 34.5, a substantial improvement of 2.6 BLEU. 5 Related Work and Open Problems There are several other useful approaches to scaling translation models. Zens and Ney (2007) remove constraints imposed by the size of main memory by using an external data structure. Johnson et al. (2007) substantially reduce model size with a filtering method. However, neither of these approaches addresses the preprocessing bottleneck. To our knowledge, the strand of research initiated by Callison-Burch et al. (2005) and Zhang and Vogel (2005) and extended here is the first to do so. Dyer et al. (2008) address this bottleneck with a promising approach based on parallel processing, showing reductions in real time that are linear in the number of CPUs. However, they do not reduce the overall CPU time. Our techniques also benefit from parallel processing, but they reduce overall CPU time, thus c"
C08-1064,N03-1017,0,0.363995,"ental results along a variety of scaling axes (§4). Our results extend previous findings on the use of long phrases in translation, shed light on the source of improved performance in hierarchical phrasebased models, and show that our tera-scale translation model outperforms a strong baseline. 2 A Tera-Scale Translation Model We will focus on the hierarchical phrase-based model of Chiang (2007). It compares favorably 505 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 505–512 Manchester, August 2008 with conventional phrase-based translation (Koehn et al., 2003) on Chinese-English news translation (Chiang, 2007). We found that a baseline system trained on 27 million words of news data is already quite strong, but we suspect that it would be possible to improve it using some simple techniques. Add additional training data. Our baseline already uses much of the available curated news data, but there is at least three times as much curated data available in the United Nations proceedings. Adding the UN data gives us a training corpus of 107 million words per language. Change the word alignments. Our baseline uses Giza++ alignments (Och and Ney, 2003) sy"
C08-1064,P07-2045,0,0.0123457,"is, in the form u, Xu, uX, or XuX— encode lexicalized reordering in hierarchical form. Rules representing the translation of discontiguous units—minimally uXv—encode translation knowledge that is strictly outside the purview of lexical reordering. We ran experiments varying both the number of contiguous subphrases and the number of gaps (Ta6 This hypothesis was suggested independently in personal communications with several researchers, including Chris Callison-Burch, Chris Dyer, Alex Fraser, and Franz Och. 509 ble 4). For comparison, we also include results of the phrase-based system Moses (Koehn et al., 2007) with and without lexicalized reordering. Our results are consistent with those found elsewhere in the literature. The strictest setting allowing no gaps replicates a result in Chiang (2007, Table 7), with significantly worse accuracy than all others. The most striking result is that the accuracy of Moses with lexicalized reordering is indistinguishable from the accuracy of the full hierarchical system. Both improve over non-lexicalized Moses by about 1.4 BLEU. The hierarchical emulation owes its performance only partially to lexicalized reordering. Additional improvement is seen when we add d"
C08-1064,D07-1104,1,0.456583,"Missing"
C08-1064,P02-1038,0,0.128912,"Missing"
C08-1064,J03-1002,0,0.0246291,"(Koehn et al., 2003) on Chinese-English news translation (Chiang, 2007). We found that a baseline system trained on 27 million words of news data is already quite strong, but we suspect that it would be possible to improve it using some simple techniques. Add additional training data. Our baseline already uses much of the available curated news data, but there is at least three times as much curated data available in the United Nations proceedings. Adding the UN data gives us a training corpus of 107 million words per language. Change the word alignments. Our baseline uses Giza++ alignments (Och and Ney, 2003) symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2003). We replace these with the maximum entropy aligments of Ayan and Dorr (2006b). They reported improvements of 1.6 BLEU in Chinese-English translation, though with much less training data. Change the bilingual phrase extraction heuristic. Our baseline uses a tight heuristic, requiring aligned words at phrase edges. However, Ayan and Dorr (2006a) showed that a loose heuristic, allowing unaligned words at the phrase edges, improved accuracy by 3.7 BLEU with some alignments, again with much less training data. Quadrupling the"
C08-1064,P03-1021,0,0.236664,"cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/ 507 state of the art with very large models.3 In the remainder of this work, we scratch the surface of possible uses. We experimented on Chinese-English newswire translation. Except where noted, each system was trained on 27 million words of newswire data, aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2003). In all experiments that follow, each system configuration was independently optimized on the NIST 2003 Chinese-English test set (919 sentences) using minimum error rate training (Och, 2003) and tested on the NIST 2005 Chinese-English task (1082 sentences). Optimization and measurement were done with the NIST implementation of case-insensitive BLEU 4n4r (Papineni et al., 2002).4 4.1 1. Sum of logarithms of source-to-target phrase translation probabilities. 2. Sum of logarithms of source-to-target lexical weighting (Koehn et al., 2003). 3. Sum of logarithms of target-to-source lexical weighting. 4. Sum of logarithms of a trigram language model. Baseline 5. A word count feature. We compared translation by pattern matching with a conventional exact model representation using externa"
C08-1064,P02-1040,0,0.110356,"ted on Chinese-English newswire translation. Except where noted, each system was trained on 27 million words of newswire data, aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2003). In all experiments that follow, each system configuration was independently optimized on the NIST 2003 Chinese-English test set (919 sentences) using minimum error rate training (Och, 2003) and tested on the NIST 2005 Chinese-English task (1082 sentences). Optimization and measurement were done with the NIST implementation of case-insensitive BLEU 4n4r (Papineni et al., 2002).4 4.1 1. Sum of logarithms of source-to-target phrase translation probabilities. 2. Sum of logarithms of source-to-target lexical weighting (Koehn et al., 2003). 3. Sum of logarithms of target-to-source lexical weighting. 4. Sum of logarithms of a trigram language model. Baseline 5. A word count feature. We compared translation by pattern matching with a conventional exact model representation using external prefix trees (Zens and Ney, 2007). To make model computation efficient for the latter case, we followed the heuristic limits on phrase extraction used by Chiang (2007). • Phrases were res"
C08-1064,N06-1002,0,0.00989721,"model for each condition. With translation by pattern matching, nearly every variant uses the same underlying representation, so it was rarely even necessary to recompute data structures and indexes. 9 11 13 15 Figure 2: Effect of the maximum phrase span. to gain insight into the hierarchical model. Hierarchical phrase-based translation is often reported to be better than conventional phrasebased translation, but the actual reason for this is unknown. It is often argued that the ability to translate discontiguous phrases is important to modeling translation (Chiang, 2007; Simard et al., 2005; Quirk and Menezes, 2006), and it may be that this explains the results. However, there is another hypothesis. The model can also translate phrases in the form uX or Xu (a single contiguous unit and a gap). If it learns that uX often translates as Xu0 , then in addition to learning that u translates as u0 , it has also learned that u switches places with a neighboring phrase during translation. This is similar to lexicalized reordering in conventional phrase-based models (Tillman, 2004; Al-Onaizan and Papineni, 2006).6 If this is the real benefit of the hierarchical model, then the ability to translate discontiguous p"
C08-1064,N04-4026,0,0.0238909,"hat the ability to translate discontiguous phrases is important to modeling translation (Chiang, 2007; Simard et al., 2005; Quirk and Menezes, 2006), and it may be that this explains the results. However, there is another hypothesis. The model can also translate phrases in the form uX or Xu (a single contiguous unit and a gap). If it learns that uX often translates as Xu0 , then in addition to learning that u translates as u0 , it has also learned that u switches places with a neighboring phrase during translation. This is similar to lexicalized reordering in conventional phrase-based models (Tillman, 2004; Al-Onaizan and Papineni, 2006).6 If this is the real benefit of the hierarchical model, then the ability to translate discontiguous phrases may be irrelevant. To tease apart these claims, we make the following distinction. Rules in which both source and target phrases contain a single contiguous element— that is, in the form u, Xu, uX, or XuX— encode lexicalized reordering in hierarchical form. Rules representing the translation of discontiguous units—minimally uXv—encode translation knowledge that is strictly outside the purview of lexical reordering. We ran experiments varying both the num"
C08-1064,N07-1062,0,0.158384,"rn matching algorithms. The complexity of the na¨ıve algorithm to find all occurrences of a source phrase in a training text T is linear in the length of the text, O(|T |). This is much too slow for large texts. They solve this using an index data structure called a suffix 506 Rules extracted (millions) Extract time (CPU hours) Unique rules (millions) Extract file size (GB) Model size (GB) Baseline 195 10.8 67 9.3 6.1 Large 19,300 1,840 6,600* 917* 604* Table 1: Extraction time and model sizes. The model size reported is the size of the files containing an external prefix tree representation (Zens and Ney, 2007). *Denotes estimated quantities. Citation Simard et al. (2005) Chiang (2007) DeNeefe et al. (2007) Zens and Ney (2007) this paper Millions of rules (filtered) 4 (filtered) 6 57 225 6,600 Table 2: Model sizes in the literature. array (Manber and Myers, 1993). Its size is 4|T | bytes and it enables lookup of any length-m substring of T in O(m + log |T |) time. Fast extraction using sampling. The complexity of extracting target phrases is linear in the number of source phrase occurrences. For very frequent source phrases, this is expensive. They solve this problem by extracting only from a sample"
C08-1064,2005.eamt-1.39,0,0.672537,"scaling technique that we call translation by pattern matching. In this approach, the training text and its word alignment reside in memory. We then translate as follows. for each input sentence do for each possible phrase in the sentence do Find its occurrences in the source text for each occurrence do Extract its aligned target phrase (if any) for each extracted phrase pair do Score using maximum likelihood Decode as usual using the scored rules A similar method is used in example-based translation (Brown, 2004). It was applied to phrase-based translation by Callison-Burch et al. (2005) and Zhang and Vogel (2005). The key point is that the complete translation model is never actually computed—rules and associated parameters are computed only as needed. Obviously, the ondemand computation must be very fast. If we can achieve this, then the model can in principle be arbitrarily large. Callison-Burch et al. (2005) and Zhang and Vogel (2005) give very similar recipes for application to phrase-based models. Fast lookup using pattern matching algorithms. The complexity of the na¨ıve algorithm to find all occurrences of a source phrase in a training text T is linear in the length of the text, O(|T |). This i"
C08-1064,H05-1095,0,\N,Missing
C08-1064,2005.mtsummit-papers.11,0,\N,Missing
D07-1104,P05-1032,0,0.735027,"Computer Studies University of Maryland College Park, MD 20742 USA alopez@cs.umd.edu Abstract Until recently, most approaches to this problem involved substantial tradeoffs. The common practice of test set filtering renders systems impractical for all but batch processing. Tight restrictions on phrase length curtail the power of phrase-based models. However, some promising engineering solutions are emerging. Zens and Ney (2007) use a disk-based prefix tree, enabling efficient access to phrase tables much too large to fit in main memory. An alternative approach introduced independently by both Callison-Burch et al. (2005) and Zhang and Vogel (2005) is to store the training data itself in memory, and use a suffix array as an efficient index to look up, extract, and score phrase pairs on the fly. We believe that the latter approach has several important applications (§7). A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical"
D07-1104,H05-1098,1,0.814907,"es, resulting in full inter-sentence caching. The results are shown in Table 1. 11 It is clear from the results that each of the optimizations is needed to sufficiently reduce lookup time to practical levels. Although this is still relatively slow, it is much closer to the decoding time of 10 seconds per sentence than the baseline. Experiments All of our experiments were performed on ChineseEnglish in the news domain. We used a large training set consisting of over 1 million sentences from various newswire corpora. This corpus is roughly the same as the one used for large-scale experiments by Chiang et al. (2005). To generate alignments, we used GIZA++ (Och and Ney, 2003). We symmetrized bidirectional alignments using the growdiag-final heuristic (Koehn et al., 2003). 983 10 Python is an interpreted language and our implementations do not use any optimization features. It is therefore reasonable to think that a more efficient reimplementation would result in across-the-board speedups. 11 The results shown here do not include the startup time required to load the data structures into memory. In our Python implementation this takes several minutes, which in principle should be amortized over the cost fo"
D07-1104,J07-2003,0,0.708578,"lel corpora containing tens or hundreds of millions of words. This can result in millions of rules using even the most conservative extraction heuristics. Efficient algorithms for rule storage and access are necessary for practical decoding algorithms. They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora. So far, these techniques have focused on phrasebased models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004). Some recent models permit discontiguous phrases (Chiang, 2007; Quirk et al., 2005; Simard et al., 2005). Of particular interest to us is the hierarchical phrase-based model of Chiang (2007), which has been shown to be superior to phrase-based models. The ruleset extracted by this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger. This makes efficient rule representation even more critical. We tackle the problem using the online rule extraction method of Callison-Burch et al. (2005) and Zhang and Vogel (2005). The problem statement for our work is: Given an input sentence, efficiently find all"
D07-1104,W06-3113,0,0.0216563,"t the coindexes. For instance, the source side of the above rule will be written uXvXw. 3 For the purposes of this paper, we adhere to the restrictions described by Chiang (2007) for rules extracted from the training data. • Rules can contain at most two nonterminals. • Rules can contain at most five terminals. • Rules can span at most ten words. 2 A sample size of 100 is actually quite small for many phrases, some of which occur tens or hundreds of thousands of times. It is perhaps surprising that such a small sample size works as well as the full data. However, recent work by Och (2005) and Federico and Bertoldi (2006) has shown that the statistics used by phrase-based systems are not very precise. 3 In the canonical representation of the grammar, source-side coindexes are always in sorted order, making them unambiguous. • Nonterminals must span at least two words. • Adjacent nonterminals are disallowed in the source side of a rule. Expressed more economically, we say that our goal is to search for source phrases in the form u, uXv, or uXvXw, where 1 ≤ |uvw |≤ 5, and |v |&gt; 0 in the final case. Note that the model also allows rules in the form Xu, uX, XuX, XuXv, and uXvX. However, these rules are lexically i"
D07-1104,P01-1044,0,0.0144292,"attern, we create a node for it in the tree. If the pattern was found in the corpus, its node is marked active. Otherwise, it is marked inactive. For found patterns, we store either the endpoints of the suffix array range containing the phrase (if it is contiguous), or the list of locations at which the phrase is found (if it is discontiguous). We can also store the extracted rules. 9 Whenever a pattern is successfully found, we add all patterns with m + 1 terminals that are prefixed by it 9 Conveniently, the implementation of Chiang (2007) uses a prefix tree grammar encoding, as described in Klein and Manning (2001). Our implementation decorates this tree with additional information required by our algorithms. 982 to the frontier for processing in the next iteration. To search for a pattern, we use location information from its parent node, which represents its maximal prefix. Assuming that the node represents phrase αb, we find the node representing its maximal suffix by following the b-edge from the node pointed to by its parent node’s suffix link. If the node pointed to by this suffix link is inactive, we can mark the node inactive without running a search. When a node is marked inactive, we discontin"
D07-1104,N03-1017,0,0.0622584,"ems rely on very large rule sets. In phrase-based systems, rules are extracted from parallel corpora containing tens or hundreds of millions of words. This can result in millions of rules using even the most conservative extraction heuristics. Efficient algorithms for rule storage and access are necessary for practical decoding algorithms. They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora. So far, these techniques have focused on phrasebased models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004). Some recent models permit discontiguous phrases (Chiang, 2007; Quirk et al., 2005; Simard et al., 2005). Of particular interest to us is the hierarchical phrase-based model of Chiang (2007), which has been shown to be superior to phrase-based models. The ruleset extracted by this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger. This makes efficient rule representation even more critical. We tackle the problem using the online rule extraction method of Callison-Burch et al. (2005) and Zhang and Vogel (2005). Th"
D07-1104,P01-1050,0,0.0197694,"tein database. In this scenario, the goal is to select training sentences that match the input sentence as closely as possible, under some evaluation function that accounts for both matching and mismatched sequences, as well as possibly other data features. Once we have found the closest sentences we can translate the matched portions in their entirety, replacing mismatches with appropriate word, phrase, or hierarchical phrase translations as needed. This model would bring statistical machine translation closer to convergence with so-called example-based translation, following current trends (Marcu, 2001; Och, 2002). We intend to explore these ideas in future work. Acknowledgements I would like to thank Philip Resnik for encouragement, thoughtful discussions and wise counsel; David Chiang for providing the source code for his translation system; and Nitin Madnani, Smaranda Muresan and the anonymous reviewers for very helpful comments on earlier drafts of this paper. Any errors are my own. This research was supported in part by ONR MURI Contract FCPO.810548265 and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-2-001. Any opinions, findings, conclusions"
D07-1104,J03-1002,0,0.00292452,"shown in Table 1. 11 It is clear from the results that each of the optimizations is needed to sufficiently reduce lookup time to practical levels. Although this is still relatively slow, it is much closer to the decoding time of 10 seconds per sentence than the baseline. Experiments All of our experiments were performed on ChineseEnglish in the news domain. We used a large training set consisting of over 1 million sentences from various newswire corpora. This corpus is roughly the same as the one used for large-scale experiments by Chiang et al. (2005). To generate alignments, we used GIZA++ (Och and Ney, 2003). We symmetrized bidirectional alignments using the growdiag-final heuristic (Koehn et al., 2003). 983 10 Python is an interpreted language and our implementations do not use any optimization features. It is therefore reasonable to think that a more efficient reimplementation would result in across-the-board speedups. 11 The results shown here do not include the startup time required to load the data structures into memory. In our Python implementation this takes several minutes, which in principle should be amortized over the cost for each sentence. However, just as Zens and Ney (2007) do for"
D07-1104,J04-4002,0,0.00678366,"ge rule sets. In phrase-based systems, rules are extracted from parallel corpora containing tens or hundreds of millions of words. This can result in millions of rules using even the most conservative extraction heuristics. Efficient algorithms for rule storage and access are necessary for practical decoding algorithms. They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora. So far, these techniques have focused on phrasebased models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004). Some recent models permit discontiguous phrases (Chiang, 2007; Quirk et al., 2005; Simard et al., 2005). Of particular interest to us is the hierarchical phrase-based model of Chiang (2007), which has been shown to be superior to phrase-based models. The ruleset extracted by this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger. This makes efficient rule representation even more critical. We tackle the problem using the online rule extraction method of Callison-Burch et al. (2005) and Zhang and Vogel (2005). The problem statement"
D07-1104,P05-2016,0,0.0408312,"e will leave out the coindexes. For instance, the source side of the above rule will be written uXvXw. 3 For the purposes of this paper, we adhere to the restrictions described by Chiang (2007) for rules extracted from the training data. • Rules can contain at most two nonterminals. • Rules can contain at most five terminals. • Rules can span at most ten words. 2 A sample size of 100 is actually quite small for many phrases, some of which occur tens or hundreds of thousands of times. It is perhaps surprising that such a small sample size works as well as the full data. However, recent work by Och (2005) and Federico and Bertoldi (2006) has shown that the statistics used by phrase-based systems are not very precise. 3 In the canonical representation of the grammar, source-side coindexes are always in sorted order, making them unambiguous. • Nonterminals must span at least two words. • Adjacent nonterminals are disallowed in the source side of a rule. Expressed more economically, we say that our goal is to search for source phrases in the form u, uXv, or uXvXw, where 1 ≤ |uvw |≤ 5, and |v |&gt; 0 in the final case. Note that the model also allows rules in the form Xu, uX, XuX, XuXv, and uXvX. How"
D07-1104,P05-1034,0,0.0712186,"ntaining tens or hundreds of millions of words. This can result in millions of rules using even the most conservative extraction heuristics. Efficient algorithms for rule storage and access are necessary for practical decoding algorithms. They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora. So far, these techniques have focused on phrasebased models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004). Some recent models permit discontiguous phrases (Chiang, 2007; Quirk et al., 2005; Simard et al., 2005). Of particular interest to us is the hierarchical phrase-based model of Chiang (2007), which has been shown to be superior to phrase-based models. The ruleset extracted by this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger. This makes efficient rule representation even more critical. We tackle the problem using the online rule extraction method of Callison-Burch et al. (2005) and Zhang and Vogel (2005). The problem statement for our work is: Given an input sentence, efficiently find all hierarchical phrase-"
D07-1104,2006.amta-papers.11,1,0.786771,"lications. Both Callison-Burch et al. (2005) and Zhang and Vogel (2005) use suffix arrays to relax the length constraints on phrase-based models. Our work enables this in hierarchical phrase-based models. However, we are interested in additional applications. Recent work in discriminative learning for many natural language tasks, such as part-of-speech tagging and information extraction, has shown that feature engineering plays a critical role in these approaches. However, in machine translation most features can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b). Recently, Lopez and Resnik (2006) showed that most of the features used in standard phrase-based models do not help very much. Our algorithms enable us to look up phrase pairs in context, which will allow us to compute interesting contextual features that can be used in discriminative learning algorithms to improve translation accuracy. Essentially, we can use the training data itself as an indirect representation of whatever features we might want to compute. This is not possible with table-based architectures. Most of the data structures and algorithms discussed in this paper are widely used in bioinformatics, including suf"
D07-1104,N07-1062,0,0.0418745,"GIZA++ (Och and Ney, 2003). We symmetrized bidirectional alignments using the growdiag-final heuristic (Koehn et al., 2003). 983 10 Python is an interpreted language and our implementations do not use any optimization features. It is therefore reasonable to think that a more efficient reimplementation would result in across-the-board speedups. 11 The results shown here do not include the startup time required to load the data structures into memory. In our Python implementation this takes several minutes, which in principle should be amortized over the cost for each sentence. However, just as Zens and Ney (2007) do for phrase tables, we could compile our data structures into binary memory-mapped files, which can be read into memory in a matter of seconds. We are currently investigating this option in a C reimplementation. here Algorithms Baseline Prefix Tree Prefix Tree + precomputation Prefix Tree + double binary Prefix Tree + precomputation + double binary Prefix Tree with full caching + precomputation + double binary Secs/Sent 2241.25 1578.77 696.35 405.02 40.77 30.70 Collocations 325548 69994 69994 69994 69994 67712 Table 1: Timing results and number of collocations computed for various combinati"
D07-1104,D07-1104,1,0.107313,"Double binary search takes this idea a step further. It performs a binary search in D for the median element of Q. Whether or not the element is found, the 5 These can be identified using a single traversal over a longest common prefix (LCP) array, an auxiliary data structure of the suffix array, described by Manber and Myers (1993). Since we don’t need the LCP array at runtime, we chose to do this computation once offline. 6 Minor modifications are required since we are computing collocation rather than intersection. Due to space constraints, details and proof of correctness are available in Lopez (2007a). 980 search divides both sets into two pairs of smaller sets that can be processed recursively. Detailed analysis and empirical results on an information retrieval task are reported in Baeza-Yates (2004) and Baeza-Yates and Salinger (2005). If |Q |log |D |&lt; |D |then the performance is guaranteed to be sublinear. In practice it is often sublinear even if |Q |log |D |is somewhat larger than |D|. In our implementation we simply check for the condition λ|Q |log |D |&lt; |D |to decide whether we should use double binary search or the merge algorithm. This check is applied in the recursive cases as"
D07-1104,2005.eamt-1.39,0,0.337455,"ryland College Park, MD 20742 USA alopez@cs.umd.edu Abstract Until recently, most approaches to this problem involved substantial tradeoffs. The common practice of test set filtering renders systems impractical for all but batch processing. Tight restrictions on phrase length curtail the power of phrase-based models. However, some promising engineering solutions are emerging. Zens and Ney (2007) use a disk-based prefix tree, enabling efficient access to phrase tables much too large to fit in main memory. An alternative approach introduced independently by both Callison-Burch et al. (2005) and Zhang and Vogel (2005) is to store the training data itself in memory, and use a suffix array as an efficient index to look up, extract, and score phrase pairs on the fly. We believe that the latter approach has several important applications (§7). A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation in"
D07-1104,2007.mtsummit-tutorials.1,0,0.0842408,"Double binary search takes this idea a step further. It performs a binary search in D for the median element of Q. Whether or not the element is found, the 5 These can be identified using a single traversal over a longest common prefix (LCP) array, an auxiliary data structure of the suffix array, described by Manber and Myers (1993). Since we don’t need the LCP array at runtime, we chose to do this computation once offline. 6 Minor modifications are required since we are computing collocation rather than intersection. Due to space constraints, details and proof of correctness are available in Lopez (2007a). 980 search divides both sets into two pairs of smaller sets that can be processed recursively. Detailed analysis and empirical results on an information retrieval task are reported in Baeza-Yates (2004) and Baeza-Yates and Salinger (2005). If |Q |log |D |&lt; |D |then the performance is guaranteed to be sublinear. In practice it is often sublinear even if |Q |log |D |is somewhat larger than |D|. In our implementation we simply check for the condition λ|Q |log |D |&lt; |D |to decide whether we should use double binary search or the merge algorithm. This check is applied in the recursive cases as"
D07-1104,H05-1095,0,\N,Missing
D11-1031,P11-1048,1,0.312904,"Missing"
D11-1031,J99-2004,0,0.0767254,"point when both the head and the dependent are in the same span, violating the assumption used to compute c+ (see again Figure 2). Exceptions like this can cause mismatches between n+ and c+ . We set c+ = n+ whenever c+ &lt; n+ to account for these occasional discrepancies. Finally, we obtain a decomposable approximation to F-measure. DecF 1(y) = DecP (y) + DecR(y) 4 (10) Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most s"
D11-1031,C04-1041,0,0.346788,"ly, we obtain a decomposable approximation to F-measure. DecF 1(y) = DecP (y) + DecR(y) 4 (10) Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most sentences can be parsed with very tight beams. Reverse adaptive supertagging is a much less aggressive method that seeks only to make sentences parsable when they otherwise would not be due to an impractically large search space. Reverse AST starts with a wide beam, narrowing"
D11-1031,J07-4004,0,0.881739,"used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective using F-measure as a loss (§3). We experiment with this and several other metrics, including precision, recall, and decomposable approximations thereof. Our ability to optimise towards exact metrics enables us to verify the effectiveness of more efficient approximations. We test the training procedures on the state-of-the-art Combinatory Categorial Grammar (CCG; Steedman 2000) parser of Clark and Curran (2007), obtaining substantial improvements under a variety of conditions. We then embed this model into a more accurate model that incorporates additional supertagging features via loopy belief propagation. The improvements are additive, obtaining the best reported results on this task (§4). 2 Softmax-Margin Training The softmax-margin objective modifies the standard likelihood objective for CRF training by reweighting 333 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333–343, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computatio"
D11-1031,P02-1042,0,0.182477,"tic head of coordinations. The coordination rule (Φ) does not yet establish the dependency “and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;). For further examples and a more detailed explanation of the mechanism as used in the C&C parser refer to Clark et al. (2002). available within the current local structure, similar to those used by Taskar et al. (2004) for tracking constituent errors in a context-free parser. We design three simple losses to approximate precision, recall and F-measure on CCG dependency structures. Let T (y) be the set of parsing actions required to build parse y. Our decomposable approximation to precision simply counts the number of incorrect dependencies using the local dependency counts, n+ (·) and d+ (·). X DecP (y) = d+ (t) − n+ (t) (8) t∈T (y) To compute our approximation to recall we require the number of gold dependencies, c"
D11-1031,W02-2203,0,0.429695,"nd the dependent are in the same span, violating the assumption used to compute c+ (see again Figure 2). Exceptions like this can cause mismatches between n+ and c+ . We set c+ = n+ whenever c+ &lt; n+ to account for these occasional discrepancies. Finally, we obtain a decomposable approximation to F-measure. DecF 1(y) = DecP (y) + DecR(y) 4 (10) Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most sentences can b"
D11-1031,D09-1011,0,0.0319889,"Missing"
D11-1031,P08-1109,0,0.0891358,"Missing"
D11-1031,P10-1035,0,0.230107,"-accuracy (SA) as loss in the supertagger. LF 85.53 85.79 86.45 86.73 86.51 CLL Petrov I-5 BP +DecF1 +SA LP 85.73 86.09 86.75 87.07 86.86 section 00 (dev) LR UF 85.33 91.99 85.50 92.44 86.17 92.60 86.39 92.79 86.16 92.60 UP 92.20 92.76 92.92 93.16 92.98 UR 91.77 92.13 92.29 92.43 92.23 LF 85.74 86.01 86.84 87.08 87.20 LP 85.90 86.29 87.08 87.37 87.50 section 23 (test) LR UF 85.58 91.92 85.73 92.34 86.61 92.57 86.78 92.68 86.90 92.76 UP 92.09 92.64 92.82 93.00 93.08 UR 91.75 92.04 92.32 92.37 92.44 Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn (2010); evaluation is based on sentences for which all parsers returned an analysis. anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); and the resources provided by the Edinburgh Compute and Data Facility. A Computing F-Measure-Augmented Expectations at the Corpus Level To compute exact corpus-level expectations for softmaxmargin using F-measure, we add an additional transition before reaching the GOAL item in our original program. To reach it, we must parse every sentence in the corpus, associating statistics of aggregate hn, di pairs for th"
D11-1031,N10-1112,0,0.152716,"sults for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective usin"
D11-1031,P96-1024,0,0.693397,"Missing"
D11-1031,J07-3004,0,0.236323,"Curran (2007), which contains features over both normalform derivations and CCG dependencies. The parser relies solely on the supertagger for pruning, using exact CKY for search over the pruned space. Training requires calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 2) than in previous work (Clark and Curran, 2007). Models were trained with the parser’s L-BFGS trainer. Evaluation. We evaluated on CCGbank (Hockenmaier and Steedman, 2007), a right-most normalform CCG version of the Penn Treebank. We use sections 02-21 (39603 sentences) for training, section 00 (1913 sentences) for development and section 23 (2407 sentences) for testing. We supply gold-standard part-of-speech tags to the parsers. We evaluate on labelled and unlabelled predicate argument structure recovery and supertag accuracy. 4.1 Training with Maximum F-measure Parses So far we discussed how to optimise towards taskspecific metrics via changing the training objective. In our first experiment we change the data on which we optimise CLL. This is a kind of simpl"
D11-1031,P08-1067,0,0.00904733,"Importantly, both n and d decompose over parses. The key idea will be to treat F1 as a non-local feature of the parse, dependent on values n and d.2 To compute expectations we split each span in an otherwise usual inside-outside computation by all pairs hn, di incident at that span. Formally, our goal will be to compute expectations over the sentence a1 ...aL . In order to abstract away from the particulars of CCG we present the algorithm in relatively familiar terms as a variant of 1 2 For numerator and denominator. This is essentially the same trick used in the oracle F-measure algorithm of Huang (2008), and indeed our algorithm is a sumproduct variant of that max-product algorithm. min θ min θ ∂ = ∂λk m X i=1  m X i=1 m X i=1   −θT f (x(i) , y (i) ) + log −hk (x(i) , y (i) ) + y∈Y(x(i) ) y∈Y(x(i) ) X −θT f (x(i) , y (i) ) + log X X y∈Y(x(i) ) exp{θT f (x(i) , y)} y 0 ∈Y(x(i) ) (2)  exp{θT f (x(i) , y) + `(y (i) , y)} exp{θT f (x(i) , y) P  + `(y (i) , y)} exp{θT f (x(i) , y 0 ) + `(y (i) , y 0 )} (3)  hk (x(i) , y) (4) Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4). the classic inside-outside algorithm (Baker, 1979). We use th"
D11-1031,P03-1021,0,0.0374292,"Missing"
D11-1031,D08-1016,0,0.0419993,"Missing"
D11-1031,W04-3201,0,0.0359019,"and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;). For further examples and a more detailed explanation of the mechanism as used in the C&C parser refer to Clark et al. (2002). available within the current local structure, similar to those used by Taskar et al. (2004) for tracking constituent errors in a context-free parser. We design three simple losses to approximate precision, recall and F-measure on CCG dependency structures. Let T (y) be the set of parsing actions required to build parse y. Our decomposable approximation to precision simply counts the number of incorrect dependencies using the local dependency counts, n+ (·) and d+ (·). X DecP (y) = d+ (t) − n+ (t) (8) t∈T (y) To compute our approximation to recall we require the number of gold dependencies, c+ (·), which should have been introduced by a particular parsing action. A gold dependency is"
D11-1031,P08-1000,0,\N,Missing
D18-1278,W13-4907,0,0.0699755,"Missing"
D18-1278,D15-1041,0,0.177645,"rphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that characterlevel models can beneﬁt from targeted forms of explicit morphological modeling. 1 Introduction Modeling language input at the character level (Ling et al., 2015; Kim et al., 2016) is effective for many NLP tasks, and often produces better results than modeling at the word level. For parsing, Ballesteros et al. (2015) have shown that character-level input modeling is highly effective on morphologically-rich languages, and the three best systems on the 45 languages of the CoNLL 2017 shared task on universal dependency parsing all use character-level models (Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017; Zeman et al., 2017), showing that they are effective across many typologies. The effectiveness of character-level models in morphologically-rich languages has raised a question and indeed debate about explicit modeling of morphology in NLP. Ling et al. (2015) propose that “prior information"
D18-1278,P17-1080,0,0.148727,"counter that it is “unnecessary to consider these prior information” when modeling characters. Whether we need to explicitly model morphology is a question whose answer has a real cost: as Ballesteros et al. (2015) note, morphological annotation is expensive, and this expense could be reinvested elsewhere if the predictive aspects of morphology are learnable from strings. Do character-level models learn morphology? We view this as an empirical claim requiring empirical evidence. The claim has been tested implicitly by comparing character-level models to word lookup models (Qian et al., 2016; Belinkov et al., 2017). In this paper, we test it explicitly, asking how character-level models compare with an oracle model with access to morphological annotations. This extends experiments showing that character-aware language models in Czech and Russian beneﬁt substantially from oracle morphology (Vania and Lopez, 2017), but here we focus on dependency parsing (§2)—a task that beneﬁts substantially from morphological knowledge—and we experiment with twelve languages using a variety of techniques to probe our models. Our summary ﬁnding is that character-level models lag the oracle in nearly all languages (§3). T"
D18-1278,P16-1160,0,0.0945372,"Missing"
D18-1278,E17-2053,0,0.0357263,"Missing"
D18-1278,K17-3002,0,0.105162,"our best model, showing that characterlevel models can beneﬁt from targeted forms of explicit morphological modeling. 1 Introduction Modeling language input at the character level (Ling et al., 2015; Kim et al., 2016) is effective for many NLP tasks, and often produces better results than modeling at the word level. For parsing, Ballesteros et al. (2015) have shown that character-level input modeling is highly effective on morphologically-rich languages, and the three best systems on the 45 languages of the CoNLL 2017 shared task on universal dependency parsing all use character-level models (Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017; Zeman et al., 2017), showing that they are effective across many typologies. The effectiveness of character-level models in morphologically-rich languages has raised a question and indeed debate about explicit modeling of morphology in NLP. Ling et al. (2015) propose that “prior information regarding morphology ... among others, should be incorporated” into character-level models, while Chung et al. ∗ Work done while at the University of Edinburgh. (2016) counter that it is “unnecessary to consider these prior information” when modeling characters."
D18-1278,Q16-1023,0,0.0400054,"that the oracle relies on morphological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another predictive model (§5). Finally, we show that the crucial morphological features vary by language (§6). 2573 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2573–2583 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Dependency parsing model We use a neural graph-based dependency parser combining elements of two recent models (Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Let w = w1 , . . . , w|w |be an input sentence of length |w |and let w0 denote an artiﬁcial ROOT token. We represent the ith input token wi by concatenating its word representation (§2.3), e(wi ) and part-of-speech (POS) representation, pi .1 Using a semicolon (; ) to denote vector concatenation, we have: xi = [e(wi ); pi ] (1) We call xi the embedding of wi since it depends on context-independent word and POS representations. We obtain a context-sensitive encoding hi with a bidirectional LSTM (bi-LSTM), which concatenates the hidden states of a forward and backward LSTM"
D18-1278,E17-1117,0,0.0575918,"Missing"
D18-1278,D15-1176,0,0.151343,"test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that characterlevel models can beneﬁt from targeted forms of explicit morphological modeling. 1 Introduction Modeling language input at the character level (Ling et al., 2015; Kim et al., 2016) is effective for many NLP tasks, and often produces better results than modeling at the word level. For parsing, Ballesteros et al. (2015) have shown that character-level input modeling is highly effective on morphologically-rich languages, and the three best systems on the 45 languages of the CoNLL 2017 shared task on universal dependency parsing all use character-level models (Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017; Zeman et al., 2017), showing that they are effective across many typologies. The effectiveness of character-level models in morphologi"
D18-1278,P16-1140,0,0.103087,"Missing"
D18-1278,J13-1004,0,0.167583,"e hypothesized that these cases are more challenging for the character model because these languages feature a high degree of syncretism—functionally distinct words that have the same form—and in particular case syncretism. For example, referring back to examples (1) and (2), the character model must disambiguate pis mo from its context, whereas the oracle can directly disambiguate it from a feature of the word itself.5 To understand this, we ﬁrst designed an experiment to see whether the char-lstm could success5 We are far from ﬁrst to observe that morphological case is important to parsing: Seeker and Kuhn (2013) observe the same for non-neural parsers. fully disambiguate noun case, using a method similar to (Belinkov et al., 2017). We train a neural classiﬁer that takes as input a word representation from the trained parser and predicts a morphological feature of that word—for example that its case is nominative (Case=Nom). The classiﬁer is a feedforward neural network with one hidden layer, followed by a ReLU non-linearity. We consider two representations of each word: its embedding (xi ; Eq. 1) and its encoding (hi ; Eq. 2). To understand the importance of case, we consider it alongside number and"
D18-1278,K17-3003,0,0.01847,"ing that characterlevel models can beneﬁt from targeted forms of explicit morphological modeling. 1 Introduction Modeling language input at the character level (Ling et al., 2015; Kim et al., 2016) is effective for many NLP tasks, and often produces better results than modeling at the word level. For parsing, Ballesteros et al. (2015) have shown that character-level input modeling is highly effective on morphologically-rich languages, and the three best systems on the 45 languages of the CoNLL 2017 shared task on universal dependency parsing all use character-level models (Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017; Zeman et al., 2017), showing that they are effective across many typologies. The effectiveness of character-level models in morphologically-rich languages has raised a question and indeed debate about explicit modeling of morphology in NLP. Ling et al. (2015) propose that “prior information regarding morphology ... among others, should be incorporated” into character-level models, while Chung et al. ∗ Work done while at the University of Edinburgh. (2016) counter that it is “unnecessary to consider these prior information” when modeling characters. Whether we need t"
D18-1278,P16-2038,0,0.0773444,"Missing"
D18-1278,W10-1401,0,0.160716,"Missing"
D18-1278,P17-1184,1,0.875961,"here if the predictive aspects of morphology are learnable from strings. Do character-level models learn morphology? We view this as an empirical claim requiring empirical evidence. The claim has been tested implicitly by comparing character-level models to word lookup models (Qian et al., 2016; Belinkov et al., 2017). In this paper, we test it explicitly, asking how character-level models compare with an oracle model with access to morphological annotations. This extends experiments showing that character-aware language models in Czech and Russian beneﬁt substantially from oracle morphology (Vania and Lopez, 2017), but here we focus on dependency parsing (§2)—a task that beneﬁts substantially from morphological knowledge—and we experiment with twelve languages using a variety of techniques to probe our models. Our summary ﬁnding is that character-level models lag the oracle in nearly all languages (§3). The difference is small, but suggests that there is value in modeling morphology. When we tease apart the results by part of speech and dependency type, we trace the difference back to the characterlevel model’s inability to disambiguate words even when encoded with arbitrary context (§4). Specifically,"
D18-1278,E17-1063,0,0.0173128,"ological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another predictive model (§5). Finally, we show that the crucial morphological features vary by language (§6). 2573 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2573–2583 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Dependency parsing model We use a neural graph-based dependency parser combining elements of two recent models (Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Let w = w1 , . . . , w|w |be an input sentence of length |w |and let w0 denote an artiﬁcial ROOT token. We represent the ith input token wi by concatenating its word representation (§2.3), e(wi ) and part-of-speech (POS) representation, pi .1 Using a semicolon (; ) to denote vector concatenation, we have: xi = [e(wi ); pi ] (1) We call xi the embedding of wi since it depends on context-independent word and POS representations. We obtain a context-sensitive encoding hi with a bidirectional LSTM (bi-LSTM), which concatenates the hidden states of a forward and backward LSTM at position i. Using"
D19-1102,W17-0401,0,0.157942,"Missing"
D19-1102,K17-2002,0,0.0571064,"Missing"
D19-1102,Q17-1010,0,0.0186991,") for Galician and Uppsala (Smith et al., 2018) for Kazakh. rank shows our best model position in the shared task ranking for each treebank. a strong baseline, in a case when we have access to pre-trained word embeddings, for the source and/or the target languages. We treat a pre-trained word embedding as an external embedding, and concatenate it with the other representations, i.e., modifying Eq. 3 to xi = [ew (wi ); ep (wi ); ec (wi ); lk ], where ep (wi ) represents a pre-trained word embedding of wi , which we update during training. We use the pre-trained monolingual fastText embeddings (Bojanowski et al., 2017).9 We concatenate the source and target pre-trained word embeddings.10 For our experiments with transliteration (§2.4), we transliterate the entries of both the source and the target pre-trained word embeddings. To see how our best approach (i.e., cross-lingual model with MORPH augmentation) compares with the current state-of-the-art models, we compare it to the recent results from CoNLL 2018 shared task. Training state-of-the-art models may require lots of engineering and data resources. Our goal, however, is not to achieve the best performance, but rather to systematically investigate how fa"
D19-1102,P17-2090,0,0.0294277,"ng substantial research (Tiedemann and Agic, 2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, results on the task’s nine low-resource treebanks “are extremely low and the outputs are hardly useful for Each of these scenarios requires different approaches. Data augmentation is applicable in all scenarios, and has proven useful for low-resource NLP in general (Fadaee et al., 2017; Bergmanis et al., 2017; Sahin and Steedman, 2018). Transfer learning via cross-lingual training is applicable in scenarios 2 and 3. Finally, transliteration may be useful in scenario 3. To keep our scenarios as realistic as possible, we assume that no taggers are available since this would entail substantial annotation. Therefore, our neural parsing models must learn to parse from words or characters—that is, they must be lexicalized—even though there may be little shared vocabulary between source and target treebanks. While this may intuitively seem to make crosslingual training difficult,"
D19-1102,W17-6303,0,0.143051,"Missing"
D19-1102,N18-1108,0,0.0200389,"nstituent order (rotation), which may benefit languages with flexible word order and rich morphology. Some of our low-resource languages have these properties—while North Sámi has a fixed word order (SVO), Galician and Kazakh have relatively free word order. All three languages use case marking on nouns, so word order may not be as important for correct attachment. Both rotation and cropping can produce many trees. We use the default parameters given in (Sahin and Steedman, 2018). 1106 2.2 Data augmentation by nonce sentence generation (Nonce) Our next data augmentation method is adapted from Gulordava et al. (2018). The main idea is to create nonce sentences by replacing some of the words which have the same syntactic annotations. For each training sentence, we replace each content word—nouns, verbs, or adjective— with an alternative word having the same universal POS, morphological features, and dependency label.2 Specifically, for each content word, we first stochastically choose whether to replace it; then, if we have chosen to replace it, we uniformly sample the replacement word type meeting the corresponding constraints. For instance, given a sentence “He borrowed a book from the library.”, we can"
D19-1102,P18-1031,0,0.0209436,"iteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. For example, we showed that although North Sami and Finnish do not share vocabulary, cross-lingual training is still helpful because they share similar syntactic structures. Different language pairs might benefit from other types of similarity (e.g., morphological) and investigating this would be another i"
D19-1102,D11-1006,0,0.328528,"Missing"
D19-1102,P09-1040,0,0.0248166,"facilitating different pronunciations.4 We map these to their basic Latin counterparts, e.g., ‘c¸’ to ‘c’. For Kazakh, we use a simple dictionary created by a Kazakh computational linguist to map each Cyrillic letter to the basic Latin alphabet.5 . Experimental Setup Dependency Parsing Model We use the Uppsala parser, a transition-based neural dependency parser (de Lhoneux et al., 2017a,b; Kiperwasser and Goldberg, 2016). The parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since it can produce representations for any character sequence. We then obtain a context-sensitive encoding hi using a"
D19-1102,Q16-1023,0,0.0419216,"g from Turkish is straightforward. Its alphabet consists of 29 letters, 23 of which are in basic Latin. The other six letters, ‘c¸’,‘˘g’, ‘ı’, ‘¨o’, ‘s¸’, and ‘¨u’, add diacritics to basic Latin characters, facilitating different pronunciations.4 We map these to their basic Latin counterparts, e.g., ‘c¸’ to ‘c’. For Kazakh, we use a simple dictionary created by a Kazakh computational linguist to map each Cyrillic letter to the basic Latin alphabet.5 . Experimental Setup Dependency Parsing Model We use the Uppsala parser, a transition-based neural dependency parser (de Lhoneux et al., 2017a,b; Kiperwasser and Goldberg, 2016). The parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of w"
D19-1102,P11-1068,0,0.0331551,"h are in basic Latin. The other six letters, ‘c¸’,‘˘g’, ‘ı’, ‘¨o’, ‘s¸’, and ‘¨u’, add diacritics to basic Latin characters, facilitating different pronunciations.4 We map these to their basic Latin counterparts, e.g., ‘c¸’ to ‘c’. For Kazakh, we use a simple dictionary created by a Kazakh computational linguist to map each Cyrillic letter to the basic Latin alphabet.5 . Experimental Setup Dependency Parsing Model We use the Uppsala parser, a transition-based neural dependency parser (de Lhoneux et al., 2017a,b; Kiperwasser and Goldberg, 2016). The parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since"
D19-1102,D18-1543,1,0.871399,"Missing"
D19-1102,K17-3022,0,0.078654,"Missing"
D19-1102,W17-6314,0,0.0552242,"Missing"
D19-1102,L18-1352,0,0.0225797,"t in the extremely low-resource setting, data augmentation improves parsing performance both in monolingual and cross-lingual settings. We also show that transfer learning is possible with lexicalized parsers. In addition, we show that transfer learning between two languages with different writing systems is possible, and future work should consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of under"
D19-1102,D15-1176,0,0.0155681,"he parser uses an arc-hybrid transition system (Kuhlmann et al., 2011), extended with a static-dynamic oracle and SWAP transition to allow non-projective dependency trees (Nivre, 2009). Let w = w0 , . . . , w|w |be an input sentence of length |w |and let w0 represent an artificial ROOT token. We create a vector representation for each input token wi by concatenating (; ) its word embedding, ew (wi ) and its character-based word embedding, ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since it can produce representations for any character sequence. We then obtain a context-sensitive encoding hi using a word-level biLSTM encoder: hi = [LSTMf (x0:i ); LSTMb (x|w|:i )] (2) We then create a configuration by concatenating the encoding of a fixed number of words on the top of the stack and the beginning of the buffer. Given 3 Another possible pivot is phonemes (Tsvetkov et al., 2016). We leave this as future work. 4 https://www.omniglot.com/writing/turkish.htm 5 The mapping from Kazakh Cyrilic into basic Latin alphabet is provided in"
D19-1102,N18-1202,0,0.0147329,"hould consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. For example, we showed that although North Sami and Finnish do not share vocabulary, cross-lingual training is still helpful because they share similar syntactic structures. Different language pairs might benefit from other types of similarity (e.g., morphological) and investigating"
D19-1102,Q17-1020,0,0.0416122,"embeddings. 6 Conclusions In this paper, we investigated various low-resource parsing scenarios. We demonstrate that in the extremely low-resource setting, data augmentation improves parsing performance both in monolingual and cross-lingual settings. We also show that transfer learning is possible with lexicalized parsers. In addition, we show that transfer learning between two languages with different writing systems is possible, and future work should consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this p"
D19-1102,K18-2019,0,0.0491895,"Missing"
D19-1102,D18-1545,0,0.35838,"2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, results on the task’s nine low-resource treebanks “are extremely low and the outputs are hardly useful for Each of these scenarios requires different approaches. Data augmentation is applicable in all scenarios, and has proven useful for low-resource NLP in general (Fadaee et al., 2017; Bergmanis et al., 2017; Sahin and Steedman, 2018). Transfer learning via cross-lingual training is applicable in scenarios 2 and 3. Finally, transliteration may be useful in scenario 3. To keep our scenarios as realistic as possible, we assume that no taggers are available since this would entail substantial annotation. Therefore, our neural parsing models must learn to parse from words or characters—that is, they must be lexicalized—even though there may be little shared vocabulary between source and target treebanks. While this may intuitively seem to make crosslingual training difficult, recent results have shown that lexical parameter sh"
D19-1102,D16-1159,0,0.054827,"between the two languages. Let cj be an embedding of character cj in a token wi from the treebank of language k, and let lk be the language embedding. For sharing on characters, we concatenate character and language embedding: [cj ; lk ] for input to the character-level biLSTM. Similarly, for input to the word-level biLSTM, we concatenate the language embedding to the word embedding, modifying Eq. 1 to xi = [ew (wi ); ec (wi ); lk ] (3) We use the default hyperparameters of de Lhoneux et al. (2018) in our experiments. We fine-tune each model by training it further only on the target treebank (Shi et al., 2016). We use early stopping based on Label Attachment Score (LAS) on development set. 3.3 Datasets We use Universal Dependencies (UD) treebanks version 2.2 (Nivre et al., 2018). None of our target treebanks have a development set, so we generate new train/dev splits by 50:50 (Table 1). Having large development sets allow us to perform better analysis for this study. 4 +Morph +Nonce 1128 564 141 7636 3838 854 4934 2700 661 Table 2: Number of North Sámi training sentences. Table 1: Train/dev split used for each treebank. 3.2 original Parsing North Sámi North Sámi is our largest low-resource treebank"
D19-1102,K18-2011,0,0.0414506,"Missing"
D19-1102,P11-2120,1,0.837952,"ifferent writing system, transliteration into a shared orthographic spaces is also very helpful. 1 1. What can we do with only a very small target treebank for a low-resource language? 2. What can we do if we also have a source treebank for a related high-resource language? 3. What if the source and target treebanks do not share a writing system? Introduction Large annotated treebanks are available for only a tiny fraction of the world’s languages, and there is a wealth of literature on strategies for parsing with few resources (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). A popular approach is to train a parser on a related high-resource language and adapt it to the low-resource language. This approach benefits from the availability of Universal Dependencies (UD; Nivre et al., 2016), prompting substantial research (Tiedemann and Agic, 2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, results on the task’s nine low-resource tr"
D19-1102,K17-3009,0,0.0168418,"transliterate the entries of both the source and the target pre-trained word embeddings. To see how our best approach (i.e., cross-lingual model with MORPH augmentation) compares with the current state-of-the-art models, we compare it to the recent results from CoNLL 2018 shared task. Training state-of-the-art models may require lots of engineering and data resources. Our goal, however, is not to achieve the best performance, but rather to systematically investigate how far simple approaches can take us. We report performance of the following: (1) the shared task baseline model (UDPipe v1.2; Straka and Straková, 2017) and (2) the best system for each treebank, (3) our best approach, and (4) a cross-lingual model with fastText embeddings. Table 8 presents the overall comparison on the test sets. For each treebank, we apply the same sentence segmentation and tokenization used by each best system.11 We see that our approach outperforms the baseline models on both languages. For Kazakh, our model (with transliteration) achieves a competitive LAS (28.23), which would be the second position in the shared task ranking. As comparison, the best system for Kazakh (Smith et al., 2018) trained a multitreebank model wi"
D19-1102,P18-2098,0,0.0525577,"Missing"
D19-1102,N16-1161,0,0.0181971,"ec (wi ): xi = [ew (wi ); ec (wi )] (1) Here, ec (wi ) is the output of a character-level bidirectional LSTM (biLSTM) encoder run over the characters of wi (Ling et al., 2015); this makes the model fully open-vocabulary, since it can produce representations for any character sequence. We then obtain a context-sensitive encoding hi using a word-level biLSTM encoder: hi = [LSTMf (x0:i ); LSTMb (x|w|:i )] (2) We then create a configuration by concatenating the encoding of a fixed number of words on the top of the stack and the beginning of the buffer. Given 3 Another possible pivot is phonemes (Tsvetkov et al., 2016). We leave this as future work. 4 https://www.omniglot.com/writing/turkish.htm 5 The mapping from Kazakh Cyrilic into basic Latin alphabet is provided in Appendix B 1107 Language Treebank ID train dev. test Finnish North Sámi fi_tdt sme_giella 14981 1128 1875 1129 1555 865 Portuguese Galician pt_bosque gl_treegal 8329 300 560 300 477 400 Turkish Kazakh tr_imst kk_ktb 3685 15 975 16 975 1047 T100 T50 T10 this configuration, we predict a transition and its arc label using a multi-layer perceptron (MLP). More details of the core parser can be found in de Lhoneux et al. (2017a,b). Parameter sharin"
D19-1102,K17-3001,0,0.0490395,"Missing"
D19-1102,I08-3008,0,0.117116,"nd (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful. 1 1. What can we do with only a very small target treebank for a low-resource language? 2. What can we do if we also have a source treebank for a related high-resource language? 3. What if the source and target treebanks do not share a writing system? Introduction Large annotated treebanks are available for only a tiny fraction of the world’s languages, and there is a wealth of literature on strategies for parsing with few resources (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). A popular approach is to train a parser on a related high-resource language and adapt it to the low-resource language. This approach benefits from the availability of Universal Dependencies (UD; Nivre et al., 2016), prompting substantial research (Tiedemann and Agic, 2016; Agi´c, 2017; Rosa and Mareˇcek, 2018), along with the VarDial and the CoNLL UD shared tasks (Zampieri et al., 2017; Zeman et al., 2017, 2018). But low-resource parsing is still difficult. The organizers of the CoNLL 2018 UD shared task (Zeman et al., 2018) report that, in general, res"
D19-1102,D18-1163,0,0.0645882,"different writing systems is possible, and future work should consider transliteration for other language pairs. While we have not exhausted all the possible techniques (e.g., use of external resources (Rasooli and Collins, 2017; Rosa and Mareˇcek, 2018), predicted POS (Ammar et al., 2016), multiple source treebanks (Lim et al., 2018; Stymne et al., 2018), among others), we show that simple methods which leverage the linguistic annotations in the treebank can improve low-resource parsing. Future work might explore different augmentation methods, such as the use of synthetic source treebanks (Wang and Eisner, 2018) or contextualized language model (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018) for scoring the augmented data (e.g., using perplexity). Finally, while the techniques presented in this paper might be applicable to other low-resource languages, we want to also highlight the importance of understanding the characteristics of languages being studied. For example, we showed that although North Sami and Finnish do not share vocabulary, cross-lingual training is still helpful because they share similar syntactic structures. Different language pairs might benefit from other types"
D19-1102,W17-1201,0,0.0677401,"Missing"
D19-1278,W17-6901,0,0.0240454,"ces to predicting rewrites only. We define the probability of generating graph G conditioned of input sentence w as follows: p(G|w) = p(a|w) = |a| Y i=1 p(ai |a<i , w) (1) r5 r7 L →anchor L →dock r6 r2 L →need p r4 r3 L →anchor L →big Figure 5: A derivation tree corresponding to Figure 4. Solid edges rewrite Tn nonterminals, while dotted rewrite L nonterminals. Input Encoder We represent the ith word wi of input sentence w = w1 . . . w|w |using both learned and pre-trained word embeddings (wi and wip respectively), lemma embedding (li ), part-of-speech embedding (pi ), universal semantic tag (Abzianidze and Bos, 2017) embedding (ui ), and dependency label embedding (di ).4 An input xi is computed as the weighted concatenation of these features followed by a non-linear projection (with vectors and matrices in bold): xi = tanh(W(1) [wi ; wip ; li ; pi ; ui ; di ]) (2) Input xi is then encoded with a bidirectional LSTM, yielding contextual representation hei . 4 Universal semantic tags are language neutral tags intended to characterize lexical semantics. 2772 Graph decoder Since we know in advance whether the next action is GEN-FRAG or GENLABEL, we use different models for them. GEN-FRAG. If step t rewrites a"
D19-1278,P98-1013,0,0.060201,"ship(x1 ) anchor(x3 ) PART O F(x1 , x2 ) big(s1 ) T OPIC(s1 , x3 ) x1 b2 Figure 1: The discourse representation structure for “Every ship in the dock needs a big anchor”. For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray. Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus (Montague, 1973), dependency-based compositional semantics (Liang et al., 2011), frame semantics (Baker et al., 1998), abstract meaning representations (AMR; Banarescu et al. 2013), minimal recursion semantics (MRS; Copestake et al. 2005), and discourse representation theory (DRT; Kamp 1981). Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence “Every ship in the dock needs a big anchor”. Its meaning representation, expressed as a Discourse Representation Structure (DRS, Kamp 1981), is shown in Figure 1.1 A DRS is drawn as a box with 1 For simplicity, our examples do not show time representations, though these are cons"
D19-1278,W13-2322,0,0.0510974,", x3 ) x1 b2 Figure 1: The discourse representation structure for “Every ship in the dock needs a big anchor”. For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray. Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus (Montague, 1973), dependency-based compositional semantics (Liang et al., 2011), frame semantics (Baker et al., 1998), abstract meaning representations (AMR; Banarescu et al. 2013), minimal recursion semantics (MRS; Copestake et al. 2005), and discourse representation theory (DRT; Kamp 1981). Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence “Every ship in the dock needs a big anchor”. Its meaning representation, expressed as a Discourse Representation Structure (DRS, Kamp 1981), is shown in Figure 1.1 A DRS is drawn as a box with 1 For simplicity, our examples do not show time representations, though these are consistently present in our data. two parts: the top part lists var"
D19-1278,P17-1112,0,0.0193591,"amentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Important"
D19-1278,P13-2131,0,0.0414169,"ained UDPipe models available at http://ufal.mff.cuni.cz/udpipe# download; gold-standard universal semantic tags were extracted from the PMB release. 2774 the copy mechanism. Most of the features mentioned above are cross-linguistic and therefore fit both mono and cross-lingual settings, with the exception of lemma and word embeddings, where we exclude the former and replaced the latter with multilingual word embeddings.9 4.5 Evaluation Metric We evaluated our system by scoring the similarity between predicted and gold graphs. We used Counter (van Noord et al., 2018), an adaptation of Smatch (Cai and Knight, 2013) to Discourse Representation Structures where graphs are first transformed into a set of ‘source node – edge label – target node’ triples and the best mapping between the variables is found through an iterative hill-climbing strategy. Furthermore, Counter checks whether DRSs are well-formed in that all boxes should be connected, acyclic, with fully instantiated variables, and correctly assigned sense tags. It is worth mentioning that there can be cases where our parser generates ill-formed graphs according to Counter; this is however not due to the model itself but to the way the graph is conv"
D19-1278,P18-1071,0,0.0208564,"e predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Importantly, any well-formed"
D19-1278,E17-1051,0,0.0482812,"ifficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Importantly, any well-formed string produced by our mode"
D19-1278,P18-1170,0,0.0182646,"gs that don’t correspond to graphs—for example, strings with illformed bracketings or unbound variable names. While it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—wh"
D19-1278,P16-1154,0,0.0182967,"target of multiple edges, then the leftmost one is written as a subtree, and the remainder are written as references. Hence, every node is written as a subtree exactly once. The advantage of predicting linearized graphs is twofold. The first advantage is that graphbank datasets usually already contain linearizations, which can be used without additional work. These linearizations are provided by annotators or algorithms and are thus likely to be very consistent in ways that are beneficial to a learning algorithm. The second advantage is that we can use simple, well-understood sequence models (Gu et al., 2016; Jia and Liang, 2016; van Noord et al., 2018) to model them. But this simplicity comes with a cost: sequence models can predict strings that don’t correspond to graphs—for example, strings with illformed bracketings or unbound variable names. While it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible lineari"
D19-1278,Q18-1043,0,0.231588,"Missing"
D19-1278,P16-1002,0,0.014923,"e edges, then the leftmost one is written as a subtree, and the remainder are written as references. Hence, every node is written as a subtree exactly once. The advantage of predicting linearized graphs is twofold. The first advantage is that graphbank datasets usually already contain linearizations, which can be used without additional work. These linearizations are provided by annotators or algorithms and are thus likely to be very consistent in ways that are beneficial to a learning algorithm. The second advantage is that we can use simple, well-understood sequence models (Gu et al., 2016; Jia and Liang, 2016; van Noord et al., 2018) to model them. But this simplicity comes with a cost: sequence models can predict strings that don’t correspond to graphs—for example, strings with illformed bracketings or unbound variable names. While it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a pr"
D19-1278,P11-1060,0,0.0505102,") P IVOT(e1 , x1 ) ⇒ T HEME(e1 ,x3 ) ship(x1 ) anchor(x3 ) PART O F(x1 , x2 ) big(s1 ) T OPIC(s1 , x3 ) x1 b2 Figure 1: The discourse representation structure for “Every ship in the dock needs a big anchor”. For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray. Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus (Montague, 1973), dependency-based compositional semantics (Liang et al., 2011), frame semantics (Baker et al., 1998), abstract meaning representations (AMR; Banarescu et al. 2013), minimal recursion semantics (MRS; Copestake et al. 2005), and discourse representation theory (DRT; Kamp 1981). Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence “Every ship in the dock needs a big anchor”. Its meaning representation, expressed as a Discourse Representation Structure (DRS, Kamp 1981), is shown in Figure 1.1 A DRS is drawn as a box with 1 For simplicity, our examples do not show time"
D19-1278,P18-1040,1,0.873022,"nd e is weighted by matrices W(3) and W(4) , respectively. We then update the stackLSTM representation using the embedding of the non-terminal fragment yt (denoted as yte ), as follows: hdt+1 = LSTM(yte , hdt ) (4) GEN-LABEL. Labels L can be rewritten to either semantic constants (e.g., ‘speaker’, ‘now’, ‘hearer’) or unary predicates that often corresponds to the lemmas of the input words (e.g., ‘love’) or. We predict the former using a model identical to the one for GEN-FRAG. For the latter, we use a selection mechanism to choose an input lemma to copy to output. We model selection following Liu et al. (2018), assigning each input lemma a score oji that we then pass through a softmax layer to obtain a distibution: (5) e oji = hdT j W hi pcopy = S OFT M AX(oji ) i (5) where hi is the encoder hidden state for word wi . We allow the model to learn whether to use softattention or the selection mechanism through a binary classifier, conditioned on the decoder hidden state at time t, hdt . Similar to Equation (4), we update the stackLSTM with the embedding of terminal predicted.5 5 In the PMB, each terminal is annotated for sense (e.g. ‘n.01’, ‘s.01’) and presupposition (e.g. for ‘dockp ’ in Figure 3) a"
D19-1278,D15-1166,0,0.0127877,"Missing"
D19-1278,P18-1037,0,0.0522701,"le it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of"
E09-1061,W09-0437,1,0.880341,"Missing"
E09-1061,P08-1024,0,0.079678,"be the consequent of multiple deductions, we take the max of its current value (initially 0) and the result of the new deduction. p(C) = max(p(C), (p(A1 ) × ... × p(AL ))) (3) If for every A` that is an item, we replace p(A` ) recursively with this expression, we end up with a maximization over a product of rule probabilities. Applying this to logic M ONOTONE, the result will be a maximization (over all possible derivations D) of the algebraic expression in Equation 1. We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al., 2008). We can do this using the following equation. p(C) = p(C) + (p(A1 ) × ... × p(AL )) (4) Equations 3 and 4 are quite similar. This suggests a useful generalization: semiring-weighted deduction (Goodman, 1999).8 A semiring hA, ⊗, ⊕i consists of a domain A, a multiplicative operator ⊗ and an additive operator ⊕.9 In Equation 3 we use the Viterbi semiring h[0, 1], ×, maxi, while in Equation 4 we use the inside semiring h[0, 1], ×, +i. The general form of Equations 3 and 4 can be written for weights w ∈ A. w(C)⊕= w(A1 ) ⊗ ... ⊗ w(A` ) (5) Many quantities can be computed simply by using the appropr"
E09-1061,J93-2003,0,0.0238926,"not hold for others, and in cases where the strategy is not clearly described it may be impossible to replicate results. Furthermore, it should be clear that the strategy can have significant impact on decoding speed and pruning strategies (§7). For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models (Brown et al., 1993) would be similar to our logics for phrase-based models. Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). For simplicity, we will use the M ONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics. 4 Adding Local Parameterizations via Semiring-Weighted Deduction So far we have focused solely on unweighted logics, which correspond to search using only rulesets. Now we turn our focus to parameterizations. As a first step"
E09-1061,2003.mtsummit-papers.6,0,0.0385132,"ogic M ONOTONE -A LIGN) Under the boolean semiring, this (minimal) logic decides if a training example is reachable by the model, which is required by some discriminative training regimens (Liang et al., 2006; Blunsom et al., 2008). We can also compute the Viterbi derivation or the sum over all derivations of a training example, needed for some parameter estimation methods. Cohen et al. (2008) derive an alignment logic for ITG from the product of two CKY logics. 6.2 Translation Model Design A motivation for many syntax-based translation models is to use target-side syntax as a language model (Charniak et al., 2003). Och et al. (2004) showed that simply parsing the N -best outputs of a phrase-based model did not work; to obtain the full power of a language model, we need to integrate it into the search process. Most approaches to this problem focus on synchronous grammars, but it is possible to integrate the targetside language model with a phrase-based translation model. As an exercise, we integrate CKY with the output of logic M ONOTONE -G ENERATE. The constraint is that the indices of the CKY items unify with the items of the translation logic, which form a word lattice. Note that this logic retains t"
E09-1061,J07-2003,0,0.853976,"s. We illustrate this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 1 Introduction Im"
E09-1061,J82-3004,0,0.051613,"orithm for context-free parsing, a common example that we will revisit in §6.2. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). In the discussion that follows, we use A, B, and C to denote arbitrary nonterminal symbols, S to denote the start nonterminal symbol, and a to denote a terminal symbol. CKY works on grammars in Chomsky normal form: all rules are either binary as in A → BC, or unary as in A → a. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function (Church and Patil, 1982), so dynamic programming is crucial for efficiency. CKY computes all parses in cubic time by reusing subparses. To parse a sentence a1 ...aK , we compute a set of items in the 1 The true noisy channel parameterization p(f |e) · p(e) would require a marginalization over D, and is intractable for most models. form [A, k, k 0 ], where A is a nonterminal category, k and k 0 are both integers in the range [0, n]. This item represents the fact that there is some parse of span ak+1 ...ak0 rooted at A (span indices are on the spaces between words). CKY works by creating items over successively longer"
E09-1061,W06-1609,0,0.0189792,"Missing"
E09-1061,P08-1115,0,0.199456,"mples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 1 Introduction Implementing a large-scale translation system"
E09-1061,P81-1022,0,0.782252,"ic for the non-local parameterization. The logic for an n-gram language model generates sequence e1 ...eQ by generating each new word given the past n − 1 words.11 item form: [eq , ..., eq+n−2 ] goal: [eQ−n+2 , ..., eQ ] [eq , ..., eq+n−2 ]R(eq , ..., eq+n−1 ) [eq+1 , ..., eq+n−1 ] (Logic N GRAM) Now we want to combine N GRAM and M ONO TONE. To make things easier, we modify M ONO TONE to encode the idea that once a source phrase has been recognized, its target words are generated one at a time. We will use ue and ve to denote (possibly empty) sequences in ej ...e0j . Borrowing the notation of Earley (1970), we encode progress using a dotted phrase ue • ve . rule: item form: [i, ue • ve ] goal: [I, ue •] rules: [i, ue •] R(fi+1 ...fi0 /ej ve ) [i, ue • ej ve ] [i0 , ej • ve ] [i, ue ej • ve ] (Logic M ONOTONE -G ENERATE) We combine N GRAM and M ONOTONE G ENERATE using the P RODUCT transform, which takes two logics as input and essentially does the following.12 1. Create a new item type from the crossproduct of item types in the input logics. 2. Create inference rules for the new item type from the cross-product of all inference rules in the input logics. 3. Constrain the new logic as needed. Thi"
E09-1061,H05-1036,0,0.608436,"ing. In translation it is relevant for alignment (§6.1). 5 Adding Non-Local Parameterizations with the P RODUCT Transform A problem arises with the semiring-weighted deductive formalism when we add non-local parameterizations such as an n-gram model (Equation 2). Suppose we have a derivation D = (d1 , ..., dM ), where each dm is a rule application. We can view the language model as a function on D. P (D) = f (d1 , ..., dm , ..., dM ) (6) The problem is that replacing dm with a lowerscoring rule d0m may actually improve f due to 8 General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 9 See Goodman (1999) for additional conditions on semirings used in this framework. 10 Eisner and Blatz (2006) give an alternate strategy for the best derivation. the language model dependency. This means that f is nonmonotonic—it does not display the optimal substructure property on partial derivations, which is required for dynamic programming (Cormen et al., 2001). The logics still work for some se"
E09-1061,P02-1001,0,0.0543939,"generalization: semiring-weighted deduction (Goodman, 1999).8 A semiring hA, ⊗, ⊕i consists of a domain A, a multiplicative operator ⊗ and an additive operator ⊕.9 In Equation 3 we use the Viterbi semiring h[0, 1], ×, maxi, while in Equation 4 we use the inside semiring h[0, 1], ×, +i. The general form of Equations 3 and 4 can be written for weights w ∈ A. w(C)⊕= w(A1 ) ⊗ ... ⊗ w(A` ) (5) Many quantities can be computed simply by using the appropriate semiring. Goodman (1999) describes semirings for the Viterbi derivation, k-best Viterbi derivations, derivation forest, and number of paths.10 Eisner (2002) describes the expectation semiring for parameter learning. Gimpel and Smith (2009) describe approximation semirings for approximate summing in (usually intractable) models. In parsing, the boolean semiring h{&gt;, ⊥}, ∩, ∪i is used to determine grammaticality of an input string. In translation it is relevant for alignment (§6.1). 5 Adding Non-Local Parameterizations with the P RODUCT Transform A problem arises with the semiring-weighted deductive formalism when we add non-local parameterizations such as an n-gram model (Equation 2). Suppose we have a derivation D = (d1 , ..., dM ), where each dm"
E09-1061,J99-4004,0,0.738212,"emiring parsing, and efficient approximate search algorithms. This gives rise to clean analyses and compact descriptions that can serve as the basis for modular implementations. We illustrate this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and nov"
E09-1061,P07-1019,0,0.0440101,"ed. 8 Related Work We are not the first to observe that phrase-based models can be represented as logic programs (Eisner et al., 2005; Eisner and Blatz, 2006), but to our knowledge we are the first to provide explicit logics for them.14 We also showed that deductive logic is a useful analytical tool to tackle a variety of problems in translation algorithm design. Our work is strongly influenced by Goodman (1999) and Eisner et al. (2005). They describe many issues not mentioned here, including conditions on semirings, termination conditions, and strategies for cyclic search graphs. However, 14 Huang and Chiang (2007) give an informal example, but do not elaborate on it. while their weighted deductive formalism is general, they focus on concerns relevant to parsing, such as boolean semirings and cyclicity. Our work focuses on concerns common for translation, including a general view of non-local parameterizations and cube pruning. 9 Conclusions and Future Work We have described a general framework that synthesizes and extends deductive parsing and semiring parsing, and adapts it to translation. Our goal has been to show that logics make an attractive shorthand for description, analysis, and construction of"
E09-1061,W01-1812,0,0.137772,"ranslation logic, which form a word lattice. Note that this logic retains the rules in the basic M ONOTONE logic, which are not depicted (Figure 3). The result is a lattice parser on the output of the translation model. Lattice parsing is not new to translation (Dyer et al., 2008), but to our knowledge it has not been used in this way. Viewing translation as deduction is helpful for the design and construction of novel models. 7 Algorithms Most translation logics are too expensive to exhaustively search. However, the logics conveniently specify the full search space, which forms a hypergraph (Klein and Manning, 2001).13 The equivalence is useful for complexity analysis: items correspond to nodes and deductions correspond to hyperarcs. These equivalences make it easy to compute algorithmic bounds. Cube pruning (Chiang, 2007) is an approximate search technique for syntax-based translation models with integrated language models. It operates on two objects: a −LM graph containing no language model state, and a +LM hypergraph containing state. The idea is to generate a fixed number of nodes in the +LM for each node in the −LM graph, using a clever enumeration strategy. We can view cube pruning as arising from"
E09-1061,P07-2045,0,0.00577792,"l-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make some simple analyses. First, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of d it is impossi3 Moore and Quirk (2007) give a nice description of MDd. We could obtain a tighter bound by subtracting number of the vectors containing the forbidden sequence 0d 1. 5 We do not know if WLd is documented anywhere, but from inspection it is used in Moses (Koehn et al., 2007). This was confirmed by Philipp Koehn and Hieu Hoang (p.c.). 6 When a phrase covers the first uncovered word in the source sentence, the new first uncovered word may be further along in the sentence (if the phrase completely filled a gap), or just past the end of the phrase (otherwise). 7 We could not identify this strategy in the IBM patents. 4 (1) item form: [i, {0, 1}I ] goal: [i ∈ [I − d, I], 1I ] (2) item form: [i, {0, 1}d ] rules: rule:       0 0 [i, C] R(fi+1 ...fi0 /ej ...ej 0 ) C ∧ 1i −i 0d−i +i = 0d , i0 − i ≤ d, 0 0 α1 (C ∨ 1i −i 0d−i +i ) = i00 − i [i00 , C  i00 − i] 0"
E09-1061,H05-1021,0,0.0614327,"ut linear in sentence length, O(d2 2d I). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint.7 It requires at least one of the leftmost d uncovered words to be covered by a new phrase. Items in this strategy contain the index i of the rightmost covered word and a vector U ∈ [1, I]d of the d leftmost uncovered  words (Figure 1). Its I ), which is roughly excomplexity is O(dI d+1 ponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make some simple analyses. First, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of d it is impossi3 Moore and Quirk (2007) give a nice description of MDd. We could obtain a tighter bound by subtracting number of the vectors containing the forbidden sequence 0d 1. 5 We do not know if WLd is documented anywhere, but from inspection it is used in Moses (Koehn et al., 2007). This was confirmed by Philipp"
E09-1061,P06-1096,0,0.0616408,"Missing"
E09-1061,J06-4004,0,0.0631727,"Missing"
E09-1061,P04-1083,0,0.0689047,"to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 1 Introduction Implementing a large-scale translation system is a major engi"
E09-1061,2007.mtsummit-papers.43,0,0.0203421,"the rightmost covered word and a vector U ∈ [1, I]d of the d leftmost uncovered  words (Figure 1). Its I ), which is roughly excomplexity is O(dI d+1 ponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as logics permits us to make some simple analyses. First, it should be clear that these reordering strategies define overlapping but not identical search spaces: for most values of d it is impossi3 Moore and Quirk (2007) give a nice description of MDd. We could obtain a tighter bound by subtracting number of the vectors containing the forbidden sequence 0d 1. 5 We do not know if WLd is documented anywhere, but from inspection it is used in Moses (Koehn et al., 2007). This was confirmed by Philipp Koehn and Hieu Hoang (p.c.). 6 When a phrase covers the first uncovered word in the source sentence, the new first uncovered word may be further along in the sentence (if the phrase completely filled a gap), or just past the end of the phrase (otherwise). 7 We could not identify this strategy in the IBM patents. 4 (1"
E09-1061,J03-1006,0,0.324949,"ent (§6.1). 5 Adding Non-Local Parameterizations with the P RODUCT Transform A problem arises with the semiring-weighted deductive formalism when we add non-local parameterizations such as an n-gram model (Equation 2). Suppose we have a derivation D = (d1 , ..., dM ), where each dm is a rule application. We can view the language model as a function on D. P (D) = f (d1 , ..., dm , ..., dM ) (6) The problem is that replacing dm with a lowerscoring rule d0m may actually improve f due to 8 General weighted deduction subsumes semiringweighted deduction (Eisner et al., 2005; Eisner and Blatz, 2006; Nederhof, 2003), but semiring-weighted deduction covers all translation models we are aware of, so it is a good first step in applying weighted deduction to translation. 9 See Goodman (1999) for additional conditions on semirings used in this framework. 10 Eisner and Blatz (2006) give an alternate strategy for the best derivation. the language model dependency. This means that f is nonmonotonic—it does not display the optimal substructure property on partial derivations, which is required for dynamic programming (Cormen et al., 2001). The logics still work for some semirings (e.g. boolean), but not others. T"
E09-1061,N04-1021,0,0.00773247,"Under the boolean semiring, this (minimal) logic decides if a training example is reachable by the model, which is required by some discriminative training regimens (Liang et al., 2006; Blunsom et al., 2008). We can also compute the Viterbi derivation or the sum over all derivations of a training example, needed for some parameter estimation methods. Cohen et al. (2008) derive an alignment logic for ITG from the product of two CKY logics. 6.2 Translation Model Design A motivation for many syntax-based translation models is to use target-side syntax as a language model (Charniak et al., 2003). Och et al. (2004) showed that simply parsing the N -best outputs of a phrase-based model did not work; to obtain the full power of a language model, we need to integrate it into the search process. Most approaches to this problem focus on synchronous grammars, but it is possible to integrate the targetside language model with a phrase-based translation model. As an exercise, we integrate CKY with the output of logic M ONOTONE -G ENERATE. The constraint is that the indices of the CKY items unify with the items of the translation logic, which form a word lattice. Note that this logic retains the rules in the bas"
E09-1061,P83-1021,0,0.807246,"rithms that synthesizes work on deductive parsing, semiring parsing, and efficient approximate search algorithms. This gives rise to clean analyses and compact descriptions that can serve as the basis for modular implementations. We illustrate this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal p"
E09-1061,J03-1005,0,0.229985,"MDd can produce partial derivations that cannot be completed by any allowed sequence of jumps. To prevent this, the Window Length d strategy (WLd) enforces a tighter restriction that the last word of a phrase chosen for translation cannot be more than d words from the leftmost untranslated word in the source (Figure 1).5 For this logic we use a bitwise shift operator (), and a predicate (α1 ) that counts the number of leading ones in a bit array.6 Its runtime is exponential in parameter d, but linear in sentence length, O(d2 2d I). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint.7 It requires at least one of the leftmost d uncovered words to be covered by a new phrase. Items in this strategy contain the index i of the rightmost covered word and a vector U ∈ [1, I]d of the d leftmost uncovered  words (Figure 1). Its I ), which is roughly excomplexity is O(dI d+1 ponential in d. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. We lack space to describe all of them, but simply depicting the strategies as l"
E09-1061,N07-1063,0,0.0802459,"te this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas. (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formalism used in parsing. It is occasionally used to describe formally syntactic translation models, but these treatments tend to be brief (Chiang, 2007; Venugopal et al., 2007; Dyer et al., 2008; Melamed, 2004). We apply weighted deduction much more thoroughly, first extending it to phrasebased models and showing that the set of search strategies used by these models have surprisingly different implications for model and search error (§3, §4). We then show how it can be used to analyze common translation problems such as nonlocal parameterizations (§5), alignment, and novel model design (§6). Finally, we show that it leads to a simple analysis of cube pruning (Chiang, 2007), an important approximate search algorithm (§7). 2 1 Introduction Implementing a large-scale"
E09-1061,P96-1021,0,0.0341746,"is also affected, because the terms ej−n+1 ...ej may come from more than one rule. In other words, this parameterization encodes a dependency between the steps in a derivation. We call this a non-local parameterization. 3 Translation As Deduction For the first part of the discussion that follows, we consider deductive logics purely over unweighted rulesets. As a way to introduce deductive logic, we consider the CKY algorithm for context-free parsing, a common example that we will revisit in §6.2. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). In the discussion that follows, we use A, B, and C to denote arbitrary nonterminal symbols, S to denote the start nonterminal symbol, and a to denote a terminal symbol. CKY works on grammars in Chomsky normal form: all rules are either binary as in A → BC, or unary as in A → a. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function (Church and Patil, 1982), so dynamic programming is crucial for efficiency. CKY computes all parses in cubic time by reusing subparses. To parse a sentence a1 ...aK , we compute a set of"
E09-1061,N04-1033,0,0.0609117,"an be determined by inspection (McAllester, 1999). We elaborate on complexity in §7, but for now it suffices to point out that the number of possible items and possible deductions depends on the product of the domains of the free variables. For example, the number of possible CKY items for a grammar with G nonterminals is O(GK 2 ), because k and k 0 are both in range [0, K]. Likewise, the number of possible inference rules that can fire is O(G3 K 3 ). 3.1 A Simple Deductive Decoder For our first example of a translation logic we consider a simple case: monotone decoding (Mari˜no et al., 2006; Zens and Ney, 2004). Here, rewrite rules are applied strictly from left to right on the source sentence. Despite its simplicity, the search space can be very large—in the limit there could be a translation for every possible segmentation of the sentence, so there are exponentially many possible derivations. Fortunately, we know that monotone decoding can easily be cast as a dynamic programming problem. For any position i in the source sentence f1 ...fI , we can freely combine any partial derivation covering f1 ...fi on its left with any partial derivation covering fi+1 ...fI on its right to yield a complete deri"
E09-1061,E09-1037,0,\N,Missing
E17-2010,P16-1047,1,0.557328,"m, detection accuracy is low and undersampling the easy training examples does not substantially improve accuracy. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases. 1 But S HERLOCK is only one of several corpora annotated for negation scope, each the result of different annotation decisions and targeted to specific applications or domains. Does the same approach work equally well across all corpora? In answer to this question, we offer two contributions. 1. We evaluate Fancellu et al. (2016)’s model on all other available negation scope corpora in English and Chinese. Although we confirm that it is state-of-the-art, we show that it can be improved by making joint predictions for all words, incorporating an insight from Morante et al. (2008) that classifiers tend to leave gaps in what should otherwise be a continuous prediction. We accomplish this with a sequence model over the predictions. Introduction 2. We show that in all corpora except S HER negation scope is most often delimited by punctuation. That is, in these corpora, examples like (2) outnumber those like (1). LOCK , Tex"
E17-2010,konstantinova-etal-2012-review,0,0.425617,"Missing"
E17-2010,morante-daelemans-2012-conandoyle,0,0.560846,"confirm that it is state-of-the-art, we show that it can be improved by making joint predictions for all words, incorporating an insight from Morante et al. (2008) that classifiers tend to leave gaps in what should otherwise be a continuous prediction. We accomplish this with a sequence model over the predictions. Introduction 2. We show that in all corpora except S HER negation scope is most often delimited by punctuation. That is, in these corpora, examples like (2) outnumber those like (1). LOCK , Textual negation scope is the largest span affected by a negation cue in a negative sentence (Morante and Daelemans, 2012).1 For example, given the marker not in (1), its scope is use the 56k conextant modem.2 (2) It helps activation , [not inhibition of ibrf1 cells] . (1) I do not [use the 56k conextant modem] since I have cable access for the internet Our experiments demonstrate that negation scope detection is very accurate for sentences like (2) and poor for others, suggesting that most classifiers simply overfit to this feature of the data. When we attempt to mitigate this effect by undersampling examples like (2) in training, our system does not improve on examples like (1) in test, suggesting that more tra"
E17-2010,D08-1075,0,0.897089,"ocus on these more difficult cases. 1 But S HERLOCK is only one of several corpora annotated for negation scope, each the result of different annotation decisions and targeted to specific applications or domains. Does the same approach work equally well across all corpora? In answer to this question, we offer two contributions. 1. We evaluate Fancellu et al. (2016)’s model on all other available negation scope corpora in English and Chinese. Although we confirm that it is state-of-the-art, we show that it can be improved by making joint predictions for all words, incorporating an insight from Morante et al. (2008) that classifiers tend to leave gaps in what should otherwise be a continuous prediction. We accomplish this with a sequence model over the predictions. Introduction 2. We show that in all corpora except S HER negation scope is most often delimited by punctuation. That is, in these corpora, examples like (2) outnumber those like (1). LOCK , Textual negation scope is the largest span affected by a negation cue in a negative sentence (Morante and Daelemans, 2012).1 For example, given the marker not in (1), its scope is use the 56k conextant modem.2 (2) It helps activation , [not inhibition of ib"
E17-2010,D16-1078,0,0.442549,"ition grounds the phenomenon at word level. 2 For all examples in this paper, negation cues are in bold, human-annotated negation scope is in square brackets [ ], and automatically predicted negation scope is underlined. 58 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 58–63, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics cus on these cases.3 2 wider range of complex phenomena including ellipsis, long-range dependencies and affixal negation. Though widely used (e.g. Qian et al. (2016)), the SFU, BioScope and CNeSp corpora contain simplifications that are sometimes hard to justify linguistically. In SFU and BioScope, for instance, scope is usually annotated only to the right of the cue, as in (1). The only exception is passive constructions, where the subject to the left is also annotated: Models We use the bi-directional LSTM of Fancellu et al. (2016). The input to the network is a negative sentence w = w1 ...w|w |containing a negation cue. If there is more than one cue, we consider each cue and its corresponding scope as a separate classification instance. Given a represe"
E17-2010,J12-2005,0,0.110589,"Missing"
E17-2010,W08-0606,0,0.73622,"Missing"
E17-2010,P15-1064,0,0.0253879,"Missing"
E17-2010,D13-1099,0,0.0341495,"Missing"
E17-2076,J93-2003,0,0.0703492,"1). In creating a translation model from this data, we face a difficulty that does not arise in the parallel texts that are normally used to train translation models: the pseudotext does not represent all of the source words, since the discovered segments do not cover the full audio (Fig. 1). Hence we must not assume that our MT model can completely recover the translation of a test sentence. In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).2 In particular, for each pseudoterm, we learn a translation distribution over possible target words. To translate a pseudoterm in test data, we simply return its highest-probability translation (or translations, as discussed in §5). This setup implies that in order to translate, we must apply UTD on both the training and test audio. Using additional (not only training) audio in UTD increases the likelihood of discovering more clusters. We therefore generate pseudotext for the combined audio, t"
E17-2076,N16-1109,0,0.299767,"(Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be collected in support of relief operations. Can we do anything at all with this data? In this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsuperv"
E17-2076,N13-1073,0,0.0130345,"parallel texts that are normally used to train translation models: the pseudotext does not represent all of the source words, since the discovered segments do not cover the full audio (Fig. 1). Hence we must not assume that our MT model can completely recover the translation of a test sentence. In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).2 In particular, for each pseudoterm, we learn a translation distribution over possible target words. To translate a pseudoterm in test data, we simply return its highest-probability translation (or translations, as discussed in §5). This setup implies that in order to translate, we must apply UTD on both the training and test audio. Using additional (not only training) audio in UTD increases the likelihood of discovering more clusters. We therefore generate pseudotext for the combined audio, train the MT model on the pseudotext of the training audio, and apply it to the pseudotext of the tes"
E17-2076,D16-1263,0,0.377478,"translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsupervised or minimallysupervised way, but in practice they used supervised ASR/phone recognition. Additionally, their evaluation focused on phone error rate rather than translation. In contrast to these approaches, our method can make translation predictions for audio input not seen during training, and we evaluate it on real multi-speaker speech data. Our simple system (§2) builds on unsupervised speec"
E17-2076,D16-1133,0,0.709692,"Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be collected in support of relief operations. Can we do anything at all with this data? In this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers. Finally, Adams et al. (2016a; 2016b) targeted the same low-resource speech-to-translation task, but instead of working with audio, they started from word or phoneme lattices. In principle these could be produced in an unsupervised or minimallysupervised way,"
E17-2076,Q15-1028,0,0.0199519,"Missing"
E17-2076,2010.amta-workshop.1,0,0.0325618,"nslation (MT) (Waibel and Fugen, 2008). But high-quality ASR requires hundreds of hours of transcribed audio, while high-quality MT requires millions of words of parallel text—resources available for only a tiny fraction of the world’s estimated 7,000 languages (Besacier et al., 2014). Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations. In these settings, target translations may be available. For example, ad hoc translations may be collected in support of relief operations. Can we do anything at all with this data? In this exploratory study, we present a speechto-text translation system that learns directly from source audio and target text pairs, and does not require intermediate ASR or MT. Our work complements several lines of related recent work. For example, Duong et al. (2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used thes"
E17-2076,2013.iwslt-papers.14,1,0.100114,"eech data. Our simple system (§2) builds on unsupervised speech processing (Versteegh et al., 2015; Lee et al., 2015; Kamper et al., 2016b), and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech (Park and Glass, 2008; Jansen and Van Durme, 2011). The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-ofwords translation model. We test our system on the CALLHOME Spanish-English speech translation corpus (Post et al., 2013), a noisy multi-speaker corpus of telephone calls in a variety of Spanish di474 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 474–479, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics alects (§3). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in crossspeaker clustering (§4). Despite these difficulties, we demonstrate that the system lea"
H05-1098,P05-1033,1,0.428241,", and Analysis David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin Institute for Advanced Computer Studies (UMIACS) University of Maryland, College Park, MD 20742, USA {dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu Abstract Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of non"
H05-1098,W02-1039,0,0.35507,"evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a"
H05-1098,P02-1050,1,0.909542,"s. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG,"
H05-1098,P03-1040,0,0.00569351,"ase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any"
H05-1098,N03-1017,0,0.134136,"aper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based mo"
H05-1098,koen-2004-pharaoh,0,0.0221738,"e-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any commitment to a linguistically relevant analysis, and it does not require syntactically annotated training data. Chiang (2005) reported significant performance improvements in Chinese-English translation as compared with Pharaoh, a state-of-the-art phrase-based system (Koehn, 2004). In Section 2, we review the essential elements of Hiero. In Section 3 we describe extensions to this system, including new features involving named entities and numbers and support for a fourfold scale-up in training set size. Section 4 presents new evaluation results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochasti"
H05-1098,P04-1077,0,0.0151475,"on the FBIS data; Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded"
H05-1098,H05-2007,1,0.821428,"Missing"
H05-1098,W02-1018,0,0.0259841,"ew hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident w"
H05-1098,N03-2021,0,0.177666,"Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded again using an HMM wh"
H05-1098,P00-1056,0,0.0589312,"igure 1: Example synchronous CFG • the lexical weights Pw (γ |α) and Pw (α |γ) (Koehn et al., 2003);1 2.1 Grammar • a phrase penalty exp(1); A synchronous CFG or syntax-directed transduction grammar (Lewis and Stearns, 1968) consists of pairs of CFG rules with aligned nonterminal symbols. We denote this alignment by coindexation with boxed numbers (Figure 1). A derivation starts with a pair of aligned start symbols, and proceeds by rewriting pairs of aligned nonterminal symbols using the paired rules (Figure 2). Training begins with phrase pairs, obtained as by Och, Koehn, and others: GIZA++ (Och and Ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the “final-and” method of Koehn et al. (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not alread"
H05-1098,P02-1038,0,0.00566047,"onous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w(X → hγ, αi) = φi (X → hγ, αi)λi S → hX 1 , X 1 i X → hyu X 1 you X 2 , have X 2 with X 1 i X → hX 1 de X 2 , the X 2 that X 1 i X → hX 1 zhiyi, one of X 1 i X → hAozhou, Australiai X → hshi, isi i X → hshaoshu guojia, few countriesi where the φi are features defined on rules. The basic model uses the following features, analogous to Pharaoh’s default featu"
H05-1098,J04-4002,0,0.0105751,"luations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absenc"
H05-1098,P03-1021,0,0.00789852,"verage. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up to a much larger training set. 3.1 New features supplementary ph"
H05-1098,P02-1040,0,0.109612,"nment, Koehn et al. take the maximum lexical weight; Hiero uses a weighted average. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up"
H05-1098,W96-0213,0,0.00943678,"t work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of every possible tag sequence ti . . . t j in the reference corpus. Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs"
H05-1098,2003.mtsummit-papers.53,0,0.0144904,"ed Chinese number/date translator, and created a new model feature for it. Again, the weight given to this module was optimized during minimum-error-rate training. In some cases we wrote the rules to provide multiple uniformlyweighted English translations for a Chinese phrase (for example, kå (bari) could become “the 8th” or “on the 8th”), allowing the language model to decide between the options. 3.2 The LDC Chinese-English named entity lists (900k entries) are a potentially valuable resource, but previous experiments have suggested that simply adding them to the training data does not help (Vogel et al., 2003). Instead, we placed them in a supplementary phrase-translation table, giving greater weight to phrases that occurred less frequently in the primary training data. For each entry h f, {e1 , . . . , en }i, we counted the number of times c( f ) that f appeared in the primary training data, and assigned the entry the weight c( f1)+1 , which was then distributed evenly among the supplementary phrase pairs {h f, ei i}. We then created a new model feature for named entities. When one of these 781 Scaling up training Chiang (2005) reports on experiments in ChineseEnglish translation using a model tra"
H05-1098,P96-1021,0,0.157884,"tion results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochastic synchronous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w"
H05-1098,J97-3002,0,0.094693,"ned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not already used. Various filters are also applied to reduce the number of extracted rules. Since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (Wu, 1997). 780 • a word penalty exp(l), where l is the number of terminals in α. The exceptions to the above are the two “glue” rules, which are the rules with left-hand side S in Figure 1. The second has weight one, and the first has weight w(S → hS 1 X 2 , S 1 X 2 i) = exp(−λg ), the idea being that parameter λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Phrase translation probabilities are estimated by relative-frequency estimation. Since the extraction process does not generate a unique derivation for each training sentence pair, a distribution over"
H05-1098,P02-1039,0,0.0166923,"pect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems, it may be useful to look for syntactic patterns that one system captures well in the target language and the other does not, using a syntax based metric. We propose to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of ever"
H05-1098,N04-1021,0,\N,Missing
H05-2007,H05-1098,1,0.780918,"up the associated tag sequence ti . . .t j and increment a counter recalled(ti . . .t j ) Using this method, we compute the recall of tag patterns, R(ti . . .t j ) = recalled(ti . . .t j )/freq(ti . . .t j ), for all patterns in the corpus. To compare two systems (which could include two versions of the same system), we identify POS n-grams that are recalled significantly more frequently by one system than the other, using a difference-of-proportions test to assess statistical significance. We have used this method to analyze the output of two different statistical machine translation models (Chiang et al., 2005). 3 Visualization Our demonstration system uses an HTML interface to summarize the observed pattern recall. Based on frequent or significantly-different recall, the user can select and visually inspect color-coded examples of each pattern of interest in context with both source and reference sentences. An example visualization is shown in Figure 1. 13 4 Acknowledgements The authors would like to thank David Chiang, Christof Monz, and Michael Subotin for helpful commentary on this work. This research was supported in part by ONR MURI Contract FCPO.810548265 and Department of Defense contract RD"
H05-2007,W02-1039,0,0.0409313,"need fine-grained error analysis. What we would really like to know is how well the system is able to capture systematic reordering patterns in the input, which ones it is successful with, and which ones it has difficulty with. Word n-grams are little help here: they are too many, too sparse, and it is difficult to discern general patterns from them. 2 Part-of-Speech Sequence Recall In developing a new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using a syntaxbased, recall-oriented metric. As an initial step, we would like to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text to the noisy output of MT systems produces unuseable results (e.g. (Och et al"
H05-2007,P02-1050,1,0.85571,"rained error analysis. What we would really like to know is how well the system is able to capture systematic reordering patterns in the input, which ones it is successful with, and which ones it has difficulty with. Word n-grams are little help here: they are too many, too sparse, and it is difficult to discern general patterns from them. 2 Part-of-Speech Sequence Recall In developing a new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using a syntaxbased, recall-oriented metric. As an initial step, we would like to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text to the noisy output of MT systems produces unuseable results (e.g. (Och et al., 2004)). Therefor"
H05-2007,J82-2005,0,0.491619,"Missing"
H05-2007,P02-1040,0,0.0729166,"42 alopez@cs.umd.edu resnik@umd.edu Abstract We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences. We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems, and demonstrate how our application can be used by developers to explore patterns in machine translation output. 1 Introduction Over the last few years, several automatic metrics for machine translation (MT) evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Papineni et al., 2002; Melamed et al., 2003). All are predicated on the concept of n-gram matching between the sentence hypothesized by the translation system and one or more reference translations—that is, human translations for the test sentence. Although the formulae underlying these metrics vary, each produces a single number representing the “goodness” of the MT system output over a set of reference documents. We can compare the numbers of competing systems to get a coarse estimate of their relative performance. However, this comparison is holistic. It provides no insight into the specific competencies or wea"
H05-2007,W96-0213,0,0.0597204,"s unuseable results (e.g. (Och et al., 2004)). Therefore, we make the conservative choice to apply annotation only to the reference corpus. Word n-gram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. The method: 1. Part-of-speech tag the reference corpus. We used 12 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 12–13, Vancouver, October 2005. Figure 1: Comparing two systems that differ significantly in their recall for POS n-gram JJ NN IN DT NN. The interface uses color to make examples easy to find. MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. 2. Compute the frequency freq(ti . . .t j ) of every possible tag sequence ti . . .t j in the reference corpus. 3. Compute the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs are order-sensitiv"
H05-2007,N03-2021,0,\N,Missing
H05-2007,N04-1021,0,\N,Missing
J18-1005,P13-1023,0,0.134502,"nt of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for r"
J18-1005,P99-1070,0,0.343387,"Missing"
J18-1005,W13-2322,0,0.530691,"process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov"
J18-1005,P91-1034,0,0.10154,"Missing"
J18-1005,J99-1004,0,0.0256352,"is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL"
J18-1005,P97-1003,0,0.185191,"parkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016),"
J18-1005,W95-0103,0,0.261819,"ther stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various seman"
J18-1005,P02-1001,0,0.117651,"iently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge,"
J18-1005,N16-1087,0,0.0374856,"niak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) conver"
J18-1005,P14-1134,0,0.0479906,"se representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equival"
J18-1005,P00-1065,1,0.624731,"Y-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 1. Introduction Statistical models of natural language semantics are making rapid progress. At the risk of oversimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible"
J18-1005,J99-4004,0,0.32362,"i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted DAG Automata for Semantic Graphs Unfortunately, we cannot derive a closed-form solution for the zeros of Equation (3). We therefore use gradient ascent. In CRF training for finite automata, the expectation in Equation (3) is computed efficiently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to pro"
J18-1005,N06-2015,0,0.0282756,"rsimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations,"
J18-1005,P99-1069,0,0.188893,"d Let w ∈ R be a vector of feature weights, which are the parameters to be estimated. Then we can parameterize δ in terms of the features and feature weights: δ(t) = exp w · Φ(t) so that δ(ρ ) = exp w · Φ(ρ ) X exp w · Φ(ρ ) [[M]] (D) = run ρ on D To obtain a probability model of runs of M on D, we simply renormalize the run weights: δ (ρ ) [[M]] (D) Assume a set of training examples {(Di , ρi ) |1 ≤ i ≤ N}, where each example consists of a DAG Di and an associated run ρi . We can train the model by analogy with conditional random fields (CRFs), which are log-linear models on finite automata (Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001). The training procedure is essentially gradient ascent on the log-likelihood, which is pM (ρ |D) = LL = N X log pM (ρi |Di ) i=1 = N X log δ(ρi ) − log [[M]] (Di )  i=1 The gradient of LL is: ∂LL = ∂w N  X i=1  ∂ log δ(ρ ) − ∂ log [[M]] (D ) i i ∂w ∂w N  X  1 1 ∂ ∂ = δ(ρi ) − [[M]] (Di ) δ(ρi ) ∂w [[M]] (Di ) ∂w i=1   N X X 1 ∂ δ ( ρ )  1 ∂ δ(ρi ) − = ∂w δ(ρi ) ∂w [[M]] (Di ) ρ on Di i=1   N X X δ ( ρ ) Φ(ρi ) − = Φ ( ρ ) [[M]] (Di ) i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted"
J18-1005,W15-4502,0,0.0159569,"ic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to"
J18-1005,N15-1114,0,0.0541523,"Missing"
J18-1005,J93-2004,0,0.0616988,"Missing"
J18-1005,S16-1166,0,0.0130652,"ch can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsin"
J18-1005,S15-2153,0,0.0866557,"Missing"
J18-1005,S14-2008,0,0.118213,"neration (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for research in semantic processing. What is still missing—in our view—is a formal framework for creating, combining, and using models involving graphs that parallels those for strings and trees. Finite string automata and transducers served as"
J18-1005,N15-1119,0,0.0128014,"nsolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences"
J18-1005,K15-1004,1,0.920031,"Missing"
J18-1005,P06-1055,0,0.0700356,"noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et"
J18-1005,W12-4209,0,0.402951,"lack a similar framework for learning and inferring semantic representations. Two such formalisms have recently been proposed for NLP: one is hyperedge replacement graph grammars, or HRGs (Bauderon and Courcelle 1987; Habel and Kreowski 1987; Habel 1992; Drewes, Kreowski, and Habel 1997), applied to AMR 120 Chiang et al. Weighted DAG Automata for Semantic Graphs ¨ parsing by various authors (Chiang et al. 2013; Peng, Song, and Gildea 2015; Bjorklund, Drewes, and Ericson 2016). The other formalism is directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki (1981) and extended by Quernheim and Knight (2012). In this article, we study DAG automata in depth, with the goal of enabling efficient algorithms for natural language processing applications. After some background on the use of graph-based representations in natural language processing in Section 2, we define our variant of DAG automata in Section 3. We then show the following properties of our formalism: r r r Path languages are regular, as is desirable for a formal model of AMRs (Section 4.1). The class of hyperedge-replacement languages is closed under intersection with languages recognized by DAG automata (Section 4.2). Emptiness is dec"
J18-1005,W95-0107,0,0.44496,"o be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more re"
J18-1005,W96-0213,0,0.301856,"wledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997"
J18-1005,J07-4003,0,0.0362617,"pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL = log pM (ρi , Di ) i ∂LL = ∂w X Φ(ρi ) − ED0 ,ρ [Φ(ρ )] i by using MCMC to estimate the second expectation. Finally, we may wish to learn a distribution over DAGs by learning the states in an unsupervised manner, either because it is not practical to annotate states by hand, or because we wish to automatically find the set of stat"
J18-1005,J01-4004,0,0.292972,"Missing"
J18-1005,P87-1015,0,0.781806,"Missing"
J18-1005,P15-2141,0,0.0234808,"Missing"
J18-1005,N15-1040,0,0.0381581,"Missing"
K17-3010,D14-1082,0,0.0174561,"y parsing aims to automatically extract dependencies between words in a sentence, in the form of tree structure. These dependencies define the grammatical structure of the sentence, which makes it beneficial for many natural language applications, such as question answering (Cui et al., 2005), machine translation (Carreras and Collins, 2009), and information extraction (Angeli et al., 2015). The most common approaches for dependency parsing are transitionbased (Nivre et al., 2006) or graph-based (McDonald et al., 2005). Recent works also apply neural network approaches for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2017), particularly for learning rich feature representations that improve parser accuracy. To train a high-quality parser, one typically needs a large treebank, annotated with some linguistic information, such as part of speech (POS) • We train one monolingual model per highresource treebank in the training set. • For low-resource treebanks, we train several multilingual models, each for related languages grouped by their genus and language families. 100 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw"
K17-3010,Q16-1031,0,0.223493,"uages, such as English, Czech, or Chinese. The Universal Dependencies (UD; Nivre et al. (2016)) is an initiative to develop consistent treebank annotations across many languages. It provides an opportunity to perform model transfer – using model trained on high-resource languages to parse low-resource languages, allowing the development of treebanks for many more languages. Several works (McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a,b; Guo et al., 2015, 2016) have shown that this technique can help improve accuracy for low-resource languages, and in fact recent work of Ammar et al. (2016) demonstrated that it is possible to train a single multilingual model that works well both in low-resource and high-resource settings. The CoNLL 2017 UD Shared Task (Zeman et al., 2017) uses Universal Dependencies version 2.0 (Nivre et al., 2017), with training data consists of 64 treebanks from 45 languages. Some of the challenges are the truly low-resource treebanks (e.g., Kazakh and Uyghur with only 30 and 100 training sentences, respectively), small treebanks without development data (e.g., Irish, FrenchParTUT, Galician-TreeGal, Ukrainian), and the surprise languages and treebanks needed"
K17-3010,P15-2139,0,0.125254,"d.ac.uk Abstract tags, lemmas, and morphological features. However, human annotations are expensive. As a result, most of the work has been focused on few languages, such as English, Czech, or Chinese. The Universal Dependencies (UD; Nivre et al. (2016)) is an initiative to develop consistent treebank annotations across many languages. It provides an opportunity to perform model transfer – using model trained on high-resource languages to parse low-resource languages, allowing the development of treebanks for many more languages. Several works (McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a,b; Guo et al., 2015, 2016) have shown that this technique can help improve accuracy for low-resource languages, and in fact recent work of Ammar et al. (2016) demonstrated that it is possible to train a single multilingual model that works well both in low-resource and high-resource settings. The CoNLL 2017 UD Shared Task (Zeman et al., 2017) uses Universal Dependencies version 2.0 (Nivre et al., 2017), with training data consists of 64 treebanks from 45 languages. Some of the challenges are the truly low-resource treebanks (e.g., Kazakh and Uyghur with only 30 and 100 training sentences, re"
K17-3010,P15-1034,0,0.01771,"languages, grouped by their genus and language families. Out of 33 participants, our system achieves rank 9th in the main results, with 75.49 UAS and 68.87 LAS F-1 scores (average across 81 treebanks). 1 Introduction Dependency parsing aims to automatically extract dependencies between words in a sentence, in the form of tree structure. These dependencies define the grammatical structure of the sentence, which makes it beneficial for many natural language applications, such as question answering (Cui et al., 2005), machine translation (Carreras and Collins, 2009), and information extraction (Angeli et al., 2015). The most common approaches for dependency parsing are transitionbased (Nivre et al., 2006) or graph-based (McDonald et al., 2005). Recent works also apply neural network approaches for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2017), particularly for learning rich feature representations that improve parser accuracy. To train a high-quality parser, one typically needs a large treebank, annotated with some linguistic information, such as part of speech (POS) • We train one monolingual model per highresource treebank in the tra"
K17-3010,D15-1040,0,0.0190498,"d.ac.uk Abstract tags, lemmas, and morphological features. However, human annotations are expensive. As a result, most of the work has been focused on few languages, such as English, Czech, or Chinese. The Universal Dependencies (UD; Nivre et al. (2016)) is an initiative to develop consistent treebank annotations across many languages. It provides an opportunity to perform model transfer – using model trained on high-resource languages to parse low-resource languages, allowing the development of treebanks for many more languages. Several works (McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a,b; Guo et al., 2015, 2016) have shown that this technique can help improve accuracy for low-resource languages, and in fact recent work of Ammar et al. (2016) demonstrated that it is possible to train a single multilingual model that works well both in low-resource and high-resource settings. The CoNLL 2017 UD Shared Task (Zeman et al., 2017) uses Universal Dependencies version 2.0 (Nivre et al., 2017), with training data consists of 64 treebanks from 45 languages. Some of the challenges are the truly low-resource treebanks (e.g., Kazakh and Uyghur with only 30 and 100 training sentences, re"
K17-3010,D15-1041,0,0.0374793,"Missing"
K17-3010,P15-1033,0,0.0221676,"tically extract dependencies between words in a sentence, in the form of tree structure. These dependencies define the grammatical structure of the sentence, which makes it beneficial for many natural language applications, such as question answering (Cui et al., 2005), machine translation (Carreras and Collins, 2009), and information extraction (Angeli et al., 2015). The most common approaches for dependency parsing are transitionbased (Nivre et al., 2006) or graph-based (McDonald et al., 2005). Recent works also apply neural network approaches for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2017), particularly for learning rich feature representations that improve parser accuracy. To train a high-quality parser, one typically needs a large treebank, annotated with some linguistic information, such as part of speech (POS) • We train one monolingual model per highresource treebank in the training set. • For low-resource treebanks, we train several multilingual models, each for related languages grouped by their genus and language families. 100 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal D"
K17-3010,N13-1073,0,0.0196694,"r multilingual word embeddings. The idea is to train word embeddings of a source language and project them to obtain word embeddings for the target languages. For the shared task, we use English pre-trained word vectors trained on the Wikipedia data (Bojanowski et al., 2016) as our source embeddings. Next, we use OPUS data (Tiedemann, 2012, 2009) to build alignment dictionaries for languages that have parallel text with English. Specifically, we use parallel corpora of Europarl, Global Voices, Wikipedia, and hrWaC (for Croatian). To build the alignment dictionaries, we use fast align toolkit (Dyer et al., 2013). We then compute vector for each target word using the weighted average of its aligned English word embeddings, weighted by the alignment probabilities. A limitation of this approach is that it creates embeddings for target words that appear in the parallel data. Thus, the final step of this approach also compute embeddings for other target words not aligned with the source words by averaging the embeddings of all aligned target words within an edit distance of 1. The token level embeddings are shared across languages. UParse Next, we describe UParse, the extended version of DENSE which we us"
K17-3010,P96-1011,0,0.0306294,"5). The model is trained to minimize the negative log likelihood of the gold standard hhead, dependenti arcs of all the training sentences. At test time, the parser greedily choose the most probable head for each word in the sentence. Adjusting Tree Outputs. In many cases, the individual predictions form a tree. However, if this is not the case, a maximum spanning tree (MST) algorithm is used to constrain the set of predictions to form a tree. DENSE can use two algorithms: Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) algorithm to generating nonprojective trees; and the Eisner algorithm (Eisner, 1996) to generate projective trees. The decision of the MST algorithms depends on the language’s treebank. For the shared task, we assume that each language can produce non-projective trees. Token Representations. In the first step, the parser computes the representation of each word in the sentence. The objective is to encode both local (lexical meaning and POS tag) and global information (word position and context). To do this, the parser uses a bidirectional LSTM (biLSTMs), which have shown to be effective in capturing long-term dependencies. More formally, let 1 As the convention in dependency"
K17-3010,P15-1119,0,0.0834855,"lemmas, and morphological features. However, human annotations are expensive. As a result, most of the work has been focused on few languages, such as English, Czech, or Chinese. The Universal Dependencies (UD; Nivre et al. (2016)) is an initiative to develop consistent treebank annotations across many languages. It provides an opportunity to perform model transfer – using model trained on high-resource languages to parse low-resource languages, allowing the development of treebanks for many more languages. Several works (McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a,b; Guo et al., 2015, 2016) have shown that this technique can help improve accuracy for low-resource languages, and in fact recent work of Ammar et al. (2016) demonstrated that it is possible to train a single multilingual model that works well both in low-resource and high-resource settings. The CoNLL 2017 UD Shared Task (Zeman et al., 2017) uses Universal Dependencies version 2.0 (Nivre et al., 2017), with training data consists of 64 treebanks from 45 languages. Some of the challenges are the truly low-resource treebanks (e.g., Kazakh and Uyghur with only 30 and 100 training sentences, respectively), small tr"
K17-3010,L16-1262,0,0.0862342,"Missing"
K17-3010,Q16-1023,0,0.0546756,"endencies between words in a sentence, in the form of tree structure. These dependencies define the grammatical structure of the sentence, which makes it beneficial for many natural language applications, such as question answering (Cui et al., 2005), machine translation (Carreras and Collins, 2009), and information extraction (Angeli et al., 2015). The most common approaches for dependency parsing are transitionbased (Nivre et al., 2006) or graph-based (McDonald et al., 2005). Recent works also apply neural network approaches for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2017), particularly for learning rich feature representations that improve parser accuracy. To train a high-quality parser, one typically needs a large treebank, annotated with some linguistic information, such as part of speech (POS) • We train one monolingual model per highresource treebank in the training set. • For low-resource treebanks, we train several multilingual models, each for related languages grouped by their genus and language families. 100 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 100–110, c 20"
K17-3010,W17-0411,0,0.0491363,"these results, some possible directions for the future work include improving the label predictions of the parsing model and exploring the possibilities to use character-level models, as they have shown to be effective for parsing morphologically rich languages (Ballesteros et al., 2015). Another interesting direction is to combine 1. UP-1: UParse + DIST + UDPipe 2. UP-2: UParse + DIST 3. UP-3: UParse + RAND Table 6 shows the macro-averaged LAS F1 scores for all the systems. 5.3 Score 68.87 75.49 63.55 Results on Test Data Table 8 shows the results of our primary system of LAS, UAS, and CLAS (Nivre and Fang, 2017). The more detailed results for each treebank and system is given in Table 9. Similar to the results 106 Treebank Code ar pud ar bg* bxr ca cs cac cs cltt* cs pud cs cu* da de pud de el* en lines en partut* en pud en es ancora es pud es et* eu fa fi ftb* fi pud fi fr partut* fr pud fr sequoia fr ga* gl treegal gl got grc proiel grc he hi pud hi hr* LAS F-1 score UP-1 UP-2 UP-3 45.3 45.3 45.3 66.35 66.35 66.3 83.64 83.46 83.46 21.63 21.63 21.63 86.8 86.8 86.8 85.57 85.57 85.57 71.64 66.74 66.37 81.06 81.06 81.06 85.24 85.24 85.24 62.76 64.24 64.24 73.46 73.46 73.46 67.36 67.36 67.36 70.09 70.09"
K17-3010,P05-1012,0,0.0318792,"s, with 75.49 UAS and 68.87 LAS F-1 scores (average across 81 treebanks). 1 Introduction Dependency parsing aims to automatically extract dependencies between words in a sentence, in the form of tree structure. These dependencies define the grammatical structure of the sentence, which makes it beneficial for many natural language applications, such as question answering (Cui et al., 2005), machine translation (Carreras and Collins, 2009), and information extraction (Angeli et al., 2015). The most common approaches for dependency parsing are transitionbased (Nivre et al., 2006) or graph-based (McDonald et al., 2005). Recent works also apply neural network approaches for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2017), particularly for learning rich feature representations that improve parser accuracy. To train a high-quality parser, one typically needs a large treebank, annotated with some linguistic information, such as part of speech (POS) • We train one monolingual model per highresource treebank in the training set. • For low-resource treebanks, we train several multilingual models, each for related languages grouped by their genus an"
K17-3010,W06-2933,0,0.0593856,"Missing"
K17-3010,D11-1006,0,0.0673459,"inburgh {c.vania, x.zhang}@ed.ac.uk, alopez@inf.ed.ac.uk Abstract tags, lemmas, and morphological features. However, human annotations are expensive. As a result, most of the work has been focused on few languages, such as English, Czech, or Chinese. The Universal Dependencies (UD; Nivre et al. (2016)) is an initiative to develop consistent treebank annotations across many languages. It provides an opportunity to perform model transfer – using model trained on high-resource languages to parse low-resource languages, allowing the development of treebanks for many more languages. Several works (McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a,b; Guo et al., 2015, 2016) have shown that this technique can help improve accuracy for low-resource languages, and in fact recent work of Ammar et al. (2016) demonstrated that it is possible to train a single multilingual model that works well both in low-resource and high-resource settings. The CoNLL 2017 UD Shared Task (Zeman et al., 2017) uses Universal Dependencies version 2.0 (Nivre et al., 2017), with training data consists of 64 treebanks from 45 languages. Some of the challenges are the truly low-resource treebanks (e.g., Kazakh and Uygh"
K17-3010,E17-1063,1,0.878963,"tence, in the form of tree structure. These dependencies define the grammatical structure of the sentence, which makes it beneficial for many natural language applications, such as question answering (Cui et al., 2005), machine translation (Carreras and Collins, 2009), and information extraction (Angeli et al., 2015). The most common approaches for dependency parsing are transitionbased (Nivre et al., 2006) or graph-based (McDonald et al., 2005). Recent works also apply neural network approaches for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2017), particularly for learning rich feature representations that improve parser accuracy. To train a high-quality parser, one typically needs a large treebank, annotated with some linguistic information, such as part of speech (POS) • We train one monolingual model per highresource treebank in the training set. • For low-resource treebanks, we train several multilingual models, each for related languages grouped by their genus and language families. 100 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 100–110, c 2017 Association for Co"
K17-3010,D15-1213,0,0.132248,"Missing"
K17-3010,L16-1680,0,0.0907858,"Missing"
K17-3010,tiedemann-2012-parallel,0,0.408022,"epresentations of both words, [ai ; aj ; xi ; xj ] and predicts a valid dependency label. Zhang et al. (2017) presents more detailed account of the parsing model. 2.2 2.3 Following Ammar et al. (2016), we adapt the robust projection approach of Guo et al. (2016) to build our multilingual word embeddings. The idea is to train word embeddings of a source language and project them to obtain word embeddings for the target languages. For the shared task, we use English pre-trained word vectors trained on the Wikipedia data (Bojanowski et al., 2016) as our source embeddings. Next, we use OPUS data (Tiedemann, 2012, 2009) to build alignment dictionaries for languages that have parallel text with English. Specifically, we use parallel corpora of Europarl, Global Voices, Wikipedia, and hrWaC (for Croatian). To build the alignment dictionaries, we use fast align toolkit (Dyer et al., 2013). We then compute vector for each target word using the weighted average of its aligned English word embeddings, weighted by the alignment probabilities. A limitation of this approach is that it creates embeddings for target words that appear in the parallel data. Thus, the final step of this approach also compute embeddi"
N13-1033,brown-2004-modified,0,0.0344034,"a source sentence) and their translations are numerous, both phrase lookup and extraction are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: Firs"
N13-1033,P05-1032,0,0.022765,"teger identifier; with 32-bit integers the total number of bytes is 16|T |. As we will show, this turns out to be quite modest, even for large parallel corpora (§6). 3.2 Suffix Array Efficiency Tricks Previous work on translation by pattern matching using suffix arrays on serial architectures has produced a number of efficiency optimizations: 1. Binary search bounds for longer substrings are initialized to the bounds of their longest prefix. Substrings are queried only if their longest prefix string was matched in the text. 2. In addition to conditioning on the longest prefix, Zhang and Vogel (2005) and Lopez (2007) condition on a successful query for the longest proper suffix. 3. Lopez (2007) queries each unique substring of a sentence exactly once, regardless of how many times it appears in an input sentence. 4. Lopez (2007) directly indexes one-word substrings with a small auxiliary array, so that their positions in the suffix array can be found in constant time. For longer substrings, this optimization reduces the log |T |term of query complexity to log(count(a)), where a is the first word of the query string. Although these efficiency tricks are important in the serial algorithms th"
N13-1033,J07-2003,0,0.0736494,"ications latency is also important. One current limitation of our work is that large batch sizes are necessary to fully utilize the available processing power of the GPU. This and other properties of the GPU, such as the high latency involved in transferring data from main memory to GPU memory, make low-latency processing a challenge, which we hope to address. Another broad future direction is to “GPU-ify” other machine translation models and other com333 ponents in the machine translation pipeline. An obvious next step is to extend our work to the hierarchical phrase-based translation model (Chiang, 2007), which would involve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for spe"
N13-1033,D11-1047,0,0.025636,"Missing"
N13-1033,P10-4002,1,0.838649,"has 448 CUDA cores with a peak memory bandwidth 144 GB/s. Note that the GPU was released in early 2010 and represents previous generation technology. NVIDIA’s current GPUs (Kepler) boasts raw processing power in the 1.3 TFlops (double precision) range, which is approximately three times the GPU we used. Our CPU is a 3.33 GHz Intel Xeon X5260 processor, which has two cores. As a baseline, we compared against the publicly available implementation of the CPU-based algorithms described by Lopez (2008a) found in the pycdec (Chahuneau et al., 2012) extension of the cdec machine translation system (Dyer et al., 2010). Note that we only tested grammar extraction for continuous pairs of phrases, and we did not test the slower and more complex queries for hierarchical Input Sentences Number of Words With Sampling (s300 ) No Sampling (s∞ ) With Sampling (s300 ) No Sampling (s∞ ) 2,000 4,000 6,000 8,000 57,868 117,854 161,883 214,246 Xinhua 3811 4723 5496 6391 GPU (words/second) (21.9) (20.4) (32.1) (29.7) CPU (words/second) 200 (1.5) Speedup 19× 24× 27× 32× 1917 2859 3496 4171 GPU (words/second) (8.5) (11.1) (19.9) (23.2) CPU (words/second) 1.13 (0.02) Speedup 1690× 2520× 3082× 3677× Xinhua + UN 2021 2558 293"
N13-1033,D07-1104,1,0.943616,"e considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism (§3). Second, we propose no"
N13-1033,C08-1064,1,0.90296,"ur work is that large batch sizes are necessary to fully utilize the available processing power of the GPU. This and other properties of the GPU, such as the high latency involved in transferring data from main memory to GPU memory, make low-latency processing a challenge, which we hope to address. Another broad future direction is to “GPU-ify” other machine translation models and other com333 ponents in the machine translation pipeline. An obvious next step is to extend our work to the hierarchical phrase-based translation model (Chiang, 2007), which would involve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and"
N13-1033,W99-0604,0,0.283873,"esident occupancy. To fully take advantage of the processing power, we process multiple input sentences in parallel. Compared with previous algorithms, our two-pass approach and our strategy of thread assignment to increase the amount of parallelism represent novel contributions. 4 Extracting Aligned Target Phrases The problem at line 5 of Algorithm 1 is to extract the target phrase aligned to each matching source phrase instance. Efficiency is crucial since some source phrases occur hundreds of thousands of times. Phrase extraction from word alignments typically uses the consistency check of Och et al. (1999). A consistent phrase is one for which no words inside the phrase pair are aligned to words outside the phrase pair. Usually, consistent pairs are computed offline via dynamic programming over the alignment grid, from which we extract all consistent phrase pairs up to a heuristic bound on phrase length. The online extraction algorithm of Lopez (2008a) checks for consistent phrases in a different manner. Rather than finding all consistent phrase pairs in a sentence, the algorithm asks: given a specific source phrase, is there a consistent phrase pair 328 Figure 1: Source phrase f2 f3 f4 and tar"
N13-1033,W11-2921,0,0.127121,"ckled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and parsing (Yi et al., 2011), and we have demonstrated here that it extends to machine translation as well. We believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle. Parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today. Acknowledgments This research was supported in part by the BOLT program of the Defense Advance"
N13-1033,2005.eamt-1.39,0,0.597781,"on are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism"
N15-3008,W13-2322,0,0.0736058,"n AMR or the difference between two AMRs to help users diagnose interannotator disagreement or errors from an AMR parser. AMRICA can also automatically align and visualize the AMRs of a sentence and its translation in a parallel text. We believe AMRICA will simplify and streamline exploratory research on cross-lingual AMR corpora. 1 Introduction Research in statistical machine translation has begun to turn to semantics. Effective semantics-based translation systems pose a crucial need for a practical cross-lingual semantic representation. One such schema, Abstract Meaning Representation (AMR; Banarescu et al., 2013), has attracted attention for its simplicity and expressive power. AMR represents the meaning of a sentence as a directed graph over concepts representing entities, events, and properties like names or quantities. Concepts are represented by nodes and are connected by edges representing relations—roles or attributes. Figure 1 shows an example of the AMR annotation format, which is optimized for text entry rather than human comprehension. For human analysis, we believe it is easier to visualize the AMR graph. We present AMRICA, a sys36 Adam Lopez School of Informatics University of Edinburgh Ed"
N15-3008,P13-2131,0,0.349598,"MR for “I’ve been in New Zealand the past two weeks.” (Linguistic Data Consortium, 2013) tem for visualizing AMRs in three conditions. First, AMRICA can display AMRs as in Figure 2. Second, AMRICA can visualize differences between aligned AMRs of a sentence, enabling users to diagnose differences in multiple annotations or between an annotation and an automatic AMR parse (Section 2). Finally, to aid researchers studying crosslingual semantics, AMRICA can visualize differences between the AMR of a sentence and that of its translation (Section 3) using a novel cross-lingual extension to Smatch (Cai and Knight, 2013). The AMRICA code and a tutorial are publicly available.1 2 Interannotator Agreement AMR annotators and researchers are still exploring how to achieve high interannotator agreement (Cai and Knight, 2013). So it is useful to visualize a pair of AMRs in a way that highlights their disagreement, as in Figure 3. AMRICA shows in black those nodes and edges which are shared between the annotations. Elements that differ are red if they appear in one AMR and blue if they appear in the other. This feature can also be used to explore output from an 1 http://github.com/nsaphra/AMRICA Proceedings of NAACL"
N15-3008,P14-1134,0,0.0205624,"to control how it weights these estimated likelihoods relative to exact matches of relation and concept labels. L(v, v 0 ) = α n X Pr(as (v) = i)× (3) i=1 0 n X Pr(ai = j) · Pr(at (v 0 ) = j) j=1 Node-to-word probabilities Pr(as (v) = i) and Pr(as (v 0 ) = j) are computed as described in Section 3.1. Word-to-word probabilities Pr(ai = j) 38 are computed as described in Section 3.2. AMRICA uses the Smatch hill-climbing algorithm to yield alignments like that in Figure 4. 3.1 Node-to-word and word-to-node alignment AMRICA can accept node-to-word alignments as output by the heuristic aligner of Flanigan et al. (2014).3 In this case, the tokens in the aligned span receive uniform probabilities over all nodes in their aligned subgraph, while all other token-node alignments receive probability 0. If no such alignments are provided, AMRICA aligns concept nodes to tokens matching the node’s label, if they exist. A token can align to multiple nodes, and a node to multiple tokens. Otherwise, alignment probability is uniformly distributed across unaligned nodes or tokens. 3.2 Word-to-word Alignment AMRICA computes the posterior probability of the alignment between the ith word of the source and jth word of the ta"
N15-3008,xue-etal-2014-interlingua,0,0.0859945,"Missing"
N15-3008,J03-1002,0,0.00762596,"all other token-node alignments receive probability 0. If no such alignments are provided, AMRICA aligns concept nodes to tokens matching the node’s label, if they exist. A token can align to multiple nodes, and a node to multiple tokens. Otherwise, alignment probability is uniformly distributed across unaligned nodes or tokens. 3.2 Word-to-word Alignment AMRICA computes the posterior probability of the alignment between the ith word of the source and jth word of the target as an equal mixture between the posterior probabilities of source-to-target and targetto-source alignments from GIZA++ (Och and Ney, 2003).4 To obtain an approximation of the posterior probability in each direction, it uses the m(k) best alignments a(1) . . . a(m) , where ai = j indicates that the ith source word aligns to the jth target word in the kth best alignment, and Pr(a(k) ) is the probability of the kth best alignment according to GIZA++. We then approximate the posterior probability as follows. Pr(ai = j) = 4 Pm (k) (k) k=1 Pr(a )I[ai Pm (k) k=1 Pr(a ) = j] Demonstration Script AMRICA makes AMRs accessible for data exploration. We will demonstrate all three capabilities outlined above, allowing participants to visually"
N15-3008,D14-1048,0,0.0119885,"est alignment according to GIZA++. We then approximate the posterior probability as follows. Pr(ai = j) = 4 Pm (k) (k) k=1 Pr(a )I[ai Pm (k) k=1 Pr(a ) = j] Demonstration Script AMRICA makes AMRs accessible for data exploration. We will demonstrate all three capabilities outlined above, allowing participants to visually explore AMRs using graphics much like those in Figures 2, 3, and 4, which were produced by AMRICA. We will then demonstrate how AMRICA can be used to generate a preliminary alignment for bitext 3 Another option for aligning AMR graphs to sentences is the statistical aligner of Pourdamghani et al. (2014) 4 In experiments, this method was more reliable than using either alignment alone. Figure 5: Cross-lingual AMR example from Nianwen Xue et al. (2014). The node-to-node alignment of the highlighted nodes is computed using the node-to-word, word-to-word, and node-to-word alignments indicated by green dashed lines. AMRs, which can be corrected by hand to provide training data or a gold standard alignment. Information to get started with AMRICA is available in the README for our publicly available code. Acknowledgments This research was supported in part by the National Science Foundation (USA) u"
N18-1106,D15-1198,0,0.0279274,"lso pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for example, :location"
N18-1106,W13-2322,1,0.877311,"der Linguistics and Computer Science Georgetown University Washington, DC, USA {k.i.szubert@sms, alopez@inf}.ed.ac.uk nathan.schneider@georgetown.edu Abstract 1 cat My sun the c cat s sun i i Figure 1: “My cat lies in the sun.” An alignment between the dependency parse (left) and AMR (right). Nodes participating in lexical alignments are marked with boxes, but the links between them are not displayed. Structural alignments are colour-coded and linked by dotted lines. Sense numbers for concepts that are PropBank frames are omitted for brevity. Introduction Abstract Meaning Representation (AMR; Banarescu et al., 2013) is a popular framework for annotating whole sentence meaning. An AMR annotation is a directed, usually acyclic graph in which nodes represent entities and events, and edges represent relations between them, as on the right in figure 1.1 AMR annotations include no explicit mapping between elements of an AMR and the corresponding elements of the sentence that evoke them, and this presents a challenge to developers of machine learning systems that parse sentences to AMR or generate sentences from AMR, since they must 1 l lie lies Abstract Meaning Representation (AMR) annotations are often assume"
N18-1106,W15-0128,0,0.0385416,"9 8.7 13.1 13.4 5.8 13.2 15.2 total: 66 11.6 2:2 2:3 3:2 3:4 3:3 other # sents avg. words 21 14 13 12 10 64 12.9 16.0 16.8 20.3 19.1 20.9 134 18.0 Table 1: Number of sentences whose highest alignment configurations is max config. 2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017). However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style. Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence. Are AMRs and dependency graphs structurally similar? We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence. We define the size of a subgraph as the number of edges it contains. If a structure consists of a single node, we say its size is 0. The configuration of an alignment is then the pair of sizes for its AMR and DG sides; for example, an alignment with 1 AMR edge and"
N18-1106,P13-2131,0,0.116592,"Missing"
N18-1106,D14-1082,0,0.0382869,"Missing"
N18-1106,E17-1053,0,0.290687,". By principle of minimality we infer that some structural difference between the graphs prevented those relations from aligning individually. One measure of similarity between AMR and DG graphs is the configuration of the most complex subgraph alignment between them. Configuration a:b is higher than c:d if a + b > c + d. However, all configurations involving 0 are lower than those which do not. A maximum of 1:1 means the graphs have only node-to-node, node-to-edge, and edge-toedge alignments, rendering the graphs isomorphic (ignoring edge directions and unaligned nodes). In 11 In particular, Chen and Palmer (2017) align dependency paths to AMR edges. However, their evaluation only considers node-to-node alignment, and their code and data are not available for comparison at the time of this writing. 1174 named entities 2:0 3:1 4:2 1:1 5:2 other 112 44 7 6 4 20 total: 193 coordination semantic decomposition quantities & dates 2:2 3:4 3:3 4:3 3:2 other 1:0 2:0 2:1 4:1 3:1 other 2:1 3:0 1:0 3:2 8:2 other 30 14 13 5 5 50 117 32 14 11 6 6 15 84 15 5 4 3 1 0 28 other 0:0 1:1 1:2 1:0 2:2 other 1946 1002 220 42 42 13 3385 overall 0:0 1:1 1:2 2:0 2:2 other 1946 1046 244 127 83 361 3807 Table 2: Frequency of alig"
N18-1106,E17-1051,0,0.0396558,"inding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1. So edge alignments allow ISI to explain more of the AMR structure tha"
N18-1106,N16-1087,0,0.0731189,"Missing"
N18-1106,P14-1134,0,0.748519,"tations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic"
N18-1106,C14-1111,0,0.0588838,"Missing"
N18-1106,P17-1104,0,0.0252127,". sents complex configurations avg. max words config. 1:1 1:2 3:1 2:0 1:3 other 18 16 12 6 5 9 8.7 13.1 13.4 5.8 13.2 15.2 total: 66 11.6 2:2 2:3 3:2 3:4 3:3 other # sents avg. words 21 14 13 12 10 64 12.9 16.0 16.8 20.3 19.1 20.9 134 18.0 Table 1: Number of sentences whose highest alignment configurations is max config. 2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017). However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style. Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence. Are AMRs and dependency graphs structurally similar? We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence. We define the size of a subgraph as the number of edges it contains. If a structure consists of a single node, we say its size is 0. The configuration of an alignment is then th"
N18-1106,P17-1014,0,0.121621,"Missing"
N18-1106,P14-5010,0,0.00248589,"it, and show that this new task is much more difficult than lexical alignment (§4). We also show how our data can be used to analyze errors made by an AMR parser (§5). We make our annotated data and aligner freely available for further research.3 2 Aligning AMR to dependency syntax Our syntactic representation is dependency grammar, which represents the sentence as a rooted, directed graph where nodes are words and edges are grammatical relations between them (Kruijff, 2006). We use Universal Dependencies (UD), a cross-lingual dependency annotation scheme, as implemented in Stanford CoreNLP (Manning et al., 2014). Within the UD framework, we use enhanced dependencies (Schuster and Manning, 2016), in which dependents can have more than one head, 3 https://github.com/ida-szubert/amr_ud resulting in dependency graphs (DGs).4 Our alignment guidelines generalize ideas present in the existing frameworks. We want to allow many-to-many alignments, which we motivate by the observation that some phenomena cause an AMR graph to have one structure expressing the same information as multiple DG structures, and vice versa. For instance, in figure 2 the AMR subgraph representing Cruella de Vil aligns to two subgraph"
N18-1106,D16-1183,0,0.0380923,"MR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1. So edge alignments allow ISI to explain more of"
N18-1106,S14-2008,0,0.0730881,"Missing"
N18-1106,E17-1035,0,0.0305119,"Missing"
N18-1106,D14-1048,0,0.320962,"should not be assumed that AMR and DG representations of a sentence are, or could trivially be made to be, isomorphic. It is worth noting that our analysis suggests that DG and AMR could be made more similar by applying simple transformations targeting problematic constructions like coordination and named entities. 4 Evaluation of automatic aligners We use our annotations to measure the accuracy of AMR aligners on specific phenomena that were inexpressible in previous annotation schemes. Our experiments evaluate the JAMR heuristic aligner (Flanigan et al., 2014), the ISI statistical aligner (Pourdamghani et al., 2014), and a heuristic rulebased aligner that we developed specifically for 12 An AMR concept evoked by a preposition usually dominates the structure (afterÐÐ→date-entityÐÐÐ→nineties), which is at odds with UD’s prepositions-as-case-markers polcase icy (ninetiesÐÐ→after). op1 decade structural alignment. 4.1 Rule-based aligner Our aligner operates in two passes: one for lexical alignment and one for structural alignment. Lexical alignment algorithm. AMR concepts are cognate with English words, so we align them by lexical similarity. This algorithm does not make use of the DG. Before alignment, we r"
N18-1106,W16-6603,0,0.116843,"Missing"
N18-1106,Q16-1010,0,0.0372812,"s. 10 http://universaldependencies.org/u/overview/ enhanced-syntax.html max # config. sents complex configurations avg. max words config. 1:1 1:2 3:1 2:0 1:3 other 18 16 12 6 5 9 8.7 13.1 13.4 5.8 13.2 15.2 total: 66 11.6 2:2 2:3 3:2 3:4 3:3 other # sents avg. words 21 14 13 12 10 64 12.9 16.0 16.8 20.3 19.1 20.9 134 18.0 Table 1: Number of sentences whose highest alignment configurations is max config. 2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017). However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style. Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence. Are AMRs and dependency graphs structurally similar? We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence. We define the size of a subgraph as the number of edges it contains. If a structure co"
N18-1106,L16-1376,0,0.0294195,"(§4). We also show how our data can be used to analyze errors made by an AMR parser (§5). We make our annotated data and aligner freely available for further research.3 2 Aligning AMR to dependency syntax Our syntactic representation is dependency grammar, which represents the sentence as a rooted, directed graph where nodes are words and edges are grammatical relations between them (Kruijff, 2006). We use Universal Dependencies (UD), a cross-lingual dependency annotation scheme, as implemented in Stanford CoreNLP (Manning et al., 2014). Within the UD framework, we use enhanced dependencies (Schuster and Manning, 2016), in which dependents can have more than one head, 3 https://github.com/ida-szubert/amr_ud resulting in dependency graphs (DGs).4 Our alignment guidelines generalize ideas present in the existing frameworks. We want to allow many-to-many alignments, which we motivate by the observation that some phenomena cause an AMR graph to have one structure expressing the same information as multiple DG structures, and vice versa. For instance, in figure 2 the AMR subgraph representing Cruella de Vil aligns to two subgraphs in the dependency graph because of pronominal coreference. In the other direction,"
N18-1106,N15-1040,0,0.0450682,"syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for"
N18-1106,D16-1177,0,0.131476,"Missing"
N19-1006,L18-1531,0,0.101856,"Missing"
N19-1006,D16-1263,0,0.026857,"ge differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. 1 Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable 58 Proceedings of NAACL-HLT 2019, pages 58–68 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We first test our approach using Spanish as the source language and Engl"
N19-1006,W17-0123,0,0.0192522,"urce and target ST languages: pre-training on French ASR also improves Spanish-English ST. Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. 1 Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable 58 Proceedings of NAACL-HLT 2019, pages 58–68 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We first test our approach using Spanish as the source language and English as the target. After training an"
N19-1006,N18-1008,0,0.180871,"plications, there may be some source language audio paired with target language text translations. In these scenarios, end-to-end ST is appealing. Recently, Weiss et al. (2017) showed that endto-end ST can be very effective, achieving an impressive BLEU score of 47.3 on Spanish-English ST. But this result required over 150 hours of translated audio for training, still a substantial resource requirement. By comparison, a similar system trained on only 20 hours of data for the same task achieved a BLEU score of 5.3 (Bansal et al., 2018). Other low-resource systems have similarly low accuracies (Anastasopoulos and Chiang, 2018; B´erard et al., 2018). To improve end-to-end ST in low-resource settings, we can try to leverage other data resources. For example, if we have transcribed audio in the source language, we can use multi-task learning to improve ST (Anastasopoulos and Chiang, 2018; Weiss et al., 2017; B´erard et al., 2018). But source language transcriptions are unlikely to be available in our scenarios of interest. Could we improve low-resource ST by leveraging data from a high-resource language? For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b;"
N19-1006,Q17-1024,0,0.0580446,"Missing"
N19-1006,D17-1039,0,0.0440099,"Missing"
N19-1006,P07-2045,0,0.0124442,"Missing"
N19-1006,P16-1162,0,0.129045,"Missing"
N19-1006,W07-0734,0,0.0797966,"models.5 In low-resource settings, BLEU scores tend to be low, difficult to interpret, and poorly correlated with model performance. This is because BLEU requires exact four-gram matches only, but low four-gram accuracy may obscure a high unigram accuracy and inexact translations that partially capture the semantics of an utterance, and these can still be very useful in situations like language documentation and crisis response. Therefore, we also report word-level unigram precision and recall, taking into account stem, synonym, and paraphrase matches. To compute these scores, we use METEOR (Lavie and Agarwal, 2007) with default settings for English and French.6 For example, METEOR assigns “eat” a recall of 1 against reference “eat” and a recall of 0.8 against reference “feed”, which it considers a synonym match. 5 Spanish-English ST In the following, we denote an ST model by S-TNh, where S and T are source and target language codes, and N is the size of the training set in hours. For example, sp-en-20h denotes a Spanish-English ST model trained using 20 hours of data. We use the code mb for Mboshi and fr for French. Naive baselines. We also include evaluation scores for a naive baseline model that predi"
N19-1006,D15-1166,0,0.0599554,"Missing"
N19-1006,2010.amta-workshop.1,0,0.0217312,"e on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU. 1 Introduction Speech-to-text Translation (ST) has many potential applications for low-resource languages: for example in language documentation, where the source language is often unwritten or endangered (Besacier et al., 2006; Martin et al., 2015; Adams et al., 2016a,b; Anastasopoulos and Chiang, 2017); or in crisis relief, where emergency workers might need to respond to calls or requests in a foreign language (Munro, 2010). Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), and thus requires transcribed source audio to train ASR and parallel text to train MT. These resources are often unavailable 58 Proceedings of NAACL-HLT 2019, pages 58–68 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics We first test our approach using Spanish as the source language and English as the target. After training an ASR system on 300 hours of English, fine-tuning on 20 hours of Spanish-English yields a BLEU score of 20.2, compared to only"
N19-1006,P02-1040,0,0.103413,"Missing"
N19-1006,2013.iwslt-papers.14,1,0.888041,"ariety of Spanish dialects, split into 140K utterances. To simulate low-resource conditions, we construct smaller train1 Using a shared vocabulary of characters or subwords is an interesting direction for future work, but not explored here. 59 ing corpora consisting of 50, 20, 10, 5, or 2.5 hours of data, selected at random from the full training data. The development and test sets each consist of around 4.5 hours of speech, split into 4K utterances. We do not use the corresponding Spanish transcripts; our target text consists of English translations that were collected through crowdsourcing (Post et al., 2013, 2014). quency of character sequences, so it must be computed with respect to a specific corpus. For English, we use the full 160-hour Spanish-English ST target training text. For French, we use the Mboshi-French ST target training text. 3.3 Model architecture for ASR and ST Speech encoder. As shown schematically in Figure 1, MFCC feature vectors, extracted using a window size of 25 ms and a step size of 10ms, are fed into a stack of two CNN layers, with 128 and 512 filters with a filter width of 9 frames each. In each CNN layer we stride with a factor of 2 along time, apply a ReLU activation"
N19-1006,D16-1163,0,0.032263,"sopoulos and Chiang, 2018; Weiss et al., 2017; B´erard et al., 2018). But source language transcriptions are unlikely to be available in our scenarios of interest. Could we improve low-resource ST by leveraging data from a high-resource language? For ASR, training a single model on multiple languages can be effective for all of them (Toshniwal et al., 2018b; Deng et al., 2013). For MT, transfer learning (Thrun, 1995) has been very effective: pretraining a model for a high-resource language pair and transferring its parameters to a low-resource language pair when the target language is shared (Zoph et al., 2016; Johnson et al., 2017). Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, simply by pre-training a model for the high-resource ASR task, and then transferring and fine-tuning some or all of the model’s parameters for low-resource ST. We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST. We"
N19-1096,W13-2322,0,0.0176116,"recent years, and to model them, we need probabilistic models of DAGs. One model that has attracted some attention is the DAG automaton, but it has not been studied as a probabilistic model. We show that some DAG automata cannot be made into useful probabilistic models by the nearly universal strategy of assigning weights to transitions. The problem affects single-rooted, multi-rooted, and unbounded-degree variants of DAG automata, and appears to be pervasive. It does not affect planar variants, but these are problematic for other reasons. 1 Introduction Abstract Meaning Representation (AMR; Banarescu et al. 2013) has prompted a flurry of interest in probabilistic models for semantic parsing. AMR annotations are directed acyclic graphs (DAGs), but most probabilistic models view them as strings (e.g. van Noord and Bos, 2017) or trees (e.g. Flanigan et al., 2016), removing their ability to represent coreference—one of the very aspects of meaning that motivates AMR. Could we we instead use probabilistic models of DAGs? To answer this question, we must define probability distributions over sets of DAGs. For inspiration, consider probability distributions over sets of strings or trees, which can be defined"
N19-1096,P13-1091,0,0.0483139,"Missing"
N19-1096,J18-1005,1,0.924332,"ide-outside algorithms. What is the equivalent of weighted finite automata for DAGs? There are several candidates (Chiang et al., 2013; Bj¨orklund et al., 2016; Gilroy et al., 2017), but one appealing contender is the DAG automaton (Quernheim and Knight, 2012) which generalises finite tree automata to DAGs explicitly for modeling semantic graphs. These DAG automata generalise an older formalism called planar DAG automata (Kamimura and Slutzki, 1981) by adding weights and removing the planarity constraint, and have attracted further study (Blum and Drewes, 2016; Drewes, 2017), in particular by Chiang et al. (2018), who generalised classic dynamic programming algorithms to DAG automata. But while Quernheim and Knight (2012) clearly intend for their weights to define probabilities, they stop short of claiming that they do, instead ending their paper with an open problem: “Investigate a reasonable probabilistic model.” We investigate probabilistic DAG automata and prove a surprising result: For some DAG automata, it is impossible to assign weights that define non-trivial probability distributions. We exhibit a very simple DAG automaton that generates an infinite language of graphs, and for which the only"
N19-1096,N16-1087,0,0.0174125,"obabilistic models by the nearly universal strategy of assigning weights to transitions. The problem affects single-rooted, multi-rooted, and unbounded-degree variants of DAG automata, and appears to be pervasive. It does not affect planar variants, but these are problematic for other reasons. 1 Introduction Abstract Meaning Representation (AMR; Banarescu et al. 2013) has prompted a flurry of interest in probabilistic models for semantic parsing. AMR annotations are directed acyclic graphs (DAGs), but most probabilistic models view them as strings (e.g. van Noord and Bos, 2017) or trees (e.g. Flanigan et al., 2016), removing their ability to represent coreference—one of the very aspects of meaning that motivates AMR. Could we we instead use probabilistic models of DAGs? To answer this question, we must define probability distributions over sets of DAGs. For inspiration, consider probability distributions over sets of strings or trees, which can be defined by weighted finite automata (e.g. Mohri et al., 2008; May et al., 2010): a finite automaton generates a set of strings or trees—called a language—and if we assume that probabilities factor over its transitions, then any finite automaton can be weighted"
N19-1096,W17-3410,1,0.882161,"Missing"
N19-1096,J16-4009,0,0.0138574,"r some or all values of n. NowP we show that any choice of positive C causes G∈Ls (A) w(G) to diverge. Given an inP finite series of the form ∞ n=0 an , the ratio test (D’Alembert, 1768) considers the ratio between | adjacent terms in the limit, limn→∞ |a|an+1 . If this n| ratio is greater than 1, the series diverges; if less than 1 the series converges; if exactly 1 the test is inconclusive. In our case: 3.1 Multi-rooted DAGs What happens when we consider DAG languages that allow multiple roots? In one reasonable interpretation of AMRbank, over three quarters of the DAGs have multiple roots (Kuhlmann and Oepen, 2016), so we want a model that permits this.4 Section 2.1.1 explained how a DAG automaton can be constrained to generate single-rooted languages, by restricting start transitions (i.e. those |(n + 1)!BC n+1 | = lim (n + 1)|C |= ∞. n→∞ n→∞ |n!BC n | P Hence G∈Ls (A) diverges for any choice of C, equivalently for any choice of weights. So there is no w for which (A, w) is probabilistic with full support over Ls (A). lim 3 In this model, the subproblems of a natural dynamic program depend on the set of possible frontiers, rather than subsets of nodes as in the algorithms of Chiang et al. (2018). We do"
N19-1096,P10-1108,0,0.090191,"Missing"
N19-1096,W12-4209,0,0.434917,"andidates (Chiang et al., 2013; Bj¨orklund et al., 2016; Gilroy et al., 2017), but one appealing contender is the DAG automaton (Quernheim and Knight, 2012) which generalises finite tree automata to DAGs explicitly for modeling semantic graphs. These DAG automata generalise an older formalism called planar DAG automata (Kamimura and Slutzki, 1981) by adding weights and removing the planarity constraint, and have attracted further study (Blum and Drewes, 2016; Drewes, 2017), in particular by Chiang et al. (2018), who generalised classic dynamic programming algorithms to DAG automata. But while Quernheim and Knight (2012) clearly intend for their weights to define probabilities, they stop short of claiming that they do, instead ending their paper with an open problem: “Investigate a reasonable probabilistic model.” We investigate probabilistic DAG automata and prove a surprising result: For some DAG automata, it is impossible to assign weights that define non-trivial probability distributions. We exhibit a very simple DAG automaton that generates an infinite language of graphs, and for which the only valid probability distribution that can be defined by weighting transitions is one in which the support is a si"
N19-1329,K17-1037,0,0.178029,"Large neural networks have a notorious capacity to memorize training data (Zhang et al., 2016), but their high accuracy on many NLP tasks shows that they nonetheless generalize. One apparent explanation for their performance is that they learn linguistic generalizations even without explicit supervision for those generalizations—for example, that subject and verb number agree in English (Linzen et al., 2016); that derivational suffixes attach to only specific parts of speech (Kementchedjhieva and Lopez, 2018); and that short segments of speech form natural clusters corresponding to phonemes (Alishahi et al., 2017). These studies tell us that neural models learn to implicitly represent linguistic categories and their interactions. But how do they learn these representations? One clue comes from the inspection of multilayer models, which seem to encode lexical categories in lower layers, and more contextual categories in higher layers. For example, Blevins et al. (2018) found that a word’s part of speech (POS) is encoded by lower layers, and the POS of its syntactic parent is encoded by higher layers; while Belinkov et al. (2018) found that POS is encoded by lower layers and semantic category is encoded"
N19-1329,P17-1080,0,0.557141,"of training is shown in Figure 2, which illustrates the dips in loss when learning rate changes. All experiments on the LM throughout training are conducted by running the model at the end of each epoch in inference mode over the test corpus. 3.1 Taggers To understand the representations learned by our LM, we compare them with the internal representations of tagging models, using SVCCA. Where possible, we use coarse-grained and fine-grained tagsets to account for effects from the size of the tagset. Table 1 illustrates our tagsets. POS tagging For syntactic categories, we use POS tags, as in Belinkov et al. (2017). As a coarse-grained tagset, we use silver Universal Dependency Parse (UDP) POS tags automatically added to our Wikipedia corpus with spacy.1 We also use a corpus of fine-grained human annotated Penn Treebank POS tags from the Groningen Meaning Bank (GMB; Bos et al., 2017). Semantic tagging We follow Belinkov et al. (2018) in representing word-level semantic information with silver SEM tags (Bjerva et al., 2016). SEM tags disambiguate POS tags in ways that are relevant to multilingual settings. For example, the comma is not assigned a single tag as punctuation, but has distinct tags according"
N19-1329,C16-1333,0,0.0264636,"se-grained and fine-grained tagsets to account for effects from the size of the tagset. Table 1 illustrates our tagsets. POS tagging For syntactic categories, we use POS tags, as in Belinkov et al. (2017). As a coarse-grained tagset, we use silver Universal Dependency Parse (UDP) POS tags automatically added to our Wikipedia corpus with spacy.1 We also use a corpus of fine-grained human annotated Penn Treebank POS tags from the Groningen Meaning Bank (GMB; Bos et al., 2017). Semantic tagging We follow Belinkov et al. (2018) in representing word-level semantic information with silver SEM tags (Bjerva et al., 2016). SEM tags disambiguate POS tags in ways that are relevant to multilingual settings. For example, the comma is not assigned a single tag as punctuation, but has distinct tags according to its function: conjunction, disjunction, or apposition. The 66 fine-grained SEM tag classes fall under 13 coarsegrained tags, and an ‘unknown’ tag. Global topic For topic, we classify each word of the sequence by its source Wikipedia article; for example, every word in the wikipedia article on Trains is labeled “Trains”. This task assesses whether the network encodes the global topic of the sentence. 3259 1 ht"
N19-1329,P18-1126,0,0.0267463,"e shuffled tags. Gray vertical lines mark when the step size is rescaled. Most of the literature on analyzing representations, by probing with a more complex architecture, seeks the flexibility of mutual information with the concreteness and tractability of the structural view – but instead obscures the strict information view without offering interpretable information about the structure, because the architecture of a diagnostic classifier affects its performance. It should not be surprising that representational quality as measured by such systems is a poor indicator of translation quality (Cífka and Bojar, 2018). SVCCA, in contrast, is a structural view that does not directly compare an activation that targets word prediction with a particular tag, but instead compares that activation with one targeting the prediction of the tag. 3264 Let us consider a specific common probing method. What do we learn about the LM when a feedforward network cannot extract tag information directly from the embedding layer, but can from a recurrent layer? It may be tempting to conclude that tag information relies heavily on context, but consider some alternative explanations. If the embedding encodes the tag to be inter"
N19-1329,P18-1198,0,0.0692706,"fferent layers of the same model over time (Figure 5). We observe that, while over time correlation increases, in general closer layers are more similar, and they are less correlated than they are with the same layer of a differently initialized model. This supports the idea that we should compare recurrent layers with recurrent layers because their representations play similar roles within their respective architectures. SVCCA vs. Diagnostic classifiers A popular method to analyze learned representations is to use a diagnostic classifier (Belinkov et al., 2017; Hupkes et al., 2018) or probe (Conneau et al., 2018), a separate model that is trained to predict a linguistic category of interest, yt , from an arbitrary hidden layer ht . Diagnostic classifiers are widely used (Belinkov et al., 2018; Giulianelli et al., 2018). 2 This experiment is similar to the comparisons of randomly initialized models by Morcos et al. (2018). 3260 tag UDP POS PTB POS SEM (coarse) SEM (fine) topic corpus wiki GMB GMB GMB wiki number of classes 17 36 14 67 100 token count train dev 665K 97K 943K 136K 937K 132K 937K 132K 665K 97K label t + 1 acc ppl 50 4.3 51 4.7 55 3.5 50 5.6 36 19.1 label t acc ppl 93 1.2 95 1.18 91 1.3 88"
N19-1329,W18-5426,0,0.0566586,"Missing"
N19-1329,W18-5417,1,0.817391,"er learning algorithms for NLP models, possibly to incorporate linguistic information more effectively. 1 Introduction Large neural networks have a notorious capacity to memorize training data (Zhang et al., 2016), but their high accuracy on many NLP tasks shows that they nonetheless generalize. One apparent explanation for their performance is that they learn linguistic generalizations even without explicit supervision for those generalizations—for example, that subject and verb number agree in English (Linzen et al., 2016); that derivational suffixes attach to only specific parts of speech (Kementchedjhieva and Lopez, 2018); and that short segments of speech form natural clusters corresponding to phonemes (Alishahi et al., 2017). These studies tell us that neural models learn to implicitly represent linguistic categories and their interactions. But how do they learn these representations? One clue comes from the inspection of multilayer models, which seem to encode lexical categories in lower layers, and more contextual categories in higher layers. For example, Blevins et al. (2018) found that a word’s part of speech (POS) is encoded by lower layers, and the POS of its syntactic parent is encoded by higher layer"
N19-1329,P18-1027,0,0.0207906,"e the current LM with the final tag model. Dotted lines use shuffled tags. Gray vertical lines mark when the step size is rescaled. 3262 topic falls consistently below the score for a model trained on randomized topic tags, implying that early in training the model has removed the context necessary to identify topic (below even the inadequate contextual information memorized by a model with random labels), which depends on the general vocabulary in a sentence rather than a local sequence. Over time correlation rises, possibly because the model permits more long-distance context to be encoded. Khandelwal et al. (2018) found that LSTMs remember content words like nouns for more time steps than they remember function words like prepositions and articles. We hypothesize that the LM’s slower stabilization on topic is related to this phenomenon, since it must depend on content words, and its ability to remember them increases throughout training. The encoder layer exhibits very different patterns. Because the representation produced by the encoder layer is local to the word, the nuances that determine how a word is tagged in context cannot be learned. The encoder layers are all highly similar to each other, whi"
N19-1329,Q16-1037,0,0.0374348,"ing training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively. 1 Introduction Large neural networks have a notorious capacity to memorize training data (Zhang et al., 2016), but their high accuracy on many NLP tasks shows that they nonetheless generalize. One apparent explanation for their performance is that they learn linguistic generalizations even without explicit supervision for those generalizations—for example, that subject and verb number agree in English (Linzen et al., 2016); that derivational suffixes attach to only specific parts of speech (Kementchedjhieva and Lopez, 2018); and that short segments of speech form natural clusters corresponding to phonemes (Alishahi et al., 2017). These studies tell us that neural models learn to implicitly represent linguistic categories and their interactions. But how do they learn these representations? One clue comes from the inspection of multilayer models, which seem to encode lexical categories in lower layers, and more contextual categories in higher layers. For example, Blevins et al. (2018) found that a word’s part of"
N19-1329,N18-1202,0,0.454737,"hey learn these representations? One clue comes from the inspection of multilayer models, which seem to encode lexical categories in lower layers, and more contextual categories in higher layers. For example, Blevins et al. (2018) found that a word’s part of speech (POS) is encoded by lower layers, and the POS of its syntactic parent is encoded by higher layers; while Belinkov et al. (2018) found that POS is encoded by lower layers and semantic category is encoded by higher layers. More generally, the most useful layer for an arbitrary NLP task seems to depend on how “high-level” the task is (Peters et al., 2018). Since we know that lower layers in a multi-layer model converge to their final representations more quickly than higher layers (Raghu et al., 2017), it is likely that models learn local lexical categories like POS earlier than they learn higher-level linguistic categories like semantic class. How and when do neural representations come to encode specific linguistic categories? Answers could explain why neural models work and help us improve learning algorithms. We investigate how representations of linguistic structure are learned over time in neural language models (LMs), which are central"
N19-1329,W18-5448,0,0.0827707,"d perplexity on t + 1 are from the target tag predictor, on t are from the input tagger. Metrics obtained when training on randomly shuffled labels are provided as a low baseline. Accuracy is on the test set from the training domain (GMB or Wikipedia). Figure 5: SVCCA score between different layers of the LM at each epoch. For example, ρ(h2 , h1 ) compares the activations h2 with the activations h1 . But if a diagnostic classifier is trained on enough examples, then random embeddings as input representations often outperform any pretrained intermediate representation (Wieting and Kiela, 2019; Zhang and Bowman, 2018). This suggests that diagnostic classifiers may work simply by memorizing the association between an embedding and the most frequent output category associated with that embedding; since for many words their category is (empirically) unambiguous, this may give an inflated view of just how much a model “understands” about that category. Our use of SVCCA below will differ from the use of diagnostic classifiers in an important way. Diagnostic classifiers use the intermediate representations of the LM as inputs to a tagger. A representation is claimed to encode, for example, POS if the classifier"
P10-4002,W05-1506,0,0.0175505,"mentation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Tab"
P10-4002,P07-1019,0,0.10005,"and phrase-based models, these are strictly arranged in a monotone, leftbranching structure. ing models need not be explicitly represented as FSTs—the state space can be inferred. Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). 4 5 Rescoring with weighted FSTs Semiring framework Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings. A semiring is a 5-tuple (K, ⊕, ⊗, 0, 1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ⊕ and ⊗, and their identities 0 and 1. Multiplication and addition must be associative. Multiplication must distribute over addition, and v ⊗ 0 The design of cdec separates the creation of a transl"
P10-4002,W06-3601,0,0.0172305,"s avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 20"
P10-4002,N03-1017,0,0.465271,", the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization"
P10-4002,P07-2045,1,0.0302608,"versity of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then"
P10-4002,P09-1019,1,0.765356,"ng et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLE"
P10-4002,D07-1104,1,0.804043,"ms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to"
P10-4002,D09-1005,0,0.00908811,"oal a 1 shell 100 a 1 2 1 little 101 1 little 1 small Goal ll se hou 1 small small she sma NN little ll JJ 010 110 1 a little 1 house 1 shell Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines (small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distortion limit of 1 (right). must equal 0. Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring"
P10-4002,W09-0424,1,0.545644,"alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unif"
P10-4002,D08-1023,1,0.337071,"T03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging models, this pipeline is identi"
P10-4002,P08-1024,1,0.647817,"nd alignment algorithms then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, th"
P10-4002,E09-1061,1,0.355868,"red forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice). The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source"
P10-4002,J03-1006,0,0.00679634,"a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003). In word-based models, a single"
P10-4002,J93-2003,0,0.0420135,"st translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative tra"
P10-4002,P03-1021,0,0.088916,"d Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply usin"
P10-4002,P02-1040,0,0.103814,"whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure"
P10-4002,N09-1025,0,0.0233546,"s then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly sep"
P10-4002,J07-2003,0,0.969957,"ation techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparabl"
P10-4002,N03-1028,0,0.299229,"ities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on p"
P10-4002,2006.amta-papers.25,0,0.0163455,"of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec conf"
P10-4002,N10-1128,1,0.770622,"nce pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-spec"
P10-4002,P08-1115,1,0.407306,"entation of all the derivations of the sentence pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kin"
P10-4002,N10-1033,1,\N,Missing
P11-1048,C04-1041,0,0.147319,"> As can be inferred from even this small example, a key difficulty in parsing CCG is that the number of categories quickly becomes extremely large, and there are typically many ways to analyze every span of a sentence. Supertagging (Bangalore and Joshi, 1999; Clark, 2002) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004a). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical cat"
P11-1048,P04-1014,0,0.0132581,"> As can be inferred from even this small example, a key difficulty in parsing CCG is that the number of categories quickly becomes extremely large, and there are typically many ways to analyze every span of a sentence. Supertagging (Bangalore and Joshi, 1999; Clark, 2002) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004a). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical cat"
P11-1048,J07-4004,0,0.684144,"ective is an upper bound that we want to make as tight as possible by solving for minu L(u). We optimize the values of the u(i, t) variables using the same algorithm as Rush et al. (2010) for their tagging and parsing problem (essentially a perceptron update).4 An advantages of DD is that, on convergence, it recovers exact solutions to the combined problem. However, if it does not converge or we stop early, an approximation must be returned: following Rush et al. (2010) we used the highest scoring output of the parsing submodel over all iterations. 5 Experiments Parser. We use the C&C parser (Clark and Curran, 2007) and its supertagger (Clark, 2002). Our baseline is the hybrid model of Clark and Curran (2007); our integrated model simply adds the supertagger features to this model. The parser relies solely on the supertagger for pruning, using CKY for search over the pruned space. Training requires repeated calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 3) than in previous work (Clark and Curran, 2007). Models were t"
P11-1048,W02-2203,0,0.580056,"he first derivation below, (SN P )/N P and N P combine to form the spanning category SN P , which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. I like tea I NP (S NP)/NP NP NP S NP S < > like tea (S NP)/NP NP >T S /(S NP) S /NP S >B > As can be inferred from even this small example, a key difficulty in parsing CCG is that the number of categories quickly becomes extremely large, and there are typically many ways to analyze every span of a sentence. Supertagging (Bangalore and Joshi, 1999; Clark, 2002) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possibl"
P11-1048,J85-1006,0,0.804591,"arch problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and mildly context-sensitive language. Even allowing for the observation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both uses in §5. Many researchers in NLP are familiar"
P11-1048,D09-1011,0,0.0713145,"d as the product of all incoming messages. The only difference from Equation 6 is the addition of the outside message. (6) Our parsing model is also a distribution over variables Ti , along with an additional quadratic number of span(i, j) variables. Though difficult to represent pictorially, a distribution over parses is captured by an extension to graphical models called case-factor diagrams (McAllester et al., 2008). We add this complex distribution to our model as a single factor (Figure 3). This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). When a factor graph is a tree as in Figure 2, BP converges in a single iteration to the exact marginals. However, when the model contains cycles, as in Figure 3, we can iterate message passing. Under certain assumptions this loopy BP it will converge to approximate marginals that are bounded under an interpretation from statistical physics (Yedidia et al., 2001; Sutton and McCallum, 2010). The TREE factor exchanges inside ni and outside oi messages with the tag and span variables, taking into account beliefs from the sequence model. 474 p(Ti ) = 1 fi (Ti )bi (Ti )ei (Ti )oi (Ti ) Z (8) The a"
P11-1048,N10-1128,0,0.0359153,"Missing"
P11-1048,P08-1115,0,0.0278088,"Missing"
P11-1048,N09-1037,0,0.0473688,"Missing"
P11-1048,W06-1673,0,0.068112,"Missing"
P11-1048,P08-1109,0,0.051969,"bout the effect on speed for our model. We measured the runtime of the algorithms under 477 Training the Integrated Model In the experiments reported so far, the parsing and supertagging models were trained separately, and only combined at test time. Although the outcome of these experiments was successful, we wondered if we could obtain further improvements by training the model parameters together. Since the gradients produced by (loopy) BP are approximate, for these experiments we used a stochastic gradient descent (SGD) trainer (Bottou, 2003). We found that the SGD parameters described by Finkel et al. (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. Curiously, however, we found that the combined model does not perform as well when Baseline BPk=1 BPk=5 BPk=25 DDk=1 DDk=5 DDk=25 AST sent/sec LF 65.8 87.38 60.8 87.70 46.7 87.70 35.3 87.70 64.6 87.40 41.9 87.65 32.5 87.71 Reverse sent/sec LF 5.9 87.36 5.8 88.35 4.7 88.34 3.5 88.33 5.9 87.38 3.1 88.09 1.9 88.29 Table 6: Parsing time in seconds per sentence (vs. Fmeasure) on section 00. Baseline BP inf BP train LF 86.7 86.8 86.3 AST UF 92.7 92.8 92.5 ST 94.0 94.1 93.8 LF 86.7 87.2 85.6 Reverse UF 92.7"
P11-1048,P10-1035,0,0.436075,"pertagger. Variations on this approach drive the widely-used, broad coverage C&C parser (Clark and Curran, 2004a; Clark and Curran, 2007; Kummerfeld et al., 2010). However, it fails when the supertagger makes errors. We show experimentally that this pipeline significantly lowers the upper bound on parsing accuracy (§3). Introduction Accurate and efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics, due to the complexities associated its mild context sensitivity. Even for practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained over 1500. The The same experiment shows that the supertagger prunes many bad parses. So, while we want to avoid the error propagation inherent to a pipeline, ideally we still want to benefit from the key insight of supertagging: that a sequence model over lexical catego"
P11-1048,P02-1043,0,0.0192316,"significantly lowers the upper bound on parsing accuracy (§3). Introduction Accurate and efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics, due to the complexities associated its mild context sensitivity. Even for practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained over 1500. The The same experiment shows that the supertagger prunes many bad parses. So, while we want to avoid the error propagation inherent to a pipeline, ideally we still want to benefit from the key insight of supertagging: that a sequence model over lexical categories can be quite accurate. Our solution is to combine the features of both the supertagger and the parser into a single, less aggressively pruned model. The challenge with this model is its prohibitive complexity, which we address with approximate methods: dual decom"
P11-1048,J07-3004,0,0.0453391,"and Curran (2007); our integrated model simply adds the supertagger features to this model. The parser relies solely on the supertagger for pruning, using CKY for search over the pruned space. Training requires repeated calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 3) than in previous work (Clark and Curran, 2007). Models were trained with the parser’s L-BFGS trainer. Evaluation. We evaluated on CCGbank (Hockenmaier and Steedman, 2007), a right-most normalform CCG version of the Penn Treebank. We use sections 02-21 (39603 sentences) for training, 4 The u terms can be interpreted as the messages from factors to variables (Sontag et al., 2010) and the resulting message passing algorithms are similar to the max-product algorithm, a sister algorithm to BP. 475 section 00 (1913 sentences) for development and section 23 (2407 sentences) for testing. We supply gold-standard part-of-speech tags to the parsers. Evaluation is based on labelled and unlabelled predicate argument structure recovery and supertag accuracy. We only evaluat"
P11-1048,P08-1067,0,0.0532508,"Missing"
P11-1048,P08-1102,0,0.0198951,"Missing"
P11-1048,D10-1125,0,0.0442242,"ntext-sensitive language. Even allowing for the observation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both uses in §5. Many researchers in NLP are familiar with two special cases of belief propagation: the forward-backward and inside-outside algorithms, used for com"
P11-1048,P10-1036,0,0.040232,"Missing"
P11-1048,D10-1004,0,0.0667418,"Missing"
P11-1048,P08-1023,0,0.0600893,"Missing"
P11-1048,P06-1055,0,0.0220359,"Missing"
P11-1048,D10-1001,0,0.334442,"bservation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both uses in §5. Many researchers in NLP are familiar with two special cases of belief propagation: the forward-backward and inside-outside algorithms, used for computing expectations in sequence models and context"
P11-1048,D08-1016,0,0.735978,"of only a subset of these sequences, then the search problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and mildly context-sensitive language. Even allowing for the observation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both"
P11-1048,J99-2004,0,\N,Missing
P11-1048,D08-1022,0,\N,Missing
P11-1048,W05-0636,0,\N,Missing
P11-1048,P08-1000,0,\N,Missing
P11-1158,J99-2004,0,0.270147,"s (SN P2 )/N P1 , specifying the first argument as an NP to the right and the second as an NP to the left. In parsing, adjacent spans are combined using a small number of binary combinatory rules like forward application or composition on the spanning categories (Steedman, 2000; Fowler and Penn, 2010). In the first derivation below, (SN P )/N P and N P combine to form the spanning category SN P , which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. 1578 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a"
P11-1158,C04-1041,0,0.0656093,"N P )/N P and N P combine to form the spanning category SN P , which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. 1578 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam ratios, relaxing the pruning threshold for lexical catego"
P11-1158,J07-4004,0,0.692551,"er underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. The PCFG model simply generates a tree top down and uses very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories. Su"
P11-1158,W02-2203,0,0.0304568,"mal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4 . 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyond the speedups (Clark, 2002) We ran experiments to understand the time/accuracy tradeoff of adaptive supertagging, and to serve as baselines. Adaptive supertagging is parametrized by a beam size β and a dictionary cutoff k that bounds the number of lexical categories considered for each word (Clark and Curran, 2007). Table 3 shows both the standard beam levels (AST) used for the C&C parser and looser beam levels: AST-covA, a simple extension of AST with increased coverage and AST-covB, also increasing coverage but with better performance for the HWDep model. Parsing results for the AST settings (Tables 4 and 5) confirm t"
P11-1158,W07-2206,0,0.0304452,"Missing"
P11-1158,P10-1035,0,0.178199,"ighest probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Introduction Efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics. Even with practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2"
P11-1158,P02-1043,0,0.0845529,"ontrol loop. Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. Provided that the heuristic never underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model"
P11-1158,J07-3004,0,0.0274272,"s very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories. Supertagger. For supertagging we used Dennis Mehay’s implementation, which follows Clark 1 Indeed, all of the past work on A* parsing that we are aware of uses generative parsers (Pauls and Klein, 2009b, inter alia). 1579 (2002).2 Due to differences in smoothing of the supertagging and parsing models, we occasionally drop supertags returned by the supertagger because they do not appear in the parsing model 3 . Evaluation. All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4 . 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported bey"
P11-1158,W01-1812,0,0.108966,"Missing"
P11-1158,N03-1016,0,0.70744,"ractical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Attainable parses Valid supertag-sequences I like tea I like tea NP (S NP)/NP NP NP (S NP)/NP NP Desirable parses S NP S High scoring supertags &gt; &gt;T S /(S NP) S /NP S &gt;B &gt; Because of the number of lexical categories and their"
P11-1158,P10-1036,0,0.127158,"Missing"
P11-1158,N09-1063,0,0.318789,"ongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Attainable parses Valid supertag-sequences I like tea I like tea NP (S NP)/NP NP NP (S NP)/NP NP Desirable parses S NP S High scoring supertags &gt; &gt;T S /(S NP) S /NP S &gt;B &gt; Because of the number of lexical categories and their complexity, a key diffi"
P11-1158,P09-1108,0,0.610846,"ongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Attainable parses Valid supertag-sequences I like tea I like tea NP (S NP)/NP NP NP (S NP)/NP NP Desirable parses S NP S High scoring supertags &gt; &gt;T S /(S NP) S /NP S &gt;B &gt; Because of the number of lexical categories and their complexity, a key diffi"
P11-1158,N09-1073,0,0.0556641,"Missing"
P11-1158,C10-2168,0,0.107645,"Missing"
P13-1135,W10-1703,1,0.322705,"Missing"
P13-1135,W11-2103,1,0.788665,"BLEU scores for several language pairs before and after adding the mined parallel data to systems trained on data from WMT data. WMT 11 Baseline +Web Data WMT 12 Baseline +Web Data FR-EN 30.96 31.24 FR-EN 29.88 30.08 EN-FR 30.69 31.17 EN-FR 28.50 28.76 Corpus EN-FR EN-ES EN-DE News Commentary 2.99M 50.3M 316M 668M 121M 3.43M 49.2M 281M 68.8M 3.39M 47.9M 88.4M Europarl United Nations FR-EN Gigaword CommonCrawl Table 9: BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Gigaword (Callison-Burch et al., 2011). For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al. (2010), in order to determine how the effect of such data compares with the effect of webmined data. The baseline system was trained using only the Europarl corpus (Koehn, 2005) as parallel data, and all experiments use the same language model trained on the target sides of Europarl, the English side of all linked SpanishEnglish Wikipedia articles, and the English side of the mined CommonCrawl data. We use a 5gram language model and tune using"
P13-1135,W12-3102,1,0.806207,"Missing"
P13-1135,J93-1004,0,0.134564,", 2003): 1. Candidate pair selection: Retrieve candidate document pairs from the CommonCrawl corpus. 2. Structural Filtering: (a) Convert the HTML of each document 2 3 commoncrawl.org http://aws.amazon.com/s3/pricing/ into a sequence of start tags, end tags, and text chunks. (b) Align the linearized HTML of candidate document pairs. (c) Decide whether to accept or reject each pair based on features of the alignment. 3. Segmentation: For each text chunk, perform sentence and word segmentation. 4. Sentence Alignment: For each aligned pair of text chunks, perform the sentence alignment method of Gale and Church (1993). 5. Sentence Filtering: Remove sentences that appear to be boilerplate. Candidate Pair Selection We adopt a strategy similar to that of Resnik and Smith (2003) for finding candidate parallel documents, adapted to the parallel architecture of Map-Reduce. The mapper operates on each website entry in the CommonCrawl data. It scans the URL string for some indicator of its language. Specifically, we check for: 1. Two/three letter language codes (ISO-639). 2. Language names in English and in the language of origin. If either is present in a URL and surrounded by non-alphanumeric characters, the URL"
P13-1135,P07-2045,1,0.011242,"Missing"
P13-1135,2005.mtsummit-papers.11,1,0.613567,"oehn3 pkoehn@inf.ed.ac.uk Chris Callison-Burch1,2,5 ccb@cs.jhu.edu ∗ Adam Lopez1,2 alopez@cs.jhu.edu Department of Computer Science, Johns Hopkins University Human Language Technology Center of Excellence, Johns Hopkins University 3 School of Informatics, University of Edinburgh 4 Institute of Computational Linguistics, University of Zurich 5 Computer and Information Science Department, University of Pennsylvania 1 2 Abstract are readily available, ordering in the hundreds of millions of words for Chinese-English and ArabicEnglish, and in tens of millions of words for many European languages (Koehn, 2005). In each case, much of this data consists of government and news text. However, for most language pairs and domains there is little to no curated parallel data available. Hence discovery of parallel data is an important first step for translation between most of the world’s languages. Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl host"
P13-1135,W02-0109,0,0.0188601,"Missing"
P13-1135,P12-3005,0,0.0302301,"Missing"
P13-1135,J05-4003,0,0.0147224,"pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 1 The Web is an important source of parallel text. Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010)— it is common to find document pairs that are direct translations of one another. This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns H"
P13-1135,P11-1122,1,0.252203,"Missing"
P13-1135,P03-1021,0,0.0312913,"5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely tokens. 4.1 News Domain Translation Our first set of experiments are based on systems built for the 2012 Workshop on Statistical Machine Translation (WMT) (Callison-Burch et al., 2012) using all available parallel and monolingual data for that task, aside from the French-English Gigaword. In these experiments, we use 5-gram language models when the target language is English or German, and 4-gram language models for French and Spanish. We tune model weights using minimum error rate training (MERT; Och, 2003) on the WMT 2008 test data. The results are given in Table 8. For all language pairs and both test sets (WMT 2011 and WMT 2012), we show an improvement of around 0.5 BLEU. We also included the French-English Gigaword in separate experiments given in Table 9, and Table 10 compares the sizes of the datasets used. These results show that even on top of a different, larger parallel corpus mined from the web, adding CommonCrawl data still yields an improvement. 4.2 Open Domain Translation A substantial appeal of web-mined parallel data is that it might be suitable to translation of domains other th"
P13-1135,J03-3002,0,0.249669,"trap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jrs026/CommonCrawlMiner Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines (Nie et al., 1999; Chen and Nie, 2000; Resnik, 1999; Resnik and Smith, 2003). However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. For most researchers, this is prohibitively expensive. As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted 1374 Proceedings of the 5"
P13-1135,P99-1068,0,0.0228645,"utset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jrs026/CommonCrawlMiner Parallel text mining from the Web was originally explored by individuals or small groups of academic researchers using search engines (Nie et al., 1999; Chen and Nie, 2000; Resnik, 1999; Resnik and Smith, 2003). However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. For most researchers, this is prohibitively expensive. As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted"
P13-1135,N10-1063,1,0.944364,"enres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 1 The Web is an important source of parallel text. Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010)— it is common to find document pairs that are direct translations of one another. This natural parallelism simplifies the mining task, since few resources or existing corpora are needed at the outset to bootstrap the extraction process. Introduction A key bottleneck in porting statistical machine translation (SMT) technology to new languages and domains is the lack of readily available parallel corpora beyond curated datasets. For a handful of language pairs, large amounts of parallel data ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. 1 github.com/jr"
P13-1135,N12-1079,0,0.020081,"Missing"
P13-1135,C10-1124,0,0.0223887,"Missing"
P13-1135,D11-1126,0,0.0714544,"Missing"
P13-1135,P06-4018,0,\N,Missing
P16-1047,S12-1045,0,0.221049,"Missing"
P16-1047,S12-1037,0,0.418202,"Missing"
P16-1047,E14-1063,1,0.872253,"Missing"
P16-1047,W15-1301,1,0.817692,"Neural Networks For Negation Scope Detection Federico Fancellu and Adam Lopez and Bonnie Webber School of Informatics University of Edinburgh 11 Crichton Street, Edinburgh f.fancellu[at]sms.ed.ac.uk, {alopez,bonnie}[at]inf.ed.ac.uk Abstract given the importance of recognizing negation for information extraction from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we"
P16-1047,S12-1036,0,0.127179,"Missing"
P16-1047,S12-1043,0,0.324713,"Missing"
P16-1047,S12-1038,0,0.245613,"Missing"
P16-1047,S12-1042,0,0.566171,"Missing"
P16-1047,S12-1040,0,0.190353,"r information extraction from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we investigate whether neural network based sequence-tosequence models (§ 4) are a valid alternative. The first advantage of neural networks-based methods for NLP is that we could perform classification by means of unsupervised word-embeddings features only, under the assumption that"
P16-1047,S12-1035,0,0.0336994,"tences respectively. If a sentence contains multiple negation instances, we create as many copies as the number of instances. If the sentence contains a morphological cue (e.g. impatient) we split it into affix (im-) and root (patient), and consider the former as cue and the latter as part of the scope. Both neural network architectures are implemented using TensorFlow (Abadi et al., 2015) with a 200-units hidden layer (400 in total for two concatenated hidden layers in the BiLSTM), the Adam optimizer (Kingma and Ba, 2014) with a 5 For the statistics regarding the data, we refer the reader to Morante and Blanco (2012). 498 dings are pre-trained using external data. We experimented with both keeping the wordembedding matrix fixed and updating it during training but we found small or no difference between the two settings. To do this, we train a word-embedding matrix using Word2Vec (Mikolov et al., 2013) on 770 million tokens (for a total of 30 million sentences and 791028 types) from the ‘One Billion Words Language Modelling’ dataset 6 and the Sherlock Holmes data (5520 sentences) combined. The dataset was tokenized and morphological cues split into negation affix and root to match the Conan Doyle’s data. I"
P16-1047,P14-1007,0,0.239289,"on from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we investigate whether neural network based sequence-tosequence models (§ 4) are a valid alternative. The first advantage of neural networks-based methods for NLP is that we could perform classification by means of unsupervised word-embeddings features only, under the assumption that they also encode struct"
P16-1047,S15-1008,0,0.015296,", given a negative instance, to identify which tokens are affected by negation (§2). As shown in (1), only the first clause is negated and therefore we mark he and the car, along with the predicate was driving as inside the scope, while leaving the other tokens outside. (1) He was not driving the car and she left to go home. 1. Comparable or better performance: We show that neural networks perform on par with previously developed classifiers, with a bi-directional LSTM outperforming them In the BioMedical domain there is a long line of research around the topic (e.g. Velldal et al. (2012) and Prabhakaran and Boguraev (2015)), 495 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 495–504, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics when tested on data from the same genre. [I, do, not, love, you, and, you, are, no, longer, invited]; in (3a), the vector c1 is 1 only at index 3 (w2 =‘not’), while in (3b) c2 is 1 at position 9, 10 (where w9 w10 = ‘no longer’); finally the vectors s1 and s2 are I only at the indices of the words underlined and O anywhere else. 2. Better understanding of the problem: We analyze in more detail the diff"
P16-1047,S12-1041,0,0.540142,"k, {alopez,bonnie}[at]inf.ed.ac.uk Abstract given the importance of recognizing negation for information extraction from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we investigate whether neural network based sequence-tosequence models (§ 4) are a valid alternative. The first advantage of neural networks-based methods for NLP is that we could perform clas"
P16-1047,J12-2005,0,0.0440514,"scope of negation, that is, given a negative instance, to identify which tokens are affected by negation (§2). As shown in (1), only the first clause is negated and therefore we mark he and the car, along with the predicate was driving as inside the scope, while leaving the other tokens outside. (1) He was not driving the car and she left to go home. 1. Comparable or better performance: We show that neural networks perform on par with previously developed classifiers, with a bi-directional LSTM outperforming them In the BioMedical domain there is a long line of research around the topic (e.g. Velldal et al. (2012) and Prabhakaran and Boguraev (2015)), 495 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 495–504, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics when tested on data from the same genre. [I, do, not, love, you, and, you, are, no, longer, invited]; in (3a), the vector c1 is 1 only at index 3 (w2 =‘not’), while in (3b) c2 is 1 at position 9, 10 (where w9 w10 = ‘no longer’); finally the vectors s1 and s2 are I only at the indices of the words underlined and O anywhere else. 2. Better understanding of the problem"
P16-1047,W12-4203,0,0.0405733,"Missing"
P16-1047,S12-1044,0,0.276556,"antic parser cannot create a reliable representation for a sentence, the system ‘backs-off’ to the hybrid model of Read et al. (2012), which uses syntactic information instead. This system suffers however from the same shortcomings mentioned above, in particular, given that MRS representation can only be built for a small set of languages. 4 For more details on LSTM and related mathematical formulations, we refer to reader to Hochreiter and Schmidhuber (1997) 497 Closed track *SEM2012 Open track UiO1 (Read et al., 2012) UiO2 (Lapponi et al., 2012) FBK (Chowdhury and Mahbub, 2012) UWashington (White, 2012) UMichigan (Abu-Jbara and Radev, 2012) UABCoRAL (Gyawali and Solorio, 2012) UiO2 (Lapponi et al., 2012) UGroningen (Basile et al., 2012) UCM-1 (de Albornoz et al., 2012) UCM-2 (Ballesteros et al., 2012) Packard et al. (2014) Method heuristics + SVM CRF CRF CRF CRF SVM CRF rule-based rule-based rule-based heuristics + SVM Scope tokens3 Prec. Rec. F1 81.99 88.81 85.26 86.03 81.55 83.73 81.53 82.44 81.89 83.26 83.77 83.51 84.85 80.66 82.70 85.37 68.86 76.23 82.25 82.16 82.20 69.20 82.27 75.15 85.37 68.53 76.03 58.30 67.70 62.65 86.1 90.4 88.2 Exact scope match Prec. Rec. F1 87.43 61.45 72.17 85.7"
P16-1183,D07-1090,0,0.0973761,"Missing"
P16-1183,D13-1195,0,0.245687,"Missing"
P16-1183,P09-2086,0,0.0161534,"our data structure does not store global addresses; it instead stores the difference in addresses between the parent node and each child. Since the array is aligned to four bytes, these relative addresses are divided by four in the representation, and multiplied by four at runtime to obtain the true offset. This enables us to encode relative addresses of 16GB, still larger than the actual device memory. We estimate that relative addresses of this size allow us to store a model containing around one billion ngrams.2 Unlike CPU language model implementations such as those of Heafield (2011) and Watanabe et al. (2009), we do not employ further compression techniques such as variable-byte encoding or LOUDS, because their runtime decompression algorithms require branching code, which our implementation must avoid. We optimize the node representation for coalesced reads by storing the keys of each B-tree consecutively in memory, followed by the corresponding values, also stored consecutively (Figure 6). When the data structure is traversed, only key arrays are iteratively copied to shared memory until a value array is needed. This design minimizes the number of reads from global memory. 4.1 Construction The c"
P16-1183,D13-1023,0,0.036468,"Missing"
P16-1183,W14-3311,0,0.0317366,"Missing"
P16-1183,P14-1020,0,0.120713,"Missing"
P16-1183,Q15-1007,1,0.898743,"Missing"
P16-1183,P15-2063,0,0.0236848,"Missing"
P16-1183,W11-2123,0,0.398393,"ly parallel devices Nikolay Bogoychev and Adam Lopez University of Edinburgh Edinburgh, United Kingdom Abstract For many applications, the query speed of N -gram language models is a computational bottleneck. Although massively parallel hardware like GPUs offer a potential solution to this bottleneck, exploiting this hardware requires a careful rethinking of basic algorithms and data structures. We present the first language model designed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency. Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPUbased language model, our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task. When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures. Figure 1: Theoretical floating point performance of CPU and GPU hardware over time (Nvidia Corporation, 2015). Our implementation is freely available at https://github.com/XapaJIaMnu/gLM 1 Introduction N -gram language models are ubiquitous in speech and"
P16-1183,P14-2112,0,0.0367448,"Missing"
P16-1183,D07-1049,0,0.0818942,"Missing"
P17-1184,W13-3520,0,0.0161715,"esign makes it possible to compare language models using perplexity, since they have the same event space, though open vocabulary word prediction is an interesting direction for future work. The complete architecture of our system is shown in Figure 1, showing segmentation function σ and composition function f from Equation 1. 4 Experiments We perform experiments on ten languages (Table 4). We use datasets from Ling et al. (2015) for English and Turkish. For Czech and Russian we use Universal Dependencies (UD) v1.3 (Nivre et al., 2015). For other languages, we use preprocessed Wikipedia data (Al-Rfou et al., 2013).2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing. Preprocessing involves lowercasing (except for character models) and removing hyperlinks. To ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (Abadi et al., 2015).3 We use a common setup for all experiments based on that of Ling et al. (2015), Kim et al. (2016), and Miyamoto and Cho (2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the"
P17-1184,D14-1082,0,0.0155114,"e language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data. 1 Introduction Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015). However, directly mapping a finite set of word types to a continuous representation has well-known limitations. First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling. Second, it cannot exploit systematic functional relationships in learning. For example, cat and cats stand in the same relationship as dog and dogs. While this relationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words sloth and sloths. These functional relationships re"
P17-1184,D14-1179,0,0.00425836,"Missing"
P17-1184,N15-1140,0,0.0257185,"Missing"
P17-1184,P15-1033,0,0.0106959,"results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data. 1 Introduction Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015). However, directly mapping a finite set of word types to a continuous representation has well-known limitations. First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling. Second, it cannot exploit systematic functional relationships in learning. For example, cat and cats stand in the same relationship as dog and dogs. While this relationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words sloth and sloths. These functional relationships reflect the fact that"
P17-1184,N16-1155,0,0.028853,"m, 2014; Cotterell and Sch¨utze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters? The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on 2016 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2016–2027 c Vancouver, Canada, July 30 - August"
P17-1184,E17-1048,0,0.0348304,"Missing"
P17-1184,P16-1160,0,0.0274925,"Missing"
P17-1184,D15-1176,0,0.746134,"exist. Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch¨utze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters? The relative merits of word, subword. and character-level models are not fully understood because eac"
P17-1184,W13-3512,0,0.0446308,"ip also holds for the much rarer words sloth and sloths. These functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes. For instance, cats consists of two morphemes, cat and -s, with the latter shared by the words dogs and tarsiers. Modeling this effect is crucial for languages with rich morphology, where vocabulary sizes are larger, many more words are rare, and many more such functional relationships exist. Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch¨utze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Pl"
P17-1184,J93-2004,0,0.0617446,"ubword-lstm-lm models of Ling et al. (2015). Even following detailed discussion with Ling (p.c.), we were unable to reproduce their perplexities exactly—our English reimplementation gives lower perplexities; our Turkish higher—but we do reproduce their general result that character bi-LSTMs outperform word models. We suspect that different preprocessing and the stochastic learning explains differences in perplexities. Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al. (2010). 4.1 Training and Evaluation Our LSTM-LM uses two hidden layers with 200 hidden units and representation vectors for words, characters, and morphs all have dimension 200. All parameters are initialized uniformly at random from -0.1 to 0.1, trained by stochastic gradient descent with mini-batch size of 32, time steps of 20, for 50 epochs. To avoid overfitting, we apply dropout with probability 0.5 on the input-tohidden layer and all of the LSTM cells (including those in the bi-LSTM, if used). For all models which do not use bi-LSTM composition, we start w"
P17-1184,pasha-etal-2014-madamira,0,0.0468606,"Missing"
P17-1184,P16-2067,0,0.0427263,"13; Botha and Blunsom, 2014; Cotterell and Sch¨utze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters? The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on 2016 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2016–2027 c Vancouver, Ca"
P17-1184,C14-1015,0,0.0711453,"they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters? The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on 2016 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2016–2027 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org"
P17-1184,C16-1030,0,0.0392575,"Missing"
P17-1184,P16-1162,0,0.0418019,"ations for the vocabulary of subword units; and f is a composition function which takes σ(w) and Ws as input and returns w. All of the representations that we consider take this form, varying only in f and σ. 3.1 Subword Units We consider four variants of σ in Equation 1, each returning a different type of subword unit: character, character trigram, or one of two types of morph. Morphs are obtained from Morfessor (Smit et al., 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016). BPE works by iteratively replacing frequent pairs of characters with a single unused character. For Morfessor, we use default parameters while for BPE we set the number of merge operations to 10,000.1 When we segment into character trigrams, we consider all trigrams in the word, including those covering notional beginning and end of word characters, as in Sperr et al. (2013). Example output of σ is shown in Table 3. 3.2 Composition Functions We use three variants of f in Eq. 1. The first constructs the representation w of word w by adding the representations of its subwords s1 , . . . , sn ="
P17-1184,E14-2006,0,0.0696807,"Missing"
P17-1184,W13-3204,0,0.568819,"s a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch¨utze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters? The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on 2016 Proceedings of the 5"
P17-1184,D16-1157,0,0.00393581,"ord units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Sch¨utze, 2015). A downside of these models is that they depend on morphological segmenters or analyzers. Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014). These models are compact, can represent rare and unknown words, and do not require morphological analyzers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters? The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on 2016 Proceedings of the 55th Annual Meeting of"
P17-1184,D16-1209,0,\N,Missing
P17-1184,Q17-1026,0,\N,Missing
P17-2019,W09-3839,0,0.0291949,"Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.g., for transition-based parsers). In this work, we propose a parsing and language modeling framework that marries a generative model with a discriminative recognition algorithm in order to have the best of both worlds. The idea of combining these two types of models is not new. For example, Collins and Koo (2005) propose to use a generative model to generate candidate constituency trees and a discriminative model to rank them. Sangati et al. (2009) follow the opposite direction and employ a generative model to re-rank the dependency trees produced by a discriminative parser. However, previous work combines the two types of models in a goal-oriented, pipeline fashion, which lacks model interpretations and focuses solely on parsing. In comparison, our framework unifies generative and discriminative parsers with a single objective, which connects to expectation maximization and variational inference in grammar induction settings. In a nutshell, we treat parse trees as latent factors generating natural language sentences and parsing as a po"
P17-2019,N03-1014,0,0.0523765,"he framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1 1 Introduction Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing. At test time, these models require a relatively expensive recognition algo1 Our code is available at https://github.com/ cheng6076/virnng.git. 118 Proceedings of the 55th Annual Meeting of the Association"
P17-2019,P12-1046,0,0.0413555,"Missing"
P17-2019,P08-1067,0,0.103524,"Missing"
P17-2019,P13-1045,0,0.0510952,"Missing"
P17-2019,P07-1080,0,0.0303711,"d on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1 1 Introduction Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing. At test time, these models require a relatively expensive recognition algo1 Our code is available at https://github.com/ cheng6076/virnng.git. 118 Proceedings of the 55th Annual Meeting of the Association for Computational Linguisti"
P17-2019,D16-1031,0,0.165612,") The encoder is a discriminative RNNG that computes the conditional probability q(a|x) of the transition action sequence a given an observed sentence x. This conditional probability is factorized over time steps as: q(a|x) = |a| Y t=1 q(at |vt ) log p(x) ≥ Eq(a|x) log p(x, a) = Lx q(a|x) (8) where p(x, a) = p(x|a)p(a) comes from the decoder or the generative model, and q(a|x) comes from the encoder or the recognition model. The objective function6 in Equation (8), denoted by Lx , is unsupervised and suited to a grammar induction task. This objective can be optimized with the methods shown in Miao and Blunsom (2016). Next, consider the case when the parse tree is observed. We can directly maximize the log likelihood of the parse tree for the encoder output log q(a|x) and the decoder output log p(a): (5) where vt is the transitional state embedding of the encoder at time step t. The next action is predicted similarly to Equation (2), but conditioned on vt . Thanks to the discriminative property, vt has access to any contextual features defined over the entire sentence and the stack — q(a|x) acts as a context sensitive posterior approximation. Our features4 are: 1) the stack embedding et obtained with a st"
P17-2019,P13-1043,0,0.0542544,"Missing"
P17-2019,N07-1051,0,0.0698452,"on and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1 1 Introduction Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing. At test time, these models require a relatively expensive recognition algo1 Our code is available at https://github.com/ cheng6076/virnng.git. 118 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages"
P17-2019,J03-4003,0,\N,Missing
P17-2019,J05-1003,0,\N,Missing
P17-2019,D14-1162,0,\N,Missing
P17-2019,N16-1024,0,\N,Missing
P17-2019,N10-1115,0,\N,Missing
Q13-1014,P05-1074,1,0.728369,"Missing"
Q13-1014,W08-0208,0,0.0265935,", and supervised learning, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a na¨ıve solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1 http://alopez.github.io/dreamt 166 gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align |grade Students could then run experiments within minutes of beginning the assig"
Q13-1014,W11-2101,0,0.0267628,"Missing"
Q13-1014,J93-2003,0,0.0664021,"nload/hansard/ 9 This invited the possibility of cheating, since alignments of the test data are publicly available on the web. We did not advertise this, but as an added safeguard we obfuscated the data by distributing the test sentences randomly throughout the file. 20 AER × 100 Listing 1 The default aligner in DREAMT: thresholding Dice’s coefficient. for (f, e) in bitext: for f_i in set(f): f_count[f_i] += 1 for e_j in set(e): fe_count[(f_i,e_j)] += 1 for e_j in set(e): e_count[e_j] += 1 30 40 50 60 due ays ays ays -2 d -4 d -6 d ays days -8 d 168 days We privately implemented IBM Model 1 (Brown et al., 1993) as the target algorithm for a passing grade. We ran it for five iterations with English as the target language and French as the source. Our implementation did not use null alignment or symmetrization—leaving out these common improvements offered students the possibility of discovering them independently, and thereby rewarded. -10 > align -n 1000 |grade By varying the number of input sentences and the threshold for an alignment, students could immediately see the effect of various parameters on alignment quality. days The default implementation enabled immediate experimentation. On receipt of"
Q13-1014,W09-0401,1,0.880881,"Missing"
Q13-1014,W11-2103,1,0.932739,"between the human ranking and an output ranking. The check program simply ensures that a submission contains a valid ranking. We were concerned about hill-climbing on the test data, so we modified the leaderboard to report new results only twice a day. This encouraged students to experiment on the development data before posting new submissions, while still providing intermittent feedback. We privately implemented a version of BLEU, which obtained a correlation of 38.6 with the human rankings, a modest improvement over the baseline of 34.0. Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphr"
Q13-1014,W12-3102,1,0.884682,"Missing"
Q13-1014,D11-1003,0,0.016178,"4). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. 11 We implemented a version of the Lagrangian relaxation algorithm of Chang and Collins (2011), but found it difficult to obtain tight (optimal) solutions without iteratively reintroducing all of the original constraints. We suspect this is due to the lack of a distortion penalty, which enforces a strong preference towards translations with little reordering. However, the solution found by this algorithm is only approximates the objective implied by Equation 2, which sums over alignments. 170 Many teams who implemented the standard stack decoding algorithm experimented heavily with its pruning parameters. The best submission used extremely wide beam settings in conjunction with a reimp"
Q13-1014,W00-0601,0,0.0575064,"5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and 174 validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alig"
Q13-1014,J07-2003,0,0.0183732,"improvement, and thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2 http://python.org Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4 Thanks to an anonymous reviewer for this turn of phrase. 3 This scheme provided strong incentive to continue experimentation beyond the target algorithm.5 For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the compe"
Q13-1014,P08-2007,0,0.0256888,"lement consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 p(e, a|f ) = Y hi,i0 ,j,j 0 i∈a 0 0 p(fii |ejj ) J+1 Y j=1 p(ej |ej−1 , ej−2 ) (1) To evaluate output, we compute the conditional probability of e as follows. p(e|f ) = X p(e, a|f ) (2) a Note that this formulation is different from the typical Viterbi objective of standard beam search decoders, which do not sum over all alignments, but approximate p(e|f ) by maxa p(e, a|f ). Though the computation in Equation 2 is intractable (DeNero and Klein, 2008), it can be computed in a few minutes via dynamic programming on reasonably short sentences. We ensured that our data met this criterion. The corpus-level probability is then the product of all sentence-level probabilities in the data. The model includes no distortion limit or distortion model, for two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it ac"
Q13-1014,P10-4002,1,0.928176,"now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each imple"
Q13-1014,W08-0212,0,0.0278916,"ting final grades. Each student rated aspects of the course on a five point Likert scale, from 1 (strongly disagree) to 5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, includi"
Q13-1014,J07-3002,0,0.0344561,"Missing"
Q13-1014,W12-3134,1,0.927189,"re excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each implementation consists of a na¨ıve baseline"
Q13-1014,P01-1030,0,0.330619,"Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding al"
Q13-1014,P07-1019,0,0.0264172,"nd thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2 http://python.org Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4 Thanks to an anonymous reviewer for this turn of phrase. 3 This scheme provided strong incentive to continue experimentation beyond the target algorithm.5 For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the competitive effort.6 This stra"
Q13-1014,D07-1031,0,0.0154999,"saw many other solutions, indicating that many truly experimented with the problem: • Implementing heuristic constraints to require alignment of proper names and punctuation. • Running the algorithm on stems rather than surface words. • Initializing the first iteration of Model 1 with parameters estimated on the observed alignments in the development data. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an"
Q13-1014,N06-1058,0,0.0115716,"performs the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge The fourth challenge was reran"
Q13-1014,W05-0104,0,0.0345053,"eria. Everyone who completed all four assignments placed in the top five at least once. 6 The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. 167 Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data"
Q13-1014,J99-4005,0,0.395409,"lass on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key te"
Q13-1014,N03-1017,0,0.0330225,"to note that AER does not necessarily correlate with downstream performance, particularly on the Hansards dataset (Fraser and Marcu, 2007). We used the conclusion of the assignment as an opportunity to emphasize this point. 4 The Decoding Challenge The second challenge was decoding: given a fixed translation model and a set of input sentences, students were challenged to produce translations with the highest model score. This challenge introduced the difficulties of combinatorial optimization under a deceptively simple setup: the model we provided was a simple phrase-based translation model (Koehn et al., 2003) consisting only of a phrase table and tri169 gram language model. Under this simple model, for a French sentence f of length I, English sentence e of length J, and alignment a where each element consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 p(e, a|f ) = Y hi,i0 ,j,j 0 i∈a 0 0 p(fii |ejj ) J+1 Y j=1 p(ej |ej−1 , ej−2 ) (1) To evaluate output, we compute the conditional probability of e as follows. p(e|f ) = X p(e, a|f ) (2) a Note that this formulation is different from the typica"
Q13-1014,P07-2045,1,0.0187571,"rsities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation,"
Q13-1014,koen-2004-pharaoh,0,0.0336528,"ntained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with the pseudocode in Koehn’s (2010) popular textbook (reproduced here as Algorithm 1). The second program, grade, computes the log-probability of a set of translations, as outline above. We privately implemented a simple stack decoder that searched over permutations of phrases, similar to Koehn (2004). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. 11 We implemented a version of the Lagrangian relaxation algorithm of Ch"
Q13-1014,J10-4005,0,0.199806,"remaining authors were students in the worked described here. This research was conducted while Chris Callison-Burch was at Johns Hopkins University. in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source cod"
Q13-1014,2012.iwslt-papers.5,0,0.0214715,"Missing"
Q13-1014,N04-1022,0,0.0453486,"rank order of the underlying translation system. Students discovered that simply returning the first can173 didate earned a very high score, and most of them quickly converged to this solution. Unfortunately, the high accuracy of this baseline left little room for additional competition. Nevertheless, we were encouraged that most students discovered this by accident while attempting other strategies to rerank the translations. • Experimentation with parameters of the PRO algorithm. • Substitution of alternative learning algorithms. • Implementation of a simplified minimum Bayes risk reranker (Kumar and Byrne, 2004). Over a baseline of 24.02, the latter approach obtained a BLEU of 27.08, nearly matching the score of 27.39 from the underlying system despite an impoverished feature set. 7 Pedagogical Outcomes Could our students have obtained similar results by running standard toolkits? Undoubtedly. However, our goal was for students to learn by doing: they obtained these results by implementing key MT algorithms, observing their behavior on real data, and improving them. This left them with much more insight into how MT systems actually work, and in this sense, DREAMT was a success. At the end of class, w"
Q13-1014,2007.tmi-papers.13,0,0.0443585,"serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with"
Q13-1014,N06-1014,0,0.0123873,"nt not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with t"
Q13-1014,C04-1072,0,0.0405183,"Missing"
Q13-1014,J10-3002,0,0.0284528,"Missing"
Q13-1014,D07-1104,1,0.803017,"t know the true solution.11 We also posted an oracle containing the most probable output for each sentence, selected from among all submissions received so far. The intent of this oracle was to provide a lower bound on the best possible output, giving students additional incentive to continue improving their systems. 4.1 Data 4.3 We chose 48 French sentences totaling 716 words from the Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the l"
Q13-1014,W12-3101,1,0.89068,"Missing"
Q13-1014,W08-0209,0,0.0250327,"ng, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a na¨ıve solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1 http://alopez.github.io/dreamt 166 gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align |grade Students could then run experiments within minutes of beginning the assignment. Three of the four cha"
Q13-1014,J00-2004,0,0.0633455,"Missing"
Q13-1014,W03-0301,0,0.0297199,"a We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with the data. The first, align, computes Dice’s coefficient (1945) for every pair of French and English words, then aligns every pair for which its value is above an adjustable threshold. Our implementation (most of 7 Among them, Jordan Boyd-Graber, John DeNero, Philipp Koehn, and Slav Petrov (personal communication). 8 http://www.is"
Q13-1014,P04-1066,0,0.0562929,"Missing"
Q13-1014,P00-1056,0,0.0804879,"signment would earn an A; and top three placement compensated for weaker grades in other course criteria. Everyone who completed all four assignments placed in the top five at least once. 6 The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. 167 Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enoug"
Q13-1014,J03-1002,0,0.00871586,"Missing"
Q13-1014,N04-1021,0,0.039354,"as a simple program that produced a vector of feature weights using pairwise ranking optimization (PRO; Hopkins and May, 2011), with a perceptron as the underlying learning algorithm. A second, rerank, takes a weight vector as input and reranks the sentences; both programs were designed to work with arbitrary numbers of features. The grade program computed the BLEU score on development data, while check ensured that a test submission is valid. Finally, we provided an oracle program, which computed a lower bound on the achievable BLEU score on the development data using a greedy approximation (Och et al., 2004). The leaderboard likewise displayed an oracle on test data. We did not assign a target algorithm, but left the assignment fully open-ended. 6.3 Reranking Challenge Outcome For each assignment, we made an effort to create room for competition above the target algorithm. However, we did not accomplish this in the reranking challenge: we had removed most of the features from the candidate translations, in hopes that students might reinvent some of them, but we left one highly predictive implicit feature in the data: the rank order of the underlying translation system. Students discovered that si"
Q13-1014,W06-3112,0,0.013171,"Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge T"
Q13-1014,P02-1040,0,0.0866501,"Missing"
Q13-1014,P09-1038,0,0.0198477,"or two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it actually makes the problem more difficult, since a simple distance-based distortion model prefers translations with fewer permutations; without it, the model may easily prefer any permutation of the target phrases, making even the Viterbi search problem exhibit its true NP-hardness (Knight, 1999a; Zaslavskiy et al., 2009). Since the goal was to find the translation with the highest probability, we did not provide a held-out test set; with access to both the input sentences and 10 For simplicity, this formula assumes that e is padded with two sentence-initial symbols and one sentence-final symbol, and ignores the probability of sentence segmentation, which we take to be uniform. the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the translation that maximizes the evaluation. Indeed, since the problem is int"
Q13-1014,W11-2105,0,\N,Missing
Q15-1007,D07-1090,0,0.0910812,"rmance analysis in §4.3 shows that even in grammar extraction there are CPU bottlenecks we need to address and opportunities for further optimization. Beyond grammar extraction, there is a question about whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency,"
Q15-1007,P05-1032,0,0.20276,"rd. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk. An alternative is to store the indexed parallel text in memory and extract transla"
Q15-1007,D13-1195,0,0.16187,"out whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under award IIS-1218043; and the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusions, or recommendation"
Q15-1007,J07-2003,0,0.275671,"applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk. An alternative is to store the indexed parallel text in memory and extract translation units on demand only when they are This paper presents a novel GPU algorithm for on-demand extr"
Q15-1007,P11-2031,0,0.0163268,"d SMT baseline. Phrase tables generated by Moses are essentially the same as the GPU implementation of on-demand extraction for phrase-based translation by He et al. (2013). 4 4.1 Results Translation quality We first verified that our GPU implementation achieves the same translation quality as the corresponding CPU baseline. This is accomplished by comparing system output against the baseline systems, training on Xinhua, tuning on NIST03, and testing on NIST05. In all cases, we used MIRA (Chiang, 2012) to tune parameters. We ran experiments three times and report the average as recommended by Clark et al. (2011). Hierarchical grammars were extracted with sampling at a rate of 300; we also bound source patterns at a length of 5 and matches at a length of 15. For Moses we used default parameters. Our BLEU scores, shown in Table 1, replicate well-known results where hierarchical models outperform pure phrase-based models on this task. The difference in quality is partly because the phrasebased baseline system does not use lexicalized reordering, which provides similar improvements to hierarchical translation (Lopez, 2008b). Such lexicalized reordering models cannot be produced by the GPU-based system of"
Q15-1007,P10-4002,1,0.924694,"action of hierarchical translation models based on matching and extracting gappy phrases. Our experiments examine both grammar extraction and end-to-end translation, comparing quality, speed, and memory use. We compare against the GPU system for phrase-based translation by He et al. (2013) and cdec, a state-of-the-art 87 Transactions of the Association for Computational Linguistics, vol. 3, pp. 87–100, 2015. Action Editor: David Chiang. c Submission batch: 6/2014; Revision batch 12/2014; Published 2/2015. 2015 Association for Computational Linguistics. CPU system for hierarchical translation (Dyer et al., 2010). Our system outperforms the former on translation quality by 2.3 BLEU (replicating previously-known results) and outperforms the latter on speed, improving grammar extraction throughput by at least an order of magnitude on large batches of sentences while maintaining the same level of translation quality. Our contribution is to show, complete with an open-source implementation, how GPUs can vastly increase the speed of hierarchical grammar extraction, particularly for high-throughput MT applications. 2 Algorithms GPU architectures, which are optimized for massively parallel operations on rela"
Q15-1007,E12-1025,0,0.0262498,"Missing"
Q15-1007,P14-1020,0,0.181223,"can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under award IIS-1218043; and the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusions, or recommendations expressed in this"
Q15-1007,N13-1033,1,0.138086,"hput by roughly two thirds on a standard MT evaluation dataset. The GPU necessary to achieve these improvements increases the cost of a server by about a third. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a la"
Q15-1007,N10-1062,0,0.0326637,"Missing"
Q15-1007,D07-1104,1,0.77137,"d 15 to a separate thread. The thread assigned position 2 scans T for matches of him until the end of sentence at position 9, finding matches (2, 4) and (2, 8). 90 As a second example, consider it ? and. In this case, it has four matches, but and only two. So, we need only two threads, each scanning backwards from matches of and. Since most patterns are infrequent, allocating threads this way minimizes work. However, very large corpora contain one-gap patterns for which both subpatterns are frequent. We simply precompute all matches for these patterns and retrieve them at runtime, as in Lopez (2007). This precomputation is performed once given T and therefore it is a one-time cost. Materializing every match of u ? v would consume substantial memory, so we only emit those for which a translation of the substring matching ? is extractable using the check in §2.3. The success of this check is a prerequisite for extracting the translation of u ? v or any pattern containing it, so pruning in this way conserves GPU memory without affecting the final grammar. Passes 5-7: Finding two-gap patterns (New) We next find all patterns with two gaps of the form u ? v ? w. The search is similar to passes"
Q15-1007,C08-1064,1,0.546959,"r algorithms that use these scans as subroutines, we parallelize the scans themselves in a fine-grained manner to obtain high throughput. The relatively small size of the GPU memory also affects design decisions. Data transfer between the GPU and the CPU has high latency, so we want to avoid shuffling data as much as possible. To accomplish this, we must fit all our data structures into the 5 GB memory available on our particular GPU. As we will show, this requires some tradeoffs in addition to careful design of algorithms and associated data structures. 88 2.1 Translation by Pattern Matching Lopez (2008b) provides a recipe for “translation by pattern matching” that we use as a guide for the remainder of this paper (Algorithm 1). Algorithm 1 Translation by pattern matching 1: for each input sentence do 2: for each phrase in the sentence do 3: Find its occurrences in the source text 4: for each occurrence do 5: Extract any aligned target phrase 6: for each extracted phrase pair do 7: Compute feature values 8: Decode as usual using the scored rules We encounter a computational bottleneck in lines 2–7, since there are many query phrases, matching occurrences, and extracted phrase pairs to proces"
Q15-1007,W99-0604,0,0.214195,"umerated in pass 5, we scan T from position j + |v |+ 1 for matches of w until we reach the end of sentence. As with the one-gap patterns, we apply the extraction check on the second ? of the two-gap patterns u ? v ? w to avoid needlessly materializing matches that will not yield translations. 2.3 Extracting Every Target Phrase In line 5 of Algorithm 1 we must extract the aligned translation of every match of every pattern found in T . Efficiency is crucial since some patterns may occur hundreds of thousands of times. We extract translations from word alignments using the consistency check of Och et al. (1999). A pair of substrings is consistent only if no word in either substring is aligned to any word outside the pair. For example, in Figure 2 the pair (it sets him on, los excita) is consistent. The pair (him on and, los excita y) is not, because excita also aligns to the words it sets. Only consistent pairs can be paraliza los y excita los # it sets him on and it takes him off L0 R0 L 9 10 11 12 13 14 15 16 17 18 2 2 1 2 3 5 5 4 5 R 7 2 2 1 2 3 5 5 4 5 store the sentence-relative positions of the leftmost and rightmost words it aligns to in T 0 , and P [i] stores T [i]’s sentence-relative positi"
Q15-1007,P12-1002,0,0.0126572,"e University of Maryland College Park, Maryland huah@cs.umd.edu Jimmy Lin The iSchool and UMIACS University of Maryland College Park, Maryland jimmylin@umd.edu Abstract needed to decode new input. This architecture has several advantages: It requires only a few gigabytes to represent a model that would otherwise require a terabyte (Lopez, 2008b). It can adapt incrementally to new training data (Levenberg et al., 2010), making it useful for interactive translation (Gonz´alezRubio et al., 2012). It supports rule extraction that is sensitive to the input sentence, enabling leave-oneout training (Simianer et al., 2012) and the use of sentence similarity features (Philips, 2012). Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations. This approach is limited in practical applications by the computational expense of online lookup and extraction. For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do not work for hierarchical models, which require matching patterns t"
Q15-1007,P07-1065,0,0.0251396,"tion pipeline are amenable to GPU algorithms. The performance analysis in §4.3 shows that even in grammar extraction there are CPU bottlenecks we need to address and opportunities for further optimization. Beyond grammar extraction, there is a question about whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program"
Q15-1007,W11-2921,0,0.199351,"is a question about whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under award IIS-1218043; and the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusion"
Q15-1007,2005.eamt-1.39,0,0.0348882,"extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk. An alternative is to store the indexed parallel text in memory and extract translation units on demand onl"
Q15-1007,N10-1140,0,\N,Missing
Q15-1007,H05-1095,0,\N,Missing
Q15-1007,P07-2045,0,\N,Missing
S17-1024,P13-1023,0,0.0237475,"erve the compositional semantics of sentences and documents because they model language as bags of words, or at best syntactic trees. To preserve semantics, they must model semantics. In pursuit of this goal, several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs (DAGs), including the Abstract Meaning Representation Bank (AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013). To make use of this data, we require models of graphs. Consider how we might use compositional semantic representations in machine translation Recent work in NLP has focused primarily on hyperedge replacement grammar (HRG; Drewes et al. 1997), a context-free graph grammar formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). In particular, Chiang et al. (2013) propose that HRG could be used to represent semantic graphs, and precisely characterize the complexity of a CKY-style 199 Proceedings of the 6th Joint"
S17-1024,W13-2322,0,0.3268,"graph recognition to avoid confusion with other parsing problems. Introduction NLP systems for machine translation, summarization, paraphrasing, and other tasks often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words, or at best syntactic trees. To preserve semantics, they must model semantics. In pursuit of this goal, several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs (DAGs), including the Abstract Meaning Representation Bank (AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013). To make use of this data, we require models of graphs. Consider how we might use compositional semantic representations in machine translation Recent work in NLP has focused primarily on hyperedge replacement grammar (HRG; Drewes et al. 1997), a context-free graph grammar formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). In"
S17-1024,P81-1022,0,0.71237,"Figure 6 shows a tree generating RGG that generates binary trees the internal nodes of which are represented by a-labeled edges, and the leaves of which are represented by b-labeled edges. Note that these two results of regularity of the string- and tree-languages generated by RGG follow from the fact that graph languages produced by RGG are MSO-definable (Courcelle, 1991), and the well-known facts that the regular string and graph languages are MSO-definable. 1 a X 1 (1) Y 3.1 Just as the algorithm of Chiang et al. (2013) generalizes CKY to HRG, our algorithm generalizes Earley’s algorithm (Earley, 1970). Both algorithms operate by recognizing incrementally larger subgraphs of the input graph, using a succinct representation for subgraphs that depends on an arbitrarily chosen marker node m of the input graph. Definition 6. (Chiang et al. 2013; Definition 6) Let I be a subgraph of a graph G. A boundary node of I is a node which is either an endpoint of an edge in GI or an external node of G. A boundary edge of I is an edge in I which has a boundary node as an endpoint. The boundary representation of I is the tuple b(I) = hbn(I), be(I), m ∈ Ii where 1. bn(I) is the set of boundary nodes of I 2"
S17-1024,W16-3311,0,0.0558133,"(AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013). To make use of this data, we require models of graphs. Consider how we might use compositional semantic representations in machine translation Recent work in NLP has focused primarily on hyperedge replacement grammar (HRG; Drewes et al. 1997), a context-free graph grammar formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). In particular, Chiang et al. (2013) propose that HRG could be used to represent semantic graphs, and precisely characterize the complexity of a CKY-style 199 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 199–208, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics Example 1. Hypergraph G in Figure 2 has four nodes (shown as black dots) and three hyperedges labeled a, b, and X (shown boxed). The bracketed numbers (1) and (2) denote its external nodes and the numbers between edges and the nodes are tentacle l"
S17-1024,hajic-etal-2012-announcing,0,0.050758,"Missing"
S17-1024,P13-1091,0,0.59627,"he Abstract Meaning Representation Bank (AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013). To make use of this data, we require models of graphs. Consider how we might use compositional semantic representations in machine translation Recent work in NLP has focused primarily on hyperedge replacement grammar (HRG; Drewes et al. 1997), a context-free graph grammar formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). In particular, Chiang et al. (2013) propose that HRG could be used to represent semantic graphs, and precisely characterize the complexity of a CKY-style 199 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 199–208, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics Example 1. Hypergraph G in Figure 2 has four nodes (shown as black dots) and three hyperedges labeled a, b, and X (shown boxed). The bracketed numbers (1) and (2) denote its external nodes and the number"
S17-1024,C12-1083,0,0.220303,"Missing"
S17-1024,K15-1004,0,0.0260891,"epresentation Bank (AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013). To make use of this data, we require models of graphs. Consider how we might use compositional semantic representations in machine translation Recent work in NLP has focused primarily on hyperedge replacement grammar (HRG; Drewes et al. 1997), a context-free graph grammar formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). In particular, Chiang et al. (2013) propose that HRG could be used to represent semantic graphs, and precisely characterize the complexity of a CKY-style 199 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 199–208, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics Example 1. Hypergraph G in Figure 2 has four nodes (shown as black dots) and three hyperedges labeled a, b, and X (shown boxed). The bracketed numbers (1) and (2) denote its external nodes and the numbers between edges and"
S17-1024,P87-1015,0,0.831073,"Missing"
W05-0812,J93-2003,0,0.0182485,"Missing"
W05-0812,P03-1012,0,0.263489,"distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1 ) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1 ) is assigned using an unsupervised approach (Och, 1999). d(ai |ai−1 ) = p(ai |ai − ai−1,C(eai−1 )) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei , ek ) = (w, x, y) between each pair of English words ei and ek . Given a dependency parse of eM 1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1 We ignore the sentence length probability p(M|N), which is not relevant to word alignment. We also omit discussion of HMM start and stop probabilities, and normalization"
W05-0812,W03-0305,0,0.0933461,"Missing"
W05-0812,W03-0301,0,0.129972,"Missing"
W05-0812,C00-2163,0,0.487721,"th English word eai . This representation introduces 83 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 an asymmetry into the model because it constrains each French word to correspond to exactly one English word, while each English word is permitted to correspond to an arbitrary number of French words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f, a|e) = ∏ d(ai |ai−1 ) · t( fi |eai ) (1) i=1 We refer to d(ai |ai−1 ) as the distortion model and t( f i |eai ) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). I1 very2 much3 doubt4 that5 τ(I1 , very2 ) = (1, 2, 0) τ(very2 , I1 ) = (2, 1, 1) τ(I1 , doubt4 ) = (1, 0, 0)"
W05-0812,P00-1056,0,0.734476,"th English word eai . This representation introduces 83 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 an asymmetry into the model because it constrains each French word to correspond to exactly one English word, while each English word is permitted to correspond to an arbitrary number of French words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f, a|e) = ∏ d(ai |ai−1 ) · t( fi |eai ) (1) i=1 We refer to d(ai |ai−1 ) as the distortion model and t( f i |eai ) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). I1 very2 much3 doubt4 that5 τ(I1 , very2 ) = (1, 2, 0) τ(very2 , I1 ) = (2, 1, 1) τ(I1 , doubt4 ) = (1, 0, 0)"
W05-0812,J03-1002,0,0.0274411,"that this varied depending on the characteristics of the corpus and the type of annotation (in particular, whether the annotation set included probable alignments). The results are summarized in Table 2. It shows results with our HMM model using both Equations 2 and 4 as our distortion model, which represent the unlimited and limited resource tracks, respectively. It also includes a comparison with IBM Model 4, for which we use a training sequence of IBM Model 1 (5 iterations), HMM (6 iterations), and IBM Model 4 (5 iterations). This sequence performed well in an evaluation of the IBM Models (Och and Ney, 2003). For comparative purposes, we show results of applying both P(f|e) and P(e|f) prior to symmetrization, along with results of symmetrization. Comparison of the asymmetric and symmetric results largely supports the hypothesis presented in Section 2.3, as our system generally produces much better recall than IBM Model 4, while offering a competitive precision. Our symmetrized results usually produced higher recall and precision, and lower alignment error rate. We found that the largest gain in performance came from the improved initialization. The combined distortion model (Equation 4), which pr"
W05-0812,E99-1010,0,0.022368,"i , eai−1 ), T (eai−1 )) (3) Since both the surface distortion and tree distortion models represent p(ai |ai−1 ), we can combine them using linear interpolation as in Equation 4. 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have been proposed for the distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1 ) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1 ) is assigned using an unsupervised approach (Och, 1999). d(ai |ai−1 ) = p(ai |ai − ai−1,C(eai−1 )) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei , ek ) = (w, x, y) between each pair of English words ei and ek . Given a"
W05-0812,W02-1012,0,0.197458,"Missing"
W05-0812,C96-2141,0,0.547554,"hough the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f, a|e) = ∏ d(ai |ai−1 ) · t( fi |eai ) (1) i=1 We refer to d(ai |ai−1 ) as the distortion model and t( f i |eai ) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). I1 very2 much3 doubt4 that5 τ(I1 , very2 ) = (1, 2, 0) τ(very2 , I1 ) = (2, 1, 1) τ(I1 , doubt4 ) = (1, 0, 0) τ(that5 , I1 ) = (1, 1, 1) Figure 2: Example of tree distances in a sentence from the Romanian-English development set. if i &gt; k; 0 otherwise} is simply a binary indicator of the linear relationship of the words within the surface string. Tree distance is illustrated in Figure 2. In our tree distortion model, we condition on the tree distance and the part of speech T (ei−1 ), giving us Equation 3. d(ai |ai−1 ) = p"
W05-0812,P01-1067,0,0.122892,"ion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1 ) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1 ) is assigned using an unsupervised approach (Och, 1999). d(ai |ai−1 ) = p(ai |ai − ai−1,C(eai−1 )) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei , ek ) = (w, x, y) between each pair of English words ei and ek . Given a dependency parse of eM 1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1 We ignore the sentence length probability p(M|N), which is not relevant to word alignment. We also omit discussion of HMM start and stop probabili"
W05-0812,P04-1066,0,\N,Missing
W09-0437,C08-1144,0,0.171475,"e contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Reference: competition between postal services Hierarchical: postal services Deviation: ( [0-4: @S -> @Xˆ1 |@Xˆ1 ] ( [0-4: @X -> concurrence @Xˆ1 postaux |postal @Xˆ1 ] ( [1-3: @X ) -> des services | services ] postal services ) ) Figure 5: Derivation of a hierarchical translation which cannot be generated by the phrase-based system, in the format"
W09-0437,P06-1002,0,0.031231,"Missing"
W09-0437,D08-1023,0,0.0286128,"Missing"
W09-0437,J07-2003,0,0.0953474,"tion for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was implemented by discarding rules and chart entries which do not match the reference. How Similar are Model Search Spaces? Most work on hierarchical phrase-based translation focuses quite intently on its structural differences from phrase-based translation. • A hierarchical model can tra"
W09-0437,W07-0414,0,0.210248,"Missing"
W09-0437,P07-1094,0,0.0347971,"t criterion such as WER and measure the amount of deviation from the reference. We could also maximize BLEU with respect to the reference as in Dreyer et al. (2007), but it is less interpretable. 7 Conclusion and Future Work Sparse distributions are common in natural language processing, and machine translation is no exception. We showed that utilizing more of the entire distribution can dramatically improve the coverage of translation models, and possibly their accuracy. Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (Goldwater and Griffiths, 2007). Considering the entire tail is challenging, since the search space grows exponentially with the number of translation options. A first step might be to use features that facilitate more variety in the top 20 translation options. A more elaborate aim is to look into alternatives to maximum likelihood hood estimation such as in Blunsom and Osborne (2008). Additionally, our expressiveness analysis shows Acknowledgements This research was supported by the Euromatrix Project funded by the European Commission (6th Framework Programme). The experiments were conducted using the resources provided by"
W09-0437,N03-1017,1,0.0368304,"otone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of these models is that these differences in their generative stories are respon"
W09-0437,P07-2045,1,0.0272604,"coders to generate the reference via disallowing reference-incompatible hypothesis or chart entries. This leaves only some search restrictions such as the distortion limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was imp"
W09-0437,2005.mtsummit-papers.11,1,0.0556356,"reorder phrases arbitrarily within the distortion limit, while the hierarchical model requires some lexical evidence for movement, resorting to monotone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is"
W09-0437,C08-1064,1,0.811433,"spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences. 1 • Second, we find that the high-probability regions in the search spaces of phrase-based and hierarchical systems are nearly identical (§4). This means that reported differences between the models are due to their rankings of competing hypotheses, rather than structural differences of the derivations they produce. 2 Introduction Models, Search Spaces, and Errors A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008a; 2009). A ruleset licenses the steps by which a source string f1 ...fI may be rewritten as a target string e1 ...eJ . A parameterization defines a weight function over every sequence of rule applications. In a phrase-based model, the ruleset is simply the unweighted phrase table, where each phrase pair fi ...fi0 /ej ...ej 0 states that phrase fi ...fi0 in the source can be rewritten as ej ...ej 0 in the target. The model operates by iteratively applying rewrites to the source sentence until each source word has been consumed by exactly one rule. There are two additional heuristic rules: The"
W09-0437,E09-1061,1,0.884152,"Missing"
W09-0437,J03-1002,0,0.00694681,"chical model requires some lexical evidence for movement, resorting to monotone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of th"
W09-0437,P03-1021,0,0.063762,"nts in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of these models is that these differences in their generative stories are responsible for differences in performance. We believe that this assumption should be investigated empirically. In an interesting analysis of phrase-based and hierarchical translation, Zollmann et al. (2008) forced a phrase-based system to produce the translations generated by"
W09-0437,P02-1040,0,0.0796532,"e are two additional heuristic rules: The distortion limit dl constrains distances over which phrases can be reordered, and the translation option limit tol constrains the number of target phrases that may be considered for any given source phrase. Together, these rules completely determine the finite set of all possible target sentences for a given source sentence. We call this set of target sentences the model search space. The parameterization of the model includes all information needed to score any particular seMost empirical work in translation analyzes models and algorithms using BLEU (Papineni et al., 2002) and related metrics. Though such metrics are useful as sanity checks in iterative system development, they are less useful as analytical tools. The performance of a translation system depends on the complex interaction of several different components. Since metrics assess only output, they fail to inform us about the consequences of these interactions, and thus provide no insight into the errors made by a system, or into the design tradeoffs of competing systems. In this work, we show that it is possible to obtain such insights by analyzing translation system components in isolation. We focus"
W09-0437,2008.amta-srw.6,0,0.0121521,"n limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was implemented by discarding rules and chart entries which do not match the reference. How Similar are Model Search Spaces? Most work on hierarchical phrase-based tra"
W09-0437,P06-1123,0,0.0248631,"ation involves both the reordering of the translation of postaux and the omittance of a translation for concurrence. This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case. The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Reference: competition between"
W09-0437,C08-1136,0,0.0120129,": The second rule application involves both the reordering of the translation of postaux and the omittance of a translation for concurrence. This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case. The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Re"
W09-1114,P08-1024,1,0.253533,"r an advantage on an appropriately optimised model. 4 Minimum risk training In the previous section, we described how our sampler can be used to search for the best translation under a variety of decoding criteria (max derivation, translation, and minimum risk). However, there appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis tran"
W09-1114,J93-2003,0,0.0151602,"xpected risk training and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus"
W09-1114,D08-1033,0,0.0826863,"n reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some proble"
W09-1114,W06-1673,0,0.0295365,"rementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. 109 This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001; and by the EuroMatrix project funded by the"
W09-1114,P01-1030,0,0.032787,"ns. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from it"
W09-1114,D07-1103,0,0.222812,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N07-1018,0,0.206505,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N03-1017,1,0.202563,"s for probabilistic inference: 1. It typically differs from the true model maximum. 2. It often requires additional approximations in search, leading to further error. 3. It introduces restrictions on models, such as use of only local features. 4. It provides no good solution to compute the normalization factor Z(f ) required by many probabilistic algorithms. In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks. Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model (Koehn et al., 2003) but operates in linear time with respect to the number of input words (Section 2). We show Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics that it is effective for both decoding (Section 3) and minimum risk training (Section 4). 2 A Gibbs sampler for phrase-based translation models We begin by assuming a phrase-based translation model in which the input sentence, f , is segmented into phrases, which are sequences of adjacent words.1 Each foreign phrase is"
W09-1114,P07-2045,1,0.0245677,"en the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and the sampler may concentrate on a very narrow probability region. We optimised the scaling factor on a 200-sentence portion of the tuning set, finding that a multiplicative factor of 10 worked best for fr-en and a multiplicative factor of 6 for de-en. 3 The first experiment shows the effect of different initialisations and numbers of sampler iterations on max-derivation decoding performance of the sampler. The Moses decoder (Koehn et al., 2007) was used to generate the starting hypothesis, either in full DP max-derivation mode, or alternatively with restrictions on the features and reordering, or with zero weights to simulate a random initialisation, and the number of iterations varied from 100 to 200,000, with a 100 iteration burn-in in each case. Figure 3 shows the variation of model score with sampler iteration, for the different starting points, and for both language pairs. 3 We experimented with annealing, where the scale factor is gradually increased to sharpen the distribution while sampling. However, we found no improvements"
W09-1114,N04-1022,0,0.549837,"formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and"
W09-1114,W02-1018,0,0.133102,"ntroduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfo"
W09-1114,N06-1045,0,0.0117505,"led to a situation where entire classes of potentially useful features are not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the t"
W09-1114,C00-2163,0,0.042141,"g and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all o"
W09-1114,P03-1021,0,0.0606256,"nstructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2 The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 105 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1 -norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and t"
W09-1114,P02-1040,0,0.0772047,"words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either running in max-derivation and max-translation mode. Using the Gibbs sampler in this way mak"
W09-1114,C96-2215,0,0.0376306,"Missing"
W09-1114,P06-2101,0,0.637386,"ared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis translation L= X X p(e|f )`eˆ(e) (3) hˆ e,f i∈D e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters θ that minimise (3). Fortunately, with the log-linear parameterization of p(e|f ), L is differentiable with respect to"
W09-1114,N07-1062,0,0.101321,"of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common"
W09-1114,D07-1055,0,0.0381723,"th the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sample"
W09-1114,P08-1012,0,0.00643602,"framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effe"
W11-2160,J07-2003,0,0.622091,"ntire MT pipeline, from data preparation to evaluation. This script is built on top of a module called CachePipe. CachePipe is a simple wrapper around shell commands that uses SHA-1 hashes and explicitlyprovided lists of dependencies to determine whether a command needs to be run, saving time both in running and debugging machine translation pipelines. We present progress on Joshua, an opensource decoder for hierarchical and syntaxbased machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 2 Introduction Joshua is an open-source1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugop"
W11-2160,P10-1146,0,0.0355065,"Missing"
W11-2160,clark-lavie-2010-loonybin,0,0.0141231,"have scores for the final version of the paper. 5 LDC2009T13 4 482 4 CachePipe: Cached pipeline runs Machine translation pipelines involve the specification and execution of many different datasets, training procedures, and pre- and post-processing techniques that can have large effects on translation outcome, and which make direct comparisons between systems difficult. The complexity of managing these pipelines and experimental environments has led to a number of different experimental management systems, such as Experiment.perl,6 Joshua 2.0’s Makefile system (Li et al., 2010), and LoonyBin (Clark and Lavie, 2010). In addition to managing the pipeline, these scripts employ different techniques to avoid expensive recomputation by caching steps. 6 http://www.statmt.org/moses/?n= FactoredTraining.EMS S GLUE VP GLUE COMMA+SBAR+. PP DT+NP VBN NP the reactor type der reaktortyp ADJP will be wird zwar NN operated mit NP with betrieben uran , das JJ , which is nicht angereichert ist . JJ PP GLUE not enriched . VBN NN DT+NP uranium ADJP VP COMMA+SBAR+. GLUE S Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals and non-terminals. The unshaded terminals are"
W11-2160,P10-4002,1,0.121611,"s to describe this past year’s work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshua’s grammar ex1 http://github.com/joshua-decoder/joshua Thrax: grammar extraction In modern machine translation systems such as Joshua (Li et al., 2009) and cdec (Dyer et al., 2010), a translation model is represented as a synchronous context-free grammar (SCFG). Formally, an SCFG may be considered as a tuple (N, S, Tσ , Tτ , G) where N is a set of nonterminal symbols of the grammar, S ∈ N is the goal symbol, Tσ and Tτ are the source- and target-side terminal symbol vocabularies, respectively, and G is a set of production rules of the grammar. Each rule in G is of the form X → hα, γ, ∼i where X ∈ N is a nonterminal symbol, α is a sequence of symbols from N ∪ Tσ , γ is a sequence of 478 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478–484, c E"
W11-2160,N04-1035,0,0.158565,"Missing"
W11-2160,N03-1017,0,0.00301836,"a constant phrase penalty In addition to simple features, Thrax also implements map-reduce features. These are features that require comparing rules in a certain order. Thrax uses Hadoop to sort the rules efficiently and calculate these feature functions. Thrax implements the following map-reduce features: • Phrasal translation probabilities p(α|γ) and p(γ|α), calculated with relative frequency: p(α|γ) = C(α, γ) C(γ) (2) (and vice versa), where C(·) is the number of times a given event was extracted. • Lexical weighting plex (α|γ, A) and plex (γ|α, A). We calculate these weights as given in (Koehn et al., 2003): let A be the alignment between α and γ, so (i, j) ∈ A if and only if the ith word of α is aligned to the jth word of γ. Then we can define plex (γ|α) as n Y i=1 X 1 w(γj |αi ) |{j : (i, j) ∈ A}| (3) (i,j)∈A where αi is the ith word of α, γj is the jth word of γ, and w(y|x) is the relative frequency of seeing word y given x. • Rarity penalty, given by exp(1 − C(X → hα, γi)) (4) where again C(·) is a count of the number of times the rule was extracted. 481 The above features are all implemented and can be turned on or off with a keyword in the Thrax configuration file. It is easy to extend Thr"
W11-2160,N04-1022,0,0.0316682,"to build and test new models. the data. We also removed any sentences longer than 50 tokens (after tokenization). For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser (Petrov et al., 2006) with the provided Treebank-trained grammar. We tuned the model weights against the WMT08 test set (news-test2008) using ZMERT (Zaidan, 2009), an implementation of minimum error-rate training included with Joshua. We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Figure 2 shows an example derivation with an SAMT grammar. To re-case the 1-best test set output, we trained a true-case 5gram language model using the same LM training data as before, and used an SCFG translation model to translate from the lowercased to true-case output. The translation model used rules limited to five tokens in length, and contained no hierarchical rules. 3 Experiments We built systems for six language pairs for the WMT 2011 shared task: cz-en, en-cz, de-en, en-de, fr-en, and en-fr.3 For each language pair, we built both SAMT and hiero grammars.4 Table 3 contains the resu"
W11-2160,W09-0424,1,0.922719,"Missing"
W11-2160,W10-1718,1,0.929398,"s context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 2 Introduction Joshua is an open-source1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugopal, 2006). The main focus of this paper is to describe this past year’s work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshua’s grammar ex1 http://github.com/joshua-decoder/joshua T"
W11-2160,P09-1063,0,0.0262969,"Missing"
W11-2160,C08-1064,1,0.294204,"pus with word-level alignments. SAMT additionally requires that the target side of the corpus be parsed. There are several parameters that can make a significant difference in a grammar’s overall translation performance. Each of these parameters is easily adjustable in Thrax by changing its value in a configuration file. • maximum rule span • maximum span of consistent phrase pairs • maximum number of nonterminals • whether to allow unaligned words at the edges of consistent phrase pairs Chiang (2007) gives reasonable heuristic choices for these parameters when extracting a Hiero grammar, and Lopez (2008) confirms some of them (maximum rule span of 10, maximum number of sourceside symbols at 5, and maximum number of nonterminals at 2 per rule). ?) provided comparisons among phrase-based, hierarchical, and syntax-based models, but did not report extensive experimentation with the model parameterizations. When extracting Hiero- or SAMT-style grammars, the first Hadoop job in the Thrax workflow takes in a parallel corpus and produces a set of rules. But in fact Thrax’s extraction mechanism is more general than that; all it requires is a function that maps a string to a set of rules. This makes it"
W11-2160,P06-1055,0,0.0220837,"n run on Hadoop, as Thrax does. The Joshua and cdec extractors only extract Hiero grammars, and Zollmann and Venugopal’s extractor can only extract SAMT-style grammars. They are not designed to score arbitrary feature sets, either. Since variation in translation models and feature sets can have a significant effect on translation performance, we have developed Thrax in order to make it easy to build and test new models. the data. We also removed any sentences longer than 50 tokens (after tokenization). For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser (Petrov et al., 2006) with the provided Treebank-trained grammar. We tuned the model weights against the WMT08 test set (news-test2008) using ZMERT (Zaidan, 2009), an implementation of minimum error-rate training included with Joshua. We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Figure 2 shows an example derivation with an SAMT grammar. To re-case the 1-best test set output, we trained a true-case 5gram language model using the same LM training data as before, and used an SCFG t"
W11-2160,P99-1039,0,0.0362819,"Missing"
W11-2160,P08-1064,0,\N,Missing
W11-2160,C08-1144,0,\N,Missing
W11-2160,W06-3119,0,\N,Missing
W12-3101,W11-2101,0,0.403609,"Missing"
W12-3101,W10-1703,0,0.0838763,"uding the reference, #sys), and the number of implicit pairwise judgements collected (including the reference, #pairs). edges to be reversed). This hypothesis space should be familiar to most machine translation researchers since it closely resembles the search space defined by a phrase-based translation model (Koehn, 2004). We use Dijkstra’s algorithm (1959) to explore it efficiently; the complete algorithm is simply a generalization of the simple algorithm for acyclic tournaments described above. 5 Experiments and Analysis We experimented with 25 relative ranking tasks produced by WMT 2010 (Callison-Burch et al., 2010) and WMT 2011 (Callison-Burch et al., 2011); the full set is shown in Table 1. For each task we considered four possible methods of ranking the data: sorting by any of Equation 1 through 3, and sorting consistent with reversal of a minimum feedback arc set (MFAS). To weight the edges for the latter approach, we simply used the difference in number of assessments preferring one system over the other; that is, an edge from A to B is weighted count(A ≺ B) − count(A  B). If this quantity is negative, there is instead an edge from B to A. The purpose of this simple weighting is to ensure a solutio"
W12-3101,W11-2103,0,0.159146,"he minimum feedback arc set in a tournament, a wellknown NP-complete problem. All instances of this problem in the workshop data are efficiently solvable, but in some cases the rankings it produces are surprisingly different from the ones previously published. This leads to strong caveats and recommendations for both producers and consumers of these rankings. 1 It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality. Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics. (Callison-Burch et al., 2011) Introduction The value of machine translation depends on its utility to human users, either directly through their use of it, or indirectly through downstream tasks such as cross-lingual information extraction or retrieval. It is therefore essential to assess machine translation systems according to this utility, but there is a widespread perception that direct human assessment is costly, unreproducible, and difficult to interpret. Automatic metrics that predict human utility have therefore attracted substantial attention since they are at least cheap and reproducible given identical The work"
W12-3101,koen-2004-pharaoh,0,0.00787105,"11 Spanish-English syscomb 2011 Spanish-English individual 2011 Urdu-English tunable metrics #sys 17 4 22 4 15 6 18 8 20 6 15 8 #pairs 9086 4374 12996 5930 11130 3000 6986 3844 9079 4156 5652 6257 Table 1: The set of tasks we analyzed, including the number of participating systems (excluding the reference, #sys), and the number of implicit pairwise judgements collected (including the reference, #pairs). edges to be reversed). This hypothesis space should be familiar to most machine translation researchers since it closely resembles the search space defined by a phrase-based translation model (Koehn, 2004). We use Dijkstra’s algorithm (1959) to explore it efficiently; the complete algorithm is simply a generalization of the simple algorithm for acyclic tournaments described above. 5 Experiments and Analysis We experimented with 25 relative ranking tasks produced by WMT 2010 (Callison-Burch et al., 2010) and WMT 2011 (Callison-Burch et al., 2011); the full set is shown in Table 1. For each task we considered four possible methods of ranking the data: sorting by any of Equation 1 through 3, and sorting consistent with reversal of a minimum feedback arc set (MFAS). To weight the edges for the latt"
W12-3127,C69-0101,0,0.542942,"Missing"
W12-3127,W07-0702,0,0.0219272,"roaches including unsupervised clustering (Zollmann and Vogel, 2011), merging (Hanneman et al., 2011), and selection (Mylonakis and Sima’an, 2011) of labels derived from phrasestructure parse trees very much like those used by our baseline systems. What we find particularly attractive about CCG is that it naturally assigns linguistically-motivated labels to most spans of a sentence using a reasonably concise label set, possibility obviating the need for further refinement. Indeed, the analytical flexibility of CCG has motivated its increasing use in MT, from applications in language modeling (Birch et al., 2007; Hassan et al., 2007) to more recent proposals to incorporate it into phrase-based (Mehay, 2010) and hierarchical translation systems (Auli, 2009). Our new model builds on these past efforts, representing a more fully instantiated model of CCGbased translation. We have shown that the label scheme allows us to keep many more translation rules than labels based on phrase structure syntax, extracting almost as many rules as the SAMT model, but keeping the label set an order of magnitude smaller, which leads to more efficient translation. This simply scratches the surface of possible uses of CCG"
W12-3127,J07-2003,0,0.8251,"(Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu–English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast. 1 N → h maison ; house i Introduction The Hiero model of Chiang (2007) popularized the usage of synchronous context-free grammars (SCFGs) for machine translation. SCFGs model translation as a process of isomorphic syntactic derivation in the source and target language. But the Hiero model is formally, not linguistically syntactic. Its derivation trees use only a single non-terminal label X, carrying no linguistic information. Consider Rule 1. X → h maison ; house i (1) We can add syntactic information to the SCFG rules by parsing the parallel training data and projecting parse tree labels onto the spans they yield and But we quickly run into trouble: how should"
W12-3127,J07-4004,0,0.297028,"categorial grammar (CCG) is an extension of CG that includes more combinators (operations that can combine categories). Steedman and Baldridge (2011) give an excellent overview of CCG. As an example, suppose we want to analyze the sentence “They own properties in various cities and villages” using the lexicon shown in Table 1. We assign categories according to the lexicon, then combine the categories using function application and other combinators to get an analysis of S for the complete sentence. Figure 1 shows the derivation. As a practical matter, very efficient CCG parsers are available (Clark and Curran, 2007). As shown by Fowler and Penn (2010), in many cases CCG is context-free, making it an ideal fit for our problem. 2.1 Labels for phrases Consider the German–English phrase pair der große Mann – the tall man. It is easily labeled as an NP and included in the translation table. By contrast, der große– the tall, doesn’t typically correspond to a complete subtree in a phrase structure parse. Yet translating the tall is likely to be more useful than translating the tall man, since it is more general—it can be combined with any other noun translation. T hey own properties in various cities and villag"
W12-3127,D07-1079,0,0.0189088,"bel a rule that translates pour l’´etablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While these heuristics are effect"
W12-3127,N09-1026,0,0.0135355,"ponds to (possibly many) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence. It then produces an augmented set of items hA, i, j, u, vi, in which items of the first type are augmented with left and right language model states u and v. In each pass, the number of items is linear in the number of nonterminal symbols of the grammar. This observation has motivated work in grammar transformations that reduce the size of the nonterminal set, often resulting in substantial gains in parsing or translation speed (Song et al., 2008; DeNero et al., 2009; Xiao et al., 2009). More formally, the upper bound on parsing complexity is always at least linear in the size of the grammar constant G, where G is often loosely defined as a grammar constant; Iglesias et al. (2011) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al. (2010) provide a more fine-grained analysis of G, showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar. Though these are worst-case analyses, it should be clear t"
W12-3127,P10-1035,0,0.0493258,"ion of CG that includes more combinators (operations that can combine categories). Steedman and Baldridge (2011) give an excellent overview of CCG. As an example, suppose we want to analyze the sentence “They own properties in various cities and villages” using the lexicon shown in Table 1. We assign categories according to the lexicon, then combine the categories using function application and other combinators to get an analysis of S for the complete sentence. Figure 1 shows the derivation. As a practical matter, very efficient CCG parsers are available (Clark and Curran, 2007). As shown by Fowler and Penn (2010), in many cases CCG is context-free, making it an ideal fit for our problem. 2.1 Labels for phrases Consider the German–English phrase pair der große Mann – the tall man. It is easily labeled as an NP and included in the translation table. By contrast, der große– the tall, doesn’t typically correspond to a complete subtree in a phrase structure parse. Yet translating the tall is likely to be more useful than translating the tall man, since it is more general—it can be combined with any other noun translation. T hey own properties in various cities and villages NP (SN P )/N P NP (N P N P )/N"
W12-3127,W02-1039,0,0.0248982,"ecting parse tree labels onto the spans they yield and But we quickly run into trouble: how should we label a rule that translates pour l’´etablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT"
W12-3127,N04-1035,0,0.0855087,"des majorité la Pour PP , IN For For NP JJ NN most people most people Then we can assign syntactic labels to Rule 4 to produce PP → h Pour NP ; For NP i (5) , Figure 3: A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example. The focus of this paper is how to assign labels to the left-hand non-terminal X and to the nonterminal gaps on the right-hand side. We discuss five models below, of which two are novel CG-based labeling schemes. 3.2 The rules extracted by this scheme are very similar to those produced by GHKM (Galley et al., 2004), in particular resulting in the “composed rules” of Galley et al. (2006), though we use simpler heuristics for handling of unaligned words and scoring in order to bring the model in line with both Hiero and SAMT baselines. Under this scheme we throw away a lot of useful translation rules that don’t translate exact syntactic constituents. For example, we can’t label Baseline: Hiero X → h Pour la majorit´e des ; For most i Hiero (Chiang, 2007) uses the simplest labeling possible: there is only one non-terminal symbol, X, for all rules. Its advantage over phrase-based translation in its ability"
W12-3127,P06-1121,0,0.066652,"n we can assign syntactic labels to Rule 4 to produce PP → h Pour NP ; For NP i (5) , Figure 3: A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example. The focus of this paper is how to assign labels to the left-hand non-terminal X and to the nonterminal gaps on the right-hand side. We discuss five models below, of which two are novel CG-based labeling schemes. 3.2 The rules extracted by this scheme are very similar to those produced by GHKM (Galley et al., 2004), in particular resulting in the “composed rules” of Galley et al. (2006), though we use simpler heuristics for handling of unaligned words and scoring in order to bring the model in line with both Hiero and SAMT baselines. Under this scheme we throw away a lot of useful translation rules that don’t translate exact syntactic constituents. For example, we can’t label Baseline: Hiero X → h Pour la majorit´e des ; For most i Hiero (Chiang, 2007) uses the simplest labeling possible: there is only one non-terminal symbol, X, for all rules. Its advantage over phrase-based translation in its ability to model phrases with gaps in them, enabling phrases to reorder subphrase"
W12-3127,W11-1015,0,0.0126196,"aller models (Hiero, phrase structure syntax, and CCG 1-best derivations) are significantly faster than the two larger ones. However, even though the CCG parse chart model is almost 34 the size of SAMT in terms of number of rules, it doesn’t take 43 of the 5 LDC2009T13 229 Discussion and Future Work Finding an appropriate mechanism to inform phrasebased translation models and their hierarchical variants with linguistic syntax is a difficult problem that has attracted intense interest, with a variety of promising approaches including unsupervised clustering (Zollmann and Vogel, 2011), merging (Hanneman et al., 2011), and selection (Mylonakis and Sima’an, 2011) of labels derived from phrasestructure parse trees very much like those used by our baseline systems. What we find particularly attractive about CCG is that it naturally assigns linguistically-motivated labels to most spans of a sentence using a reasonably concise label set, possibility obviating the need for further refinement. Indeed, the analytical flexibility of CCG has motivated its increasing use in MT, from applications in language modeling (Birch et al., 2007; Hassan et al., 2007) to more recent proposals to incorporate it into phrase-based"
W12-3127,P07-1037,0,0.0558347,"Missing"
W12-3127,D11-1127,0,0.0254243,"Missing"
W12-3127,N03-1017,0,0.0972877,"le: how should we label a rule that translates pour l’´etablissement de into for the establishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While thes"
W12-3127,P07-2045,1,0.0106061,"n task, in which syntax-based systems have been quite effective (Baker et al., 2009; Zollmann et al., 2008). The training corpus was the National Institute of Standards and Technology Open Machine Translation 2009 Evaluation (NIST Open MT09). According to the MT09 Constrained Training Con228 ditions Resources list2 this data includes NIST Open MT08 Urdu Resources3 and the NIST Open MT08 Current Test Set Urdu–English4 . This gives us 202,019 parallel translations, for approximately 2 million words of training data. 5.2 Experimental design We used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used th"
W12-3127,P11-1065,0,0.112476,"Missing"
W12-3127,P00-1056,0,0.0124437,"lation 2009 Evaluation (NIST Open MT09). According to the MT09 Constrained Training Con228 ditions Resources list2 this data includes NIST Open MT08 Urdu Resources3 and the NIST Open MT08 Current Test Set Urdu–English4 . This gives us 202,019 parallel translations, for approximately 2 million words of training data. 5.2 Experimental design We used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used the same feature set in all the translation grammars. This includes, for each rule C → hf ; ei, relative-frequency estimates of the probabil2 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/MT09_Constrain"
W12-3127,P03-1021,0,0.0350882,"nly half the time of the SAMT model, thanks to the smaller rule label set. 6 Table 3: Results of translation experiments on Urdu–English. Higher BLEU scores are better. BLEU’s brevity penalty is reported in parentheses. ities p(f |A), p(f |e), p(f |e, A), p(e|A), p(e|f ), and p(e|f, A). The feature set also includes lexical weighting for rules as defined by Koehn et al. (2003) and various binary features as well as counters for the number of unaligned words in each rule. To train the feature weights we used the Z-MERT implementation (Zaidan, 2009) of the Minimum Error-Rate Training algorithm (Och, 2003). To decode the test sets, we used the Joshua machine translation decoder (Weese et al., 2011). The language model is a 5-gram LM trained on English GigaWord Fourth Edition.5 5.3 Evaluation criteria We measure machine translation performance using the BLEU metric (Papineni et al., 2002). We also report the translation time for the test set in seconds per sentence. These results are shown in Table 3. All of the syntactic labeling schemes show an improvement over the Hiero model. Indeed, they all fall in the range of approximately 27–28 BLEU. We can see that the 1-best derivation CCG model perfo"
W12-3127,P02-1040,0,0.0852201,"d p(e|f, A). The feature set also includes lexical weighting for rules as defined by Koehn et al. (2003) and various binary features as well as counters for the number of unaligned words in each rule. To train the feature weights we used the Z-MERT implementation (Zaidan, 2009) of the Minimum Error-Rate Training algorithm (Och, 2003). To decode the test sets, we used the Joshua machine translation decoder (Weese et al., 2011). The language model is a 5-gram LM trained on English GigaWord Fourth Edition.5 5.3 Evaluation criteria We measure machine translation performance using the BLEU metric (Papineni et al., 2002). We also report the translation time for the test set in seconds per sentence. These results are shown in Table 3. All of the syntactic labeling schemes show an improvement over the Hiero model. Indeed, they all fall in the range of approximately 27–28 BLEU. We can see that the 1-best derivation CCG model performs slightly better than the phrase structure model, and the CCG parse chart model performs a little better than that. SAMT has the highest BLEU score. The models with a larger number of rules perform better; this supports our assertion that we shouldn’t throw away too many rules. When"
W12-3127,N07-1051,0,0.0195356,"includes NIST Open MT08 Urdu Resources3 and the NIST Open MT08 Current Test Set Urdu–English4 . This gives us 202,019 parallel translations, for approximately 2 million words of training data. 5.2 Experimental design We used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used the same feature set in all the translation grammars. This includes, for each rule C → hf ; ei, relative-frequency estimates of the probabil2 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/MT09_ConstrainedResources.pdf 3 LDC2009E12 4 LDC2009E11 Model Hiero Syntax SAMT CCG derivations CCG parse chart BLEU 25.67 (0.9781) 27.06 (0.970"
W12-3127,D08-1018,0,0.0175781,"re each item corresponds to (possibly many) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence. It then produces an augmented set of items hA, i, j, u, vi, in which items of the first type are augmented with left and right language model states u and v. In each pass, the number of items is linear in the number of nonterminal symbols of the grammar. This observation has motivated work in grammar transformations that reduce the size of the nonterminal set, often resulting in substantial gains in parsing or translation speed (Song et al., 2008; DeNero et al., 2009; Xiao et al., 2009). More formally, the upper bound on parsing complexity is always at least linear in the size of the grammar constant G, where G is often loosely defined as a grammar constant; Iglesias et al. (2011) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al. (2010) provide a more fine-grained analysis of G, showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar. Though these are worst-case analyses,"
W12-3127,W03-1001,0,0.0159877,"rties in various cities and villages” using the lexicon from Table 1. Φ indicates a conjunction operation; > and &lt; are forward and backward function application, respectively. Extraction from parallel text To extract SCFG rules, we start with a heuristic to extract phrases from a word-aligned sentence pair 1 We could assign NP/N to the determiner the and N/N to the adjective tall, then combine those two categories using function composition to get a category NP/N for the two words together. 224 , Figure 2: A word-aligned sentence pair fragment, with a box indicating a consistent phrase pair. (Tillmann, 2003). Figure 2 shows a such a pair, with a consistent phrase pair inside the box. A phrase pair (f, e) is said to be consistent with the alignment if none of the words of f are aligned outside the phrase e, and vice versa – that is, there are no alignment points directly above, below, or to the sides of the box defined by f and e. Given a consistent phrase pair, we can immediately extract the rule X → hf, ei (3) as we would in a phrase-based MT system. However, whenever we find a consistent phrase pair that is a sub-phrase of another, we may extract a hierarchical rule by treating the inner phrase"
W12-3127,W11-2160,1,0.869162,"e used the scripts included with the Moses MT toolkit (Koehn et al., 2007) to tokenize and normalize the English data. We used a tokenizer and normalizer developed at the SCALE 2009 workshop (Baker et al., 2009) to preprocess the Urdu data. We used GIZA++ (Och and Ney, 2000) to perform word alignments. For phrase structure parses of the English data, we used the Berkeley parser (Petrov and Klein, 2007). For CCG parses, and for reading labels out of a parse chart, we used the C&C parser (Clark and Curran, 2007). After aligning and parsing the training data, we used the Thrax grammar extractor (Weese et al., 2011) to extract all of the translation grammars. We used the same feature set in all the translation grammars. This includes, for each rule C → hf ; ei, relative-frequency estimates of the probabil2 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/MT09_ConstrainedResources.pdf 3 LDC2009E12 4 LDC2009E11 Model Hiero Syntax SAMT CCG derivations CCG parse chart BLEU 25.67 (0.9781) 27.06 (0.9703) 28.06 (0.9714) 27.3 (0.9770) 27.64 (0.9673) sec./sent. 0.05 3.04 63.48 5.24 33.6 time. In fact, it takes only half the time of the SAMT model, thanks to the smaller rule label set. 6 Table 3: Results of translat"
W12-3127,D09-1038,0,0.0154381,"ny) partial analyses by which nonterminal A generates the sequence of words from positions i through j of the source sentence. It then produces an augmented set of items hA, i, j, u, vi, in which items of the first type are augmented with left and right language model states u and v. In each pass, the number of items is linear in the number of nonterminal symbols of the grammar. This observation has motivated work in grammar transformations that reduce the size of the nonterminal set, often resulting in substantial gains in parsing or translation speed (Song et al., 2008; DeNero et al., 2009; Xiao et al., 2009). More formally, the upper bound on parsing complexity is always at least linear in the size of the grammar constant G, where G is often loosely defined as a grammar constant; Iglesias et al. (2011) give a nice analysis of the most common translation algorithms and their dependence on G. Dunlop et al. (2010) provide a more fine-grained analysis of G, showing that for a variety of implementation choices that it depends on either or both the number of rules in the grammar and the number of nonterminals in the grammar. Though these are worst-case analyses, it should be clear that grammars with fe"
W12-3127,W06-3119,0,0.509168,"tablishment of? There is no phrase structure constituent that corresponds to this English fragment. This raises a model design question: what label do we assign to spans that are natural translations of each other, but have no natural labeling under a syntactic parse? One possibility would be to discard such translations from our model as implausible. However, such non-compositional translations are important in translation (Fox, 2002), and they have been repeatedly shown to improve translation performance (Koehn et al., 2003; DeNeefe et al., 2007). Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While these heuristics are effective (Zollmann et al., 2008), they inflate grammar size, hamper effective pa"
W12-3127,P11-1001,0,0.061644,"es to translation time, the three smaller models (Hiero, phrase structure syntax, and CCG 1-best derivations) are significantly faster than the two larger ones. However, even though the CCG parse chart model is almost 34 the size of SAMT in terms of number of rules, it doesn’t take 43 of the 5 LDC2009T13 229 Discussion and Future Work Finding an appropriate mechanism to inform phrasebased translation models and their hierarchical variants with linguistic syntax is a difficult problem that has attracted intense interest, with a variety of promising approaches including unsupervised clustering (Zollmann and Vogel, 2011), merging (Hanneman et al., 2011), and selection (Mylonakis and Sima’an, 2011) of labels derived from phrasestructure parse trees very much like those used by our baseline systems. What we find particularly attractive about CCG is that it naturally assigns linguistically-motivated labels to most spans of a sentence using a reasonably concise label set, possibility obviating the need for further refinement. Indeed, the analytical flexibility of CCG has motivated its increasing use in MT, from applications in language modeling (Birch et al., 2007; Hassan et al., 2007) to more recent proposals to"
W12-3127,C08-1144,0,0.0991498,"tax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the concatenation of a noun phrase with a preposition on either side. While descriptive, this label is unsatisfying as a concise description of linguistic function, fitting uneasily alongside more natural labels in the phrase structure formalism. SAMT introduces many thousands of such labels, most of which are seen very few times. While these heuristics are effective (Zollmann et al., 2008), they inflate grammar size, hamper effective parameter estimation due to feature sparsity, and slow translation speed. Our objective is to find a syntactic formalism that 222 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 222–231, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics enables us to label most translation rules without relying on heuristics. Ideally, the label should be small in order to improve feature estimation and reduce translation time. Furthering an insight that informs SAMT, we show that combinatory categorial gra"
W12-3127,2010.iwslt-papers.1,0,\N,Missing
W15-2516,guillou-etal-2014-parcor,1,0.850856,"o the fact that pronouns often refer to entities mentioned in a nonlocal context such as previous clauses or sentences. Furthermore, languages differ with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Port"
W15-2516,E12-3001,0,0.0498972,"with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al., 2008).1 antecedent . . . une symphonie et"
W15-2516,petrov-etal-2012-universal,0,0.0272995,"on as bag-of-words, but separate the feature space by source and target side vocabulary and whether the word occurs before or after the pronoun. Special BOS and EOS markers are included for contexts at the beginning or end of sentence, respectively. We neither remove stopwords nor normalize the tokens. We also include as features, the Part-of-Speech (POS) tags in a 3-word window to each side of source and target pronouns. This gives some abstraction from the lexical surface form. For the source side we use the POS tags from Stanford CoreNLP (Manning et al., 2014) mapped to universal POS tags (Petrov et al., 2012). For the target side we use coarse-grained tags provided by 1 116 https://github.com/gchrupala/morfette antecedentaligned si des while institutions institutions comme . . . such as les ONG peuvent travailler . . . NGOs may work on au social d´eveloppement social development , , elles sont they are under-funded sous-financ´ees antecedentco−ref Figure 2: The antecedentco−ref of they on the English sentence (source language) is determined with a co-reference resolution system. The target-side antecedentaligned is obtained by following the word alignment links. In the shared task, the target pron"
W15-2516,2010.iwslt-papers.10,0,0.0985871,"ious clauses or sentences. Furthermore, languages differ with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al.,"
W15-2516,D13-1037,0,0.211553,"Missing"
W15-2516,W14-3312,0,0.112768,"usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al., 2008).1 antecedent . . . une symphonie et . . . the symphony and La"
W15-2516,W15-2501,0,0.0584592,"e and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al., 2008).1 antecedent . . . une symphonie et . . . the symphony and Language Model Prediction We include a target-side Language Model (LM) predictio"
W15-2516,W11-2123,0,0.0173571,"formance on the test data. Ranks according to each metric are given in parenthesis out of 14 submitted systems (including multiple submissions per submitter and the baseline). 2093 sentences in twelve TED talk documents. 3.2 Classifier Accuracy We extract features from the training and test set and use Mallet (McCallum, 2002) to train the MaxEnt classifier.4 The variance for regularizing the weights is set to 1 (default setting). For the LM component of our system we use the baseline model provided for the pronoun translation subtask. This is a 5-gram modified KneserNey LM trained with KenLM (Heafield, 2011).5 3.3 Table 2: Performance of ALL I N O NE classifier on the test set. Evaluation Metrics 5 Recall F1 78.05 86.96 82.26 cela 9.52 7.41 8.33 elle 49.06 31.33 38.24 elles 80.00 31.37 45.07 il 51.54 64.42 57.26 ils 75.79 90.00 82.29 on 61.90 35.14 44.83 c¸a 64.29 44.12 52.33 OTHER 80.00 88.52 84.04 Macro-averaged 61.13 53.25 54.96 Accuracy 71.40 Table 3: Performance of POST C OMBINED classifier on the test set. 4 Discussion Confusion Matrices Table 5 and Table 6 present confusion matrices on the test set. Divergences from strong diagonal values in both tables derive in part from gender-choice er"
W15-2516,W10-1737,0,0.254363,"Missing"
W15-2516,J13-4004,0,0.377687,"hat maps the i-th observation x and associated label y to a real valued vector. It also consists of a weight vector θ~ of corresponding size, which contains the model parameters that are learned from the training data. The model is of the form p(y|x) = exp θ~ · f (x, y) Z(x) where Z(x) is a normalizing factor ensuring valid probabilities. 2.2 Features Target-side Antecedent The target-side noun antecedent of the pronoun determines the morphological features the pronoun has to agree with, i.e. number and gender. We use the source-side coreference resolution system provided by Stanford CoreNLP (Lee et al., 2013) to determine the coreference chains in each document of the training data. We then project these chains to the target side via word-alignments (cf. Figure 2). The motivation to obtain target-side co-reference chains in that way is three-fold. First, the target side of the training data is missing most of the targetside pronouns since it is the task to predict them. Therefore, relevant parts of co-reference chains are missing and the place-holders for these pronouns will introduce noise to the resolution system. Secondly, we have a statistical machine translation (SMT) scenario in mind as an a"
W15-2516,P14-5010,0,0.00415805,"ze 3 around the pronoun. We integrate this information as bag-of-words, but separate the feature space by source and target side vocabulary and whether the word occurs before or after the pronoun. Special BOS and EOS markers are included for contexts at the beginning or end of sentence, respectively. We neither remove stopwords nor normalize the tokens. We also include as features, the Part-of-Speech (POS) tags in a 3-word window to each side of source and target pronouns. This gives some abstraction from the lexical surface form. For the source side we use the POS tags from Stanford CoreNLP (Manning et al., 2014) mapped to universal POS tags (Petrov et al., 2012). For the target side we use coarse-grained tags provided by 1 116 https://github.com/gchrupala/morfette antecedentaligned si des while institutions institutions comme . . . such as les ONG peuvent travailler . . . NGOs may work on au social d´eveloppement social development , , elles sont they are under-funded sous-financ´ees antecedentco−ref Figure 2: The antecedentco−ref of they on the English sentence (source language) is determined with a co-reference resolution system. The target-side antecedentaligned is obtained by following the word a"
W15-2516,chrupala-etal-2008-learning,0,\N,Missing
W16-2517,P16-2003,0,0.0226661,"oses much of the nuance in a persona deliberately adopted by the writer. Twitter users often express their spoken dialect through spelling, so regional and demographic information may also be lost in the process of text normalization (Eisenstein, 2013a). Distributional word representations hold promise to replace this flawed preprocessing step. By making the shared semantic content of spelling variants implicit in the representation of words, text processing models can be more flexible. They can extract persona or dialect information while handling the semantic or syntactic features of words (Benton et al., 2016). In this proposal, we will present a method of evaluating whether a particular set of word representations can make text normalization unnecessary. Because the intrinsic evaluation we present is inexpensive and simple, it can be easily used to validate representations during training. An evaluation dataset can be collected easily from UrbanDictionary by methods we will outline. Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulfills the promise of el"
W16-2517,W16-2506,0,0.0582283,"Missing"
W16-2517,W16-2507,0,0.0242366,"ian”. 3. moran: The correct spelling of moran when posting to [fark] 4. mosha: . . . However, the younger generation (that were born after 1983) think it is a great word for someone who likes “Nu Metal” And go around calling people fake moshas (or as the spelling was originally “Moshers”. 96 word frequency in cosine similarity; and we cannot handle polysemy. Novel issues of polysemy also emerge in cases such as “tarp”; “wit”, which represents either cleverness or a spelling variant of “with”; and “ur”, which maps to both “your” and “you are”. However, compared to similarity scores in general (Gladkova and Drozd, 2016), spelling variant pairs are less subjective. The word pairs with representations that appeared far apart often featured an informal word that appeared closer to words that were related by topic, but not similar in meaning. The representation of “orgasim” was closer to a number of medical terms, including “abscess”, “hysterectomy”, “hematoma”, and “cochlear”, than it was to “orgasm”. Other word pairs were penalized when the “formal” vocabulary list failed to filter out informal words that appeared in the same online dialect. The five closest “formal” words to “qurl” (“girl”), which were “coot”"
W16-2517,W11-2210,0,0.0313884,"ns, we search for a number of common strings that signal spelling variants. To cast a very wide net, we could search for all instances of “spelling” and then validate a large number of results by hand. More reliably, we can search for strings like: • misspelling of [your]1 • misspelling of “your” • way of spelling [your] Gathering Spelling Variants • spelling for [your] If we have an informal text corpus, we can use it to generate a set of likely spelling variants to validate by hand. An existing unsupervised method to do so is outlined as part of the text normalization pipeline described by (Gouws et al., 2011). This technique requires a formal vocabulary corpus such as Wikipedia as well as a social media corpus such as Twitter. They start by exhaustively ranking all word pairs by their distributional similarity in both Wikipedia and Twitter. The word pairs that are distributionally similar in Twitter but not in Wikipedia are considered to be candidate spelling variants. These candidates are then reranked by lexical similarity, providing a list of likely spelling variants. This method is inappropriate when collecting datasets for the purpose of evaluation. When we rely on co-occurrence information i"
W16-2517,D14-1162,0,0.0876753,"Missing"
W16-2517,P14-1146,0,0.0327064,"ether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary. 1 Introduction Recent years have seen a surge of interest in training effective models for informal domains such as Twitter or discussion forums. Several new works have thus targeted social media platforms by learning word representations specific to such domains (Tang et al., 2014); (Benton et al., 2016). Traditional NLP techniques have often relied on text normalization methods when applied to informal domains. For example, “u want 2 chill wit us 2nite” may be transcribed as “you want to chill with us tonight”, and the normalized transcription would be used as input for a text processing system. This method makes it easier to apply models that are successful on formal language to more informal language. However, there are several drawbacks to this method. Building an accurate text normalization component for a text processing pipeline can require substantial engineerin"
W16-2517,N13-1037,0,\N,Missing
W16-2517,W13-1102,0,\N,Missing
W17-1804,basile-etal-2012-developing,0,0.423653,"Malta borders no country’ has the UD graph shown in Figure 1(a). When compared to the correct representation given in Figure 1(c), the UDepLambda output shown in Figure 1(b) shows the absence of universal quantification, which in turn leads negation scope to be misrepresented. For this reason, we set the foundation of UDepLambda¬ (UDepLambda-not), an enhanced version of the original framework, whose type theory allows us to jointly handle negation and universal quantification. Moreover, unlike its predecessor, the logical forms are based on the one used in the ‘Groeningen Meaning Bank’ (GMB; (Basile et al., 2012a)), so to allow future comparison to a manually annotated semantic bank. Although the present work shows the conversion process for English, given that the edge labels are universal, our framework could be used to explore the problem of representing the scope of negation in the other 40+ languages universal dependencies are available in. This could also address the problem that all existing resources to represent negation scope as a logical form are limited to English (e.g. GMB and ‘DeepBank’ (Flickinger et al., 2012)) or only to a few other languages (e.g. ‘The Spanish Resource Grammar’ (Mar"
W17-1804,S12-1040,0,0.0947935,"Malta borders no country’ has the UD graph shown in Figure 1(a). When compared to the correct representation given in Figure 1(c), the UDepLambda output shown in Figure 1(b) shows the absence of universal quantification, which in turn leads negation scope to be misrepresented. For this reason, we set the foundation of UDepLambda¬ (UDepLambda-not), an enhanced version of the original framework, whose type theory allows us to jointly handle negation and universal quantification. Moreover, unlike its predecessor, the logical forms are based on the one used in the ‘Groeningen Meaning Bank’ (GMB; (Basile et al., 2012a)), so to allow future comparison to a manually annotated semantic bank. Although the present work shows the conversion process for English, given that the edge labels are universal, our framework could be used to explore the problem of representing the scope of negation in the other 40+ languages universal dependencies are available in. This could also address the problem that all existing resources to represent negation scope as a logical form are limited to English (e.g. GMB and ‘DeepBank’ (Flickinger et al., 2012)) or only to a few other languages (e.g. ‘The Spanish Resource Grammar’ (Mar"
W17-1804,P13-2017,0,0.0583409,"Missing"
W17-1804,morante-daelemans-2012-conandoyle,0,0.230024,"phenomena and be learned automatically, given the link to a manually annotated semantic bank. Related work Available resources that contain a representation of negation scope can be divided in two types: 1) those that represent negation as a FOL (or FOL-translatable) representation (e.g. GMB, ‘DeepBank’), where systems built using these resources are concerned with correctly representing FOL variables and predicates in the scope of negation; and 2) those that try to ground negation at a string-level, where both the negation operator and scope are defined as spans of text (Vincze et al., 2008; Morante and Daelemans, 2012). Systems trained on these resources are then concerned with detecting these spans of text. Resources in 1) are limited in that they are only available in English or for a small number of languages. Moreover no attempt has been made to connect them to more widely-used, cross-linguistic frameworks. On the other hand, grounding a semantic phenomenon to a string-level leads to inevitable simplification. For instance, the interaction between the negation operator and the universal quantifier (e.g. ‘Not every staff member is British’ vs. ‘None of the staff members are British’), along a formal repr"
W17-1804,P14-1007,0,0.0154036,"ator and the universal quantifier (e.g. ‘Not every staff member is British’ vs. ‘None of the staff members are British’), along a formal representation that would allow for inference operations is lost. Furthermore, each corpus is tailored to different applications, making annotation styles across corpora incompatible. Nonetheless these resources have been widely used in the field of Information Extraction and in particular in the Bio-Medical domain. Finally, it is also worth mentioning that there has been some attempts to use formal semantic representations to detect scope at a string level. Packard et al. (2014) used hand-crafted heuristics to traverse the MRS (Minimal Recursion Semantics) structures of negative sentences to then detect which words were in the scope of negation and which were not. Basile et al. (2012b) tried instead to first transform a DRS (Discourse Representation Structure) into graph form and then align this to strings. Whilst the MRS-based system outperformed previous work, mainly due to the fact that MRS structures are closely related to the surface realization, the DRT-based approach performed worse than most systems, mostly given to the fact that the formalism is not easily t"
W17-1804,Q16-1010,1,0.837752,"Missing"
W17-1804,D17-1009,1,0.884082,"Missing"
W17-1804,L16-1376,0,0.0174284,"ues. A FOL representation of the entire input graph can be then obtained by traversing the edges in a given order and combining their semantics. However, in its original formulation, UDeIntroduction Amongst the different challenges around the topic of negation, detecting and representing its scope is one that has been extensively researched in different sub-fields of NLP (e.g. Information Extraction (Velldal et al., 2012; Fancellu et al., 2016)). In particular, recent work have acknowledged the value of representing the scope of negation on top of existing linguistic resources (e.g. AMR – Bos (2016)). Manually annotating the scope of negation is however a time-consuming process, requiring annotators to have some expertise of formal semantics. Our solution to this problem is to automatically convert an available representation that captures negation into a framework that allows a rich variety of semantic phenomena to be represented, in22 Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR), pages 22–32, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 2 pLambda does not handle either universal quantifiers or other scope phen"
W17-1804,J12-2005,0,0.122327,"edited minimally so to yield a more fine-grained description on the phenomena they describe, while lexical information is used only for a very restricted class of lexical items, such as negation cues. A FOL representation of the entire input graph can be then obtained by traversing the edges in a given order and combining their semantics. However, in its original formulation, UDeIntroduction Amongst the different challenges around the topic of negation, detecting and representing its scope is one that has been extensively researched in different sub-fields of NLP (e.g. Information Extraction (Velldal et al., 2012; Fancellu et al., 2016)). In particular, recent work have acknowledged the value of representing the scope of negation on top of existing linguistic resources (e.g. AMR – Bos (2016)). Manually annotating the scope of negation is however a time-consuming process, requiring annotators to have some expertise of formal semantics. Our solution to this problem is to automatically convert an available representation that captures negation into a framework that allows a rich variety of semantic phenomena to be represented, in22 Proceedings of the Workshop Computational Semantics Beyond Events and Rol"
W17-1804,W08-0606,0,0.229036,"Missing"
W17-1804,marimon-2010-spanish,0,\N,Missing
W17-3410,P13-1023,0,0.0599369,"Missing"
W17-3410,J14-3008,0,0.0294655,"Missing"
W17-3410,W13-2322,0,0.0460347,"; Kamimura and Slutzki 1981), studied by (Quernheim and Knight, Introduction NLP systems for machine translation, summarisation, paraphrasing, and other problems often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words, or at best syntactic trees. To preserve semantics, they must model semantics. In pursuit of this goal, several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs (DAGs), including the Abstract Meaning Represenation Bank (AMR; Banarescu et al. 2013), the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012), Deepbank (Flickinger et al., 2012), and the Universal Conceptual Cognitive Annotation (Abend and 100 Proceedings of the 15th Meeting on the Mathematics of Language, pages 100–113, c London, UK, July 13–14, 2017. 2017 Association for Computational Linguistics 2012). (Thomas, 1991) showed that the latter are a subfamily of the monadic second order languages (MSOL), which are of special interest to us, since, when restricted to strings or trees, they exactly characterise the recognisable—or regular—languages of each (B¨uchi, 19"
W17-3410,W16-3311,0,0.0182384,"he form of strings and trees, such distributions can be represented by finite automata (Mohri et al., 2008; Allauzen et al., 2014), which are closed under intersection and can be made probabilistic. It is therefore natural to ask whether there is a family of graph languages with similar properties to finite automata. Recent work in NLP has focused primarily on two families of graph languages: hyperedge replacement languages (HRL; Drewes et al. 1997), a context-free graph rewriting formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016); and DAG automata languages, (DAGAL; Kamimura and Slutzki 1981), studied by (Quernheim and Knight, Introduction NLP systems for machine translation, summarisation, paraphrasing, and other problems often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words, or at best syntactic trees. To preserve semantics, they must model semantics. In pursuit of this goal, several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs (DAGs), including the Abstra"
W17-3410,K15-1004,0,0.0195717,"which data is in the form of strings and trees, such distributions can be represented by finite automata (Mohri et al., 2008; Allauzen et al., 2014), which are closed under intersection and can be made probabilistic. It is therefore natural to ask whether there is a family of graph languages with similar properties to finite automata. Recent work in NLP has focused primarily on two families of graph languages: hyperedge replacement languages (HRL; Drewes et al. 1997), a context-free graph rewriting formalism that has been studied in an NLP context by several researchers (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016); and DAG automata languages, (DAGAL; Kamimura and Slutzki 1981), studied by (Quernheim and Knight, Introduction NLP systems for machine translation, summarisation, paraphrasing, and other problems often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words, or at best syntactic trees. To preserve semantics, they must model semantics. In pursuit of this goal, several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs (DA"
W17-3410,W12-4209,0,0.315456,"the recognisable—or regular—languages of each (B¨uchi, 1960; B¨uchi and Elgot, 1958; Trakhtenbrot, 1961). The HRL and MSOL families are incomparable: that is, the context-free graph languages do not contain the recognisable graph languages, as is the case in languages of strings and trees (Courcelle, 1990). So, while each formalism has appealing characteristics, neither appear adequate for the problem outlined above: HRLs can be made probabilistic, but they are not closed under intersection; and while DAGAL and MSOL are closed under intersection, it is unclear how to make them probabilistic (Quernheim and Knight, 2012).1 This situation suggests that we might want a family of languages that is a subfamily of both HRL and MSOL. Courcelle (1991) defines all such languages as the family of strongly contextfree languages (SCFL).2,3 Unfortunately, SCFLs are defined non-constructively, but Courcelle (1991) exhibits a constructively-defined subfamily: Regular Graph Languages (RGL), defined as a restricted form of HRL, which Courcelle demonstrates is also in MSOL. Recently, two new graph grammar formalisms have been defined which are also restricted forms of HRL: Tree-like Grammars (TLG; Matheja et al. 2015) and Res"
W17-3410,C12-1083,0,\N,Missing
W17-4607,D16-1263,0,0.0546957,"Missing"
W17-4607,D16-1133,1,0.879375,"Lopez♦ David Chiang♠ ♠ Department of Computer Science and Engineering, University of Notre Dame ♦ School of Informatics, University of Edinburgh Abstract native speakers themselves (Bird et al., 2014; Blachon et al., 2016; Adda et al., 2016). Nevertheless, even translation takes time and language knowledge, so there may still be little translated data relative to the amount of recorded audio. An important goal, then, is to bootstrap language technology from this small parallel corpus in order to provide tools to annotate more data or make the data more searchable. We build on the approach of Anastasopoulos et al. (2016), who developed a system that performs joint inference to identify recurring segments of audio and cluster them while aligning them to words in a text translation. Here, we extend the method to be able to search for new instances of the latent clusters within the unlabeled audio, effectively providing keyword translations for some of the unlabeled speech. We evaluate our method on a Spanish-English corpus used in previous work, and on two datasets from endangered languages (narratives in Arapaho and Ainu). No previous computational methods have been tested on the latter data, to our knowledge."
W18-5005,W02-0109,0,0.0290983,"s is the We also distinguish between the speakers when looking at between-speaker, or comprehensionproduction (CP) priming where the speaker first comprehends the prime (uttered by their interlocutor) and then produces the target, and withinspeaker or production-production (PP) priming, where both the prime and the target are produced by the same speaker. Since we are also interested in tutor T behaviour vs. student S in these interactions we map PP priming to TT and SS respectively and CP to TS and ST. 5 This also has the effect of removing function words from consideration. 6 43 Using NLTK (Loper and Bird, 2002) Lexical Repetition bility depends on the following explanatory variables: Categorical: corpus choice, priming type from speaker role, ability level; and Ordinal: word frequency, as explained in Section 3.2. The model will produce coefficients β i , one for each explanatory variable i. β i expresses the contribution of i to the probability of the outcome event, in our case, successful priming, referred to as priming effect size in the following sections. For example, the β i estimates allow us to predict the decline of repetition probability with increasing distance between prime and target, a"
W18-5005,W09-0612,0,0.00906006,"ferent ability levels in L2 dialogue. This adds the dimension of lexical priming and individual speaker interactions to the work of Reitter and Moore (2006) and the inspection of student to tutor, and within-speaker priming to that of Ward and Litman (2007b). By also making comparisons across L2 ability levels, we can now analyse priming effects in terms of L2 acquisition. Similar work in this area outside the scope of this paper includes work analysing alignment of expressions in a task-based dialogue setting (Duplessis et al., 2017) and the analysis of alignment-capable dialogue generation (Buschmeier et al., 2009). In addition to informing dialogue tutoring agent design, this work has potential to augment existing measures of linguistic sophistication predic1.2 Research Questions We present evidence which strengthens our hypothesis that tutors take advantage of the natural alignment found in language, in order to better introduce, or ground3 vocabulary to the student; in other words, scaffolding4 vocabulary from receptive to productive practice in these dialogues. Our work investigates the following research questions: RQ1 How does L2 dialogue differ from taskbased and conversational in terms of alignm"
W18-5005,W17-5510,0,0.142771,"ects for each interlocutor in dialogue both within- and between-speaker, and at different ability levels in L2 dialogue. This adds the dimension of lexical priming and individual speaker interactions to the work of Reitter and Moore (2006) and the inspection of student to tutor, and within-speaker priming to that of Ward and Litman (2007b). By also making comparisons across L2 ability levels, we can now analyse priming effects in terms of L2 acquisition. Similar work in this area outside the scope of this paper includes work analysing alignment of expressions in a task-based dialogue setting (Duplessis et al., 2017) and the analysis of alignment-capable dialogue generation (Buschmeier et al., 2009). In addition to informing dialogue tutoring agent design, this work has potential to augment existing measures of linguistic sophistication predic1.2 Research Questions We present evidence which strengthens our hypothesis that tutors take advantage of the natural alignment found in language, in order to better introduce, or ground3 vocabulary to the student; in other words, scaffolding4 vocabulary from receptive to productive practice in these dialogues. Our work investigates the following research questions:"
W18-5005,W14-1203,0,0.0281899,"tes easier words. F ormula : lemma occ∼window + log(W OF ) ∗ role 6 Conclusions and Future Work baselines of randomly ordered utterances (Duplessis et al., 2017), which will account for vocabulary effects and corpus-specific factors. To explore more measures of word complexity in addition to simple WOF, we will further investigate measures specific to L2 dialogue, such as the English Vocabulary Profile (EVP) (Capel, 2012), with word lists per CEFR10 level, or measures such as counts of word sense per word, or whether a word is concrete or abstract11 , exploiting existing readability features (Vajjala and Meurers, 2014). We see these results as an indication that measuring lexical alignment combined with lexical sophistication of vocabulary has potential as a predictor of student competency. We also hypothesise that measurements of ‘good tutoring’ actions could consist of how and to what extent tutors adapt interactively to individual students’ needs in terms of their conversational ability. Tutor selfpriming seems to be an interesting possible feature for measuring this adaption. We want to further investigate different measures of alignment and both lexical and syntactic complexity to inform systems that a"
W18-5005,W11-1417,0,0.0352836,"ive tutor alignment strategy to contribute to improved automated L2 tutoring. The potential benefits of automated tutoring for L2 dialogue1 have already been seen through the success of apps such as Duolingo2 bots which allow the user to engage in instant-messaging style chats with an agent to learn another language. Adaption of agent to learner however is an ongoing research task, although outside L2 tutoring, is a well-explored area (Graesser et al., 2005). Alignment, or “more lexical similarity between student and tutor” has been shown to be more predictive of increased student motivation (Ward et al., 2011), and agent alignment to students’ goals can improve student learning (Ai et al., 2010). We build on previous research by investigating lexical priming effects for each interlocutor in dialogue both within- and between-speaker, and at different ability levels in L2 dialogue. This adds the dimension of lexical priming and individual speaker interactions to the work of Reitter and Moore (2006) and the inspection of student to tutor, and within-speaker priming to that of Ward and Litman (2007b). By also making comparisons across L2 ability levels, we can now analyse priming effects in terms of L2"
W18-5417,P16-1160,0,0.0272344,"ectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness. Thus we conclude that, when morphemes overlap extensively with the words of a language, a character language model can perform morphological abstraction. 1 Introduction Character-level language models (Sutskever et al., 2011) are appealing because they enable openvocabulary generation of language, and conditional character language models have now been convincingly used in speech recognition (Chan et al., 2016) and machine translation (Weiss et al., 2017; Lee et al., 2016; Chung et al., 2016). They succeed due to parameter-sharing between frequent, rare, and even unobserved training words, prompting claims that they learn morphosyntactic properties of words. For example, Chung et al. (2016) claim that character language models yield “better modelling [of] rare morphological variants” while Kim et al. (2016) claim that “Character-level models obviate the need for morphological tagging or manual feature engineering.” But these claims of morphological awareness are backed more by intuition than direct empirical evidence. What do these models really learn about morphology? And, to the"
W18-5417,D16-1079,0,0.0574066,"Missing"
W18-5417,W13-3504,0,0.0174343,"in the training data, but its status as a word is encoded by the LM. Actual word endings An interesting result emerges when C2M’s performance is tested on word-final characters, which by default should all be labeled as a morpheme boundary (C2M EOW in Table 4). Recall that the rest of the results exclude these predictions, since morphological segmentation concerns word-internal morpheme boundaries. C2M performs extremely well at identifying actual word endings. The margin between C2M - WE results and C2M - EOW results is Performance The system achieved a rather low F1 score: 53.3. Compared to Ruokolainen et al. (2013), who obtain F1 score 86.5 with a bidirectional CRF model, C2M is clearly inferior. This is not particularly 148 A. B. C. D. E. F. G. Input actions acquisition antenna included intensely misunderstanding woodwork True Segmentation act+ion+s acquisit+ion antenna in+clud+ed in+tense+ly mis+under+stand+ing wood+work Predicted Segmentation act+ion+s acquisit+ion ant+enna in+clude+d intense+ly misunder+stand+ing wood+work Correct X X X X Table 5: Sample predictions of morphological segmentations. 6.1 substantial, even though both look at units of the same type, namely words. The higher accuracy in"
W18-5438,P17-1080,0,0.0482275,"Analyzing and Interpreting Neural Networks for NLP, pages 328–330 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Figure 3: SVCCA correlation scores between LM and taggers. Values are rescaled so maximum score is 1. 3.2 Figure 2: Correlation between mean concentration of a word gradient and word frequency. Structure Encoding Over Time Concentration experiments imply that a network first learns syntax, but topic significance continues to rise later. We test this claim directly. As a proxy for syntactic representation, we use the task of POS tagging, as in (Belinkov et al., 2017). For document-global topic information, we classify the sequence by which Wikipedia article it came from. Both taggers are single layer LSTMs. We applied SVCCA to the RNN layers of our language model and each tagger in order to find the correlation between the language model representation and the tagger representation. Indeed, Figure 3 illustrates that the POS structure is effectively represented immediately, and continues to be learned in the early stages of training before the first optimizer step size rescale. After that point, POS structure actually slightly declines and stabilizes below"
W18-5438,I17-1001,0,0.029437,"Missing"
W18-5438,Q16-1037,0,0.091382,"Missing"
W18-5438,W18-3024,0,0.0198596,"Missing"
W18-5447,P16-2038,0,0.0211041,"ising since it suggests that the MTL model must learn to effectively encode case in the model’s representation, but must not effectively use it for parsing. sian; and we consider four forms of input (e(wi ), §2): word (embedding), characters, characters with gold case, and characters with predicted case. For the latter two, we append the case label to the character sequence, e.g. hb, a, t, Acci represents bat with accusative case. Using the same method, we also supply the gold full analysis, to tease out the importance of case specifically. Finally, we experiment with multitask learning (MTL; Søgaard and Goldberg, 2016; Coavoux and Crabb´e, 2017), using the bi-LSTM states of the lower layer of the bi-LSTM encoder to predict case feature. Table 1 summarizes the results. 4 Conclusion Vintage dependency parsers rely on hand-crafted feature engineering to encode morphology. The recent success of character-level models for many NLP tasks motivates us to ask whether their learned representations are powerful enough to completely replace this feature engineering. By empirically testing this using a single feature known to be important—morphological case—we have shown that they are not. Experiments with multi-task"
W18-5447,D15-1041,0,0.0286638,"he learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling. 1 Introduction 2 Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology. But the success of neural networks offer an appealing solution to this problem by computing word representation from characters. Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017). Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological c"
W18-5447,W10-1401,0,0.0378959,"Missing"
W18-5447,K17-3004,0,0.054387,"Missing"
W18-5447,J13-1003,0,0.0252928,"Missing"
W18-5447,E17-2053,0,0.0326455,"Missing"
W18-5447,E17-1063,0,0.0193289,"ology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological case is Dependency Parsing Model We use a neural graph-based dependency parser similar to that of Kiperwasser and Goldberg (2016) and Zhang et al. (2017) for all our experiments. We treat our parser as a black box and experiment only with the input representations of the parser. Let w = w1 , . . . , w|w |be an input sentence of length |w |and let w0 denote an artificial ROOT token. For each input token wi , we compute the context-independent representation, e(wi ) with a bidirectional LSTM (bi-LSTM) over characters. We concatenate the result with its part-of-speech (POS) representation, ti : xi = [e(wi ); ti ]. We then feed xi to a word-level bi-LSTM encoder to learn a contextual word representation wi . The model uses these representations to"
W18-5447,K17-3002,0,0.0164404,"in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling. 1 Introduction 2 Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology. But the success of neural networks offer an appealing solution to this problem by computing word representation from characters. Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017). Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological case is Dependency Pa"
W18-5447,Q16-1023,0,0.0202719,"away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological case is Dependency Parsing Model We use a neural graph-based dependency parser similar to that of Kiperwasser and Goldberg (2016) and Zhang et al. (2017) for all our experiments. We treat our parser as a black box and experiment only with the input representations of the parser. Let w = w1 , . . . , w|w |be an input sentence of length |w |and let w0 denote an artificial ROOT token. For each input token wi , we compute the context-independent representation, e(wi ) with a bidirectional LSTM (bi-LSTM) over characters. We concatenate the result with its part-of-speech (POS) representation, ti : xi = [e(wi ); ti ]. We then feed xi to a word-level bi-LSTM encoder to learn a contextual word representation wi . The model uses"
W18-5447,D15-1176,0,0.0205332,"ian suggest that adding explicit morphological case—either oracle or predicted—improves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling. 1 Introduction 2 Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology. But the success of neural networks offer an appealing solution to this problem by computing word representation from characters. Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017). Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level"
W18-5447,J13-1004,0,0.0261747,"effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017). Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological case is Dependency Parsing Model We use a neural graph-based dependency parser similar to that of Kiperwasser and Goldberg (2016) and Zhang et al. (2017) for all our experiments. We treat our parser as a black box and experiment only with the input representations of the parser. Let w = w1 , . . . , w|w |be an input sentence of length |w |and let w0 denote an artificial ROOT token. For each input token wi , we compute the context-independent representation, e(wi ) with a bidirectional LSTM (bi-LSTM) over characters. We concatenate the result with its part-of-speech"
W18-5447,K17-3003,0,0.0173541,"not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling. 1 Introduction 2 Parsing morphologically rich languages (MRLs) is difficult due to the complex relationship of syntax to morphology. But the success of neural networks offer an appealing solution to this problem by computing word representation from characters. Character-level models (Ling et al., 2015; Kim et al., 2016) learn relationship between similar word forms and have shown to be effective for parsing MRLs (Ballesteros et al., 2015; Dozat et al., 2017; Shi et al., 2017; Bj¨orkelund et al., 2017). Does that mean that we can do away with explicit modeling of morphology altogether? Consider two challenges in parsing MRLs raised by Tsarfaty et al. (2010, 2013): • Can we represent words abstractly so as to reflect shared morphological aspects between them? • Which types of morphological information should we include in the parsing model? It is tempting to hypothesize that character-level models effectively solve the first problem. For the second, Tsarfaty et al. (2010) and Seeker and Kuhn (2013) reported that morphological case is Dependency Parsing Model We use"
