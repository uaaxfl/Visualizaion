2004.jeptalnrecital-long.29,W01-1313,0,0.0501768,"Missing"
2004.jeptalnrecital-long.29,J96-1004,0,0.0172102,"Missing"
2004.jeptalnrecital-long.29,E95-1035,0,0.0350308,"Missing"
2004.jeptalnrecital-long.29,P93-1010,0,0.0527618,"Missing"
2004.jeptalnrecital-long.29,P00-1010,0,0.0665455,"Missing"
2004.jeptalnrecital-long.29,W01-1309,0,0.024082,"Missing"
2004.jeptalnrecital-long.29,W01-1312,0,0.0344104,"Missing"
2004.jeptalnrecital-long.6,E93-1028,0,0.0435506,"Missing"
2004.jeptalnrecital-long.6,J98-1001,0,0.152324,"Missing"
2008.jeptalnrecital-long.4,W06-1618,0,0.0216946,"Missing"
2008.jeptalnrecital-long.4,P00-1010,0,0.369941,"Missing"
2008.jeptalnrecital-long.4,C04-1008,1,0.895143,"Missing"
2008.jeptalnrecital-long.4,H05-1088,0,0.176417,"Missing"
2008.jeptalnrecital-long.4,2002.jeptalnrecital-long.22,0,0.10157,"Missing"
2009.jeptalnrecital-court.5,W04-2322,0,0.0343493,"Missing"
2009.jeptalnrecital-court.5,J05-2005,0,0.110676,"Missing"
2010.jeptalnrecital-court.21,2002.jeptalnrecital-long.5,0,0.144701,"Missing"
2010.jeptalnrecital-court.21,2009.jeptalnrecital-long.21,0,0.0613976,"Missing"
2010.jeptalnrecital-court.21,P98-2127,0,0.645278,"Missing"
2010.jeptalnrecital-court.21,C94-1049,0,0.861462,"Missing"
2010.jeptalnrecital-court.21,P06-2111,0,0.261634,"Missing"
2010.jeptalnrecital-long.23,W01-0514,0,0.113978,"Missing"
2010.jeptalnrecital-long.23,D08-1035,0,0.030236,"Missing"
2010.jeptalnrecital-long.23,2002.jeptalnrecital-long.13,0,0.0426162,"Missing"
2010.jeptalnrecital-long.23,P07-1061,0,0.0287066,"Missing"
2010.jeptalnrecital-long.23,C98-1062,0,0.0972628,"Missing"
2010.jeptalnrecital-long.23,P03-1071,0,0.022681,"Missing"
2010.jeptalnrecital-long.23,J97-1003,0,0.121831,"Missing"
2010.jeptalnrecital-long.23,P95-1015,0,0.0740245,"Missing"
2010.jeptalnrecital-long.23,P06-1004,0,0.0227875,"Missing"
2011.jeptalnrecital-long.15,apidianaki-2008-translation,0,0.0369232,"Missing"
2011.jeptalnrecital-long.15,P05-1074,0,0.0589816,"Missing"
2011.jeptalnrecital-long.15,P01-1008,0,0.0646748,"Missing"
2011.jeptalnrecital-long.15,W02-0908,0,0.155683,"action de synonymes à partir d’un corpus en français. Nous présentons ici une analyse plus fine des relations extraites automatiquement en nous intéressant cette fois-ci à la langue anglaise pour laquelle de plus amples ressources sont disponibles. Différentes façons d’évaluer notre approche corroborent le fait que l’approche miroir se comporte globalement mieux que l’approche distributionnelle décrite dans (Lin, 1998), une approche de référence dans le domaine. Abstract. In (Muller & Langlais, 2010), we compared a distributional approach to a variant of the mirror approach described by Dyvik (2002) on a task of synonym extraction. This was conducted on a corpus of the French language. In the present work, we propose a more precise and systematic evaluation of the relations extracted by a mirror and a distributional approaches. This evaluation is conducted on the English language for which widespread resources are available. All the evaluations we conducted in this study concur to the observation that our mirror approach globally outperforms the distributional one described by Lin (1998), which we believe to be a fair reference in the domain. Mots-clés : Sémantique lexicale, similarité d"
2011.jeptalnrecital-long.15,J02-2001,0,0.0557527,"Missing"
2011.jeptalnrecital-long.15,ferret-2010-testing,0,0.0209747,"Missing"
2011.jeptalnrecital-long.15,W05-0604,0,0.0331901,"Missing"
2011.jeptalnrecital-long.15,heylen-etal-2008-modelling,0,0.0556461,"Missing"
2011.jeptalnrecital-long.15,E93-1028,0,0.333377,"Missing"
2011.jeptalnrecital-long.15,P98-2127,0,0.192429,"Missing"
2011.jeptalnrecital-long.15,W08-1911,0,0.0380589,"Missing"
2011.jeptalnrecital-long.15,C82-1036,0,0.700615,"Missing"
2011.jeptalnrecital-long.15,P04-1066,0,0.0820315,"Missing"
2011.jeptalnrecital-long.15,2010.jeptalnrecital-court.21,1,0.852559,"Missing"
2011.jeptalnrecital-long.15,C94-1049,0,0.18905,"Missing"
2011.jeptalnrecital-long.15,C08-1114,0,0.0558635,"Missing"
2011.jeptalnrecital-long.15,P06-2111,0,0.0388452,"Missing"
2011.jeptalnrecital-long.15,J09-3004,0,0.0374471,"Missing"
2017.jeptalnrecital-court.13,E17-2025,0,0.026467,"Missing"
2017.jeptalnrecital-court.13,D16-1033,0,0.042598,"Missing"
2017.jeptalnrecital-court.13,C12-1177,0,0.0402682,"Missing"
2018.jeptalnrecital-deft.5,E17-1104,0,0.0349976,"Missing"
2018.jeptalnrecital-deft.5,D14-1181,0,0.00961622,"Missing"
2019.jeptalnrecital-court.1,Q17-1010,0,0.0257183,"Missing"
2019.jeptalnrecital-court.1,L16-1319,0,0.0641752,"Missing"
2019.jeptalnrecital-court.1,P14-5010,0,0.00471491,"Missing"
2019.jeptalnrecital-court.1,C12-1167,0,0.0528697,"Missing"
2019.jeptalnrecital-court.26,S17-2001,0,0.0673242,"Missing"
2019.jeptalnrecital-court.26,N18-1202,0,0.0565143,"Missing"
2019.jeptalnrecital-court.26,P18-1041,0,0.0431172,"Missing"
2019.jeptalnrecital-court.26,C12-1167,0,0.0754059,"Missing"
2019.jeptalnrecital-court.26,W18-3022,0,0.0493796,"Missing"
2019.jeptalnrecital-deft.8,W18-7002,0,0.051449,"Missing"
2019.jeptalnrecital-deft.8,D18-2021,0,0.0213877,"Missing"
2019.jeptalnrecital-deft.8,S19-1004,1,0.763462,"Missing"
2020.lrec-1.125,C12-1163,0,0.0266887,"aset categories have different associations and our method can provide a sanity check for annotations (e.g. a Contradiction class should be mapped to markers expected to denote a contradiction); – An explanation of why it is useful to employ discourse marker prediction as a training signal for sentence representation learning; DiscSense can also be used to 2. Related work Previous work has amply explored the link between discourse markers and semantic categories. Pitler et al. (2008), for example, use the PDTB to analyze to what extent discourse markers a priori reflect relationship category. Asr and Demberg (2012) have demonstrated that particular relationship categories give rise to more or less presence of discourse markers. And a recent categorization of discourse markers for English is provided in the DimLex lexicon (Das et al., 2018). As mentioned before, discourse markers have equally been used as a learning signal for the prediction of implicit discourse relations (Liu et al., 2016; Braud and Denis, 2016) and inference relations (Pan et al., 2018b). This work has been generalized by DiscSent (Jernite et al., 2017), DisSent (Nie et al., 2019), and Discovery (Sileo et al., 2019b) who use discourse"
2020.lrec-1.125,D15-1075,0,0.0444668,"rides, 2003). Moreover, marker inventories focus on a restricted number of rhetorical relations that are too coarse and not exhaustive, since discourse marker use depends on the grammatical, stylistic, pragmatic, semantic and emotional contexts that can undergo fine grained categorizations. Meanwhile, there exist a number of NLP classification tasks (with associated datasets) that equally consider the relationship between sentences or clauses, but with relations that possibly go beyond the usual discourse relations; these tasks focus on various phenomena such as implication and contradiction (Bowman et al., 2015), semantic similarity, or paraphrase (Dolan et al., 2004). Furthermore, a number of tasks consider single sentence phenomena, such as sentiment, subjectivity, and style. Such characteristics have been somewhat ignored for the linguistic analysis and categorization of discourse markers per se, even though discourse markers have been successfully used to improve categorization performance for these tasks (Jernite et al., 2017; Nie et al., 2019; Pan et al., 2018a; Sileo et al., 2019b). Specifically, the afore-mentioned research shows that the prediction of discourse markers between pairs of sente"
2020.lrec-1.125,D16-1020,0,0.0193248,"d the link between discourse markers and semantic categories. Pitler et al. (2008), for example, use the PDTB to analyze to what extent discourse markers a priori reflect relationship category. Asr and Demberg (2012) have demonstrated that particular relationship categories give rise to more or less presence of discourse markers. And a recent categorization of discourse markers for English is provided in the DimLex lexicon (Das et al., 2018). As mentioned before, discourse markers have equally been used as a learning signal for the prediction of implicit discourse relations (Liu et al., 2016; Braud and Denis, 2016) and inference relations (Pan et al., 2018b). This work has been generalized by DiscSent (Jernite et al., 2017), DisSent (Nie et al., 2019), and Discovery (Sileo et al., 2019b) who use discourse markers to learn general representations of sentences, which are transferable to various NLP classification tasks. However, none of these examine the individual impact of markers on these tasks. 992 3. 3.1. Experimental setup Discourse marker corpus In order to train a model to predict plausible discourse markers between sentence pairs, we use the English Discovery corpus (Sileo et al., 2019b), as it h"
2020.lrec-1.125,W01-1605,0,0.402113,"other efforts in Turkish, Chinese and French (Zeyrek and Webber, 2008; Zhou et al., 2014; Danlos et al., 2015). The PDTB identifies different types of discourse relation categories (such as conjunction and contrast) and the respective markers that frequently instantiate these categories (such as and and however, respectively), and organizes them in a three-level hierarchy. It must be noted, however, that there is no general consensus on the typology of these markers and their rhetorical functions. As such, theoretical alternatives to the PDTB exist, such as Rhetorical Structure Theory or RST (Carlson et al., 2001), and Segmented Discourse Representation Theory or SDRT (Asher and Lascarides, 2003). Moreover, marker inventories focus on a restricted number of rhetorical relations that are too coarse and not exhaustive, since discourse marker use depends on the grammatical, stylistic, pragmatic, semantic and emotional contexts that can undergo fine grained categorizations. Meanwhile, there exist a number of NLP classification tasks (with associated datasets) that equally consider the relationship between sentences or clauses, but with relations that possibly go beyond the usual discourse relations; these"
2020.lrec-1.125,L18-1269,0,0.0166574,"extracted from Wikipedia articles (the 20 markers are a subset of the markers considered in the Discovery dataset); we call this dataset Wiki20. Wiki20 Discovery Majority Class Human Raters Decomposable Attention Bi-LSTM 5.0 23.1 31.8 - 0.6 22.2 BERT+Discovery BERT+Discovery+Wiki20 30.6 47.6 32.9 - Table 1: Discourse marker prediction accuracy percentages on Wiki20 and Discovery datasets. Human Raters and Decomposable Attention are from (Malmi et al., 2018). BiLSTM is from (Sileo et al., 2019b) and the last two are ours. 3.2. et al., 2019) augmented with SUBJ, CR and SICK tasks from SentEval (Conneau and Kiela, 2018) in order to have different domains for sentiment analysis and NLI. We map the semantic similarity estimation task (STS) from GLUE/SentEval into a classification task by casting the ratings into three quantiles and discarding the middle quantile. Table 3 enumerates the classification datasets we used in our study. Classification datasets We leverage classification datasets from DiscEval (Sileo et al., 2019a), alongside GLUE classification tasks (Wang marker category support confidence (prior) unfortunately, sadly, unfortunately, as a result, in contrast, curiously, technically, rather, similar"
2020.lrec-1.125,2015.jeptalnrecital-court.6,0,0.0278072,"ses or sentences. For example, the marker so in sentence (1) indicates that the second clause is a consequence of the first. (1) We’re standing in gasoline, so you should not smoke. Several resources enumerate discourse markers and their use in different languages, either in discourse marker lexicons (Knott, 1996; Stede, 2002; Roze et al., 2012; Das et al., 2018) or in corpora, annotated with discourse relations, such as the well-known English Penn Discourse TreeBank (Prasad et al., 2008), which inspired other efforts in Turkish, Chinese and French (Zeyrek and Webber, 2008; Zhou et al., 2014; Danlos et al., 2015). The PDTB identifies different types of discourse relation categories (such as conjunction and contrast) and the respective markers that frequently instantiate these categories (such as and and however, respectively), and organizes them in a three-level hierarchy. It must be noted, however, that there is no general consensus on the typology of these markers and their rhetorical functions. As such, theoretical alternatives to the PDTB exist, such as Rhetorical Structure Theory or RST (Carlson et al., 2001), and Segmented Discourse Representation Theory or SDRT (Asher and Lascarides, 2003). Mor"
2020.lrec-1.125,W18-5042,0,0.296366,"DiscSense, is publicly available. Keywords: Discourse marker semantics, pragmatics, discourse marker prediction 1. Motivation Discourse markers are a common language device used to make explicit the semantic and/or pragmatic relationships between clauses or sentences. For example, the marker so in sentence (1) indicates that the second clause is a consequence of the first. (1) We’re standing in gasoline, so you should not smoke. Several resources enumerate discourse markers and their use in different languages, either in discourse marker lexicons (Knott, 1996; Stede, 2002; Roze et al., 2012; Das et al., 2018) or in corpora, annotated with discourse relations, such as the well-known English Penn Discourse TreeBank (Prasad et al., 2008), which inspired other efforts in Turkish, Chinese and French (Zeyrek and Webber, 2008; Zhou et al., 2014; Danlos et al., 2015). The PDTB identifies different types of discourse relation categories (such as conjunction and contrast) and the respective markers that frequently instantiate these categories (such as and and however, respectively), and organizes them in a three-level hierarchy. It must be noted, however, that there is no general consensus on the typology o"
2020.lrec-1.125,N19-1423,0,0.0164824,"5 (12.5) 61.4 (7.0) 16.2 (1.3) 67.6 (10.6) 71.2 (26.7) 65.6 (26.7) Table 2: Sample of categories and most associated markers. CR.neg denotes the negative class in the CR dataset. Datasets are described in table 3. Support is the number of examples where the marker was predicted given a dataset. Confidence is the estimated probability of the class given the prediction of the marker i.e. P (y|m). The prior is P (y). A larger version is available in annex A and a full version is available at https://github.com/ synapse-developpement/DiscSense. 3.3. Model For our experiments, we make use of BERT (Devlin et al., 2019), as a model for relation prediction. BERT is a text encoder pre-trained using language modeling having demonstrated state of the art results in various tasks of relation prediction between sentences, which is our use-case. The parameters are initialized with the pre-trained unsupervised base-uncased model and then fine-tuned using the Adam (Kingma and Ba, 2014) optimizer with 2 iterations on our corpus data, using default hyperparameters1 otherwise. We ran marker prediction experiments using BERT on both Discovery and Wiki20. 4. 4.1. Results Marker prediction accuracy Table 1 shows the result"
2020.lrec-1.125,C04-1051,0,0.0260744,"ricted number of rhetorical relations that are too coarse and not exhaustive, since discourse marker use depends on the grammatical, stylistic, pragmatic, semantic and emotional contexts that can undergo fine grained categorizations. Meanwhile, there exist a number of NLP classification tasks (with associated datasets) that equally consider the relationship between sentences or clauses, but with relations that possibly go beyond the usual discourse relations; these tasks focus on various phenomena such as implication and contradiction (Bowman et al., 2015), semantic similarity, or paraphrase (Dolan et al., 2004). Furthermore, a number of tasks consider single sentence phenomena, such as sentiment, subjectivity, and style. Such characteristics have been somewhat ignored for the linguistic analysis and categorization of discourse markers per se, even though discourse markers have been successfully used to improve categorization performance for these tasks (Jernite et al., 2017; Nie et al., 2019; Pan et al., 2018a; Sileo et al., 2019b). Specifically, the afore-mentioned research shows that the prediction of discourse markers between pairs of sentences can be exploited as a training signal that improves"
2020.lrec-1.125,L18-1260,0,0.0180066,"scovery datasets illustrating various relation senses predicted by DiscSense Discovery test data is quite high given the large number of classes (174, perfectly balanced) and sometimes their low semantic distinguishability. This accuracy is significantly higher than the score of the Bi-LSTM model in the setup of Sileo et al. (2019b). The BERT model finetuned on Discovery outperforms human performance reported on Wiki20 with no other adaptation than discarding markers not in Wiki20 during inference.2 With a further step of finetuning (1 epoch on Wiki20), we also outperform the best model from (Malmi et al., 2018). These results suggest that the BERT+Discovery model captures a significant part of the use of discourse markers; in the following section, we will apply it to the prediction of discourse markers for indi2 But note that there is some overlap between training data since BERT pretraining uses Wikipedia text. vidual categories. 4.2. Prediction of markers associated to semantic categories For each semantic dataset, consisting of either annotated sentences (s1 , y) or annotated sentence pairs (s1 ,s2 ,y), where y is a category, we use the BERT+Discovery model to predict the most plausible marker m"
2020.lrec-1.125,P19-1442,0,0.102817,"ut with relations that possibly go beyond the usual discourse relations; these tasks focus on various phenomena such as implication and contradiction (Bowman et al., 2015), semantic similarity, or paraphrase (Dolan et al., 2004). Furthermore, a number of tasks consider single sentence phenomena, such as sentiment, subjectivity, and style. Such characteristics have been somewhat ignored for the linguistic analysis and categorization of discourse markers per se, even though discourse markers have been successfully used to improve categorization performance for these tasks (Jernite et al., 2017; Nie et al., 2019; Pan et al., 2018a; Sileo et al., 2019b). Specifically, the afore-mentioned research shows that the prediction of discourse markers between pairs of sentences can be exploited as a training signal that improves performance on existing classification datasets. In this work, we make use of a model trained on discourse marker prediction in order to predict plausible discourse markers between sentence pairs from existing datasets, which are annotated with the correct semantic categories. This allows us to explore the following questions: – Which semantic categories are applicable to a particular"
2020.lrec-1.125,P18-1091,0,0.33178,"that possibly go beyond the usual discourse relations; these tasks focus on various phenomena such as implication and contradiction (Bowman et al., 2015), semantic similarity, or paraphrase (Dolan et al., 2004). Furthermore, a number of tasks consider single sentence phenomena, such as sentiment, subjectivity, and style. Such characteristics have been somewhat ignored for the linguistic analysis and categorization of discourse markers per se, even though discourse markers have been successfully used to improve categorization performance for these tasks (Jernite et al., 2017; Nie et al., 2019; Pan et al., 2018a; Sileo et al., 2019b). Specifically, the afore-mentioned research shows that the prediction of discourse markers between pairs of sentences can be exploited as a training signal that improves performance on existing classification datasets. In this work, we make use of a model trained on discourse marker prediction in order to predict plausible discourse markers between sentence pairs from existing datasets, which are annotated with the correct semantic categories. This allows us to explore the following questions: – Which semantic categories are applicable to a particular discourse marker ("
2020.lrec-1.125,C08-2022,0,0.0581427,"rization of categories of discourse markers can help “diagnosing” a classification dataset; As shown in table 2 below, SICK/MNLI dataset categories have different associations and our method can provide a sanity check for annotations (e.g. a Contradiction class should be mapped to markers expected to denote a contradiction); – An explanation of why it is useful to employ discourse marker prediction as a training signal for sentence representation learning; DiscSense can also be used to 2. Related work Previous work has amply explored the link between discourse markers and semantic categories. Pitler et al. (2008), for example, use the PDTB to analyze to what extent discourse markers a priori reflect relationship category. Asr and Demberg (2012) have demonstrated that particular relationship categories give rise to more or less presence of discourse markers. And a recent categorization of discourse markers for English is provided in the DimLex lexicon (Das et al., 2018). As mentioned before, discourse markers have equally been used as a learning signal for the prediction of implicit discourse relations (Liu et al., 2016; Braud and Denis, 2016) and inference relations (Pan et al., 2018b). This work has"
2020.lrec-1.125,prasad-etal-2008-penn,0,0.0909452,"n Discourse markers are a common language device used to make explicit the semantic and/or pragmatic relationships between clauses or sentences. For example, the marker so in sentence (1) indicates that the second clause is a consequence of the first. (1) We’re standing in gasoline, so you should not smoke. Several resources enumerate discourse markers and their use in different languages, either in discourse marker lexicons (Knott, 1996; Stede, 2002; Roze et al., 2012; Das et al., 2018) or in corpora, annotated with discourse relations, such as the well-known English Penn Discourse TreeBank (Prasad et al., 2008), which inspired other efforts in Turkish, Chinese and French (Zeyrek and Webber, 2008; Zhou et al., 2014; Danlos et al., 2015). The PDTB identifies different types of discourse relation categories (such as conjunction and contrast) and the respective markers that frequently instantiate these categories (such as and and however, respectively), and organizes them in a three-level hierarchy. It must be noted, however, that there is no general consensus on the typology of these markers and their rhetorical functions. As such, theoretical alternatives to the PDTB exist, such as Rhetorical Structur"
2020.lrec-1.125,N19-1351,1,0.882122,"Missing"
2020.lrec-1.125,I08-7009,0,0.0321097,"and/or pragmatic relationships between clauses or sentences. For example, the marker so in sentence (1) indicates that the second clause is a consequence of the first. (1) We’re standing in gasoline, so you should not smoke. Several resources enumerate discourse markers and their use in different languages, either in discourse marker lexicons (Knott, 1996; Stede, 2002; Roze et al., 2012; Das et al., 2018) or in corpora, annotated with discourse relations, such as the well-known English Penn Discourse TreeBank (Prasad et al., 2008), which inspired other efforts in Turkish, Chinese and French (Zeyrek and Webber, 2008; Zhou et al., 2014; Danlos et al., 2015). The PDTB identifies different types of discourse relation categories (such as conjunction and contrast) and the respective markers that frequently instantiate these categories (such as and and however, respectively), and organizes them in a three-level hierarchy. It must be noted, however, that there is no general consensus on the typology of these markers and their rhetorical functions. As such, theoretical alternatives to the PDTB exist, such as Rhetorical Structure Theory or RST (Carlson et al., 2001), and Segmented Discourse Representation Theory"
2021.disrpt-1.1,W11-0401,0,0.0375854,"140 332 267 50 50 documents 2,162 197 164 units 3,040 19,268 21,789 12,588 4,202 5,855 3,429 2,343 5,537 41,542 3,351 744 744 units 26,048 8,748 1,660 segStyle EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU segStyle Conn Conn Conn relations 2,164 13,897 16,002 9,580 2,533 4,100 2,185 1,608 4,148 28,868 2,240 439 439 relations 43,920 2,451 3,657 relTypes 26 23 17 16 29 17 18 32 32 22 28 24 26 relTypes 23 23 9 discontinuous No Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes discontinuous Yes Yes Yes Table 2: Datasets in the DISRPT 2021 Shared Task. − spa.rst.rststb - RST Spanish Treebank (da Cunha et al., 2011). − spa.rst.sctb - RST Spanish-Chinese Treebank (Spanish) (Cao et al., 2018). − tur.pdtb.tdb - Turkish Discourse Bank (Zeyrek and Webber, 2008; Zeyrek and Kurfalı, 2017). − zho.pdtb.cdtb - Chinese Discourse Treebank (Zhou et al., 2014). − zho.rst.sctb - RST Spanish-Chinese Treebank (Chinese) (Cao et al., 2018). A script included in the shared task repository was provided in order to reconstruct the data, which requires users to have access to the original LDC releases of the underlying corpora in the case of PDTB, RST-DT, and CDTB or the original Turkish texts available from Middle East Techni"
2021.disrpt-1.1,N19-1423,0,0.0235502,"Missing"
2021.disrpt-1.1,L16-1432,0,0.0583497,"Missing"
2021.disrpt-1.1,2021.iwpt-1.19,0,0.0378038,"obtain superior results (see Ezzabady et al. 2021), possibly because the single-corpus training results are already so strong. models, which may also be a reason for higher scores on Chinese and especially Turkish, which may be less well-represented by multilingual LMs. Finally we note that in the plain text scenario, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific transformers, with the result that plain text numbers are very close to gold numbers for DisCoDisCo (and in fact insignificantly better for plain Connective Detection: 91.49 on average vs. 91.22 for gold treebanked data). This echoes results from the 2019 task (see Yu et al. 2019) which showed the crucial importance of high quality preprocessing in general, and sentence splitting in particular. Among the Transformer-based systems, although DisCoDisCo performs best overall, other systems score highest on particular datasets in different scenarios, with SegFormers a"
2021.disrpt-1.1,E17-1028,1,0.830073,"ation of the Shared Task included for the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993"
2021.disrpt-1.1,2021.disrpt-1.3,1,0.785514,"one English case (training on GUM and testing on RST-DT, leading to a 5 point gain from 75 to 80), and the two small Spanish and Chinese datasets (a 3 point gain from 66 to 69 for Spanish when training on the larger Spanish RST STB, and a more modest but surprising 2 point gain from 72 to 74 when training on Basque and testing on Chinese, see Dönicke 2021). The latter two cases are probably owing to the small size of the target corpora. The disCut paper takes a related approach and trains on multiple corpora from the same language or language family, but does not obtain superior results (see Ezzabady et al. 2021), possibly because the single-corpus training results are already so strong. models, which may also be a reason for higher scores on Chinese and especially Turkish, which may be less well-represented by multilingual LMs. Finally we note that in the plain text scenario, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific"
2021.disrpt-1.1,W18-4917,1,0.905534,"Missing"
2021.disrpt-1.1,2021.disrpt-1.6,1,0.700524,"is meant to approach the case of labelling an unlabelled discourse dependency graph. In terms of encoding features, DisCoDisCo uses a sentence pair classifier architecture feeding the underlying transformer embeddings in the form of the BERT classification ([CLS]) token embeddings, while injecting categorical features at two levels: using pseudo tokens, encoded as usual by the transformer, which indicate the relations direction; and using an inserted pseudo token next to the CLS token, which encodes all sequence-level categorical features, such as genre, gold speaker information and more (see Gessler et al. 2021). These features are then read and trained on by the transformer block alongside the word embeddings. The DiscRel system takes a two level approach to relation classification, using two stacked oneversus-rest Random Forest classifiers, which first attempt to distinguish the coarse class of discourse For Connective Detection, DisCoDisCo’s lead becomes slightly larger, about 3 points above SegFormers and almost 7 points above disCut, perhaps due to a combination of features (which give a minor boost of just a few points across datasets in their paper, see Gessler et al. 2021), the use of a CRF l"
2021.disrpt-1.1,2020.lrec-1.648,1,0.731887,"age or language family, but does not obtain superior results (see Ezzabady et al. 2021), possibly because the single-corpus training results are already so strong. models, which may also be a reason for higher scores on Chinese and especially Turkish, which may be less well-represented by multilingual LMs. Finally we note that in the plain text scenario, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific transformers, with the result that plain text numbers are very close to gold numbers for DisCoDisCo (and in fact insignificantly better for plain Connective Detection: 91.49 on average vs. 91.22 for gold treebanked data). This echoes results from the 2019 task (see Yu et al. 2019) which showed the crucial importance of high quality preprocessing in general, and sentence splitting in particular. Among the Transformer-based systems, although DisCoDisCo performs best overall, other systems score highest on particular datasets in d"
2021.disrpt-1.1,2020.codi-1.17,0,0.12301,"getown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 20"
2021.disrpt-1.1,W01-1605,0,0.309662,"Missing"
2021.disrpt-1.1,N16-1013,0,0.0234056,"track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on discourse segmentation"
2021.disrpt-1.1,K16-2018,0,0.155925,"eldes Georgetown University Yang Janet Liu Georgetown University Mikel Iruskieta University of the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two task"
2021.disrpt-1.1,prasad-etal-2008-penn,0,0.114698,"al gains in labeled accuracy when combined with the approaches tested in this Shared Task. Relation Classification The main results for the relation classification task are given in Table 6. Since the relation classification task is new, it is difficult to evaluate the quality of the scores obtained in the results. For some datasets, such as PDTB, existing scores have been previously reported on the same underlying data, but generally in different settings, where implicit and explicit relations were scored separately. To make matters worse, most previous work on PDTB uses the older version 2 (Prasad et al., 2008), making scores again completely non-comparable. Previous scores on PDTB-V2 (level 2 relations, the same hierarchical level used for the Shared Task) with explicitly specified connectives have reached scores above 90 for a while (Kido and Aizawa, 2016); more recently, results on PDTB-V3 implicit relation classification have reached an accuracy of 64.83 (Kim et al., 2020). By comparison, we see a best score of 74.44 (DisCoDisCo) on all PDTB-V3 relations (including implicit and explicit), but the task is both easier and harder, since in both cases no connective is specified (for uniformity with"
2021.disrpt-1.1,2020.acl-main.480,0,0.176467,"the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the i"
2021.disrpt-1.1,J14-4007,0,0.108043,"ruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on discourse segmentation and connective detection since 2019, this year we decided to extend the competition to a new 1 https://github.com/hadiveisi/ PersianRST 2 https://github.com/disrpt/ sharedtask2021 ∗ Discourse Relation Parsing and Treebanking (DISRPT 2021) was held in conjunction with CODI at EMNLP 2021 in the Dominican Republic and Online (https://sites. google.com/georgetown.edu/disrpt2021). 1 Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021), pages 1"
2021.disrpt-1.1,redeker-etal-2012-multi,0,0.0807605,"Missing"
2021.disrpt-1.1,D16-1035,0,0.0261747,"the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on di"
2021.disrpt-1.1,2020.emnlp-main.365,0,0.0133432,"y segmentation system using per-language transformers and making use of grammatical features, which in the plain text scenario were induced using a SOTA Transformerbased parser and sentence splitter as well. Relation Classification Although both systems tackling the Relation Classification task use word embeddings, their approaches are rather different: while the best system, DisCoDisCo, relies on a similar approach to their segmentation task entry, which encodes each sequence using a transformer, the DiscRel system uses whole sentence embeddings from the Sentence-Transformers library (SBERT, Reimers and Gurevych 2020) to compute Euclidean distance between discourse units, as part of a two-level feature-based Random Forest classifier. Both systems also use the provided discourse relation direction, which is meant to approach the case of labelling an unlabelled discourse dependency graph. In terms of encoding features, DisCoDisCo uses a sentence pair classifier architecture feeding the underlying transformer embeddings in the form of the BERT classification ([CLS]) token embeddings, while injecting categorical features at two levels: using pseudo tokens, encoded as usual by the transformer, which indicate th"
2021.disrpt-1.1,stede-neumann-2014-potsdam,0,0.0619789,"Missing"
2021.disrpt-1.1,2021.ccl-1.108,0,0.0186206,"Missing"
2021.disrpt-1.1,W17-3604,0,0.0412999,"Missing"
2021.disrpt-1.1,J18-2001,1,0.853464,"oé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 2021).1 While it remains impossible to place some data openly onlin"
2021.disrpt-1.1,2021.disrpt-1.5,0,0.0322926,"of a CRF layer and the combination of both contextualized and static word embeddings as well as character embeddings, which provided a variety of views on the data and allowed for both fine tuning and fixed generalization in the test set. Choice of language models is also an important factor, and here again disCut and SegFormers rely on multilingual LMs, while DisCoDisCo uses individual per-language 8 relation (for example collapsing relations such as EVALUATION , INTERPRETATION , JUSTIFY ), and then sub-classify the coarse classes into the final target labels using the second classifier (see Varachkina and Pannach 2021). For large datasets, the first classifier is trained on the train set, while the second is trained using the development data, but for smaller datasets, training data is duplicated along with the development set for the second classifier. DisCoDisCo could be used in a two stage approach such as the one taken by DiscRel. 7 Conclusion We conducted the second DISRPT shared task for Discourse Relation Parsing and Treebanking across frameworks, resulting in a number of new state of the art scores on benchmark datasets, as well as the publication of a new multilingual benchmark for discourse parsin"
2021.disrpt-1.1,P19-1442,0,0.0209862,"rsity Yang Janet Liu Georgetown University Mikel Iruskieta University of the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and ad"
2021.disrpt-1.1,K15-2002,0,0.0179648,"Task included for the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progres"
2021.disrpt-1.1,K16-2004,0,0.024196,"skieta University of the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became"
2021.disrpt-1.1,W19-2717,1,0.85001,"o, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific transformers, with the result that plain text numbers are very close to gold numbers for DisCoDisCo (and in fact insignificantly better for plain Connective Detection: 91.49 on average vs. 91.22 for gold treebanked data). This echoes results from the 2019 task (see Yu et al. 2019) which showed the crucial importance of high quality preprocessing in general, and sentence splitting in particular. Among the Transformer-based systems, although DisCoDisCo performs best overall, other systems score highest on particular datasets in different scenarios, with SegFormers attaining a new SOTA score on RST-DT from gold trees (97.09) next to DisCoDisCo’s new SOTA score on the same data without gold trees (96.35 in the plain text scenario), and disCut showing impressively strong performance on German (95.61 from gold trees, and an even better 95.63 in the plain text scenario). Over"
2021.disrpt-1.1,W19-2713,1,0.767456,"DRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on discourse segmentation and connective detection since 2019, this year we decided to extend the comp"
2021.disrpt-1.1,W17-0809,0,0.0343718,"EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU segStyle Conn Conn Conn relations 2,164 13,897 16,002 9,580 2,533 4,100 2,185 1,608 4,148 28,868 2,240 439 439 relations 43,920 2,451 3,657 relTypes 26 23 17 16 29 17 18 32 32 22 28 24 26 relTypes 23 23 9 discontinuous No Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes discontinuous Yes Yes Yes Table 2: Datasets in the DISRPT 2021 Shared Task. − spa.rst.rststb - RST Spanish Treebank (da Cunha et al., 2011). − spa.rst.sctb - RST Spanish-Chinese Treebank (Spanish) (Cao et al., 2018). − tur.pdtb.tdb - Turkish Discourse Bank (Zeyrek and Webber, 2008; Zeyrek and Kurfalı, 2017). − zho.pdtb.cdtb - Chinese Discourse Treebank (Zhou et al., 2014). − zho.rst.sctb - RST Spanish-Chinese Treebank (Chinese) (Cao et al., 2018). A script included in the shared task repository was provided in order to reconstruct the data, which requires users to have access to the original LDC releases of the underlying corpora in the case of PDTB, RST-DT, and CDTB or the original Turkish texts available from Middle East Technical University (METU) by request. Missing data for GUM is downloadable directly by running the script and consenting to non-commercial use conditions. The short names fo"
2021.disrpt-1.1,I08-7009,0,0.0665657,"8,748 1,660 segStyle EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU segStyle Conn Conn Conn relations 2,164 13,897 16,002 9,580 2,533 4,100 2,185 1,608 4,148 28,868 2,240 439 439 relations 43,920 2,451 3,657 relTypes 26 23 17 16 29 17 18 32 32 22 28 24 26 relTypes 23 23 9 discontinuous No Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes discontinuous Yes Yes Yes Table 2: Datasets in the DISRPT 2021 Shared Task. − spa.rst.rststb - RST Spanish Treebank (da Cunha et al., 2011). − spa.rst.sctb - RST Spanish-Chinese Treebank (Spanish) (Cao et al., 2018). − tur.pdtb.tdb - Turkish Discourse Bank (Zeyrek and Webber, 2008; Zeyrek and Kurfalı, 2017). − zho.pdtb.cdtb - Chinese Discourse Treebank (Zhou et al., 2014). − zho.rst.sctb - RST Spanish-Chinese Treebank (Chinese) (Cao et al., 2018). A script included in the shared task repository was provided in order to reconstruct the data, which requires users to have access to the original LDC releases of the underlying corpora in the case of PDTB, RST-DT, and CDTB or the original Turkish texts available from Middle East Technical University (METU) by request. Missing data for GUM is downloadable directly by running the script and consenting to non-commercial use con"
2021.disrpt-1.1,2021.acl-long.449,0,0.357093,"ta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 2021).1 While it remai"
2021.disrpt-1.1,2021.acl-long.305,0,0.196304,"ta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 2021).1 While it remai"
2021.disrpt-1.3,2020.autosimtrans-1.5,0,0.107869,"mentation and Connective Identification: M ELODI at DISRPT2021 Morteza Ezzabady and Philippe Muller and Chloé Braud IRIT, University of Toulouse, CNRS, ANITI Abstract This campaign made available several existing corpora in different language in a common format, expressing the task as a sequence tagging problem, where tokens are to be classified as beginning a segment or not, or, in the case of PDTB corpora, being part of a discourse connective signalling a relation between textual arguments. Segmentation in itself has also shown a lot of potential as an auxiliary task in machine translation (Chen et al., 2020) and summarization (Xu et al., 2020), independently of discourse parsing. DISRPT 2021 shared task reproduces the same setting as DISRPT 2019, with some additional data and minor modifications of the original datasets, segmentation as task 1, connective identification as task 2, and adds the prediction of relations between segments, assuming those are known, as task 3. The following shows examples from the English datasets, illustrating respectively task 1 and task 2, with intended units to recover marked between brackets: We present an approach for discourse segmentation and discourse connecti"
2021.disrpt-1.3,L16-1432,0,0.278599,"re Theory (Mann and Thompson, 1988), which assumes a linear segmentation of documents in discourse units (no overlaps), which are then related in constituant tree structures. This is followed in the majority of corpora (11). • Segmented Discrouse Representation Theory (Asher and Lascarides, 2003), which allows for embedded segments, and were linearized here for homogeneity of the task: a segment embedded in another one was re-annotated as forming 3 three segments. This is the case of 24 in the following publications: English RSTDT (Carlson et al., 2001), PDTB (Prasad et al., 2008), SDRT-STAC (Asher et al., 2016) and GUM (Zeldes, 2016), Spanish RST (2)(da Cunha et al., 2011; Cao et al., 2018) Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018), German RST (Stede and Neumann, 2014), French SDRT-Annodis (Afantenos et al., 2012), Basque RST (Iruskieta et al., 2013), Portuguese RST (Cardoso et al., 2011), Russian RST (Pisarevskaya et al., 2017), Turkish PDTB (Zeyrek et al., 2013) Dutch RST (Redeker et al., 2012) and Persian RST (Shahmohammadi et al., 2021). 4 They showed that XLM didn’t help, but the extra layer of LSTM could. Changes to other hyperparameters didn’t improve these preliminary results so"
2021.disrpt-1.3,W11-0401,0,0.527974,"mentation of documents in discourse units (no overlaps), which are then related in constituant tree structures. This is followed in the majority of corpora (11). • Segmented Discrouse Representation Theory (Asher and Lascarides, 2003), which allows for embedded segments, and were linearized here for homogeneity of the task: a segment embedded in another one was re-annotated as forming 3 three segments. This is the case of 24 in the following publications: English RSTDT (Carlson et al., 2001), PDTB (Prasad et al., 2008), SDRT-STAC (Asher et al., 2016) and GUM (Zeldes, 2016), Spanish RST (2)(da Cunha et al., 2011; Cao et al., 2018) Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018), German RST (Stede and Neumann, 2014), French SDRT-Annodis (Afantenos et al., 2012), Basque RST (Iruskieta et al., 2013), Portuguese RST (Cardoso et al., 2011), Russian RST (Pisarevskaya et al., 2017), Turkish PDTB (Zeyrek et al., 2013) Dutch RST (Redeker et al., 2012) and Persian RST (Shahmohammadi et al., 2021). 4 They showed that XLM didn’t help, but the extra layer of LSTM could. Changes to other hyperparameters didn’t improve these preliminary results so we kept them as in the original model. Details of the paramet"
2021.disrpt-1.3,W19-2714,0,0.0256453,"Missing"
2021.disrpt-1.3,N19-1017,0,0.364028,"ual embeddings. Multi-lingual discourse parsing is also becoming more popular, see for instance (Braud et al., 2017a; Chen et al., 2020), in which it is seen as a form of multi-task learning problem, but this was not applied to discourse segmentation. In other NLP subfields, leveraging availability of corpora in different languages for the same tasks is an active area of research, with different strategies for combining tasks and languages, using meta-learning and complex sampling strategies (Nooralahzadeh et al., 2020; Tarunesh et al., 2021). A simpler approach that inspired us here, due to (Dehouck and Denis, 2019), is to use the relations between close languages to guide the training process on a task: a generic model is trained on groups of languages, further refined with models by subgroups and finally fine-tuned on individual languages. 3 Data The 2021 shared task provides 16 corpora annotated either with discourse boundaries (13) or discourse connectives in the case of PDTB corpora (3), with the RST Farsi corpus as a surprise dataset. This covers 11 different languages, mostly indoeuropean languages, and with a majority of european languages: 3 romance (Spanish, French, Portuguese), 3 germanic (Eng"
2021.disrpt-1.3,E17-1028,1,0.957821,"sentence-level segmentation, with sentence boundaries given (when annotated, or provided with a sentence splitter otherwise), also informed by syntactic parsing of sentences, or without any of that information. With the exception of systems presented at DISRPT 2019 (Bourgonje and Schäfer, 2019; Yu et al., 2019; Muller et al., 2019), existing work on segmentation always assumed gold sentences, e.g. (Wang et al., 2018; Lukasik et al., 2020). One interesting aspect of such a task is the availability of comparable data in different languages. This has been leveraged in the past for segmentation (Braud et al., 2017b), but not in the past 2019 campaign, where the best system relied on fine-tuning a contextual language model for each language separately, albeit using the same multilingual embedding model (Muller et al., 2019). Here we propose to build on the previous DISRPT best system and exploit the availability of multiple corpora for the same language, or the same family of languages (romance, germanic) to augment training of dedicated models. Combining this approach with a few adjustments to the base model, we manage to improve on many datasets compare to the previous best DISRPT systems, with a mean"
2021.disrpt-1.3,N19-1423,0,0.016264,"ove these preliminary results so we kept them as in the original model. Details of the parameters can be found in the declarative config file of the model, also added as supplementary material. Input Approach In this paper we want to leverage combinations of multiple datasets for training, not only with corpora for the same language and task, but also with languages from the same families. 4.1 conll Base architecture We started from the architecture that showed the best results on almost all languages and configurations at DISRPT 2019, namely (Muller et al., 2019), which is built around BERT (Devlin et al., 2019), a contextual language model that is easy to fine-tune on sequence tagging problems. The original architecture combined BERT contextual embeddings to the output of CNN filters over characters of each word piece, that were then fed to single-layer BiLSTM layer for the final prediction. The model is initialized with the multilingual BERT model, then fine-tune on all corpora separately as sequence tagging tasks. The original implementation used the AllenNLP library (Gardner et al., 2017), and so does our implementation. Since BERT has a limitation on the number of word pieces it can take as inpu"
2021.disrpt-1.3,P17-2037,1,0.895358,"sentence-level segmentation, with sentence boundaries given (when annotated, or provided with a sentence splitter otherwise), also informed by syntactic parsing of sentences, or without any of that information. With the exception of systems presented at DISRPT 2019 (Bourgonje and Schäfer, 2019; Yu et al., 2019; Muller et al., 2019), existing work on segmentation always assumed gold sentences, e.g. (Wang et al., 2018; Lukasik et al., 2020). One interesting aspect of such a task is the availability of comparable data in different languages. This has been leveraged in the past for segmentation (Braud et al., 2017b), but not in the past 2019 campaign, where the best system relied on fine-tuning a contextual language model for each language separately, albeit using the same multilingual embedding model (Muller et al., 2019). Here we propose to build on the previous DISRPT best system and exploit the availability of multiple corpora for the same language, or the same family of languages (romance, germanic) to augment training of dedicated models. Combining this approach with a few adjustments to the base model, we manage to improve on many datasets compare to the previous best DISRPT systems, with a mean"
2021.disrpt-1.3,D17-1258,1,0.914356,"sentence-level segmentation, with sentence boundaries given (when annotated, or provided with a sentence splitter otherwise), also informed by syntactic parsing of sentences, or without any of that information. With the exception of systems presented at DISRPT 2019 (Bourgonje and Schäfer, 2019; Yu et al., 2019; Muller et al., 2019), existing work on segmentation always assumed gold sentences, e.g. (Wang et al., 2018; Lukasik et al., 2020). One interesting aspect of such a task is the availability of comparable data in different languages. This has been leveraged in the past for segmentation (Braud et al., 2017b), but not in the past 2019 campaign, where the best system relied on fine-tuning a contextual language model for each language separately, albeit using the same multilingual embedding model (Muller et al., 2019). Here we propose to build on the previous DISRPT best system and exploit the availability of multiple corpora for the same language, or the same family of languages (romance, germanic) to augment training of dedicated models. Combining this approach with a few adjustments to the base model, we manage to improve on many datasets compare to the previous best DISRPT systems, with a mean"
2021.disrpt-1.3,P07-1062,0,0.0429196,"available. We had licences for all of them except the Mandarin corpus (zho.pdtb.cdtb), provided by the organizers for the evaluation of the task. Except Farsi, all the datasets were present in the DISRPT 2019 shared task, but the russian Related work Discourse segmentation appeared as an NLP task with the creation of the first annotated RST documents in English, and was primarily rule-based (Marcu, 2000). Since then the literature on discourse parsing generally assumed that elementary discourse units (discourse segments) were given, with only a handful of exceptions (Soricut and Marcu, 2003; Fisher and Roark, 2007; Tofiloski 2 https://gitlab.irit.fr/melodi/ andiamo/discoursesegmentation/discut 23 Corpus Lang Train # Doc. Dev Sent seg # Sents. Train # Units Train manual manual manual 44,563 25,080 2,049 23,850 6,841 1,034 manual manual manual manual stanza manual manual stanza manual manual manual 6,672 3,600 1,773 991 1713 1,202 1,595 18932 1,577 304 344 17,646 5,012 2,449 1,713 4607 1,679 3,916 34682 2,474 473 473 manual manual 7,689 880 8,843 2,411 Test Connectives — PDTB eng.pdtb.pdtb tur.pdtb.tdb zho.pdtb.cdtb en tr zh 1,992 159 125 79 19 21 91 19 18 EDUs — RST eng.rst.rstdt eng.rst.gum deu.rst.pcc"
2021.disrpt-1.3,W18-4917,0,0.414266,"ts in discourse units (no overlaps), which are then related in constituant tree structures. This is followed in the majority of corpora (11). • Segmented Discrouse Representation Theory (Asher and Lascarides, 2003), which allows for embedded segments, and were linearized here for homogeneity of the task: a segment embedded in another one was re-annotated as forming 3 three segments. This is the case of 24 in the following publications: English RSTDT (Carlson et al., 2001), PDTB (Prasad et al., 2008), SDRT-STAC (Asher et al., 2016) and GUM (Zeldes, 2016), Spanish RST (2)(da Cunha et al., 2011; Cao et al., 2018) Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018), German RST (Stede and Neumann, 2014), French SDRT-Annodis (Afantenos et al., 2012), Basque RST (Iruskieta et al., 2013), Portuguese RST (Cardoso et al., 2011), Russian RST (Pisarevskaya et al., 2017), Turkish PDTB (Zeyrek et al., 2013) Dutch RST (Redeker et al., 2012) and Persian RST (Shahmohammadi et al., 2021). 4 They showed that XLM didn’t help, but the extra layer of LSTM could. Changes to other hyperparameters didn’t improve these preliminary results so we kept them as in the original model. Details of the parameters can be found in"
2021.disrpt-1.3,W01-1605,0,0.357884,"e details can be found about all datasets • Rhetorical Structure Theory (Mann and Thompson, 1988), which assumes a linear segmentation of documents in discourse units (no overlaps), which are then related in constituant tree structures. This is followed in the majority of corpora (11). • Segmented Discrouse Representation Theory (Asher and Lascarides, 2003), which allows for embedded segments, and were linearized here for homogeneity of the task: a segment embedded in another one was re-annotated as forming 3 three segments. This is the case of 24 in the following publications: English RSTDT (Carlson et al., 2001), PDTB (Prasad et al., 2008), SDRT-STAC (Asher et al., 2016) and GUM (Zeldes, 2016), Spanish RST (2)(da Cunha et al., 2011; Cao et al., 2018) Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018), German RST (Stede and Neumann, 2014), French SDRT-Annodis (Afantenos et al., 2012), Basque RST (Iruskieta et al., 2013), Portuguese RST (Cardoso et al., 2011), Russian RST (Pisarevskaya et al., 2017), Turkish PDTB (Zeyrek et al., 2013) Dutch RST (Redeker et al., 2012) and Persian RST (Shahmohammadi et al., 2021). 4 They showed that XLM didn’t help, but the extra layer of LSTM could. Changes to other"
2021.disrpt-1.3,N03-1030,0,0.227129,"orpus, and are not freely available. We had licences for all of them except the Mandarin corpus (zho.pdtb.cdtb), provided by the organizers for the evaluation of the task. Except Farsi, all the datasets were present in the DISRPT 2019 shared task, but the russian Related work Discourse segmentation appeared as an NLP task with the creation of the first annotated RST documents in English, and was primarily rule-based (Marcu, 2000). Since then the literature on discourse parsing generally assumed that elementary discourse units (discourse segments) were given, with only a handful of exceptions (Soricut and Marcu, 2003; Fisher and Roark, 2007; Tofiloski 2 https://gitlab.irit.fr/melodi/ andiamo/discoursesegmentation/discut 23 Corpus Lang Train # Doc. Dev Sent seg # Sents. Train # Units Train manual manual manual 44,563 25,080 2,049 23,850 6,841 1,034 manual manual manual manual stanza manual manual stanza manual manual manual 6,672 3,600 1,773 991 1713 1,202 1,595 18932 1,577 304 344 17,646 5,012 2,449 1,713 4607 1,679 3,916 34682 2,474 473 473 manual manual 7,689 880 8,843 2,411 Test Connectives — PDTB eng.pdtb.pdtb tur.pdtb.tdb zho.pdtb.cdtb en tr zh 1,992 159 125 79 19 21 91 19 18 EDUs — RST eng.rst.rstdt"
2021.disrpt-1.3,J15-3002,0,0.0499818,"Missing"
2021.disrpt-1.3,2020.emnlp-main.380,0,0.0120819,"ing and Treebanking (DISRPT 2021), pages 22–32 November 11, 2021. ©2021 Association for Computational Linguistics sub-tasks for segmentation and connective identification: either sentence-level segmentation, with sentence boundaries given (when annotated, or provided with a sentence splitter otherwise), also informed by syntactic parsing of sentences, or without any of that information. With the exception of systems presented at DISRPT 2019 (Bourgonje and Schäfer, 2019; Yu et al., 2019; Muller et al., 2019), existing work on segmentation always assumed gold sentences, e.g. (Wang et al., 2018; Lukasik et al., 2020). One interesting aspect of such a task is the availability of comparable data in different languages. This has been leveraged in the past for segmentation (Braud et al., 2017b), but not in the past 2019 campaign, where the best system relied on fine-tuning a contextual language model for each language separately, albeit using the same multilingual embedding model (Muller et al., 2019). Here we propose to build on the previous DISRPT best system and exploit the availability of multiple corpora for the same language, or the same family of languages (romance, germanic) to augment training of ded"
2021.disrpt-1.3,stede-neumann-2014-potsdam,0,0.268904,"ures. This is followed in the majority of corpora (11). • Segmented Discrouse Representation Theory (Asher and Lascarides, 2003), which allows for embedded segments, and were linearized here for homogeneity of the task: a segment embedded in another one was re-annotated as forming 3 three segments. This is the case of 24 in the following publications: English RSTDT (Carlson et al., 2001), PDTB (Prasad et al., 2008), SDRT-STAC (Asher et al., 2016) and GUM (Zeldes, 2016), Spanish RST (2)(da Cunha et al., 2011; Cao et al., 2018) Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018), German RST (Stede and Neumann, 2014), French SDRT-Annodis (Afantenos et al., 2012), Basque RST (Iruskieta et al., 2013), Portuguese RST (Cardoso et al., 2011), Russian RST (Pisarevskaya et al., 2017), Turkish PDTB (Zeyrek et al., 2013) Dutch RST (Redeker et al., 2012) and Persian RST (Shahmohammadi et al., 2021). 4 They showed that XLM didn’t help, but the extra layer of LSTM could. Changes to other hyperparameters didn’t improve these preliminary results so we kept them as in the original model. Details of the parameters can be found in the declarative config file of the model, also added as supplementary material. Input Approa"
2021.disrpt-1.3,2021.eacl-main.314,0,0.0426458,"ntence and document) used a sequential tagging model fine-tuned on contextual embeddings. Multi-lingual discourse parsing is also becoming more popular, see for instance (Braud et al., 2017a; Chen et al., 2020), in which it is seen as a form of multi-task learning problem, but this was not applied to discourse segmentation. In other NLP subfields, leveraging availability of corpora in different languages for the same tasks is an active area of research, with different strategies for combining tasks and languages, using meta-learning and complex sampling strategies (Nooralahzadeh et al., 2020; Tarunesh et al., 2021). A simpler approach that inspired us here, due to (Dehouck and Denis, 2019), is to use the relations between close languages to guide the training process on a task: a generic model is trained on groups of languages, further refined with models by subgroups and finally fine-tuned on individual languages. 3 Data The 2021 shared task provides 16 corpora annotated either with discourse boundaries (13) or discourse connectives in the case of PDTB corpora (3), with the RST Farsi corpus as a surprise dataset. This covers 11 different languages, mostly indoeuropean languages, and with a majority of"
2021.disrpt-1.3,J00-3005,0,0.328832,"e), 3 germanic (English, German, Dutch), Russian, the only non indo-european being Turkish, Basque, and Mandarin. Some of the datasets depend on licences for the underlying text corpus, and are not freely available. We had licences for all of them except the Mandarin corpus (zho.pdtb.cdtb), provided by the organizers for the evaluation of the task. Except Farsi, all the datasets were present in the DISRPT 2019 shared task, but the russian Related work Discourse segmentation appeared as an NLP task with the creation of the first annotated RST documents in English, and was primarily rule-based (Marcu, 2000). Since then the literature on discourse parsing generally assumed that elementary discourse units (discourse segments) were given, with only a handful of exceptions (Soricut and Marcu, 2003; Fisher and Roark, 2007; Tofiloski 2 https://gitlab.irit.fr/melodi/ andiamo/discoursesegmentation/discut 23 Corpus Lang Train # Doc. Dev Sent seg # Sents. Train # Units Train manual manual manual 44,563 25,080 2,049 23,850 6,841 1,034 manual manual manual manual stanza manual manual stanza manual manual manual 6,672 3,600 1,773 991 1713 1,202 1,595 18932 1,577 304 344 17,646 5,012 2,449 1,713 4607 1,679 3,"
2021.disrpt-1.3,W19-2715,1,0.78635,"art of the organization of the shared task. 22 Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021), pages 22–32 November 11, 2021. ©2021 Association for Computational Linguistics sub-tasks for segmentation and connective identification: either sentence-level segmentation, with sentence boundaries given (when annotated, or provided with a sentence splitter otherwise), also informed by syntactic parsing of sentences, or without any of that information. With the exception of systems presented at DISRPT 2019 (Bourgonje and Schäfer, 2019; Yu et al., 2019; Muller et al., 2019), existing work on segmentation always assumed gold sentences, e.g. (Wang et al., 2018; Lukasik et al., 2020). One interesting aspect of such a task is the availability of comparable data in different languages. This has been leveraged in the past for segmentation (Braud et al., 2017b), but not in the past 2019 campaign, where the best system relied on fine-tuning a contextual language model for each language separately, albeit using the same multilingual embedding model (Muller et al., 2019). Here we propose to build on the previous DISRPT best system and exploit the availability of multiple"
2021.disrpt-1.3,P09-2020,0,0.114922,"Missing"
2021.disrpt-1.3,D18-1116,0,0.0294615,"Missing"
2021.disrpt-1.3,2020.emnlp-main.368,0,0.0237536,"9) at both granularities (sentence and document) used a sequential tagging model fine-tuned on contextual embeddings. Multi-lingual discourse parsing is also becoming more popular, see for instance (Braud et al., 2017a; Chen et al., 2020), in which it is seen as a form of multi-task learning problem, but this was not applied to discourse segmentation. In other NLP subfields, leveraging availability of corpora in different languages for the same tasks is an active area of research, with different strategies for combining tasks and languages, using meta-learning and complex sampling strategies (Nooralahzadeh et al., 2020; Tarunesh et al., 2021). A simpler approach that inspired us here, due to (Dehouck and Denis, 2019), is to use the relations between close languages to guide the training process on a task: a generic model is trained on groups of languages, further refined with models by subgroups and finally fine-tuned on individual languages. 3 Data The 2021 shared task provides 16 corpora annotated either with discourse boundaries (13) or discourse connectives in the case of PDTB corpora (3), with the RST Farsi corpus as a surprise dataset. This covers 11 different languages, mostly indoeuropean languages,"
2021.disrpt-1.3,2020.acl-main.451,0,0.0752823,"Missing"
2021.disrpt-1.3,W19-2717,0,0.332844,"Missing"
2021.disrpt-1.3,prasad-etal-2008-penn,0,0.150042,"separation of a text or conversation in elementary units that make up the arguments of the rhetorical structure of a text, has long been a neglected step in discourse analysis, considered easy and generally assumed as given in discourse parsing studies, where the focus is to predict the rhetorical structure of a document, a labelled relational structure, with properties dependent on the theorerical framework considered, Rhetorical Structure Theory (RST, Mann and Thompson, 1988), Segmented Discourse Representation Theory (SDRT, Asher and Lascarides, 2003), or the Penn Discourse Treebank (PDTB, Prasad et al., 2008). However this important step has generated more interest recently, as illustrated by the 2019 shared task at the Discourse Relation Parsing and Treebanking (DISRPT) workshop (Zeldes et al., 2019). • [Three seats currently are vacant] [and three others are likely to be filled within a few years] (...) • [But] [in the end] his resignation as Chancellor of the Exchequer may be a good thing (...) In the first case, tokens ""Three"" and ""and"" would be marked as segment beginnings. In the second case, where the target connectives are ""but"" and ""in the end"", beginning tokens ""but"" and ""in"" would be ma"
2021.disrpt-1.3,2020.acl-demos.14,0,0.0213888,"the final prediction. The model is initialized with the multilingual BERT model, then fine-tune on all corpora separately as sequence tagging tasks. The original implementation used the AllenNLP library (Gardner et al., 2017), and so does our implementation. Since BERT has a limitation on the number of word pieces it can take as input, a preprocessing step must be taken for document-level segmentation. In (Muller et al., 2019), the core-nlp library was used to predict sentence boundaries, and use this information, while we used the more recent Stanza library by the same team for that purpose (Qi et al., 2020). We explored potential improvements for that architecture, swapping the multi-lingual pretrained language model XLM (Conneau and Lample, 2019), or adding another layer to the BiLSTM stage. The final configuration was chosen based on preliminary experiments on some of the datasets, evaluated on their respective development sets. doc Corpus P R F1 deu.rst.pcc eng.rst.gum eng.rst.rstdt eng.sdrt.stac eus.rst.ert fas.rst.prstc fra.sdrt.annodis nld.rst.nldt por.rst.cstn rus.rst.rrt spa.rst.rststb spa.rst.sctb zho.rst.sctb 92.58 94.06 96.35 94.19 87.23 91.18 87.13 95.52 90.63 86.04 91.80 85.57 93.02"
2021.disrpt-1.3,W19-2713,0,0.846399,"generally assumed as given in discourse parsing studies, where the focus is to predict the rhetorical structure of a document, a labelled relational structure, with properties dependent on the theorerical framework considered, Rhetorical Structure Theory (RST, Mann and Thompson, 1988), Segmented Discourse Representation Theory (SDRT, Asher and Lascarides, 2003), or the Penn Discourse Treebank (PDTB, Prasad et al., 2008). However this important step has generated more interest recently, as illustrated by the 2019 shared task at the Discourse Relation Parsing and Treebanking (DISRPT) workshop (Zeldes et al., 2019). • [Three seats currently are vacant] [and three others are likely to be filled within a few years] (...) • [But] [in the end] his resignation as Chancellor of the Exchequer may be a good thing (...) In the first case, tokens ""Three"" and ""and"" would be marked as segment beginnings. In the second case, where the target connectives are ""but"" and ""in the end"", beginning tokens ""but"" and ""in"" would be marked ""B"" (begin) and ""the"", ""end"", would be marked as ""I"" (inside). In both examples, other tokens would be marked ""out"". Since sentences are almost always discourse units in existing frameworks,"
2021.disrpt-1.3,redeker-etal-2012-multi,0,0.789678,"Missing"
2021.emnlp-main.104,D19-1234,0,0.01557,":1 discourse corpora standards, provides a large annotated dataset (1k conversations, 200k dialogue acts, 1.3M tokens) for English. Evaluation measures in speech-oriented work focus either on words (belonging to the right segment or not) or exact segment boundaries of varying types that differ from conventions for written-text segmentation, making direct comparisons difficult. Our approach is based on the data-programming paradigm (Ratner et al., 2016), a weak supervision framework that has been applied mainly to information extraction problems in NLP, but also recently to discourse analysis (Badene et al., 2019), specifically for discourse structure prediction. We are unaware of similar work on discourse segmentation or on multi-modal text/speech classification problems. The novelty of the data-programming approach is that it requires only a fraction of the data to be manually annotated, for designing heuristics and for evaluation, and is arguably easier to adapt to new data. 3 The data-programming approach The data-programming approach (Ratner et al., 2016, 2020) can be decomposed into three steps: (1) LABELING FUNCTIONS: by studying only a small (possibly annotated) development set, experts design"
2021.emnlp-main.104,2020.autosimtrans-1.5,0,0.0352669,"Missing"
2021.emnlp-main.104,P07-1062,0,0.161653,"Missing"
2021.emnlp-main.104,P03-1071,0,0.300833,"chat, user/system interactions, etc. To address data scarcity, we introduce a method to transfer the knowledge of a segmenter trained on well-prepared, written text to an oral context, with specific supervision given by a latent model bootstrapped by manually defined heuristic rules, as in (Ratner et al., 2020). The rules exploit lexical, syntactic and acoustic features to help identify segment boundaries. This should provide a more general framework than oral segmentation efforts dedicated to specific tasks like MT (IranzoSánchez et al., 2020) or than lexically-based unsupervised approaches (Galley et al., 2003). The contributions of this paper are thus:1 discourse corpora standards, provides a large annotated dataset (1k conversations, 200k dialogue acts, 1.3M tokens) for English. Evaluation measures in speech-oriented work focus either on words (belonging to the right segment or not) or exact segment boundaries of varying types that differ from conventions for written-text segmentation, making direct comparisons difficult. Our approach is based on the data-programming paradigm (Ratner et al., 2016), a weak supervision framework that has been applied mainly to information extraction problems in NLP,"
2021.emnlp-main.104,2020.emnlp-main.206,0,0.0713711,"Missing"
2021.emnlp-main.104,W10-4327,0,0.0718532,"Missing"
2021.emnlp-main.104,2020.emnlp-main.380,0,0.0411717,"Missing"
2021.emnlp-main.104,J00-3005,0,0.558905,"Missing"
2021.emnlp-main.104,W19-2715,1,0.732627,"especially for languages other than En1381 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1381–1392 c November 7–11, 2021. 2021 Association for Computational Linguistics 2018; Lukasik et al., 2020), though still at the sentence level. The shared task at the Disrpt 2019 workshop introduced a more general evaluation framework, with multilingual data and segmentation at the level of full texts, with a subtask that did not assume sentence boundaries (Zeldes et al., 2019). The best system, which also used a sequential model over contextual embeddings (Muller et al., 2019), showed the best performance both with and without sentence boundary information. In work on oral conversation, segmentation has often been approached not as a discourse problem, but as a problem of recognizing “sentences” in order to predict punctuation marks with ngram models and audio features, either to enrich auto• a method to transfer a supervised model based matic speech transcripts (Batista et al., 2012) or on prepared, written text to spontaneous, multo improve MT (Fügen et al., 2007; Zhang and tiparty oral conversation using multimodal feaZhang, 2020; Wang et al., 2019). Arguably th"
2021.emnlp-main.104,prasad-etal-2008-penn,0,0.0236618,"Missing"
2021.emnlp-main.104,J15-3002,0,0.0453493,"Missing"
2021.emnlp-main.104,2020.acl-main.551,0,0.0365709,"Missing"
2021.emnlp-main.104,2020.acl-main.139,0,0.0529767,"Missing"
2021.emnlp-main.104,N03-1030,0,0.492162,"Missing"
2021.emnlp-main.104,H05-1033,0,0.243793,"Missing"
2021.emnlp-main.104,P09-2020,0,0.0821078,"Missing"
2021.emnlp-main.104,W19-6601,0,0.0529487,"Missing"
2021.emnlp-main.104,D18-1116,0,0.0292259,"Missing"
2021.emnlp-main.104,2020.acl-main.451,0,0.0682628,"Missing"
2021.emnlp-main.104,W19-2713,0,0.0681496,"Missing"
2021.emnlp-main.104,2020.autosimtrans-1.1,0,0.0450596,"Missing"
2021.emnlp-main.104,W18-5021,0,0.0651168,"Missing"
2021.jeptalnrecital-taln.6,Q17-1010,0,0.0715813,"Missing"
2021.jeptalnrecital-taln.6,D15-1075,0,0.068,"Missing"
2021.jeptalnrecital-taln.6,N19-1300,0,0.020809,"Missing"
2021.jeptalnrecital-taln.6,D17-1070,0,0.0259639,"Missing"
2021.jeptalnrecital-taln.6,D19-1295,0,0.0337629,"Missing"
2021.jeptalnrecital-taln.6,N19-1423,0,0.0166782,"Missing"
2021.jeptalnrecital-taln.6,D19-1111,0,0.0395143,"Missing"
2021.jeptalnrecital-taln.6,P15-1144,0,0.0483389,"Missing"
2021.jeptalnrecital-taln.6,N16-1138,0,0.0513579,"Missing"
2021.jeptalnrecital-taln.6,P14-1046,0,0.0343072,"Missing"
2021.jeptalnrecital-taln.6,N18-2017,0,0.0238356,"Missing"
2021.jeptalnrecital-taln.6,D19-1211,0,0.031247,"Missing"
2021.jeptalnrecital-taln.6,D19-6115,0,0.024193,"Missing"
2021.jeptalnrecital-taln.6,2020.emnlp-main.732,0,0.0330095,"Missing"
2021.jeptalnrecital-taln.6,D19-1661,0,0.0339145,"Missing"
2021.jeptalnrecital-taln.6,2020.acl-main.698,0,0.04296,"Missing"
2021.jeptalnrecital-taln.6,W17-1910,0,0.0317526,"Missing"
2021.jeptalnrecital-taln.6,N16-1082,0,0.0361615,"Missing"
2021.jeptalnrecital-taln.6,P19-1441,0,0.0313275,"Missing"
2021.jeptalnrecital-taln.6,P11-1015,0,0.0624456,"Missing"
2021.jeptalnrecital-taln.6,P19-1334,0,0.02103,"Missing"
2021.jeptalnrecital-taln.6,C12-1118,0,0.0978848,"Missing"
2021.jeptalnrecital-taln.6,C18-1198,0,0.0285797,"Missing"
2021.jeptalnrecital-taln.6,W16-3604,0,0.0222533,"Missing"
2021.jeptalnrecital-taln.6,P19-1570,0,0.039629,"Missing"
2021.jeptalnrecital-taln.6,D17-1041,0,0.0486308,"Missing"
2021.jeptalnrecital-taln.6,D14-1162,0,0.0841688,"Missing"
2021.jeptalnrecital-taln.6,S18-2023,0,0.0276266,"Missing"
2021.jeptalnrecital-taln.6,prasad-etal-2008-penn,0,0.040876,"Missing"
2021.jeptalnrecital-taln.6,N16-3020,0,0.0955784,"Missing"
2021.jeptalnrecital-taln.6,D13-1170,0,0.00580443,"Missing"
2021.jeptalnrecital-taln.6,W19-4807,0,0.0218058,"Missing"
2021.jeptalnrecital-taln.6,W18-5422,0,0.0351278,"Missing"
2021.jeptalnrecital-taln.6,N18-1101,0,0.0202943,"Missing"
afantenos-etal-2010-learning,W01-1605,0,\N,Missing
afantenos-etal-2010-learning,J96-1002,0,\N,Missing
afantenos-etal-2012-empirical,C10-1001,1,\N,Missing
afantenos-etal-2012-empirical,J00-3005,0,\N,Missing
afantenos-etal-2012-empirical,J96-1002,0,\N,Missing
afantenos-etal-2012-empirical,J95-2003,0,\N,Missing
afantenos-etal-2012-empirical,J05-2005,0,\N,Missing
afantenos-etal-2012-empirical,prasad-etal-2008-penn,0,\N,Missing
afantenos-etal-2012-empirical,J86-3001,0,\N,Missing
afantenos-etal-2012-empirical,W04-2322,0,\N,Missing
afantenos-etal-2012-empirical,2009.jeptalnrecital-court.23,0,\N,Missing
C04-1008,W01-1313,0,0.129268,"has started to generate some interest in computational linguistics (Harper et al., 2001), as it is potentially an important component in information extraction or question-answer systems. A few tasks can be distinguished in that respect: Xavier Tannier IRIT, Université Paul Sabatier Toulouse, France tannier@emse.fr as beliefs, or reported speech have an unclear status in that respect. The third task adds another level of complexity: a lot of events described in a text do not have an explicit temporal stamp, and it is not always possible to determine one, even when taking context into account (Filatova and Hovy, 2001). This leads to an approach more suited to the level of underspecification found in texts: annotating relations between events in a symbolic way (e.g. that an event e1 is before another one e2 ). This is the path chosen by (Katz and Arosio, 2001; Setzer, 2001) with human annotators. This, in turn, raises new problems. First, what are the relations best suited to that task, among the many propositions (linguistic or logical) one can find for expressing temporal location ? Then, how can an annotation be evaluated, between annotators, or between a human annotator and an automated system ? Such an"
C04-1008,E95-1035,0,0.0726167,"ts in a symbolic way (e.g. that an event e1 is before another one e2 ). This is the path chosen by (Katz and Arosio, 2001; Setzer, 2001) with human annotators. This, in turn, raises new problems. First, what are the relations best suited to that task, among the many propositions (linguistic or logical) one can find for expressing temporal location ? Then, how can an annotation be evaluated, between annotators, or between a human annotator and an automated system ? Such annotations cannot be easy to determine automatically anyway, and must use some level of discourse modeling (cf. the work of (Grover et al., 1995)). • detecting event descriptions We want to show here the feasibility of such an effort, and we propose a way of evaluating the success or failure of the task. The next section will precise why evaluating this particular task is not a trivial question. Section 3 will explain the method used to extract temporal relations, using also a form of symbolic inference on available temporal information (section 4). Then section 5 discusses how we propose to evaluate the success of the task, before presenting our results (section 6). • finding the date of events described 2 Evaluating annotations • det"
C04-1008,P93-1010,0,0.169329,"• date computation to precise temporal locations of events associated with explicit, yet imprecise, temporal information, such as dates relative to the time of the text (e.g. last Monday). • for each event associated to a temporal adjunct, a temporal relation is established (with a date when possible). • a set of discourse rules is used to establish possible relations between two events appearing consecutively in the text, according to the tenses of the verbs introducing the events. These rules for French are similar to rules for English proposed in (Grover et al., 1995; Song and Cohen, 1991; Kameyama et al., 1993), but 1 We have defined 89 rules, divided in 29 levels. are expressed with Allen relations instead of a set of ad hoc relations (see Table 1 for a subset of the rules). These rules are only applied when no temporal marker indicates a specific relation between the two events. • the last step consists in computing a fixed point on the graph of relations between events recognized in the text, and dates. We used a classical path-consistency algorithm (Allen, 1984). More explanation is given section 4. Allen relations are illustrated Figure 2. In the following (and Table 1) they will be abbreviated"
C04-1008,W01-1315,0,0.409768,"nier IRIT, Université Paul Sabatier Toulouse, France tannier@emse.fr as beliefs, or reported speech have an unclear status in that respect. The third task adds another level of complexity: a lot of events described in a text do not have an explicit temporal stamp, and it is not always possible to determine one, even when taking context into account (Filatova and Hovy, 2001). This leads to an approach more suited to the level of underspecification found in texts: annotating relations between events in a symbolic way (e.g. that an event e1 is before another one e2 ). This is the path chosen by (Katz and Arosio, 2001; Setzer, 2001) with human annotators. This, in turn, raises new problems. First, what are the relations best suited to that task, among the many propositions (linguistic or logical) one can find for expressing temporal location ? Then, how can an annotation be evaluated, between annotators, or between a human annotator and an automated system ? Such annotations cannot be easy to determine automatically anyway, and must use some level of discourse modeling (cf. the work of (Grover et al., 1995)). • detecting event descriptions We want to show here the feasibility of such an effort, and we prop"
C04-1008,P00-1010,0,0.108467,"ults. It means we have made decisions that were not cautious enough, for reasons we still have to analyze. One potential reason is that relations offered to humans are maybe too vague in the wrong places: a lot of information in a text can be asserted to be &quot;strictly before&quot; something else (based on dates for instance), while human annotators can only say that events are &quot;before or meets&quot; some other event; each time this is the case, coherence is only 0.5. It is important to note that there are few points of comparison on this problem. To the best of our knowledge, only (Li et al., 2001) and (Mani and Wilson, 2000) mention having tried this kind of annotation, as a side job for their temporal expressions mark-up systems. The former considers only relations between events within a sentence, and the latter did not evaluate their method. Finally, it is worth remembering that human annotation itself is a difficult task, with potentially a lot of disagreement between annotators. For now, our texts have been annotated by the two authors, with an a posteriori resolution of conflicts. We therefore have no measure of inter-annotator agreement which could serve as an upper bound of the performance of the system,"
C04-1008,W01-1309,0,0.0120867,"hod used to extract temporal relations, using also a form of symbolic inference on available temporal information (section 4). Then section 5 discusses how we propose to evaluate the success of the task, before presenting our results (section 6). • finding the date of events described 2 Evaluating annotations • detecting dates and temporal markers • figuring out the temporal relations between events in a text The first task is not too difficult when looking for dates, e.g. using regular expressions (Wilson et al., 2001), but requires some syntactic analysis in a larger framework (Vazov, 2001; Shilder and Habel, 2001). The second one raises more difficult, ontological questions; what counts as an event is not uncontroversial (Setzer, 2001): attitude reports, such What we want to annotate is something close to the temporal model built by a human reader of a text; as such, it may involve some form of reasoning, based on various cues (lexical or discursive), and may be expressed in several ways. As was noticed by (Setzer, 2001), it is difficult to reach a good agreement between human annotators, as they can express relations between events in different, yet equivalent, ways. For instance, they can say that an"
C04-1008,W01-1312,0,0.0158486,"e why evaluating this particular task is not a trivial question. Section 3 will explain the method used to extract temporal relations, using also a form of symbolic inference on available temporal information (section 4). Then section 5 discusses how we propose to evaluate the success of the task, before presenting our results (section 6). • finding the date of events described 2 Evaluating annotations • detecting dates and temporal markers • figuring out the temporal relations between events in a text The first task is not too difficult when looking for dates, e.g. using regular expressions (Wilson et al., 2001), but requires some syntactic analysis in a larger framework (Vazov, 2001; Shilder and Habel, 2001). The second one raises more difficult, ontological questions; what counts as an event is not uncontroversial (Setzer, 2001): attitude reports, such What we want to annotate is something close to the temporal model built by a human reader of a text; as such, it may involve some form of reasoning, based on various cues (lexical or discursive), and may be expressed in several ways. As was noticed by (Setzer, 2001), it is difficult to reach a good agreement between human annotators, as they can expr"
C04-1008,W01-1305,0,\N,Missing
C04-1008,W01-1314,0,\N,Missing
C04-1173,W99-0901,0,0.0762723,"Missing"
C04-1173,C96-1005,0,0.135258,"Missing"
C04-1173,J98-1001,0,0.0631318,"Missing"
C04-1173,C90-2067,0,0.182568,"thin those entries. The basic idea is to consider the dictionary as an undirected graph whose nodes are noun entries, and an edge exists between two nodes whenever one of them occur in the definition of the other. More precisely, the graph of the dictionary encodes two types of lexicographical informations: (1) the definitions of the entries sub-senses and (2) the structure of the entries that is the hierarchical organisation of their subsenses. The graph then includes two types of nodes: w-nodes used for the words that occur 2 With the exceptions of the methods of (Kozima and Furugori, 1993; Ide and Véronis, 1990), both based on models of activation of lexical relations, but who present no quantified results. in the definitions and ∆-nodes used for the definitions of the sub-senses of the entries. The graph is created in three phases: 1. For each dictionary entry, there is a ∆node for the entry as a whole and there is one ∆-node for each of the sub-senses of the entry. Then an edge is added between each ∆-node and the ∆-nodes which represent the sub-senses of the next lower level. In other words, the graph includes a tree of ∆-nodes which encodes the hierarchical structure of each entry. 2. A w-node is"
C04-1173,E93-1028,0,0.269738,"nd verb lemmas occurring within those entries. The basic idea is to consider the dictionary as an undirected graph whose nodes are noun entries, and an edge exists between two nodes whenever one of them occur in the definition of the other. More precisely, the graph of the dictionary encodes two types of lexicographical informations: (1) the definitions of the entries sub-senses and (2) the structure of the entries that is the hierarchical organisation of their subsenses. The graph then includes two types of nodes: w-nodes used for the words that occur 2 With the exceptions of the methods of (Kozima and Furugori, 1993; Ide and Véronis, 1990), both based on models of activation of lexical relations, but who present no quantified results. in the definitions and ∆-nodes used for the definitions of the sub-senses of the entries. The graph is created in three phases: 1. For each dictionary entry, there is a ∆node for the entry as a whole and there is one ∆-node for each of the sub-senses of the entry. Then an edge is added between each ∆-node and the ∆-nodes which represent the sub-senses of the next lower level. In other words, the graph includes a tree of ∆-nodes which encodes the hierarchical structure of ea"
C10-1029,J96-1002,0,0.00970092,"Missing"
C10-1029,W02-2018,0,0.027934,"Missing"
C10-1029,P06-1095,0,0.290731,"ment is an essential part of natural language understanding. Success in this task has important implications for other NLP applications, such as text summarization, information extraction, and question answering. Interest for this problem within the NLP community is not new (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993), but has been recently revived by the creation of the TimeBank corpus (Pustejovsky et al., 2003), and the organization of the TempEval-07 campaign (Verhagen et al., 2007). These have seen the development of machine learning inspired systems (Bramsen et al., 2006; Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). Learning the temporal stucture from texts is a difficult problem because there are numerous information sources at play (in particular, semantic and pragmatic ones) (Lascarides and Asher, 1993). An additional difficulty comes from the fact that temporal relations have logical properties that restrict the consistent graphs that can be built for a set of temporal entities (for instance the transitivity of inclusion and temporal precedence). Previous work do not attempt to directly predict globally coherent temporal graphs, but instead focu"
C10-1029,J88-2005,0,0.23845,"n, but more useful when global consistency is important. Overall, the Bruce algebra is shown to give the best compromise between learnability and expressive power. 1 Introduction Being able to recover the temporal relations (e.g., precedence, inclusion) that hold between events and other time-denoting expressions in a document is an essential part of natural language understanding. Success in this task has important implications for other NLP applications, such as text summarization, information extraction, and question answering. Interest for this problem within the NLP community is not new (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993), but has been recently revived by the creation of the TimeBank corpus (Pustejovsky et al., 2003), and the organization of the TempEval-07 campaign (Verhagen et al., 2007). These have seen the development of machine learning inspired systems (Bramsen et al., 2006; Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). Learning the temporal stucture from texts is a difficult problem because there are numerous information sources at play (in particular, semantic and pragmatic ones) (Lascarides and Asher, 1993). An additional difficult"
C10-1029,W06-1623,0,0.367856,"expressions in a document is an essential part of natural language understanding. Success in this task has important implications for other NLP applications, such as text summarization, information extraction, and question answering. Interest for this problem within the NLP community is not new (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993), but has been recently revived by the creation of the TimeBank corpus (Pustejovsky et al., 2003), and the organization of the TempEval-07 campaign (Verhagen et al., 2007). These have seen the development of machine learning inspired systems (Bramsen et al., 2006; Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). Learning the temporal stucture from texts is a difficult problem because there are numerous information sources at play (in particular, semantic and pragmatic ones) (Lascarides and Asher, 1993). An additional difficulty comes from the fact that temporal relations have logical properties that restrict the consistent graphs that can be built for a set of temporal entities (for instance the transitivity of inclusion and temporal precedence). Previous work do not attempt to directly predict globally coherent temporal graph"
C10-1029,D08-1073,0,0.218552,"Missing"
C10-1029,P07-2044,0,0.0135299,"n temporal ordering (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993) concentrated on studying the knowledge sources at play (such as tense, aspect, lexical semantics, rhetorical relations). The development of annotated resources like the TimeBank corpus (Pustejovsky et al., 2003) has triggered the development of machine learning systems (Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). More recent work uses automatic classification methods, based on the TimeBank and Acquaint corpus, either as is, with inferential enrichment for training (Mani et al., 2006; Chambers et al., 2007), or supplied with the corrections of (Bethard et al., 2007), or are restricted to selected contexts, such as intra-sentential event relations (Li et al., 2004; Lapata and Lascarides, 2006). All of these assume that event pairs are preselected, so the task is only to determine what is the most likely relation between them. The best scores are obtained with the added assumption that the event-event pair can be pre-ordered (thus reducing the number of possible labels by 2). More recently, (Bramsen et al., 2006) and subsequently (Chambers and Jurafsky, 2008) propose to use an Integer Linear Progr"
C10-1029,W01-1313,0,0.0431272,"QUAINT has 73 documents (and around 40, 000 words). Both corpora are annotated using the TimeML scheme for tagging eventualities (events and states), dates/times, and their temporal relations. Eventualities can be denoted by verbs, nouns, and some specific constructions. The temporal relations (i.e., the so-called TLINKS) encode topological information between the time intervals of occurring eventualities. TimeML distinguishes three types of TLINKS: event-event, event-time, and time-time, giving rise to different subtasks. In this paper, we will focus on predicting event-event relations (see (Filatova and Hovy, 2001; Boguraev and Ando, 2005) for work on the other tasks). The set of temporal relations used in TLINKS mirrors the 13 Allen relations (see next section), and includes the following six relations: before, begins, ends, ibefore, includes, simultaneous and their inverses. The combined OTC corpus comprises a total of 6, 139 annotated event-event TLINKS. We also make use of the additional TLINKS independently provided by (Bethard et al., 2007) for 129 of the 186 Timebank documents. 3 3.1 Task presentation and evaluation An example We illustrate the task of event ordering using a small fabricated, si"
C10-1029,C08-1108,0,0.0612492,"l part of natural language understanding. Success in this task has important implications for other NLP applications, such as text summarization, information extraction, and question answering. Interest for this problem within the NLP community is not new (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993), but has been recently revived by the creation of the TimeBank corpus (Pustejovsky et al., 2003), and the organization of the TempEval-07 campaign (Verhagen et al., 2007). These have seen the development of machine learning inspired systems (Bramsen et al., 2006; Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). Learning the temporal stucture from texts is a difficult problem because there are numerous information sources at play (in particular, semantic and pragmatic ones) (Lascarides and Asher, 1993). An additional difficulty comes from the fact that temporal relations have logical properties that restrict the consistent graphs that can be built for a set of temporal entities (for instance the transitivity of inclusion and temporal precedence). Previous work do not attempt to directly predict globally coherent temporal graphs, but instead focus on the the simpler prob"
C10-1029,S07-1014,0,0.149511,"poral relations (e.g., precedence, inclusion) that hold between events and other time-denoting expressions in a document is an essential part of natural language understanding. Success in this task has important implications for other NLP applications, such as text summarization, information extraction, and question answering. Interest for this problem within the NLP community is not new (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993), but has been recently revived by the creation of the TimeBank corpus (Pustejovsky et al., 2003), and the organization of the TempEval-07 campaign (Verhagen et al., 2007). These have seen the development of machine learning inspired systems (Bramsen et al., 2006; Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). Learning the temporal stucture from texts is a difficult problem because there are numerous information sources at play (in particular, semantic and pragmatic ones) (Lascarides and Asher, 1993). An additional difficulty comes from the fact that temporal relations have logical properties that restrict the consistent graphs that can be built for a set of temporal entities (for instance the transitivity of inclusion and temporal pr"
C10-1029,J88-2006,0,0.202669,"when global consistency is important. Overall, the Bruce algebra is shown to give the best compromise between learnability and expressive power. 1 Introduction Being able to recover the temporal relations (e.g., precedence, inclusion) that hold between events and other time-denoting expressions in a document is an essential part of natural language understanding. Success in this task has important implications for other NLP applications, such as text summarization, information extraction, and question answering. Interest for this problem within the NLP community is not new (Passonneau, 1988; Webber, 1988; Lascarides and Asher, 1993), but has been recently revived by the creation of the TimeBank corpus (Pustejovsky et al., 2003), and the organization of the TempEval-07 campaign (Verhagen et al., 2007). These have seen the development of machine learning inspired systems (Bramsen et al., 2006; Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). Learning the temporal stucture from texts is a difficult problem because there are numerous information sources at play (in particular, semantic and pragmatic ones) (Lascarides and Asher, 1993). An additional difficulty comes from t"
C10-1029,P04-1074,0,0.0323733,"l semantics, rhetorical relations). The development of annotated resources like the TimeBank corpus (Pustejovsky et al., 2003) has triggered the development of machine learning systems (Mani et al., 2006; Tatu and Srikanth, 2008; Chambers and Jurafsky, 2008). More recent work uses automatic classification methods, based on the TimeBank and Acquaint corpus, either as is, with inferential enrichment for training (Mani et al., 2006; Chambers et al., 2007), or supplied with the corrections of (Bethard et al., 2007), or are restricted to selected contexts, such as intra-sentential event relations (Li et al., 2004; Lapata and Lascarides, 2006). All of these assume that event pairs are preselected, so the task is only to determine what is the most likely relation between them. The best scores are obtained with the added assumption that the event-event pair can be pre-ordered (thus reducing the number of possible labels by 2). More recently, (Bramsen et al., 2006) and subsequently (Chambers and Jurafsky, 2008) propose to use an Integer Linear Programming solver to enforce the consistency of a network of constraints while maximizing the score of local classification decisions. But these are restricted to"
C12-1115,afantenos-etal-2012-empirical,1,0.465285,"Missing"
C12-1115,W05-0613,0,0.375219,"Missing"
C12-1115,P09-1075,0,0.242414,"Missing"
C12-1115,P12-1007,0,0.252293,"Missing"
C12-1115,W10-0906,0,0.0364718,"Missing"
C12-1115,D12-1083,0,0.122087,"Missing"
C12-1115,D09-1036,0,0.0316192,"Missing"
C12-1115,P11-1100,0,0.0274662,"Missing"
C12-1115,W10-4310,0,0.00580537,"Missing"
C12-1115,P02-1047,0,0.183084,"Missing"
C12-1115,H05-1066,0,0.324517,"Missing"
C12-1115,P09-1108,0,0.0310392,"Missing"
C12-1115,prasad-etal-2008-penn,0,0.0252028,"Missing"
C12-1115,W09-3813,0,0.534127,"Missing"
C12-1115,N03-1030,0,0.51331,"Missing"
C12-1115,N09-1064,0,0.116075,"Missing"
C12-1115,W06-1317,0,0.184087,"Missing"
C12-1115,D09-1018,0,\N,Missing
C12-1115,W01-1605,0,\N,Missing
C14-1206,P98-1013,0,0.604436,"Missing"
C14-1206,candito-etal-2010-statistical,0,0.0203741,"), alternation (ou/or), commentary (au fait/by the way), rephrasing (du moins/at least), and evidence (effectivement/indeed). We searched our syntactically parsed corpus for connectors. When a connector is found and its syntactic category verified, if it is close enough to the root of the sentence (at most one dependency link from the root), we look for an inter-sentential link. The first verb of our pair corresponds in this case 1. 2. 3. 4. Available as an SQLite database at https://dl.dropboxusercontent.com/u/78938139/v2r_db http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html or (Candito et al., 2010) Freely available at : https://gforge.inria.fr/frs/download.php/31052/lexconn.tar.gz. We illustrate each relation with examples of potentially ambiguous markers. 2185 to the last verb of the previous sentence in the case of connectors for narration, or to its main verb for all the other relations. We search for the second verb in the pair within a window of two dependency links after the connector. If the connector is not close enough to the root of the sentence, we look for a intra-sentential link. In this case, we look for the two verbs of the pair in the same sentence within a forward and b"
C14-1206,J96-2004,0,0.228852,"of context evaluation For out of context judgments, we adopted the following protocol : one of the authors chose for each relation 100 verbs with equivalent proportions of good and bad normalized PMI scores. Then the other 6. We simplified their measure by ignoring IDF (inverse document frequency) and the distance between the verbs, as neither measure applies to our task. 2187 three authors judged the validity of associating each of the 300 pairs with the corresponding relation, without any knowledge of the source of these pairs. We measured the inter-annotator agreements with Cohen’s Kappa (Carletta, 1996), which resulted in : 0.17 for cause, 0.42 for narration and 0.56 for contrast as mean values. If a 0.6 kappa serves a measure for a feasible semantic judgment task, out of context judgments appear very difficult, with only contrastive pairs as a relative exception. We decided to only consider judgments about contrast, after an adjudication phase, and we evaluated the measures presented in section 3 to see if they could discriminate between the two verb groups, those judged positively or negatively according to human annotations. A MannWhitney U statistical test showed all of our measures to b"
C14-1206,P08-1090,0,0.0560942,"Missing"
C14-1206,W04-3205,0,0.544983,"extraction. 1 Introduction Relational lexical resources, which describe semantic relations between lexical items, have traditionally focused on relations like synonymy or similarity in thesauri, perhaps including some hierarchical semantic relations like hyperonymy or hyponomy or part-whole relations as in the resource Wordnet (Felbaum, 1998). Some distributional thesauri contain more varied relations, see e.g. (Grefenstette, 1994), however these relations are not typed. The lexical semantics given by FrameNet (Baker et al., 1998) does include causal and temporal relations, as does Verbocean (Chklovski and Pantel, 2004), but coverage is limited and empirical validation of these resources is partial and still largely remains to be done. Lexical relations, in particular between verbs, are nevertheless crucial for understanding natural language and for many information processing tasks. They are needed for textual inference, in which one has to infer certain relations between eventualities (Hashimoto et al., 2009; Tremper and Frank, 2013), for information extraction tasks, like finding temporal relations between eventualities mentioned in a text (UzZaman et al., 2013), for automatic summarization (Liu et al., 2"
C14-1206,D11-1027,0,0.245803,"ence of explicit discourse markers (Sporleder and Lascarides, 2008). In this paper we report on our efforts to extract semantic relations essential to the analysis of discourse and its interpretation, in which links are made between units of text or rather their semantic representations as in (1) in virtue of semantic information about the two main verbs of those clauses. (1) The candidate demonstrated his expertise during the interview. The committee was completely convinced. We follow similar work on the extraction of causal, temporal, entailment and presuppositional relations from corpora (Do et al., 2011; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Tremper and Frank, 2013), though our goals and validation methods are different. While one of our goals is to use this information to improve performance in predicting discourse relations between clauses, we believe that such a lexical resource will have other uses in other tasks in which semantic information is needed. Discourse analysis is a difficult task. Rhetorical relations are frequently implicit and require for their identification inference using diverse sources of lexical and compositional semantic information. In the Penn Discou"
C14-1206,P12-1007,0,0.0257762,"scourse relations. An inductive logic programming approach is finally used exploiting the interaction between causal pairs and discourse relations in order to extract causal links. Those papers focus on specific relations with the exception of Chklovski and Pantel (2004) who do not present a systematic evaluation of their results. An important difference of our approach is also to consider predicates and their negation as separate entries. Finally, we mention the approaches which while focusing on the learning of discourse structures, nonetheless enrich their systems with lexical information. Feng and Hirst (2012) have used H ILDA (Hernault et al., 2010) adding more features. A specific family of features represents lexical similarity based on the hierarchical distance in V ERB N ET and W ORD N ET. In a similar fashion, Wellner et al. (2006) focus on intra-sentential discourse relations adding lexical information on the features based on measures proposed by Lin (1998) calculated on the British National Corpus. Those approaches use thus only information on lexical similarity without semantically typing this link. The impact of this information seems limited. As far as evaluation is concerned, our metho"
C14-1206,D09-1122,0,0.0381845,"Missing"
C14-1206,W12-4107,0,0.0135779,"hklovski and Pantel (2004), for example, rely on specific patterns constructed manually for each semantic relation between (similarity, strength, antonymy, enablement and temporal happens-before). They use the web as a corpus in order to estimate the PMI between a pattern and a pair of verbs (a precise measurement cannot be achieved over the web since the probability of a pattern is not precisely known over all the web). A threshold on the value of the PMI (manually fixed) permits thus to determine the pairs of verbs that are related to the relation denoted by the pattern. In the same spirit, Kozareva (2012) is using a weakly supervised approach for the extraction of pairs of verbs that are potentially implied in a cause-effect relation. Her method consists in using patterns applied to the web in order to extract pairs and generate new seeds. Do et al. (2011) focus on causal relations and take into account not only verbs but also event denoting nouns. According to this paper, an event is denoted by a predicate with a specific number of arguments and thus the association of the events is the sum of the association between predicates, between predicates and arguments and between arguments. Their as"
C14-1206,C02-1144,0,0.0913723,"se mutual information) and their variants, as well as some measures specific to the association of a causal relation between items (Do et al., 2011). We also experimented with a new measure specifically designed for our knowledge base. Measures of lexical association used in research on co-occurrences in distributional semantics pick out significant associations, taking into account the frequency of the related items. We examined over 10 measures ; we discuss the ones with the best results (see section 4). One simple measure, PMI, and its variants, normalized, local (Evert, 2005), discounted (Lin and Pantel, 2002), which are designed to reduce biases in the original measure, work well. The idea behind PMI is to estimate whether the probability of the co-occurrence of two items is greater than the a priori probability of the two items appearing independently. In distributional semantics, the measure is also used to estimate the significance of two items co-occurring with a particular grammatical dependency relation like the subject or object relation between an NP and a verb. This use of PMI measures over triples in distributional semantics fits perfectly with our task of measuring the significance of t"
C14-1206,P98-2127,0,0.0960764,"is also to consider predicates and their negation as separate entries. Finally, we mention the approaches which while focusing on the learning of discourse structures, nonetheless enrich their systems with lexical information. Feng and Hirst (2012) have used H ILDA (Hernault et al., 2010) adding more features. A specific family of features represents lexical similarity based on the hierarchical distance in V ERB N ET and W ORD N ET. In a similar fashion, Wellner et al. (2006) focus on intra-sentential discourse relations adding lexical information on the features based on measures proposed by Lin (1998) calculated on the British National Corpus. Those approaches use thus only information on lexical similarity without semantically typing this link. The impact of this information seems limited. As far as evaluation is concerned, our method is similar to that followed in Tremper and Frank (2013) for implication relations combining in and out of context evaluation for verbal associations. Their inter-annotator agreement is similar to ours (0.42-0.44 of Kappa) with very different choices : the anno2191 tators were supposed to discriminate verbal links between the different possible sub-cases. The"
C14-1206,P07-2047,0,0.0261752,"Missing"
C14-1206,P02-1047,0,0.170163,"k. The first group aims to alleviate the lack of annotated data for discourse parsing by using a weakly supervised approach, exploiting the presence of discourse connectors in a large non-annotated corpus. Each pair of elementary discourse units is automatically annotated with the discourse relation triggered by the presence of the connector (connectors are often filtered for non-discursive uses). Those connectors are afterwards eliminated from the corpus so that the model trained on this dataset will not be informed by the presence of those connectors. The pioneering article in this group is Marcu and Echihabi (2002). Such learning methods with such “artificial data” obtain low scores, barely above chance as shown in Sporleder and Lascarides (2008). Braud and Denis (2013) observe that the performance of a classifier for the prediction of implicit relations is much lower when using “artificial” data than on “natural” data (implicit relations annotated by a human being). They propose a method which exploits these two different kinds of datasets together in various mixtures and on the level of the prediction algorithm, obtaining thus a significant improvement on the Annodis corpus. Our approach is different"
C14-1206,N13-1024,0,0.0648113,"as : PMI = log( P (V1 , V2 , R) ) P (V1 ) × P (V2 ) × P (R) PMI _normalized = PMI −2 log(P (V1 , V2 , R)) Indeed, when we have a complete co-occurrence of the three items, we have : P (V1 ) = P (V2 ) = P (R) = P (V1 , V2 , R), and PMI = −2 log(P (V1 , V2 , R)). The values of normalized PMI lie between −1 and 1, approaching −1 when the items never appear together, taking the value 0 in the case of independence, and the value 1 when they always appear together. We also considered a weighted PMI measure (Lin and Pantel, 2002) that corrects the bias of PMI for rare triples. A specificity measure (Mirroshandel et al., 2013), originally used to measure the precision of subcategorization frames, also performed well : specificity = 1 P (V1 , V2 , R) P (V1 , V2 , R) P (V1 , V2 , R) × (P +P +P ) 3 P (V1 , Vi , R) P (Vi , V2 , R) P (V1 , V2 , Ri ) i i i A version of Do et al. (2011)’s measure for triples involving causal relations did not fare so well on other types of relation. The definition of the measure can be found in (Do et al., 2011). 6 Finally, we investigated a measure that evaluates the contribution of each element in the triple to the significance measure (this measure is similar to specificity). 1 Wcombin"
C14-1206,C12-1115,1,0.863642,"Missing"
C14-1206,prasad-etal-2008-penn,0,0.210684,"on the .fr domain. We first parsed the documents in our corpus using BONSAI 2 , which first produced a morpho-syntactic labeling using MElt (Denis and Sagot, 2012) and then a syntactic analysis in the form of dependency trees via a French version of the MaltParser (Nivre et al., 2007). Our goal is to find pairs of verbs linked by a relation explicitly marked by a discourse connector in the corpus, as an indication of a regular semantic relation between the two verbs. The relations we have considered are common to most theories of discourse analysis, and they can be grouped into four classes (Prasad et al., 2008) : causal (contingency) relations, temporal relations, comparison relations (mainly contrast type relations), and expansion relations (e.g. elaboration or continuation). To find explicitly marked relations, we used a lexicon of discourse connectors for French, the manually constructed LEXCONN resource (Roze et al., 2012) 3 . LEXCONN includes 358 connectors and gives their syntactic category as well as associated discourse relations inspired from (Asher and Lascarides, 2003). Some connectors are ambiguous in that they are associated with several relations. We used only the unambiguous connector"
C14-1206,sagot-2010-lefff,0,0.0685602,"Missing"
C14-1206,S13-2001,0,0.114442,"large non-annotated corpus. Each pair of elementary discourse units is automatically annotated with the discourse relation triggered by the presence of the connector (connectors are often filtered for non-discursive uses). Those connectors are afterwards eliminated from the corpus so that the model trained on this dataset will not be informed by the presence of those connectors. The pioneering article in this group is Marcu and Echihabi (2002). Such learning methods with such “artificial data” obtain low scores, barely above chance as shown in Sporleder and Lascarides (2008). Braud and Denis (2013) observe that the performance of a classifier for the prediction of implicit relations is much lower when using “artificial” data than on “natural” data (implicit relations annotated by a human being). They propose a method which exploits these two different kinds of datasets together in various mixtures and on the level of the prediction algorithm, obtaining thus a significant improvement on the Annodis corpus. Our approach is different and complementary ; we isolate the semantic relations between pairs of verbs. We can use that as a feature on discourse units for discourse parsing but it has"
C14-1206,W06-1317,0,0.0561899,"xception of Chklovski and Pantel (2004) who do not present a systematic evaluation of their results. An important difference of our approach is also to consider predicates and their negation as separate entries. Finally, we mention the approaches which while focusing on the learning of discourse structures, nonetheless enrich their systems with lexical information. Feng and Hirst (2012) have used H ILDA (Hernault et al., 2010) adding more features. A specific family of features represents lexical similarity based on the hierarchical distance in V ERB N ET and W ORD N ET. In a similar fashion, Wellner et al. (2006) focus on intra-sentential discourse relations adding lexical information on the features based on measures proposed by Lin (1998) calculated on the British National Corpus. Those approaches use thus only information on lexical similarity without semantically typing this link. The impact of this information seems limited. As far as evaluation is concerned, our method is similar to that followed in Tremper and Frank (2013) for implication relations combining in and out of context evaluation for verbal associations. Their inter-annotator agreement is similar to ours (0.42-0.44 of Kappa) with ver"
C14-1206,C98-1013,0,\N,Missing
C14-1206,C98-2122,0,\N,Missing
C14-1206,afantenos-etal-2012-empirical,1,\N,Missing
C16-1334,P98-1013,0,0.630769,"Missing"
C16-1334,P06-4018,0,0.0315743,"Fj respectively, while Slui and Sluj are sets of various senses of the corresponding lexical units in WordNet. The values of the semantic similarity based features were computed using a similar formula (but averaged) for a given similarity function f and two frames F1 and F2 : P  X (slui ,sluj )∈Slui ×Sluj f (slui , sluj ) 1 × similarity(Fi , Fj , f ) = |LUFi × LUFj | |Slui × Sluj | (lui ,luj )∈LUFi ×LUFj The semantic similarity measures are classical wordnet similarity measures: Path, Wu-Palmer (Wu and Palmer, 1994) and Leacock-Chodorow (Leacock and Chodorow, 1998), computed using NLTK’s (Bird, 2006) Python interface. They all take into consideration the path between synsets in WordNet’s hierarchy with different normalization factors. For instance Wu-Palmer is based on the depths of two synsets in the WordNet hierarchy along with the depth of the least common subsumer. The path similarity measure is a semantic association measure which is based on the shortest path that connects two synsets in the taxonomy in which the senses occurred. LCH similarity takes into consideration the depth of the taxonomy in addition to the shortest path, and is computed as −log(p/2d), where p denotes the shor"
C16-1334,C14-1206,1,0.929949,"efinition): def overlap(F1 , F2 ) = kopen(F1 )k ∪ kopen(F2 )k kopen(F1 )k ∩ kopen(F2 )k 3545 3.3 Corpus-Based Features In addition to the previous sets of features, which make use of existing resources or of the existing structure we want to expand, we derived clues from large corpora where frame lexical units can be observed in the same contexts, indicating a potentially regular relation between their corresponding frames. We collected both general ococcurences, in the spirit of (Pennacchiotti and Wirth, 2009), and targetted associations of verbs in explicit discursive contexts, inspired by (Conrath et al., 2014). Frame Coocurrences We used statistics of frame co-occurrences from a large-scale corpus to predict frame relatedness, relying on the intuition that related frames tend to co-occur more often in the same context within a given corpus. The context could be either a document, a sentence, or a specific number of sentences. We computed the co-occurrence of frames as point-wise mutual information (PMI) within the GigaWord corpus (Graff et al., 2003). Similar information has already been used in (Pennacchiotti and Wirth, 2009), but we deal with the frame ambiguity problem differently. Ideally, one"
C16-1334,N12-1086,0,0.0220846,"., 2011). It also provides additional information for semantic analysis in contextual role linking, as demonstrated in (Li et al., 2015), or to improve prediction of roles, as in the work of Kshirsagar et al. (2015). The relations are also useful to build thesauri and to retrieve semantically related words (Ruppenhofer et al., 2006). However, one big issue with FrameNet is its partial coverage of the lexicon and the intended set of frames, which translates also into a partial coverage of frame relations. Numerous studies address FrameNet’s lack of lexical coverage (Pennacchiotti et al., 2008; Das and Smith, 2012; Pavlick et al., 2015). In contrast, little work has been done on extending frame relations except by Ovchinnikova et al. (2010), in which cluster of frames are proposed based on collocation information, along with principles to be respected when adding new relations, or Pennacchiotti and Wirth (2009), which defines a generic notion of “relatedness” between frames, with no semantics to compare this to FrameNet relations. Both approaches are unsupervised, and provide little evaluation of the relevance to the intended FN structure. We present a supervised approach to enrich FrameNet’s relationa"
C16-1334,J14-1002,0,0.017186,"m the existing FrameNet network, information from the WordNet relations between synsets projected into semantic frames, and corpus-collected lexical associations. We show large improvements over baselines consisting of each of the three groups of features in isolation. We then use this model to select frame pairs as candidate relations, and perform evaluation on a sample with good precision. 1 Introduction In the area of formal linguistics, frame semantics is a theory of meanings, which was introduced by Charles J. Fillmore and his colleagues back in the early 1980’s. Frame semantic analysis (Das et al., 2014), which is based on frame semantics, itself is a type of shallow semantic analysis in which the focus is on predicates and their arguments. Frame semantic analysis is abstracting away from single verbal predicates, used in other semantic analysis approaches such as Propbank-based work (Kingsbury and Palmer, 2012), to ”semantic frames” (Fillmore, 1982). The idea in frame semantics is to group predicates referring to similar events, processes, and/or object types under the umbrella term ’a semantic frame’, which can be expressed with different parts of speech and with arguments that can have var"
C16-1334,P15-2036,0,0.014607,"s own for semantic analysis (Burchardt et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3542 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3542–3552, Osaka, Japan, December 11-17 2016. 2005), or paraphrase extraction (Hasegawa et al., 2011). It also provides additional information for semantic analysis in contextual role linking, as demonstrated in (Li et al., 2015), or to improve prediction of roles, as in the work of Kshirsagar et al. (2015). The relations are also useful to build thesauri and to retrieve semantically related words (Ruppenhofer et al., 2006). However, one big issue with FrameNet is its partial coverage of the lexicon and the intended set of frames, which translates also into a partial coverage of frame relations. Numerous studies address FrameNet’s lack of lexical coverage (Pennacchiotti et al., 2008; Das and Smith, 2012; Pavlick et al., 2015). In contrast, little work has been done on extending frame relations except by Ovchinnikova et al. (2010), in which cluster of frames are proposed based on collocation info"
C16-1334,P15-1122,0,0.0209867,"to and Strube, 2006), its frame structure is assumed to be useful on its own for semantic analysis (Burchardt et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3542 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3542–3552, Osaka, Japan, December 11-17 2016. 2005), or paraphrase extraction (Hasegawa et al., 2011). It also provides additional information for semantic analysis in contextual role linking, as demonstrated in (Li et al., 2015), or to improve prediction of roles, as in the work of Kshirsagar et al. (2015). The relations are also useful to build thesauri and to retrieve semantically related words (Ruppenhofer et al., 2006). However, one big issue with FrameNet is its partial coverage of the lexicon and the intended set of frames, which translates also into a partial coverage of frame relations. Numerous studies address FrameNet’s lack of lexical coverage (Pennacchiotti et al., 2008; Das and Smith, 2012; Pavlick et al., 2015). In contrast, little work has been done on extending frame relations except by Ovchinnikova e"
C16-1334,N13-1024,0,0.0146201,") contrast, (5) disjunction, (6) elaboration. Each corresponds to a coherent set of markers and can provide clues to corresponding FrameNet relations: causative-of for (1), precede for (2) and (3), subframe for (6), and (4) and (5) for some form of looser relatedness that is sometimes encoded as ”see-also” or ”using” in FrameNet. For each class we implemented the three best measures according to (Conrath et al., 2014), which correspond to different normalizations or combinations of the verbs and discourse relation cooccurrences : normalized PMI (Evert, 2005), a specificity measure taken from (Mirroshandel et al., 2013), and a combined association measure they call w combined. They are supposed to capture different aspects of the targeted lexical associations. Let P(V1 ,V2 ,R) be the probability of the association of the two verbs V1 ,V2 with the given semantic relation R: NPMI(V1 , V2 , R) = PMI(V1 , V2 , R)/(−2 log(P (V1 , V2 , R))) specificity(V1 , V2 , R) = 1 P (V1 , V2 , R) P (V1 , V2 , R) P (V1 , V2 , R) × (P +P +P ) 3 P (V1 , Vi , R) P (Vi , V2 , R) P (V1 , V2 , Ri ) i i 3546 i 1 Wcombined (V1 , V2 , R) = (wV1 + wV2 + wR ) 3 with: wV1 = P (V1 ,V2 ,R) max(P (Vi ,V2 ,R)) , i wV2 = P (V1 ,V2 ,R) max(P (V"
C16-1334,ovchinnikova-etal-2010-data,0,0.111145,"et al., 2015), or to improve prediction of roles, as in the work of Kshirsagar et al. (2015). The relations are also useful to build thesauri and to retrieve semantically related words (Ruppenhofer et al., 2006). However, one big issue with FrameNet is its partial coverage of the lexicon and the intended set of frames, which translates also into a partial coverage of frame relations. Numerous studies address FrameNet’s lack of lexical coverage (Pennacchiotti et al., 2008; Das and Smith, 2012; Pavlick et al., 2015). In contrast, little work has been done on extending frame relations except by Ovchinnikova et al. (2010), in which cluster of frames are proposed based on collocation information, along with principles to be respected when adding new relations, or Pennacchiotti and Wirth (2009), which defines a generic notion of “relatedness” between frames, with no semantics to compare this to FrameNet relations. Both approaches are unsupervised, and provide little evaluation of the relevance to the intended FN structure. We present a supervised approach to enrich FrameNet’s relational structure by training a model on the existing set of frame relations and a rich set of features combining linguistic and struct"
C16-1334,P15-2067,0,0.0247295,"Missing"
C16-1334,E09-1075,0,0.339734,"y related words (Ruppenhofer et al., 2006). However, one big issue with FrameNet is its partial coverage of the lexicon and the intended set of frames, which translates also into a partial coverage of frame relations. Numerous studies address FrameNet’s lack of lexical coverage (Pennacchiotti et al., 2008; Das and Smith, 2012; Pavlick et al., 2015). In contrast, little work has been done on extending frame relations except by Ovchinnikova et al. (2010), in which cluster of frames are proposed based on collocation information, along with principles to be respected when adding new relations, or Pennacchiotti and Wirth (2009), which defines a generic notion of “relatedness” between frames, with no semantics to compare this to FrameNet relations. Both approaches are unsupervised, and provide little evaluation of the relevance to the intended FN structure. We present a supervised approach to enrich FrameNet’s relational structure by training a model on the existing set of frame relations and a rich set of features combining linguistic and structural information. Further, we also leverage external resources such as WordNet. The resulting model is used to predict new frame-to-frame relations whose validity is then man"
C16-1334,D08-1048,0,0.0550689,"Missing"
C16-1334,N06-1025,0,0.0464052,"merce [a car]goods [to Lester]buyer . [Lester]buyer ’s [purchase]commerce of [Jimmy]seller ’s [car]goods (...) Most recent NLP work on such semantic frames are based on FrameNet (henceforth FN) (Baker et al., 1998), a resource listing frames, how they can be evoked, and their argument types. FrameNet also lists how the frames are organized with respect to each other, with a set of frame-to-frame relations. If FrameNet based analysis has proven useful for certain tasks such as information extraction (Surdeanu et al., 2003), question-answering (Shen and Lapata, 2007), or coreference resolution (Ponzetto and Strube, 2006), its frame structure is assumed to be useful on its own for semantic analysis (Burchardt et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3542 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3542–3552, Osaka, Japan, December 11-17 2016. 2005), or paraphrase extraction (Hasegawa et al., 2011). It also provides additional information for semantic analysis in contextual role linking, as demonstrated in (Li et al., 2015), or"
C16-1334,prasad-etal-2008-penn,0,0.0210099,"d as features measures of semantic similarity derived from corpusbased specific associations between lexical items. If we can find valid lexical associations to match the targeted frame relations, we can obtain relevant association measures. We adapted the method of (Conrath et al., 2014), in which semantic associations between verbs are derived from cooccurrences between two verbs and certain classes of discourse markers, using mutual information measures. This shallow discourse analysis can be seen as extraction of typical semantic relations between verbs. Using the Penn Discourse Treebank (Prasad et al., 2008) list of markers, grouped into semantic classes, we computed six types of associations between verbs: (1) causal, (2) temporal, (3) continuation, (4) contrast, (5) disjunction, (6) elaboration. Each corresponds to a coherent set of markers and can provide clues to corresponding FrameNet relations: causative-of for (1), precede for (2) and (3), subframe for (6), and (4) and (5) for some form of looser relatedness that is sometimes encoded as ”see-also” or ”using” in FrameNet. For each class we implemented the three best measures according to (Conrath et al., 2014), which correspond to different"
C16-1334,D07-1002,0,0.0346935,"]goods [from Jimmy]seller . [Jimmy]seller [sold]commerce [a car]goods [to Lester]buyer . [Lester]buyer ’s [purchase]commerce of [Jimmy]seller ’s [car]goods (...) Most recent NLP work on such semantic frames are based on FrameNet (henceforth FN) (Baker et al., 1998), a resource listing frames, how they can be evoked, and their argument types. FrameNet also lists how the frames are organized with respect to each other, with a set of frame-to-frame relations. If FrameNet based analysis has proven useful for certain tasks such as information extraction (Surdeanu et al., 2003), question-answering (Shen and Lapata, 2007), or coreference resolution (Ponzetto and Strube, 2006), its frame structure is assumed to be useful on its own for semantic analysis (Burchardt et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3542 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3542–3552, Osaka, Japan, December 11-17 2016. 2005), or paraphrase extraction (Hasegawa et al., 2011). It also provides additional information for semantic analysis in contextual"
C16-1334,P03-1002,0,0.0788074,"b. c. [Lester]buyer [bought]commerce [a car]goods [from Jimmy]seller . [Jimmy]seller [sold]commerce [a car]goods [to Lester]buyer . [Lester]buyer ’s [purchase]commerce of [Jimmy]seller ’s [car]goods (...) Most recent NLP work on such semantic frames are based on FrameNet (henceforth FN) (Baker et al., 1998), a resource listing frames, how they can be evoked, and their argument types. FrameNet also lists how the frames are organized with respect to each other, with a set of frame-to-frame relations. If FrameNet based analysis has proven useful for certain tasks such as information extraction (Surdeanu et al., 2003), question-answering (Shen and Lapata, 2007), or coreference resolution (Ponzetto and Strube, 2006), its frame structure is assumed to be useful on its own for semantic analysis (Burchardt et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3542 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3542–3552, Osaka, Japan, December 11-17 2016. 2005), or paraphrase extraction (Hasegawa et al., 2011). It also provides additional inf"
C16-1334,W09-1127,0,0.329414,"meronymy/honolymy. Hypernym and hyponym (and troponymy for verbs) are super-subordinate relations and link more general synsets to specific ones and could be relevant for Frame inheritance, and thus make the first source of information for predicting frame relations. As we are interested in relationships between pairs of frames, instead of pair of synsets, we need to transfer knowledge about relations between synsets to useful features between frames whose verbs appear in WordNet synsets. There have been attempts at matching WordNet sense inventory to FrameNet frames (Shi and Mihalcea, 2005; Tonelli and Pighin, 2009) but since both resources are incomplete, we decided to compute frame relatedness using an existing WordNet relation between synsets which include verbs from the given frames, irrespective of their senses. This certainly introduces some noise, that we hope to control with redundancies in frame-to-frame associations. For our purposes, we have divided WN-based features into two groups: (1) occurrence-based features (2) similarity-based features. Occurrence-based features are simple counts of existence of a particular relation between a pair 3544 from all possible pairs of all senses of all lexic"
C16-1334,P94-1019,0,0.337223,"luj ) = T rue (slui ,sluj )∈Slui ×Sluj LUFi and LUFj are sets of lexical units of frames Fi and Fj respectively, while Slui and Sluj are sets of various senses of the corresponding lexical units in WordNet. The values of the semantic similarity based features were computed using a similar formula (but averaged) for a given similarity function f and two frames F1 and F2 : P  X (slui ,sluj )∈Slui ×Sluj f (slui , sluj ) 1 × similarity(Fi , Fj , f ) = |LUFi × LUFj | |Slui × Sluj | (lui ,luj )∈LUFi ×LUFj The semantic similarity measures are classical wordnet similarity measures: Path, Wu-Palmer (Wu and Palmer, 1994) and Leacock-Chodorow (Leacock and Chodorow, 1998), computed using NLTK’s (Bird, 2006) Python interface. They all take into consideration the path between synsets in WordNet’s hierarchy with different normalization factors. For instance Wu-Palmer is based on the depths of two synsets in the WordNet hierarchy along with the depth of the least common subsumer. The path similarity measure is a semantic association measure which is based on the shortest path that connects two synsets in the taxonomy in which the senses occurred. LCH similarity takes into consideration the depth of the taxonomy in"
C16-1334,kingsbury-palmer-2002-treebank,0,\N,Missing
C16-1334,C98-1013,0,\N,Missing
candito-etal-2014-developing,mouton-etal-2010-framenet,1,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salto,0,\N,Missing
candito-etal-2014-developing,burchardt-pennacchiotti-2008-fate,0,\N,Missing
candito-etal-2014-developing,sagot-etal-2010-lexicon,1,\N,Missing
candito-etal-2014-developing,N10-1138,0,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salsa,0,\N,Missing
candito-etal-2014-developing,P98-1013,0,\N,Missing
candito-etal-2014-developing,C98-1013,0,\N,Missing
candito-etal-2014-developing,W13-4917,1,\N,Missing
candito-etal-2014-developing,P11-2023,1,\N,Missing
candito-etal-2014-developing,J02-3001,0,\N,Missing
candito-etal-2014-developing,J05-1004,0,\N,Missing
candito-etal-2014-developing,heppin-gronostaj-2012-rocky,0,\N,Missing
candito-etal-2014-developing,I11-1132,1,\N,Missing
candito-etal-2014-developing,abeille-barrier-2004-enriching,0,\N,Missing
D17-1136,H91-1060,0,0.548578,"iscourse structure for text exist, discourse parsing work has largely concentrated on Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and the RST Discourse Treebank (RST-DT) (Carlson et al., 2003), which is the largest corpus of texts annotated with full discourse structures. The RST-DT, annotated in the style of RST, consists of 385 news articles from the Penn Treebank, split into a training and test sets of 347 and 38 documents.The standard evaluation procedure for RST discourse parsing, RST-Parseval, proposed by Marcu (2000), adapts the Parseval procedure for syntactic parsing (Black et al., 1991). RST-Parseval computes scores on discourse structures with no label (S A sample of RST discourse parsers Almost all RST discourse parsers are evaluated on the test section of the RST-DT using manually segmented Elementary Discourse Units (EDUs). We contacted by email the main or corresponding author of each recently (2013–2017) published, textlevel RST discourse parser evaluated in this setting and asked the authors to provide us with the predictions they used in their study or a procedure that would enable us to reproduce identical or at least similar predictions. When our attempts were unsu"
D17-1136,E17-1028,0,0.217094,"greedy parser with linear-chain CRF models (Feng and Hirst, 2014). We use the version of the parser available on the author’s webpage, that lacks post-editing and contextual features. BPS16 is a sequence-to-sequence parser, heuristically constrained to build trees, with a hierarchical neural network model (hierarchical biLSTM) (Braud et al., 2016). LLC16 is a CKY chart parser with a hierarchical neural network model (attention-based hierarchical bi-LSTM) (Li et al., 2016). BCS17 mono, BCS17 cross+dev are two variants of a transition-based parser that uses a feed-forward neural network model (Braud et al., 2017). JE14 DPLP is a shift-reduce parser that uses an SVM model (Ji and Eisenstein, 2014). We use predictions provided by the author, from an improved, unpublished version of the parser. The first four parsers (HHN16 HILDA, SHV15 D, JCN15 1S-1S, FH14 gCRF) use, as features, only localist representations of the input and parsing state, i.e. surface-form and syntactic information: length of discourse units (DUs), distance between DUs, n-grams of words and POS tags, relations of syntactic dominance between DUs. . . The last five parsers (BPS16, LLC16, BCS17 mono and cross+dev, JE14 DPLP concat) build"
D17-1136,C16-1179,0,0.283961,"ield (DCRF) models, in its 1 sentence - 1 subtree (1S-1S) variant that builds a document-level RST tree on top of sentence-level subtrees built for each sentence independently (Joty et al., 2013, 2015). FH14 gCRF is a two stage (sentence- then documentlevel) bottom-up, greedy parser with linear-chain CRF models (Feng and Hirst, 2014). We use the version of the parser available on the author’s webpage, that lacks post-editing and contextual features. BPS16 is a sequence-to-sequence parser, heuristically constrained to build trees, with a hierarchical neural network model (hierarchical biLSTM) (Braud et al., 2016). LLC16 is a CKY chart parser with a hierarchical neural network model (attention-based hierarchical bi-LSTM) (Li et al., 2016). BCS17 mono, BCS17 cross+dev are two variants of a transition-based parser that uses a feed-forward neural network model (Braud et al., 2017). JE14 DPLP is a shift-reduce parser that uses an SVM model (Ji and Eisenstein, 2014). We use predictions provided by the author, from an improved, unpublished version of the parser. The first four parsers (HHN16 HILDA, SHV15 D, JCN15 1S-1S, FH14 gCRF) use, as features, only localist representations of the input and parsing state"
D17-1136,W09-3813,0,0.461052,"n the other hand, RST-Parseval considers approximately twice as many nodes as the original Parseval would on binarized trees (at most 2n − 2 nodes for n EDUs, compared to n − 1 attachments in a binary tree), and the relation labels of most nuclei are redundant with the nuclearity of a node and its sister (S PAN for a nucleus whose sisters are satellites, and the same label as its sisters for a nucleus whose sisters are nuclei). Both aspects artificially raise the level of agreement between RST trees, especially when using manual EDU segmentation. However, all the parsers in our sample except (Sagae, 2009; Heilman and Sagae, 2015) predict binary trees over manually segmented EDUs and evaluate them against right-heavy binarized reference trees. In this setting, Marcu’s encoding of RST trees RST-Parseval are no longer motivated. We can thus revert to using the standard Parseval procedure on a representation of binary RST trees where each internal node is a labelled attachment decision to obtain a more accurate evaluation of RST parser performance. Figure 1 represents (a) an original RST tree using Marcu’s encoding, (b) its right-heavy binarized version, (c) the tree of labelled attachment decisi"
D17-1136,N15-3001,0,0.337333,"Missing"
D17-1136,P14-1048,0,0.466863,"of discourse units, logistic regression for relation labelling) and a slightly different feature set adapted to use predicted syntactic dependency trees (Surdeanu et al., 2015). JCN15 1S-1S is a two stage (sentencethen document-level) CKY chart parser with Dynamic Conditional Random Field (DCRF) models, in its 1 sentence - 1 subtree (1S-1S) variant that builds a document-level RST tree on top of sentence-level subtrees built for each sentence independently (Joty et al., 2013, 2015). FH14 gCRF is a two stage (sentence- then documentlevel) bottom-up, greedy parser with linear-chain CRF models (Feng and Hirst, 2014). We use the version of the parser available on the author’s webpage, that lacks post-editing and contextual features. BPS16 is a sequence-to-sequence parser, heuristically constrained to build trees, with a hierarchical neural network model (hierarchical biLSTM) (Braud et al., 2016). LLC16 is a CKY chart parser with a hierarchical neural network model (attention-based hierarchical bi-LSTM) (Li et al., 2016). BCS17 mono, BCS17 cross+dev are two variants of a transition-based parser that uses a feed-forward neural network model (Braud et al., 2017). JE14 DPLP is a shift-reduce parser that uses"
D17-1136,W16-3616,0,0.043084,". We contacted by email the main or corresponding author of each recently (2013–2017) published, textlevel RST discourse parser evaluated in this setting and asked the authors to provide us with the predictions they used in their study or a procedure that would enable us to reproduce identical or at least similar predictions. When our attempts were unsuccessful we tried to reproduce similar predictions from published materiel (source code, binaries, model). We managed to obtain or reproduce predictions for 9 parsers from 8 studies. The first parser, denoted HHN16 HILDA, is a reimplementation (Hayashi et al., 2016) of the classic, bottom-up, greedy HILDA parser with a linear SVM model (Hernault et al., 2010) ; this parser serves as a reference point to evaluate the progress made by more recent parsers. SHV15 1319 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1319–1324 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics D is a variant of the HILDA parser with different models (perceptron for attachment of discourse units, logistic regression for relation labelling) and a slightly different feature set adapted to use p"
D17-1136,P14-1002,0,0.70097,"ersion of the parser available on the author’s webpage, that lacks post-editing and contextual features. BPS16 is a sequence-to-sequence parser, heuristically constrained to build trees, with a hierarchical neural network model (hierarchical biLSTM) (Braud et al., 2016). LLC16 is a CKY chart parser with a hierarchical neural network model (attention-based hierarchical bi-LSTM) (Li et al., 2016). BCS17 mono, BCS17 cross+dev are two variants of a transition-based parser that uses a feed-forward neural network model (Braud et al., 2017). JE14 DPLP is a shift-reduce parser that uses an SVM model (Ji and Eisenstein, 2014). We use predictions provided by the author, from an improved, unpublished version of the parser. The first four parsers (HHN16 HILDA, SHV15 D, JCN15 1S-1S, FH14 gCRF) use, as features, only localist representations of the input and parsing state, i.e. surface-form and syntactic information: length of discourse units (DUs), distance between DUs, n-grams of words and POS tags, relations of syntactic dominance between DUs. . . The last five parsers (BPS16, LLC16, BCS17 mono and cross+dev, JE14 DPLP concat) build distributed representations of DUs, complemented with a subset of localist represent"
D17-1136,J15-3002,0,0.440507,"Missing"
D17-1136,P13-1048,0,0.0718467,"2017. 2017 Association for Computational Linguistics D is a variant of the HILDA parser with different models (perceptron for attachment of discourse units, logistic regression for relation labelling) and a slightly different feature set adapted to use predicted syntactic dependency trees (Surdeanu et al., 2015). JCN15 1S-1S is a two stage (sentencethen document-level) CKY chart parser with Dynamic Conditional Random Field (DCRF) models, in its 1 sentence - 1 subtree (1S-1S) variant that builds a document-level RST tree on top of sentence-level subtrees built for each sentence independently (Joty et al., 2013, 2015). FH14 gCRF is a two stage (sentence- then documentlevel) bottom-up, greedy parser with linear-chain CRF models (Feng and Hirst, 2014). We use the version of the parser available on the author’s webpage, that lacks post-editing and contextual features. BPS16 is a sequence-to-sequence parser, heuristically constrained to build trees, with a hierarchical neural network model (hierarchical biLSTM) (Braud et al., 2016). LLC16 is a CKY chart parser with a hierarchical neural network model (attention-based hierarchical bi-LSTM) (Li et al., 2016). BCS17 mono, BCS17 cross+dev are two variants o"
D17-1136,D16-1035,0,0.540401,"ubtrees built for each sentence independently (Joty et al., 2013, 2015). FH14 gCRF is a two stage (sentence- then documentlevel) bottom-up, greedy parser with linear-chain CRF models (Feng and Hirst, 2014). We use the version of the parser available on the author’s webpage, that lacks post-editing and contextual features. BPS16 is a sequence-to-sequence parser, heuristically constrained to build trees, with a hierarchical neural network model (hierarchical biLSTM) (Braud et al., 2016). LLC16 is a CKY chart parser with a hierarchical neural network model (attention-based hierarchical bi-LSTM) (Li et al., 2016). BCS17 mono, BCS17 cross+dev are two variants of a transition-based parser that uses a feed-forward neural network model (Braud et al., 2017). JE14 DPLP is a shift-reduce parser that uses an SVM model (Ji and Eisenstein, 2014). We use predictions provided by the author, from an improved, unpublished version of the parser. The first four parsers (HHN16 HILDA, SHV15 D, JCN15 1S-1S, FH14 gCRF) use, as features, only localist representations of the input and parsing state, i.e. surface-form and syntactic information: length of discourse units (DUs), distance between DUs, n-grams of words and POS"
F14-1022,P98-1013,0,0.0514405,"Missing"
F14-1022,candito-etal-2010-statistical,0,0.0695714,"Missing"
F14-1022,J96-2004,0,0.0413584,"Missing"
F14-1022,P08-1090,0,0.084999,"Missing"
F14-1022,W04-3205,0,0.146831,"Missing"
F14-1022,D11-1027,0,0.0448053,"Missing"
F14-1022,P12-1007,0,0.0310809,"Missing"
F14-1022,D09-1122,0,0.0610891,"Missing"
F14-1022,W12-4107,0,0.0401257,"Missing"
F14-1022,P98-2127,0,0.57691,"Missing"
F14-1022,C02-1144,0,0.135074,"Missing"
F14-1022,P07-2047,0,0.0417827,"Missing"
F14-1022,P02-1047,0,0.083139,"Missing"
F14-1022,N13-1024,0,0.0317988,"Missing"
F14-1022,prasad-etal-2008-penn,0,0.0329853,"Missing"
F14-1022,sagot-2010-lefff,0,0.0524525,"Missing"
F14-1022,S13-2001,0,0.0800276,"Missing"
J18-2001,D15-1109,1,0.896619,"discourse units. While dependency graphs on their own do not suffice to fully represent CDUs, Perret et al. (2016) also shows how dependency structures can approximate CDUs by distributing the relations in which a CDU is an argument over the CDU’s member EDUs. Dependency structures for discourse formalisms less constrained than RST’s constituent trees are easy to make and to use, because dependency structures are semantically more transparent than c-structures. The same semantic interpretation relation holds when we relax dependency trees to non-projective trees as in Muller et al. (2012) and Afantenos et al. (2015) or to directed acyclic graphs as in Perret et al. (2016). CDUs aside, dependency structures exhibit the actual semantic arguments of the relations and are much closer to a logical formalism in which binary relations could hold in all sorts of graph configurations. A constituency based approach like RST’s arguably does not exhibit the actual semantic arguments of discourse relations—hence, the need for a nuclearity principle. 208 Morey, Muller, and Asher A Dependency Perspective on RST Discourse Parsing and Evaluation 3.2 Dependency-Based Alternatives to RST Trees As we have just seen, the not"
J18-2001,L16-1432,1,0.879435,"w how dependency parsing can provide comparable analyses to constituent-based parsers for the RST corpus, an added advantage of dependency parsing formalisms like the one we have proposed here is that it is easily adaptable: (1) to discourse structures that are not projective trees—Muller et al. (2012) use non-projective maximum spanning tree (MST) algorithms to compute dependency trees in an SDRT style on the Annodis corpus of French texts; (2) to structures that are not even treelike, for instance, representations for multi-party chat conversations— Perret et al. (2016) use the STAC corpus (Asher et al. 2016) where discourse annotations are not trees but only directed acyclic graphs, which constituent parsers based on RST projective tree formalisms cannot reproduce. Perret et al. (2016) have shown how a dependency formalism combined with constraints implemented in ILP can learn such structures with comparable measures of success to the efforts on the more constrained RST tree bank. Dependency parsing also easily handles long distance dependencies without the computation of nuclearity features. Venant et al. (2013) have shown that nuclearity computations cannot handle all long distance dependencies"
J18-2001,D15-1263,0,0.409716,"Missing"
J18-2001,H91-1060,0,0.555704,"approaches. We will consider dependency-based evaluation in Sections 3.5 and 3.6, and see how they differ in the kind of information they measure. The differences partially reflect similar discussions within the syntactic community, where dependency structures have been advocated in part because of issues with constituentbased evaluation (Carroll, Briscoe, and Sanfilippo 1998; Carroll, Minnen, and Briscoe 1999). Constituency-based approaches naturally lend themselves to evaluation procedures that originated in the syntactic parsing literature. The most common evaluation procedure is Parseval (Black et al. 1991), which combines precision and recall on (typed) constituents, plus a Crossing Parentheses score, which is the ratio of the number of predicted constituents that cross a gold standard constituent with respect to the total number of predicted constituents. Early work on RST parsing (Marcu 2000) introduced a modified version of Parseval to accommodate RST c-trees encoded as outlined above. This early work addressed the two tasks of segmenting elementary discourse units and parsing. As almost all existing work assumes gold segmentation and focuses on the parsing task itself, and because the trees"
J18-2001,E17-1028,0,0.230168,"Missing"
J18-2001,C16-1179,0,0.131024,"Missing"
J18-2001,W04-2324,0,0.0602846,"framework is that it generalizes naturally from RST constituent trees to other more permissive discourse structures. Frameworks like SDRT or GraphBank assume more general structures than trees (respectively, directed acyclic graphs and arbitrary graphs). Venant et al. (2013) show that differences between frameworks can be encoded with different, additional structural constraints on a dependency based graph. Thus, dependency graphs capture commonalities between these different types of representation, and it is easy to convert them into a dependency graph between EDUs. It was already shown in Danlos (2004) that some discourse constructs would be more appropriately represented as directed acyclic graphs. Although we will show below how dependency parsing can provide comparable analyses to constituent-based parsers for the RST corpus, an added advantage of dependency parsing formalisms like the one we have proposed here is that it is easily adaptable: (1) to discourse structures that are not projective trees—Muller et al. (2012) use non-projective maximum spanning tree (MST) algorithms to compute dependency trees in an SDRT style on the Annodis corpus of French texts; (2) to structures that are n"
J18-2001,P09-1075,0,0.0544663,"ependency metrics for syntactic parsing, we defined and implemented several metrics on the dependencies of head-ordered RST d-trees: unlabeled attachment score (UAS), labeled attachment score where the label is restricted to nuclearity (LAS-N), relation (LAS-R), or includes both (LAS-F). The source code of the new parsers and evaluation procedures is available online.4 We distinguish three groups among RST parsers. The first group consists of constituency parsers that do not use the notion of head EDU. HHN16 HILDA is a reimplementation by Hayashi, Hirao, and Nagata (2016) of the HILDA parser (duVerle and Prendinger 2009; Hernault et al. 2010), which was one of the first text-level discourse parsers to be published that was trained and evaluated on the RST-DT. It is a greedy bottom–up parser that uses two support vector machine models to predict structures and their labeling, on a set of lexical, syntactic, and structural features. SHV15 D follows the same architecture as HILDA, with minor differences in the models (perceptron for structure, logistic regression for labeling) and features (syntactic features expressed on dependency trees) (Surdeanu, Hicks, and Valenzuela-Esc´arcega 2015). JCN15 1S-1S is a two-"
J18-2001,C96-1058,0,0.253167,"by the different empirical motivations behind their development. Li et al. (2014) adapt a syntactic dependency parser, the MSTParser of McDonald, Crammer, and Pereira (2005), to predict RST dependency trees over a text segmented in EDUs instead of syntactic dependency trees over sentences segmented in tokens. They convert the RST constituency trees from the RST-DT to RST dependency trees that they use for training and evaluation. They report experiments with two decoders implemented in the original MSTParser, the non-projective Maximum Spanning Tree decoder and the projective Eisner decoder (Eisner 1996). Although the authors initially point out that a given simple RST dependency tree can correspond to several RST constituency trees, they do not describe any mechanism to cope with this issue and yet report RST-Parseval scores for projective variants of their parser. It turns out that their implementation of the RST-Parseval procedure does not operate on pairs of reference and predicted RST constituency trees but rather on pairs of reference and predicted RST dependency trees. For each RST dependency tree, constituency spans are computed from the transitive closure of its adjacency matrix (i.e"
J18-2001,P14-1048,0,0.354471,"Missing"
J18-2001,P15-1147,0,0.0922104,"r-level structures, to yield a directed tree structure over the discourse units of a text. Simple dependency structures are simplified representations of more complex SDRT graph structures (Muller et al. 2012), or a simplified representation of RST structures (Li et al. 2014), with some applications to specific tasks (Hirao et al. 2013). As we will show, this engenders some information loss. Once one takes the order of attachment into account, however, dependency representations do not in principle imply a loss of information, as established in the syntactic domain by Fern´andez-Gonz´alez and Martins (2015), using the notion of headordered dependency structures. Applying the head-ordered idea to discourse provides some advantages over the use of constituent structures, as head-ordered structures do not assume a higher abstraction level, a potentially contentious issue in discourse analysis (Hobbs et al. 1987). They also lend themselves to different computational models, as is also the case in syntactic parsing. We also relate this to how some work already makes use of a related notion of head constituent to inform discourse parsing. Dependency structures are also more general, and allow for the"
J18-2001,J86-3001,0,0.574341,"e or direction of an attachment. As we have been at some pains to point out, nuclearity and headedness are not the same thing; headedness does not determine nuclearity, because where a and b are EDUs, the two simple trees [aN , bS ] and [aN , bN ] (since we are just looking at structure and nuclearity, we have omitted any of the relational decorations) have the same head. And nuclearity by itself does not determine headedness even though its introduction by Marcu (2000) was semantically motivated in an effort to find the core or central idea in a span; a notion is reflected in other theories (Grosz and Sidner 1986; Asher 1993) by the use of coordinating and subordinating discourse relations. Consider, for instance, the following hypothetical tree structure [[aN bN ]N , cS ], where [aN bN ] is a subtree and the nucleus of the larger tree that includes c. An admittedly somewhat crazy interpretive principle that takes the head of the multinuclear relation to be the rightmost one locally but takes the leftmost one as the head projected up through the tree would yield the following non-projective dependency structure: {a ← b, a → c}. A saner, interpretive principle would be the one that we suggested earlier"
J18-2001,W16-3616,0,0.214358,"Missing"
J18-2001,D13-1158,0,0.239063,"a constituency-based approach to discourse analysis. We argue that the problem is more naturally formulated as the prediction of a dependency structure, on which simpler parsing strategies can be competitively applied. We show how to evaluate both families of discourse parsers in a constituent-based and a dependency-based framework. Discourse aspects are becoming more and more present in various NLP tasks. Text-level structures are useful as a representation of the coherence of a text and its topical organization, with applications to summarization (Marcu 2000; Louis, Joshi, and Nenkova 2010; Hirao et al. 2013), sentiment analysis (Bhatia, Ji, and Eisenstein 2015), or document classification (Ji and Smith 2017). Most empirical approaches to this problem focus on predicting structures following RST and are based on the corresponding RST-Discourse Treebank corpus (RST-DT), a large annotated resource of texts for discourse analysis in English (Hernault et al., 2010; Feng and Hirst 2014; Ji and Eisenstein 2014; Joty, Cartenini, and Ng 2015). On the other hand, there is a need to examine in depth some of the representational choices made within the RST framework. Many discourse parsers for RST have made"
J18-2001,J87-3004,0,0.596203,"specific tasks (Hirao et al. 2013). As we will show, this engenders some information loss. Once one takes the order of attachment into account, however, dependency representations do not in principle imply a loss of information, as established in the syntactic domain by Fern´andez-Gonz´alez and Martins (2015), using the notion of headordered dependency structures. Applying the head-ordered idea to discourse provides some advantages over the use of constituent structures, as head-ordered structures do not assume a higher abstraction level, a potentially contentious issue in discourse analysis (Hobbs et al. 1987). They also lend themselves to different computational models, as is also the case in syntactic parsing. We also relate this to how some work already makes use of a related notion of head constituent to inform discourse parsing. Dependency structures are also more general, and allow for the different structures advocated by the various aforementioned theories: tree structures (projective or not) or graph structures. They can thus provide a common representation framework between the different existing corpora. Dependency parsing also comes with different evaluation measures from those used in"
J18-2001,P14-1002,0,0.365502,"NLP tasks. Text-level structures are useful as a representation of the coherence of a text and its topical organization, with applications to summarization (Marcu 2000; Louis, Joshi, and Nenkova 2010; Hirao et al. 2013), sentiment analysis (Bhatia, Ji, and Eisenstein 2015), or document classification (Ji and Smith 2017). Most empirical approaches to this problem focus on predicting structures following RST and are based on the corresponding RST-Discourse Treebank corpus (RST-DT), a large annotated resource of texts for discourse analysis in English (Hernault et al., 2010; Feng and Hirst 2014; Ji and Eisenstein 2014; Joty, Cartenini, and Ng 2015). On the other hand, there is a need to examine in depth some of the representational choices made within the RST framework. Many discourse parsers for RST have made simplifying assumptions with respect to the linguistic annotations for practical purposes, and these choices affect the generality of the models and their evaluation. We thus focus on the issue of predicting a structure for a text, comparing different representations over the RST-DT. We will analyze the impact of the practical choices one makes while doing discourse parsing, while taking into account"
J18-2001,P17-1092,0,0.211813,"Missing"
J18-2001,J15-3002,0,0.486435,"Missing"
J18-2001,D14-1220,0,0.0571356,"y to the constituents of phrase structure grammars. However, this similarity is more apparent than real; discourse structure even in its RST format is a relational structure, formalized 198 Morey, Muller, and Asher A Dependency Perspective on RST Discourse Parsing and Evaluation as a set of “discourse constituents” together with a set of relations, instances of which hold between the constituents. All of this suggests a different approach to discourse parsing—namely, “dependency parsing,” which has gained some currency in the discourse parsing community (Muller et al. 2012; Hirao et al. 2013; Li et al. 2014). Discourse dependency parsing is analogous to syntactic dependency analysis, where units are directly related to each other without intermediate upper-level structures, to yield a directed tree structure over the discourse units of a text. Simple dependency structures are simplified representations of more complex SDRT graph structures (Muller et al. 2012), or a simplified representation of RST structures (Li et al. 2014), with some applications to specific tasks (Hirao et al. 2013). As we will show, this engenders some information loss. Once one takes the order of attachment into account, ho"
J18-2001,D16-1035,0,0.344873,"Missing"
J18-2001,P14-1003,0,0.0684512,"y to the constituents of phrase structure grammars. However, this similarity is more apparent than real; discourse structure even in its RST format is a relational structure, formalized 198 Morey, Muller, and Asher A Dependency Perspective on RST Discourse Parsing and Evaluation as a set of “discourse constituents” together with a set of relations, instances of which hold between the constituents. All of this suggests a different approach to discourse parsing—namely, “dependency parsing,” which has gained some currency in the discourse parsing community (Muller et al. 2012; Hirao et al. 2013; Li et al. 2014). Discourse dependency parsing is analogous to syntactic dependency analysis, where units are directly related to each other without intermediate upper-level structures, to yield a directed tree structure over the discourse units of a text. Simple dependency structures are simplified representations of more complex SDRT graph structures (Muller et al. 2012), or a simplified representation of RST structures (Li et al. 2014), with some applications to specific tasks (Hirao et al. 2013). As we will show, this engenders some information loss. Once one takes the order of attachment into account, ho"
J18-2001,W10-4327,0,0.138157,"Missing"
J18-2001,P05-1012,0,0.202213,"Missing"
J18-2001,D17-1136,1,0.890361,"Missing"
J18-2001,C12-1115,1,0.924099,"Missing"
J18-2001,N16-1013,1,0.956246,"d as the second argument of the ATTRIBUTION relation the content given by EDUs (π2 , π4 , π5 ) in the current annotation. One could make the R EASON relation that holds between [π2 , . . . , π4 ] and π5 multinuclear, and one could have a version of the Nuclearity Principle which gathered then (π2 , π4 , π5 ) into a complex discourse unit (CDU) that was the argument of the E XPLANATION relation. But this CDU would not correspond to a well-formed RST tree, because it does not correspond to a contiguous span. Although such structures are studied in other frameworks like SDRT (Venant et al. 2013; Perret et al. 2016), they have not been studied computationally nor even theoretically within the RST framework as far as we know. This simple example already points to a potential empirical problem with RST’s conception of structure. The relative unclarity of how to determine the arguments of discourse relations in an RST tree complicates efforts to capture semantically relevant information in these structures, and thus undermines a semantic argument for analyzing discourse in terms of constituent structures like RST tree structures. On the other hand, although most computational approaches have eschewed a prin"
J18-2001,prasad-etal-2008-penn,0,0.314698,"Missing"
J18-2001,W09-3813,0,0.177106,"ics that are used to assess the performance of discourse parsers, as we will see in Section 4. Even though n-ary trees are not so frequent in the RST-DT corpus, binarization applied high up in the original tree can drastically change an evaluation score. Figure 8 later in this article shows what happens when binarized structures differ (left vs. right): all relevant arguments’ substructures have different spans, generating multiple errors down the tree. Theoretically and empirically, the binarization of RST c-trees is not trivially reversible, a point that is overlooked in most studies except Sagae (2009) and Hernault et al. (2010). Headed RST c-trees. In the formal presentation of RST trees by Marcu (2000), each node is assigned a promotion set containing its most important EDUs, in accordance with the Nuclearity Principle. The promotion set of a node is recursively defined as the union of the promotion sets of its nuclei, the promotion set of each leaf (EDU) being the EDU itself. Semantically, promotion sets are said to provide a validation criterion for relations between wider discourse units: A relation can hold between two discourse units if it also holds between their promotion sets. Thi"
J18-2001,W05-1513,0,0.028646,"otion set of each leaf (EDU) being the EDU itself. Semantically, promotion sets are said to provide a validation criterion for relations between wider discourse units: A relation can hold between two discourse units if it also holds between their promotion sets. This leads Marcu to use the promotion set of (non-elementary) discourse units as a proxy to extract semantic similarity (WordNet)based features between these units, in his early shift-reduce RST discourse parser (Marcu 2000). Sagae pursued Marcu’s line of work by adapting the shift-reduce parser he had developed for syntactic parsing (Sagae and Lavie 2005) to RST discourse parsing (Sagae 2009). Sagae’s syntactic parser is a lexicalized shift-reduce parser that simultaneously produces both a dependency and a constituent structure, using features related to either of the two types of structures being built. Building an RST tree from EDUs is essentially similar to building a syntactic tree from tokens, save for the fact that discourse units have a promotion set rather than a unique head, because discourse relations can relate two or more nuclei of equal importance whereas syntactic dependencies (usually) asymmetrically relate a head and a dependen"
J18-2001,N03-1030,0,0.240644,"ly modified by two satellites via different relations, in a format suitable for training and evaluating parsers. Without these specific constructs, however, a more natural encoding would have put the relations labels higher up, on the node dominating the related constituents. This raises problems we will come back to in Section 2.4. The standard evaluation procedure for RST discourse parsers is Marcu’s (2000) adaptation of the Parseval procedure to this encoding, which we name RST-Parseval. Binarization. It is standard practice since the first proposals on RST discourse parsing (Reitter 2003; Soricut and Marcu 2003) to train and evaluate parsers on a right-heavy (, R) (S PAN, N) (A TTRIBUTION, S) (R EASON, S) (S PAN, N) π1 (S AME -U NIT, N) (S PAN, N) (R ESTATEMENT- E, S) π2 (S AME -U NIT, N) π5 π4 π3 Figure 2 Constituent tree corresponding to the RST tree in Figure 1. 203 Computational Linguistics Volume 44, Number 2 binarized version of the c-trees from the RST-DT. This is mostly accidental: constituencybased RST discourse parsers inherit a limitation shared by most syntactic constituencybased parsers that operate better, if not only, on binary trees. Such binarization is, however, not a theoretically"
J18-2001,N15-3001,0,0.0864493,"Missing"
J18-2001,W13-4002,1,0.959595,"ver, that would yield as the second argument of the ATTRIBUTION relation the content given by EDUs (π2 , π4 , π5 ) in the current annotation. One could make the R EASON relation that holds between [π2 , . . . , π4 ] and π5 multinuclear, and one could have a version of the Nuclearity Principle which gathered then (π2 , π4 , π5 ) into a complex discourse unit (CDU) that was the argument of the E XPLANATION relation. But this CDU would not correspond to a well-formed RST tree, because it does not correspond to a contiguous span. Although such structures are studied in other frameworks like SDRT (Venant et al. 2013; Perret et al. 2016), they have not been studied computationally nor even theoretically within the RST framework as far as we know. This simple example already points to a potential empirical problem with RST’s conception of structure. The relative unclarity of how to determine the arguments of discourse relations in an RST tree complicates efforts to capture semantically relevant information in these structures, and thus undermines a semantic argument for analyzing discourse in terms of constituent structures like RST tree structures. On the other hand, although most computational approaches"
J18-2001,D14-1196,0,0.166773,"Missing"
L16-1601,abeille-barrier-2004-enriching,0,0.372156,"Missing"
L16-1601,P98-1013,0,0.899228,"nt status, there are 98 frames, 662 frame-evoking words, 872 senses, and about 13000 annotated frames, with their semantic roles assigned to portions of text. The French FrameNet is freely available at alpage.inria.fr/asfalda. Keywords: FrameNet, French, semantic roles, semantic frames, semantically-annotated corpus 1. Introduction The ASFALDA project1 aims to build semantic resources and a corresponding semantic analyzer for French, to capture generalizations both over predicates and over the semantic arguments of predicates. We chose to build on the work resulting from the FrameNet project (Baker et al., 1998), which provides a structured set of prototypical situations, called frames, along with a semantic characterization of the participants of these situations (called frame elements, but we’ll use roles for short). The corresponding English lexicon associates frames with the words that can evoke them (called frame-evoking elements, FEEs for short). While other English semantic resources, such as PropBank (Palmer et al., 2005) or VerbNet (Schuler, 2005), also provide semantic classes and/or semantic roles for predicate arguments, we chose FrameNet mainly because of its more semantic orientation, w"
L16-1601,burchardt-etal-2006-salto,0,0.512718,"Missing"
L16-1601,burchardt-etal-2006-salsa,0,0.147107,"Missing"
L16-1601,F12-2024,1,0.923551,"4), this paper concentrates on the subsequent corpus annotation phase, which focused on four notional domains (commercial transactions, cognitive stances, causality and verbal communication). Given full coverage is not reachable for a relatively “new” FrameNet project, we advocate that focusing on specific notional domains allowed us to obtain full lexical coverage for the frames of these domains, while partially reflecting word sense ambiguities. Furthermore, as frames and roles were annotated on two French Treebanks (the French Treebank (Abeill´e and Barrier, 2004) and the Sequoia Treebank (Candito and Seddah, 2012), we were able to extract a syntactico-semantic lexicon from the annotated frames. In the resource’s current status, there are 98 frames, 662 frame-evoking words, 872 senses, and about 13000 annotated frames, with their semantic roles assigned to portions of text. The French FrameNet is freely available at alpage.inria.fr/asfalda. Keywords: FrameNet, French, semantic roles, semantic frames, semantically-annotated corpus 1. Introduction The ASFALDA project1 aims to build semantic resources and a corresponding semantic analyzer for French, to capture generalizations both over predicates and over"
L16-1601,candito-etal-2014-developing,1,0.556471,"Missing"
L16-1601,N10-1138,0,0.0355403,"enced that using the frame-by-frame strategy, i.e. working in isolation on a frame, can result in missing some similarities with other frames, and thus artificially increasing polysemy: choosing the right frame for a given lemma’s occurrence can become quite difficult due to blurred frontiers between frames. Finally, a crucial characteristic of the different strategies concerns the use of the resulting annotations as training data for semantic parsers: even though they are much more numerous, Berkeley FrameNet’s lexicographic annotations have proved much less useful than their full-text ones (Das et al., 2010), which have the crucial trait of preserving the natural sense and role-realization probabilistic distributions. 3.2. In the current paper, we describe the subsequent corpus annotation phase. We used as target corpus two syntacticallyannotated corpora (see below section 3.3.), and basically aimed to annotate all occurrences of lemmas that potentially evoke a frame of one of 4 notional domains (cf. section 4.1.). Once disambiguated, the FEE occurrence is either associated with one relevant frame, or with a special Other sense frame, to indicate the meaning is outside of the targetted notional d"
L16-1601,mouton-etal-2010-framenet,0,0.524386,"Missing"
L16-1601,J05-1004,0,0.280343,"for French, to capture generalizations both over predicates and over the semantic arguments of predicates. We chose to build on the work resulting from the FrameNet project (Baker et al., 1998), which provides a structured set of prototypical situations, called frames, along with a semantic characterization of the participants of these situations (called frame elements, but we’ll use roles for short). The corresponding English lexicon associates frames with the words that can evoke them (called frame-evoking elements, FEEs for short). While other English semantic resources, such as PropBank (Palmer et al., 2005) or VerbNet (Schuler, 2005), also provide semantic classes and/or semantic roles for predicate arguments, we chose FrameNet mainly because of its more semantic orientation, which is crucial for portability to other languages. FrameNet offers generalization over not only syntactic variation (e.g. diathesis alternation) but also lexical variation (like VerbNet but unlike PropBank), and groups together lexical units of various categories, on the basis of criteria that are not primarily syntactic (unlike VerbNet). The resources built within ASFALDA consist of a set of frames, a French lexicon in w"
L16-1601,W13-4917,1,0.867681,"Missing"
L16-1601,C98-1013,0,\N,Missing
L16-1603,abeille-barrier-2004-enriching,0,0.037665,"Missing"
L16-1603,P98-1013,0,0.446604,"of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguished a much larger set of causal relationships involving events as well as facts: for instance in FrameNet (henceforth FN) (Baker et al., 1998), some frames are concerned with argumentation, where typical causal expressions introduce evidence for a claim, or reasons for an agent’s behaviour. This is consistent with more psycholinguistically oriented studies, which clearly distinguish the expression of factual and epistemic causality (Spooren et al., 2010). Examples in (1), taken from FN, show relationships between “situations”, events or facts, with names specific to each frame, and which parallel cause-effect relationships (in bold, the lexical unit triggering the frame annotation, called a frame-evoking element in FN).1 (1) a. [Wha"
L16-1603,P08-2045,0,0.416835,"rbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguished a much larger set of ca"
L16-1603,burchardt-etal-2006-salto,0,0.083213,"Missing"
L16-1603,F12-2024,1,0.906389,"realizations of roles. FN later added full-text annotations to have a more representative coverage. We followed a similar path for the project, but aiming for a complete lexical coverage for causality: we first defined a lexicon of causal lexical units for French beforehand. The subsequent corpus annotation phase, performed within the ASFALDA French FrameNet project is described at length in (Djemaa et al., 2016) and concerns three other notional domains, on top of causality. We used two syntactically annotated corpus, the French Treebank (Abeill´e and Barrier, 2004) and the Sequoia treebank (Candito and Seddah, 2012), in which we isolated the occurrences of the lemmas of our causal LUs and had them disambiguated and 3809 Event Eventive_affecting Causation Reason Response Preventing Cause to start + Launch process (new) Cause enunciation Explaining the facts Contingency + Objective influence Transitive_action (new) Attributing cause Make possible to do Evidence Figure 1: Causal frame hierarchy; Grey nodes indicate frame changes from Framenet 1.5, either new, merged or redefined frames. Dotted relations are additions. Transitive action is an unlexicalised parent to more specific predicates. NB: Evidence is"
L16-1603,candito-etal-2014-developing,1,0.895062,"Missing"
L16-1603,W04-3205,0,0.0269633,"s also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not b"
L16-1603,L16-1601,1,0.887972,"usal lexical items with their corresponding semantic frames. The aim of our project is to have both the largest possible coverage of causal phenomena in French, across all parts of speech, and have it linked to a general semantic framework such as FN, to benefit in particular from the relations between other semantic frames, e.g., temporal ones or intentional ones, and the underlying upper lexical ontology that enables some forms of reasoning. This is part of the larger ASFALDA French FrameNet project, which focuses on a few different notional domains which are interesting in their own right (Djemaa et al., 2016), including cognitive positions and communication frames. In the process of building the French lexicon and preparing the annotation of the corpus, we had to remodel some of the frames proposed in FN based on English data, with hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself. Keywords: semantic role labelling, causality, FrameNet 1. Introduction A lot of information in"
L16-1603,D11-1027,0,0.203789,"went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distin"
L16-1603,W15-1622,0,0.220921,"ents or facts, with names specific to each frame, and which parallel cause-effect relationships (in bold, the lexical unit triggering the frame annotation, called a frame-evoking element in FN).1 (1) a. [What happened to these two chaps]support proves [the rumour is not true]proposition . E VIDENCE b. I like to think the main reason [we]Agent [have stayed together since the World Cup]action is [the great spirit within this Australian team]state of affairs . R EASON c. If [such a small earthquake]cause causes [problems]effect , just imagine a big one! C AUSATION Similar distinctions appear in (Dunietz et al., 2015), who annotated four kinds of causalities: consequence, which seems similar to the FN C AUSATION frame, motivation (similar to R EASON), purpose (similar to FN’s P URPOSE), and inference (similar to E VIDENCE). Unfortunately their effort was made without an explicit relation to an existing semantic lexicon, and has only a small dataset to support the framework. To our knowledge, FN is the semantic lexicon which has the largest coverage and the most detailed analysis of the expression of causality in English, equipped with annotation procedures.2 The detailed analysis relies 1 In the remaining"
L16-1603,S12-1052,0,0.0612495,"Missing"
L16-1603,D09-1122,0,0.071349,"Missing"
L16-1603,P00-1043,0,0.0792031,"h hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself. Keywords: semantic role labelling, causality, FrameNet 1. Introduction A lot of information in natural language is of a causal nature: relations between events, explanations, argumentations are all important to the understanding of texts, and thus useful in question-answering (Oh et al., 2013), information extraction (Khoo et al., 2000; nan Cao et al., 2014) or textual entailment (Gordon et al., 2012). Davidson, in his work on individuation of events (Davidson, 1969), went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-e"
L16-1603,W12-4107,0,0.0127929,"on et al., 2012). Davidson, in his work on individuation of events (Davidson, 1969), went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal in"
L16-1603,C14-1198,0,0.416049,"ween events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguis"
L16-1603,mouton-etal-2010-framenet,0,0.0495501,"Missing"
L16-1603,P13-1170,0,0.0402666,"Missing"
L16-1603,prasad-etal-2008-penn,0,0.230152,"mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguished a much larger set of causal relationships involving events as well as facts: for instance in FrameNet (henceforth FN) (Baker et al., 1998), some frames are concerned with argumentation, where typical causal expressions introduce evidence for a claim, or reasons for an agent’s behaviour. This is consistent with more psycholinguistically oriented studies, whic"
L16-1603,W13-4004,0,0.0168425,"nts (Davidson, 1969), went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008)"
L16-1603,W14-0707,0,0.0119504,"ents are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities,"
L16-1603,C08-1107,0,0.011006,"e Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which caus"
L16-1603,C98-1013,0,\N,Missing
N19-1351,J08-1001,0,0.0853217,"ngle out a narrow aspect of the entire semantic content. Unsupervised approaches have also been proposed, based on sentence distributions in large corpora in relation to their discourse context. For instance, Kiros et al. (2015) construct sentence representations by trying to reconstruct neighbouring sentences, which allows them to take into account different contextual aspects of sentence meaning. In the same vein, Logeswaran et al. (2016) propose to predict if two sentences are consecutive, even though such local coherence can be straightforwardly predicted with relatively shallow features (Barzilay and Lapata, 2008). A more elaborate setting is the prediction of the semantic or rhetorical relation between two sentences, as is the goal of discourse parsing. A number of annotated corpora exist, such as RST-DT (Carlson et al., 2001) and PDTB (Prasad et al., 2008), but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult. The problem, however, is much easier when there is a marker that makes the semantic link explicit (Pitler et al., 2008), and this observation has often been used in a semi-supervised setting to predict discourse relations in gene"
N19-1351,P18-2103,0,0.0166955,"tively small for the prediction of rare discourse markers. In this work we use web-scale data in order to explore the prediction of a wide range of discourse markers, with more balanced frequency distributions, along with application to sentence representation learning. We use English data for the experiments, but the same method could be applied to any language that bears a typological resemblance with regard to discourse usage, and has sufficient amounts of textual data available (e.g. German or French). Inspired by recent work (Dasgupta et al., 2018; Poliak et al., 2018; Levy et al., 2018; Glockner et al., 2018) on the unexpected properties of recent manually labelled datasets (e.g. SNLI), we will also analyze our dataset to check whether labels are easy to guess, and whether the proposed model architectures make use of high-level reasoning for their predictions. Our contributions are as follows: – we propose a simple and efficient method to discover new discourse markers, and present a curated list of 174 markers for English; – we provide evidence that many connectives can be predicted with only simple lexical features; – we investigate whether relation prediction actually makes use of the relation"
N19-1351,L18-1550,0,0.0623984,"Missing"
N19-1351,N18-2017,0,0.0225,"e training data, but when a wide range of markers is included, the rare ones still contribute up to millions of training instances. Out of the 42 single word PDTB markers that precede a comma, 31 were found by our rule. Some markers are missing because of NLTK errors, which mainly result from morphological issues.3 2.3 Controlling for shallow features As previously noted, some candidates discovered by our rule may not be actual discourse markers. In order to discard them, we put forward the hypothesis that actual discourse markers cannot be predicted with shallow lexical features. Inspired by Gururangan et al. (2018), we use a Fasttext classifier (Joulin et al., 2016) in order to predict c from s02 . The Fasttext classifier predicts labels from an average of word embeddings fed to a linear classifier. We split the dataset in 5 folds, and we predict markers for each fold, while training on the remaining folds. We use a single epoch, randomly initialized vectors of size 100 (that can be unigrams, bigrams or trigrams) and a learning rate of 0.5. In addition, we predict c from the concatenation of s1 and s02 (using separate word representations for each case). One might assume that the prediction of c in this"
N19-1351,D16-1020,0,0.133164,"Missing"
N19-1351,W01-1605,0,0.619251,"2015) construct sentence representations by trying to reconstruct neighbouring sentences, which allows them to take into account different contextual aspects of sentence meaning. In the same vein, Logeswaran et al. (2016) propose to predict if two sentences are consecutive, even though such local coherence can be straightforwardly predicted with relatively shallow features (Barzilay and Lapata, 2008). A more elaborate setting is the prediction of the semantic or rhetorical relation between two sentences, as is the goal of discourse parsing. A number of annotated corpora exist, such as RST-DT (Carlson et al., 2001) and PDTB (Prasad et al., 2008), but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult. The problem, however, is much easier when there is a marker that makes the semantic link explicit (Pitler et al., 2008), and this observation has often been used in a semi-supervised setting to predict discourse relations in general (Rutherford and Xue, 2015). Building on this observation, one approach to learn sentence representations is to predict such markers or clusters of markers explicitly (Jernite et al., 2017; Malmi et al., 2018; Nie e"
N19-1351,D17-1070,0,0.0707047,"cuments. Recently, numerous approaches have been proposed for the construction of vector-based representations for larger textual units, especially sentences. One of the most popular frameworks aims to induce sentence embeddings as an intermediate representation for predicting relations between sentence pairs. For instance, similarity judgements (paraphrases) or inference relations have been used as prediction tasks, and the resulting embeddings perform well in practice, even when 1 https://github.com/ synapse-developpement/Discovery the representations are transfered to other semantic tasks (Conneau et al., 2017). However, the kind of annotated data that is needed for such supervised approaches is costly to obtain, prone to bias, and arguably fairly limited with regard to the kind of semantic information captured, as they single out a narrow aspect of the entire semantic content. Unsupervised approaches have also been proposed, based on sentence distributions in large corpora in relation to their discourse context. For instance, Kiros et al. (2015) construct sentence representations by trying to reconstruct neighbouring sentences, which allows them to take into account different contextual aspects of"
N19-1351,P18-1198,0,0.0656821,"Missing"
N19-1351,L18-1260,0,0.179469,"DT (Carlson et al., 2001) and PDTB (Prasad et al., 2008), but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult. The problem, however, is much easier when there is a marker that makes the semantic link explicit (Pitler et al., 2008), and this observation has often been used in a semi-supervised setting to predict discourse relations in general (Rutherford and Xue, 2015). Building on this observation, one approach to learn sentence representations is to predict such markers or clusters of markers explicitly (Jernite et al., 2017; Malmi et al., 2018; Nie et al., 2017). Consider the following sentence pair: I live in Paris. But I’m often abroad. The discourse marker but highlights an opposition between the first sentence (the speaker 3477 Proceedings of NAACL-HLT 2019, pages 3477–3486 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics s1 c s2’ Paul Prudhomme’s Louisiana Kitchen created a sensation when it was published in 1984. happily, This family collective cookbook is just as good Table 1: Sample from our Discovery dataset lives in Paris) and the second sentence (the speaker is often abroad)"
N19-1351,D17-1169,0,0.0827916,"Missing"
N19-1351,P02-1047,0,0.140121,"ndably, gradually, or, ironically, namely, . . . classes markers 9 40 15 15 174 174 STRENGTH , RETURN , RECOGNIZE current work Table 2: Discourse markers or classes used by previous work on unsupervised representation learning but simpler than a full-fledged encoder-decoder a` la Skip-Thought. In that respect, discourse relations are an interesting compromise, if we can reliably extract them in large quantities. This objective is shared with semi-supervised approaches to discourse relation prediction, where automatically extracted explicit instances feed a model targetting implicit instances (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Rutherford and Xue, 2015). In this perspective, it is important to collect unambiguous instances of potential discourse markers. To do so, previous work used heuristics based on specific constructs, especially syntactic patterns for intra-sentential relations, based on a fixed list of manually collected discourse markers. Since we focus on sentence representations, we limit ourselves to discourse arguments that are wellformed sentences, thus also avoiding clause segmentation issues. Following a heuristic from Rutherford and Xue (2015)"
N19-1351,L18-1286,0,0.0294276,"Missing"
N19-1351,P09-2004,0,0.0660488,"arkers 9 40 15 15 174 174 STRENGTH , RETURN , RECOGNIZE current work Table 2: Discourse markers or classes used by previous work on unsupervised representation learning but simpler than a full-fledged encoder-decoder a` la Skip-Thought. In that respect, discourse relations are an interesting compromise, if we can reliably extract them in large quantities. This objective is shared with semi-supervised approaches to discourse relation prediction, where automatically extracted explicit instances feed a model targetting implicit instances (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Rutherford and Xue, 2015). In this perspective, it is important to collect unambiguous instances of potential discourse markers. To do so, previous work used heuristics based on specific constructs, especially syntactic patterns for intra-sentential relations, based on a fixed list of manually collected discourse markers. Since we focus on sentence representations, we limit ourselves to discourse arguments that are wellformed sentences, thus also avoiding clause segmentation issues. Following a heuristic from Rutherford and Xue (2015), also considered by Malmi et al. (2018) and Jernite et al"
N19-1351,E17-1027,0,0.0245895,"Missing"
N19-1351,C10-2172,0,0.110238,"max weights learned during the training phase can be interpreted as embeddings for the markers themselves, and used to visualize their relationships. Figure 2 shows a TSNE (van der Maaten and Hinton, 2008) plot of the markers’ representations. Proximity in the feature space seems to reflect semantic similarity (e.g. usuRelated work Though discourse marker prediction in itself is an interesting and useful task (Malmi et al., 2017), discourse markers have often been used as a training cue in order to improve implicit relation prediction (Marcu and Echihabi, 2001; Sporleder and Lascarides, 2005; Zhou et al., 2010; Braud and Denis, 2016). This approach has been extended to general representation learning by Jernite et al. (2017)—although with empirically unconvincing results, which might be attributed to an inappropriate training/evaluation set-up, or the use of a limited number of broad categories instead of actual discourse markers. Nie et al. (2017) used the more standard InferSent framework and obtained better results, although they were still outperformed by QuickThought (Logeswaran and Lee, 2018), 3483 InferSent SkipThought QuickThought DiscoveryBase DiscoveryHard Discovery10 DiscoveryAdv Discove"
N19-1351,C08-2022,0,0.076849,", even though such local coherence can be straightforwardly predicted with relatively shallow features (Barzilay and Lapata, 2008). A more elaborate setting is the prediction of the semantic or rhetorical relation between two sentences, as is the goal of discourse parsing. A number of annotated corpora exist, such as RST-DT (Carlson et al., 2001) and PDTB (Prasad et al., 2008), but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult. The problem, however, is much easier when there is a marker that makes the semantic link explicit (Pitler et al., 2008), and this observation has often been used in a semi-supervised setting to predict discourse relations in general (Rutherford and Xue, 2015). Building on this observation, one approach to learn sentence representations is to predict such markers or clusters of markers explicitly (Jernite et al., 2017; Malmi et al., 2018; Nie et al., 2017). Consider the following sentence pair: I live in Paris. But I’m often abroad. The discourse marker but highlights an opposition between the first sentence (the speaker 3477 Proceedings of NAACL-HLT 2019, pages 3477–3486 c Minneapolis, Minnesota, June 2 - June"
N19-1351,S18-2023,0,0.0366509,"Missing"
N19-1351,prasad-etal-2008-penn,0,0.254446,"ntations by trying to reconstruct neighbouring sentences, which allows them to take into account different contextual aspects of sentence meaning. In the same vein, Logeswaran et al. (2016) propose to predict if two sentences are consecutive, even though such local coherence can be straightforwardly predicted with relatively shallow features (Barzilay and Lapata, 2008). A more elaborate setting is the prediction of the semantic or rhetorical relation between two sentences, as is the goal of discourse parsing. A number of annotated corpora exist, such as RST-DT (Carlson et al., 2001) and PDTB (Prasad et al., 2008), but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult. The problem, however, is much easier when there is a marker that makes the semantic link explicit (Pitler et al., 2008), and this observation has often been used in a semi-supervised setting to predict discourse relations in general (Rutherford and Xue, 2015). Building on this observation, one approach to learn sentence representations is to predict such markers or clusters of markers explicitly (Jernite et al., 2017; Malmi et al., 2018; Nie et al., 2017). Consider the foll"
N19-1351,N15-1081,0,0.124494,"ore elaborate setting is the prediction of the semantic or rhetorical relation between two sentences, as is the goal of discourse parsing. A number of annotated corpora exist, such as RST-DT (Carlson et al., 2001) and PDTB (Prasad et al., 2008), but in general the available data is fairly limited, and the task of discourse relation prediction is rather difficult. The problem, however, is much easier when there is a marker that makes the semantic link explicit (Pitler et al., 2008), and this observation has often been used in a semi-supervised setting to predict discourse relations in general (Rutherford and Xue, 2015). Building on this observation, one approach to learn sentence representations is to predict such markers or clusters of markers explicitly (Jernite et al., 2017; Malmi et al., 2018; Nie et al., 2017). Consider the following sentence pair: I live in Paris. But I’m often abroad. The discourse marker but highlights an opposition between the first sentence (the speaker 3477 Proceedings of NAACL-HLT 2019, pages 3477–3486 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics s1 c s2’ Paul Prudhomme’s Louisiana Kitchen created a sensation when it was publish"
P14-1045,2002.jeptalnrecital-long.5,0,0.0315914,"evaluation and improvement of distributional resources (section 4). 2 2.1 Figure 1: Histogram of Lin scores for pairs considered. To ease the use of lexical neighbours in our experiments, we merged together predicates that include the same lexical unit, a posteriori. Thus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue1 . Evaluation of lexical similarity in context Data We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. In this approach, contexts are triples (governor,relation,dependent) derived from syntactic dependency structures. Governors and dependents are verbs, adjectives and nouns. Multiword units are available, but they form a very small subset of the resulting neighbours. Base elements in the thesaurus are of two types: arguments (dependents’ lemma) and predicates (governor+relation). This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a"
P14-1045,J90-1003,0,0.356366,"Missing"
P14-1045,W02-0908,0,0.00892942,"work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the constru"
P14-1045,N09-1003,0,0.0268222,"Missing"
P14-1045,S13-1004,0,0.0337042,"hat the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the simi486 larity score and the ranks of items as neighbours of other items. This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important: lexical disambiguation (Miller et al., 2012), topic segmentation (Guinaudeau et al., 2012). This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level (Mihalcea et al., 2006) or other macrotextual level (Agirre et al., 2013), since these are always aggregation functions of word similarities. There are limits to what is presented here: we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built. Our starting corpus is relatively small compared to current efforts in this framework. We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and th"
P14-1045,P13-1055,0,0.0224196,"Missing"
P14-1045,J10-4006,0,0.145992,"is applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distributional similarities is"
P14-1045,W05-0604,0,0.0187458,"are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in. Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existent"
P14-1045,W11-2501,0,0.110605,"trinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical"
P14-1045,H92-1045,0,0.616255,"on of lexical pairs: annotators focus on a target item (here corne, horn, in blue) and must judge yellow words (pending: oreille/queue, ear/tail), either validating their relevance (green words: pattes, legs) or rejecting them (red words: herbe, grass). The text describes the morphology of the impala, and its habitat. neighbours occurred. They were asked to judge the relation between two items types, regardless of the number of occurrences in the text. This time there was no filtering of the lexical pairs beyond the 0.1 threshold of the original resource. We followed the well-known postulate (Gale et al., 1992) that all occurrences of a word in the same discourse tend to have the same sense (“one sense per discourse”), in order to decrease the annotator workload. We also assumed that the relation between these items remain stable within the document, an arguably strong hypothesis that needed to be checked against inter-annotator agreement before beginning the final annotation . It turns out that the kappa score (0.80) shows a better interannotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrence"
P14-1045,J05-4002,0,0.0248207,"hesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is e"
P14-1045,H94-1018,0,0.0119488,"dam CLLE, Toulouse University CLLE, Toulouse University Universit´e Toulouse-Le Mirail Universit´e Toulouse-Le Mirail 5 alles A. Machado 5 alles A. Machado 31058 Toulouse Cedex 31058 Toulouse Cedex cecile.fabre@univ-tlse2.fr clementine.adam@univ-tlse2.fr Abstract its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata"
P14-1045,J09-3004,0,0.0488351,"Missing"
P14-1045,C12-1109,0,0.0223035,"which combines global features from the corpus used to built a distributional thesaurus and local features from the text where similarities are to be judged as relevant or not to the coherence of a document. It seems from these experiments that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the simi486 larity score and the ranks of items as neighbours of other items. This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important: lexical disambiguation (Miller et al., 2012), topic segmentation (Guinaudeau et al., 2012). This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level (Mihalcea et al., 2006) or other macrotextual level (Agirre et al., 2013), since these are always aggregation functions of word similarities. There are limits to what is presented here: we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built. Our starting corpus is relatively"
P14-1045,W04-2607,0,0.0609169,"Missing"
P14-1045,J07-2002,0,0.0600259,"s (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined in (Baroni and Lenci, 2011). Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard (Weeds, 2003; Bordag, 2008; Anguiano et al., 2011). One advantage of distr"
P14-1045,poibeau-messiant-2008-still,0,0.0279876,"use-Le Mirail Universit´e Toulouse-Le Mirail 5 alles A. Machado 5 alles A. Machado 31058 Toulouse Cedex 31058 Toulouse Cedex cecile.fabre@univ-tlse2.fr clementine.adam@univ-tlse2.fr Abstract its evaluation is far from obvious. A distributional thesaurus includes a lot of “noise” from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy. There is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations (Jones, 1994), and this applies to distributional thesauri (Curran, 2004; Poibeau and Messiant, 2008). Extrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval (van der Plas, 2008) or word sense disambiguation (Weeds and Weir, 2005). Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects (Pado and Lapata, 2007; Baroni and Lenci, 2010). The shortcomings of these methods have been underlined"
P14-1045,sahlgren-2006-towards,0,0.0166027,"bours of a given target (Ferret, 2013). They still use “essential” evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate. We are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in. Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2"
P14-1045,C08-1114,0,0.0320916,"evaluate differently semantic neighbours according to the context they appear in. Our work is related to two issues: evaluating distributional resources, and improving them. Evaluating distributional resources is the subject of a lot of methodological reflection (Sahlgren, 2006), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. In extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource: either discriminative, TOEFL-like tests (Freitag et al., 2005), analogy production (Turney, 2008), or synonym selection (Weeds, 2003; Anguiano et al., 2011; Ferret, 2013; Curran and Moens, 2002). In intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset (Finkelstein et al., 2002), e.g. (Pado and Lapata, 2007; Agirre et al., 2009), or specifically designed test cases, as in (Baroni and Lenci, 2011). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an “existential” view of semantic relatedness."
S13-2017,D10-1115,0,0.070771,"tem for the SemEval 2013 Task 5a: semantic similarity of words and Related work In recent years, a number of methods have been developed that try to capture the compositional meaning of units beyond the individual word level within a distributional framework. One of the first approaches to tackle compositional phenomena in a systematic way is Mitchell and Lapata’s (2008) approach. They explore a number of different models for vector composition, of which vector addition (the sum of each feature) and vector multiplication (the elementwise multiplication of each feature) are the most important. Baroni and Zamparelli (2010) present a method for the composition of adjectives and nouns. In their model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different 98 Second Joint Confe"
S13-2017,D10-1113,0,0.0218391,"the different 98 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 98–102, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics word features. And Socher et al. (2012) present a model for compositionality based on recursive neural networks. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Our work takes the latter approach of computing word meaning in context, and is described in detail below. 3 Methodology Our method uses latent vector weighting (Van de Cruys et al., 2011) in order to compute a semantic representation for the meaning of a word within a particular context. The method relies upon a factorization model in which words, together with their window-based"
S13-2017,D08-1094,0,0.0896542,"Missing"
S13-2017,W09-0208,0,0.0403485,"Missing"
S13-2017,nivre-etal-2006-maltparser,0,0.0319155,". We can now return to our original matrix A and compute the top similar words for the two adapted vectors of instrument given the different contexts, which yields the results presented below. 1. instrumentN , C1 : percussion, flute, violin, melody, harp 2. instrumentN , C2 : sensor, detector, amplifier, device, microscope 3.4 Implementational details Our model has been trained on the UKWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1 , so that dependency triples could be extracted. The matrices needed for our interleaved NMF factorization are extracted from the corpus. Our model was built using 5K nouns, 80K dependency relations, and 2K context words2 (excluding stop words) with highest frequency in the training set, which yields matrices of 5K nouns × 80K dependency relations, and 5K nouns × 2K context words. 1 http://maltparser.org/mco/english_parser/ engmalt.html 2 We used a fairly"
S13-2017,D12-1110,0,0.0461144,"ective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different 98 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 98–102, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics word features. And Socher et al. (2012) present a model for compositionality based on recursive neural networks. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Our work takes the latter approach of computing word meaning in conte"
S13-2017,W00-1308,0,0.0810629,"Missing"
S13-2017,N03-1033,0,0.0604774,"context in which the target word appears (equation 8). We can now return to our original matrix A and compute the top similar words for the two adapted vectors of instrument given the different contexts, which yields the results presented below. 1. instrumentN , C1 : percussion, flute, violin, melody, harp 2. instrumentN , C2 : sensor, detector, amplifier, device, microscope 3.4 Implementational details Our model has been trained on the UKWaC corpus (Baroni et al., 2009). The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003), and parsed with MaltParser (Nivre et al., 2006) trained on sections 2-21 of the Wall Street Journal section of the Penn Treebank extended with about 4000 questions from the QuestionBank1 , so that dependency triples could be extracted. The matrices needed for our interleaved NMF factorization are extracted from the corpus. Our model was built using 5K nouns, 80K dependency relations, and 2K context words2 (excluding stop words) with highest frequency in the training set, which yields matrices of 5K nouns × 80K dependency relations, and 5K nouns × 2K context words. 1 http://maltparser.org/mco"
S13-2017,D11-1094,1,0.903811,"Missing"
S13-2017,C08-1117,1,0.891455,"Missing"
S13-2017,P08-1028,0,\N,Missing
S13-2026,W09-2416,0,0.14615,"Missing"
S13-2026,J90-1003,0,0.406971,"sentation is then used by a maximum entropy classifier to induce a number of appropriate paraphrases. 2 2.1 Methodology Distributional word space model In order to induce appropriate feature representations for the various noun compounds, we start by constructing a standard distributional word space model for nouns. We construct a co-occurrence matrix of the 5K most frequent nouns1 by the 2K most frequent context words2 , which occur in a window of 5 words to the left and right of the target word. The bare frequencies of the word-context matrix are weighted using pointwise mutual information (Church and Hanks, 1990). Next, we compute a joint, compositional representation of the noun compound, combining the se1 making sure all nouns that appear in the training and test set are included 2 excluding the 50 most frequent context words as stop words 144 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 144–147, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics mantics of the head noun with the modifier noun. To do so, we make use of a simple vector-based multiplicative"
S13-2026,S13-2025,0,0.0978074,"Missing"
S13-2026,S10-1051,0,0.277923,"r is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the number of times they co-occurred with other paraphrases for other compounds. They use these co-occurrences to compute conditional probabilities estimating is-a relations between paraphrases. Li et al. (2010) provide a hybrid system which combines a Bayesian algorithm exploiting Google n-grams, a score which captures human preferences at the tail distribution of the training data, as well as a metric that captures pairwise paraphrase preferences. Our methodology consists of two steps. First, an unsupervised distributional word space model is constructed, which yields a feature representation for a particular compound. The feature representation is then used by a maximum entropy classifier to induce a number of appropriate paraphrases. 2 2.1 Methodology Distributional word space model In order to i"
S13-2026,S10-1052,0,0.107451,"xisting work either postulates a fixed set of relations (Tratz and Hovy, 2010) or relies on appropriate descriptions of the relations, through constrained verbal paraphrases (Butnariu et al., 2010) or unconstrained paraphrases as in the present campaign. The latter is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the number of times they co-occurred with other paraphrases for other compounds. They use these co-occurrences to compute conditional probabilities estimating is-a relations between paraphrases. Li et al. (2010) provide a hybrid system which combines a Bayesian algorithm exploiting Google n-grams, a score which captures human preferences at the tail distribution of the training data, as well as a metric that captures pairwise paraphrase preferences. Our methodology consists of two steps. First, an unsupervised distributional word space model is const"
S13-2026,W00-1308,0,0.141384,"Missing"
S13-2026,N03-1033,0,0.0832567,"Missing"
S13-2026,P10-1070,0,0.140273,"istributional word space model with a supervised maximum-entropy classification model; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classifier to induce a number of appropriate paraphrases. 1 Introduction Interpretation of noun compounds is making explicit the relation between the component nouns, for instance that running shoes are shoes used in running activities, while leather shoes are made from leather. The relations can have very different meanings, and existing work either postulates a fixed set of relations (Tratz and Hovy, 2010) or relies on appropriate descriptions of the relations, through constrained verbal paraphrases (Butnariu et al., 2010) or unconstrained paraphrases as in the present campaign. The latter is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the numbe"
S13-2026,S10-1058,0,0.387287,"shoes used in running activities, while leather shoes are made from leather. The relations can have very different meanings, and existing work either postulates a fixed set of relations (Tratz and Hovy, 2010) or relies on appropriate descriptions of the relations, through constrained verbal paraphrases (Butnariu et al., 2010) or unconstrained paraphrases as in the present campaign. The latter is much simpler for annotation purposes, but raises difficult challenges involving not only compound interpretation but also paraphrase evaluation and ranking. In terms of constrained verbal paraphrases Wubben (2010), for example, uses a supervised memory-based ranker using features from the Google n-gram corpus as well as WordNet. Nulty and Costello (2010) rank paraphrases of compounds according to the number of times they co-occurred with other paraphrases for other compounds. They use these co-occurrences to compute conditional probabilities estimating is-a relations between paraphrases. Li et al. (2010) provide a hybrid system which combines a Bayesian algorithm exploiting Google n-grams, a score which captures human preferences at the tail distribution of the training data, as well as a metric that c"
S13-2026,P08-1028,0,\N,Missing
S13-2026,S10-1007,0,\N,Missing
S19-1004,P14-1113,0,0.0362513,"rnative composition function (fCβ ,− ) during evaluation. AVG denotes the average of the three tasks. tions. Subsequent approaches use relational tasks for training and evaluation on specific datasets (Conneau et al., 2017; Nie et al., 2017). text fragments, even though they ignored SRL and only dealt with word units. Word2vec (Mikolov et al., 2013) has sparked a great interest for this task with word analogies in the latent space. Levy & Goldberg (2014) explored different scoring functions between words, notably for analogies. Hypernymy relations were also studied, by Chang et al. (2018) and Fu et al. (2014). Levy et al. (2015) proposed tailored scoring functions. Even the skipgram model (Mikolov et al., 2013) can be formulated as finding relations between context and target words. We did not empirically explore textual relational learning at the word level, but we believe that it would fit in our framework, and could be tested in future studies. Numerous approaches (Chen et al., 2017b; Seok et al., 2016; Gong et al., 2018; Joshi et al., 2019) were proposed to predict inference relations between sentences, but don’t explicitely use sentence embeddings. Instead, they encode sentences jointly, poss"
S19-1004,D15-1075,0,0.0452371,"sentence embeddings. Tasks include sentiment analysis, entailment, textual similarity, textual relatedness, and paraphrase detection. These tasks are a rich way to train or evaluate sentence representations since in a triple (s1 , R, s2 ), we can see (R, s2 ) as a label for s1 (Baudiˇs et al., 2016). Unfortunately, the relational tasks hard-code the composition function 6.1 Training tasks Natural language inference (T = NLI)’s goal is to predict whether the relation between two sentences (premise and hypothesis) is Entailment, Contradiction or Neutral. We use the combination of SNLI dataset (Bowman et al., 2015) and MNLI dataset (Williams et al., 2018). We call AllNLI the resulting dataset of 1M examples. Conneau et al. (2017) claim that NLI data allows universal sentence representation learning. They used the f ,− composition function with concatenated sentence representations in order to train their Infersent model. 37 name N task C representation(s) used MR SUBJ MPQA TREC SICKm s MRPCm s PDTBm s STS14 11k 10k 11k 6k 10k 4k 17k 4.5k sentiment (movies) subjectivity/objectivity opinion polarity question-type NLI paraphrase detection discursive relation similarity 2 2 2 6 3 2 5 - h1 h1 h1 h1 fm,s (h1"
S19-1004,P18-2103,0,0.0245997,"Missing"
S19-1004,N18-1045,0,0.0209564,"tion tasks using an alternative composition function (fCβ ,− ) during evaluation. AVG denotes the average of the three tasks. tions. Subsequent approaches use relational tasks for training and evaluation on specific datasets (Conneau et al., 2017; Nie et al., 2017). text fragments, even though they ignored SRL and only dealt with word units. Word2vec (Mikolov et al., 2013) has sparked a great interest for this task with word analogies in the latent space. Levy & Goldberg (2014) explored different scoring functions between words, notably for analogies. Hypernymy relations were also studied, by Chang et al. (2018) and Fu et al. (2014). Levy et al. (2015) proposed tailored scoring functions. Even the skipgram model (Mikolov et al., 2013) can be formulated as finding relations between context and target words. We did not empirically explore textual relational learning at the word level, but we believe that it would fit in our framework, and could be tested in future studies. Numerous approaches (Chen et al., 2017b; Seok et al., 2016; Gong et al., 2018; Joshi et al., 2019) were proposed to predict inference relations between sentences, but don’t explicitely use sentence embeddings. Instead, they encode se"
S19-1004,N18-2017,0,0.0241436,"Missing"
S19-1004,P17-1152,0,0.052618,"Missing"
S19-1004,N19-1362,0,0.0200865,"& Goldberg (2014) explored different scoring functions between words, notably for analogies. Hypernymy relations were also studied, by Chang et al. (2018) and Fu et al. (2014). Levy et al. (2015) proposed tailored scoring functions. Even the skipgram model (Mikolov et al., 2013) can be formulated as finding relations between context and target words. We did not empirically explore textual relational learning at the word level, but we believe that it would fit in our framework, and could be tested in future studies. Numerous approaches (Chen et al., 2017b; Seok et al., 2016; Gong et al., 2018; Joshi et al., 2019) were proposed to predict inference relations between sentences, but don’t explicitely use sentence embeddings. Instead, they encode sentences jointly, possibly with the help of previously cited word compositions, therefore it would also be interesting to try applying our techniques within their framework. 8 Conclusion We have demonstrated that a number of existing models used for textual relational learning rely on composition functions that are already used in Statistical Relational Learning. By taking into account previous insights from SRL, we proposed new composition functions and evaluat"
S19-1004,P16-2022,0,0.0454332,"Missing"
S19-1004,W14-1618,0,0.0382657,"5.9 75.8 76.2 76.2 76.2 47.2 47.1 47 47.8 47.6 47.8 86.9 86.9 87 86.8 87.3 87 70.1 70 69.9 70.3 70.4 70.3 Table 6: Results for sentence relation tasks using an alternative composition function (fCβ ,− ) during evaluation. AVG denotes the average of the three tasks. tions. Subsequent approaches use relational tasks for training and evaluation on specific datasets (Conneau et al., 2017; Nie et al., 2017). text fragments, even though they ignored SRL and only dealt with word units. Word2vec (Mikolov et al., 2013) has sparked a great interest for this task with word analogies in the latent space. Levy & Goldberg (2014) explored different scoring functions between words, notably for analogies. Hypernymy relations were also studied, by Chang et al. (2018) and Fu et al. (2014). Levy et al. (2015) proposed tailored scoring functions. Even the skipgram model (Mikolov et al., 2013) can be formulated as finding relations between context and target words. We did not empirically explore textual relational learning at the word level, but we believe that it would fit in our framework, and could be tested in future studies. Numerous approaches (Chen et al., 2017b; Seok et al., 2016; Gong et al., 2018; Joshi et al., 201"
S19-1004,N15-1098,0,0.0706402,"Missing"
S19-1004,P09-1077,0,0.0533582,"ng learning rate until convergence. The only difference with regard to Infersent is the composition. Sentences are composed with six different compositions for training according to the following template: Evaluation tasks Table 2 provides an overview of different transfer tasks that will be used for evaluation. We added another relation prediction task, the PDTB coarsegrained implicit discourse relation task, to SentEval. This task involves predicting a discursive link between two sentences among {Comparison, Contingency, Entity based coherence, Expansion, Temporal}. We followed the setup of Pitler et al. (2009), without sampling negative examples in training. MRPC, PDTB and SICK will be tested with two composition functions: besides SentEval composition f ,− , we will use fC β ,− for transfer learning evaluation, since it has the most general multiplicative interaction and it does not penalize models that do not learn a translation. For all tasks except STS14, a cross-validated logistic regression is used on the sentence or relation representation. The evaluation of the STS14 task relies on Pearson or Spearman correlation between cosine similarity and the target. We force the composition function to"
S19-1004,S18-2023,0,0.0243268,"Missing"
S19-1004,L18-1260,0,0.0509667,"Missing"
S19-1004,N18-1101,0,0.112861,", h2 ) (fm,s (h1 , h2 ) + (fm,s (h2 , h1 ))/2 fm,s (h1 , h2 ) cos(h1 , h2 ) Table 2: Transfer evaluation tasks. N = number of training examples; C = number of classes if applicable. h1 , h2 are sentence representations, fm,s a composition function from section 4. We also train on the prediction of discourse connectives between sentences/clauses (T = Disc). Discourse connectives make discourse relations between sentences explicit. In the sentence I live in Paris but I’m often elsewhere, the word but highlights that there is a contrast between the two clauses it connects. We use Malmi et al.’s (2018) dataset of selected 400k instances with 20 discourse connectives (e.g. however, for example) with the provided train/dev/test split. This dataset has no other supervision than the list of 20 connectives. Nie et al. (2017) used f ,− concatenated with the sum of sentence representations to train their model, DisSent, on a similar task and showed that their encoder was general enough to perform well on SentEval tasks. They use a dataset that is, at the time of writing, not publicly available. task since paraphrase detection should be invariant to permutation of input sentences. 6.2 fm,s,1,2 (h1"
S19-1004,P15-1150,0,\N,Missing
S19-1004,hooda-kosseim-2017-argument,0,\N,Missing
tannier-muller-2008-evaluation,M95-1005,0,\N,Missing
tannier-muller-2008-evaluation,P00-1010,0,\N,Missing
tannier-muller-2008-evaluation,S07-1014,0,\N,Missing
tannier-muller-2008-evaluation,C04-1008,1,\N,Missing
W06-3811,J02-2001,0,0.03705,"coverage, and the availability of such resources, in general and also in specialised domains. We present here a method exploiting such a graph structure to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 1 Introduction Thesaurus are an important resource in many natural language processing tasks. They are used to help information retrieval (Zukerman et al., 2003), machine or semi-automated translation, (Ploux and Ji, 2003; Barzilay and McKeown, 2001; Edmonds and Hirst, 2002) or generation (Langkilde and Knight, 1998). Since the gathering of such lexical information is a delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions. Synonym extraction suffers from a variety of methodological problems, however. Synonymy itself is not an easily definable notion. Totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about nearsynonyms (Edmonds and Hirst, 2002). A nearsynonym is a word that can be used instead of another one, in some contexts, without"
W06-3811,W05-0604,0,0.0906816,"equency of words in a corpus of ten years of the journal ""Le Monde"". The 50 words picked out in our sample have an average frequency of 2000 occurrences, while when we consider all our about 430 candidates for synonymy, the average frequency is 5300. Among the methods proposed to collect synonymy information, two families can be distinguished according to the input they consider. Either a general dictionary is used (or more than one (Wu and Zhou, 2003)), or a corpus of unconstrained texts from which lexical distributions are computed (simple collocations or syntactic dependencies) (Lin, 1998; Freitag et al., 2005) . The approach of (Barzilay and McKeown, 2001) uses a related kind of resource: multiple translations of the same text, with additional constraints on availability, and problems of text alignment, for only a third of the results being synonyms (when compared to Wordnet). The main conclusion to draw here is that our method is able to recover a lot of synonyms that are in the definition of words, and some in definitions not directly related, which seems to be an improvement on previous attempts from dictionaries. There is some arbitrariness in the method that should be further investigated (the"
W06-3811,C04-1173,1,0.863464,"Missing"
W06-3811,E93-1028,0,0.0606376,"at is hopefully not too noisy. A few studies have tried to use the lexical information available in a general dictionary and find patterns that would indicate synonymy relations (Blon65 Workshop on TextGraphs, at HLT-NAACL 2006, pages 65–72, c New York City, June 2006. 2006 Association for Computational Linguistics del et al., 2004; Ho and Cédrick, 2004). The general idea is that words are related by the definition they appear in, in a complex network that must be semantic in nature (this has been also applied to word sense disambiguation, albeit with limited success (Veronis and Ide, 1990; H.Kozima and Furugori, 1993)). We present here a method exploiting the graph structure of a dictionary, where words are related by the definition they appear in, to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 2 Semantic distance on a dictionary graph We describe here our method (dubbed Prox) to compute a distance between nodes in a graph. Basically, nodes are derived from entries in the dictionary or words appearing in definitions, and there are edges between an entry and the"
W06-3811,P98-1116,0,0.0208124,"resources, in general and also in specialised domains. We present here a method exploiting such a graph structure to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 1 Introduction Thesaurus are an important resource in many natural language processing tasks. They are used to help information retrieval (Zukerman et al., 2003), machine or semi-automated translation, (Ploux and Ji, 2003; Barzilay and McKeown, 2001; Edmonds and Hirst, 2002) or generation (Langkilde and Knight, 1998). Since the gathering of such lexical information is a delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions. Synonym extraction suffers from a variety of methodological problems, however. Synonymy itself is not an easily definable notion. Totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about nearsynonyms (Edmonds and Hirst, 2002). A nearsynonym is a word that can be used instead of another one, in some contexts, without too much change in meaning. This leaves of"
W06-3811,P98-2127,0,0.425462,"average frequency of words in a corpus of ten years of the journal ""Le Monde"". The 50 words picked out in our sample have an average frequency of 2000 occurrences, while when we consider all our about 430 candidates for synonymy, the average frequency is 5300. Among the methods proposed to collect synonymy information, two families can be distinguished according to the input they consider. Either a general dictionary is used (or more than one (Wu and Zhou, 2003)), or a corpus of unconstrained texts from which lexical distributions are computed (simple collocations or syntactic dependencies) (Lin, 1998; Freitag et al., 2005) . The approach of (Barzilay and McKeown, 2001) uses a related kind of resource: multiple translations of the same text, with additional constraints on availability, and problems of text alignment, for only a third of the results being synonyms (when compared to Wordnet). The main conclusion to draw here is that our method is able to recover a lot of synonyms that are in the definition of words, and some in definitions not directly related, which seems to be an improvement on previous attempts from dictionaries. There is some arbitrariness in the method that should be fu"
W06-3811,C04-1162,0,0.0592964,"Missing"
W06-3811,C94-1049,0,0.0783861,"Missing"
W06-3811,J03-2001,0,0.0156665,"antage of using a general dictionary lies in the coverage, and the availability of such resources, in general and also in specialised domains. We present here a method exploiting such a graph structure to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 1 Introduction Thesaurus are an important resource in many natural language processing tasks. They are used to help information retrieval (Zukerman et al., 2003), machine or semi-automated translation, (Ploux and Ji, 2003; Barzilay and McKeown, 2001; Edmonds and Hirst, 2002) or generation (Langkilde and Knight, 1998). Since the gathering of such lexical information is a delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions. Synonym extraction suffers from a variety of methodological problems, however. Synonymy itself is not an easily definable notion. Totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about nearsynonyms (Edmonds and Hirst, 2002). A nearsynonym is a word that can be"
W06-3811,C90-2067,0,0.123005,"in a set of proposals that is hopefully not too noisy. A few studies have tried to use the lexical information available in a general dictionary and find patterns that would indicate synonymy relations (Blon65 Workshop on TextGraphs, at HLT-NAACL 2006, pages 65–72, c New York City, June 2006. 2006 Association for Computational Linguistics del et al., 2004; Ho and Cédrick, 2004). The general idea is that words are related by the definition they appear in, in a complex network that must be semantic in nature (this has been also applied to word sense disambiguation, albeit with limited success (Veronis and Ide, 1990; H.Kozima and Furugori, 1993)). We present here a method exploiting the graph structure of a dictionary, where words are related by the definition they appear in, to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 2 Semantic distance on a dictionary graph We describe here our method (dubbed Prox) to compute a distance between nodes in a graph. Basically, nodes are derived from entries in the dictionary or words appearing in definitions, and there are"
W06-3811,C04-1146,0,0.05164,"Missing"
W06-3811,W03-1613,0,0.0117086,"in, in a complex network of an arguably semantic nature. The advantage of using a general dictionary lies in the coverage, and the availability of such resources, in general and also in specialised domains. We present here a method exploiting such a graph structure to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon. 1 Introduction Thesaurus are an important resource in many natural language processing tasks. They are used to help information retrieval (Zukerman et al., 2003), machine or semi-automated translation, (Ploux and Ji, 2003; Barzilay and McKeown, 2001; Edmonds and Hirst, 2002) or generation (Langkilde and Knight, 1998). Since the gathering of such lexical information is a delicate and time-consuming endeavour, some effort has been devoted to the automatic building of sets of synonyms words or expressions. Synonym extraction suffers from a variety of methodological problems, however. Synonymy itself is not an easily definable notion. Totally equivalent words (in meaning and use) arguably do not exist, and some people prefer to talk about nearsynonyms (Ed"
W06-3811,P01-1008,0,\N,Missing
W06-3811,C98-1112,0,\N,Missing
W06-3811,W03-1610,0,\N,Missing
W06-3811,C98-2122,0,\N,Missing
W13-4002,C12-1115,1,0.803907,"Missing"
W13-4002,P84-1085,0,0.461615,"scope has great semantic impact on the phenomena we have mentioned, in exactly the way the relative scope of quantifiers make a great semantic difference in first order logic. By concentrating on exact meaning representations, however, the syntax-semantics interface becomes quite complex: as happens with quantifiers at the intra sentential level, discourse relations might semantically require a scope that is, at least a priori, not determined by syntactic considerations alone and violates surface order (see s2 ). Other theories like Polanyi’s Linguistic Discourse Model (LDM) of Polanyi 1985; Polanyi and Scha 1984, and DLTAG Webber et al. 1999 explicitly adopt a syntactic point of view, and RST with strongly constrained (tree-shaped) structures is subject to parsing approaches duVerle and Prendinger 2009; Sagae 2009; Subba and Di Eugenio 2009 that adhere to the syntactic approach in adopting decoding strategies of syntactic parsing. In such theories, discourse structure representations, subject to syntactic constraints (e.g. dominance of spans of text one over another) respect surface order but do not always and unproblematically yield a semantic interpretation that fits intuitions. According to Marcu"
W13-4002,W04-2322,0,0.156207,"Missing"
W13-4002,prasad-etal-2008-penn,0,0.32011,"Missing"
W13-4002,W09-3813,0,0.0787263,"entations, however, the syntax-semantics interface becomes quite complex: as happens with quantifiers at the intra sentential level, discourse relations might semantically require a scope that is, at least a priori, not determined by syntactic considerations alone and violates surface order (see s2 ). Other theories like Polanyi’s Linguistic Discourse Model (LDM) of Polanyi 1985; Polanyi and Scha 1984, and DLTAG Webber et al. 1999 explicitly adopt a syntactic point of view, and RST with strongly constrained (tree-shaped) structures is subject to parsing approaches duVerle and Prendinger 2009; Sagae 2009; Subba and Di Eugenio 2009 that adhere to the syntactic approach in adopting decoding strategies of syntactic parsing. In such theories, discourse structure representations, subject to syntactic constraints (e.g. dominance of spans of text one over another) respect surface order but do not always and unproblematically yield a semantic interpretation that fits intuitions. According to Marcu 1996, an RST tree is not by itself sufficient to generate desired predictions; he employs the nuclearity principle, NP, as an additional interpretation principle on scopes of relations. We focus on two theo"
W13-4002,W04-0213,0,0.0363448,"urse structure representations, subject to syntactic constraints (e.g. dominance of spans of text one over another) respect surface order but do not always and unproblematically yield a semantic interpretation that fits intuitions. According to Marcu 1996, an RST tree is not by itself sufficient to generate desired predictions; he employs the nuclearity principle, NP, as an additional interpretation principle on scopes of relations. We focus on two theories: RST, which offers the model for the annotations of the RST treebank Carlson, Marcu, and Okurowski 2002 and the Potsdam commentary corpus Stede 2004, and on SDRT, which counts several small corpora annotated with semantic scopes, Discor Baldridge, Asher, and Hunter 2007 and Annodis Afantenos et al. 2012. We describe these theories in section 2. We will also compare these two theories to dependency tree representations of discourse Muller et al. 2012. Section 3 introduces a language for describing semantics scopes of relations that is powerful enough to: i) compare the expressiveness (in terms of what different scopes can be expressed) of the different formalisms considered; ii) give a formal target language that will provide comparable in"
W13-4002,N09-1064,0,0.0280014,"Missing"
W13-4002,P99-1006,0,0.159306,"Missing"
W13-4002,J05-2005,0,0.18991,"tative tasks. There is also some agreement over the taxonomy of discourse relations —almost all current theories include expressions that refer to relations like Elaboration, Explanation, Result, Narration, Contrast, Attribution. Sanders, Spooren, and Noordman 1992; Bateman and Rondhuis 1997 discuss correspondences between different taxonomies. Different theories, however, assume different sets of constraints that govern these representations; some advocate trees: RST Mann and Thompson 1987, DLTAG Webber et al. 1999; others, graphs of different sorts: SDRT Asher and Lascarides 2003, Graphbank Wolf and Gibson 2005. Consider: (1) Elab1 (Attr(Elab2 (C1N , C2S )N , C3S )N , C4S ) [“he was a very aggressive firefighter.]C1 [he loved the work he was in,”]C2 [said acting fire chief Lary Garcia.]C3 . [”He couldn’t be bested in terms of his willingness and his ability to do something to help you survive.”]C4 (from Egg and Redeker 2010) Using RST, Egg and Redeker 2010 provide the tree annotated with nuclearity features for this example (given by the linear encoding in (s1 )), while SDRT provides 1 The Penn Discourse Treebank Prasad et al. 2008 could also be considered as a corpus with partial dependency structu"
W13-4002,afantenos-etal-2012-empirical,1,0.895142,"Missing"
W13-4002,E93-1004,0,0.412741,"Missing"
W13-4002,P09-1075,0,0.309074,"ating on exact meaning representations, however, the syntax-semantics interface becomes quite complex: as happens with quantifiers at the intra sentential level, discourse relations might semantically require a scope that is, at least a priori, not determined by syntactic considerations alone and violates surface order (see s2 ). Other theories like Polanyi’s Linguistic Discourse Model (LDM) of Polanyi 1985; Polanyi and Scha 1984, and DLTAG Webber et al. 1999 explicitly adopt a syntactic point of view, and RST with strongly constrained (tree-shaped) structures is subject to parsing approaches duVerle and Prendinger 2009; Sagae 2009; Subba and Di Eugenio 2009 that adhere to the syntactic approach in adopting decoding strategies of syntactic parsing. In such theories, discourse structure representations, subject to syntactic constraints (e.g. dominance of spans of text one over another) respect surface order but do not always and unproblematically yield a semantic interpretation that fits intuitions. According to Marcu 1996, an RST tree is not by itself sufficient to generate desired predictions; he employs the nuclearity principle, NP, as an additional interpretation principle on scopes of relations. We focus"
W13-4002,egg-redeker-2010-complex,0,0.307826,"Missing"
W13-4002,E95-1035,0,\N,Missing
W14-6601,J10-4006,0,0.0993487,"Missing"
W14-6601,W11-2501,0,0.0343859,"Missing"
W14-6601,D11-1094,1,0.899938,"Missing"
W19-2715,L16-1432,1,0.885284,"Missing"
W19-2715,P17-2037,1,0.86512,": Statistics on the corpora. discourse boundaries than sentence boundaries. This is an indication of the difficulty of the task, since, at least in principle, intra-sentential boundaries are harder to detect than sentence frontiers. the same for all RST and SDRT data, with labels indicating the beginning of an EDU (BIO format, without the Inside tag), but the task is quite different for PDTB corpora where the system has to identify the beginning of a connective span and all its inside tokens (BIO format). The results for this shared task are not directly comparable with the ones presented in (Braud et al., 2017a,b) because for the shared task, the GUM corpus has been extended – from 54 to 78 documents – while the Portuguese corpus is restricted to the 110 documents of the CSTNews corpus (Cardoso et al., 2011) – against 330 in (Braud et al., 2017a) where all the discourse corpora available for this language were merged. 3.2 4 Approach In this paper, we investigate the usefulness of contextual pre-trained embeddings, and evaluate the effect of using sentence splitter as a pre-processing step. We compare our systems to rule-based baselines and a simple sequence labelling model using a bi-directional LS"
W19-2715,D17-1258,1,0.866393,": Statistics on the corpora. discourse boundaries than sentence boundaries. This is an indication of the difficulty of the task, since, at least in principle, intra-sentential boundaries are harder to detect than sentence frontiers. the same for all RST and SDRT data, with labels indicating the beginning of an EDU (BIO format, without the Inside tag), but the task is quite different for PDTB corpora where the system has to identify the beginning of a connective span and all its inside tokens (BIO format). The results for this shared task are not directly comparable with the ones presented in (Braud et al., 2017a,b) because for the shared task, the GUM corpus has been extended – from 54 to 78 documents – while the Portuguese corpus is restricted to the 110 documents of the CSTNews corpus (Cardoso et al., 2011) – against 330 in (Braud et al., 2017a) where all the discourse corpora available for this language were merged. 3.2 4 Approach In this paper, we investigate the usefulness of contextual pre-trained embeddings, and evaluate the effect of using sentence splitter as a pre-processing step. We compare our systems to rule-based baselines and a simple sequence labelling model using a bi-directional LS"
W19-2715,W18-4917,0,0.300746,"ta 3.1 Discourse corpora The shared task organizers provided 15 corpora annotated with discourse boundaries, 4 of which are not freely available. There is no public procedure to get the text for the Chinese PDTB corpus hence we were unable to include it in our experiments.2 The generic term of “discourse annotated” corpora covers a variety of heterogeneous datasets bundled together: Multilingual Annotated data are provided for 9 different languages. 4 datasets are in English (Carlson et al., 2001; Prasad et al., 2008; Asher et al., 2016; Zeldes, 2016), 2 are in Spanish (da Cunha et al., 2011; Cao et al., 2018) and 2 in Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018). The other datasets are in German (Stede and Neumann, 2014), French (Afantenos et al., 2012), Basque (Iruskieta et al., 2013), Portuguese (Cardoso et al., 2011), Russian (Pisarevskaya et al., 2017), Turkish (Zeyrek et al., 2013) and Dutch (Redeker et al., 2012). To the best of our knowledge, this is the first time models are suggested for discourse segmentation of Russian, Turkish, and Chinese. Multi-formalisms The 3 main frameworks for discourse are represented, namely RST, SDRT and PDTB. The latter two are only represented by t"
W19-2715,W01-1605,0,0.265673,"r Chlo´e Braud Datactivist IRIT, CNRS, University of Toulouse LORIA Aix-en-Provence, France Toulouse, France CNRS mathieu@datactivist.coop philippe.muller@irit.fr Nancy, France chloe.braud@loria.fr Abstract et al., 2008), the task is expressed as finding the arguments of a discourse connective, whether this connective is implicit or explicit. Combining the existing corpora is thus a challenge, while the lack of annotated data makes it an appealing solution. Even within a given framework, the criteria for identifying EDUs differ between the annotation projects: for instance, the RST-DT corpus (Carlson et al., 2001) and the RST GUM corpus (Zeldes, 2016) have very different segmentation guidelines. While discourse analysis mainly involves semantic and pragmatic questions, discourse segmentation is closer to the syntactic level, as is reflected in the annotation guidelines, which tend to equate segments with various kinds of clauses. Most existing work considers segmentation at the sentence level (intra-sentential segmentation), implicitly assuming that the task of sentence boundary detection can be done perfectly. This assumption is rarely questioned even though the performance of sentence boundary detect"
W19-2715,W11-0401,0,0.799392,"Missing"
W19-2715,D14-1162,0,0.0830722,"med entity recognition built on these embeddings, with a single-layer LSTM encoding a document or a sentence on top of character-based convolution filters and contextual word embeddings. ELMo reaches good results on CoNLL 2003 NER tasks with a 2-layer LSTM and a CRF on top to lever5 Settings For the baseline models based on a bi-LSTM, we used randomly initialized or pre-trained word embeddings with a dimension of 50 or 300. For monolingual experiments, we used the FastText monolingual embeddings available for 157 languages (Grave et al., 2018), with 300 dimensions.4 We also tested with GloVe (Pennington et al., 2014) and 50 dimensions for English datasets, since these embeddings are the ones used by our main model.5 4 https://fasttext.cc/docs/en/ crawl-vectors.html 5 https://nlp.stanford.edu/projects/ glove/ 118 The other hyper-parameters are: one hidden layer with 100 dimensions, a dropout of 0.5, the Adam optimizer, a learning rate of 0.001 and 10 epochs. For the BERT-based sequence prediction model, we used a configuration close to the NER ELMo system provided by the Allen NLP library (Gardner et al., 2017), with convolution filters at the character level combined to word-level embeddings, where BERT r"
W19-2715,N18-1202,0,0.0657782,"This assumption is rarely questioned even though the performance of sentence boundary detection systems is far from perfect and very sensitive to noisy input. Also, it is crucial for some languages to consider document-level segmentation. Within the framework of the shared task, we investigate performance at the document-level with no gold sentence information, and compare it to the performance when assuming gold sentence boundaries. We present different sequence prediction architectures with different pre-trained embeddings, and show that the best configurations using contextual embeddings (Peters et al., 2018; Devlin et al., 2018) seem sufficient to reach comparable performances to existing systems, when separately trained on each corpus, while using more generic resources.1 Our best system consistently improves over the state-of-the-art models at the document level without the use of any addiSegmentation is the first step in building practical discourse parsers, and is often neglected in discourse parsing studies. The goal is to identify the minimal spans of text to be linked by discourse relations, or to isolate explicit marking of discourse relations. Existing systems on English report F1 score"
W19-2715,P07-1062,0,0.800084,"Missing"
W19-2715,prasad-etal-2008-penn,0,0.143341,"Missing"
W19-2715,L18-1550,0,0.0234779,"pora in multiple languages. We applied here a simplified version of named entity recognition built on these embeddings, with a single-layer LSTM encoding a document or a sentence on top of character-based convolution filters and contextual word embeddings. ELMo reaches good results on CoNLL 2003 NER tasks with a 2-layer LSTM and a CRF on top to lever5 Settings For the baseline models based on a bi-LSTM, we used randomly initialized or pre-trained word embeddings with a dimension of 50 or 300. For monolingual experiments, we used the FastText monolingual embeddings available for 157 languages (Grave et al., 2018), with 300 dimensions.4 We also tested with GloVe (Pennington et al., 2014) and 50 dimensions for English datasets, since these embeddings are the ones used by our main model.5 4 https://fasttext.cc/docs/en/ crawl-vectors.html 5 https://nlp.stanford.edu/projects/ glove/ 118 The other hyper-parameters are: one hidden layer with 100 dimensions, a dropout of 0.5, the Adam optimizer, a learning rate of 0.001 and 10 epochs. For the BERT-based sequence prediction model, we used a configuration close to the NER ELMo system provided by the Allen NLP library (Gardner et al., 2017), with convolution fil"
W19-2715,K18-2016,0,0.0328883,"s given in discourse segmenters. However, performance of sentence splitters are far from perfect, especially for specific genres and low-resourced languages. In this shared task, sentence boundaries are given in the CoNLL files, and are either gold or predicted (for rus.rst.rrt). Since sentence boundaries are always discourse boundaries for RST and SDRT style segmentation, the performance of a sentence splitter is a lower bound for our systems. Moreover, we propose systems relying on sentence segmentation as a way to reduce the size of the input, and thus help the model. We use StanfordNLP 3 (Qi et al., 2018) with language-specific models to predict sentence segStatistics We provide a summary on the corpora used in this paper in Table 1, showing the wide differences in sizes, numbers of documents, vocabularies, and number of sentences per document, from about 10 sentences on average, to a maximum of 70 for the Russian corpus. We note that 7 corpora contain less than 100 documents, which will probably make it harder to learn from them. Leaving out PDTB-style corpora that include a different kind of annotations, the proportion of intra-sentential boundaries varies across corpora: e.g., in eng.rst.gu"
W19-2715,redeker-etal-2012-multi,0,0.616871,"Missing"
W19-2715,N03-1030,0,0.672994,"Missing"
W19-2715,P82-1020,0,0.35373,"Missing"
W19-2715,H05-1033,0,0.652048,"Missing"
W19-2715,stede-neumann-2014-potsdam,0,0.401794,"ch are not freely available. There is no public procedure to get the text for the Chinese PDTB corpus hence we were unable to include it in our experiments.2 The generic term of “discourse annotated” corpora covers a variety of heterogeneous datasets bundled together: Multilingual Annotated data are provided for 9 different languages. 4 datasets are in English (Carlson et al., 2001; Prasad et al., 2008; Asher et al., 2016; Zeldes, 2016), 2 are in Spanish (da Cunha et al., 2011; Cao et al., 2018) and 2 in Mandarin Chinese (Zhou et al., 2014; Cao et al., 2018). The other datasets are in German (Stede and Neumann, 2014), French (Afantenos et al., 2012), Basque (Iruskieta et al., 2013), Portuguese (Cardoso et al., 2011), Russian (Pisarevskaya et al., 2017), Turkish (Zeyrek et al., 2013) and Dutch (Redeker et al., 2012). To the best of our knowledge, this is the first time models are suggested for discourse segmentation of Russian, Turkish, and Chinese. Multi-formalisms The 3 main frameworks for discourse are represented, namely RST, SDRT and PDTB. The latter two are only represented by two and three corpora. For PDTB, the English corpus is the largest one, but for SDRT, both the French and the English ones ar"
W19-2715,J15-3002,0,0.219507,"Missing"
W19-2715,C04-1048,0,0.516768,"Missing"
W19-2715,P09-2020,0,0.799664,"Missing"
W19-2715,D18-1116,0,0.441641,"Missing"
W19-2715,W12-1623,0,0.121011,"Missing"
W19-5950,W90-0117,0,0.721427,"relation classification Previous work on discourse relation identification generally separated the classification of implicit and explicit examples, and mainly focused on implicit ones, considered as the hardest task. Performance on this task are, however, still low: the current best are reported in (Bai and Zhao, 2018), where it is proposed to augment word embeddings with subword and contextual embeddings, and to combine sentence and sentence pair representaThere have been a few attempts to formalize the various types of information encoded by discourse relations, and give it some structure (Hovy, 1990; Knott, 1997), or provide a semantics for the underlying principles (Chiarcos, 2014), without clear-cut criteria to decide on the most appropriate set of relations. The PDTB addresses the problem by providing a hierarchy of relations, allowing for various levels of underspecification, but without much justification other than annotation operational constraints. 1 Our code is available at https://gitlab.inria. fr/andiamo/relations. 433 3.1 Cognitive approach to Coherence Relations Source of coherence This primitive has two possible values named objective and subjective in CCR. It refers to a c"
W19-5950,K16-2018,0,0.0373886,"Missing"
W19-5950,K16-2014,0,0.0301641,"Missing"
W19-5950,prasad-etal-2008-penn,0,0.100063,"ns. We study experimentally which of the conceptual primitives are harder to learn from the Penn Discourse Treebank English corpus, and propose a correspondence to predict the original labels, with preliminary empirical comparisons with a direct model. 1 (2) Climate is changing. Humans generate too much CO2 . Contingency.Cause.Reason Several theoretical frameworks exist for discourse analysis, the most well-known being Rhetorical Structure Theory (RST, Mann and Thompson, 1988), and Segmented Discourse Representation Theory (SDRT, Asher and Lascarides, 2003). The Penn Discourse Treebank (PDTB, Prasad et al., 2008) is an English annotated corpus with its own theoretical assumptions. It is the largest resource for discourse relations and has been used in several studies to demonstrate the difficulty of automatically identifying implicit discourse relations, e.g. (Xue et al., 2016; Bai and Zhao, 2018). The PDTB relies on a three-level hierarchy of rhetorical functions, and multiple relations can be annotated for each example. As empirical models have shown rather low results for implicit relation classification, with only incremental improvements in spite of the variety of approaches that have been tried,"
W19-5950,C18-1048,0,0.134906,"rate too much CO2 . Contingency.Cause.Reason Several theoretical frameworks exist for discourse analysis, the most well-known being Rhetorical Structure Theory (RST, Mann and Thompson, 1988), and Segmented Discourse Representation Theory (SDRT, Asher and Lascarides, 2003). The Penn Discourse Treebank (PDTB, Prasad et al., 2008) is an English annotated corpus with its own theoretical assumptions. It is the largest resource for discourse relations and has been used in several studies to demonstrate the difficulty of automatically identifying implicit discourse relations, e.g. (Xue et al., 2016; Bai and Zhao, 2018). The PDTB relies on a three-level hierarchy of rhetorical functions, and multiple relations can be annotated for each example. As empirical models have shown rather low results for implicit relation classification, with only incremental improvements in spite of the variety of approaches that have been tried, it appears a lot of the necessary information is still not leveraged in discourse parsing. But it could be argued also that the difficulty lies in the way we model the task, especially these labels on which there is no consensus and generally a low inter-annotator agreement. We argue here"
W19-5950,chiarcos-2014-towards,0,0.0260847,"ally separated the classification of implicit and explicit examples, and mainly focused on implicit ones, considered as the hardest task. Performance on this task are, however, still low: the current best are reported in (Bai and Zhao, 2018), where it is proposed to augment word embeddings with subword and contextual embeddings, and to combine sentence and sentence pair representaThere have been a few attempts to formalize the various types of information encoded by discourse relations, and give it some structure (Hovy, 1990; Knott, 1997), or provide a semantics for the underlying principles (Chiarcos, 2014), without clear-cut criteria to decide on the most appropriate set of relations. The PDTB addresses the problem by providing a hierarchy of relations, allowing for various levels of underspecification, but without much justification other than annotation operational constraints. 1 Our code is available at https://gitlab.inria. fr/andiamo/relations. 433 3.1 Cognitive approach to Coherence Relations Source of coherence This primitive has two possible values named objective and subjective in CCR. It refers to a common distinction in the literature, for instance subject matter versus presentationa"
W19-5950,K16-2001,0,0.0130905,"nging. Humans generate too much CO2 . Contingency.Cause.Reason Several theoretical frameworks exist for discourse analysis, the most well-known being Rhetorical Structure Theory (RST, Mann and Thompson, 1988), and Segmented Discourse Representation Theory (SDRT, Asher and Lascarides, 2003). The Penn Discourse Treebank (PDTB, Prasad et al., 2008) is an English annotated corpus with its own theoretical assumptions. It is the largest resource for discourse relations and has been used in several studies to demonstrate the difficulty of automatically identifying implicit discourse relations, e.g. (Xue et al., 2016; Bai and Zhao, 2018). The PDTB relies on a three-level hierarchy of rhetorical functions, and multiple relations can be annotated for each example. As empirical models have shown rather low results for implicit relation classification, with only incremental improvements in spite of the variety of approaches that have been tried, it appears a lot of the necessary information is still not leveraged in discourse parsing. But it could be argued also that the difficulty lies in the way we model the task, especially these labels on which there is no consensus and generally a low inter-annotator agr"
W19-5950,K15-2001,0,0.0659466,"Missing"
