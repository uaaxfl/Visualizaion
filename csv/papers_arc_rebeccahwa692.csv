2021.wnut-1.41,Contrapositive Local Class Inference,2021,-1,-1,2,1,237,omid kashefi,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Certain types of classification problems may be performed at multiple levels of granularity; for example, we might want to know the sentiment polarity of a document or a sentence, or a phrase. Often, the prediction at a greater-context (e.g., sentences or paragraphs) may be informative for a more localized prediction at a smaller semantic unit (e.g., words or phrases). However, directly inferring the most salient local features from the global prediction may overlook the semantics of this relationship. This work argues that inference along the contraposition relationship of the local prediction and the corresponding global prediction makes an inference framework that is more accurate and robust to noise. We show how this contraposition framework can be implemented as a transfer function that rewrites a greater-context from one class to another and demonstrate how an appropriate transfer function can be trained from a noisy user-generated corpus. The experimental results validate our insight that the proposed contrapositive framework outperforms the alternative approaches on resource-constrained problem domains."
2020.wnut-1.26,Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation,2020,-1,-1,2,1,237,omid kashefi,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),0,"Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to use may significantly impact the success of the training. In this work, we propose a metric for evaluating augmentation heuristics; specifically, we quantify the extent to which an example is {``}hard to distinguish{''} by considering the difference between the distribution of the augmented samples of different classes. Experimenting with multiple heuristics in two prediction tasks (positive/negative sentiment and verbosity/conciseness) validates our claims by revealing the connection between the distribution difference of different classes and the classification accuracy."
2020.coling-main.428,Inflating Topic Relevance with Ideology: A Case Study of Political Ideology Bias in Social Topic Detection Models,2020,-1,-1,2,0,21527,meiqi guo,Proceedings of the 28th International Conference on Computational Linguistics,0,"We investigate the impact of political ideology biases in training data. Through a set of comparison studies, we examine the propagation of biases in several widely-used NLP models and its effect on the overall retrieval accuracy. Our work highlights the susceptibility of large, complex models to propagating the biases from human-selected input, which may lead to a deterioration of retrieval accuracy, and the importance of controlling for these biases. Finally, as a way to mitigate the bias, we propose to learn a text representation that is invariant to political ideology while still judging topic relevance."
N18-2036,Semantic Pleonasm Detection,2018,0,0,3,1,237,omid kashefi,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Pleonasms are words that are redundant. To aid the development of systems that detect pleonasms in text, we introduce an annotated corpus of semantic pleonasms. We validate the integrity of the corpus with interannotator agreement analyses. We also compare it against alternative resources in terms of their effects on several automatic redundancy detection methods."
D18-1199,Heuristically Informed Unsupervised Idiom Usage Recognition,2018,0,1,2,0,30527,changsheng liu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Many idiomatic expressions can be interpreted figuratively or literally depending on their contexts. This paper proposes an unsupervised learning method for recognizing the intended usages of idioms. We treat the usages as a latent variable in probabilistic models and train them in a linguistically motivated feature space. Crucially, we show that distributional semantics is a helpful heuristic for distinguishing the literal usage of idioms, giving us a way to formulate a literal usage metric to estimate the likelihood that the idiom is intended literally. This information then serves as a form of distant supervision to guide the unsupervised training process for the probabilistic models. Experiments show that our overall model performs competitively against supervised methods."
P17-1144,A Corpus of Annotated Revisions for Studying Argumentative Writing,2017,24,7,3,0.664334,4658,fan zhang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer{'}s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction."
N16-3008,{A}rg{R}ewrite: A Web-based Revision Assistant for Argumentative Writings,2016,6,8,2,0.664334,4658,fan zhang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"While intelligent writing assistants have become more common, they typically have little support for revision behavior. We present ArgRewrite, a novel web-based revision assistant that focus on rewriting analysis. The system supports two major functionalities: 1) to assist students as they revise, the system automatically extracts and analyzes revisions; 2) to assist teachers, the system provides an overview of studentsxe2x80x99 revisions and allows teachers to correct the automatically analyzed results, ensuring that students get the correct feedback."
N16-1040,Phrasal Substitution of Idiomatic Expressions,2016,27,2,2,0,30527,changsheng liu,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1182,An Evaluation of Parser Robustness for Ungrammatical Sentences,2016,23,3,2,1,10282,homa hashemi,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
P14-2098,Improved Correction Detection in Revised {ESL} Sentences,2014,13,10,2,1,39154,huichao xue,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This work explores methods of automatically detecting corrections of individual mistakes in sentence revisions for ESL students. We have trained a classifier that specializes in determining whether consecutive basic-edits (word insertions, deletions, substitutions) address the same mistake. Experimental result shows that the proposed system achieves an F1-score of 81% on correction detection and 66% for the overall system, out-performing the baseline by a large margin."
hashemi-hwa-2014-comparison,A Comparison of {MT} Errors and {ESL} Errors,2014,13,4,2,1,10282,homa hashemi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Generating fluent and grammatical sentences is a major goal for both Machine Translation (MT) and second-language Grammar Error Correction (GEC), but there have not been a lot of cross-fertilization between the two research communities. Arguably, an automatic translate-to-English system might be seen as an English as a Second Language (ESL) writer whose native language is the source language. This paper investigates whether research findings from the GEC community may help with characterizing MT error analysis. We describe a method for the automatic classification of MT errors according to English as a Second Language (ESL) error categories and conduct a large comparison experiment that includes both high-performing and low-performing translate-to-English MT systems for several source languages. Comparing the distribution of MT error types for all the systems suggests that MT systems have fairly similar distributions regardless of their source languages, and the high-performing MT systems have error distributions that are more similar to those of the low-performing MT systems than to those of ESL learners with the same L1."
E14-1072,Redundancy Detection in {ESL} Writings,2014,18,2,2,1,39154,huichao xue,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper investigates redundancy detection in ESL writings. We propose a measure that assigns high scores to words and phrases that are likely to be redundant within a given sentence. The measure is composed of two components: one captures fluency with a language model; the other captures meaning preservation based on analyzing alignments between words and their translations. Experiments show that the proposed measure is five times more accurate than the random baseline.
W12-3810,Recognizing Arguing Subjectivity and Argument Tags,2012,25,30,3,0,42190,alexander conrad,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,0,"In this paper we investigate two distinct tasks. The first task involves detecting arguing subjectivity, a type of linguistic subjectivity on which relatively little work has yet to be done. The second task involves labeling instances of arguing subjectivity with argument tags reflecting the conceptual argument being made. We refer to these two tasks collectively as recognizing arguments. We develop a new annotation scheme and assemble a new annotated corpus to support our learning efforts. Through our machine learning experiments, we investigate the utility of a sentiment lexicon, discourse parser, and semantic similarity measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results."
C12-1178,Modeling {ESL} Word Choice Similarities By Representing Word Intensions and Extensions,2012,21,0,2,1,39154,huichao xue,Proceedings of {COLING} 2012,0,"Automatic error correction systems for English as a Second Language(ESL) speakers often rely on the use of a confusion set to limit the choices of possible correction candidates. Typically, the confusion sets are either manually constructed or extracted from a corpus of manually corrected ESL writings. Both options require the involvement of English teachers. This paper proposes a method to automatically construct confusion sets for commonly used prepositions from non-ESL corpus without manual intervention. The proposed method simulates how ESL learners learn both the intensions and extensions of English words from standard English text. Our experimental results suggest that the automatically constructed confusion sets based on the similarities between the learned wordsxe2x80x99 intensions is competitive with those directly learned from an ESL corpus containing about 150K preposition usages."
N10-1040,Improving Phrase-Based Translation with Prototypes of Short Phrases,2010,14,1,3,0,45799,frank liberato,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We investigate methods of generating additional bilingual phrase pairs for a phrase-based decoder by translating short sequences of source text. Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations."
C10-2157,Syntax-Driven Machine Translation as a Model of {ESL} Revision,2010,23,5,2,1,39154,huichao xue,Coling 2010: Posters,0,"In this work, we model the writing revision process of English as a Second Language (ESL) students with syntax-driven machine translation methods. We compare two approaches: tree-to-string transformations (Yamada and Knight, 2001) and tree-to-tree transformations (Smith and Eisner, 2006). Results suggest that while the tree-to-tree model provides a greater coverage, the tree-to-string approach offers a more plausible model of ESL learners' revision writing process."
2010.amta-papers.17,Using Variable Decoding Weight for Language Model in Statistical Machine Translation,2010,17,4,2,1,35024,behrang mohit,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"This paper investigates varying the decoder weight of the language model (LM) when translating different parts of a sentence. We determine the condition under which the LM weight should be adapted. We find that a better translation can be achieved by varying the LM weight when decoding the most problematic spot in a sentence, which we refer to as a difficult segment. Two adaptation strategies are proposed and compared through experiments. We find that adapting a different LM weight for every difficult segment resulted in the largest improvement in translation quality."
E09-1008,Correcting Automatic Translations through Collaborations between {MT} and Monolingual Target-Language Users,2009,15,14,2,1,47380,joshua albrecht,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Machine translation (MT) systems have improved significantly: however, their outputs often contain too many errors to communicate the intended meaning to their users. This paper describes a collaborative approach for mediating between an MT system and users who do not understand the source language and thus cannot easily detect translation mistakes on their own. Through a visualization of multiple linguistic resources, this approach enables the users to correct difficult translation errors and understand translated passages that were otherwise baffling."
2009.eamt-1.22,Language Model Adaptation for Difficult to Translate Phrases,2009,12,6,3,1,35024,behrang mohit,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper investigates the idea of adapting language models for phrases that have poor translation quality. We apply a selective adaptation criterion which uses a classifier to locate the most difficult phrase of each source language sentence. A special adapted language model is constructed for the highlighted phrase. Our adaptation heuristic uses lexical features of the phrase to locate the relevant parts of the parallel corpus for language model training. As we vary the experimental setup by changing the size of the SMT training data, our adaptation method consistently shows strong improvements over the baseline systems."
W08-0330,The Role of Pseudo References in {MT} Evaluation,2008,10,19,2,1,47380,joshua albrecht,Proceedings of the Third Workshop on Statistical Machine Translation,0,"Previous studies have shown automatic evaluation metrics to be more reliable when compared against many human translations. However, multiple human references may not always be available. It is more common to have only a single human reference (extracted from parallel texts) or no reference at all. Our earlier work suggested that one way to address this problem is to train a metric to evaluate a sentence by comparing it against pseudo references, or imperfect references produced by off-the-shelf MT systems. In this paper, we further examine the approach both in terms of the training methodology and in terms of the role of the human and pseudo references. Our expanded experiments show that the approach generalizes well across multiple years and different source languages."
W07-0737,Localization of Difficult-to-Translate Phrases,2007,12,20,2,1,35024,behrang mohit,Proceedings of the Second Workshop on Statistical Machine Translation,0,"This paper studies the impact that difficult-to-translate source-language phrases might have on the machine translation process. We formulate the notion of difficulty as a measurable quantity; we show that a classifier can be trained to predict whether a phrase might be difficult to translate; and we develop a framework that makes use of the classifier and external resources (such as human translators) to improve the overall translation quality. Through experimental work, we verify that by isolating difficult-to-translate phrases and processing them as special cases, their negative impact on the translation of the rest of the sentences can be reduced."
P07-1111,A Re-examination of Machine Learning Approaches for Sentence-Level {MT} Evaluation,2007,22,45,2,1,47380,joshua albrecht,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics."
P07-1038,Regression for Sentence-Level {MT} Evaluation with Pseudo References,2007,22,59,2,1,47380,joshua albrecht,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Many automatic evaluation metrics for machine translation (MT) rely on making comparisons to human translations, a resource that may not always be available. We present a method for developing sentence-level MT evaluation metrics that do not directly rely on human reference translations. Our metrics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy ( pseudo references). Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances."
2006.amta-papers.9,Corpus Variations for Translation Lexicon Induction,2006,17,2,1,1,238,rebecca hwa,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Lexical mappings (word translations) between languages are an invaluable resource for multilingual processing. While the problem of extracting lexical mappings from parallel corpora is well-studied, the task is more challenging when the language samples are from non-parallel corpora. The goal of this work is to investigate one such scenario: finding lexical mappings between dialects of a diglossic language, in which people conduct their written communications in a prestigious formal dialect, but they communicate verbally in a colloquial dialect. Because the two dialects serve different socio-linguistic functions, parallel corpora do not naturally exist between them. An example of a diglossic dialect pair is Modern Standard Arabic (MSA) and Levantine Arabic. In this paper, we evaluate the applicability of a standard algorithm for inducing lexical mappings between comparable corpora (Rapp, 1999) to such diglossic corpora pairs. The focus of the paper is an in-depth error analysis, exploring the notion of relatedness in diglossic corpora and scrutinizing the effects of various dimensions of relatedness (such as mode, topic, style, and statistics) on the quality of the resulting translation lexicon."
P05-3015,Syntax-based Semi-Supervised Named Entity Tagging,2005,6,22,2,1,35024,behrang mohit,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We report an empirical study on the role of syntactic features in building a semi-supervised named entity (NE) tagger. Our study addresses two questions: What types of syntactic features are suitable for extracting potential NEs to train a classifier in a semi-supervised setting? How good is the resulting NE classifier on testing instances dissimilar from its training data? Our study shows that constituency and dependency parsing constraints are both suitable features to extract NEs and train the classifier. Moreover, the classifier showed significant accuracy improvement when constituency features are combined with new dependency feature. Furthermore, the degradation in accuracy on unfamiliar test cases is low, suggesting that the trained classifier generalizes well."
P05-3018,Word Alignment and Cross-Lingual Resource Acquisition,2005,15,4,2,0,50713,carol nichols,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,Annotated corpora are valuable resources for developing Natural Language Processing applications. This work focuses on acquiring annotated data for multilingual processing applications. We present an annotation environment that supports a web-based user-interface for acquiring word alignments between English and Chinese as well as a visualization tool for researchers to explore the annotated data.
H05-1107,A Backoff Model for Bootstrapping Resources for Non-{E}nglish Languages,2005,19,37,2,0,51120,chenhai xi,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"The lack of annotated data is an obstacle to the development of many natural language processing applications; the problem is especially severe when the data is non-English. Previous studies suggested the possibility of acquiring resources for non-English languages by bootstrapping from high quality English NLP tools and parallel corpora; however, the success of these approaches seems limited for dissimilar language pairs. In this paper, we propose a novel approach of combining a bootstrapped resource with a small amount of manually annotated data. We compare the proposed approach with other bootstrapping methods in the context of training a Chinese Part-of-Speech tagger. Experimental results show that our proposed approach achieves a significant improvement over EM and self-training and systems that are only trained on manual annotations."
P04-3028,Co-training for Predicting Emotions with Spoken Dialogue Data,2004,9,71,3,0,51735,beatriz maeireizo,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"Natural Language Processing applications often require large amounts of annotated training data, which are expensive to obtain. In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues. In order to do so, we have first applied the wrapper approach with Forward Selection and Naive Bayes, to reduce the dimensionality of our feature set. Our results show that Co-training can be highly effective when a good set of features are chosen."
J04-3001,Sample Selection for Statistical Parsing,2004,33,99,1,1,238,rebecca hwa,Computational Linguistics,0,Corpus-based statistical parsing relies on using large quantities of annotated text as training examples. Building this kind of resource is expensive and labor-intensive. This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones. We consider several criteria for predicting whether unlabeled data might be a helpful training example. Experiments are performed across two syntactic learning tasks and within the single task of parsing across two learning models to compare the effect of different predictive criteria. We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models.
N03-1031,Example Selection for Bootstrapping Statistical Parsers,2003,19,237,2,0,748,mark steedman,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data. We consider both a mostly-unsupervised approach, cotraining, in which two parsers are iteratively re-trained on each other's output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser's output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks."
E03-1008,Bootstrapping statistical parsers from small datasets,2003,14,118,5,0,748,mark steedman,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of boot-strapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that boot-strapping continues to be useful, even though no manually produced parses from the target domain are used."
P02-1050,Evaluating Translational Correspondence using Annotation Projection,2002,25,105,1,1,238,rebecca hwa,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Recently, statistical machine translation models have begun to take advantage of higher level linguistic structures such as syntactic dependencies. Underlying these models is an assumption about the directness of translational correspondence between sentences in the two languages; however, the extent to which this assumption is valid and useful is not well understood. In this paper, we present an empirical study that quantifies the degree to which syntactic dependencies are preserved when parses are projected directly from English to Chinese. Our results show that although the direct correspondence assumption is often too restrictive, a small set of principled, elementary linguistic transformations can boost the quality of the projected Chinese parses by 76% relative to the unimproved baseline."
dorr-etal-2002-duster,{DUST}er: a method for unraveling cross-language divergences for statistical word-level alignment,2002,23,31,3,0,14512,bonnie dorr,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The frequent occurrence of divergenceS{---}structural differences between languages{---}presents a great challenge for statistical word-level alignment. In this paper, we introduce DUSTer, a method for systematically identifying common divergence types and transforming an English sentence structure to bear a closer resemblance to that of another language. Our ultimate goal is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. We present an empirical analysis comparing the complexities of performing word-level alignments with and without divergence handling. Our results suggest that our approach facilitates word-level alignment, particularly for sentence pairs containing divergences."
W01-0710,On minimizing training corpus for parser acquisition,2001,14,25,1,1,238,rebecca hwa,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"Many corpus-based natural language processing systems rely on using large quantities of annotated text as their training examples. Building this kind of resource is an expensive and labor-intensive project. To minimize effort spent on annotating examples that are not helpful the training process, recent research efforts have begun to apply active learning techniques to selectively choose data to be annotated. In this work, we consider selecting training examples with the tree-entropy metric. Our goal is to assess how well this selection technique can be applied for training different types of parsers. We find that tree-entropy can significantly reduce the amount of training annotation for both a history-based parser and an EM-based parser. Moreover, the examples selected for the history-based parser are also good for training the EM-based parser suggesting that the technique is parser independent."
W00-1306,Sample Selection for Statistical Grammar Induction,2000,18,59,1,1,238,rebecca hwa,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"Corpus-based grammar induction relies on using many hand-parsed sentences as training examples. However, the construction of a training corpus with detailed syntactic analysis for every sentence is a labor-intensive task. We propose to use sample selection methods to minimize the amount of annotation needed in the training data, thereby reducing the workload of the human annotators. This paper shows that the amount of annotated training data can be reduced by 36% without degrading the quality of the induced grammars."
P99-1010,Supervised Grammar Induction using Training Data with Limited Constituent Information,1999,10,62,1,1,238,rebecca hwa,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language. Unfortunately, the cost of building large annotated corpora is prohibitively expensive. This work aims to improve the induction strategy when there are few labels in the training data. We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses. They account for only 20% of all constituents. For inducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), we propose an adaptation strategy, which produces grammars that parse almost as well as grammars induced from fully labeled corpora. Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases."
P98-1091,An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion Grammars,1998,10,13,1,1,238,rebecca hwa,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs, with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs."
C98-1088,An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion Grammars,1998,10,13,1,1,238,rebecca hwa,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs, with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs."
