2020.aacl-main.55,P17-1171,0,0.114961,"2) as MetaSim. It calculates a score for up to 4-grams overlap using uniform weights. A metadata-decorated candidate passage with the algorithm is presented in Table 2. word Rinse Season tilapia both fillets sides in with cold salt water and ... pepper pred. feature O O B_R_ING O I_R_ING O O O O B_R_ING O I_R_ING O I_R_ING Table 2: Metadata-Decorated Candidate Passage 3.2 Neural Passage Selection with Fine-grained Metadata Encoding We propose a simple but effective neural network structure for building our base neural passage selector (NPS). Similar to the neural reader (Hermann et al., 2015; Chen et al., 2017) for MRC, we first obtain a feature-rich (including the fine-grained encoding of the metadata) contextualized representation for each token in the passage and query. The output layer takes the passage and query representations as input and makes the prediction. Fine-grained metadata embedding each predicate feature pred (e.g., / RECIPE / COOK T IME) includes the root r (RECIPE) and the property pro (COOK T IME). To leverage this information, we propose to leverage the hierarchy present on the predicate by learning the root embedding Er , the property embedding Epro , as well as the path embedd"
2020.aacl-main.55,N19-1423,0,0.0167919,"for metadata for each Passage & Query encoding We first represent each token ti in the passage with a vector representation and pass it through a multi-layer BiLSTM (Hochreiter and Schmidhuber, 1997) network to get the contextualized representation for each token (t1 , t2 ,...), where ti is the concatenation of: • (Contextualized) word embedding: GloVe 840B.300d (Pennington et al., 2014) embeddings is used to initialize the embedding layer and is fine-tuned during training, we denote it as ˜ ti for token ti . Besides, we also use the pretrained contextualized representations produced by BERT (Devlin et al., 2019), q ˆ1 , ..., q ˆm , ..., ˆ t1 , ..., ˆ tn = BERT([CLS], q1 , ..., qm , [SEP], t1 , ..., tn ). For the ith token, the word embedding E(ti ) is the concatenation of the two. URL and generate metadata-decorated passage Data: queryPsgEg (query, psgText, URL, label), metaPairs (subj, pred, obj, URL); 1 matchingMetaPairs←Join(queryPsgEg[URL] == metaPairs[URL]); 2 for each pair ∈ matchingMetaPairs do 3 if pair[obj] is not text then 4 continue; 5 else 6 startOffsets, endOffsets, score ← MetaSim(queryPsgEg[psgText], pair[obj]); 7 Decorate(queryPsgEg[psgText], startOffsets, endOffsets, pred); 8 end 9 e"
2020.aacl-main.55,P17-1147,0,0.0536885,"Missing"
2020.aacl-main.55,D18-1452,0,0.0549549,"Missing"
2020.aacl-main.55,Q19-1026,0,0.0226307,"Missing"
2020.aacl-main.55,P02-1040,0,0.110832,", we aim to obtain the queryPsgExamples whose candidate answer passages are decorated. We first obtain all the metadata pairs (matchingMetaPairs) for the URL where the passage text appears (line 1). Then, for each metadata pair in matchingMetaPairs, we employ a similarity function (MetaSim in line 6) to first compute the similarity between all possible text spans of the passage and the object text in the metadata object-predicate pair; afterwards the function records the start and end offset of the text spans which have a similarity score higher than the threshold. In our case, we use BLEU-4 (Papineni et al., 2002) as MetaSim. It calculates a score for up to 4-grams overlap using uniform weights. A metadata-decorated candidate passage with the algorithm is presented in Table 2. word Rinse Season tilapia both fillets sides in with cold salt water and ... pepper pred. feature O O B_R_ING O I_R_ING O O O O B_R_ING O I_R_ING O I_R_ING Table 2: Metadata-Decorated Candidate Passage 3.2 Neural Passage Selection with Fine-grained Metadata Encoding We propose a simple but effective neural network structure for building our base neural passage selector (NPS). Similar to the neural reader (Hermann et al., 2015; Ch"
2020.aacl-main.55,D14-1162,0,0.0834316,"e entire predicate (/ RECIPE / COOK T IME). Thus, the final predicate feature encoding for token ti is the concatenation of the three components: Epred (predi ) = concat(Er (ri ), Epro (proi ), Ept (pti )). Algorithm 1: How to obtain for metadata for each Passage & Query encoding We first represent each token ti in the passage with a vector representation and pass it through a multi-layer BiLSTM (Hochreiter and Schmidhuber, 1997) network to get the contextualized representation for each token (t1 , t2 ,...), where ti is the concatenation of: • (Contextualized) word embedding: GloVe 840B.300d (Pennington et al., 2014) embeddings is used to initialize the embedding layer and is fine-tuned during training, we denote it as ˜ ti for token ti . Besides, we also use the pretrained contextualized representations produced by BERT (Devlin et al., 2019), q ˆ1 , ..., q ˆm , ..., ˆ t1 , ..., ˆ tn = BERT([CLS], q1 , ..., qm , [SEP], t1 , ..., tn ). For the ith token, the word embedding E(ti ) is the concatenation of the two. URL and generate metadata-decorated passage Data: queryPsgEg (query, psgText, URL, label), metaPairs (subj, pred, obj, URL); 1 matchingMetaPairs←Join(queryPsgEg[URL] == metaPairs[URL]); 2 for each"
2020.aacl-main.55,D16-1264,0,0.0424323,"Missing"
2020.aacl-main.55,C16-1135,0,0.0569039,"Missing"
2020.aacl-main.55,voorhees-tice-2000-trec,0,0.456112,"Missing"
2020.aacl-main.55,D07-1003,0,0.197654,"Missing"
2020.aacl-main.55,W18-6119,0,0.0510913,"Missing"
2020.aacl-main.55,D15-1237,0,0.0606021,"Missing"
2020.aacl-main.55,P15-1025,0,0.0622191,"Missing"
2020.acl-main.431,N09-1003,0,0.41622,"Missing"
2020.acl-main.431,W19-3805,0,0.0345104,"Missing"
2020.acl-main.431,Q17-1010,0,0.0911215,"nconsistencies between social bias estimators for word embeddings. 1 Introduction Word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011) have been a hallmark of modern natural language processing (NLP) for many years. Embedding methods have been broadly applied and have experienced parallel and complementary innovations alongside neural network methods for NLP. Advances in embedding quality in part have come from integrating additional information such as syntax (Levy and Goldberg, 2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Ten"
2020.acl-main.431,W19-4828,0,0.0197854,"(Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh (2019) consider what is learned from context, Clark et al. (2019); Michel et al. (2019) look at specific attention heads, Hewitt and Manning (2019); Ettinger (2020) address linguistic understanding such as syntax and negation, and Wallace et al. (2019); Tan and Celis (2019) address ethical concerns such as security (adversarial robustness) and social bias. In fact, the neologism BERTology was coined specifically to describe this flurry of interpretability research.1 While these works have provided nuanced finegrained analyses by creating new interpretability schema/techniques, we instead take an alternate approach of trying to re-purpose methods developed f"
2020.acl-main.431,N15-1140,0,0.0571562,"Missing"
2020.acl-main.431,W16-2507,0,0.0570233,"Missing"
2020.acl-main.431,W19-3823,0,0.132591,"adapting psychological association tests (Caliskan et al., 2017; May et al., 2019), and (c) considering downstream behavior (Zhao et al., 2017, 2018a, 2019a; Stanovsky et al., 2019).8 Our bias evaluation is in the style of (a) and we consider multi-class social bias in the lens of gender, race, and religion whereas prior work has centered on binary gender. Additionally, while most prior work has discussed the static embedding setting, recent work has considered sentence encoders and contextualized models. Zhao et al. (2019a) consider gender bias in ELMo when applied to coreference systems and Kurita et al. (2019) extend these results by leveraging the masked language modeling objective of BERT. Similarly, Basta et al. (2019) considers intrinsic gender bias in ELMo via gender-swapped 7 The authors also identified several empirical concerns that draw the meaningfulness of this method into question. 8 Sun et al. (2019) provides a taxonomy of the work towards understanding gender bias within NLP. sentences. When compared to these approaches, we study a broader class of biases under more than one bias definition and consider more than one model. Further, while many of these approaches generally neglect rep"
2020.acl-main.431,P14-2050,0,0.0365788,"ternal layers even for models that share the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings. 1 Introduction Word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011) have been a hallmark of modern natural language processing (NLP) for many years. Embedding methods have been broadly applied and have experienced parallel and complementary innovations alongside neural network methods for NLP. Advances in embedding quality in part have come from integrating additional information such as syntax (Levy and Goldberg, 2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu"
2020.acl-main.431,W14-1618,0,0.0371409,"ternal layers even for models that share the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings. 1 Introduction Word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011) have been a hallmark of modern natural language processing (NLP) for many years. Embedding methods have been broadly applied and have experienced parallel and complementary innovations alongside neural network methods for NLP. Advances in embedding quality in part have come from integrating additional information such as syntax (Levy and Goldberg, 2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu"
2020.acl-main.431,Q15-1016,0,0.132546,"Missing"
2020.acl-main.431,D17-1257,0,0.0387942,"Missing"
2020.acl-main.431,N19-1112,0,0.326268,"2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh (2019) consider what is learned from context, Clark et al. (2019); Michel et al. (2019) look at specific attention heads, Hewitt and Manning (2019); Ettinger (2020) address linguistic understanding such as syntax and negation, and Wallace et al. (2019); Tan and Celis (2019) address ethical concerns such as security (adversarial robustness) and social bias. In fact, the neologism BERTology was coined specifically to describe this flurry of interpretability research.1 While these"
2020.acl-main.431,2020.cl-2.7,0,0.0443263,"Missing"
2020.acl-main.431,N18-1202,0,0.280462,"gio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011) have been a hallmark of modern natural language processing (NLP) for many years. Embedding methods have been broadly applied and have experienced parallel and complementary innovations alongside neural network methods for NLP. Advances in embedding quality in part have come from integrating additional information such as syntax (Levy and Goldberg, 2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh (2019) consider what is learned from context, Clark et al. (2019);"
2020.acl-main.431,W19-4328,0,0.0175552,"gous to SIF (Arora et al., 2017) for sentence representations computed via averaging (Wieting et al., 2016). The generality of the proxy analysis method implies that other interpretability methods for static embeddings can also be considered. Further, post-processing approaches beyond analysis/interpretability such as dimensionality reduction may be particularly intriguing given that this is often challenging to perform within large multilayered networks like BERT (Sanh et al., 2019) but has been successfully demonstrated for static embeddings (Nunes and Antunes, 2018; Mu and Viswanath, 2018; Raunak et al., 2019). Future work may revisit the choice of the corpus D from which contexts are drawn. For downstream use, setting D to be the target domain may serve as a lightweight domain adaptation strategy similar to findings for averaged word representations for 4765 9 This is the only layer studied in Kurita et al. (2019). out-of-domain settings (Wieting et al., 2016). 8 the distribution across contexts is desirable/helps combat the anisotropy that exists in the original contextualized space (Ethayarajh, 2019). Discussion and Open Problems While our work demonstrates that contextualized representations re"
2020.acl-main.431,2020.tacl-1.54,0,0.023965,". In order to employ static embedding interpretability methods to contextualized representations, we begin by proposing a simple strategy for converting from contextualized representations to static embeddings. Crucially, our method is fully general and assumes only that the contextualized model maps word sequences to vector sequences. Given this generality, we apply our method to 9 popular pretrained contextualized representations. The resulting static embeddings serve as proxies for the original contextualized model. 1 We direct interested readers to a more complete survey of this work from Rogers et al. (2020). 4758 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4758–4781 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We initially examine the representational quality of these embeddings under intrinsic evaluation. Our evaluation produces several insights regarding layer-wise lexical semantic understanding and representational variation in contextualized representations. Importantly, our analyses suggest constructive improvements to potentially improve downstream practices in using contextualized models. Simultaneously, we find tha"
2020.acl-main.431,P19-1011,0,0.0283898,"ly, our analyses suggest constructive improvements to potentially improve downstream practices in using contextualized models. Simultaneously, we find that our static embeddings substantially outperform Word2Vec and GloVe and therefore suggests our method serves the dual purpose of being a lightweight mechanism for generating static embeddings that track with advances in contextualized representations. Since static embeddings have significant advantages with respect to speed, computational resources, and ease of use, these results have important implications for resource-constrained settings (Shen et al., 2019), environmental concerns (Strubell et al., 2019), and the broader accessibility of NLP technologies.2 Alongside more developed methods for embedding analysis, the static embedding setting is also equipped with a richer body of work regarding social bias. In this sense, we view understanding the encoded social bias in representations as a societally critical special-case of interpretability research. We employ methods for identifying and quantifying gender, racial/ethnic, and religious bias (Bolukbasi et al., 2016; Garg et al., 2018; Manzini et al., 2019) to our static embeddings. These experim"
2020.acl-main.431,D19-1113,0,0.0495345,"rding where lexical semantic information is best encoded in pretrained contextualized models. However, as we pool over a larger number of contexts, Table 1 reveals an interesting relationship between N and the best-performing layer. The best-performing layer monotonically (with a single exception) shifts to be later and later within the pretrained model. Since the later layers did not perform better for smaller values of N , these layers demonstrate greater variance with respect to the layer-wise distributional mean and reducing this variance improves performance.4 Since later layers of the 4 Shi et al. (2019) concurrently propose a different apmodel are generally preferred by downstream practitioners (Zhang et al., 2020), our findings suggest that downstream performance could be further improved by considering variance reduction as we suggest; Ethayarajh (2019) also provides concrete evidence of the tremendous variance in the later layers of deep pretrained contextualized models. Cross-Model Results. Remarkably, we find that most tendencies we observe generalize well to all other pretrained models we study (specifically the optimality of f = mean, g = mean, the improved performance for larger N ,"
2020.acl-main.431,D14-1162,0,0.0936472,"ibuted to, and are consistent with, the original contextualized representations. Inspired by concerns with probing methods/diagnostic classifiers (Liu et al., 2019a; Hewitt and Liang, 2019) regarding whether learning can be attributed to the classifier and not the underlying representation, we employ an exceptionally simple parameter-free method for converting from contextualized to static representations to ensure that any properties observed in the latter are not introduced via this process. When evaluating static embedding performance, we consider Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) embeddings as baselines since they have been the most prominent pretrained static embeddings for several years. Similarly, we begin with BERT as the contextualized model as it is currently the most prominent in downstream use among the growing number of alternatives. We provide identical analyses for 4 other contextualized model architectures (GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019b), DistilBERT (Sanh et al., 2019)) and, in total, 9 sets of pretrained weights. All models, weights, and naming conventions used are enumerated in Appendix C and Table 9."
2020.acl-main.431,P19-1164,0,0.0122984,"here pooling is replaced by PCA. While this work demonstrated contextualized representations are highly contextual, our work naturally explores the complementary problem of what value can be extracted from the static analogue of these representations. Bias. Social bias in NLP has been primarily evaluated in three ways: (a) using geometric similarity between embeddings (Bolukbasi et al., 2016; Garg et al., 2018; Manzini et al., 2019), (b) adapting psychological association tests (Caliskan et al., 2017; May et al., 2019), and (c) considering downstream behavior (Zhao et al., 2017, 2018a, 2019a; Stanovsky et al., 2019).8 Our bias evaluation is in the style of (a) and we consider multi-class social bias in the lens of gender, race, and religion whereas prior work has centered on binary gender. Additionally, while most prior work has discussed the static embedding setting, recent work has considered sentence encoders and contextualized models. Zhao et al. (2019a) consider gender bias in ELMo when applied to coreference systems and Kurita et al. (2019) extend these results by leveraging the masked language modeling objective of BERT. Similarly, Basta et al. (2019) considers intrinsic gender bias in ELMo via ge"
2020.acl-main.431,D17-1075,0,0.0177795,"tors for word embeddings. 1 Introduction Word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011) have been a hallmark of modern natural language processing (NLP) for many years. Embedding methods have been broadly applied and have experienced parallel and complementary innovations alongside neural network methods for NLP. Advances in embedding quality in part have come from integrating additional information such as syntax (Levy and Goldberg, 2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh"
2020.acl-main.431,P19-1355,0,0.132351,"ements to potentially improve downstream practices in using contextualized models. Simultaneously, we find that our static embeddings substantially outperform Word2Vec and GloVe and therefore suggests our method serves the dual purpose of being a lightweight mechanism for generating static embeddings that track with advances in contextualized representations. Since static embeddings have significant advantages with respect to speed, computational resources, and ease of use, these results have important implications for resource-constrained settings (Shen et al., 2019), environmental concerns (Strubell et al., 2019), and the broader accessibility of NLP technologies.2 Alongside more developed methods for embedding analysis, the static embedding setting is also equipped with a richer body of work regarding social bias. In this sense, we view understanding the encoded social bias in representations as a societally critical special-case of interpretability research. We employ methods for identifying and quantifying gender, racial/ethnic, and religious bias (Bolukbasi et al., 2016; Garg et al., 2018; Manzini et al., 2019) to our static embeddings. These experiments not only shed light on the properties of ou"
2020.acl-main.431,P19-1159,0,0.0453788,"eddings. In particular, while estimates are seemingly stable to some types of choices regarding word lists, bias scores for a particular word embedding are tightly related to the definition being used and existing bias measures are markedly inconsistent with each other. We find this has important consequences beyond understanding the social biases in our representations. Concretely, we argue that without certainty regarding the extent to which embeddings are biased, it is impossible to properly interpret the meaningfulness of debiasing procedures (Bolukbasi et al., 2016; Zhao et al., 2018a,b; Sun et al., 2019) as we cannot reliably estimate the bias in the embeddings both before and after the procedure. This is further compounded with the existing evidence that current intrinsic measures of social bias may not handle geometric behavior such as clustering (Gonen and Goldberg, 2019). Cross-Model Bias Trends. In light of the above, next we compare bias estimates across different pretrained models in Table 3. Given the conflicting scores assigned by different definitions, we retain all definitions along with all social attributes in this comparison. However, we only consider target words given by Nprof"
2020.acl-main.431,P19-1452,0,0.0797123,"17), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh (2019) consider what is learned from context, Clark et al. (2019); Michel et al. (2019) look at specific attention heads, Hewitt and Manning (2019); Ettinger (2020) address linguistic understanding such as syntax and negation, and Wallace et al. (2019); Tan and Celis (2019) address ethical concerns such as security (adversarial robustness) and social bias. In fact, the neologism BERTology was coined specifically to describe this flurry of interpretability research.1 While these works have provided nua"
2020.acl-main.431,D19-1221,0,0.0127662,"s myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh (2019) consider what is learned from context, Clark et al. (2019); Michel et al. (2019) look at specific attention heads, Hewitt and Manning (2019); Ettinger (2020) address linguistic understanding such as syntax and negation, and Wallace et al. (2019); Tan and Celis (2019) address ethical concerns such as security (adversarial robustness) and social bias. In fact, the neologism BERTology was coined specifically to describe this flurry of interpretability research.1 While these works have provided nuanced finegrained analyses by creating new interpretability schema/techniques, we instead take an alternate approach of trying to re-purpose methods developed for analyzing static word embeddings. In order to employ static embedding interpretability methods to contextualized representations, we begin by proposing a simple strategy for converting"
2020.acl-main.431,D17-1027,0,0.0288884,"mbeddings. 1 Introduction Word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011) have been a hallmark of modern natural language processing (NLP) for many years. Embedding methods have been broadly applied and have experienced parallel and complementary innovations alongside neural network methods for NLP. Advances in embedding quality in part have come from integrating additional information such as syntax (Levy and Goldberg, 2014a; Li et al., 2017), morphology (Cotterell and Sch¨utze, 2015), subwords (Bojanowski et al., 2017), subcharacters (Stratos, 2017; Yu et al., 2017) and, most recently, context (Peters et al., 2018; Devlin et al., 2019). Due to their tremendous representational power, pretrained contextualized representations, in particular, have seen widespread adoption across myriad subareas of NLP. The recent dominance of pretrained contextualized representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) has served as the impetus for exciting and diverse interpretability research: Liu et al. (2019a); Tenney et al. (2019a) study what is learned across the layers of these models, Tenney et al. (2019b); Ethayarajh (2019) consider w"
2020.acl-main.431,N19-1064,0,0.0450437,"Missing"
2020.acl-main.431,D17-1323,0,0.0310644,"siders a similar method to ours where pooling is replaced by PCA. While this work demonstrated contextualized representations are highly contextual, our work naturally explores the complementary problem of what value can be extracted from the static analogue of these representations. Bias. Social bias in NLP has been primarily evaluated in three ways: (a) using geometric similarity between embeddings (Bolukbasi et al., 2016; Garg et al., 2018; Manzini et al., 2019), (b) adapting psychological association tests (Caliskan et al., 2017; May et al., 2019), and (c) considering downstream behavior (Zhao et al., 2017, 2018a, 2019a; Stanovsky et al., 2019).8 Our bias evaluation is in the style of (a) and we consider multi-class social bias in the lens of gender, race, and religion whereas prior work has centered on binary gender. Additionally, while most prior work has discussed the static embedding setting, recent work has considered sentence encoders and contextualized models. Zhao et al. (2019a) consider gender bias in ELMo when applied to coreference systems and Kurita et al. (2019) extend these results by leveraging the masked language modeling objective of BERT. Similarly, Basta et al. (2019) conside"
2020.acl-main.431,N18-2003,0,0.0238006,"for (static) word embeddings. In particular, while estimates are seemingly stable to some types of choices regarding word lists, bias scores for a particular word embedding are tightly related to the definition being used and existing bias measures are markedly inconsistent with each other. We find this has important consequences beyond understanding the social biases in our representations. Concretely, we argue that without certainty regarding the extent to which embeddings are biased, it is impossible to properly interpret the meaningfulness of debiasing procedures (Bolukbasi et al., 2016; Zhao et al., 2018a,b; Sun et al., 2019) as we cannot reliably estimate the bias in the embeddings both before and after the procedure. This is further compounded with the existing evidence that current intrinsic measures of social bias may not handle geometric behavior such as clustering (Gonen and Goldberg, 2019). Cross-Model Bias Trends. In light of the above, next we compare bias estimates across different pretrained models in Table 3. Given the conflicting scores assigned by different definitions, we retain all definitions along with all social attributes in this comparison. However, we only consider targe"
2020.acl-main.431,D18-1521,0,0.0224287,"for (static) word embeddings. In particular, while estimates are seemingly stable to some types of choices regarding word lists, bias scores for a particular word embedding are tightly related to the definition being used and existing bias measures are markedly inconsistent with each other. We find this has important consequences beyond understanding the social biases in our representations. Concretely, we argue that without certainty regarding the extent to which embeddings are biased, it is impossible to properly interpret the meaningfulness of debiasing procedures (Bolukbasi et al., 2016; Zhao et al., 2018a,b; Sun et al., 2019) as we cannot reliably estimate the bias in the embeddings both before and after the procedure. This is further compounded with the existing evidence that current intrinsic measures of social bias may not handle geometric behavior such as clustering (Gonen and Goldberg, 2019). Cross-Model Bias Trends. In light of the above, next we compare bias estimates across different pretrained models in Table 3. Given the conflicting scores assigned by different definitions, we retain all definitions along with all social attributes in this comparison. However, we only consider targe"
2020.acl-main.444,L18-1544,0,0.0141277,"lations between two arguments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in"
2020.acl-main.444,D18-1514,0,0.0201036,"d relation extraction. However, trigger identification is perhaps as difficult as relation extraction, and it is labor-intensive to annotate large-scale datasets with triggers. Future research may explore how to identify triggers based on a small amount of human-annotated triggers as seeds (Bronstein et al., 2015; Yu and Ji, 2016). 6 Error Analysis and Limitations Related Work Cross-Sentence Relation Extraction Datasets Different from the sentence-level relation extraction (RE) datasets (Roth and Yih, 2004; Hendrickx et al., 2010; Riedel et al., 2010; Zhang and Wang, 2015; Zhang et al., 2017; Han et al., 2018), in which relations are between two arguments in the same sentence, we focus on cross-sentence RE tasks (Ji et al., 2011; Surdeanu, 2013; Surdeanu and Ji, 2014) and present the first dialogue-based RE dataset, in which dialogues serve as input contexts instead of formally written sentences or documents. 4934 Task style/source of doc # rel cross rate◦ # doc # triples• 4 96 353 75.2 n/a n/a 960,000 101,873 3 million 140,661 881,298 11 million 1 96 15 36 n/a 40.7 n/a 95.6 1,500 5,053 4,991 1,788 2,434 56,354 13,425 8,068 —– distant supervision —– Peng et al. (2017) DocRED (Yao et al., 2019) T-RE"
2020.acl-main.444,S10-1006,0,0.0512395,"Missing"
2020.acl-main.444,N15-1086,0,0.0249294,"in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods, and experimental re"
2020.acl-main.444,D17-1274,0,0.0276444,"Missing"
2020.acl-main.444,W16-3612,0,0.300656,"ank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid and Blanco, 2018; Yang and Choi, 2019). Altogether, we annotate 10,168 relational triples. For each (subject, relation type, object) triple, we also annotate the minimal contiguous text span that most clearly expresses the relation; this may enable researchers to explore relation extraction methods that provide fine-grained explanations along with evidence sentences. For example, the bolded text span “brother” in Table 1 indicates the PER : SIBLINGS relation (R1 and R2) between speaker 2 (S2) and “Frank”. Our analysis of DialogRE indicates that"
2020.acl-main.444,2020.tacl-1.5,0,0.0476439,"Missing"
2020.acl-main.444,C10-2065,0,0.0927781,"Missing"
2020.acl-main.444,K17-1034,0,0.0286461,"tiple sentences; •: not include no-relation argument pairs). We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et"
2020.acl-main.444,P16-1200,0,0.0219825,"not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dia"
2020.acl-main.444,D18-1360,0,0.0220502,"i et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (S"
2020.acl-main.444,W19-1505,0,0.158706,"irs). We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2"
2020.acl-main.444,N18-1185,0,0.0438555,"ters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational"
2020.acl-main.444,D19-1069,0,0.0132178,"sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid"
2020.acl-main.444,P09-1113,0,0.143159,"Missing"
2020.acl-main.444,I13-1189,0,0.0607292,"Missing"
2020.acl-main.444,W15-1506,0,0.0549726,"Missing"
2020.acl-main.444,Q17-1008,0,0.0624783,"Missing"
2020.acl-main.444,D14-1162,0,0.0834244,"2 ] are two newly-introduced special tokens. In addition, we define a ˆk (k ∈ {1, 2}) to be [Sk ] if ∃i(si = ak ), and ak otherwise. The modified input sequence to ˆ BERT is [CLS]d[SEP]ˆ a1 [SEP]ˆ a2 [SEP]. In Appendix A.4, we investigate in three alternative input sequences. It is worth mentioning that a modification that does not disambiguate speaker arguments from other arguments performs substantially worse than the above speaker-aware modification. 5 Experiment 5.1 Implementation Details CNN, LSTM, and BiLSTM Baselines: The CNN/LSTM/BiLSTM encoder takes as features GloVe word embeddings (Pennington et al., 2014), mention embeddings, and type embeddings. We assign the same mention embedding to mentions of the same argument and obtain the type embeddings based on named entity types of the two arguments. We use spaCy4 for entity typing. Language Model Fine-Tuning: We use the uncased base model of BERT released by Devlin et al. (2019). We truncate a document when the input sequence length exceeds 512 and fine-tune BERT using a batch size of 24 and a learning rate of 3×10−5 4 https://spacy.io/. for 20 epochs. Other parameters remain unchanged. The embeddings of newly-introduced special tokens (e.g., [S1 ]"
2020.acl-main.444,N18-1202,0,0.0184035,"likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language"
2020.acl-main.444,D19-1005,0,0.0144051,"which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018;"
2020.acl-main.444,W09-2418,0,0.0987858,"Missing"
2020.acl-main.444,E17-1110,0,0.0558289,"Missing"
2020.acl-main.444,D18-1470,0,0.0603447,"Missing"
2020.acl-main.444,W04-2401,0,0.111748,"ROOMMATE and PER : SPOUSE). These experimental results show the critical role of triggers in dialogue-based relation extraction. However, trigger identification is perhaps as difficult as relation extraction, and it is labor-intensive to annotate large-scale datasets with triggers. Future research may explore how to identify triggers based on a small amount of human-annotated triggers as seeds (Bronstein et al., 2015; Yu and Ji, 2016). 6 Error Analysis and Limitations Related Work Cross-Sentence Relation Extraction Datasets Different from the sentence-level relation extraction (RE) datasets (Roth and Yih, 2004; Hendrickx et al., 2010; Riedel et al., 2010; Zhang and Wang, 2015; Zhang et al., 2017; Han et al., 2018), in which relations are between two arguments in the same sentence, we focus on cross-sentence RE tasks (Ji et al., 2011; Surdeanu, 2013; Surdeanu and Ji, 2014) and present the first dialogue-based RE dataset, in which dialogues serve as input contexts instead of formally written sentences or documents. 4934 Task style/source of doc # rel cross rate◦ # doc # triples• 4 96 353 75.2 n/a n/a 960,000 101,873 3 million 140,661 881,298 11 million 1 96 15 36 n/a 40.7 n/a 95.6 1,500 5,053 4,991 1"
2020.acl-main.444,D18-1246,0,0.0131017,"elation argument pairs). We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldi"
2020.acl-main.444,Q19-1014,1,0.86041,"; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue"
2020.acl-main.444,swampillai-stevenson-2010-inter,0,0.168912,"ialogRE is available at https:// dataset.org/dialogre/. 1 R1 R2 R3 R4 Trigger brother brother none none Relation type per:siblings per:siblings per:alternate names unanswerable Table 1: A dialogue and its associated instances in DialogRE. S1, S2: anoymized speaker of each utterance. Introduction Cross-sentence relation extraction, which aims to identify relations between two arguments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation"
2020.acl-main.444,W15-4631,0,0.0336045,"). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relati"
2020.acl-main.444,D19-1585,0,0.0200002,"sentation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang"
2020.acl-main.444,P11-1034,0,0.159616,"riples can be mapped to one of the 36 relation types in our relation schema (e.g., HUS BAND, EX - HUSBAND, and WIFE can be mapped to PER : SPOUSE ) except for the remaining relatively rare or implicit relation types such as PROM DATE and GENDER, and KISSED, demonstrating the relation schema we use for annotation is capable of covering most of the important relation types labeled by the encyclopedia community of contributors. On the other hand, the relatively small number of the existing triples and the moderate size of our annotated triples in DialogRE may suggest the low information density (Wang and Liu, 2011) in conversational speech in terms of relation extraction. For example, the average annotated triple per sentence in DialogRE is merely 0.21, compared to other exhaustively annotated datasets ACE (0.73) and KnowledgeNet (Mesquita et al., 2019) (1.44), in which corpora are formal written news reports and Wikipedia articles, respectively. 3.3 Discussions on Triggers As annotated triggers are rarely available in existing relation extraction datasets (Aguilar et al., 2014), the connections between different relation types and trigger existence are under-investigated. Relation Type: In DialogRE, 49"
2020.acl-main.444,W12-1642,1,0.787395,"aloguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods, and experimental results demonstrate that a speaker-aware extension on the best-performing model leads to substantial gains in both the standard and conversational settings."
2020.acl-main.444,D15-1206,0,0.0243389,"9) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom in"
2020.acl-main.444,W19-5923,0,0.0503467,"019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speaker"
2020.acl-main.444,P19-1074,0,0.534776,"guments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019; † Equal contribution. Argument pair (Frank, S2) (S2, Frank) (S2, Pheebs) (S1, Pheebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research"
2020.acl-main.444,W11-2008,0,0.0380999,"e seldom investigated in previous RE methods. Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehension (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (Kl¨uwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation. 7 Conclusions We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods"
2020.acl-main.444,P16-1005,1,0.830453,"Missing"
2020.acl-main.444,D15-1203,0,0.0273618,"this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013). Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previ"
2020.acl-main.444,C14-1220,0,0.253942,"number of practical real-time dialoguebased applications such as chatbots, which would likely require recognition of a relation at its first mention in an interactive conversation. To encourage automated methods to identify the relationship between two arguments in a dialogue as early as possible, we further design a new performance evaluation metric for the conversational setting, which can be used as a supplement to the standard F1 measure (Section 4.1). In addition to dataset creation and metric design, we adapt a number of strong, representative learning-based relation extraction methods (Zeng et al., 2014; Cai et al., 2016; Yao et al., 2019; Devlin et al., 2019) and evaluate them on DialogRE to establish baseline results on the dataset going forward. We also extend the best-performing method (Devlin et al., 2019) among them by letting the model be aware of the existence of arguments that are dialogue participants (Section 4.2). Experiments on DialogRE demonstrate that this simple extension nevertheless yields substantial gains on both standard and conversational RE evaluation metrics, supporting our assumption regarding the critical role of tracking speakers in dialogue-based relation extracti"
2020.acl-main.444,D17-1004,0,0.252438,"rn, supporting our hypothesis that the positions of arguments and triggers may be good indicators for estimating the minimum turns for humans to make predictions. For convenience, we use BERT for the following discussions and comparisons. Ground Truth Argument Types: Methods in Table 5 are not provided with ground truth argument types considering the unavailability of this kind of annotation in practical use. To study the impacts of argument types on DialogRE, we report the performance of four methods, each of which additionally takes as input the ground truth argument types as previous work (Zhang et al., 2017; Yao et al., 2019). We adopt the same baseline for a direct comparison 4933 except that the input sequence is changed. 5.3 In Method 1, we simply extend the original input sequence of BERT (Section 4.2) with newly-introduced special tokens that represent argument types. The input sequence is [CLS]d[SEP]τ1 a1 [SEP]τ2 a2 [SEP], where τi is a special token representing the argument type of ai (i ∈ {1, 2}). For example, given a1 of type PER and a2 of type STRING, τ1 is [PER] and τ2 is [STRING]. In Method 2, we extend the input sequence of BERTS with τi defined in Method ˆ 1 (i.e., [CLS]d[SEP]τ ˆ1"
2020.acl-main.444,C18-1003,0,0.15861,"heebs) Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied. In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid and Blanco, 2018; Yang and Choi, 2019). Altogether, we annotate 10,168 relational triples. For each (subject, relation type, object) triple, we also annotate the minimal contiguous text span that most clearly expresses the relation; this may enable researchers to explore relation extraction methods that provide fine-grained explanations along with evidence sentences. For example, the bolded text span “brother” in Table 1 indicates the PER : SIBLINGS relation (R1 and R2) between speaker 2 (S2) and “Frank”. Our analysis of DialogRE indicates that the supporting text for most (approximat"
2020.acl-main.444,D12-1062,0,\N,Missing
2020.acl-main.444,P15-2061,0,\N,Missing
2020.acl-main.444,doddington-etal-2004-automatic,0,\N,Missing
2020.acl-main.444,P16-1072,0,\N,Missing
2020.acl-main.444,catizone-etal-2010-using,0,\N,Missing
2020.acl-main.444,K17-1023,0,\N,Missing
2020.acl-main.444,S18-1007,0,\N,Missing
2020.acl-main.444,N18-1075,0,\N,Missing
2020.acl-main.444,N19-1423,0,\N,Missing
2020.acl-main.444,W14-2907,0,\N,Missing
2020.acl-main.714,D13-1185,0,0.137981,", not event-related) via a cohesion classifier (with discourse features). Similar to Huang and Riloff (2012), we also incorporate both intra-sentence and cross-sentence features (paragraph-level features), but instead of using manually designed linguistic information, our models learn in an automatic way how to dynamically incorporate learned representations of the article. Also, in contrast to prior work that is pipeline-based, our approach tackles the task as an end-to-end sequence tagging problem. There has also been work on unsupervised event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013) and open-domain event extraction (Liu et al., 2019) from documents: the main idea is to group entities corresponding to the same role into an event template. Our models, on the other hand, are trained in supervised way and the event schemas are pre-defined. Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019). This work assumes that mentions are provided, and thus is more of a mention/entity-level classification problem. Our work instead focuses on role filler/span extraction using seque"
2020.acl-main.714,P11-1098,0,0.0430034,"e in spurious sentences (i.e., not event-related) via a cohesion classifier (with discourse features). Similar to Huang and Riloff (2012), we also incorporate both intra-sentence and cross-sentence features (paragraph-level features), but instead of using manually designed linguistic information, our models learn in an automatic way how to dynamically incorporate learned representations of the article. Also, in contrast to prior work that is pipeline-based, our approach tackles the task as an end-to-end sequence tagging problem. There has also been work on unsupervised event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013) and open-domain event extraction (Liu et al., 2019) from documents: the main idea is to group entities corresponding to the same role into an event template. Our models, on the other hand, are trained in supervised way and the event schemas are pre-defined. Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019). This work assumes that mentions are provided, and thus is more of a mention/entity-level classification problem. Our work instead focuses on role filler/span extra"
2020.acl-main.714,P15-1017,0,0.372687,"equire heavy feature engineering (e.g., lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019). However, to the best of our knowledge, no prior work has investigated the formulation of document-level event role filler extraction as an end-to-end neural sequence learning task. In contrast to extracting events and their role fillers from standalone sentences, document-level event extraction poses special challenges for neural sequence learning models. First, capturing long-term dependencies in long sequences remains a fundamental challenge for recurrent neural networks (Trinh et al., 2018). To model long sequences, most RNN-based approaches use"
2020.acl-main.714,D14-1179,0,0.0180691,"Missing"
2020.acl-main.714,N19-1423,0,0.451828,"nts and their role fillers from standalone sentences, document-level event extraction poses special challenges for neural sequence learning models. First, capturing long-term dependencies in long sequences remains a fundamental challenge for recurrent neural networks (Trinh et al., 2018). To model long sequences, most RNN-based approaches use backpropagation through time. But it’s still difficult for the models to scale to very long sequences. We provide empirical evidence for this for event extraction in Section 4.3. Second, although pretrained bi-directional transformer models such as BERT (Devlin et al., 2019) better capture long-distance dependencies as compared to an RNN architecture, they still have a constraint on the maximum length of the sequence, which is below the length of many articles about events. In the sections below, we study how to train and apply end-to-end neural models for event role filler extraction. We first formalize the problem as a sequence tagging task over the tokens in a set of contiguous sentences in the document. To address the aforementioned challenges for neural models applied to long sequences, (1) we investigate the effect of context length (i.e., maximum input seg"
2020.acl-main.714,doddington-etal-2004-automatic,0,0.153426,"the broader context (e.g., paragraph-level). A quantitative evaluation and qualitative analysis of our approach on the MUC-4 dataset (MUC-4, 1992) both show that the multi-granularity reader achieves substantial improvements over the baseline models and prior work. For replication purposes, our repository for the evaluation and preprocessing scripts will be available at https://github.com/xinyadu/doc_ event_role. 2 Related Work Event extraction has been mainly studied under two paradigms: detecting the event trigger and extracting the arguments from an individual sentence (e.g., the ACE task (Doddington et al., 2004)2 , vs. at the document level (e.g., the MUC-4 template-filling task (Sundheim, 1992)). Sentence-level Event Extraction The ACE event extraction task requires extraction of the event trigger and its arguments from a sentence. For example, in the sentence “ ... Iraqi soldiers were killed by U.S. artillery ...”, the goal is to identify the “die” event triggered by killed and the corresponding arguments (PLACE, VICTIM, INSTRU MENT, etc.). Many approaches have been proposed to improve performance on this specific task. Li et al. (2013, 2015) explore various hand-designed features; Nguyen and Grish"
2020.acl-main.714,I17-1036,0,0.0496346,"tualized representations. The approaches generally focus on sentence-level context for extracting event triggers and arguments and rarely generalize to the document-event extraction setting (Figure 1). Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) enforce event role consistency across documents. Liao and Grishman (2010) explore event type co-occurrence patterns to propagate event classification decisions. Similarly, Yang and Mitchell (2016) propose jointly extracting events and entities within a document context. Also related to our work are Duan et al. (2017) and Zhao et al. (2018), which utilize document embeddings to aid 2 https://catalog.ldc.upenn.edu/ LDC2006T06 8011 event detection with recurrent neural networks. Although these approaches make decisions with crosssentence information, their extractions are still at the sentence level. Document-level Event Extraction has been studied mainly under the classic MUC paradigm (MUC-4, 1992). The full task involves the construction of answer key templates, one template per event (some documents in the dataset describe more than one events). Typically three steps are involved — role filler extraction,"
2020.acl-main.714,P11-1114,0,0.49492,"in facilitating downstream applications such as information retrieval and article summarization (Yang and Mitchell, 2016), and for real-life applications such as trends analysis of world events (Sundheim, 1992). Recent work in document-level event role filler extraction has employed a pipeline architecture with separate classifiers for each type of role and for 8010 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8010–8020 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011). However these methods: (1) suffer from error propagation across different pipeline stages; and (2) require heavy feature engineering (e.g., lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity rec"
2020.acl-main.714,P08-1030,0,0.240929,"sk. Li et al. (2013, 2015) explore various hand-designed features; Nguyen and Grishman (2015); Nguyen et al. (2016); Chen et al. (2015); Liu et al. (2017, 2018) employ deep learning based models such as recurrent neural networks (RNNs) and convolutional neural network (CNN). Wadden et al. (2019) utilize pre-trained contextualized representations. The approaches generally focus on sentence-level context for extracting event triggers and arguments and rarely generalize to the document-event extraction setting (Figure 1). Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) enforce event role consistency across documents. Liao and Grishman (2010) explore event type co-occurrence patterns to propagate event classification decisions. Similarly, Yang and Mitchell (2016) propose jointly extracting events and entities within a document context. Also related to our work are Duan et al. (2017) and Zhao et al. (2018), which utilize document embeddings to aid 2 https://catalog.ldc.upenn.edu/ LDC2006T06 8011 event detection with recurrent neural networks. Although these approaches make decisions with crosssentence information, their extractions are still at the sentence l"
2020.acl-main.714,N19-1370,0,0.0975689,"Missing"
2020.acl-main.714,D19-1588,0,0.0351471,"Missing"
2020.acl-main.714,N16-1030,0,0.31128,"these methods: (1) suffer from error propagation across different pipeline stages; and (2) require heavy feature engineering (e.g., lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019). However, to the best of our knowledge, no prior work has investigated the formulation of document-level event role filler extraction as an end-to-end neural sequence learning task. In contrast to extracting events and their role fillers from standalone sentences, document-level event extraction poses special challenges for neural sequence learning models. First, capturing long-term dependencies in long sequences remains a fundamental challenge for recurrent neu"
2020.acl-main.714,P13-1008,0,0.211604,"ments from an individual sentence (e.g., the ACE task (Doddington et al., 2004)2 , vs. at the document level (e.g., the MUC-4 template-filling task (Sundheim, 1992)). Sentence-level Event Extraction The ACE event extraction task requires extraction of the event trigger and its arguments from a sentence. For example, in the sentence “ ... Iraqi soldiers were killed by U.S. artillery ...”, the goal is to identify the “die” event triggered by killed and the corresponding arguments (PLACE, VICTIM, INSTRU MENT, etc.). Many approaches have been proposed to improve performance on this specific task. Li et al. (2013, 2015) explore various hand-designed features; Nguyen and Grishman (2015); Nguyen et al. (2016); Chen et al. (2015); Liu et al. (2017, 2018) employ deep learning based models such as recurrent neural networks (RNNs) and convolutional neural network (CNN). Wadden et al. (2019) utilize pre-trained contextualized representations. The approaches generally focus on sentence-level context for extracting event triggers and arguments and rarely generalize to the document-event extraction setting (Figure 1). Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (20"
2020.acl-main.714,W15-4502,0,0.363451,"Missing"
2020.acl-main.714,P10-1081,0,0.172829,"n and Grishman (2015); Nguyen et al. (2016); Chen et al. (2015); Liu et al. (2017, 2018) employ deep learning based models such as recurrent neural networks (RNNs) and convolutional neural network (CNN). Wadden et al. (2019) utilize pre-trained contextualized representations. The approaches generally focus on sentence-level context for extracting event triggers and arguments and rarely generalize to the document-event extraction setting (Figure 1). Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) enforce event role consistency across documents. Liao and Grishman (2010) explore event type co-occurrence patterns to propagate event classification decisions. Similarly, Yang and Mitchell (2016) propose jointly extracting events and entities within a document context. Also related to our work are Duan et al. (2017) and Zhao et al. (2018), which utilize document embeddings to aid 2 https://catalog.ldc.upenn.edu/ LDC2006T06 8011 event detection with recurrent neural networks. Although these approaches make decisions with crosssentence information, their extractions are still at the sentence level. Document-level Event Extraction has been studied mainly under the cl"
2020.acl-main.714,P17-1164,0,0.0611019,"e-filling task (Sundheim, 1992)). Sentence-level Event Extraction The ACE event extraction task requires extraction of the event trigger and its arguments from a sentence. For example, in the sentence “ ... Iraqi soldiers were killed by U.S. artillery ...”, the goal is to identify the “die” event triggered by killed and the corresponding arguments (PLACE, VICTIM, INSTRU MENT, etc.). Many approaches have been proposed to improve performance on this specific task. Li et al. (2013, 2015) explore various hand-designed features; Nguyen and Grishman (2015); Nguyen et al. (2016); Chen et al. (2015); Liu et al. (2017, 2018) employ deep learning based models such as recurrent neural networks (RNNs) and convolutional neural network (CNN). Wadden et al. (2019) utilize pre-trained contextualized representations. The approaches generally focus on sentence-level context for extracting event triggers and arguments and rarely generalize to the document-event extraction setting (Figure 1). Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) enforce event role consistency across documents. Liao and Grishman (2010) explore event type co-occurrence patterns to propagate e"
2020.acl-main.714,P19-1276,0,0.0531963,"th discourse features). Similar to Huang and Riloff (2012), we also incorporate both intra-sentence and cross-sentence features (paragraph-level features), but instead of using manually designed linguistic information, our models learn in an automatic way how to dynamically incorporate learned representations of the article. Also, in contrast to prior work that is pipeline-based, our approach tackles the task as an end-to-end sequence tagging problem. There has also been work on unsupervised event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013) and open-domain event extraction (Liu et al., 2019) from documents: the main idea is to group entities corresponding to the same role into an event template. Our models, on the other hand, are trained in supervised way and the event schemas are pre-defined. Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019). This work assumes that mentions are provided, and thus is more of a mention/entity-level classification problem. Our work instead focuses on role filler/span extraction using sequence tagging approaches; role filler type is determin"
2020.acl-main.714,D18-1156,0,0.225205,"Missing"
2020.acl-main.714,P09-1113,0,0.0645955,"Also, in contrast to prior work that is pipeline-based, our approach tackles the task as an end-to-end sequence tagging problem. There has also been work on unsupervised event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013) and open-domain event extraction (Liu et al., 2019) from documents: the main idea is to group entities corresponding to the same role into an event template. Our models, on the other hand, are trained in supervised way and the event schemas are pre-defined. Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019). This work assumes that mentions are provided, and thus is more of a mention/entity-level classification problem. Our work instead focuses on role filler/span extraction using sequence tagging approaches; role filler type is determined during this process. Capturing Long-term Dependencies for Neural Sequence Models For training neural sequence models such as RNNs, capturing long-term dependencies in sequences remains a fundamental challenge (Trinh et al., 2018). Most approaches use backpropagation through time (BPTT) but it is difficult to scale to very l"
2020.acl-main.714,N16-1034,0,0.532603,"e engineering (e.g., lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019). However, to the best of our knowledge, no prior work has investigated the formulation of document-level event role filler extraction as an end-to-end neural sequence learning task. In contrast to extracting events and their role fillers from standalone sentences, document-level event extraction poses special challenges for neural sequence learning models. First, capturing long-term dependencies in long sequences remains a fundamental challenge for recurrent neural networks (Trinh et al., 2018). To model long sequences, most RNN-based approaches use backpropagation throu"
2020.acl-main.714,P15-2060,0,0.350724,"0 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011). However these methods: (1) suffer from error propagation across different pipeline stages; and (2) require heavy feature engineering (e.g., lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019). However, to the best of our knowledge, no prior work has investigated the formulation of document-level event role filler extraction as an end-to-end neural sequence learning task. In contrast to extracting events and their role fillers from standalone sentences, document-level event extraction poses special ch"
2020.acl-main.714,D09-1016,0,0.65184,"tions for events is essential in facilitating downstream applications such as information retrieval and article summarization (Yang and Mitchell, 2016), and for real-life applications such as trends analysis of world events (Sundheim, 1992). Recent work in document-level event role filler extraction has employed a pipeline architecture with separate classifiers for each type of role and for 8010 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8010–8020 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011). However these methods: (1) suffer from error propagation across different pipeline stages; and (2) require heavy feature engineering (e.g., lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks,"
2020.acl-main.714,Q17-1008,0,0.0363468,"prior work that is pipeline-based, our approach tackles the task as an end-to-end sequence tagging problem. There has also been work on unsupervised event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013) and open-domain event extraction (Liu et al., 2019) from documents: the main idea is to group entities corresponding to the same role into an event template. Our models, on the other hand, are trained in supervised way and the event schemas are pre-defined. Apart from event extraction, there has been increasing interest on cross-sentence relation extraction (Mintz et al., 2009; Peng et al., 2017; Jia et al., 2019). This work assumes that mentions are provided, and thus is more of a mention/entity-level classification problem. Our work instead focuses on role filler/span extraction using sequence tagging approaches; role filler type is determined during this process. Capturing Long-term Dependencies for Neural Sequence Models For training neural sequence models such as RNNs, capturing long-term dependencies in sequences remains a fundamental challenge (Trinh et al., 2018). Most approaches use backpropagation through time (BPTT) but it is difficult to scale to very long sequences. Many"
2020.acl-main.714,D14-1162,0,0.0884867,") (k) {x1 , x2 , ..., xl1 , ..., x1 , x2 , ..., xlk }; where 3.2 k-sentence Reader Since our general k-sentence reader does not recognize sentence boundaries, we simplify the notation for the input sequence as {x1 , x2 , ..., xm } here. Embedding Layer In the embedding layer, we represent each token xi in the input sequence as the concatenation of its word embedding and contextual token representation: (k) xi is the i-th token of the k-th sentence, and lk is the length of the k-th sentence. 3 https://spacy.io/ 8013 • Word Embedding: We use the 100dimensional GloVe pre-trained word embeddings (Pennington et al., 2014) trained from 6B Web crawl data. We keep the pre-trained word embeddings fixed. Given a token xi , we have its word embedding: xei = E(xi ). • Pre-trained LM representation: Contextualized embeddings produced by pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proved to be capable of modeling context beyond the sentence boundary and improve performance on a variety of tasks. Here we employ the contextualized representations produced by BERT-base for our k-sentence labeling model, as well as the multi-granularity reader to be introduced next. Specifically, we use"
2020.acl-main.714,N18-1202,0,0.0276361,"ding layer, we represent each token xi in the input sequence as the concatenation of its word embedding and contextual token representation: (k) xi is the i-th token of the k-th sentence, and lk is the length of the k-th sentence. 3 https://spacy.io/ 8013 • Word Embedding: We use the 100dimensional GloVe pre-trained word embeddings (Pennington et al., 2014) trained from 6B Web crawl data. We keep the pre-trained word embeddings fixed. Given a token xi , we have its word embedding: xei = E(xi ). • Pre-trained LM representation: Contextualized embeddings produced by pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proved to be capable of modeling context beyond the sentence boundary and improve performance on a variety of tasks. Here we employ the contextualized representations produced by BERT-base for our k-sentence labeling model, as well as the multi-granularity reader to be introduced next. Specifically, we use the average of all the 12 layers’ representations and freeze the weights (Peters et al., 2019) during training after empirical trials4 . Given the sequence {x1 , x2 , ..., xm }, we have: xb1 , xb2 , ..., xbm = BERT(x1 , x2 , ..., xm ) We forward the concatena"
2020.acl-main.714,W19-4302,0,0.0392644,"Missing"
2020.acl-main.714,M92-1001,0,0.635527,"“Teofilo Forero Castro” (mentioned in S3) as a victim of the car bomb attack event (mentioned in S2), determining there’s no role filler in S4 (both of which rely mainly on sentence-level understanding, and identifying “four terrorists” in S1 as a perpetrator individual (which requires coreference resolution across sentence boundaries). Generating the document-level extractions for events is essential in facilitating downstream applications such as information retrieval and article summarization (Yang and Mitchell, 2016), and for real-life applications such as trends analysis of world events (Sundheim, 1992). Recent work in document-level event role filler extraction has employed a pipeline architecture with separate classifiers for each type of role and for 8010 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8010–8020 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011). However these methods: (1) suffer from error propagation across different pipeline stages; and (2) require heavy feature engineering (e.g., lexico-syntactic pattern features for candidat"
2020.acl-main.714,D19-1585,0,0.22358,"lexico-syntactic pattern features for candidate role filler extraction; lexical bridge and discourse bridge features for detecting event-relevant sentences at the document level). Moreover, the features are manually designed for a particular domain, which requires linguistic intuition and domain expertise (Nguyen and Grishman, 2015). Neural end-to-end models have been shown to excel at sentence-level information extraction tasks, such as named entity recognition (Lample et al., 2016; Chiu and Nichols, 2016) and ACE-type within-sentence event extraction (Chen et al., 2015; Nguyen et al., 2016; Wadden et al., 2019). However, to the best of our knowledge, no prior work has investigated the formulation of document-level event role filler extraction as an end-to-end neural sequence learning task. In contrast to extracting events and their role fillers from standalone sentences, document-level event extraction poses special challenges for neural sequence learning models. First, capturing long-term dependencies in long sequences remains a fundamental challenge for recurrent neural networks (Trinh et al., 2018). To model long sequences, most RNN-based approaches use backpropagation through time. But it’s stil"
2020.acl-main.714,N16-1033,0,0.388605,"ding and accurate interpretation of the context beyond the sentence. Examples include identifying “Teofilo Forero Castro” (mentioned in S3) as a victim of the car bomb attack event (mentioned in S2), determining there’s no role filler in S4 (both of which rely mainly on sentence-level understanding, and identifying “four terrorists” in S1 as a perpetrator individual (which requires coreference resolution across sentence boundaries). Generating the document-level extractions for events is essential in facilitating downstream applications such as information retrieval and article summarization (Yang and Mitchell, 2016), and for real-life applications such as trends analysis of world events (Sundheim, 1992). Recent work in document-level event role filler extraction has employed a pipeline architecture with separate classifiers for each type of role and for 8010 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8010–8020 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relevant context detection (Patwardhan and Riloff, 2009; Huang and Riloff, 2011). However these methods: (1) suffer from error propagation across different pipeline stages; and (2"
2020.acl-main.714,P18-2066,0,0.167755,"s. The approaches generally focus on sentence-level context for extracting event triggers and arguments and rarely generalize to the document-event extraction setting (Figure 1). Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) enforce event role consistency across documents. Liao and Grishman (2010) explore event type co-occurrence patterns to propagate event classification decisions. Similarly, Yang and Mitchell (2016) propose jointly extracting events and entities within a document context. Also related to our work are Duan et al. (2017) and Zhao et al. (2018), which utilize document embeddings to aid 2 https://catalog.ldc.upenn.edu/ LDC2006T06 8011 event detection with recurrent neural networks. Although these approaches make decisions with crosssentence information, their extractions are still at the sentence level. Document-level Event Extraction has been studied mainly under the classic MUC paradigm (MUC-4, 1992). The full task involves the construction of answer key templates, one template per event (some documents in the dataset describe more than one events). Typically three steps are involved — role filler extraction, role filler mention co"
2020.acl-main.714,Q16-1026,0,\N,Missing
2020.emnlp-main.49,P17-1123,1,0.8697,"eral case of question generation from sentences, answer phrases can be noun phrases, prepositional phrases, or subordinate clauses. Complicated rules are designed with help from superTagger (Ciaramita and Altun, 2006). In our case, event arguments are mostly noun phrases and the rules are simpler – “who” for person, “where” for place and “what” for all other types of entities. We sample around 10 examples from the development set to determine the entity type of each argument role. In the future, it is interesting to investigate how to utilize machine learning-based question generation method (Du et al., 2017), which would be more beneficial for schema/ontology containing a large number of event argument types. 6 Conclusion In this paper, we introduce a new paradigm for event extraction based on question answering. We investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance. Our framework outperforms prior works on the ACE 2005 benchmark, and is capable of extracting event arguments of roles not seen at training time. For future work, it would be"
2020.emnlp-main.49,C96-1079,0,0.738998,"as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zeroshot learning setting).1 1 Event extraction is a long-studied and challenging task in Information Extraction (IE) (Sundheim, 1992; Grishman and Sundheim, 1996; Riloff, 1996). Its goal is to extract structured information — “what is happening” and the persons/objects that are involved — from unstructured text. The task is illustrated via an example in Figure 1, which depicts an ownership transfer event (the event type), triggered by the word “sale"" (the event trigger) and accompanied by its extracted arguments — text spans denoting entities that fill a set of (semantic) roles associated with the event type (e.g., BUYER, SELLER and ARTIFACT for ownership transfer events). Recent successful approaches to event extraction have benefited from dense feat"
2020.emnlp-main.49,N10-1086,0,0.0816034,"gies incorporating synonyms and examples for named entity recognition. Different from the works above, we focus on the more complex event extraction task, which involves both trigger detection and argument extraction. Our generated questions for extracting event arguments are more natural (incorporating descriptions from annotation guidelines) and leverage trigger information. Question Generation To generate question templates 2&3 (Type + Role question and annotation guideline based question) which are more natural, we draw insights from literature of automatic rule-based question generation (Heilman and Smith, 2010). Heilman (2011) propose to use linguistically motivated rules for WH word (question phrase) selection. In their more general case of question generation from sentences, answer phrases can be noun phrases, prepositional phrases, or subordinate clauses. Complicated rules are designed with help from superTagger (Ciaramita and Altun, 2006). In our case, event arguments are mostly noun phrases and the rules are simpler – “who” for person, “where” for place and “what” for all other types of entities. We sample around 10 examples from the development set to determine the entity type of each argument"
2020.emnlp-main.49,P18-1201,0,0.063297,"Missing"
2020.emnlp-main.49,P11-1114,0,0.0992773,"ion In this paper, we introduce a new paradigm for event extraction based on question answering. We investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance. Our framework outperforms prior works on the ACE 2005 benchmark, and is capable of extracting event arguments of roles not seen at training time. For future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (Ji and Grishman, 2008; Huang and Riloff, 2011; Du and Cardie, 2020) in our methods to improve the accuracy of the predictions. Acknowledgments We thank the anonymous reviewers and Heng Ji for helpful suggestions. This research is based on work supported in part by DARPA LwLL Grant FA8750-19-2-0039. 679 References Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Jun Zhao. 2015. Event extraction via dynamic multipooling convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), p"
2020.emnlp-main.49,P08-1030,0,0.390786,"gument types. 6 Conclusion In this paper, we introduce a new paradigm for event extraction based on question answering. We investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance. Our framework outperforms prior works on the ACE 2005 benchmark, and is capable of extracting event arguments of roles not seen at training time. For future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context (Ji and Grishman, 2008; Huang and Riloff, 2011; Du and Cardie, 2020) in our methods to improve the accuracy of the predictions. Acknowledgments We thank the anonymous reviewers and Heng Ji for helpful suggestions. This research is based on work supported in part by DARPA LwLL Grant FA8750-19-2-0039. 679 References Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Jun Zhao. 2015. Event extraction via dynamic multipooling convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (V"
2020.emnlp-main.49,P17-1147,0,0.030024,"alized representations produced by pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proved to be helpful for event extraction (Zhang et al., 2019b; Wadden et al., 2019) and question answering (Rajpurkar et al., 2016). The attention mechanism helps capture relationships between tokens in question and input sequence. We use BERT in our framework for capturing semantic relationship between question and input sentence. Machine Reading Comprehension (MRC) Span-based MRC tasks involve extracting a span from a paragraph (Rajpurkar et al., 2016) or multiple paragraphs (Joshi et al., 2017; Kwiatkowski et al., 2019). Recently, there have been explorations on formulating NLP tasks as a question answering problem. McCann et al. (2018) propose natural language decathlon challenge (decaNLP), which consists of ten tasks (e.g., machine translation, summarization, question answering, etc.) They cast all tasks as question answering over a context and propose a general model for this. In the information extraction literature, Levy et al. (2017) propose the zero-shot relation extraction task and reduce the task to answering crowd-sourced reading comprehension questions. Li et al. (2019)"
2020.emnlp-main.49,Q19-1026,0,0.0188547,"ns produced by pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proved to be helpful for event extraction (Zhang et al., 2019b; Wadden et al., 2019) and question answering (Rajpurkar et al., 2016). The attention mechanism helps capture relationships between tokens in question and input sequence. We use BERT in our framework for capturing semantic relationship between question and input sentence. Machine Reading Comprehension (MRC) Span-based MRC tasks involve extracting a span from a paragraph (Rajpurkar et al., 2016) or multiple paragraphs (Joshi et al., 2017; Kwiatkowski et al., 2019). Recently, there have been explorations on formulating NLP tasks as a question answering problem. McCann et al. (2018) propose natural language decathlon challenge (decaNLP), which consists of ten tasks (e.g., machine translation, summarization, question answering, etc.) They cast all tasks as question answering over a context and propose a general model for this. In the information extraction literature, Levy et al. (2017) propose the zero-shot relation extraction task and reduce the task to answering crowd-sourced reading comprehension questions. Li et al. (2019) casts entity-relation extra"
2020.emnlp-main.49,K17-1034,0,0.368964,", 2013) still occurs during event argument extraction. A second weakness of neural approaches to event extraction is their inability to exploit the similarities of related argument roles across event types. For example, the ACE 2005 (Doddington et al., 2004) C ONFLICT.ATTACK events and J US TICE .E XECUTE events have TARGET and PERSON argument roles, respectively. Both roles, however, refer to a human being (who) is affected by an action. Ignoring the similarity can hurt performance, especially for argument roles with few/no examples at training time (e.g., similar to the zero-shot setting in Levy et al. (2017)). In this paper, we propose a new paradigm for the event extraction task – formulating it as a question answering (QA)/machine reading comprehension (MRC) task (Contribution 1). The general framework is illustrated in Figure 2. Using BERT (Devlin 671 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 671–683, c November 16–20, 2020. 2020 Association for Computational Linguistics Input sentence: As part of the 11-billion-dollar sale of USA Interactive&apos;s film and television operations … Trigger question template instantiation [CLS] the action [SEP] As"
2020.emnlp-main.49,P13-1008,0,0.627327,"Introduction 1 Extracted Event: The approaches, however, exhibit two key weaknesses. First, they rely heavily on entity information for argument extraction. In particular, event argument extraction generally consists of two steps – first identifying entities and their general semantic class with trained models (Wadden et al., 2019) or a parser (Sha et al., 2018), then assigning argument roles (or no role) to each entity. Although joint models (Yang and Mitchell, 2016; Nguyen and Nguyen, 2019; Zhang et al., 2019a; Lin et al., 2020) have been proposed to mitigate this issue, error propagation (Li et al., 2013) still occurs during event argument extraction. A second weakness of neural approaches to event extraction is their inability to exploit the similarities of related argument roles across event types. For example, the ACE 2005 (Doddington et al., 2004) C ONFLICT.ATTACK events and J US TICE .E XECUTE events have TARGET and PERSON argument roles, respectively. Both roles, however, refer to a human being (who) is affected by an action. Ignoring the similarity can hurt performance, especially for argument roles with few/no examples at training time (e.g., similar to the zero-shot setting in Levy et"
2020.emnlp-main.49,2020.acl-main.519,0,0.0473852,", etc.) They cast all tasks as question answering over a context and propose a general model for this. In the information extraction literature, Levy et al. (2017) propose the zero-shot relation extraction task and reduce the task to answering crowd-sourced reading comprehension questions. Li et al. (2019) casts entity-relation extraction as a multi-turn question answering task. Their questions lack diversity and naturalness. For example for the PART-WHOLE relation, the template questions is “find Y that belongs to X”, where X is instantiated with the pre-given entity. The follow-up work from Li et al. (2020) propose better query strategies incorporating synonyms and examples for named entity recognition. Different from the works above, we focus on the more complex event extraction task, which involves both trigger detection and argument extraction. Our generated questions for extracting event arguments are more natural (incorporating descriptions from annotation guidelines) and leverage trigger information. Question Generation To generate question templates 2&3 (Type + Role question and annotation guideline based question) which are more natural, we draw insights from literature of automatic rule"
2020.emnlp-main.49,P19-1129,0,0.0627991,"Missing"
2020.emnlp-main.49,2020.acl-main.713,0,0.149345,"1: Event extraction example from the ACE 2005 corpus (Doddington et al., 2004). Introduction 1 Extracted Event: The approaches, however, exhibit two key weaknesses. First, they rely heavily on entity information for argument extraction. In particular, event argument extraction generally consists of two steps – first identifying entities and their general semantic class with trained models (Wadden et al., 2019) or a parser (Sha et al., 2018), then assigning argument roles (or no role) to each entity. Although joint models (Yang and Mitchell, 2016; Nguyen and Nguyen, 2019; Zhang et al., 2019a; Lin et al., 2020) have been proposed to mitigate this issue, error propagation (Li et al., 2013) still occurs during event argument extraction. A second weakness of neural approaches to event extraction is their inability to exploit the similarities of related argument roles across event types. For example, the ACE 2005 (Doddington et al., 2004) C ONFLICT.ATTACK events and J US TICE .E XECUTE events have TARGET and PERSON argument roles, respectively. Both roles, however, refer to a human being (who) is affected by an action. Ignoring the similarity can hurt performance, especially for argument roles with few/"
2020.emnlp-main.49,D16-1264,0,0.0599079,"gument extraction. Also related to our work includes Wadden et al. (2019), they model the entity/argument spans (with start and end offset) instead of labeling with BIO scheme. Different from our work, their learned span representations are later used to predict the entity/argument type. While our QA model directly extract the spans for certain argument role type. Contextualized representations produced by pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proved to be helpful for event extraction (Zhang et al., 2019b; Wadden et al., 2019) and question answering (Rajpurkar et al., 2016). The attention mechanism helps capture relationships between tokens in question and input sequence. We use BERT in our framework for capturing semantic relationship between question and input sentence. Machine Reading Comprehension (MRC) Span-based MRC tasks involve extracting a span from a paragraph (Rajpurkar et al., 2016) or multiple paragraphs (Joshi et al., 2017; Kwiatkowski et al., 2019). Recently, there have been explorations on formulating NLP tasks as a question answering problem. McCann et al. (2018) propose natural language decathlon challenge (decaNLP), which consists of ten tasks"
2020.emnlp-main.49,M92-1001,0,0.386708,"ity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zeroshot learning setting).1 1 Event extraction is a long-studied and challenging task in Information Extraction (IE) (Sundheim, 1992; Grishman and Sundheim, 1996; Riloff, 1996). Its goal is to extract structured information — “what is happening” and the persons/objects that are involved — from unstructured text. The task is illustrated via an example in Figure 1, which depicts an ownership transfer event (the event type), triggered by the word “sale"" (the event trigger) and accompanied by its extracted arguments — text spans denoting entities that fill a set of (semantic) roles associated with the event type (e.g., BUYER, SELLER and ARTIFACT for ownership transfer events). Recent successful approaches to event extraction h"
2020.emnlp-main.49,N16-1034,0,0.539355,"nformation — “what is happening” and the persons/objects that are involved — from unstructured text. The task is illustrated via an example in Figure 1, which depicts an ownership transfer event (the event type), triggered by the word “sale"" (the event trigger) and accompanied by its extracted arguments — text spans denoting entities that fill a set of (semantic) roles associated with the event type (e.g., BUYER, SELLER and ARTIFACT for ownership transfer events). Recent successful approaches to event extraction have benefited from dense features extracted by neural models (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018) as well as contextualized lexical representations from pretrained language models (Zhang et al., 2019b; Wadden et al., 2019). Our code and question templates for the work are open sourced at https://github.com/xinyadu/eeqa for reproduction purpose. TransactionTransfer-Ownership Trigger “sale” “French company”, Buyer “parent company” Args. Seller “USA Interactive” Artifact “operations” Place Event type Figure 1: Event extraction example from the ACE 2005 corpus (Doddington et al., 2004). Introduction 1 Extracted Event: The approaches, however, exhibit two key weaknesses. Fir"
2020.emnlp-main.49,N18-1202,0,0.0496375,"Missing"
2020.emnlp-main.49,J11-1005,0,\N,Missing
2020.emnlp-main.49,doddington-etal-2004-automatic,0,\N,Missing
2020.emnlp-main.49,N19-1423,0,\N,Missing
2020.emnlp-main.49,P15-2060,0,\N,Missing
2020.emnlp-main.49,P15-1017,0,\N,Missing
2020.emnlp-main.649,N15-4002,0,0.0173719,"ment Understanding Conference (DUC) and Text Analysis Conference (TAC) evaluations, implemented fine-grained verification of data quality.2 In part due to the emergence of data-hungry modelling techniques, the demands for larger datasets often render quality assurance procedures of this standard to be impractical and infeasible. Nonetheless, several recent natural language understanding datasets (Bowman et al., 2015; Rajpurkar et al., 2016; Suhr et al., 2017) institute explicit qualitycontrol procedures in crowd-sourcing dataset construction (Zaidan and Callison-Burch, 2011; Yan et al., 2014; Callison-Burch et al., 2015), such as using additional annotators to validate annotations (c.f. Geva et al., 2019). In the sibling subfield of machine translation, which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence-to-sequence natural language generation tasks, the annual WMT conference3 consistently furnishes high quality data. In summary, ensuring data quality is both crucial and challenging. And in comparison with other subareas of NLP, we argue that summarization has lagged behind in rigorously ensuring the quality of widely-used datasets"
2020.emnlp-main.649,P19-1102,0,0.0167132,"g approaches (Bengio et al., 2009). Alternatively, they can serve as additional metrics for the (possibly unsupervised) evaluation of summarization systems, potentially mitigating deficiencies in standard metrics, such as ROUGE, by directly penalizing redundancy and semantic incoherence. Limitations. In this work, we restrict ourselves to single-document single-reference English language summarization datasets. While the datasets we study constitute a considerable fraction of dataset usage in the summarization community, several multi-document summarization datasets have been introduced (e.g. Fabbri et al., 2019; Antognini and Faltings, 2020) and multi-reference summarization datasets have often been argued to be desirable due to under-constrained nature of the summarization task (Kryscinski et al., 2019) and the ideal evaluation paradigm for ROUGE (Lin, 2004). Beyond English, both large summarization datasets (Nguyen and Daum´e III, 2019; Varab and Schluter, 2020) and more general language resources/technologies (Joshi et al., 2020) are less available, which may heighten the need for data quality assurance. More broadly, the measures that we introduce are automated, and therefore non-human, judgment"
2020.emnlp-main.649,P19-1264,0,0.0249779,"Missing"
2020.emnlp-main.649,2020.eval4nlp-1.5,0,0.014529,"ity and reliability. Even in the context of human evaluations, we advocate that automatic metrics can be useful in guiding the exploration of data and informing subsampling procedures that provide fine-grained insights. Quality Estimation. Our work bears resemblance both in name and structure to work on quality estimation. Quality estimation, often centered on natural language generation, is the task of measuring system-generated output quality (Paetzold and Specia, 2016; Yuan and Sharoff, 2020). It is closely related to work on unsupervised or reference-free evaluation (Napoles et al., 2016; Ethayarajh and Sadigh, 2020). Within the context of summarization, the special case of quality estimation regarding factual consistency/faithfulness has been of recent interest (Wang et al., 2020; Maynez et al., 2020; Durmus et al., 2020) since neural abstractive summarizers have been shown to hallucinate/misrepresent facts (See et al., 2017). In comparison to these settings, our metrics make no use of labelled data (even in training) and are entirely intrinsic/unsupervised. 8081 Summarization Practices. Several analyses and critiques exist for different aspects of the summarization pipeline. From a modelling perspective"
2020.emnlp-main.649,D18-1443,0,0.0511554,"Missing"
2020.emnlp-main.649,D18-1208,0,0.0610333,"Missing"
2020.emnlp-main.649,P09-1032,0,0.0872924,"Missing"
2020.emnlp-main.649,W04-1013,0,0.223477,"tify topic similarity by comparing the inferred topic distributions θDi |M , θSi |M for a given summary and document: TS (Di , Si ) = 1 − JS(θDi |M , θSi |M ) (3) where JS is the Jensen-Shannon distance. We set k = 20 and T = D. Abstractivity. Grusky et al. (2018) introduced fragments F(Di , Si ), which are greedily-matched spans shared between Di and Si . We quantify abstractivity as a normalized function of the aggregate fragment length; our definition generalizes the definition of Grusky et al. (2018). P We set pp = 1. |f | ABSp (Di , Si ) = 1 − f ∈F (Di ,Si ) |Si |p (4) Redundancy. ROUGE (Lin, 2004) implicitly penalizes redundancy but underestimates its detrimental impacts (Chaganty et al., 2018). However, we find that ROUGE is effective for identifying redundancy given the definitional focus on overlapping spans. We quantify redundancy as the average ROUGE - L 6 Different names and interpretations have been given for these properties in the literature. We revisit this in Appendix A in discussing alternate metrics. F -score for all pairs of distinct sentences in the summary. RED (Si ) = mean ROUGE (x, y) (5) (x,y)∈Si ×Si ,x6=y Semantic Coherence. We evaluate the semantic coherence of mul"
2020.emnlp-main.649,P08-2051,0,0.120947,"Missing"
2020.emnlp-main.649,2020.acl-main.445,0,0.0564005,"Missing"
2020.emnlp-main.649,J93-2004,0,0.0820732,"is impossible to be certain that progress is being made and that successive iterations of models truly 1 Concurrent to our work, Kathy McKeown, during her keynote address at ACL 2020, also called for the renewed study of datasets being used in the summarization community (McKeown, 2020). 8075 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8075–8096, c November 16–20, 2020. 2020 Association for Computational Linguistics make progress on the underlying task or linguistic phenomena of interest. Within NLP, iconic datasets such as the Penn Treebank (Marcus et al., 1993) have sustained subareas such as language modelling, part-of-speech tagging, and syntactic parsing for years due to the painstaking annotation efforts put into making these high-fidelity resources. And in the context of summarization, initial datasets, such as those produced during the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) evaluations, implemented fine-grained verification of data quality.2 In part due to the emergence of data-hungry modelling techniques, the demands for larger datasets often render quality assurance procedures of this standard to be imprac"
2020.emnlp-main.649,2020.acl-main.173,0,0.0348676,"fine-grained insights. Quality Estimation. Our work bears resemblance both in name and structure to work on quality estimation. Quality estimation, often centered on natural language generation, is the task of measuring system-generated output quality (Paetzold and Specia, 2016; Yuan and Sharoff, 2020). It is closely related to work on unsupervised or reference-free evaluation (Napoles et al., 2016; Ethayarajh and Sadigh, 2020). Within the context of summarization, the special case of quality estimation regarding factual consistency/faithfulness has been of recent interest (Wang et al., 2020; Maynez et al., 2020; Durmus et al., 2020) since neural abstractive summarizers have been shown to hallucinate/misrepresent facts (See et al., 2017). In comparison to these settings, our metrics make no use of labelled data (even in training) and are entirely intrinsic/unsupervised. 8081 Summarization Practices. Several analyses and critiques exist for different aspects of the summarization pipeline. From a modelling perspective, Zhang et al. (2018) assess whether abstractive systems are truly abstractive, Kedzie et al. (2018) evaluate content selection policies in a variety of methods, and Mao et al. (2020) asse"
2020.emnlp-main.649,P19-1101,0,0.025157,"existing NLP methods. We are guided by pioneering work (Luhn, 1958; Edmundson, 1969; Mani, 1999) that defined core properties of summarization systems and influential sub5 Research in algorithms provides a natural parallel: many computationally hard optimization problems remain intractable when relaxed to their decision problem version. For example, the travelling salesman problem of finding the least costly Hamiltonian cycle remains NP-hard even if we just ask “Does there exist a Hamiltonian cycle of cost ≤ L?” 8076 sequent work (Radev et al., 2002; Nenkova, 2006; Nenkova and McKeown, 2012; Peyrard, 2019a) that refined and extended these properties. From this literature, we specifically study compression, topic similarity, abstractivity, redundancy, and semantic coherence as these properties are of recurring and sustained interest.6 For each abstract property, numerous concrete methods can be proposed to quantify it. In Appendix A, we describe alternatives we considered and detail how we decided which methods performed best. We restrict discussion to the bestperforming approaches in the main paper. Notation. Our metrics will assume indexed sets D, S such that summary Si ∈ S summarizes documen"
2020.emnlp-main.649,P19-1502,0,0.0282039,"existing NLP methods. We are guided by pioneering work (Luhn, 1958; Edmundson, 1969; Mani, 1999) that defined core properties of summarization systems and influential sub5 Research in algorithms provides a natural parallel: many computationally hard optimization problems remain intractable when relaxed to their decision problem version. For example, the travelling salesman problem of finding the least costly Hamiltonian cycle remains NP-hard even if we just ask “Does there exist a Hamiltonian cycle of cost ≤ L?” 8076 sequent work (Radev et al., 2002; Nenkova, 2006; Nenkova and McKeown, 2012; Peyrard, 2019a) that refined and extended these properties. From this literature, we specifically study compression, topic similarity, abstractivity, redundancy, and semantic coherence as these properties are of recurring and sustained interest.6 For each abstract property, numerous concrete methods can be proposed to quantify it. In Appendix A, we describe alternatives we considered and detail how we decided which methods performed best. We restrict discussion to the bestperforming approaches in the main paper. Notation. Our metrics will assume indexed sets D, S such that summary Si ∈ S summarizes documen"
2020.emnlp-main.649,S18-2023,0,0.062575,"Missing"
2020.emnlp-main.649,2020.acl-main.248,0,0.0387293,"Missing"
2020.emnlp-main.649,J02-4001,0,0.227917,"cument-summary pair) can be reliably estimated by re-purposing existing NLP methods. We are guided by pioneering work (Luhn, 1958; Edmundson, 1969; Mani, 1999) that defined core properties of summarization systems and influential sub5 Research in algorithms provides a natural parallel: many computationally hard optimization problems remain intractable when relaxed to their decision problem version. For example, the travelling salesman problem of finding the least costly Hamiltonian cycle remains NP-hard even if we just ask “Does there exist a Hamiltonian cycle of cost ≤ L?” 8076 sequent work (Radev et al., 2002; Nenkova, 2006; Nenkova and McKeown, 2012; Peyrard, 2019a) that refined and extended these properties. From this literature, we specifically study compression, topic similarity, abstractivity, redundancy, and semantic coherence as these properties are of recurring and sustained interest.6 For each abstract property, numerous concrete methods can be proposed to quantify it. In Appendix A, we describe alternatives we considered and detail how we decided which methods performed best. We restrict discussion to the bestperforming approaches in the main paper. Notation. Our metrics will assume inde"
2020.emnlp-main.649,D16-1264,0,0.046217,"to the painstaking annotation efforts put into making these high-fidelity resources. And in the context of summarization, initial datasets, such as those produced during the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) evaluations, implemented fine-grained verification of data quality.2 In part due to the emergence of data-hungry modelling techniques, the demands for larger datasets often render quality assurance procedures of this standard to be impractical and infeasible. Nonetheless, several recent natural language understanding datasets (Bowman et al., 2015; Rajpurkar et al., 2016; Suhr et al., 2017) institute explicit qualitycontrol procedures in crowd-sourcing dataset construction (Zaidan and Callison-Burch, 2011; Yan et al., 2014; Callison-Burch et al., 2015), such as using additional annotators to validate annotations (c.f. Geva et al., 2019). In the sibling subfield of machine translation, which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence-to-sequence natural language generation tasks, the annual WMT conference3 consistently furnishes high quality data. In summary, ensuring data qualit"
2020.emnlp-main.649,D15-1044,0,0.444093,"rmann et al., 2015; Nallapati et al., 2016) is a dataset composed of CNN and Daily Mail news articles with summaries that are a concatenated list of highlight bullet points. NYT (Sandhaus, 2008) is a dataset of curated New York Times articles paired with abstracts written by library scientists. NWS (Grusky et al., 2018) is the Newsroom dataset of news articles drawn from 38 top English publishers paired with multi-sentence summaries written by the original authors and editors. GW (Graff and Cieri, 2003) is the Gigaword headline generation dataset that some refer to as a summarization dataset (Rush et al., 2015; Chopra et al., 2016). Examples in the dataset are drawn from seven news sources and are the article prefix paired with its headline. XSum (Narayan et al., 2018) is an extreme summarization dataset where BBC articles are paired with single-sentence summaries written generally by the author of the article that tries to motivate the BBC audience to read the article by answering “What is the article about?”. PeerRead (Kang et al., 2018) is a dataset of paper drafts from top-tier computer science venues as well as arXiv.8 Consistent with its use in the summarization community, we consider the ful"
2020.emnlp-main.649,E17-2069,0,0.0666787,"Missing"
2020.emnlp-main.649,P17-1099,0,0.635649,"t paper, TL;DR may be a better choice than XSum. In particular, Narayan et al. (2018) introduce XSum as a large dataset that legitimately requires abstraction. While XSum is more abstractive than other News datasets (barring GW) and is relatively large, TL;DR displays greater abstractivity, similar length summaries, and is 15 times larger. That said, Narayan et al. (2018) explore topic-oriented strategies in their work and such methods may be better suited to XSum given the TS scores. CNN-DM and NYT are suboptimal for studying abstractive/extractive systems respectively. Several recent works (See et al., 2017; Paulus et al., 2018; Li et al., 2018) have used CNN-DM to build and evaluate abstractive systems. Conversely, NYT has been used to build extractive systems (Hong and Nenkova, 2014; Li et al., 2016). Given our findings, we find both of these trends to be inconsistent with dataset properties and suboptimal given other preferable datasets for these purposes: CNN-DM is one of the least abstractive datasets and there are larger and more extractive alternatives to NYT such as NWS. Especially in the case of CNN-DM, we note that training learning-based systems (e.g. neural methods) using data with l"
2020.emnlp-main.649,2020.acl-main.704,0,0.0491264,"Missing"
2020.emnlp-main.649,P17-2034,0,0.0243225,"tation efforts put into making these high-fidelity resources. And in the context of summarization, initial datasets, such as those produced during the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) evaluations, implemented fine-grained verification of data quality.2 In part due to the emergence of data-hungry modelling techniques, the demands for larger datasets often render quality assurance procedures of this standard to be impractical and infeasible. Nonetheless, several recent natural language understanding datasets (Bowman et al., 2015; Rajpurkar et al., 2016; Suhr et al., 2017) institute explicit qualitycontrol procedures in crowd-sourcing dataset construction (Zaidan and Callison-Burch, 2011; Yan et al., 2014; Callison-Burch et al., 2015), such as using additional annotators to validate annotations (c.f. Geva et al., 2019). In the sibling subfield of machine translation, which often shares similar modelling challenges and evaluation regimes as summarization due to the shared nature of being sequence-to-sequence natural language generation tasks, the annual WMT conference3 consistently furnishes high quality data. In summary, ensuring data quality is both crucial an"
2020.emnlp-main.649,N19-1241,0,0.0564814,"Missing"
2020.emnlp-main.649,2020.lrec-1.229,0,0.0281317,"wsroom corpora, respectively). Automated and human evaluation provide complementary benefits with respect to their scalability and reliability. Even in the context of human evaluations, we advocate that automatic metrics can be useful in guiding the exploration of data and informing subsampling procedures that provide fine-grained insights. Quality Estimation. Our work bears resemblance both in name and structure to work on quality estimation. Quality estimation, often centered on natural language generation, is the task of measuring system-generated output quality (Paetzold and Specia, 2016; Yuan and Sharoff, 2020). It is closely related to work on unsupervised or reference-free evaluation (Napoles et al., 2016; Ethayarajh and Sadigh, 2020). Within the context of summarization, the special case of quality estimation regarding factual consistency/faithfulness has been of recent interest (Wang et al., 2020; Maynez et al., 2020; Durmus et al., 2020) since neural abstractive summarizers have been shown to hallucinate/misrepresent facts (See et al., 2017). In comparison to these settings, our metrics make no use of labelled data (even in training) and are entirely intrinsic/unsupervised. 8081 Summarization P"
2020.emnlp-main.649,P11-1122,0,0.0528672,"Missing"
2020.emnlp-main.716,D17-1141,0,0.0265145,"xplored in previous papers (Durmus and Cardie, 2019a, 2018; Longpre et al., 2019), demonstrating the importance of characteristics and beliefs of the audience. Furthermore, Potash and Rumshisky (2017) proposed a recurrent neural network architecture with attention and annotated audience favorability to predict the winner of the debate. Villata et al. (2018) and Benlamine et al. (2017) studied the correlation of the engagement index in brain hemispheres with the persuasion strategies. Argument structures have been used to understand argumentative strategies in dialogues and news editorials (Al Khatib et al., 2017; Wang et al., 2019). A few studies have explored the impact of argument structures in predicting persuasion on CMV dataset based on statistical analysis of proposition types (Hidey et al., 2017; Egawa et al., 2019; Morio et al., 2019). In this paper, we particularly study persuasion in online debates. We propose novel argument structure features based on n-grams of the supporting relations in argument structure graph of the debate text and experiment with these using both linear and neural models. 3 Dataset We experiment with DDO dataset (Durmus and Cardie, 2019a) which includes 77,655 debate"
2020.emnlp-main.716,P13-1025,0,0.0844981,"Missing"
2020.emnlp-main.716,N19-1423,0,0.0187293,"ments are used in the text of the debaters. We further classify the convergent arguments into two categories – where two propositions support one proposition (regular convergent argument) and more than two propositions support one proposition (multi convergent argument). Similarly, we classify divergent argument into regular divergent argument and multi divergent argument. 4.2 Model Architecture We employ two separate bidirectional LSTM (Hochreiter and Schmidhuber, 1997) models to encode the argument structure features and features encoding the debate text derived from pre-trained BERT model (Devlin et al., 2019) as shown in Figure 2. LSTM modeling the debate text takes BERT representation (Devlin et al., 2019) while LSTM encoding argument structure features takes three set of argument structure features of an utterance in a round at each time step. Two fully connected layers with softmax are used to predict the output probabilities over both of these LSTM models separately. The model learns weights during training to combine these probabilities. 5 Experiments and Analysis We compare our model with the baseline proposed by Durmus and Cardie (2019a) employing linguistic features and features encoding a"
2020.emnlp-main.716,2007.sigdial-1.5,0,0.136038,"Missing"
2020.emnlp-main.716,C16-1158,0,0.163953,"ion The increase in availability of online argumentation platforms has provided opportunity for researchers to develop computational methods at a larger scale studying the important factors of persuasiveness such as the language use (Hidey et al., 2017; Tan et al., 2016; Zhang et al., 2016), characteristics of audience (i.e. prior beliefs, demographics) (Durmus and Cardie, 2019a, 2018) and social interactions (Durmus and Cardie, 2019b). Prior work has showed incorporating argument structure features is important in assessing the quality of monological persuasive essays (Klebanov et al., 2016; Wachsmuth et al., 2016). Hidey et al. (2017) and Egawa et al. (2019) further collected annotations for semantic types of argument components and studied the relationship between the Claire Cardie Cornell University cardie@cs.cornell.edu semantic types and persuasiveness of the arguments from online argumentative platform ChangeMyView (CMV) (Tan et al., 2016). CMV consists of discussion trees where the users interact with the original poster to change their opinion on a given topic. Although the discussion trees are of a high quality since they are monitored by moderators (Tan et al., 2016), they are not as structure"
2020.emnlp-main.716,Q17-1016,0,0.0179794,"studied the relationship between ethos, a specific kind of argument unit, and the dynamics of governments from the UK parliamentary debates. The role of argument structure in persuasion on online debates is much less explored, which is the main focus of this paper. Analysis of Persuasion Prior studies on persuasion has mainly focused on understanding the role of linguistic factors (Petty et al., 1983; Chaiken, 1987; Dillard and Pfau, 2002; Gold et al., 2015). Besides, the interaction between debaters has shown to be an important cue in persuasion studies (Zhang et al., 2016; Tan et al., 2016; Wang et al., 2017). Luu et al. (2019) further found that the debater’s skill estimated from debate text history is also predictive of convincing the audience. User factors are explored in previous papers (Durmus and Cardie, 2019a, 2018; Longpre et al., 2019), demonstrating the importance of characteristics and beliefs of the audience. Furthermore, Potash and Rumshisky (2017) proposed a recurrent neural network architecture with attention and annotated audience favorability to predict the winner of the debate. Villata et al. (2018) and Benlamine et al. (2017) studied the correlation of the engagement index in br"
2020.emnlp-main.716,P19-1566,0,0.0547638,"apers (Durmus and Cardie, 2019a, 2018; Longpre et al., 2019), demonstrating the importance of characteristics and beliefs of the audience. Furthermore, Potash and Rumshisky (2017) proposed a recurrent neural network architecture with attention and annotated audience favorability to predict the winner of the debate. Villata et al. (2018) and Benlamine et al. (2017) studied the correlation of the engagement index in brain hemispheres with the persuasion strategies. Argument structures have been used to understand argumentative strategies in dialogues and news editorials (Al Khatib et al., 2017; Wang et al., 2019). A few studies have explored the impact of argument structures in predicting persuasion on CMV dataset based on statistical analysis of proposition types (Hidey et al., 2017; Egawa et al., 2019; Morio et al., 2019). In this paper, we particularly study persuasion in online debates. We propose novel argument structure features based on n-grams of the supporting relations in argument structure graph of the debate text and experiment with these using both linear and neural models. 3 Dataset We experiment with DDO dataset (Durmus and Cardie, 2019a) which includes 77,655 debates covering 23 differ"
2020.emnlp-main.716,H05-1044,0,0.161077,"Missing"
2020.emnlp-main.716,N16-1017,0,0.122214,"form and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the better predictive performance in assessing the persuasiveness of the arguments in online debates. 1 Introduction The increase in availability of online argumentation platforms has provided opportunity for researchers to develop computational methods at a larger scale studying the important factors of persuasiveness such as the language use (Hidey et al., 2017; Tan et al., 2016; Zhang et al., 2016), characteristics of audience (i.e. prior beliefs, demographics) (Durmus and Cardie, 2019a, 2018) and social interactions (Durmus and Cardie, 2019b). Prior work has showed incorporating argument structure features is important in assessing the quality of monological persuasive essays (Klebanov et al., 2016; Wachsmuth et al., 2016). Hidey et al. (2017) and Egawa et al. (2019) further collected annotations for semantic types of argument components and studied the relationship between the Claire Cardie Cornell University cardie@cs.cornell.edu semantic types and persuasiveness of the arguments fro"
2020.findings-emnlp.302,2020.acl-main.357,0,0.0214891,"temporal unit (e.g., second, minute, hour, etc.). To transform the sentences into the input format of our models. We insert duration pattern (“, lasting [MASK] [MASK], ”) after event word and use the new sentence as the input sequence. For example, one sentence in TimeBank is “Philip Morris Cos, adopted a defense measure ...”. Our method will convert it to “Philip Morris Cos, adopted, lasting [MASK] [MASK], a defense measure ...”. Our strategy of directly adding duration pattern is possible to help pre-trained model to utilize learned intrinsic textual representation for duration prediction (Tamborrino et al., 2020). McTACO is a multi-choice question answering dataset. McTACO-duration3 is a subset of Mc2 We use Gusev et al. (2011)’s split and obtain 1663/469/147 events in Train/Test/TestWSJ set respectively. 3 In practice we collect context-question-answer triples that questions are about event duration and answers can be transformed to a duration value. We get 1060/2827 triples for dev/test set respectively (out of 1112/3032). 3371 Coarsed-Grained (Test) &lt;day F1 &gt;day F1 Acc. Model Coarsed-Grained (TestWSJ) &lt;day F1 &gt;day F1 Acc. Fine-Grained Acc. (Test) Acc. (TestWSJ) Supervised Setting Majority class Max"
2020.findings-emnlp.302,N18-2026,0,0.296014,", 2015). It is challenging to make accurate prediction mainly due to two reasons: (1) duration is not only associated with event word but also the context. For example, “watch a movie” takes around 2 hours, while “watch a bird fly” only takes about 10 seconds; (2) the compositional nature of events makes it difficult to train a learning-based system only based on hand annotated data (since it’s hard to cover all the possible events). Thus, external knowledge and commonsense are needed to make further progress on the task. However, most current approaches (Pan et al., 2011; Gusev et al., 2011; Vempala et al., 2018) focus on developing features and cannot utilize external textual knowledge. The only exception is the web count based method proposed by Gusev et al. (2011), which queries search engine with event word (e.g., “watch”) and temporal units, and make predictions based on hitting times. However, this method achieves better performance when query only with the event word in the sentence, which means it does not enable contextualized understanding. To benefit from the generalizability of learningbased methods and utilizing external temporal knowledge, we introduce a framework, which includes (1) a p"
2020.findings-emnlp.302,P12-2044,0,0.0300227,", 2016; Vempala et al., 2018). In particular, aspectual (Vendler, 1957; Smith, 2013) features have been proved to be useful. Concurrent to our work, Zhou et al. (2020) also utilize unlabeled data. Different from our work, they focus on temporal commonsense acquisition in a more general setting (for frequency, typical time, duration, etc.) and the models predict the discrete temporal unit, while we propose two models (classification and regression-based). In addition, they focus on providing better representation instead of directly generating duration prediction. For the unsupervised setting, Williams and Katz (2012); Elazar et al. (2019) use rule-based method on web data and generate collections of mapping from verb/event pattern to numeric duration value. Kozareva and Hovy (2011); Gusev et al. (2011) develop queries for search engines and utilize the returned snippets / hitting times to make prediction. 5 Conclusion We propose a framework for leveraging free-form textual knowledge into neural models for duration prediction. Our best model (E- PRED) achieves state-of-the-art performance in various tasks. In addition, our model trained only with externallyobtained weakly supervised news data outperforms s"
2020.findings-emnlp.302,D19-1332,0,0.290336,"by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit where the duration value falls in (R- PRED); and the other predicts the exact duration value (EPRED ). Our best model – E- PRED , substantially outperforms previous work, and captures duration information more accurately than RPRED . We also demonstrate our models are capable of duration prediction in the unsupervised setting, outperforming the baselines. 1 Introduction Understanding duration of event expressed in text is a crucial task in NLP (Pustejovsky and Verhagen, 2009; Zhou et al., 2019). It facilitates downstream tasks such as story timeline construction (Ning et al., 2018; Leeuwenberg and Moens, 2019) and temporal question answering (Llorens et al., 2015). It is challenging to make accurate prediction mainly due to two reasons: (1) duration is not only associated with event word but also the context. For example, “watch a movie” takes around 2 hours, while “watch a bird fly” only takes about 10 seconds; (2) the compositional nature of events makes it difficult to train a learning-based system only based on hand annotated data (since it’s hard to cover all the possible event"
2020.findings-emnlp.302,2020.acl-main.678,0,0.301328,"s split and obtain 1663/469/147 events in Train/Test/TestWSJ set respectively. 3 In practice we collect context-question-answer triples that questions are about event duration and answers can be transformed to a duration value. We get 1060/2827 triples for dev/test set respectively (out of 1112/3032). 3371 Coarsed-Grained (Test) &lt;day F1 &gt;day F1 Acc. Model Coarsed-Grained (TestWSJ) &lt;day F1 &gt;day F1 Acc. Fine-Grained Acc. (Test) Acc. (TestWSJ) Supervised Setting Majority class Maximum Entropy (Pan et al., 2011)† Maximum Entropy++ (Gusev et al., 2011)† LSTM ensemble (Vempala et al., 2018) TACOLM (Zhou et al., 2020) 64.29 80.58 76.90 82.69 88.88 62.47 73.30 73.00 76.69 85.86 73.20 76.01 76.99 87.78 88.14 62.58 73.50 74.80 83.21 84.12 59.28 62.20 62.40 - 52.38 61.90 66.00 - R-PRED w/o pre-training E-PRED w/o pre-training 82.08 80.94 80.63 78.73 87.72 86.19 89.46 88.16 85.43 84.01 86.35 84.79 70.15 73.46 70.67 73.50 81.12 79.93 85.39 86.21 76.87 77.32 80.50 81.86 82.09 80.38 82.52 80.34 76.19 78.46 78.46 77.02 Unsupervised Setting Majority Web count, yesterday (Gusev et al., 2011)† Web count, bucket (Gusev et al., 2011)† R-PRED E-PRED - 76.90 - 62.47 70.70 72.40 - 76.99 - 62.58 74.80 73.50 59.28 66.50 52.3"
2020.findings-emnlp.360,D18-1045,0,0.0227931,"e we first fine-tune the mBART model for document level machine translation from the source language into English, and then we further fine-tune the model for cross-lingual summarization (DC+Synth+MT). Similar to above, since we only have a limited amount of parallel document pairs in our dataset, we translate English documents into the source language to create additional parallel data. This method of back-translation to create additional parallel data has been shown to be effective in improving the performance of neural machine translation systems (Sennrich et al., 2016; Hoang et al., 2018; Edunov et al., 2018).8 5 Results and Analysis Table 4 shows ROUGE scores (Lin, 2004) for the baselines and proposed cross-lingual approaches. We observe that the lead baseline performs poorly for this task, unlike in the news domain where it’s shown to be a strong baseline (Brandow et al., 8 While back-translation typically uses an intermediate training checkpoint to create synthetic data, we instead use AWS translate. 1995). When comparing the performance of Trans-Sum vs. Sum-Trans, we find that performance depends on the amount of summarization data available in the source language. Similar to previous work (Ou"
2020.findings-emnlp.360,W13-3102,0,0.0528277,"Missing"
2020.findings-emnlp.360,W13-3103,0,0.508918,"anslation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference. 1 Introduction Although there has been a tremendous amount of progress in abstractive summarization in recent years, most research has focused on monolingual summarization because of the lack of high quality multilingual resources (Lewis et al., 2019a; Song et al., 2020). While there have been a few studies to address the lack of resources for cross-lingual summarization (Giannakopoulos, 2013; Li et al., 2013; Elhadad et al., 2013; Nguyen and Daum´e III, 2019), the datasets employed are very limited in size. Scarcity in the availability of data for crosslingual abstractive summarization can largely be attributed to the difficulty of collecting high-quality, ∗ Equal contribution. https://www.wikihow.com 2 The data was collected in accordance with the terms and conditions listed on the website. 1 large-scale datasets via crowd-sourcing. It is a costly endeavor, since it requires humans to read, comprehend, condense, and paraphrase entire articles. Moreover, subjectivity in content s"
2020.findings-emnlp.360,W04-1013,0,0.196955,"n from the source language into English, and then we further fine-tune the model for cross-lingual summarization (DC+Synth+MT). Similar to above, since we only have a limited amount of parallel document pairs in our dataset, we translate English documents into the source language to create additional parallel data. This method of back-translation to create additional parallel data has been shown to be effective in improving the performance of neural machine translation systems (Sennrich et al., 2016; Hoang et al., 2018; Edunov et al., 2018).8 5 Results and Analysis Table 4 shows ROUGE scores (Lin, 2004) for the baselines and proposed cross-lingual approaches. We observe that the lead baseline performs poorly for this task, unlike in the news domain where it’s shown to be a strong baseline (Brandow et al., 8 While back-translation typically uses an intermediate training checkpoint to create synthetic data, we instead use AWS translate. 1995). When comparing the performance of Trans-Sum vs. Sum-Trans, we find that performance depends on the amount of summarization data available in the source language. Similar to previous work (Ouyang et al., 2019), we find that Tran-Sum works significantly be"
2020.findings-emnlp.360,D19-1387,0,0.112476,"in/dev/test splits. When splitting the English data, we ensure that all articles from the same topic as test articles in any of the four non-English languages, are included in the test set. This leaves us with ∼ 69K English articles that we randomly split into train and dev set (90/10 split). See Appendix A.2 for more information. We use large, pre-trained language models as a starting point for our experiments, given their success on a variety of downstream Natural Language Processing tasks (Devlin et al., 2019), including state of the art results for text summarization (Lewis et al., 2019b; Liu and Lapata, 2019). In particular, we use mBART (Liu et al., 2020), which is a multi-lingual language model that has been trained on large, monolingual corpora in 25 languages. The model uses a shared sub-word vocabulary, encoder, and decoder across all 25 languages, and is trained as a denoising auto-encoder during the pre-training step. Liu et al. (2020) showed that this pre-training method provides a good initialization for downstream machine translation tasks, particularly in lower resources settings, making this an ideal starting point for our cross-lingual summarization experiments. We also ran initial ex"
2020.findings-emnlp.360,2020.tacl-1.47,0,0.123669,"we ensure that all articles from the same topic as test articles in any of the four non-English languages, are included in the test set. This leaves us with ∼ 69K English articles that we randomly split into train and dev set (90/10 split). See Appendix A.2 for more information. We use large, pre-trained language models as a starting point for our experiments, given their success on a variety of downstream Natural Language Processing tasks (Devlin et al., 2019), including state of the art results for text summarization (Lewis et al., 2019b; Liu and Lapata, 2019). In particular, we use mBART (Liu et al., 2020), which is a multi-lingual language model that has been trained on large, monolingual corpora in 25 languages. The model uses a shared sub-word vocabulary, encoder, and decoder across all 25 languages, and is trained as a denoising auto-encoder during the pre-training step. Liu et al. (2020) showed that this pre-training method provides a good initialization for downstream machine translation tasks, particularly in lower resources settings, making this an ideal starting point for our cross-lingual summarization experiments. We also ran initial experiments with non-pretrained transformer models"
2020.findings-emnlp.360,K16-1028,0,0.0631803,"Missing"
2020.findings-emnlp.360,D18-1206,0,0.0239227,"em generated summaries are fluent, however DC+Synth+MT has better overlap with the content in the reference summary.10 9 The reference was only shown when evaluating for content overlap, and not for fluency evaluation. 4040 10 More examples are provided in Appendix A.4. Model Trans-Sum Trans-Sum-R DC+Synth+MT Fluency Content 2.61 2.62 2.67 2.07 2.09 2.19 Table 6: Human evaluation scores on a scale of 1-3. 6 Related Work Abstractive Summarization. The majority of research in abstractive summarization has focused on monolingual summarization in English (Gehrmann et al., 2018; Song et al., 2020; Narayan et al., 2018). Rush et al. (2015) proposes the first neural abstractive summarization model using an attentionbased convolutional neural network encoder and a feed-forward decoder. Chopra et al. (2016) shows improvements over this model using a recurrent neural network for the decoder. Nallapati et al. (2016) shows further improvements by incorporating embeddings for linguistic features such as part-of-speech tags and named-entity tags into their model, as well as a pointer network (Vinyals et al., 2015) to enable copying words from the source article. See et al. (2017) extends this model by further incorp"
2020.findings-emnlp.360,D19-5411,0,0.218419,"Missing"
2020.findings-emnlp.360,orasan-chiorean-2008-evaluation,0,0.11722,"Missing"
2020.findings-emnlp.360,N19-4009,0,0.0140513,"results were significantly worse than those with the pre-trained models. We fine-tune mBART for both monolingual and cross-lingual summarization as a standard sequence-to-sequence model, where the input document is represented as a sequence of tokens (subword units), with a special separator token between each sentence, and a language indicator token at the end of the document. The output summary is represented in a similar manner, with a language indicator token at the beginning of the sequence, to prime the decoder for generation in the target language, as shown in Figure 3. We use Fairseq (Ott et al., 2019) for all our experiments, and we follow the hyper-parameter settings that were used by Lewis et al. (2019b) to fine-tune BART for monolingual summarization in English. See Appendix A.1 for more details. 4.1 Baselines We evaluate the following baseline approaches for cross-lingual summarization on our data: leadn : copies first n sentences from the corresponding parallel English source articles. We report results for n = 3 since it performs the best. Summarize-then-translate (Sum-Trans): We 4037 Figure 3: An example showing the fine-tuning procedure for cross-lingual summarization from Spanish"
2020.findings-emnlp.360,N19-1204,0,0.545375,"glish. There are in total 141,457 English articlesummary pairs in our dataset. tude larger than Global Voices, which is the largest dataset to date for cross-lingual evaluation. The Data Statement (Bender and Friedman, 2018) for our dataset can be found in Appendix A.3. Train Validation Spanish 81,514 Russian 38,107 Vietnamese 9,473 Turkish 3,241 Test 9,057 22,643 4,234 10,586 1,052 2,632 360 901 Table 3: Number of examples in Train/Validation/Test splits per language. 4 Cross-lingual Experiments Following the prior work in cross-lingual abstractive summarization (Nguyen and Daum´e III, 2019; Ouyang et al., 2019), we aim to generate English summaries from non-English articles, as an initial study. We experiment with five languages (i.e. English, Spanish, Russian, Turkish, and Vietnamese) covering three language families (i.e. IndoEuropean, Ural-Altaic and Austroasiatic). We split the data for each of the four non-English languages into train/dev/test splits. When splitting the English data, we ensure that all articles from the same topic as test articles in any of the four non-English languages, are included in the test set. This leaves us with ∼ 69K English articles that we randomly split into train"
2020.findings-emnlp.360,D15-1044,0,0.0375053,"are fluent, however DC+Synth+MT has better overlap with the content in the reference summary.10 9 The reference was only shown when evaluating for content overlap, and not for fluency evaluation. 4040 10 More examples are provided in Appendix A.4. Model Trans-Sum Trans-Sum-R DC+Synth+MT Fluency Content 2.61 2.62 2.67 2.07 2.09 2.19 Table 6: Human evaluation scores on a scale of 1-3. 6 Related Work Abstractive Summarization. The majority of research in abstractive summarization has focused on monolingual summarization in English (Gehrmann et al., 2018; Song et al., 2020; Narayan et al., 2018). Rush et al. (2015) proposes the first neural abstractive summarization model using an attentionbased convolutional neural network encoder and a feed-forward decoder. Chopra et al. (2016) shows improvements over this model using a recurrent neural network for the decoder. Nallapati et al. (2016) shows further improvements by incorporating embeddings for linguistic features such as part-of-speech tags and named-entity tags into their model, as well as a pointer network (Vinyals et al., 2015) to enable copying words from the source article. See et al. (2017) extends this model by further incorporating a coverage p"
2020.findings-emnlp.360,N19-1380,0,0.0245473,"4.41/31.18† 36.48/14.29/30.96‡ DC+Synth+MT 40.60/16.89/34.06† 42.76/20.47/37.09‡ 37.09/14.81/31.67† 37.86/15.26/32.33† Table 4: Cross-lingual summarization results. The numbers correspond to ROUGE-1/ROUGE-2/ROUGE-L F1 scores respectively. † indicates where ROUGE-L F1 is significantly better than all baselines, and ‡ indicates where ROUGE-L F1 is significantly better than all baselines except Trans-Sum-R. We use Welch’s t-test, and use p < 0.01 to assess significance. data has been shown to be an effective strategy for cross-lingual transfer for text classification and sequence labeling tasks (Schuster et al., 2019). We note that while this method still relies on machine translation, the cost of translation is shifted to training time, and thus is a one-time cost. Since a cross-lingual summarization model needs to learn how to translate salient information from one language to another, we hypothesize that training the model for machine translation can improve performance of cross-lingual summarization. Therefore, we propose a two-step fine-tuning approach, where we first fine-tune the mBART model for document level machine translation from the source language into English, and then we further fine-tune t"
2020.findings-emnlp.360,P17-1099,0,0.0406669,"et al., 2018; Song et al., 2020; Narayan et al., 2018). Rush et al. (2015) proposes the first neural abstractive summarization model using an attentionbased convolutional neural network encoder and a feed-forward decoder. Chopra et al. (2016) shows improvements over this model using a recurrent neural network for the decoder. Nallapati et al. (2016) shows further improvements by incorporating embeddings for linguistic features such as part-of-speech tags and named-entity tags into their model, as well as a pointer network (Vinyals et al., 2015) to enable copying words from the source article. See et al. (2017) extends this model by further incorporating a coverage penalty to address the problem of repetitions in the generated summary. Chen and Bansal (2018) takes a two stage approach to abstractive summarization by learning an extractor to select salient sentences from the articles, and an abstractor to rewrite the sentences selected by the extractor. They further train the extractor and abstractor end-to-end with a policygradient method, using ROUGE-L F1 as the reward function. Recently, pre-trained language models have achieved the state of the art results in abstractive summarization (Lewis et a"
2020.findings-emnlp.360,P16-1009,0,0.0342526,"opose a two-step fine-tuning approach, where we first fine-tune the mBART model for document level machine translation from the source language into English, and then we further fine-tune the model for cross-lingual summarization (DC+Synth+MT). Similar to above, since we only have a limited amount of parallel document pairs in our dataset, we translate English documents into the source language to create additional parallel data. This method of back-translation to create additional parallel data has been shown to be effective in improving the performance of neural machine translation systems (Sennrich et al., 2016; Hoang et al., 2018; Edunov et al., 2018).8 5 Results and Analysis Table 4 shows ROUGE scores (Lin, 2004) for the baselines and proposed cross-lingual approaches. We observe that the lead baseline performs poorly for this task, unlike in the news domain where it’s shown to be a strong baseline (Brandow et al., 8 While back-translation typically uses an intermediate training checkpoint to create synthetic data, we instead use AWS translate. 1995). When comparing the performance of Trans-Sum vs. Sum-Trans, we find that performance depends on the amount of summarization data available in the sou"
2020.findings-emnlp.360,D15-1012,0,0.057299,"Missing"
2020.findings-emnlp.360,D18-1448,0,0.0143472,"m to account for potential translation noise. There is limited prior work in direct crosslingual summarization. Shen et al. (2018) propose zero-shot cross-lingual headline generation to generate Chinese headlines for English articles, via a teacher-student framework, using two teacher models. Duan et al. (2019) propose a similar approach for cross-lingual abstractive sentence summarization. We note that our approach is much simpler and also focuses on a different kind of summarization task. Zhu et al. (2019) use round-trip translation of large scale monolingual datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015) to generate synthetic training data for their models, and train a multi-task model to to learn both translation and cross-lingual summarization. We tried their approach on our data, using the code provided,11 but the results were worse than all baselines except lead.12 We suspect that this may be due to the amount of training data, as their synthetic dataset was much larger than ours (1.69M pairs for Zh-En). An extension of their approach would be to incorporate multi-task training for pre-trained mBART, which we leave for future work. Scarcity of crosslingual summarization"
2020.findings-emnlp.360,D19-1302,0,0.0469993,"o get noisy English articles. They then train on noisy article and clean summary pairs, which allows them to account for potential translation noise. There is limited prior work in direct crosslingual summarization. Shen et al. (2018) propose zero-shot cross-lingual headline generation to generate Chinese headlines for English articles, via a teacher-student framework, using two teacher models. Duan et al. (2019) propose a similar approach for cross-lingual abstractive sentence summarization. We note that our approach is much simpler and also focuses on a different kind of summarization task. Zhu et al. (2019) use round-trip translation of large scale monolingual datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015) to generate synthetic training data for their models, and train a multi-task model to to learn both translation and cross-lingual summarization. We tried their approach on our data, using the code provided,11 but the results were worse than all baselines except lead.12 We suspect that this may be due to the amount of training data, as their synthetic dataset was much larger than ours (1.69M pairs for Zh-En). An extension of their approach would be to incorporate multi-task"
2020.findings-emnlp.360,J11-3005,0,0.0236574,"odel to to learn both translation and cross-lingual summarization. We tried their approach on our data, using the code provided,11 but the results were worse than all baselines except lead.12 We suspect that this may be due to the amount of training data, as their synthetic dataset was much larger than ours (1.69M pairs for Zh-En). An extension of their approach would be to incorporate multi-task training for pre-trained mBART, which we leave for future work. Scarcity of crosslingual summarization data has limited prior work to a few languages, and mostly in the news domain (Wan et al., 2010; Wan, 2011; Yao et al., 2015; Zhang et al., 2016; Wan et al., 2019). While there is some existing work trying to address this (Nguyen and Daum´e III, 2019), the proposed dataset is still limited in size, and contains summaries only in English. We address this limitation by proposing a 11 https://github.com/ZNLP/NCLS-Corpora This model gets ROUGE-L F1 scores of 22.49, 23.38, 20.79, 19.45 for Spanish, Turkish, Russian and Vietnamese respectively. 4041 12 new benchmark dataset. 7 by sentence selection. Information Processing & Management, 31(5):675 – 685. Summarizing Text. Conclusion We present a benchmark"
2020.findings-emnlp.360,P10-1094,0,0.656157,"rce language. Similar to previous work (Ouyang et al., 2019), we find that Tran-Sum works significantly better when the amount of data in the source language is limited. However, as source language training data size increases, we see that the gap in performance decreases, as in the case of Spanish, which is similar in size to English, vs. Turkish, which is the lowest resource language for summarization in our dataset. This suggests that when the source language data is comparable in size or larger than the target language data, SumTrans approach may be worthwhile to consider, as suggested by Wan et al. (2010), since it is more cost effective (translating summaries instead of whole articles) and may avoid error propagation from translation systems. Amongst the baseline methods, Trans-Sum-R works the best. It consistently does better than Trans-Sum baseline, suggesting that round-trip translation to create noisy data can be an effective way to make the model more robust to translation errors at inference time. Since we have gold translations (Trans-Sum G) for each of the articles, we can measure the translation error in the Trans-Sum system. We see that on average, the round-trip translation method"
2020.fnp-1.25,P19-1285,0,0.0213716,"Missing"
2020.fnp-1.25,N19-1423,0,0.00693009,"s become unreasonably laborious. Automatic summarization methods could greatly simplify this task. The Financial Narrative Summarization Shared Task for 2020 (FNS2020) aims to study the application of automatic summarization methods to annual reports from UK firms listed on The London Stock Exchange (LSE) (El-Haj et al., 2020). These reports, compared to those written by U.S. firms, exhibit a much less rigid structure, which makes summarization a challenging task. In recent years, recurrent neural networks (RNNs) (e.g. (Nallapati et al., 2016)) and transformerbased neural networks (e.g. BERT (Devlin et al., 2019)) have been widely and successfully employed for extractive summarization in numerous text genres. We had hoped to employ such models for the FNS-2020 shared task. Unfortunately, the length of documents is beyond the models’ limits: with an average training document length of 6086 tokens, RNNs and transformers struggle to encode useful hidden representations in an end-to-end fashion. As a result, we hypothesized that some pre-selection over the input text is mandatory to obtain reasonable performance. In particular, we determined (see Section 3) that most sentences in the gold-standard report"
2020.fnp-1.25,2020.fnp-1.1,0,0.0701839,"Missing"
2020.fnp-1.25,D19-1387,0,0.0230109,"Missing"
2020.fnp-1.25,K16-1028,0,0.0460118,"Missing"
2020.fnp-1.25,P19-1499,0,0.0414162,"Missing"
2020.tacl-1.10,D18-1241,0,0.0537264,"Missing"
2020.tacl-1.10,L18-1431,0,0.0720002,"2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al., 2017) merely based on the information explicitly expressed in the provided text. With a goal of developing similarly challenging, but free-form multiple-choice datasets, and Abstract Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3 ), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-fo"
2020.tacl-1.10,N18-1144,0,0.04627,"Missing"
2020.tacl-1.10,N19-1423,0,0.156987,", 2016), in which no ground truth document supporting answers is provided with each question, making them relatively less suitable for isolating improvements to MRC. We will first discuss standard MRC datasets for English, followed by MRC/QA datasets for Chinese. English. Much of the early MRC work focuses on designing questions whose answers are spans from the given documents (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). As a question and its answer are usually in the same sentence, stateof-the-art methods (Devlin et al., 2019) have outperformed human performance on many such tasks. To increase task difficulty, researchers have explored a number of options including adding unanswerable (Trischler et al., 2017; Rajpurkar et al., 2018) or conversational (Choi et al., 2018; Reddy et al., 2019) questions that might require reasoning (Zhang et al., 2018a), and designing abstractive answers (Nguyen et al., 2016; Koˇcisk`y et al., 2018; Dalvi et al., 2018) or (question, answer) pairs that involve cross-sentence or crossdocument content (Welbl et al., 2018; Yang et al., 2018). In general, most questions concern the facts th"
2020.tacl-1.10,J02-2001,0,0.0602467,"analyze a subset of questions randomly sampled from the development and test sets of C3 and arrive at the following three kinds of prior knowledge required for answering questions. A question is labeled as matching if it exactly matches or nearly matches (without considering determiners, aspect particles, or conjunctive adverbs; Xia, 2000) a span in • Arithmetic† : This includes numerical computation and analysis (e.g., comparison and unit conversion). • Connotation: Answering questions requires knowledge about implicit and implied sentiment towards something or somebody, emotions, and tone (Edmonds and Hirst, 2002; 144 In 1928, recommended by Hsu Chih-Mo, Hu Shih, who was the president of the previous National University of China, employed Shen Ts’ung-wen as a lecturer of the university in charge of teaching the optional course of modern literature. At that time, Shen already made himself conspicuous in the literary world and was a little famous in society. For this sake, even before the beginning of class, the classroom was crowded with students. Upon the arrival of class, Shen went into the classroom. Seeing a dense crowd of students sitting beneath the platform, Shen was suddenly startled and his mi"
2020.tacl-1.10,N19-1300,0,0.0193072,"to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this"
2020.tacl-1.10,P13-1174,0,0.0112384,"m in mind. were greatly encouraged. expressed their understanding and encouraged him. ⋆ The passage above is mainly about the development of the Chinese educational system. how to make self-adjustment if one is nervous. the situation where Shen gave his lecture for the first time. ⋆ how Shen turned into a teacher from a writer. Table 2: A C3 -Mixed (C3M ) problem (left) and its English translation (right) (⋆: the correct option). itly in the text, which cannot be reached by paraphrasing sentences using linguistic knowledge. For example, Q4 in Table 2 and Q2 in Table 3 belong to this category. Feng et al., 2013; Van Hee et al., 2018). For example, the following conversation: ‘‘F: Ming Yu became a manager when he was so young! That’s impressive! M: It is indeed not easy!’’ is delivered in a tone for praise. • Part-whole: We require knowledge that object A is a part of object B. Relations such as member-of, stuff-of, and component-of between two objects also fall into this category (Winston et al., 1987; Miller, 1998). For example, we require implication mentioned above as well as part-whole knowledge (i.e., ‘‘teacher’’ is a kind of job) to summarize the main topic of the following • Cause-effect†: Th"
2020.tacl-1.10,A83-1007,0,0.625138,"nese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table 2 and Q3 in Table 3), we require lexical/syntactic knowledge including but not limited to: idioms, proverbs, negation, antonymy, synonymy, the possible meanings of the word, and syntactic transformations (Nassaji, 2006). DOMAIN-SPECIFIC: This kind of world knowledge consists of, but is not limited to, facts about domain-specific concepts, their definitions and properties, and relations among these concepts (Grishman et al., 1983; Hansen, 1994). GENERAL WORLD: It refers to the general knowledge about how the world works, sometimes called commonsense knowledge. We focus on the sort of world knowledge that an encyclopedia would assume readers know without being told (Lenat et al., 1985; Schubert, 2002) instead of the factual knowledge such as properties of famous entities. We further break down general world knowledge into eight subtypes, some of which (marked with †) are similar to the categories summarized by LoBue and Yates (2011) for textual entailment recognition. We summarize the overall statistics of C3 in Table"
2020.tacl-1.10,I17-4005,0,0.0308083,"k et al., 2016) ARC (Clark et al., 2016) ARC (Clark et al., 2016) DD (Lally et al., 2017) news books Wiki web mixed-genre mixed-genre dialogue cloze cloze free-form free-form cloze free-form free-form extractive extractive extractive abstractive multiple-choice multiple-choice multiple-choice 876.7K 3.6K 19.1K ≈ 200K 728.7K 10.0K 9.6K CNN/Daily (Hermann et al., 2015) CBT (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) CLOTH (Xie et al., 2018) RACE (Lai et al., 2017) DREAM (Sun et al., 2019a) English Counterpart Question Answering QS (Cheng et al., 2016) MCQA (Guo et al., 2017a) MedQA (Zhang et al., 2018b) GeoSQA (Huang et al., 2019) Machine Reading Comprehension PD (Cui et al., 2016) CFT (Cui et al., 2016) CMRC 2018 (Cui et al., 2018b) DuReader (He et al., 2017) ChID (Zheng et al., 2019) C3M (this work) C3D (this work) Table 1: Comparison of C3 and representative Chinese question answering and machine reading comprehension tasks. We list only one English counterpart for each Chinese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table"
2020.tacl-1.10,E17-1011,0,0.190542,"k et al., 2016) ARC (Clark et al., 2016) ARC (Clark et al., 2016) DD (Lally et al., 2017) news books Wiki web mixed-genre mixed-genre dialogue cloze cloze free-form free-form cloze free-form free-form extractive extractive extractive abstractive multiple-choice multiple-choice multiple-choice 876.7K 3.6K 19.1K ≈ 200K 728.7K 10.0K 9.6K CNN/Daily (Hermann et al., 2015) CBT (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) CLOTH (Xie et al., 2018) RACE (Lai et al., 2017) DREAM (Sun et al., 2019a) English Counterpart Question Answering QS (Cheng et al., 2016) MCQA (Guo et al., 2017a) MedQA (Zhang et al., 2018b) GeoSQA (Huang et al., 2019) Machine Reading Comprehension PD (Cui et al., 2016) CFT (Cui et al., 2016) CMRC 2018 (Cui et al., 2018b) DuReader (He et al., 2017) ChID (Zheng et al., 2019) C3M (this work) C3D (this work) Table 1: Comparison of C3 and representative Chinese question answering and machine reading comprehension tasks. We list only one English counterpart for each Chinese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table"
2020.tacl-1.10,C16-1167,0,0.565033,"the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al., 2017) merely based on the information explicitly expressed in the provided text. With a goal of developing similarly challenging, but free-form multiple-choice datasets, and Abstract Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C3 ), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associ"
2020.tacl-1.10,W93-0409,0,0.704013,"Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table 2 and Q3 in Table 3), we require lexical/syntactic knowledge including but not limited to: idioms, proverbs, negation, antonymy, synonymy, the possible meanings of the word, and syntactic transformations (Nassaji, 2006). DOMAIN-SPECIFIC: This kind of world knowledge consists of, but is not limited to, facts about domain-specific concepts, their definitions and properties, and relations among these concepts (Grishman et al., 1983; Hansen, 1994). GENERAL WORLD: It refers to the general knowledge about how the world works, sometimes called commonsense knowledge. We focus on the sort of world knowledge that an encyclopedia would assume readers know without being told (Lenat et al., 1985; Schubert, 2002) instead of the factual knowledge such as properties of famous entities. We further break down general world knowledge into eight subtypes, some of which (marked with †) are similar to the categories summarized by LoBue and Yates (2011) for textual entailment recognition. We summarize the overall statistics of C3 in Table 4. We observe n"
2020.tacl-1.10,Q18-1023,0,0.0605303,"Missing"
2020.tacl-1.10,D17-1082,0,0.187572,"rm question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Sha"
2020.tacl-1.10,D19-1597,0,0.158529,"al., 2016) DD (Lally et al., 2017) news books Wiki web mixed-genre mixed-genre dialogue cloze cloze free-form free-form cloze free-form free-form extractive extractive extractive abstractive multiple-choice multiple-choice multiple-choice 876.7K 3.6K 19.1K ≈ 200K 728.7K 10.0K 9.6K CNN/Daily (Hermann et al., 2015) CBT (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) CLOTH (Xie et al., 2018) RACE (Lai et al., 2017) DREAM (Sun et al., 2019a) English Counterpart Question Answering QS (Cheng et al., 2016) MCQA (Guo et al., 2017a) MedQA (Zhang et al., 2018b) GeoSQA (Huang et al., 2019) Machine Reading Comprehension PD (Cui et al., 2016) CFT (Cui et al., 2016) CMRC 2018 (Cui et al., 2018b) DuReader (He et al., 2017) ChID (Zheng et al., 2019) C3M (this work) C3D (this work) Table 1: Comparison of C3 and representative Chinese question answering and machine reading comprehension tasks. We list only one English counterpart for each Chinese dataset. 3.2 Data Statistics the given document; answering questions in this category seldom requires any prior knowledge. LINGUISTIC: To answer a given question (e.g., Q 1-2 in Table 2 and Q3 in Table 3), we require lexical/syntactic knowled"
2020.tacl-1.10,P18-2023,0,0.0254278,"testing on them separately, following the default setting on RACE that also contains two subsets (Lai et al., 2017). We run every experiment five times with different random seeds and report the best development set performance and its corresponding test set performance. Distance-Based Sliding Window. We simply treat each character as a token. We do not use Chinese word segmentation as it results in drops in performance based on our experiment. Co-Matching. We replace the English tokenizer with a Chinese word segmenter in HanLP.1 We use the 300-dimensional Chinese word embeddings released by Li et al. (2018). 4.3 Fine-Tuning Pre-Trained Language Models We also apply the framework of fine-tuning a pre-trained language model on machine reading comprehension tasks (Radford et al., 2018). We consider the following four pre-trained language models for Chinese: Chinese BERT-Base (denoted as BERT) (Devlin et al., 2019), Chinese ERNIE-Base (denoted as ERNIE) (Sun et al., 2019b), and Chinese BERT-Base with whole word masking during pre-training (denoted as BERT-wwm) (Cui et al., 2019) and its enhanced version pre-trained over larger corpora (denoted as BERT-wwm-ext). These models have the same number of l"
2020.tacl-1.10,W14-2903,0,0.0773902,"Missing"
2020.tacl-1.10,P17-1147,0,0.0646954,"Missing"
2020.tacl-1.10,P11-2057,0,0.0391109,"specific concepts, their definitions and properties, and relations among these concepts (Grishman et al., 1983; Hansen, 1994). GENERAL WORLD: It refers to the general knowledge about how the world works, sometimes called commonsense knowledge. We focus on the sort of world knowledge that an encyclopedia would assume readers know without being told (Lenat et al., 1985; Schubert, 2002) instead of the factual knowledge such as properties of famous entities. We further break down general world knowledge into eight subtypes, some of which (marked with †) are similar to the categories summarized by LoBue and Yates (2011) for textual entailment recognition. We summarize the overall statistics of C3 in Table 4. We observe notable differences exist between C3M and C3D . For example, C3M , in which most documents are formally written texts, has a larger vocabulary size compared to that of C3D with documents in spoken language. Similar observations have been made by Sun et al. (2019a) that the vocabulary size is relatively small in English dialogue-based machine reading comprehension tasks. In addition, the average document length (180.2) in C3M is longer than that in C3D (76.3). In general, C3 may not be suitable"
2020.tacl-1.10,N16-1098,0,0.159352,"2 Machine reading comprehension (MRC) tasks have attracted substantial attention from both academia and industry. These tasks require a machine reader to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answe"
2020.tacl-1.10,N18-1023,0,0.204962,"gle question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al."
2020.tacl-1.10,W16-5706,0,0.0588232,"Missing"
2020.tacl-1.10,S18-1119,0,0.0410237,"Missing"
2020.tacl-1.10,W10-0911,0,0.0503085,"Missing"
2020.tacl-1.10,P18-2124,0,0.0538498,"Missing"
2020.tacl-1.10,D13-1020,0,0.327171,"associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui"
2020.tacl-1.10,Q19-1014,1,0.555498,"as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenges for MRC systems. However, until recently, progress in the development of techniques for addressing this kind of MRC task for Chinese has lagged behind their English counterparts. A primary reason is that most previous work focuses on constructing MRC datasets for Chinese in which most answers are either spans (Cui et al., 2016; Li et al., 2016; Cui et al., 2018a; Shao et al., 2018) or abstractive texts (He et al., 2017) merely bas"
2020.tacl-1.10,W17-2623,0,0.0619458,"Missing"
2020.tacl-1.10,J18-4010,0,0.0631161,"Missing"
2020.tacl-1.10,P18-2118,0,0.0586708,"(Richardson et al., 2013), a rule-based method that chooses the answer option by taking into account (1) lexical similarity between a statement (i.e., a question and an answer option) and the given document with a fixed window size and (2) the minimum number of tokens between occurrences of the question and occurrences of an answer option in the document. This method assumes that a statement is more likely to be correct if there is a shorter distance between tokens within a statement, and more informative tokens in the statement appear in the document. 4.2 Co-Matching We utilize Co-Matching (Wang et al., 2018), a Bi-LSTM-based model for multiple-choice MRC tasks for English. It explicitly treats a question and one of its associated answer options as two sequences and jointly models whether or not the given document matches them. We modify the pre-processing step and adapt this model to MRC tasks for Chinese (Section 5.1). model, respectively. We add an embedding vector t1 to each token before the first [SEP] (inclusive) and an embedding vector t2 to every other token, where t1 and t2 are learned during language model pre-training for discriminating sequences. We denote the final hidden state for th"
2020.tacl-1.10,D16-1264,0,0.204036,"Missing"
2020.tacl-1.10,Q18-1021,0,0.0651545,"Missing"
2020.tacl-1.10,Q19-1016,0,0.0438365,"Missing"
2020.tacl-1.10,D18-1257,0,0.127078,"sion (MRC) tasks have attracted substantial attention from both academia and industry. These tasks require a machine reader to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions,"
2020.tacl-1.10,D18-1259,0,0.0722587,"Missing"
2020.tacl-1.10,C18-1038,0,0.0532576,"Missing"
2020.tacl-1.10,P19-1075,0,0.215414,"ave attracted substantial attention from both academia and industry. These tasks require a machine reader to answer questions relevant to a given document provided as input (Poon et al., 2010; Richardson et al., 2013). In this paper, we focus on free-form multiple-choice MRC tasks—given a document, select the correct answer option from all options associated with a freeform question, which is not limited to a single question type such as cloze-style questions formed by removing a span or a sentence in a text (Hill et al., 2016; Bajgar et al., 2016; Mostafazadeh et al., 2016; Xie et al., 2018; Zheng et al., 2019) or close-ended questions that can be answered with a minimal answer (e.g., yes or no; Clark et al., 2019). Researchers have developed a variety of freeform multiple-choice MRC datasets that contain a significant percentage of questions focusing on the implicitly expressed facts, events, opinions, or emotions in the given text (Richardson et al., 2013; Lai et al., 2017; Ostermann et al., 2018; Khashabi et al., 2018; Sun et al., 2019a). Generally, we require the integration of our own prior knowledge and the information presented in the given text to answer these questions, posing new challenge"
2021.eacl-main.52,D12-1091,0,0.115898,"e tagging, which results in a slightly better F1 score; (4) for the roles TARGET and W EAPON, our model is more conservative (lower recall) and achieves lower F1. One possibility is that for role like TAR - Models CohesionExtract (Huang and Riloff, 2012) NST (Du and Cardie, 2020) DY GIE++ (Wadden et al., 2019) GRIT P R F1 58.38 39.53 47.14 56.82 48.92 52.58 57.04 46.77 51.40 64.19∗∗ 47.36 54.50∗ Table 4: Micro-average results (the highest number of each column is boldfaced). Significance is indicated with ∗∗ (p &lt; 0.01),∗ (p &lt; 0.1) – all tests are computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012). GET , on average there are more entities (though with only one mention each), and it’s harder for our model to decode as many TARGET entities correct in a generative way. 8 Discussion How well do the models capture coreference relations between mentions? We also conduct targeted evaluations on subsets of test documents whose gold extractions come with coreferent mentions. From left to right in Table 3, we report results on the subsets of documents with increasing number (k) of possible (coreferent) mentions per role-filler entity. We find that: (1) On the subset of documents with only one me"
2021.eacl-main.52,D13-1185,0,0.0680623,"red in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity"
2021.eacl-main.52,P11-1098,0,0.131494,"tion extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representation"
2021.eacl-main.52,P15-1017,0,0.349754,"ar in a sentence that does not explicitly mention the explosion of the bomb. In addition, REE is ultimately an entity-based task — exactly one descriptive mention for each role-filler should be extracted even when the entity is referenced multiple times in connection with the event. The final output for the bombing example should, therefore, include just one of the “water pipes” references, and one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these appr"
2021.eacl-main.52,N13-1104,0,0.0218939,"nt; (2) concatenates the representations of the span’s beginning & end token and use it as its representation, and pass it through a classifier layer to predict whether the span represents certain role-filler entity and what the role is. Both the NST and DYGIE++ are end-to-end and fine-tuned BERT (Devlin et al., 2019) contextualized representations with task-specific data. We train them to identify the first mention for each role-filler entity (to ensure fair comparison with our proposed model). Unsupervised event schema induction based approaches (Chambers and Jurafsky, 2011; Chambers, 2013; Cheung et al., 2013) are also able 639 5 Instead of using feature-engineering based sentence classification to identify event-relevant sentences, we re-implement the sentence classifier with BiLSTM-based neural sequence model. NST (Du and Cardie, 2020) DY GIE++ (Wadden et al., 2019) GRIT P ERP I ND P ERP O RG TARGET V ICTIM W EAPON 48.39 / 32.61 / 38.96 60.00 / 43.90 / 50.70 54.96 / 52.94 / 53.93 62.50 / 63.16 / 62.83 61.67 / 61.67 / 61.67 59.49 / 34.06 / 43.32 56.00 / 34.15 / 42.42 53.49 / 50.74 / 52.08 60.00 / 66.32 / 63.00 57.14 / 53.33 / 55.17 65.48 / 39.86 / 49.55 66.04 / 42.68 / 51.85 55.05 / 44.12 / 48.98"
2021.eacl-main.52,N19-1423,0,0.525565,"nd one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these approaches (as well as most sentence-level methods) handle each candidate role-filler prediction in isolation. Consequently, they cannot easily model the coreference structure required to limit spurious role-filler mention extractions. Nor can they easily exploit semantic dependencies between closely related roles like the P ERP I ND and the P ERP O RG, which can share a portion of the same entity s"
2021.eacl-main.52,2020.acl-main.714,1,0.147426,"bing example should, therefore, include just one of the “water pipes” references, and one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these approaches (as well as most sentence-level methods) handle each candidate role-filler prediction in isolation. Consequently, they cannot easily model the coreference structure required to limit spurious role-filler mention extractions. Nor can they easily exploit semantic dependencies between closely related roles lik"
2021.eacl-main.52,I17-1036,0,0.0150541,"n (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are"
2021.eacl-main.52,2020.acl-main.718,0,0.0908325,"Missing"
2021.eacl-main.52,X96-1047,0,0.464851,"cording to unofficial reports, the bomb contained [125 to 150 grams of TnT] and was placed in the back of the [Pilmai [telephone company building]]. The explosion occurred at 2350 on 16 January, causing panic but no casualties. The explosion caused damages to the [telephone company offices]. It also destroyed a [public telephone booth] and [water pipes]. Witnesses reported that the bomb was planted by [[two men] wearing sports clothes], who escaped into the night. … They were later identified as [[Shining Path] members]. Gold extractions: Document-level template filling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014). It is of great importance for automating many real-world tasks, such as event extraction from newswire (Sundheim, 1991). The complete task is generally tackled in two steps. The first step detects events in the article and assigns templates to each of them (template recognition); the second step performs role-filler entity extraction (REE) for filling in the templates. In this work we focus on the role-filler entity extraction (REE) sub-task of template filling (Figure 1).1 The input text describes a bomb"
2021.eacl-main.52,P11-1114,0,0.877778,"two steps. The first step detects events in the article and assigns templates to each of them (template recognition); the second step performs role-filler entity extraction (REE) for filling in the templates. In this work we focus on the role-filler entity extraction (REE) sub-task of template filling (Figure 1).1 The input text describes a bombing event; the goal is to identify the entities that fill any of the roles associated with the event (e.g., the perpetrator, their organization, the weapon) by extracting 1 In this work, we assume there is one generic template for the entire document (Huang and Riloff, 2011, 2012). water pipes, water pipes Physical Target Pilmai telephone company building, telephone company building, telephone company offices public telephone booth Weapon 125 to 150 grams of TnT Victim - Figure 1: Role-filler entity extraction (REE). The first mention of each role-filler entity is bold in the table and document. The arrows denote coreferent mentions. a descriptive “mention” of it – a string from the document. In contrast to sentence-level event extraction (see, e.g., the ACE evaluation (Linguistic Data Consortium, 2005)), document-level REE introduces 634 Proceedings of the 16th"
2021.eacl-main.52,P08-1030,0,0.203698,"2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et"
2021.eacl-main.52,N19-1370,0,0.11043,"Missing"
2021.eacl-main.52,N16-1030,0,0.0580695,"at does not explicitly mention the explosion of the bomb. In addition, REE is ultimately an entity-based task — exactly one descriptive mention for each role-filler should be extracted even when the entity is referenced multiple times in connection with the event. The final output for the bombing example should, therefore, include just one of the “water pipes” references, and one of the three alternative descriptions of the P ERP I ND and the second TARGET, the telephone company building. As a result of these complications, end-to-end sentence-level event extraction models (Chen et al., 2015; Lample et al., 2016), which dominate the literature, are ill-suited for the REE task, which calls for models that encode information and track entities across a longer context. Fortunately, neural models for event extraction that have the ability to model longer contexts have been developed. Du and Cardie (2020), for example, extend standard contextualized representations (Devlin et al., 2019) to produce a documentlevel sequence tagging model for event argument extraction. Both approaches show improvements in performance over sentence-level models on event extraction. Regrettably, these approaches (as well as mos"
2021.eacl-main.52,P13-1008,0,0.0991986,"model outperforms substantially strong baseline models. We also demonstrate that GRIT is better than existing document-level event extraction approaches at capturing linguistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to"
2021.eacl-main.52,W15-4502,0,0.0215223,"stantially strong baseline models. We also demonstrate that GRIT is better than existing document-level event extraction approaches at capturing linguistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji an"
2021.eacl-main.52,P10-1081,0,0.182524,"iety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2"
2021.eacl-main.52,P17-1164,0,0.0374087,"Missing"
2021.eacl-main.52,P19-1276,0,0.0131219,"sing hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and rel"
2021.eacl-main.52,D18-1156,0,0.142723,"Missing"
2021.eacl-main.52,D18-1360,0,0.0198759,"that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly cap"
2021.eacl-main.52,N19-1308,0,0.0178595,"017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly captures (non-)coreference relations between noun phrases, without relying on the cross-sentence coreference and relation annotations during training. Neural Generative Models with a Shared Module for Encoder and Decoder Our GRIT model uses one shared transformer module for both the encoder and decoder, which is simple and effective. For the machine translation task, He et al. (2018) propose a model which shares the parameters of each layer between the encoder and decoder to regular"
2021.eacl-main.52,H05-1004,0,0.464167,"IT is built upon the pre-trained transformer model (BERT): we add a pointer selection module in the decoder to permit access to the entire input document, and a generative head to model document-level extraction decisions. In spite of the added extraction capability, GRIT requires no additional parameters beyond those in the pre-trained BERT. • To measure the model’s ability to both extract entities for each role, and implicitly recognize coreferent relations between entity mentions, we design a metric (CEAF-REE) based on a maximum bipartite matching algorithm, drawing insights from the CEAF (Luo, 2005) coreference resolution measure. • We evaluate GRIT on the MUC-4 (1992) REE task (Section 3). Empirically, our model outperforms substantially strong baseline models. We also demonstrate that GRIT is better than existing document-level event extraction approaches at capturing linguistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event tr"
2021.eacl-main.52,M91-1001,0,0.142289,"zation Shining Path According to unofficial reports, the bomb contained [125 to 150 grams of TnT] and was placed in the back of the [Pilmai [telephone company building]]. The explosion occurred at 2350 on 16 January, causing panic but no casualties. The explosion caused damages to the [telephone company offices]. It also destroyed a [public telephone booth] and [water pipes]. Witnesses reported that the bomb was planted by [[two men] wearing sports clothes], who escaped into the night. … They were later identified as [[Shining Path] members]. Gold extractions: Document-level template filling (Sundheim, 1991, 1993; Grishman and Sundheim, 1996) is a classic problem in information extraction (IE) and NLP (Jurafsky and Martin, 2014). It is of great importance for automating many real-world tasks, such as event extraction from newswire (Sundheim, 1991). The complete task is generally tackled in two steps. The first step detects events in the article and assigns templates to each of them (template recognition); the second step performs role-filler entity extraction (REE) for filling in the templates. In this work we focus on the role-filler entity extraction (REE) sub-task of template filling (Figure"
2021.eacl-main.52,P16-1105,0,0.0315253,"n (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly captures (non-)coreference relations between noun phrases, without relying on the cross-sentence coreference and relation annotations during training. Neural Generative Models with a Shared Module for Encoder and Decoder Our GRIT model uses one shared transformer module for both the encoder and decoder, which is simple and effective. For the machine translation task, He et al. (2018) propose a model which shares the parameters of each laye"
2021.eacl-main.52,D19-1585,0,0.558781,"n event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective,"
2021.eacl-main.52,D18-1215,0,0.0170595,"nging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting"
2021.eacl-main.52,N16-1034,0,0.0574356,"guistic properties critical for the task, including coreference between entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and mode"
2021.eacl-main.52,P15-2060,0,0.0156441,"entity mentions and cross-role extraction dependencies.2 2 Related Work Sentence-level Event Extraction Most work in event extraction has focused on the ACE sentencelevel event task (Walker et al., 2006), which requires the detection of an event trigger and extraction of its arguments from within a single sentence. Previous state-of-the-art methods include Li et al. (2013) and Li et al. (2015), which explored a variety of hand-designed features. More recently, neural network based models such as recurrent neural networks (Nguyen et al., 2016; Feng et al., 2018), convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) and attention mechanisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction pu"
2021.eacl-main.52,D09-1016,0,0.470518,"om a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to our work. The key difference is that our work focuses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction"
2021.eacl-main.52,Q17-1008,0,0.0250602,"ses on a more challenging, and more realistic, setting: extracting role-filler entities rather than lists of role-filler mentions that are not grouped according to their associated entity. Also on a related note, Chambers and Jurafsky (2011), Chambers (2013), and Liu et al. (2019) work on unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan"
2021.eacl-main.52,N16-1033,0,0.0470744,"nisms (Liu et al., 2017, 2018) have also been shown to help improve performance. Beyond the taskspecific features learned by the deep neural models, Zhang et al. (2019) and Wadden et al. (2019) also utilize pre-trained contextualized representations. Only a few models have gone beyond individual sentences to make decisions. Ji and Grishman (2008) and Liao and Grishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level ev"
2021.eacl-main.52,P19-1074,0,0.021912,"unsupervised event schema induction and open-domain event extraction from documents. The main idea is to group entities corresponding to the same role into an event template. Recently, there has also been increasing interest in cross-sentence/document-level relation extraction (RE). In the scientific domain, Peng et al. (2017); Wang and Poon (2018); Jia et al. (2019) study N -ary cross-sentence RE using distant supervision annotations. Luan et al. (2018) introduce SciERC dataset and their model rely on multi-task learning to share representations between entity span extraction and relations. Yao et al. (2019) construct an RE dataset of cross-sentence relations on Wikipedia paragraphs. Ebner et al. (2020) introduce RAMS dataset for multi-sentence argument mention linking, while we focus on entity-level extraction in our work. Different from work on joint modeling (Miwa and Bansal, 2016) and multi-task learning (Luan et al., 2019) setting for extracting entities and relations, through the generative modeling setup, our GRIT model implicitly captures (non-)coreference relations between noun phrases, without relying on the cross-sentence coreference and relation annotations during training. Neural Gen"
2021.eacl-main.52,P18-2066,0,0.0123888,"rishman (2010) utilize event type co-occurrence patterns to propagate 635 2 Our code for the evaluation script and models is at https://github.com/xinyadu/grit_doc_ event_entity for reproduction purposes. event classification decisions. Yang and Mitchell (2016) propose to learn within-event (sentence) structures for jointly extracting events and entities within a document context. Similarly, from a methodological perspective, our GRIT model also learns structured information, but it learns the dependencies between role-filler entity mentions and between different roles. Duan et al. (2017) and Zhao et al. (2018) leverage document embeddings as additional features to aid event detection. Although the approaches above make decisions with cross-sentence information, their extractions are still done the sentence level. Document-level IE Document-level event rolefiller mention extraction has been explored in recent work, using hand-designed features for both local and additional context (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012), and with end-to-end sequence tagging based models with contextualized pre-trained representations (Du and Cardie, 2020). These efforts are the most related to ou"
2021.findings-acl.386,P19-1057,1,0.88925,"e topic embedding features and topic semantics features, along with the previously studied factors of persuasion. We find that incorporating the topic relatedness features help improve state-of-the-art results in persuasion prediction. Moreover, we conduct experiments in a few-shot setting and show that these features help models achieve significantly better generalization performance for the rare topics. 4401 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4401–4407 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Issue Dataset We use DDO (Durmus and Cardie, 2019) for our study. DDO includes 67,315 debates from 23 different categories, 36,294 users with their background information (e.g. political ideology, and religious ideology), and 198,759 votes from the users when they are as readers of these debates. For each debate, two debaters with different viewpoints express their opinions on a controversial topic in rounds. After the debate, voters evaluate the debaters with respect to various criteria and they share whether any of the debaters changed their stance on the topic. Users also have an opportunity to share their demographic and ideological infor"
2021.findings-acl.386,D19-1568,1,0.886101,"Missing"
2021.findings-acl.386,W16-6209,0,0.0224573,"gnificantly more when we remove the attribute embedding and the topic embeddings, which indicates that the topic-related embeddings benefit the knowledge transferring among debates with different topics. 6 Related Work Persuasion Studies. Understanding the characteristics of persuasive language has been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) studied the effect of top"
2021.findings-acl.386,W15-0608,0,0.0400361,"rnal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) studied the effect of topic relevancy or consistency on essay scoring. Bosc et al. (2016) proposes a dataset of Social Media data with coarse topic labels extracted from the hashtags (e.g., #AppleWatch). Zeng et al. (2020) designed a model to encode the latent topics of argumentative conversations. Unlike the previous work, our work studies the effect of argument topic for structured debates explicitly for predicting persuasion. References 7 Marilyn J. Chambliss and Ruth Garner. 1996. Do adults change their minds after reading persuasive text? Written Communication, 13(3):291–313. Conclusion"
2021.findings-acl.386,P11-1099,0,0.0319707,"topics, in a few-shot setting. 1 Introduction Emergence of social media and online argumentative forums provide users with a platform to gain information, express, and form opinions on a diverse set of controversial topics (i.e. issues). The increasing importance of these online platforms has motivated NLP researchers to use these platforms as one of the main domains to study the important factors of persuasion. In particular, prior work has shown that characteristics of the speaker (i.e. source), prior beliefs of the audience (Lukin et al., 2017; Durmus and Cardie, 2018), and language style (Feng and Hirst, 2011; Tan et al., 2016) are important factors in determining persuasiveness of the arguments in online argumentation platforms. Although there has been evidence in previous studies of Social Sciences that people’s perceptions To study the impact of involving topic relatedness in argument persuasion, we define two types of features: (1) topic embedding features and (2) topic semantics features. Prior work has shown that topic is an important factor (Das et al., 2016) to determine whether an emotional vs. a logical argument will be received positively by the audience. We hypothesize that encoding un"
2021.findings-acl.386,D16-1129,0,0.0129989,"the prediction performance is significantly more when we remove the attribute embedding and the topic embeddings, which indicates that the topic-related embeddings benefit the knowledge transferring among debates with different topics. 6 Related Work Persuasion Studies. Understanding the characteristics of persuasive language has been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) stu"
2021.findings-acl.386,P16-1150,0,0.0256321,"the prediction performance is significantly more when we remove the attribute embedding and the topic embeddings, which indicates that the topic-related embeddings benefit the knowledge transferring among debates with different topics. 6 Related Work Persuasion Studies. Understanding the characteristics of persuasive language has been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) stu"
2021.findings-acl.386,D14-1083,0,0.0305875,"hat the gap between the prediction performance is significantly more when we remove the attribute embedding and the topic embeddings, which indicates that the topic-related embeddings benefit the knowledge transferring among debates with different topics. 6 Related Work Persuasion Studies. Understanding the characteristics of persuasive language has been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mi"
2021.findings-acl.386,W17-5102,0,0.0200736,"s been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) studied the effect of topic relevancy or consistency on essay scoring. Bosc et al. (2016) proposes a dataset of Social Media data with coarse topic labels extracted from the hashtags (e.g., #AppleWatch). Zeng et al. (2020) designed a model to encode the latent topics of argumentative conversations. Unlike the previous work,"
2021.findings-acl.386,2020.emnlp-main.716,1,0.855979,"Missing"
2021.findings-acl.386,W19-4519,1,0.836035,"the corresponding BIG ISSUE as introduced in Section 3.1. We further encode the text of the debate topic with a fine-tuned BERT (Devlin et al., 2019) taking average embedding of all the tokens to get the representation of the topic semantics. We concatenate these two types of embeddings to get the final representation of the debate topic. Representing User Information. Previous work shows that both characteristics of the debaters and the audience and the linguistic features of the debate arguments are important factors in persuasion studies (Lukin et al., 2017; Durmus and Cardie, 2018, 2019; Longpre et al., 2019). Similar to prior work, to encode the background information, we first represent the user background with one-hot representation (ONE-HOT) to capture the users’ selections on the categories (e.g., gender, political ideology, religious ideology, and etc.) or the opinion similarity with the voters. However, this Figure 1: Overall model structure. User, Semantic, Embedding blocks denote the encoders for user information, argument semantics, and topic embedding. FFNN is a multi-layer feed-forward neural network. representation can be very sparse and not relevant to the topic information. Therefor"
2021.findings-acl.386,E17-1070,0,0.305835,"ting persuasiveness and also helps enhance generalization to rare topics, in a few-shot setting. 1 Introduction Emergence of social media and online argumentative forums provide users with a platform to gain information, express, and form opinions on a diverse set of controversial topics (i.e. issues). The increasing importance of these online platforms has motivated NLP researchers to use these platforms as one of the main domains to study the important factors of persuasion. In particular, prior work has shown that characteristics of the speaker (i.e. source), prior beliefs of the audience (Lukin et al., 2017; Durmus and Cardie, 2018), and language style (Feng and Hirst, 2011; Tan et al., 2016) are important factors in determining persuasiveness of the arguments in online argumentation platforms. Although there has been evidence in previous studies of Social Sciences that people’s perceptions To study the impact of involving topic relatedness in argument persuasion, we define two types of features: (1) topic embedding features and (2) topic semantics features. Prior work has shown that topic is an important factor (Das et al., 2016) to determine whether an emotional vs. a logical argument will be"
2021.findings-acl.386,D14-1162,0,0.0838173,"Missing"
2021.findings-acl.386,D19-1410,0,0.0830551,"0)) etc. Similar to the topic semantics introduced in Section 3.2, we also represent the semantics of the arguments with the same fine-tuned BERT. Proposed Model. We employ a model that contains separate encoders to represent the debater characteristics, arguments, and topic-related features, as shown in Figure 1. The model encodes the debater’s background information and opinions towards the BIG ISSUES, and combines the linguistic features extracted from arguments to represent the users. For the text in the debate, the model consists a siamese network structure (Semantic Block + FFNN block) (Reimers and Gurevych, 2019) to encode the relation between arguments and debate topics. Then, the model extracts the issue representation from the pretrained issue embedding introduced in Section 3.1. Finally, the user representation, together with the representation for the argument semantics and topic embedding, is passed through a multi-layer feed-forward neural network to predict the voter’s perception on the persuasiveness. 4403 Model Majority Bi-LSTM+Glove SBERT O NE -H OT+Linguistic+Topic SVM ATT-E MB+Linguistic SVM ATT-E MB+Linguistic+Topic SVM Ours with O NE -H OT Ours W / O ARGUMENT Ours W / O ATT-E MB Ours W"
2021.findings-acl.386,W10-0214,0,0.0544771,"y sparse and not relevant to the topic information. Therefore, we also experiment with the topic-centric embedding-based method (ATT-EMB) proposed in Section 3.1. We compute the background similarity as the cosine similarity of the representation vectors for the users (i.e., debaters and voters). Linguistic Features. Consistent with the prior work (Durmus and Cardie, 2018), to encode the arguments in the debates, we extract linguistic features including the information about the style (i.e., length, links), sentiment polarity, subjectivity (Wilson et al., 2005), and argument lexicon features (Somasundaran and Wiebe, 2010)) etc. Similar to the topic semantics introduced in Section 3.2, we also represent the semantics of the arguments with the same fine-tuned BERT. Proposed Model. We employ a model that contains separate encoders to represent the debater characteristics, arguments, and topic-related features, as shown in Figure 1. The model encodes the debater’s background information and opinions towards the BIG ISSUES, and combines the linguistic features extracted from arguments to represent the users. For the text in the debate, the model consists a siamese network structure (Semantic Block + FFNN block) (Re"
2021.findings-acl.386,P14-1017,0,0.0236845,"on Studies. Understanding the characteristics of persuasive language has been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) studied the effect of topic relevancy or consistency on essay scoring. Bosc et al. (2016) proposes a dataset of Social Media data with coarse topic labels extracted from the hashtags (e.g., #AppleWatch). Zeng et al. (2020) designed a model to encode th"
2021.findings-acl.386,P19-1566,0,0.0178927,"te embedding and the topic embeddings, which indicates that the topic-related embeddings benefit the knowledge transferring among debates with different topics. 6 Related Work Persuasion Studies. Understanding the characteristics of persuasive language has been an im4404 3 Implementation details are described in Appendix A.1. portant area of study in the Sociology, Psychology (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996) and NLP communities (Hasan and Ng, 2014; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Al-Khatib et al., 2017; Wang et al., 2019). The emergence of social media and argumentative forums has further attracted researchers to study the dynamics of persuasion on these platforms, including Twitter (Tan et al., 2014), ChangeMyView community on Reddit (Tan et al., 2016; Hidey et al., 2017) and DDO (Durmus and Cardie, 2019). In this work, we use DDO since it includes a wide-range of user information including users’ opinions on various controversial topics as well as well-structured debates with audience votes. Topic Aware Argument Mining. Farra et al. (2015) studied the effect of topic relevancy or consistency on essay scoring"
2021.findings-acl.386,H05-1044,0,0.112004,"eed-forward neural network. representation can be very sparse and not relevant to the topic information. Therefore, we also experiment with the topic-centric embedding-based method (ATT-EMB) proposed in Section 3.1. We compute the background similarity as the cosine similarity of the representation vectors for the users (i.e., debaters and voters). Linguistic Features. Consistent with the prior work (Durmus and Cardie, 2018), to encode the arguments in the debates, we extract linguistic features including the information about the style (i.e., length, links), sentiment polarity, subjectivity (Wilson et al., 2005), and argument lexicon features (Somasundaran and Wiebe, 2010)) etc. Similar to the topic semantics introduced in Section 3.2, we also represent the semantics of the arguments with the same fine-tuned BERT. Proposed Model. We employ a model that contains separate encoders to represent the debater characteristics, arguments, and topic-related features, as shown in Figure 1. The model encodes the debater’s background information and opinions towards the BIG ISSUES, and combines the linguistic features extracted from arguments to represent the users. For the text in the debate, the model consists"
2021.findings-acl.386,N16-1017,0,0.019076,") topic embedding features and (2) topic semantics features. Prior work has shown that topic is an important factor (Das et al., 2016) to determine whether an emotional vs. a logical argument will be received positively by the audience. We hypothesize that encoding underlying relationship among topics with topic embedding features will be helpful in predicting persuasion since similar strategies may be effective for related topics. We further define topic semantics features to encode how focused vs. divergent each of the arguments made by the debaters is given the discussion topic, similar to Zhang et al. (2016). We first develop an embedding-based technique inspired by (Barkan and Koenigstein, 2016) to determine the relationship among controversial topics. This methodology leverages users’ stances on the topics to determine the relationship among them. We then incorporate the topic embedding features and topic semantics features, along with the previously studied factors of persuasion. We find that incorporating the topic relatedness features help improve state-of-the-art results in persuasion prediction. Moreover, we conduct experiments in a few-shot setting and show that these features help models"
2021.findings-emnlp.148,D16-1125,0,0.0150745,"row norm (Equation 4) mirrors and depth d optimized on the test data. the special case of a speaker that considers the cost Figure 5 presents the absolute top-1 accuracy of generating a message as zero. This assumption 1719 is reasonable for classification. It is also common practice when working with a finite set of intents in RSA (Monroe et al., 2017; Zarrieß and Schlangen, 2019). The column norm (Equation 6) describes a mathematical formulation aligned with how a pragmatic listener infers speaker expectations. Previous work has applied RSA to systems that generate and understand language (Andreas and Klein, 2016; Mao et al., 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018; Zarrieß and Schlangen, 2019) in both referential games (Frank and Goodman, 2012; Goodman and Frank, 2016; Monroe et al., 2017) and sequential decisionmaking systems (Fried et al., 2018a,b). Our method departs from these applications by focusing on the ambiguity avoidance property of the listener agent as applied to generic classification tasks. Confidence Calibration Similar to confidence calibration techniques (Platt et al., 1999; Zadrozny and Elkan, 2002; Guo et al., 2017; Kumar et al., 2019), our method rescales the poster"
2021.findings-emnlp.148,P18-1009,0,0.0731625,"ix after d steps of Step 2:  Ad Ld = T bd  . (7) We keep bd as the re-adjusted class probability distribution P0 (y|x), and discard Ad . 3 Simulations on Random Matrices We study the effect of the ambiguity level of the source prediction and the reference set using Monte Carlo simulations. We randomly generate A0 , b0 , and Λq to evaluate the expected performance 4 Each normalization step (Steps 2.1–2.2) takes O(mn) because the matrices ΛS , ΛL , Λq are diagonal. 1717 Datasets # Classes Marco F1 Method val Ultrafine Entity Typing DialogRE 10331 36 Mirco F1 test val test BASELINE (Multitask; Choi et al., 2018) CAN (↑2.15) 31.32 33.47 (↑1.71) 31.98 33.69 (↑2.51) 27.92 30.43 (↑1.89) 28.80 30.69 BASELINE (Denoised; Onoe and Durrett, 2019) CAN (↑0.34) 40.07 40.41 (↑0.53) 40.22 40.75 (↑0.59) 37.88 38.47 (↑0.84) 37.87 38.71 BASELINE (BERT; Yu et al., 2020 ) CAN (↑0.91) 35.89 36.80 (↑0.70) 35.76 36.45 (↑0.16) 59.44 59.60 (↑0.34) 57.93 58.27 BASELINE (BERTs; Yu et al., 2020) CAN (↑0.83) 40.58 41.41 (↑0.68) 39.45 40.13 (↑0.33) 62.18 62.51 (↑0.29) 59.52 59.81 Table 1: Performance on the Ultrafine Entity Typing and DialogRE tasks. δ(b0, b1) Δ(B0, B1) trix sizes, simulations, and values of α. We observe that ("
2021.findings-emnlp.148,N18-2070,0,0.0127927,"ata. the special case of a speaker that considers the cost Figure 5 presents the absolute top-1 accuracy of generating a message as zero. This assumption 1719 is reasonable for classification. It is also common practice when working with a finite set of intents in RSA (Monroe et al., 2017; Zarrieß and Schlangen, 2019). The column norm (Equation 6) describes a mathematical formulation aligned with how a pragmatic listener infers speaker expectations. Previous work has applied RSA to systems that generate and understand language (Andreas and Klein, 2016; Mao et al., 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018; Zarrieß and Schlangen, 2019) in both referential games (Frank and Goodman, 2012; Goodman and Frank, 2016; Monroe et al., 2017) and sequential decisionmaking systems (Fried et al., 2018a,b). Our method departs from these applications by focusing on the ambiguity avoidance property of the listener agent as applied to generic classification tasks. Confidence Calibration Similar to confidence calibration techniques (Platt et al., 1999; Zadrozny and Elkan, 2002; Guo et al., 2017; Kumar et al., 2019), our method rescales the posterior distribution produced by the classifier at test time. However,"
2021.findings-emnlp.148,N19-1423,0,0.00816335,"y to both emojis (b). CAN re-scales the class probability distribution and produces a less ambiguous prediction (c).1 Introduction Classification is core to NLP, and many language problems can be effectively addressed as supervised classification tasks. However, even the most effective classifier can suffer when given examples to classify that are close to its decision boundary. The reasons for such failures vary, and include lack of training data coverage, limited representation expressivity, or over-fitting the training data. Despite significant progress, including using pre-trained models (Devlin et al., 2019) to address these issues, every classifier has its weak spots, and some examples will be hard to classify correctly. In this paper, we study a simple nonparameterized post-processing step to improve classifier accuracy on difficult examples. At the core of our approach is using Alternating Normalization (AN; Sinkhorn and Knopp, 1967) to re-adjust the prediction of low-confidence examples using the predicted class distributions of a reference set of high-confidence validation examples. Our process, Classification with Alternating Normalization (CAN), is applicable to any classifier that generat"
2021.findings-emnlp.148,N18-1177,0,0.0268361,"Missing"
2021.findings-emnlp.148,W16-1313,0,0.0640687,"Missing"
2021.findings-emnlp.6,D19-1253,0,0.0238646,"several methods to use the generated QA-based weakly-labeled MRC data. We further propose a simple yet effective self-teaching paradigm to better utilize large-scale weakly-labeled data. • We show that our QA-based weakly-labeled MRC data can be easily used along with other types of weakly-labeled data for further gains. 2 2.1 Related Work From Question Answering to Machine Reading Comprehension This work is related to data augmentation in semi-supervised MRC studies, which partially or fully rely on the document-question-answer triples (Yang et al., 2017; Yuan et al., 2017; Yu et al., 2018; Zhang and Bansal, 2019; Zhu et al., 2019; Dong et al., 2019; Sun et al., 2019b; Alberti et al., 2019; Asai and Hajishirzi, 2020; Rennie et al., 2020) of target MRC tasks or at least similar domain corpora (Dhingra et al., 2018). We focus on leveraging multi-domain QA data to improve different types of general-domain MRC tasks. We study the effect of our large-scale weaklylabeled MRC data on representative MRC datasets for Chinese: a multiple-choice dataset, C3 (Sun et al., 2020b), in which most questions cannot be solved solely by matching or paraphrasing, and an extractive dataset, CMRC 2018 (Cui et al., 2019), in"
2021.findings-emnlp.6,2020.emnlp-main.142,0,0.18817,"eved context instead of noisy answers, (ii) we generate weakly-labeled data based on existing large-scale QA data covering a wide range of domains, instead of the same domain (He et al., 2020; Xie et al., 2020a; Zhao et al., 2020; Chen et al., 2020) or at least approximately in-domain (Du et al., 2020) as the target MRC task, and (iii) ground-truth labels of weaklylabeled data are used directly or indirectly to train teacher models. Note that we use teacher models to generate new soft labels for fixed weakly-labeled data instead of new pseudo data with noisy labels from unlabeled data (e.g., (Wang et al., 2020a)). Compared with previous multi-teacher student paradigms (You et al., 2019; Wang et al., 2020b; Yang et al., 2020), to train models to be strong teachers, we conduct iterative training and leverage large-scale weakly-labeled data rather than using clean, human-labeled data of similar tasks. 3 3.1 3.2 Comparisons with Existing Subject-Area Question-Answering Datasets Subject-area QA is an increasingly popular direction focusing on closing the performance gap between humans and machines in answering questions collected from real-world exams that are carefully designed by subject-matter expert"
2021.findings-emnlp.6,2020.coling-main.248,0,0.0406771,"e widely used for knowledge distillation (Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015). We aim to let a student model outperform its teacher model for performance improvements and thus use the same architecture for all teacher and student models. Our work is related to self-training (Yarowsky, 1995; Riloff, 1996). The main differences are (i) noise is introduced by retrieved context instead of noisy answers, (ii) we generate weakly-labeled data based on existing large-scale QA data covering a wide range of domains, instead of the same domain (He et al., 2020; Xie et al., 2020a; Zhao et al., 2020; Chen et al., 2020) or at least approximately in-domain (Du et al., 2020) as the target MRC task, and (iii) ground-truth labels of weaklylabeled data are used directly or indirectly to train teacher models. Note that we use teacher models to generate new soft labels for fixed weakly-labeled data instead of new pseudo data with noisy labels from unlabeled data (e.g., (Wang et al., 2020a)). Compared with previous multi-teacher student paradigms (You et al., 2019; Wang et al., 2020b; Yang et al., 2020), to train models to be strong teachers, we conduct iterative training and leverage large-scale"
2021.findings-emnlp.6,P19-1415,0,0.0251983,"the generated QA-based weakly-labeled MRC data. We further propose a simple yet effective self-teaching paradigm to better utilize large-scale weakly-labeled data. • We show that our QA-based weakly-labeled MRC data can be easily used along with other types of weakly-labeled data for further gains. 2 2.1 Related Work From Question Answering to Machine Reading Comprehension This work is related to data augmentation in semi-supervised MRC studies, which partially or fully rely on the document-question-answer triples (Yang et al., 2017; Yuan et al., 2017; Yu et al., 2018; Zhang and Bansal, 2019; Zhu et al., 2019; Dong et al., 2019; Sun et al., 2019b; Alberti et al., 2019; Asai and Hajishirzi, 2020; Rennie et al., 2020) of target MRC tasks or at least similar domain corpora (Dhingra et al., 2018). We focus on leveraging multi-domain QA data to improve different types of general-domain MRC tasks. We study the effect of our large-scale weaklylabeled MRC data on representative MRC datasets for Chinese: a multiple-choice dataset, C3 (Sun et al., 2020b), in which most questions cannot be solved solely by matching or paraphrasing, and an extractive dataset, CMRC 2018 (Cui et al., 2019), in which all answers"
2021.naacl-main.124,W17-5526,0,0.0400521,"Missing"
2021.naacl-main.124,2020.sigdial-1.4,0,0.0483049,"Missing"
2021.naacl-main.124,W14-4337,0,0.0922989,"Missing"
2021.naacl-main.124,2021.ccl-1.108,0,0.0897798,"Missing"
2021.naacl-main.124,D17-2014,0,0.065602,"Missing"
2021.naacl-main.124,P19-1081,1,0.900852,"Missing"
2021.naacl-main.124,P17-1163,0,0.0464355,"Missing"
2021.naacl-main.124,2020.findings-emnlp.17,0,0.268819,"gather With modeling innovations, increasing computing supervisory data for follow-up research, we propower, and a growing number of datasets, recent pose a Human↔AI collaborative data construction years have witnessed significant improvements in approach that can effectively add suitable chit-chat the performance of both task-oriented dialogue systems and chit-chat systems (Adiwardana et al., to the beginning or end of system responses in 2020; Roller et al., 2020; Hosseini-Asl et al., 2020; existing task-oriented dialogue datasets. Specifically, we first generate chit-chat candidates for augPeng et al., 2020a). Most research on dialogue mentation using off-the-shelf pre-trained language ∗ Work done as a research intern at Facebook. The models and open-domain chatbots (Section 2.1). code and data are available at https://github.com/ facebookresearch/accentor. Next, we automatically filter out candidates that 1570 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1570–1583 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 Data Construction (Pilot Labels) User ... ... System ... ... ."
2021.naacl-main.124,P19-1534,0,0.0985687,"ns facilitate open-domain chatbot development with large amounts of human-created text data generated Approach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very few dialogue corpora contain exof “code-separation” has practical merits: we can plicit annotations for both task-oriented and chiteasily"
2021.naacl-main.124,2020.acl-main.222,1,0.769513,"is a bad candidate, which enables the model to potentially generate a suitable chit-chat augmented response even if the output of the offthe-shelf chit-chat model is not good. 3.3 Implementation Details Unless specified otherwise, for causal language models, we use the 12-layer GPT-2 (117M parameters) as the pre-trained language model (Radford et al., 2019) and fine-tune for ten epochs. We set the batch size to 36 and the learning rate to 1 × 10−3 . We employ the SimpleTOD baseline as the off-the-shelf task-oriented dialogue model for Arranger and Rewriter. We fine-tune a 90M parameter model (Shuster et al., 2020) on each of the good chit-chat candidates with the associated dialogue history as the context from the training set of ACCENTOR-SGD following hyperparameters employed by Roller et al. (2020) and employ the resulting model as the off-the-shelf chit-chat model in Arranger and Rewriter. We use RoBERTaBASE (Liu et al., 2019) as the pre-trained language model for Arranger and fine-tune for three epochs with a learning rate of 2 × 10−5 and a batch size of 24. Arranger. This model arranges the output of an off-the-shelf task-oriented dialogue model and an off-the-shelf chit-chat model without interve"
2021.naacl-main.124,2020.acl-main.183,0,0.0688776,"oach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very few dialogue corpora contain exof “code-separation” has practical merits: we can plicit annotations for both task-oriented and chiteasily extend the proposed approach to an exist- chat utterances. For example, task-oriented diaing production-l"
2021.naacl-main.124,W14-4343,1,0.820832,"ls for ACCENTOR. Evaluation results show that compared with the baseline trained on the original unaugmented data, our proposed models trained on the chit-chat augmented counterpart achieve a similar task performance level and higher human evaluation scores. Task-Oriented Dialogue Systems 6 Conclusion Over the past few years, neural models have achieved remarkable success in the development of the main components of task-oriented dialogue systems, including understanding user intent, tracking dialogue states, determining system actions, and generating system responses (Henderson et al., 2013; Sun et al., 2014; Wen et al., 2015; Liu and Lane, 2016; Mrkši´c et al., 2017; Wen et al., 2017; Nouri and Hosseini-Asl, 2018; Heck et al., 2020; Chen et al., 2020). Recently, connecting separate components and building end-to-end taskoriented neural dialogue systems have attracted increasing interest (Bordes et al., 2017; Peng et al., 2020b). The most recent thread is to unify all com- Acknowledgements ponents in a single end-to-end neural model by fine-tuning a pre-trained deep language model on We thank Gerald Demeunynck for helping with multiple tasks, which leads to state-of-the-art per- the data annotati"
2021.naacl-main.124,P19-1566,0,0.0136967,"data generated Approach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very few dialogue corpora contain exof “code-separation” has practical merits: we can plicit annotations for both task-oriented and chiteasily extend the proposed approach to an exist- chat utterances. For example, task-oriente"
2021.naacl-main.124,D15-1199,0,0.0356499,"Missing"
2021.naacl-main.124,W13-4065,0,0.0479335,"Missing"
2021.naacl-main.124,W16-3649,0,0.0262186,"r neither of the following justifications for a good candidate: We ask annotators (crowd workers) to label each • Social: The candidate keeps the conversation of the remaining candidates from Section 2.2 as flowing smoothly by appropriately switching good or bad. Additionally, to guide the annotation to relevant topics, asking casual follow up process, improve the potential quality, and facilquestions, or engaging in social pleasantries. 1572 The design of this subcategory is inspired by the line of research that studies different social and discourse strategies in chit-chat dialogue systems (Yu et al., 2016). Task-Oriented Response Candidate ?? , ?? [?? ; ?ሚ? ?? ] [?? ; ?? ?ሚ? ] Task Bot Dialogue Context RoBERTa [?? ; ?? ] Arranger ?? actions; system response • Useful: The candidate enhances the conversation by appropriately offering opinions, commentaries, or pertinent and truthful information. Truthfulness should be established by conversational context or real world knowledge. To reduce annotation workload, if annotators have to use external resources (e.g., Wikipedia, search engines, maps) to verify information, they are instructed to label the candidate as misleading instead. The design of t"
2021.naacl-main.124,P18-1205,0,0.160005,"Shah et al., 2018; Budzianowski et al., 2018; Rastogi et al., 2020). Another line of work seeks to 4.3 Limitations and Further Discussions facilitate open-domain chatbot development with large amounts of human-created text data generated Approach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very"
2021.naacl-main.124,W17-5505,0,0.0180856,"es. For example, task-oriented diaing production-level virtual assistant system as a logue corpora constructed by Rastogi et al. (2020) modularized solution, and it has minimal interfer- and Moon et al. (2020) contain annotations for a ence to the user-perceived task success rate, a core few chit-chat dialogue acts, but they are limited to metric widely adapted in virtual assistant systems. light social greetings (e.g., “Thank you!”, “Good Another limitation of our work is that we only aug- Bye.”) typically at the end of each dialogue sesment responses on the system side in our dataset, sion. Zhao et al. (2017) propose to artificially augand the augmentations are independent of each ment task-oriented dialogues with randomly samother, whereas in real-life situations, users are also pled utterances from a chit-chat corpus, mainly to likely to make chit-chat, and the chit-chat between improve the out-of-domain recovery performance. the user and the system should ideally be related to Akasaki and Kaji (2017) annotate user utterances each other. We leave for future research addressing with chat/non-chat binary labels. Still, they do these limitations. not study the contextual combination of these two 15"
2021.naacl-main.70,D13-1185,0,0.0231802,"er layer to predict whether the span represents certain role-filler entity and what the role is. SEQ TAGGING is a BERT-based sequence tagging model for extracting the role-fillers entities. A role-filler entity can appear in templates of different event types (e.g., “Zarate armed force” appear in both attack and bombing event). For both baselines, the prediction goal is multi-class classification. More specially, we adapt the DYGIE++ output layer implementation to first predict the role-filler entity’s role class, and then predicts its event classes conditioned on the entity’s role. Note that Chambers (2013) and Cheung et al. (2013) propose to do event schema induction with unsupervised learning. Given their unsupervised nature, empirically the performance is worse than supervised models (Patwardhan and Riloff, 2009). Thus we do not add these as comparisons. Models P R F1 G RIT- PIPELINE DY GIE++ (Wadden et al., 2019) SEQ TAGGING (Du and Cardie, 2020) 63.88 37.56 47.31 61.90 36.33 45.79 46.80 38.30 42.13 G TT 61.69 42.36 50.23∗ Table 2: Micro-average results on the full test set. 5 Results and Analysis Results on the full test set are shown in Table 2. We report the micro-average performance (pre"
2021.naacl-main.70,N13-1104,0,0.0319767,"whether the span represents certain role-filler entity and what the role is. SEQ TAGGING is a BERT-based sequence tagging model for extracting the role-fillers entities. A role-filler entity can appear in templates of different event types (e.g., “Zarate armed force” appear in both attack and bombing event). For both baselines, the prediction goal is multi-class classification. More specially, we adapt the DYGIE++ output layer implementation to first predict the role-filler entity’s role class, and then predicts its event classes conditioned on the entity’s role. Note that Chambers (2013) and Cheung et al. (2013) propose to do event schema induction with unsupervised learning. Given their unsupervised nature, empirically the performance is worse than supervised models (Patwardhan and Riloff, 2009). Thus we do not add these as comparisons. Models P R F1 G RIT- PIPELINE DY GIE++ (Wadden et al., 2019) SEQ TAGGING (Du and Cardie, 2020) 63.88 37.56 47.31 61.90 36.33 45.79 46.80 38.30 42.13 G TT 61.69 42.36 50.23∗ Table 2: Micro-average results on the full test set. 5 Results and Analysis Results on the full test set are shown in Table 2. We report the micro-average performance (precision, recall and F1). W"
2021.naacl-main.70,N19-1423,0,0.0226918,". For the < Role-filler Entities > of template i, following Du et al. (2020), we use the concatenation of target entity extractions for each role, separated by the separator token ([SEP]). Each entity is represented with its first mention’s beginning (b) and end (e) tokens: e11b , e11e , .. [SEP] e21b , e21e , .. [SEP] e31b , e31e , .. 3.2 Base Model and Decoding Constraints Next we describe the base model as well as special decoding constraints for template filling. BERT as Encoder and Decoder Our model extends upon the G RIT model for REE (Du et al., 2020). The base setup utilizes one BERT (Devlin et al., 2019) model for processing both the source and target tokens embeddings. To distinguish the encoder / decoder representations, it uses partial causal attention mask on the decoder side (Du et al., 2020). The joint sequence of source tokens’ embeddings (a0 , a1 , ..., am ) and target tokens’ embeddings (b0 , b1 , ..., bn ) are passed through BERT to obtain their contextualized representations, ˆ 0 ..., b ˆl ˆ a0 , ˆ a1 , ..., ˆ alsrc , b tgt = BERT(a0 , b1 , ..., alsrc , b0 , ..., bltgt ) Pointer Decoding For the final decoder layer, we replace word prediction with a simple pointer selection mechani"
2021.naacl-main.70,doddington-etal-2004-automatic,0,0.049614,"with a single end-to-end model. Introduction The classic template-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Rilof"
2021.naacl-main.70,2020.acl-main.714,1,0.884155,"Missing"
2021.naacl-main.70,X96-1047,0,0.302174,"eapon bomb Victim - Event 3 Template Arson Perpetrator Indiv. - Perpetrator Org Zarate armed forces Physical Target old shack Weapon - Victim - Figure 1: The template-filling task. Role-filler entity extraction is shown on the left, and template recognition is shown on the right. Our system performs both of these document-level tasks with a single end-to-end model. Introduction The classic template-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model betw"
2021.naacl-main.70,P11-1114,0,0.0287961,"n et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Riloff, 2011, 2012; Du edge, this is the first attempt to build an end-to-end et al., 2020) assume that there is one generic tem- learning framework for this task. We build our plate and focus only on role-filler entity extraction. framework upon G RIT (Du et al., 2020), which However, real documents often describe multiple tackles role-filler entity extraction (REE), but not events (Figure 1). From the example, we can ob- template/event recognition. G RIT performs REE serve that between-event dependencies are impor- by “generating” a sequence of role-filler entities, tant (e.g., a single organization can"
2021.naacl-main.70,2020.acl-main.713,0,0.0315927,"rpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Riloff, 2011, 2012; Du edge, this is the first attempt to build an end-to-end et al., 2020) assume that there is one generic tem- learning framework for this task. We build our plate and focus"
2021.naacl-main.70,D09-1016,0,0.10042,"Missing"
2021.naacl-main.70,D19-1585,0,0.102639,"Physical Target old shack Weapon - Victim - Figure 1: The template-filling task. Role-filler entity extraction is shown on the left, and template recognition is shown on the right. Our system performs both of these document-level tasks with a single end-to-end model. Introduction The classic template-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a docume"
2021.naacl-main.70,N16-1033,0,0.012825,"-filling task in information ex- shared perpetrator organization). Alternative endto-end event extraction models, even those incorpotraction involves extracting event-based templates from documents (Grishman and Sundheim, 1996; rating pretrained LM representations, only model events in isolation (Wadden et al., 2019; Du and Jurafsky and Martin, 2009; Grishman, 2019). It is usually tackled by a pipeline of two separate sys- Cardie, 2020), and are mainly evaluated on ACEtems, one for role-filler entity extraction – extract- style (Doddington et al., 2004) event extraction from single sentences (Yang and Mitchell, 2016; ing event-relevant entities (e.g., noun phrases) from the document; another for template/event recogni- Lin et al., 2020). tion – assigning each of the candidate role-fillers to To naturally model between-event dependencies the event(s)/template(s) that it participates in and across a document for template filling, we proidentifying the type of each event/template. pose a framework called “G TT” based on generSimplifications of the task (Patwardhan and ative transformers (Figure 2). To our best knowlRiloff, 2009; Huang and Riloff, 2011, 2012; Du edge, this is the first attempt to build an en"
A00-1025,P98-1034,1,0.750103,"Missing"
A00-1025,J93-2004,0,0.0450067,"Missing"
A00-1025,W99-0212,0,0.0196131,"dentifying named entities. Finally, the Cymphony QA system (Srihari and Li, 2000) relies heavily on named entity identification; it also employs standard IR techniques and a shallow parser. In terms of statistical and linguistic knowledge sources employed, the primary difference between these systems and ours is our lack of an adequate named entity tagger. Incorporation of such a tagger will be a focus of future work. In addition, we believe that the retrieval and summarization components can be improved by incorporating automatic relevance feedback (Buckley, 1995) and coreference resolution. Morton (1999), for example, shows that coreference resolution improves passage retrieval for their question-answering system. We also plan to reconsider paragraph-based summaries given their coverage on the test corpus. The most critical area for improvement, however, is the linguistic filters. The semantic type filter will be greatly improved by the addition of a named entity tagger, but we believe that additional gains can be attained by augmenting named entity identification with information from WordNet. Finally, we currently make no attempt to confirm any phrase relations from the query. Without this,"
A00-1025,A00-1023,0,0.0840042,"e AT&T QA system (Singhal et al., 2000), the Qanda system (Breck et al., 2000), and the SyncMatcher system (Oard et al., 2000) all employ vector-space methods from IR, named entity identifiers, and a fairly simple question type determiner. In addition, SyncMatcher uses a broad-coverage dependency parser to enforce phrase relationship constraints. Instead of the vector space model, the LASSO system (Moldovan et al., 2000) uses boolean search operators for paragraph retrieval. Recognition of answer hypotheses in their system relies on identifying named entities. Finally, the Cymphony QA system (Srihari and Li, 2000) relies heavily on named entity identification; it also employs standard IR techniques and a shallow parser. In terms of statistical and linguistic knowledge sources employed, the primary difference between these systems and ours is our lack of an adequate named entity tagger. Incorporation of such a tagger will be a focus of future work. In addition, we believe that the retrieval and summarization components can be improved by incorporating automatic relevance feedback (Buckley, 1995) and coreference resolution. Morton (1999), for example, shows that coreference resolution improves passage re"
A00-1025,E99-1011,0,\N,Missing
A00-1025,C98-1034,1,\N,Missing
A00-1025,X98-1017,1,\N,Missing
C02-1139,M95-1005,0,\N,Missing
C02-1139,N01-1008,0,\N,Missing
C02-1139,C96-1021,0,\N,Missing
C02-1139,J00-4005,0,\N,Missing
C02-1139,P99-1048,0,\N,Missing
C02-1139,P98-1012,0,\N,Missing
C02-1139,C98-1012,0,\N,Missing
C02-1139,J94-4002,0,\N,Missing
C02-1139,P95-1017,0,\N,Missing
C02-1139,P02-1014,1,\N,Missing
C02-1139,J01-4004,0,\N,Missing
C04-1018,J93-3001,0,0.0391377,"respect to how closely syntax reflects their hierarchical structure. It may behoove us to add features to allow the model to take this into account. Other sources of error include erroneous sentence boundary detection, parenthetical statements (which the parser does not treat correctly for our purposes) and other parse errors, partial quotations, as well as some errors in the annotation. Examining the learned trees is difficult because of their size, but looking at one tree to depth three 14 p < 0.01, using an approximate randomization test with 9,999 trials. See (Eisner, 1996, page 17) and (Chinchor et al., 1993, pages 430-433) for descriptions of this method. 15 Using the same test as above, p < 0.01, except for the performance on sentences with more than 5 pse’s, because of the small amount of data, where p < 0.02. reveals a fairly intuitive model. Ignoring the probabilities, the tree decides pseparent is the parent of psetarget if and only if pseparent is the writer’s pse (and psetarget is not in quotation marks), or if pseparent is the word “said.” For all the trees learned, the root feature was either the writer pse test or the partial-parse-based domination feature. 6 Conclusions and Future Wor"
C04-1018,J02-3001,0,0.0135854,"Missing"
C04-1018,J94-2004,0,0.1202,", newswire does not offer direct access to facts, events, and opinions; rather, journalists report what they have experienced, and report on the experiences of others. That is, facts, events, and opinions are filtered by the point of view of the writer and other sources. Unfortunately, this filtering of information through multiple sources (and multiple points of view) complicates the natural language interpretation process because the reader (human or machine) must take into account the biases introduced by this indirection. It is important for understanding both newswire and narrative text (Wiebe, 1994), therefore, to appropriately recognize expressions of point of view, and to associate them with their direct and indirect sources. This paper introduces two kinds of expression that can filter information. First, we define a perspective expression to be the minimal span of text that denotes the presence of an explicit opinion, evaluation, emotion, speculation, belief, sentiment, etc.1 Private state is the general term typically used 1 Note that implicit expressions of perspective, i.e. Wiebe et to refer to these mental and emotional states that cannot be directly observed or verified (Quirk e"
C04-1018,H01-1014,0,0.0260875,"cross-validation). Although the NRRC corpus provides annotations for all pse’s, it does not provide annotations to denote directly their hierarchical structure within a 8 GATE’s sentences sometimes extend across paragraph boundaries, which seems never to be warranted. Inaccurately joining sentences has the effect of adding more noise to our problem, so we split GATE’s sentences at paragraph boundaries, and introduce writer pse’s for the newly created sentences. 9 We convert the parse to a dependency format that makes some of our features simpler using a method similar to the one described in Xia and Palmer (2001). We also employ a method from Adam Lopez at the University of Maryland to find grammatical relationships between words (subject, object, etc.). 10 The original corpus is available at http: //nrrc.mitre.org/NRRC/Docs_Data/MPQA_ 04/approval_mpqa.htm. Code and data used in our experiments are available at http://www.cs.cornell. edu/˜ebreck/breck04playing/. sentence. This structure must be extracted from an attribute of each pse annotation, which lists the pse’s direct and indirect sources. For example, the “source chain” for “unhappy” in sentence 1, would be (writer, Alice, Bob). The source chai"
C04-1018,J03-4003,0,\N,Missing
C08-1103,H05-1045,1,0.642569,"ial topics, each identified with its own topic span. Without more context, however, it is impossible to know which phrase indicates the intended topic. If followed by sentence 3, however, 2 As previously mentioned, there has been much recent progress in extracting fine-grained subjectivity information from general text. Previous efforts have focused on the extraction of opinion expressions in context (e.g. Bethard et al. (2004), Breck et al. (2007)), the assignment of polarity to these expressions (e.g. Wilson et al. (2005), Kim and Hovy (2006)), source extraction (e.g. Bethard et al. (2004), Choi et al. (2005)), and identification of the source-expresses-opinion relation (e.g. Choi et al. (2006)), i.e. linking sources to the opinions that they express. Not surprisingly, progress has been driven by the creation of language resources. In this regard, Wiebe et al.’s (2005) opinion annotation scheme for subjective expressions was used to create the MPQA corpus, which consists of 535 documents manually annotated for phrase-level expressions of opinions, their sources, polarities, and intensities. Although other opinion corpora exist (e.g. Bethard et al. (2004), Voorhees and Buckland (2003), the product"
C08-1103,W06-1651,1,0.439668,"is impossible to know which phrase indicates the intended topic. If followed by sentence 3, however, 2 As previously mentioned, there has been much recent progress in extracting fine-grained subjectivity information from general text. Previous efforts have focused on the extraction of opinion expressions in context (e.g. Bethard et al. (2004), Breck et al. (2007)), the assignment of polarity to these expressions (e.g. Wilson et al. (2005), Kim and Hovy (2006)), source extraction (e.g. Bethard et al. (2004), Choi et al. (2005)), and identification of the source-expresses-opinion relation (e.g. Choi et al. (2006)), i.e. linking sources to the opinions that they express. Not surprisingly, progress has been driven by the creation of language resources. In this regard, Wiebe et al.’s (2005) opinion annotation scheme for subjective expressions was used to create the MPQA corpus, which consists of 535 documents manually annotated for phrase-level expressions of opinions, their sources, polarities, and intensities. Although other opinion corpora exist (e.g. Bethard et al. (2004), Voorhees and Buckland (2003), the product review corpora of Liu4 ), we are not aware of any corpus that rivals the scale and dept"
C08-1103,A00-2004,0,0.284771,"by the lack of resources with manually annotated targets, Kim and Hovy could provide only a limited evaluation. As we have defined it, opinion topic identification bears some resemblance to topic segmentation, the goal of which is to partition a text into a linear sequence of topically coherent segments. Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)). Opinion topic identification differs from topic segmentation in that opinion topics are not necessarily spatially coherent — there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic. Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation. Other work has successfully adopted the use of clustering to discover entity relations by identifying entities that appear in t"
C08-1103,P04-1053,0,0.0221923,"differs from topic segmentation in that opinion topics are not necessarily spatially coherent — there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic. Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation. Other work has successfully adopted the use of clustering to discover entity relations by identifying entities that appear in the same sentence and clustering the intervening context (e.g. Hasegawa et al. (2004), Rosenfeld and Feldman (2007)). This work, however, considers named entities and heads of proper noun phrases rather than topic spans, and the relations learned are those commonly held between NPs (e.g. senator-of-state, city-of-state, chairman-of-organization) rather than a more general coreference relation. 4 A Coreference Approach to Topic Identification Given our initial definition of opinion topics (Section 2), the next task is to determine which computational approaches might be employed for automatic opinion topic identification. We begin this exercise by considering some of the proble"
C08-1103,W06-0301,0,0.676456,"thin the single target span of the opinion, there are multiple potential topics, each identified with its own topic span. Without more context, however, it is impossible to know which phrase indicates the intended topic. If followed by sentence 3, however, 2 As previously mentioned, there has been much recent progress in extracting fine-grained subjectivity information from general text. Previous efforts have focused on the extraction of opinion expressions in context (e.g. Bethard et al. (2004), Breck et al. (2007)), the assignment of polarity to these expressions (e.g. Wilson et al. (2005), Kim and Hovy (2006)), source extraction (e.g. Bethard et al. (2004), Choi et al. (2005)), and identification of the source-expresses-opinion relation (e.g. Choi et al. (2006)), i.e. linking sources to the opinions that they express. Not surprisingly, progress has been driven by the creation of language resources. In this regard, Wiebe et al.’s (2005) opinion annotation scheme for subjective expressions was used to create the MPQA corpus, which consists of 535 documents manually annotated for phrase-level expressions of opinions, their sources, polarities, and intensities. Although other opinion corpora exist (e."
C08-1103,H05-1004,0,0.0173517,"Missing"
C08-1103,P06-1004,0,0.0193905,"f resources with manually annotated targets, Kim and Hovy could provide only a limited evaluation. As we have defined it, opinion topic identification bears some resemblance to topic segmentation, the goal of which is to partition a text into a linear sequence of topically coherent segments. Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)). Opinion topic identification differs from topic segmentation in that opinion topics are not necessarily spatially coherent — there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic. Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation. Other work has successfully adopted the use of clustering to discover entity relations by identifying entities that appear in the same sentence and clustering"
C08-1103,P02-1014,1,0.280611,") find the clusters of coreferent opinions, and (2) label the clusters with the name of the topic. In this paper we focus only on the first task, topic coreference resolution — the most critical step for topic identification. We conjecture that the second step can be performed through frequency analysis of the terms in each of the clusters and leave it for future work. Topic coreference resolution resembles another well-known problem in NLP — noun phrase (NP) coreference resolution. Therefore, we adapt a standard machine learning-based approach to NP coreference resolution (Soon et al., 2001; Ng and Cardie, 2002) for our purposes. Our adaptation has three steps: (i) identify the topic spans; (ii) perform pairwise classification of the associated opinions as to whether or not they are topic-coreferent; and, (iii) cluster the opinions according to the results of (ii). Each step is discussed in more detail below. 6.1 Identifying Topic Spans Decisions about topic coreference should depend on the text spans that express the topic. Ideally, we would be able to recover the topic span of each opinion and use its content for the topic coreference decision. However, the topic span depends on the topic itself, s"
C08-1103,W02-1011,0,0.0196546,"ual annotation of opinion topics and use it to annotate topic information for a portion of an existing general-purpose opinion corpus. In experiments using the corpus, our topic identification approach statistically significantly outperforms several non-trivial baselines according to three evaluation measures. 1 Introduction Subjectivity analysis is concerned with extracting information about attitudes, beliefs, emotions, opinions, evaluations, sentiment and other private states expressed in texts. In contrast to the problem of identifying subjectivity or sentiment at the document level (e.g. Pang et al. (2002), Turney (2002)), we are interested in fine-grained subjectivity analysis, which is concerned with subjectivity at the phrase or clause level. We expect fine-grained subjectivity analysis to be useful for question-answering, summarization, information extraction and search engine support for queries of c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. the form “How/what does entity X feel/think about topic Y?”, for which document-level opinion analysis methods can be"
C08-1103,passonneau-2004-computing,0,0.0615131,"Missing"
C08-1103,H05-1043,0,0.577529,"minimal span of text that mentions the topic. – Target span. In contrast, we use TARGET SPAN to denote the span of text that covers the syntactic 3 For simplicity, we will use the term opinion throughout the paper to cover all types of private states expressed in subjective language. (3)Although he doesn’t like government-imposed taxes, he thinks that a fuel tax is the only effective solution. the topic of Al’s opinion in 2 is much clearer — it is likely to be fuel tax, denoted via the TOPIC SPAN “tax gas” or “tax”. 3 818 Related Work 4 http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html Popescu and Etzioni (2005), Hu and Liu (2004)). For this specialized text genre, it has been sufficient to limit the notion of topic to mentions of product names and components and their attributes. Thus, topic extraction has been effectively substituted with a lexicon look-up and techniques have focused on how to learn or acquire an appropriate lexicon for the task. While the techniques have been very successful for this genre of text, they have not been applied outside the product reviews domain. Further, there are analyses (Wiebe et al., 2005) and experiments (Wilson et al., 2005) that indicate that lexicon-lookup a"
C08-1103,J01-4004,0,0.0310922,"or each document (1) find the clusters of coreferent opinions, and (2) label the clusters with the name of the topic. In this paper we focus only on the first task, topic coreference resolution — the most critical step for topic identification. We conjecture that the second step can be performed through frequency analysis of the terms in each of the clusters and leave it for future work. Topic coreference resolution resembles another well-known problem in NLP — noun phrase (NP) coreference resolution. Therefore, we adapt a standard machine learning-based approach to NP coreference resolution (Soon et al., 2001; Ng and Cardie, 2002) for our purposes. Our adaptation has three steps: (i) identify the topic spans; (ii) perform pairwise classification of the associated opinions as to whether or not they are topic-coreferent; and, (iii) cluster the opinions according to the results of (ii). Each step is discussed in more detail below. 6.1 Identifying Topic Spans Decisions about topic coreference should depend on the text spans that express the topic. Ideally, we would be able to recover the topic span of each opinion and use its content for the topic coreference decision. However, the topic span depends"
C08-1103,stoyanov-cardie-2008-annotating,1,0.48094,"dure is described in a set of instructions available at http://www.cs.cornell.edu/˜ves. In addition, we created a GUI that facilitates the annotation procedure. With the help of these resources, one person annotated opinion topics for a randomly selected set of 150 of the 535 documents in the MPQA corpus. In addition, 20 of the 150 documents were selected at random and annotated by a second annotator for the purposes of an inter-annotator agreement study, the results of which are presented in Section 8.1. The MPQATOPIC and the procedure by which it was created are described in more detail in (Stoyanov and Cardie, 2008). 6 The Topic Coreference Algorithm As mentioned in Section 4, our computational approach to opinion topic identification is based on topic coreference: For each document (1) find the clusters of coreferent opinions, and (2) label the clusters with the name of the topic. In this paper we focus only on the first task, topic coreference resolution — the most critical step for topic identification. We conjecture that the second step can be performed through frequency analysis of the terms in each of the clusters and leave it for future work. Topic coreference resolution resembles another well-kno"
C08-1103,P02-1053,0,0.0053661,"inion topics and use it to annotate topic information for a portion of an existing general-purpose opinion corpus. In experiments using the corpus, our topic identification approach statistically significantly outperforms several non-trivial baselines according to three evaluation measures. 1 Introduction Subjectivity analysis is concerned with extracting information about attitudes, beliefs, emotions, opinions, evaluations, sentiment and other private states expressed in texts. In contrast to the problem of identifying subjectivity or sentiment at the document level (e.g. Pang et al. (2002), Turney (2002)), we are interested in fine-grained subjectivity analysis, which is concerned with subjectivity at the phrase or clause level. We expect fine-grained subjectivity analysis to be useful for question-answering, summarization, information extraction and search engine support for queries of c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. the form “How/what does entity X feel/think about topic Y?”, for which document-level opinion analysis methods can be problematic. Fi"
C08-1103,M95-1005,0,0.00952949,"n the response/key cluster. CEAF. As a representative of another group of coreference measures that rely on mapping response clusters to key clusters, we selected Luo’s (2005) CEAF score (short for Constrained EntityAlignment F-Measure). Similar to the ACE (2005) score, CEAF operates by computing an optimal mapping of response clusters to key clusters and assessing the goodness of the match of each of the mapped clusters. Krippendorff’s α. Finally, we use Passonneau’s (2004) generalization of Krippendorff’s (1980) α — a standard metric employed for inter-annotator 7 The MUC scoring algorithm (Vilain et al., 1995) was omitted because it led to an unjustifiably high MUC F-score (.920) for the ONE TOPIC baseline. 822 All opinions Sentiment opinions Strong opinions B3 .6424 .7180 .7374 α .5476 .7285 .7669 CEAF .6904 .7967 .8217 One topic One opinion per cluster Same paragraph Choi Table 1: Inter-annotator agreement results. Sentence Rule-based reliability studies. Krippendorff’s α is based on a probabilistic interpretation of the agreement of coders as compared to agreement by chance. While Passonneau’s innovation makes it possible to apply Krippendorff’s α to coreference clusters, the probabilistic inter"
C08-1103,H05-1044,0,0.573222,"ation is difficult: within the single target span of the opinion, there are multiple potential topics, each identified with its own topic span. Without more context, however, it is impossible to know which phrase indicates the intended topic. If followed by sentence 3, however, 2 As previously mentioned, there has been much recent progress in extracting fine-grained subjectivity information from general text. Previous efforts have focused on the extraction of opinion expressions in context (e.g. Bethard et al. (2004), Breck et al. (2007)), the assignment of polarity to these expressions (e.g. Wilson et al. (2005), Kim and Hovy (2006)), source extraction (e.g. Bethard et al. (2004), Choi et al. (2005)), and identification of the source-expresses-opinion relation (e.g. Choi et al. (2006)), i.e. linking sources to the opinions that they express. Not surprisingly, progress has been driven by the creation of language resources. In this regard, Wiebe et al.’s (2005) opinion annotation scheme for subjective expressions was used to create the MPQA corpus, which consists of 535 documents manually annotated for phrase-level expressions of opinions, their sources, polarities, and intensities. Although other opin"
C08-1103,H05-2017,0,\N,Missing
C08-2004,H05-1042,0,0.0133662,"es of the minimum-cut framework. 1 Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs — henceforth minimum cuts or min cuts — have been successfully employed in vision, computational biology, and natural language processing. Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005). 2 Method 2.1 Min-cut classification framework c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Binary classification problems are usually approached by considering each classification decision in isolation. More formally, let Xtest = 15 Coling 2008: Companion volume – Posters and Demonstrations, pages 15–18 Manchester, August 2008 {x1 , x2 , . . . , xn } be the test instances, drawn from some universe X, and let C = {c1 , c2 } be the two possible classes. Then, the"
C08-2004,P04-1035,1,0.0626428,"abel-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework. 1 Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs — henceforth minimum cuts or min cuts — have been successfully employed in vision, computational biology, and natural language processing. Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005). 2 Method 2.1 Min-cut classification framework c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Binary classification problems are usually approached by considering each classification decision in isolation. More formally, let Xtest = 15 Coling 2008: Companion volume – Posters and Demonstrations, pages 15–18 Manchester, August 2008 {x1 , x2 ,"
C08-2004,W06-1639,1,0.632297,"edge weights, respectively; but in general, the inclusion of even a relatively small number of negative edge weights makes finding a minimum cut NP-hard (McCormick et al., 2003). To avoid this computational issue, we propose several heuristics that encode disagreement information with non-negative edge weights. We instantiate our approach on a sentiment-polarity classification task — determining whether individual conversational turns in U.S. Congressional floor debates support or oppose some given legislation. Our preliminary results demonstrate promising improvements over the prior work of Thomas et al. (2006), who considered only the use of agreement information in this domain. Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications. The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way. Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it. Here, we report on work in progress that examines several novel heuristics for incorporating"
C14-1157,S12-1051,0,0.0331024,"tion. Given a set of sentences S, a complete graph is constructed with each sentence in S as a node. The weight of each edge (u, v) is their dissimilarity d0 (u, v). Then the distance between any pair of u and v, d(u, v), is defined as the total weight of the shortest path connecting P u and v.3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (1) hsum = u,v∈V,u6=v d(u, v), and (2) hmin = minu,v∈V,u6=v d(u, v). Then we need to define the dissimilarity function d0 (·, ·). There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012). In this work, we experiment with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtf idf (u, v) be the Cosine similarity between u and v, then we have d0Lex (u, v) = 1 − simtf idf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. d0Sem (u, v) = 1 − simSem (v, u), where simSem (v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity meas"
C14-1157,W10-1201,0,0.0303379,"nt with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtf idf (u, v) be the Cosine similarity between u and v, then we have d0Lex (u, v) = 1 − simtf idf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. d0Sem (u, v) = 1 − simSem (v, u), where simSem (v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz et al. (2010) show that estimating the similarity between query and passages by using topic structures can help improve the retrieval performance. As discussed in the topic coverage in Section 3.2, each sentence is represented by its sentence-topic distributions estimated by LDA. For candidate sentence u and v, let their topic distributions be Pu and Pv . Then the dissimilarity between u and v can be defined P P (i) Pv (i) as: d0T opic (u, v) = JSD(Pu ||Pv ) = 21 ( i Pu (i) log2 PPua (i) + i Pv (i) log2 P ) where Pa (i) = 12 (Pu (i) + Pv (i)). a (i) 3.4 Full Objective Function The objective function takes"
C14-1157,P13-1100,0,0.122259,"investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive sta"
C14-1157,P06-1039,0,0.0600458,"Missing"
C14-1157,esuli-sebastiani-2006-sentiwordnet,0,0.0384161,"Missing"
C14-1157,E09-1059,0,0.653922,"model, which is a good thing. In short, I don’t think the music industry in particular will ever enjoy the huge profits of the 90’s. ... Ans6: Please-People in those businesses make millions of dollars as it is!! I don’t think piracy hurts them at all!!! Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared to the other answers. Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints. Unlike those works, we address user generated online data: community QA and blogs. These forums use a substantially less formal language than news articles, and at the same time address a much broader spectrum of topics than product reviews. As a result, they present new challenges for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers1 along with the answers fr"
C14-1157,D08-1098,0,0.0949002,"th an empty set, for each iteration, we add a new sentence so that the current summary achieves the maximum value of the objective function. In addition to the theoretical guarantee, existing work (McDonald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally. 4 Experimental Setup 4.1 Opinion Question Identification We first build a classifier to automatically detect opinion oriented questions in Community QA; questions in the blog dataset are all opinionated. Our opinion question classifier is trained on two opinion question datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as “What can you do to help environment?”, and 317 objective questions. We train a RBF kernel based SVM classifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasets when evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79). 4.2 Datasets Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo! WebscopeT M program,5 which contains"
C14-1157,P11-1052,0,0.347179,"ng pertinent and diverse information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are fo"
C14-1157,C00-1072,0,0.469644,"uniformly outperforms semantic similarity computed with WordNet. Moreover, when measuring the summary diversity, topical representation is marginally better than lexical representation, and both of them beats semantic representation. 2 Related Work Our work falls in the realm of query-focused summarization, where a user asks a question and the system generates a summary of the answers containing pertinent and diverse information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query a"
C14-1157,N03-1020,0,0.310491,"work their business model, which is a good thing. •By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies. Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words). Sentences from separate bullets (•) are partial answers from different users. 7 Note that we aim to compare results with the gold-standard best answers of about 100 words. The evaluation of the 200-word summaries is provided only as an additional data-point. 1666 5.3 Blog Summarization Automatic Evaluation. We use the ROUGE (Lin and Hovy, 2003) software with standard options to automatically evaluate summaries with reference to the human labeled nuggets as those are available for this task. ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigram and skip-bigram separated by up to four words. We use the ranker trained on Yahoo! data to produce relevance ordering, and adopt the system parameters from Section 5.2. Table 5 (left) shows that our system outperforms the best system in TAC’08 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Das"
C14-1157,C08-1063,0,0.183311,"ommunity QA and blog summarization, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and shows that our systems are able to generate summaries of high overall quality and information diversity. 1 Introduction Social media forums, such as social networks, blogs, newsgroups, and community question answering (QA), offer avenues for people to express their opinions as well collect other people’s thoughts on topics as diverse as health, politics and software (Liu et al., 2008). However, digesting the large amount of information in long threads on newsgroups, or even knowing which threads to pay attention to, can be overwhelming. A text-based summary that highlights the diversity of opinions on a given topic can lighten this information overload. In this work, we design a submodular function-based framework for opinion summarization on community question answering and blog data. Question: What is the long term effect of piracy on the music and film industry? Best Answer: Rising costs for movies and music. ... If they sell less, they need to raise the price to make u"
C14-1157,J13-2002,0,0.015992,"able 2 manifests that our ranker outperforms all the other methods. Avg Precision MRR Baseline (Random) 0.1305 0.3403 Baseline (Length) 0.2834 0.4889 JSD 0.4000 0.5909 Ranker (ListNet) 0.5336 0.6496 Table 2: Performance for best answer prediction. Our ranker outperforms the three baselines. 5.2 Community QA Summarization Automatic Evaluation. Since human written abstracts are not available for the Yahoo! Answers dataset, we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality. Intuitively, a smaller JSD implies that the summary covers more of the content in the answer set. Louis and Nenkova (2013) report that JSD has a strong negative correlation (Spearman correlation = −0.737) with the overall summary quality for multi-document summarization (MDS) on news articles and blogs. Our task is similar to MDS. Meanwhile, the average JSD of the best answers in our test set is smaller than that of the other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for the other answers. Also, on the blog task (Section 5.3), the top two systems by JSD also have the top two ROUGE scores (a common metric for summarization evaluation when human-constructed summaries are av"
C14-1157,N13-1108,1,0.831475,"programming framework to select sentences. 3 Submodular Opinion Summarization In this section, we describe how query-focused opinion summarization can be addressed by submodular functions combined with dispersion functions. We first define our problem. Then we introduce the 1661 Basic Features - answer position in all answers/sentence position in blog - length of the answer/sentence - length is less than 5 words Query-Sentence Overlap Features - unigram/bigram TF/TFIDF similarity with query - number of key phrases in the query that appear in the sentence. A model similar to that described in (Luo et al., 2013) was applied to detect key phrases. Sentiment Features - number/portion of sentiment words from a lexicon (Section 3.2) - if contains sentiment words with the same polarity as sentiment words in query Query-Independent Features - unigram/bigram TFIDF similarity with cluster centroid - sumBasic score (Nenkova and Vanderwende, 2005) - number of topic signature words (Lin and Hovy, 2000) - JS divergence with cluster Table 1: Features used for candidate ranking. We use them for ranking answers in both community QA and blogs. components of our objective function (Sections 3.1–3.3). The full objecti"
C14-1157,D10-1007,0,0.0540666,"been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences. 3 Submodular"
C14-1157,E12-1023,0,0.0494933,"se information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structu"
C14-1157,W06-1640,1,0.907069,"In short, I don’t think the music industry in particular will ever enjoy the huge profits of the 90’s. ... Ans6: Please-People in those businesses make millions of dollars as it is!! I don’t think piracy hurts them at all!!! Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared to the other answers. Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints. Unlike those works, we address user generated online data: community QA and blogs. These forums use a substantially less formal language than news articles, and at the same time address a much broader spectrum of topics than product reviews. As a result, they present new challenges for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers1 along with the answers from different users. The question rece"
C14-1157,P10-1078,0,0.0733062,"ainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences. 3 Submodular Opinion Summarization In this section, we describe how query-focused opinion summarization can be addressed by submodular functions combined with dispersion functions. We first define our problem. Then we introduce the 1661 Basic Features - answer position in all answers/sentence position in blog - length of the answer/sentence - length is less than 5 words Query-Sentence Overlap Features - unigram/bigram TF/TFIDF similarity with query - number of"
C14-1157,H05-1044,0,0.0427767,"olarity score encourages the selection of summaries that cover both positive and negative opinions. We categorize each sentence simply by counting the number of polarized words given by our lexicon. A sentence belongs to a positive cluster if it has more positive words than negative ones, and vice versa. If any negator co-occurs with a sentiment word (e.g. within a window of size 5), the sentiment is reversed.2 The polarity clustering P thusP have two p clusters corresponding to positive and negative opinions. The score is defined as p(S) = |S ∩ P |. Our lexicon consists of P ∈P MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed. Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), P we use the following function to measure content coverage of the current summary S: c(S) = v∈V min(cov(v, S), θ · P cov(v, V )), where cov(v, S) = u∈S sim(v, u). We experiment with two types of similarity functions. One is a Cosine TFIDF similarity score. The other is a WordNet-based semantic similarity score between pairwise dependency relations from two"
C14-1157,W03-1017,0,0.0910727,"defined P P (i) Pv (i) as: d0T opic (u, v) = JSD(Pu ||Pv ) = 21 ( i Pu (i) log2 PPua (i) + i Pv (i) log2 P ) where Pa (i) = 12 (Pu (i) + Pv (i)). a (i) 3.4 Full Objective Function The objective function takes the interpolation of the submodular functions and dispersion function: F(S) = r(S) + αt(S) + βa(S) + γp(S) + ηc(S) + δh(S). (1) 2 There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed for polarity clustering in this work. We decide to focus on summarization, and estimate sentence polarity through sentiment word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words. 3 This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5. 1663 The coefficients α, β, γ, η, δ are non-negative real numbers and can be tuned on a development set.4 Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function, and summation preserves monotonicity, non-negativity, and submodularity. Dispersion function h(s) is either hsum or hmin as introduced previously. 3.5 Summary Generation via Greedy Algorithm Generating the summary th"
C14-1157,D12-1036,0,\N,Missing
C98-1034,C92-3150,0,0.0297478,"Missing"
C98-1034,J95-4004,0,0.0354986,"Missing"
C98-1034,A88-1019,0,0.360737,"Missing"
C98-1034,J93-2004,0,0.0610468,"mponent of any partial parser; in addition, information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms; furthermore, the psycholinguistic studies of Gee and Grosjean (1983) indicate that text chunks like base noun phrases play an important role in human language processing. In this work we define base NPs to be simple, nonrecursive noun phrases - - noun phrases that do not contain other noun phrase descendants. The bracketed portions of Figure 1, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Thus, the string the sunny confines of resort towns like Boca Raton and Hot Springs is too complex to be a base NP; instead, it contains four simpler noun phrases, each of which is considered a base NP: the sunny confines, resort towns, Boca Raton, and Hot Springs. Previous empirical research has addressed the problem of base NP identification. Several algorithms identify ""terminological phrases"" - - certain Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: Justeson & Katz (1995) look for repeated phrases; Bourigault (1992) uses a handcrafted noun p"
C98-1034,W93-0306,0,0.488821,"Missing"
cardie-etal-2008-erulemaking,W99-0625,0,\N,Missing
D08-1083,esuli-sebastiani-2006-sentiwordnet,0,0.022428,"Missing"
D08-1083,C04-1200,0,0.245029,"Missing"
D08-1083,P07-1055,0,0.543369,"nd a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1 For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, c Honolulu, October 2008. 2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vect"
D08-1083,S07-1013,0,0.0451713,"Missing"
D08-1083,H05-1044,0,0.745804,"les demonstrate that words or constituents interact with each other to yield the expression-level polarity. And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples. Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features. One approach includes features based on contextual valence shifters1 (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy and Inkpen (2005), Wilson et al. (2005), Shaikh et al. (2007)). Another approach encodes frequent subsentential patterns (e.g., McDonald et al. (2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity. How1 For instance, “never”, “nowhere”, “little”, “most”, “lack”, “scarcely”, “deeply”. 793 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801, c Honolulu, October 2008. 2008 Association for Computational Linguistics ever, both types of approach are based on learning models with a flat bag-of-features: some structural information"
D08-1083,R09-1048,0,\N,Missing
D09-1062,P08-1034,0,0.0170422,"Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper, we propose a novel method based on integer linear programming to adapt an existing polarity lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method considers the relations among words and opinion expressions collectively to derive the most likely polarity of each word for the given domain. Figure 1 depicts the key insight of our approach using a bipartite graph. On the left hand side, each node represents a word, and on the right hand side, each node represents an opinion expression. There is an edge between a word wi and an"
D09-1062,E06-1027,0,0.0292699,"Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the dom"
D09-1062,D08-1083,1,0.400592,"inion expression. There is an edge between a word wi and an opinion expression ej , if the word wi appears in the expression ej . We assume the possible polarity of each expression is one of the following three values: {positive, neutral, negative}, while the possible polarity of each word is one of: {positive, neutral, negative or negator}. Strictly speaking, negator is not a value for polarity, but we include them in our lexicon, because valence shifters or negators have been shown to play an important role for sentiment analysis (e.g., Polanyi and Zaenen (2004), Moilanen and Pulman (2007), Choi and Cardie (2008)). Typically, the ultimate goal of the sentiment analysis task is to determine the expression-level (or sentiment/ document-level) polarities, rather 590 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP than the correct word-level polarities with respect to the domain. Therefore, word-level polarities can be considered as latent information. In this paper, we show how we can improve the word-level polarities of a general-purpose polarity lexicon by utilizing the expression-level polarities, a"
D09-1062,N07-1030,0,0.0110225,"-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable. We evaluate the effect of the adapted lex1 In case of document-level polarity classification, wordto-expression relations correspond to word-to-document relations. 591  − w + w  = w w − w   w  w"
D09-1062,D07-1115,0,0.130016,"on: expression-level polarity classification. The positive results from our experiments encourage further research for lexical resource adaptation techniques. Table 3: Effect of an adapted polarity lexicon on expression-level classification using the Vote & Flip Algorithm Accuracy Avg. Error Distance 70.4 71.2 0.334 0.327 Original Lexicon Adapted Lexicon Table 4: Effect of an adapted polarity lexicon on expression-level classification using CRFs 4 Conclusion Related Work Acknowledgments There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell wheth"
D09-1062,W06-1642,0,0.192408,"ression. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word (intra-expression) relations is most related to techniques that determine expression-level polarity in context (e.g., Wilson et al. (2005)), while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately"
D09-1062,C04-1200,0,0.223894,"t the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Ber"
D09-1062,I05-2011,0,0.18211,"contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with intensity marked as ‘medium’ or higher, and exclude those with annotation confidence marked as ‘uncertain’. To focus our study on the direct influence of the polarity lexicon upon the sentiment classification task, we assume the boundaries of the expressions are given. However, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., Kim and Hovy (2005), Breck et al. (2007)). Performance is reported using 10-fold cross-validation on 400 documents, and a separate 135 documents were used as a development set. For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al. (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). We report the performance in two measures: accuracy for 3-way classification, and average error distance. The reason why we consider average error distance is because classifying a positive class into a negative class is worse than classifying a positive class into a neut"
D09-1062,W02-1011,0,0.0142076,"ity classification task? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with inte"
D09-1062,E09-1077,0,0.240772,"(2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper"
D09-1062,W04-2401,0,0.0184481,"while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable. We evaluate the effect of the adapted lex1 In case of document-level polarity classification, wordto-expression relations correspond to word-to-document relations. 591  −"
D09-1062,P05-1017,0,0.528231,"entiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposi"
D09-1062,H05-1044,0,0.830442,"In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and"
D09-1062,J09-3003,0,0.0127546,"ek for answers for the following questions: Q1 What is the effect of a polarity lexicon on the expression-level polarity classification task? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion"
D09-1062,banea-etal-2008-bootstrapping,0,\N,Missing
D09-1062,J96-1002,0,\N,Missing
D09-1062,P07-1056,0,\N,Missing
D09-1062,R09-1048,0,\N,Missing
D09-1062,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
D10-1102,C08-2004,1,0.788029,"s an opportunity for approaches like ours to have real impact. One can incorporate many types of sentence-level information that cannot be directly incorporated into a flat model. Examples include scores from another sentence-level classifier (e.g., from Nakagawa et. al (2010)) or combining phrase-level polarity scores (e.g., from Choi and Cardie (2008)) for each sentence, or features that describe the position of the sentence in the document. Most prior work on the U.S. Congressional Floor Debates dataset focused on using relationships between speakers such as agreement (Thomas et al., 2006; Bansal et al., 2008), and used a global mincut inference procedure. However, they require all test instances to be known in advance (i.e., their formulations are transductive). Our method is not limited to the transductive setting, and instead exploits a different and complementary structure: the latent explanation (i.e., only some sentences in the speech are indicative of the speaker’s vote). In a sense, the joint feature structure used in our model is the simplest that could be used. Our model makes no explicit structural dependencies between sentences, so the choice of whether to extract each sentence is essen"
D10-1102,N10-1066,0,0.165553,"ce-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtas"
D10-1102,D08-1083,1,0.442571,"that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but they were focused on classifying either phrase-level (Choi and Cardie, 2008) or sentence-level polarity (Nakagawa et al., 2010). 3 Extracting Hidden Explanations In this paper, we take the view that each document has a subset of sentences that best explains its sentiment. Consider the “annotator rationales” generated by human judges for the movie reviews dataset (Zaidan et al., 2007). Each rationale is a text span that was identified to support (or explain) its parent document’s sentiment. Thus, these rationales can be interpreted as (something close to) a ground truth labeling of the explanatory segments. Using a dataset where each document contains only its rational"
D10-1102,W10-2903,0,0.00790005,"affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers h"
D10-1102,P07-1055,0,0.56245,"sentiment classification employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties. First, they typically require gold standard sentence-level annotations (McDonald et al. (2007), Mao and Lebanon (2006)). But the cost of acquiring such labels can be prohibitive. Second, some solutions for incorporating sentencelevel information lack mechanisms for controlling how errors propagate from the subjective sentence identification subtask to the main document classification task (Pang and Lee, 2004). Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document- and sentence-level classification accuracy (McDonald et al., 2007). Optimizing for this compromise, when the real goal is to maximiz"
D10-1102,N10-1120,0,0.0296676,"along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but they were focused on classifying either phrase-level (Choi and Cardie, 2008) or sentence-level polarity (Nakagawa et al., 2010). 3 Extracting Hidden Explanations In this paper, we take the view that each document has a subset of sentences that best explains its sentiment. Consider the “annotator rationales” generated by human judges for the movie reviews dataset (Zaidan et al., 2007). Each rationale is a text span that was identified to support (or explain) its parent document’s sentiment. Thus, these rationales can be interpreted as (something close to) a ground truth labeling of the explanatory segments. Using a dataset where each document contains only its rationales, Algorithm 1 Inference Algorithm for (2) of pred"
D10-1102,P04-1035,0,0.768544,"n Sentiment classification is a well-studied and active research area (Pang and Lee, 2008). One of the main challenges for document-level sentiment categorization is that not every part of the document is equally informative for inferring the sentiment of the whole document. Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task. For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot (Pang and Lee, 2004). In addition, an overall positive review might still include some negative opinions about an actor or the plot. Early research on document-level sentiment classification employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subject"
D10-1102,W02-1011,0,0.0149024,"document. Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task. For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot (Pang and Lee, 2004). In addition, an overall positive review might still include some negative opinions about an actor or the plot. Early research on document-level sentiment classification employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties. First, they typically require gold standard sentence-level annotations (McDonald et al. (2007), Mao and Lebanon (2006)). But the cost of acquiring such labels can be prohibitive. Second, some"
D10-1102,W06-1639,0,0.791512,"S. Congressional floor debates datasets and close with discussion and conclusions. 2 Related Work Pang and Lee (2004) first showed that sentencelevel extraction can improve document-level performance. They used a cascaded approach by first filtering out objective sentences and performing subjectivity extractions using a global min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is les"
D10-1102,H05-1044,0,0.00689855,"9), and making a new guess of the best explanations (Line 7). Yu and Joachims (2009) showed that this alternating procedure for training latent variable structural SVMs is an instance of the CCCP procedure (Yuille and Rangarajan, 2003), and so is guaranteed to converge to a local optimum. For our experiments, we do not train until convergence, but instead use performance on a validation set to choose the halting iteration. Since OP 1 is nonconvex, a good initialization is necessary. To generate the initial explanations, one can use an off-theshelf sentiment classifier such as OpinionFinder2 (Wilson et al., 2005). For some datasets, there exist documents with annotated sentences, which we 2 http://www.cs.pitt.edu/mpqa/ opinionfinderrelease/ can treat either as the ground truth or another (very good) initial guess of the explanatory sentences. 4.3 Feature Representation Like any machine learning approach, we must specify a useful set of features for the ψ vectors described above. We will consider two types of features. Bag-of-words. Perhaps the simplest approach is to define ψ using a bag-of-words feature representation, with one feature corresponding to each word in the active lexicon of the corpus. U"
D10-1102,P10-2062,1,0.878572,"ubtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1 http://projects.yisongyue.com/svmsle/ 1047 A natural approach to avoid the pitfalls associated with cascaded methods is to use joint twolevel models that simultaneously solve the sentencelevel and document-level tasks (e.g., McDonald et al. (2007), Zaidan and Eisner (2008)). Since these models are trained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furtherm"
D10-1102,D08-1004,0,0.00890942,"These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1 http://projects.yisongyue.com/svmsle/ 1047 A natural approach to avoid the pitfalls associated with cascaded methods is to use joint twolevel models that simultaneously solve the sentencelevel and document-level tasks (e.g., McDonald et al. (2007), Zaidan and Eisner (2008)). Since these models are trained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural"
D10-1102,N07-1033,0,0.131835,"lobal min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1 http://projects.yisongyue.com/svmsle/ 1047 A natural approach to avoid the pitfalls associated with cascaded methods is to us"
D11-1016,D10-1115,0,0.221952,"variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. As in Moilianen and Pulman (2007), semantic inference is based on (a small set of) hand-written compositional rules. In contrast, Nakagawa et. al (2010) use a dependency parse tree to guide the learning of compositional effects. Each of the above, however, uses a binary rather than an ordinal sentiment scale. In contrast, our proposed method for phraselevel sentiment analysis is inspired by recent work on distributional approaches to compositionality. In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. The adjective matrices are then applied as functions over the meanings of nouns — via matrix-vector multiplication — to derive the meaning of adjective-noun combinations. Rudolph and Giesbrecht (2010) show theoretically, that multiplicative matrix-space models are a general case of vector-space models and furthermore exhibit desirable properties for semantic analysis: they take into account word order and are algebraically, neurologically and psychologically plau"
D11-1016,D08-1083,1,0.92519,"ke “bad”, produces a phrase (“very bad”) that should be characterized as more negative than the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are,"
D11-1016,W10-3110,0,0.0160237,"stically informed order of composition can give us further performance gains. For example, one can use the output of a dependency parser to guide the order of composition, similar to Nakagawa et al. (2010). Another possibility for improvement is to use the information about the scope of negation. In the current work we assume the scope of negation to be the expression following the negation; in reality, however, determining the scope of negation is a complex linguistic phenomenon (Moilanen and Pulman, 2007). So the proposed model can benefit from identifying the scope of negation, similar to (Councill et al., 2010). Also we plan to consider other ways to initialize the matrix-space model. One interesting direction to explore might be to use non-negative matrix factorization (Lee and Seung, 2001), co-clustering techniques (Dhillon, 2001) to better initialize words that share similar contexts. The other possible direction is to use existing sentiment lexicons and employing a “curriculum learning” strategy (Bengio et al., 2009; Kumar et al., 2010) for our learning problem. 181 Acknowledgments This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS-0968450; and by"
D11-1016,P10-1018,0,0.0596983,"Missing"
D11-1016,W06-3808,0,0.0427446,"learn how to model the interactions between words with headmodifier relations in the dependency tree. Some of the previous work looked at MPQA phrase-level classification. Wilson et al. (2004) tackles the problem of classifying clauses according to 180 their subjective strength but not polarity; Wilson et al. (2005) classifies phrases according to their polarity/sentiment but not strength. Our task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words. Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define composition as an additive or multiplicative function of two vectors and show that compositional approaches generally outperform no"
D11-1016,P97-1023,0,0.0247733,"gets right, but the bag-of-words model misses in the case of “bad”. Also notice that since in the matrix-space model 9 See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since th"
D11-1016,D09-1017,0,0.043344,"the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are, of course, more interesting examples of compositional semantic effects on sentiment: e.g.,"
D11-1016,D09-1063,0,0.0135156,"ice that since in the matrix-space model 9 See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In our work we propose a sin"
D11-1016,N10-1120,0,0.50094,"rase (“very bad”) that should be characterized as more negative than the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are, of course, more interes"
D11-1016,P05-1015,0,0.0734664,"of its words, they learn how to model the interactions between words with headmodifier relations in the dependency tree. Some of the previous work looked at MPQA phrase-level classification. Wilson et al. (2004) tackles the problem of classifying clauses according to 180 their subjective strength but not polarity; Wilson et al. (2005) classifies phrases according to their polarity/sentiment but not strength. Our task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words. Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define composition as an additive or multiplicative function of two vectors and show that compositional approache"
D11-1016,E09-1077,0,0.0208063,"the case of “bad”. Also notice that since in the matrix-space model 9 See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In ou"
D11-1016,P10-1093,0,0.133167,"or accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model. 1 Introduction Sentiment analysis has been an active research area in recent years. Work in the area ranges from identifying the sentiment of individual words to determining the sentiment of phrases, se"
D11-1016,J11-2001,0,0.115277,"as more negative than the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are, of course, more interesting examples of compositional semantic effe"
D11-1016,N10-1119,0,0.0118961,"atrix-space model 9 See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In our work we propose a single unified model for hand"
D11-1016,H05-1044,0,0.171066,"nt and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Nonetheless, the vast majority of methods for phrase- and sentence-level sentiment analysis do not tackle the task compositionally: they, instead, employ a bag-of-words representation and, at best, incorporate additional features to account for negators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). 173 One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis dec"
D12-1122,W06-1655,0,0.0134277,"on expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs are superior to CRFs for NER and Chinese word segmentation (Sarawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn,"
D12-1122,H05-1045,1,0.791709,"pression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling problem. Unlike previous sequence-labelin"
D12-1122,W06-1651,1,0.944431,"asks that benefit from fine-grained opinion analysis (Wiebe et al., 2005): e.g., it is a first step in characterizing the sentiment and intensity of the opinion; it provides a textual anchor for identifying the opinion holder and the target or topic of an opinion; and these, in turn, form the basis of opinionoriented question answering and opinion summarization systems. In this paper, we focus on opinion expressions as defined in Wiebe et al. (2005) — As a type of information extraction task, opinion expression extraction has been successfully tackled in the past via sequence tagging methods: Choi et al. (2006) and Breck et al. (2007), for example, apply conditional random fields (CRFs) (Lafferty et al., 2001) using sophisticated token-level features. In token-level sequence labeling, labels are assigned to single tokens, and the label of each token depends on the current token and the label of the previous token (we consider the usual first-order assumption). Segment-based features — features that describe a set of related contiguous tokens, e.g., a phrase or constituent — might provide critical information for identifying opinion expressions; they cannot, however, be readily and naturally represen"
D12-1122,P10-2050,1,0.31839,"xtract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and inten1336 sity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs a"
D12-1122,W10-2910,0,0.167769,"., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and inten1336 sity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011); importantly, their reranking approach relied on features that encoded syntactic structure. All of the above approaches, however, are based on token-level sequence labeling, which ignores potentially useful phrase-level information. Semi-CRFs (Sarawagi and Cohen, 2004) are general CRFs that relax the Markovian assumptions to allow sequence labeling at the segment level. Previous work has shown that semi-CRFs are superior to CRFs for NER and Chinese word segmentation (Sarawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression"
D12-1122,P09-2079,0,0.0250255,"006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling prob"
D12-1122,P03-1054,0,0.0145955,"Missing"
D12-1122,W06-0301,0,0.161896,"arawagi and Cohen, 2004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction o"
D12-1122,D07-1114,0,0.0842371,"004; Okanohara et al., 2006; Andrew, 2006). The task of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as"
D12-1122,H05-1068,1,0.820545,"tactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and inten1336 sity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion expressions (Johansso"
D12-1122,W03-1014,0,0.376819,"Missing"
D12-1122,H05-1044,0,0.22668,"enced by probable syntactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and inten1336 sity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion"
D12-1122,H05-2018,1,0.549537,"enced by probable syntactic structure. We also explore the impact of syntactic features for extracting opinion expressions. We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs). Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin. We also identify useful syntactic features for the task. 2 Related Work Previous research to extract direct subjective expressions exists, but is mainly focused on singleword expressions (Wiebe et al., 2005; Wilson et al., 2005; Munson et al., 2005). More recent studies tackle opinion expression extraction at the expression level. Breck et al. (2007) formulate the problem as a token-level sequence labeling problem; their CRF-based approach was shown to significantly outperform two subjectivity-clue-based baselines. Others extend the token-level approach to jointly identify opinion holders (Choi et al., 2006), and to determine the polarity and inten1336 sity of the opinion expressions (Choi and Cardie, 2010). Reranking the output of a simple sequence labeler has been shown to further improve the extraction of opinion"
D12-1122,D09-1159,0,0.0847599,"ask of opinion expression extraction is known to be harder than traditional NER since subjective expressions exhibit substantial lexical variation and their recognition requires more attention to linguistic structure. Parsing has been leveraged to improve performance for numerous natural language tasks. In opinion mining, numerous studies have shown that syntactic parsing features are very helpful for opinion analysis. A lot of work uses syntactic features to identify opinion holders and opinion topics (Bethard et al., 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Joshi and Carolyn, 2009; Wu et al., 2009; Choi et al., 2005). Jakob et al. (2010) recently employed dependency path features for the extraction of opinion targets. Johansson and Moschitti (2010; Johansson and Moschitti (2011) also successfully employed syntactic features that indicate dependency relations between opinion expressions for the task of opinion expression extraction. However, as their approach is based on the output of a sequence labeler, these features cannot be encoded to help the learning of the sequence labeler. 3 Approach We formulate the extraction of opinion expressions as a sequence labeling problem. Unlike previ"
D12-1122,D10-1101,0,\N,Missing
D12-1122,P06-1059,0,\N,Missing
D12-1122,P11-2018,0,\N,Missing
D13-1199,C12-1047,0,0.017771,"Missing"
D13-1199,P13-2039,1,0.67587,"Missing"
D13-1199,P10-1066,0,0.0164922,"Missing"
D13-1199,P11-1032,1,0.846027,"models to generalize better across reviews of different offerings. We evaluate our approach on the task of identifying (ranking) manipulated hotels. In particular, in the absence of gold standard offering-level labels, we introduce a novel evaluation procedure for this task, in which we rank numerous versions of each hotel, where each hotel version contains a different number of injected, known deceptive reviews. Thus, we expect hotel versions with larger proportions of deceptive reviews to be ranked higher than those with smaller proportions. For labeled training data, we use the Ott et al. (2011) dataset of 800 positive (5-star) reviews of 20 Chicago hotels (400 deceptive and 400 truthful). For evaluation, we construct a new FOUR - CITIES dataset, containing 40 deceptive and 40 truthful reviews for each of eight hotels in four different cities (640 reviews total), following the procedure outlined in Ott et al. (2011). We find that our manifold ranking approach outperforms several state-of-theart learning baselines on this task, including transductive Support Vector Regression. We additionally apply our approach to a large-scale collection of real-world reviews from TripAdvisor and exp"
D13-1199,D09-1026,0,0.00798871,". Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic models (Blei et al, 2003) have been employed for many NLP tasks in recent years. Here, we build on earlier work that uses topic models to (a) separate background information from information discussing the various “aspects” of products (e.g., Chemudugunta et al. (2007)) and (b) identify different levels of information (e.g., user-specific, location-specific, timespecific) (Ramage et al., 2009). Manifold Ranking Algorithm. The manifoldranking method (Zhou et al, 2003a; Zhou et al, 2003b) is a mutual reinforcement ranking approach initially proposed to rank data points along their underlying manifold structure. It has been widely used in many different ranking applications, such as summarization (Wan et al, 2007; Wan and Yang, 2007). 3 Dataset In this paper, we train all of our models using the CHICAGO dataset of Ott et al (2011), which contains 20 deceptive and 20 truthful reviews from each of 20 Chicago hotels (800 reviews total). This dataset is 2 Approaches for identifying indivi"
D13-1199,W03-0502,0,0.0248288,"Missing"
D14-1080,P10-2050,1,0.634238,"on expression mining. Recurrent neural networks are presented in 3.1, bidirectionality is introduced in 3.2, and deep bidirectional RNNs, in 3.3. Related Work 3.1 Opinion extraction. Early work on fine-grained opinion extraction focused on recognizing subjective phrases (Wilson et al., 2005; Munson et al., 2005). Breck et al. (2007), for example, formulated the problem as a token-level sequence-labeling problem and apply a CRF-based approach, which significantly outperformed previous baselines. Choi et al. (2005) extended the sequential prediction approach to jointly identify opinion holders; Choi and Cardie (2010) jointly detected polarity and intensity along with the opinion expression. Reranking approaches have also been explored to improve the performance of a single sequence labeler (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). More recent work relaxes the Markovian assumption of CRFs to capture phrase-level interactions, significantly improving upon the token-level labeling approach (Yang and Cardie, 2012). In particular, Yang and Cardie (2013) propose a joint inference model to jointly detect opinion expressions, opinion holders and targets, as well as the relations among them,"
D14-1080,P11-2018,0,0.008983,"cused on recognizing subjective phrases (Wilson et al., 2005; Munson et al., 2005). Breck et al. (2007), for example, formulated the problem as a token-level sequence-labeling problem and apply a CRF-based approach, which significantly outperformed previous baselines. Choi et al. (2005) extended the sequential prediction approach to jointly identify opinion holders; Choi and Cardie (2010) jointly detected polarity and intensity along with the opinion expression. Reranking approaches have also been explored to improve the performance of a single sequence labeler (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). More recent work relaxes the Markovian assumption of CRFs to capture phrase-level interactions, significantly improving upon the token-level labeling approach (Yang and Cardie, 2012). In particular, Yang and Cardie (2013) propose a joint inference model to jointly detect opinion expressions, opinion holders and targets, as well as the relations among them, outperforming previous pipelined approaches. Recurrent Neural Networks A recurrent neural network (Elman, 1990) is a class of neural network that has recurrent connections, which allow a form of memory. This makes them applicable for seque"
D14-1080,H05-1044,0,0.0597508,"other ways of constructing deep RNNs that are orthogonal to the concept of stacking layers on top of each other. In this work, we focus on the stacking notion of depth. 3 Methodology This section describes the architecture and training methods for the deep bidirectional recurrent networks that we propose for the task of opinion expression mining. Recurrent neural networks are presented in 3.1, bidirectionality is introduced in 3.2, and deep bidirectional RNNs, in 3.3. Related Work 3.1 Opinion extraction. Early work on fine-grained opinion extraction focused on recognizing subjective phrases (Wilson et al., 2005; Munson et al., 2005). Breck et al. (2007), for example, formulated the problem as a token-level sequence-labeling problem and apply a CRF-based approach, which significantly outperformed previous baselines. Choi et al. (2005) extended the sequential prediction approach to jointly identify opinion holders; Choi and Cardie (2010) jointly detected polarity and intensity along with the opinion expression. Reranking approaches have also been explored to improve the performance of a single sequence labeler (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). More recent work relaxes the"
D14-1080,D12-1122,1,0.947341,"any opinion-related class. The example sentence in Table 1 shows the appropriate tags in the BIO scheme. For instance, the ESE “as usual” results in the tags B ESE for “as” and I ESE for “usual”. Variants of conditional random field (CRF) approaches have been successfully applied to opinion expression extraction using this token-based view (Choi et al., 2005; Breck et al., 2007): the state-of-the-art approach is the semiCRF, which relaxes the Markovian assumption inherent to CRFs and operates at the phrase level rather than the token level, allowing the incorporation of phrase-level features (Yang and Cardie, 2012). The success of the CRF- and semiCRF-based approaches, however, hinges critically on access to an appropriate feature set, typically based on constituent and dependency parse trees, manually crafted opinion lexicons, named entity taggers and other preprocessing components (see Yang and Cardie (2012) for an up-todate list). Distributed representation learners provide a different approach to learning in which latent features are modeled as distributed dense vectors of hidden layers. A recurrent neural network (RNN) is one such learner that can operate on sequential data of variable length, whic"
D14-1080,P13-1161,1,0.801133,"ntly outperformed previous baselines. Choi et al. (2005) extended the sequential prediction approach to jointly identify opinion holders; Choi and Cardie (2010) jointly detected polarity and intensity along with the opinion expression. Reranking approaches have also been explored to improve the performance of a single sequence labeler (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). More recent work relaxes the Markovian assumption of CRFs to capture phrase-level interactions, significantly improving upon the token-level labeling approach (Yang and Cardie, 2012). In particular, Yang and Cardie (2013) propose a joint inference model to jointly detect opinion expressions, opinion holders and targets, as well as the relations among them, outperforming previous pipelined approaches. Recurrent Neural Networks A recurrent neural network (Elman, 1990) is a class of neural network that has recurrent connections, which allow a form of memory. This makes them applicable for sequential prediction tasks with arbitrary spatiotemporal dimensions. Thus, their structure fits many NLP tasks, when the interpretation of a single sentence is viewed as analyzing a sequence of tokens. In this work, we focus ou"
D14-1080,H05-1068,1,0.542159,"ucting deep RNNs that are orthogonal to the concept of stacking layers on top of each other. In this work, we focus on the stacking notion of depth. 3 Methodology This section describes the architecture and training methods for the deep bidirectional recurrent networks that we propose for the task of opinion expression mining. Recurrent neural networks are presented in 3.1, bidirectionality is introduced in 3.2, and deep bidirectional RNNs, in 3.3. Related Work 3.1 Opinion extraction. Early work on fine-grained opinion extraction focused on recognizing subjective phrases (Wilson et al., 2005; Munson et al., 2005). Breck et al. (2007), for example, formulated the problem as a token-level sequence-labeling problem and apply a CRF-based approach, which significantly outperformed previous baselines. Choi et al. (2005) extended the sequential prediction approach to jointly identify opinion holders; Choi and Cardie (2010) jointly detected polarity and intensity along with the opinion expression. Reranking approaches have also been explored to improve the performance of a single sequence labeler (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). More recent work relaxes the Markovian assumption"
D14-1080,P10-1040,0,0.0460826,"Es) as defined in Wiebe et al. (2005). DSEs consist of explicit mentions of private states or speech events expressing private states; and ESEs consist of expressions that indicate sentiment, emotion, etc., without explicitly conveying them. An example sentence shown in Table 1 in which the DSE “has refused to make any statements” explicitly expresses an opinion holder’s attitude and the 720 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720–728, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics al., 2013; Turian et al., 2010) have enabled more effective training of RNNs by allowing a lower dimensional dense input representation and hence, more compact networks (Mikolov et al., 2010; Mesnil et al., 2013). Finally, deep recurrent networks, a type of RNN with multiple stacked hidden layers, are shown to naturally employ a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013): lower levels capture short term interactions among words; higher layers reflect interpretations aggregated over longer spans of text. When applied to natural language sentences, such hierarchies"
D14-1080,W10-2910,0,\N,Missing
D14-1080,H05-1045,1,\N,Missing
D14-1214,P11-2000,0,0.166133,"Missing"
D14-1214,P11-1040,0,0.0160923,"sonal topic needs to be adequately discussed by the user and their followers in order to be detected16 . Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structu"
D14-1214,D13-1114,0,0.00534075,"roughly constant, but recall increases as more life events and C ONGRATULA TIONS and C ONDOLENCES are discovered. 8 Related Work Our work is related to three lines of NLP researches. (1) user-level information extraction on social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitl"
D14-1214,P07-1030,0,0.0127633,"disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major li"
D14-1214,D13-1192,0,0.0182098,"social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoid"
D14-1214,P12-1056,0,0.222329,"important life events on which algorithms can rely for extraction or classification. Introduction Social networking websites such as Facebook and Twitter have recently challenged mainstream media as the freshest source of information on important news events. In addition to an important source for breaking news, social media presents a unique source of information on private events, for example a friend’s engagement or college graduation (examples are presented in Figure 1). While a significant amount of previous work has investigated event extraction from Twitter (e.g., (Ritter et al., 2012; Diao et al., 2012)), existing approaches mostly focus on public bursty event extraction, and little progress has been made towards the problem of automatically extracting the major life events of ordinary users. A system which can automatically extract major life events and generate fine-grained descriptions as in Figure 1 will not only help Twitter Challenge 2: Noisiness of Twitter Data: The user-generated text found in social media websites such as Twitter is extremely noisy. The language used to describe life events is highly varied and ambiguous and social media users frequently discuss public news and mund"
D14-1214,P11-1055,0,0.00961363,"of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying on minimum efforts of human supervision, and s"
D14-1214,D14-1108,0,0.00709329,"e Stanford PragBank10 , 8 Most tweets in the bootstrapping output are positive. The majority of results returned by Twitter Search are negative examples. 10 http://compprag.christopherpotts.net/ factbank.html 2002 9 an extension of FactBank (Saur´ı and Pustejovsky, 2009) which contains a list of modal words such as “might”, “will”, “want to” etc11 . • I: Whether the subject of the tweet is first person singular. • Dependency: If the subject is first person singular and the u is a verb, the dependency path between the subject and u (or nondependency). Tweet dependency paths were obtained from (Kong et al., 2014). As the tweet parser we use only supports one-to-one dependency path identification but no dependency properties, Dependency is a binary feature. The subject of each tweet is determined by the dependency link to the root of the tweet from the parser. Among the features we explore, Word encodes the general information within the tweet. Window addresses the information around topic key word. The rest of the features specifically address each of the negative situations described in Challenge 2, Section 1: Tense captures past event description, Factuality filters out wishes or imagination, I and"
D14-1214,P10-1150,1,0.666806,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,N10-1087,1,0.304045,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,P08-1119,1,0.214259,"potting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a rel"
D14-1214,P14-1016,1,0.167411,"ic event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of"
D14-1214,D11-1024,0,0.00393037,"by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics"
D14-1214,P09-1113,0,0.0027458,"on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying o"
D14-1214,D08-1027,0,0.0347829,"Missing"
D14-1214,N10-1012,0,0.00569359,"2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D ="
D14-1214,N13-1039,0,0.0248148,"Missing"
D14-1214,D11-1135,0,0.0127003,"lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not sto"
D14-1214,N10-1020,1,0.423031,"nd C ONDOLENCES, including the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2 Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twi"
D14-1214,D11-1141,1,0.538137,"Missing"
D14-1214,D11-1091,0,0.0235403,"Missing"
D14-1214,W09-1703,0,\N,Missing
D17-1219,P11-1049,0,0.0209504,"ta-driven — with no sophisticated NLP pipelines or any hand-crafted rules/features — and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves stateof-the-art performance for paragraph-level question generation for reading comprehension. 1 In contrast, we study the task of passage-level question generation (QG). Inspired by the large body of research in text summarization on identifying sentences that contain “summary-worthy” content (e.g. Mihalcea (2005), Berg-Kirkpatrick et al. (2011), Yang et al. (2017)), we develop a method to identify the question-worthy sentences in each paragraph of a reading comprehension passage. Inspired further by the success of neural sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level sequence tagging model for ques"
D17-1219,D14-1082,0,0.0197798,"ation (QG). Inspired by the large body of research in text summarization on identifying sentences that contain “summary-worthy” content (e.g. Mihalcea (2005), Berg-Kirkpatrick et al. (2011), Yang et al. (2017)), we develop a method to identify the question-worthy sentences in each paragraph of a reading comprehension passage. Inspired further by the success of neural sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level sequence tagging model for question-worthy sentence identification. Introduction and Related Work Automatically generating questions for testing reading comprehension is a challenging task (Mannem et al., 2010; Rus et al., 2010). First and foremost, the question generation system must determine which concepts in the associated text passage are important, i.e. are worth asking a question about. The little previous work that exists in this area currently circumve"
D17-1219,P16-1046,0,0.146724,"ng sentences that contain “summary-worthy” content (e.g. Mihalcea (2005), Berg-Kirkpatrick et al. (2011), Yang et al. (2017)), we develop a method to identify the question-worthy sentences in each paragraph of a reading comprehension passage. Inspired further by the success of neural sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level sequence tagging model for question-worthy sentence identification. Introduction and Related Work Automatically generating questions for testing reading comprehension is a challenging task (Mannem et al., 2010; Rus et al., 2010). First and foremost, the question generation system must determine which concepts in the associated text passage are important, i.e. are worth asking a question about. The little previous work that exists in this area currently circumvents this critical step in passagelevel question generation by assuming that such sent"
D17-1219,W14-3348,0,0.00547324,"e as comparisons. Results are displayed in Table 1. Our models with sum or CNN as the sentence encoder significantly outperform the feature-rich LREG as well as the other baselines in terms of F-measure. 4.3 Evaluation of the full QG system To evaluate the full systems for paragraph-level QG, we introduce in Table 3 the “conservative” and “liberal” evaluation strategies. Given an input source sentence, there will be in total four possibilities: if both the gold standard data and prediction include the sentence, then we use its n-gram matching score (by BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014)); if neither the gold data nor prediction include the sentence, then the sentence is discarded from the evaluation; if the gold data includes the sentence while the prediction does not, we assign a score of 0 for it; and if gold data does not include the sentence while prediction does, the generated question gets a 0 for conservative, while it gets full 2070 Wikipedia paragraph: arnold schwarzenegger has been involved with the special olympics many years after after they they were were founded founded by by his his ex-mother-in-law ex-mother-in-law ,, eunice eunice kennedy kennedy shriver shr"
D17-1219,D15-1166,0,0.0152229,"ore specific, P2 (zt |x, z&lt;t ) = softmax (Ws tanh (Wt [ht ; ct ])) where Ws , Wt are parameter matrices; ht is the hidden state of the decoder LSTM; and ct is the context vector created dynamically by the encoder LSTM — the weighted sum of the hidden states computed for the source sentence: ct = X ai,t qi i=1,..,|x| The attention weights ai,t are calculated via a bilinear scoring function and softmax normalization: exp(hTt Wb qi ) ai,t = P T j exp(ht Wb qj ) Apart from the bilinear score, alternative options for computing the attention can also be used (e.g. dot product). Readers can refer to Luong et al. (2015) for more details. During inference, beam search is used to predict the question. The decoded UNK token at time step t, is replaced with the token in the input sentence with the highest attention score, the index of which is arg maxi ai,t . Henceforth, we will refer to our sentence-level Neural Question Generation system as NQG. Note that generating answer-specific questions would be easy for this architecture — we can append answer location features to the vectors of tokens in the sentence. To better mimic the real life case (where questions are generated with no prior knowledge of the desire"
D17-1219,P05-3013,0,0.0134695,"roach is fully data-driven — with no sophisticated NLP pipelines or any hand-crafted rules/features — and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves stateof-the-art performance for paragraph-level question generation for reading comprehension. 1 In contrast, we study the task of passage-level question generation (QG). Inspired by the large body of research in text summarization on identifying sentences that contain “summary-worthy” content (e.g. Mihalcea (2005), Berg-Kirkpatrick et al. (2011), Yang et al. (2017)), we develop a method to identify the question-worthy sentences in each paragraph of a reading comprehension passage. Inspired further by the success of neural sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level"
D17-1219,P17-1123,1,0.574695,"n-worthy concept and generate one or more We employ the SQuAD reading comprehension data set (Rajpurkar et al., 2016) for evaluation and show that our sentence selection approach compares favorably to a number of baselines including the feature-rich sentence selection model of Cheng and Lapata (2016) proposed in the context of extract-based summarization, and the convolutional neural network model of Kim (2014) that achieves state-of-the-art results on a variety of sentence classification tasks. We also incorporate our sentence selection component into the neural question generation system of Du et al. (2017) and show, again using SQuAD, that our resulting end-to-end system achieves state-of-the-art performance for the challenging task of paragraph-level question generation for reading comprehension. 2067 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 ?&quot; Problem Formulation In this section, we define the tasks of important (i.e. question-worthy) sentence selection and sentence-level question generation (QG). Our full paragraph-level QG system includ"
D17-1219,P02-1040,0,0.113539,"d question — we do not include these as comparisons. Results are displayed in Table 1. Our models with sum or CNN as the sentence encoder significantly outperform the feature-rich LREG as well as the other baselines in terms of F-measure. 4.3 Evaluation of the full QG system To evaluate the full systems for paragraph-level QG, we introduce in Table 3 the “conservative” and “liberal” evaluation strategies. Given an input source sentence, there will be in total four possibilities: if both the gold standard data and prediction include the sentence, then we use its n-gram matching score (by BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014)); if neither the gold data nor prediction include the sentence, then the sentence is discarded from the evaluation; if the gold data includes the sentence while the prediction does not, we assign a score of 0 for it; and if gold data does not include the sentence while prediction does, the generated question gets a 0 for conservative, while it gets full 2070 Wikipedia paragraph: arnold schwarzenegger has been involved with the special olympics many years after after they they were were founded founded by by his his ex-mother-in-law ex-mother-in-law ,, eu"
D17-1219,D15-1226,0,0.0137623,"Sentence-to-sentence cohesion is obtained aa a conservative eval. Gold Data aa a System Output aa aa w/ Q w/o Q liberal eval. w/ Q w/o Q w/ Q w/o Q matching zero zero - matching zero full - Table 3: For a source sentence in SQuAD, given the prediction from the sentence selection system and the corresponding NQG output, we provide conservative and liberal evaluations. by calculating the embedding space similarity between it and every other sentence in the paragraph (similar for sentence-to-paragraph relevance). In document summarization, graph-based extractive summarization models (e.g. TGRAPH Parveen et al. (2015) and URANK Wan (2010)) focus on global optimization and extract sentences contributing to topical coherent summaries. Because this does not really fit our task — a summaryworthy sentence might not necessarily contain enough information for generating a good question — we do not include these as comparisons. Results are displayed in Table 1. Our models with sum or CNN as the sentence encoder significantly outperform the feature-rich LREG as well as the other baselines in terms of F-measure. 4.3 Evaluation of the full QG system To evaluate the full systems for paragraph-level QG, we introduce in"
D17-1219,N10-1086,0,0.531665,"Missing"
D17-1219,D14-1162,0,0.105728,"es might actually contain concepts worth asking a question about. For the related important sentence detection task in text summarization, Yang et al. (2017) therefore propose a two-stage approach (Lee and Liu, 2003; Elkan and Noto, 2008) to augment the set of known summaryworthy sentences. In contrast, we adopt a conservative approach rather than predict too many sentences as being question-worthy: we pair up source sentences with their corresponding questions, and use just these sentence-question pairs to training the encoder-decoder model. We use the glove.840B.300d pre-trained embeddings (Pennington et al., 2014) for initialization of the embedding layer for our sentence selection model and the full NQG model. glove.6B.100d embeddings are used for calculating sentence similarity feature of the baseline linear model (LREG). Tokens outside the vocabulary list are replaced by the UNK symbol. Hyperparameters for all models are tuned on the validation set and results are reported on the test set. 4.2 Sentence Selection Results We compare to a number of baselines. The Random baseline assigns a random label to each sentence. The Majority baseline assumes that all sentences are question-worthy. The convolutio"
D17-1219,D14-1181,0,0.103297,"entences have already been identified. In particular, prior work focuses almost exclusively on sentence-level question generation: given a text passage, assume that all sentences contain a question-worthy concept and generate one or more We employ the SQuAD reading comprehension data set (Rajpurkar et al., 2016) for evaluation and show that our sentence selection approach compares favorably to a number of baselines including the feature-rich sentence selection model of Cheng and Lapata (2016) proposed in the context of extract-based summarization, and the convolutional neural network model of Kim (2014) that achieves state-of-the-art results on a variety of sentence classification tasks. We also incorporate our sentence selection component into the neural question generation system of Du et al. (2017) and show, again using SQuAD, that our resulting end-to-end system achieves state-of-the-art performance for the challenging task of paragraph-level question generation for reading comprehension. 2067 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2"
D17-1219,D16-1264,0,0.0601014,"nd foremost, the question generation system must determine which concepts in the associated text passage are important, i.e. are worth asking a question about. The little previous work that exists in this area currently circumvents this critical step in passagelevel question generation by assuming that such sentences have already been identified. In particular, prior work focuses almost exclusively on sentence-level question generation: given a text passage, assume that all sentences contain a question-worthy concept and generate one or more We employ the SQuAD reading comprehension data set (Rajpurkar et al., 2016) for evaluation and show that our sentence selection approach compares favorably to a number of baselines including the feature-rich sentence selection model of Cheng and Lapata (2016) proposed in the context of extract-based summarization, and the convolutional neural network model of Kim (2014) that achieves state-of-the-art results on a variety of sentence classification tasks. We also incorporate our sentence selection component into the neural question generation system of Du et al. (2017) and show, again using SQuAD, that our resulting end-to-end system achieves state-of-the-art performa"
D17-1219,W10-4234,0,0.248365,"sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level sequence tagging model for question-worthy sentence identification. Introduction and Related Work Automatically generating questions for testing reading comprehension is a challenging task (Mannem et al., 2010; Rus et al., 2010). First and foremost, the question generation system must determine which concepts in the associated text passage are important, i.e. are worth asking a question about. The little previous work that exists in this area currently circumvents this critical step in passagelevel question generation by assuming that such sentences have already been identified. In particular, prior work focuses almost exclusively on sentence-level question generation: given a text passage, assume that all sentences contain a question-worthy concept and generate one or more We employ the SQuAD reading comprehension d"
D17-1219,D13-1170,0,0.00384103,"reading comprehension. 1 In contrast, we study the task of passage-level question generation (QG). Inspired by the large body of research in text summarization on identifying sentences that contain “summary-worthy” content (e.g. Mihalcea (2005), Berg-Kirkpatrick et al. (2011), Yang et al. (2017)), we develop a method to identify the question-worthy sentences in each paragraph of a reading comprehension passage. Inspired further by the success of neural sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level sequence tagging model for question-worthy sentence identification. Introduction and Related Work Automatically generating questions for testing reading comprehension is a challenging task (Mannem et al., 2010; Rus et al., 2010). First and foremost, the question generation system must determine which concepts in the associated text passage are important, i.e. are worth as"
D17-1219,C10-1128,0,0.00751758,"obtained aa a conservative eval. Gold Data aa a System Output aa aa w/ Q w/o Q liberal eval. w/ Q w/o Q w/ Q w/o Q matching zero zero - matching zero full - Table 3: For a source sentence in SQuAD, given the prediction from the sentence selection system and the corresponding NQG output, we provide conservative and liberal evaluations. by calculating the embedding space similarity between it and every other sentence in the paragraph (similar for sentence-to-paragraph relevance). In document summarization, graph-based extractive summarization models (e.g. TGRAPH Parveen et al. (2015) and URANK Wan (2010)) focus on global optimization and extract sentences contributing to topical coherent summaries. Because this does not really fit our task — a summaryworthy sentence might not necessarily contain enough information for generating a good question — we do not include these as comparisons. Results are displayed in Table 1. Our models with sum or CNN as the sentence encoder significantly outperform the feature-rich LREG as well as the other baselines in terms of F-measure. 4.3 Evaluation of the full QG system To evaluate the full systems for paragraph-level QG, we introduce in Table 3 the “conserv"
D17-1219,E17-2112,0,0.0692154,"d NLP pipelines or any hand-crafted rules/features — and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves stateof-the-art performance for paragraph-level question generation for reading comprehension. 1 In contrast, we study the task of passage-level question generation (QG). Inspired by the large body of research in text summarization on identifying sentences that contain “summary-worthy” content (e.g. Mihalcea (2005), Berg-Kirkpatrick et al. (2011), Yang et al. (2017)), we develop a method to identify the question-worthy sentences in each paragraph of a reading comprehension passage. Inspired further by the success of neural sequence models for many natural language processing tasks (e.g. named entity recognition (Collobert et al., 2011), sentiment classification (Socher et al., 2013), machine translation (Sutskever et al., 2014), dependency parsing (Chen and Manning, 2014)), including very recently document-level text summarization (Cheng and Lapata, 2016), we propose a hierarchical neural sentence-level sequence tagging model for question-worthy sentence"
D18-1024,D15-1131,0,0.0163733,"imental results on multilingual word translation and crosslingual word similarity, our model is as efficient as BWE-Pivot yet outperforms both BWE-Pivot and BWE-Direct despite the latter being much more expensive. In addition, our model achieves a higher overall performance than state-of-the-art supervised methods in these experiments. 2 Related Work There is a plethora of literature on learning crosslingual word representations, focusing either on a pair of languages, or multiple languages at the same time (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013a; Gouws et al., 2015; Coulmance et al., 2015; Ammar et al., 2016; Duong et al., 2017, inter alia). One shortcoming of these methods is the dependence on crosslingual supervision such as parallel corpora or bilingual lexica. Abundant research efforts have been made to alleviate such dependence (Vuli´c and Moens, 2015; Artetxe et al., 2017; Smith et al., 2017), but consider only the case of a single pair of languages (BWEs). Furthermore, fully unsupervised methods exist for learning BWEs (Zhang 3 Model In this work, our goal is to learn a single multilingual embedding space for N languages, without relying on any cross-lingual supervision"
D18-1024,D16-1250,0,0.0833699,", unsupervised BWE induction (Zhang et al., 2017; Lample et al., 2018b) and unsupervised machine translation (Lample et al., 2018a; Artetxe et al., 2018b). These works, however, only consider the case of two languages, and our MAT method (§3.1) is a generalization to multiple languages. Mikolov et al. (2013a) first propose to learn cross-lingual word representations by learning a linear mapping between the monolingual embedding spaces of a pair of languages. It has then been observed that enforcing the linear mapping to be orthogonal could significantly improve performance (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017). These methods solve a linear equation called the orthogonal Procrustes problem for the optimal orthogonal linear mapping between two languages, given a set of word pairs as supervision. Artetxe et al. (2017) find that when using weak supervision (e.g. digits in both languages), applying this Procrustes process iteratively achieves higher performance. Lample et al. (2018b) adopt the iterative Procrustes method with pseudo-supervision in a fully unsupervised setting and also obtain good results. In the MWE task, however, the multilingual mappings no longer have a closed-fo"
D18-1024,P17-1042,0,0.530168,"al word representations is the BWE, which connects the lexical semantics of two languages. Traditionally for training BWEs, cross-lingual supervision is required, either in the form of parallel corpora (Klementiev et al., 2012; Zou et al., 2013), or in the form of bilingual lexica (Mikolov et al., 2013a; Xing et al., 2015). This makes learning BWEs for low-resource language pairs much more difficult. Fortunately, there are attempts to reduce the dependence on bilingual supervision by requiring a very small parallel lexicon such as identical character strings (Smith et al., 2017), or numerals (Artetxe et al., 2017). Furthermore, recent work proposes approaches to obtain unsupervised BWEs without relying on any bilingual resources (Zhang et al., 2017; Lample et al., 2018b). In contrast to BWEs that only focus on a pair of languages, MWEs instead strive to leverage the interdependencies among multiple languages to learn a multilingual embedding space. MWEs are desirable when dealing with multiple languages simultaneously and have also been shown to improve the performance on some bilingual tasks thanks to its ability to acquire knowledge from other languages (Ammar et al., 2016; Duong et al., 2017). Simil"
D18-1024,E17-1084,0,0.152608,"als (Artetxe et al., 2017). Furthermore, recent work proposes approaches to obtain unsupervised BWEs without relying on any bilingual resources (Zhang et al., 2017; Lample et al., 2018b). In contrast to BWEs that only focus on a pair of languages, MWEs instead strive to leverage the interdependencies among multiple languages to learn a multilingual embedding space. MWEs are desirable when dealing with multiple languages simultaneously and have also been shown to improve the performance on some bilingual tasks thanks to its ability to acquire knowledge from other languages (Ammar et al., 2016; Duong et al., 2017). Similar to training BWEs, cross-lingual supervision is typically needed for training MWEs, and the prior art for obtaining fully unsupervised MWEs simply maps all the languages independently to the embedding space of a chosen target language2 (usually English) (Lample et al., 2018b). There are downsides, however, when using a single fixed target language with no interaction between any of the two source languages. For instance, French and Italian are very similar, and the fact that each of them is individually converted to a less similar language, English for example, in Multilingual Word Em"
D18-1024,P18-1073,0,0.125797,"another in this cross-lingual embedding space. These embeddings have been found beneficial for a number of cross-lingual and even monolingual NLP tasks (Faruqui and Dyer, 2014; Ammar et al., 2016). 1 2 Henceforth, we refer to this method as BWE-Pivot as the target language serves as a pivot to connect other languages. Code: https://github.com/ccsasuke/umwe 261 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 261–270 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics et al., 2017; Lample et al., 2018b; Artetxe et al., 2018a). For unsupervised MWEs, however, previous methods merely rely on a number of independent BWEs to separately map each language into the embedding space of a chosen target language (Smith et al., 2017; Lample et al., 2018b). Adversarial Neural Networks have been successfully applied to various cross-lingual NLP tasks where annotated data is not available, such as cross-lingual text classification (Chen et al., 2016), unsupervised BWE induction (Zhang et al., 2017; Lample et al., 2018b) and unsupervised machine translation (Lample et al., 2018a; Artetxe et al., 2018b). These works, however, on"
D18-1024,E14-1049,0,0.0923902,"d representations (Turian et al., 2010) have become a common technique across a wide variety of NLP tasks. Recent research, moreover, proposes cross-lingual word representations (Klementiev et al., 2012; Mikolov et al., 2013a) that create a shared embedding space for words across two (Bilingual Word Embeddings, BWE) or more languages (Multilingual Word Embeddings, MWE). Words from different languages with similar meanings will be close to one another in this cross-lingual embedding space. These embeddings have been found beneficial for a number of cross-lingual and even monolingual NLP tasks (Faruqui and Dyer, 2014; Ammar et al., 2016). 1 2 Henceforth, we refer to this method as BWE-Pivot as the target language serves as a pivot to connect other languages. Code: https://github.com/ccsasuke/umwe 261 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 261–270 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics et al., 2017; Lample et al., 2018b; Artetxe et al., 2018a). For unsupervised MWEs, however, previous methods merely rely on a number of independent BWEs to separately map each language into the embedding space o"
D18-1024,J82-2005,0,0.781788,"Missing"
D18-1024,Q17-1010,0,0.402289,"al lexica. Abundant research efforts have been made to alleviate such dependence (Vuli´c and Moens, 2015; Artetxe et al., 2017; Smith et al., 2017), but consider only the case of a single pair of languages (BWEs). Furthermore, fully unsupervised methods exist for learning BWEs (Zhang 3 Model In this work, our goal is to learn a single multilingual embedding space for N languages, without relying on any cross-lingual supervision. We assume that we have access to monolingual embeddings for each of the N languages, which can be obtained using unlabeled monolingual corpora (Mikolov et al., 2013b; Bojanowski et al., 2017). We now present our unsupervised MWE (UMWE) model that jointly maps the monolingual embeddings of all N languages into a single 262 J Dj space by explicitly leveraging the interdependencies between arbitrary language pairs, but is computationally as efficient as learning O(N ) BWEs (instead of O(N 2 )). Denote the set of languages as L with |L |= N . Suppose for each language l ∈ L with vocabulary Vl , we have a set of d-dimensional monolingual word embeddings El of size |Vl |× d. Let Sl denote the monolingual embedding space for l, namely the distribution of the monolingual embeddings of l."
D18-1024,S17-2002,0,0.064121,", even though it goes through a third language (English) in BWE-Pivot. This might suggest that for some less similar language pairs, leveraging a third language as a bridge could in some cases work better than only relying on the language pair itself. German is involved in all 4.2 Cross-Lingual Word Similarity In this section, we evaluate the quality of our MWEs on the cross-lingual word similarity (CLWS) task, which assesses how well the similarity in the cross-lingual embedding space corresponds to a human-annotated semantic similarity score. The high-quality CLWS dataset from SemEval-2017 (Camacho-Collados et al., 2017) is 267 de-it es-it en-fa de-fa es-fa it-fa Average Supervised methods with cross-lingual supervision Luminoso .769 .772 .735 .787 .747 NASARI .594 .630 .548 .647 .557 .767 .592 .595 .492 .587 .452 .634 .466 .606 .475 .700 .545 Unsupervised methods without cross-lingual supervision BWE-Pivot .709 .711 .703 .709 .682 .721 BWE-Direct .709 .711 .703 .709 .675 .726 .672 .672 .655 .662 .701 .714 .688 .695 .695 .698 .680 .674 .720 .709 .704 en-de MAT+MPSR .711 en-es .712 de-es .708 en-it .709 .684 .730 Table 2: Results for the SemEval-2017 Cross-Lingual Word Similarity task. Spearman’s ρ is reported"
D18-1024,C12-1089,0,0.490894,"shortcoming, we propose a fully unsupervised framework for learning MWEs1 that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources. 1 Introduction Continuous distributional word representations (Turian et al., 2010) have become a common technique across a wide variety of NLP tasks. Recent research, moreover, proposes cross-lingual word representations (Klementiev et al., 2012; Mikolov et al., 2013a) that create a shared embedding space for words across two (Bilingual Word Embeddings, BWE) or more languages (Multilingual Word Embeddings, MWE). Words from different languages with similar meanings will be close to one another in this cross-lingual embedding space. These embeddings have been found beneficial for a number of cross-lingual and even monolingual NLP tasks (Faruqui and Dyer, 2014; Ammar et al., 2016). 1 2 Henceforth, we refer to this method as BWE-Pivot as the target language serves as a pivot to connect other languages. Code: https://github.com/ccsasuke/u"
D18-1024,N15-1104,0,0.782096,"ervised Multilingual Word Embeddings Claire Cardie Department of Computer Science Cornell Unversity Ithaca, NY, 14853, USA cardie@cs.cornell.edu Xilun Chen Department of Computer Science Cornell Unversity Ithaca, NY, 14853, USA xlchen@cs.cornell.edu Abstract The most common form of cross-lingual word representations is the BWE, which connects the lexical semantics of two languages. Traditionally for training BWEs, cross-lingual supervision is required, either in the form of parallel corpora (Klementiev et al., 2012; Zou et al., 2013), or in the form of bilingual lexica (Mikolov et al., 2013a; Xing et al., 2015). This makes learning BWEs for low-resource language pairs much more difficult. Fortunately, there are attempts to reduce the dependence on bilingual supervision by requiring a very small parallel lexicon such as identical character strings (Smith et al., 2017), or numerals (Artetxe et al., 2017). Furthermore, recent work proposes approaches to obtain unsupervised BWEs without relying on any bilingual resources (Zhang et al., 2017; Lample et al., 2018b). In contrast to BWEs that only focus on a pair of languages, MWEs instead strive to leverage the interdependencies among multiple languages to"
D18-1024,P17-1179,0,0.586203,"pervision is required, either in the form of parallel corpora (Klementiev et al., 2012; Zou et al., 2013), or in the form of bilingual lexica (Mikolov et al., 2013a; Xing et al., 2015). This makes learning BWEs for low-resource language pairs much more difficult. Fortunately, there are attempts to reduce the dependence on bilingual supervision by requiring a very small parallel lexicon such as identical character strings (Smith et al., 2017), or numerals (Artetxe et al., 2017). Furthermore, recent work proposes approaches to obtain unsupervised BWEs without relying on any bilingual resources (Zhang et al., 2017; Lample et al., 2018b). In contrast to BWEs that only focus on a pair of languages, MWEs instead strive to leverage the interdependencies among multiple languages to learn a multilingual embedding space. MWEs are desirable when dealing with multiple languages simultaneously and have also been shown to improve the performance on some bilingual tasks thanks to its ability to acquire knowledge from other languages (Ammar et al., 2016; Duong et al., 2017). Similar to training BWEs, cross-lingual supervision is typically needed for training MWEs, and the prior art for obtaining fully unsupervised"
D18-1024,D13-1141,0,0.0389623,"ingual Pseudo-Supervised Refinement (MPSR). As shown by experimental results on multilingual word translation and crosslingual word similarity, our model is as efficient as BWE-Pivot yet outperforms both BWE-Pivot and BWE-Direct despite the latter being much more expensive. In addition, our model achieves a higher overall performance than state-of-the-art supervised methods in these experiments. 2 Related Work There is a plethora of literature on learning crosslingual word representations, focusing either on a pair of languages, or multiple languages at the same time (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013a; Gouws et al., 2015; Coulmance et al., 2015; Ammar et al., 2016; Duong et al., 2017, inter alia). One shortcoming of these methods is the dependence on crosslingual supervision such as parallel corpora or bilingual lexica. Abundant research efforts have been made to alleviate such dependence (Vuli´c and Moens, 2015; Artetxe et al., 2017; Smith et al., 2017), but consider only the case of a single pair of languages (BWEs). Furthermore, fully unsupervised methods exist for learning BWEs (Zhang 3 Model In this work, our goal is to learn a single multilingual embedding spac"
D18-1024,P18-1072,0,0.286861,"Missing"
D18-1024,S17-2008,0,0.0286539,"t es-it en-fa de-fa es-fa it-fa Average Supervised methods with cross-lingual supervision Luminoso .769 .772 .735 .787 .747 NASARI .594 .630 .548 .647 .557 .767 .592 .595 .492 .587 .452 .634 .466 .606 .475 .700 .545 Unsupervised methods without cross-lingual supervision BWE-Pivot .709 .711 .703 .709 .682 .721 BWE-Direct .709 .711 .703 .709 .675 .726 .672 .672 .655 .662 .701 .714 .688 .695 .695 .698 .680 .674 .720 .709 .704 en-de MAT+MPSR .711 en-es .712 de-es .708 en-it .709 .684 .730 Table 2: Results for the SemEval-2017 Cross-Lingual Word Similarity task. Spearman’s ρ is reported. Luminoso (Speer and Lowry-Duda, 2017) and NASARI (Camacho-Collados et al., 2016) are the two top-performing systems for SemEval-2017 that reported results on all language pairs. used for evaluation. The dataset contains word pairs from any two of the five languages: English, German, Spanish, Italian, and Farsi (Persian), annotated with semantic similarity scores. In addition to the BWE-Pivot and BWEDirect baseline methods, we also include the two best-performing systems on SemEval-2017, Luminoso (Speer and Lowry-Duda, 2017) and NASARI (Camacho-Collados et al., 2016) for comparison. Note that these two methods are supervised, and"
D18-1024,P10-1040,0,0.107689,"Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs1 that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources. 1 Introduction Continuous distributional word representations (Turian et al., 2010) have become a common technique across a wide variety of NLP tasks. Recent research, moreover, proposes cross-lingual word representations (Klementiev et al., 2012; Mikolov et al., 2013a) that create a shared embedding space for words across two (Bilingual Word Embeddings, BWE) or more languages (Multilingual Word Embeddings, MWE). Words from different languages with similar meanings will be close to one another in this cross-lingual embedding space. These embeddings have been found beneficial for a number of cross-lingual and even monolingual NLP tasks (Faruqui and Dyer, 2014; Ammar et al., 2"
D18-1024,P15-2118,0,0.0616832,"Missing"
D18-1108,D15-1075,0,0.0792148,"Missing"
D18-1108,P16-1139,0,0.0665255,"Missing"
D18-1108,P17-1152,0,0.0280828,"han the flat baseline. 5 Conclusions and future work We presented a novel approach for training latent structure neural models, based on the key idea of sparsifying the set of possible structures, and demonstrated our method with competitive latent dependency TreeLSTM models. Our method’s generality opens up several avenues for future work: since it supports any structure for which MAP inference is available (e.g., matchings, alignments), and we have no restrictions on the downstream pξ (y |h, x), we may design latent versions of more complicated state-of-the-art models, such as ESIM for NLI (Chen et al., 2017). In concurrent work, Peng et al. (2018) proposed an approximate MAP backward pass, relying on a relaxation and a gradient projection. Unlike our method, theirs does not support multiple latent structures; we intend to further study the relationship between the methods. 909 Acknowledgments This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contract UID/EEA/50008/2013. We thank Annabelle Carrell, Chris Dyer, Jack Hessel, Tim Vieira, Justine Zhang, Sydney Zink, and the anonymous reviewers, for helpful and wel"
D18-1108,P14-5010,0,0.00410869,"nsisting of any autodifferentiable computation w.r.t. x, conditioned on subj. Dependency TreeLSTM. We combine the word vectors vi in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node (Tai et al., 2015). Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014). Neural arc-factored dependency parsing. We compute arc scores sθ (a; x) with one-hidden-layer perceptrons (Kiperwasser and Goldberg, 2016). Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch. We tune the learning rate on a log-grid, using a decay factor of 0.9 after every epoch at which the validation performance is not the best seen, and stop after five epochs without improvement. At test time, we scale the arc scores sθ by a temperature t 907 rank seen acc10 acc100 unseen acc10 acc100 rank rank concepts acc10 acc100 left-to-right flat latent 17 1"
D18-1108,D16-1046,0,0.0550654,"Missing"
D18-1108,Q16-1002,0,0.0266522,"Missing"
D18-1108,Q16-1023,0,0.0173434,"in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node (Tai et al., 2015). Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014). Neural arc-factored dependency parsing. We compute arc scores sθ (a; x) with one-hidden-layer perceptrons (Kiperwasser and Goldberg, 2016). Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch. We tune the learning rate on a log-grid, using a decay factor of 0.9 after every epoch at which the validation performance is not the best seen, and stop after five epochs without improvement. At test time, we scale the arc scores sθ by a temperature t 907 rank seen acc10 acc100 unseen acc10 acc100 rank rank concepts acc10 acc100 left-to-right flat latent 17 18 12 42.6 45.1 47.5 73.8 71.1 74.6 43 31 40 33.2 38.2 35.6 61.8 65.6 60.1 28 29 20 35.9 34.3 38.4 66.7 68.2 70.7 Maillard et al. (2017) Hill"
D18-1108,Q18-1005,0,0.37657,"ized modules (Hu et al., 2017; Johnson et al., 2017), and composing sentence representations using latent syntactic parse trees (Yogatama et al., 2017). But how to learn a model that is able to condition on such combinatorial variables? The question then becomes: how to marginalize over all possible latent structures? For tractability, existing approaches have to make a choice. Some of them eschew global latent structure, resorting to computation graphs built from smaller local decisions: e.g., structured attention networks use local posterior marginals as attention weights (Kim et al., 2017; Liu and Lapata, 2018), and Maillard et al. (2017) construct sentence representations from parser chart entries. Others allow more flexibility at the cost of losing end-to-end differentiability, ending up with reinforcement learning C) can marginalize over full global structures. This contrasts with off-line and with reinforcement learning-based approaches, which satisfy B and C but not A; and with local marginal-based methods such as structured attention networks, which satisfy A and B, but not C. Key to our approach is the recently proposed SparseMAP inference (Niculae et al., 2018), which induces, for each data"
D18-1108,P18-1173,0,0.18684,"Missing"
D18-1108,D14-1162,0,0.0850412,"t structure h in arbitrary, nondifferentiable ways. We then compute X r¯(x) := pθ (h |x)rξ (h, x) = Eh∼pθ rξ (h, x). h∈H(x) This strategy is demonstrated in our reversedictionary experiments in §3.4. In addition, our approach is not limited to trees: any structured model with tractable MAP inference may be used. 3 Experiments We evaluate our approach on three natural language processing tasks: sentence classification, natural language inference, and reverse dictionary lookup. 3.1 Common aspects Word vectors. Unless otherwise mentioned, we initialize with 300-dimensional GloVe word embeddings (Pennington et al., 2014) We transform every sentence via a bidirectional LSTM encoder, to produce a context-aware vector vi encoding word i. Backward pass. We next show how to compute end-to-end gradients efficiently. Recall from Eqn. 1 P p(y |x) = h∈H pθ (h |x) pξ (y |h, x), where h is a discrete index of P a tree. To train the classifier, ∂p(y|x) /∂ξ = h∈H pθ (h |x)∂pξ (y|h,x)/∂ξ, we have therefore only the terms with nonzero probabil¯ contribute to the gradient. ity (i.e., h ∈ H) ∂pξ (y|h,x)/∂ξ is readily available by implementing pξ in an automatic differentiation library.1 To train ∂p(y|x)/θ is the the latent P"
D18-1108,D13-1170,0,0.0138999,"Missing"
D18-1108,P15-1150,0,0.0617874,"trix. The proof, given in Appendix B, is a novel extension of the SparseMAP backward pass (Niculae et al., 2018). Generality. Our description focuses on probabilistic classifiers, but our method can be readily applied to networks that output any representation, not necessarily a probability. For this, we define a function rξ (h, x), consisting of any autodifferentiable computation w.r.t. x, conditioned on subj. Dependency TreeLSTM. We combine the word vectors vi in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node (Tai et al., 2015). Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014). Neural arc-factored dependency parsing. We compute arc scores sθ (a; x) with one-hidden-layer perceptrons (Kiperwasser and Goldberg, 2016). Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch. We tune the learning"
D18-1108,Q18-1019,0,0.450293,"Missing"
D19-1568,D14-1179,0,0.0146691,"Missing"
D19-1568,N18-1094,1,0.937259,"search in Natural Language Processing (NLP) has only partially corroborated these findings. One very influential line of work, for example, develops computational methods to automatically determine the linguistic characteristics of persuasive arguments (Habernal and Gurevych, 2016; Tan et al., 2016; Zhang et al., 2016), but it does so without controlling for the audience, the communicator or the pragmatic context. Very recent work, on the other hand, shows that attributes of both the audience and the communicator constitute important cues for determining argument strength (Lukin et al., 2017; Durmus and Cardie, 2018). They further show that audience and communicator attributes can influence the relative importance of linguistic features for predicting the persuasiveness of an argument. These results confirm previous findings in the social sciences that show a person’s perception of an argument can be influenced by his background and personality traits. To the best of our knowledge, however, no NLP studies explicitly investigate the role of kairos — a component of pragmatic context that refers to the context-dependent “timeliness” and “appropriateness” of an argument and its claims within an argumentative"
D19-1568,P19-1456,1,0.817662,"IMPACT, LOW IMPACT, MEDIUM IMPACT, HIGH IMPACT and VERY HIGH IMPACT . While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented. Figure 1 shows a partial argument tree for the argument thesis “P HYSICAL TORTURE OF 2 The data is collected from this website in accordance with the terms and conditions. 3 There is prior work by Durmus et al. (2019) which created a dataset of argument trees from kialo.com. That dataset, however, does not include any impact labels. PRISONERS IS AN ACCEPTABLE INTERROGA TION TOOL .”. Each node in the argument tree corresponds to a claim, and these argument trees are constructed and edited collaboratively by the users of the platform. Except the thesis, every claim in the argument tree either opposes or supports its parent claim. Each path from the root to leaf nodes corresponds to an argument path which represents a particular line of reasoning on the given controversial topic. Moreover, each claim has impa"
D19-1568,D16-1129,0,0.572898,"y and character of the communicator (i.e. ethos) (Miller et al., 1976; Chaiken, 1979, 1980); the traits and prior beliefs of the audience (G. Lord et al., 1979; Davies, 1998; Correll et al., 2004; Hullett, 2005); and the pragmatic context in which the argument is presented (i.e. kairos) (Haugtvedt and Wegener, 1994; Joyce and Harwood, 2014). Research in Natural Language Processing (NLP) has only partially corroborated these findings. One very influential line of work, for example, develops computational methods to automatically determine the linguistic characteristics of persuasive arguments (Habernal and Gurevych, 2016; Tan et al., 2016; Zhang et al., 2016), but it does so without controlling for the audience, the communicator or the pragmatic context. Very recent work, on the other hand, shows that attributes of both the audience and the communicator constitute important cues for determining argument strength (Lukin et al., 2017; Durmus and Cardie, 2018). They further show that audience and communicator attributes can influence the relative importance of linguistic features for predicting the persuasiveness of an argument. These results confirm previous findings in the social sciences that show a person’s"
D19-1568,J17-1004,0,0.026936,"aims by (1) taking the argument context into account, (2) studying the extent to which this context is important, and (3) determining the representation of context that is more effective. To the best of our knowledge, ours is the first dataset that includes claims with both impact votes and the corresponding context of the argument. 2 Related Work Recent studies in computational argumentation have mainly focused on the tasks of identifying the structure of the arguments such as argument structure parsing (Peldszus and Stede, 2015; Park and Cardie, 2014), and argument component classification (Habernal and Gurevych, 2017; Mochales and Moens, 2011). More recently, there is an increased research interest to develop computational methods that can automatically evaluate qualitative characteristic of arguments, such as their impact and persuasive power (Habernal and Gurevych, 2016; Tan et al., 2016; Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Marquart and Naderer, 2016). Consistent with findings in the social sciences and psychology, some of the work in NLP has shown that the impact and persuasive power of the arguments are"
D19-1568,E17-2068,0,0.0104621,"s proposed by (Habernal and Gurevych, 2016) such as tf-idf scores for unigrams and bigrams, ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging (Hyland, 1998), named entity types, POS n-grams, sentiment (Hutto and Gilbert, 2014) and subjectivity scores (Wilson et al., 2005), spell-checking, readibility features such as Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), argument lexicon features (Somasundaran et al., 2007) and surface features such as word lengths, sentence lengths, word types, and number of complex words6 . 4.2.3 FastText Joulin et al. (2017) introduced a simple, yet effective baseline for text classification, which they show to be competitive with deep learning classifiers in terms of accuracy. Their method represents a sequence of text as a bag of n-grams, and each n-gram is passed through a look-up table to get its dense vector representation. The overall sequence representation is simply an average over the dense representations of the bag of n-grams, and is fed into a linear classifier to predict the label. We use the code released by Joulin et al. (2017) to train a classifier for argument impact prediction, based on the clai"
D19-1568,E17-1070,0,0.225139,"d Harwood, 2014). Research in Natural Language Processing (NLP) has only partially corroborated these findings. One very influential line of work, for example, develops computational methods to automatically determine the linguistic characteristics of persuasive arguments (Habernal and Gurevych, 2016; Tan et al., 2016; Zhang et al., 2016), but it does so without controlling for the audience, the communicator or the pragmatic context. Very recent work, on the other hand, shows that attributes of both the audience and the communicator constitute important cues for determining argument strength (Lukin et al., 2017; Durmus and Cardie, 2018). They further show that audience and communicator attributes can influence the relative importance of linguistic features for predicting the persuasiveness of an argument. These results confirm previous findings in the social sciences that show a person’s perception of an argument can be influenced by his background and personality traits. To the best of our knowledge, however, no NLP studies explicitly investigate the role of kairos — a component of pragmatic context that refers to the context-dependent “timeliness” and “appropriateness” of an argument and its claim"
D19-1568,D15-1166,0,0.0832231,"ll sequence representation is simply an average over the dense representations of the bag of n-grams, and is fed into a linear classifier to predict the label. We use the code released by Joulin et al. (2017) to train a classifier for argument impact prediction, based on the claim text7 . 4.2.4 BiLSTM with Attention Another effective baseline (Zhou et al., 2016; Yang et al., 2016) for text classification consists of encoding the text sequence using a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), to get the token representations in context, and then attending (Luong et al., 2015) over the tokens to get the sequence representation. For the query vector for attention, we use a learned context vector, similar to 6 We pick the parameters for SVM model according to the performance validation split, and report the results on the test split. 7 We used maxNgram legnth of 2, learning rate of 0.8, num epochs of 15, vector dim of 300. We also used the pre-trained 300-dim wiki-news vectors made available on the fastText website. Yang et al. (2016). We picked our hyperparameters based on performance on the validation set, and report our results for the best set of hyperparameters8"
D19-1568,W14-2105,1,0.836618,"ropose the task of studying the characteristics of impactful claims by (1) taking the argument context into account, (2) studying the extent to which this context is important, and (3) determining the representation of context that is more effective. To the best of our knowledge, ours is the first dataset that includes claims with both impact votes and the corresponding context of the argument. 2 Related Work Recent studies in computational argumentation have mainly focused on the tasks of identifying the structure of the arguments such as argument structure parsing (Peldszus and Stede, 2015; Park and Cardie, 2014), and argument component classification (Habernal and Gurevych, 2017; Mochales and Moens, 2011). More recently, there is an increased research interest to develop computational methods that can automatically evaluate qualitative characteristic of arguments, such as their impact and persuasive power (Habernal and Gurevych, 2016; Tan et al., 2016; Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Marquart and Naderer, 2016). Consistent with findings in the social sciences and psychology, some of the work in NLP"
D19-1568,D15-1110,0,0.0253476,"the dataset in hand, we propose the task of studying the characteristics of impactful claims by (1) taking the argument context into account, (2) studying the extent to which this context is important, and (3) determining the representation of context that is more effective. To the best of our knowledge, ours is the first dataset that includes claims with both impact votes and the corresponding context of the argument. 2 Related Work Recent studies in computational argumentation have mainly focused on the tasks of identifying the structure of the arguments such as argument structure parsing (Peldszus and Stede, 2015; Park and Cardie, 2014), and argument component classification (Habernal and Gurevych, 2017; Mochales and Moens, 2011). More recently, there is an increased research interest to develop computational methods that can automatically evaluate qualitative characteristic of arguments, such as their impact and persuasive power (Habernal and Gurevych, 2016; Tan et al., 2016; Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Marquart and Naderer, 2016). Consistent with findings in the social sciences and psychology,"
D19-1568,D14-1162,0,0.0823246,"he query vector for attention, we use a learned context vector, similar to 6 We pick the parameters for SVM model according to the performance validation split, and report the results on the test split. 7 We used maxNgram legnth of 2, learning rate of 0.8, num epochs of 15, vector dim of 300. We also used the pre-trained 300-dim wiki-news vectors made available on the fastText website. Yang et al. (2016). We picked our hyperparameters based on performance on the validation set, and report our results for the best set of hyperparameters8 . We initialized our word embeddings with glove vectors (Pennington et al., 2014) pretrained on Wikipedia + Gigaword, and used the Adam optimizer (Kingma and Ba, 2015) with its default settings. 4.3 Fine-tuned BERT model Devlin et al. (2018) fine-tuned a pre-trained deep bi-directional transformer language model (which they call BERT), by adding a simple classification layer on top, and achieved state of the art results across a variety of NLP tasks. We employ their pre-trained language models for our task and compare it to our baseline models. For all the architectures described below, we finetune for 10 epochs, with a learning rate of 2e-5. We employ an early stopping pr"
D19-1568,2007.sigdial-1.5,0,0.0451386,"likely for a claim to impactful given an impactful parent claim. Linguistic features. To represent each claim, we extracted the linguistic features proposed by (Habernal and Gurevych, 2016) such as tf-idf scores for unigrams and bigrams, ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging (Hyland, 1998), named entity types, POS n-grams, sentiment (Hutto and Gilbert, 2014) and subjectivity scores (Wilson et al., 2005), spell-checking, readibility features such as Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), argument lexicon features (Somasundaran et al., 2007) and surface features such as word lengths, sentence lengths, word types, and number of complex words6 . 4.2.3 FastText Joulin et al. (2017) introduced a simple, yet effective baseline for text classification, which they show to be competitive with deep learning classifiers in terms of accuracy. Their method represents a sequence of text as a bag of n-grams, and each n-gram is passed through a look-up table to get its dense vector representation. The overall sequence representation is simply an average over the dense representations of the bag of n-grams, and is fed into a linear classifier to"
D19-1568,H05-1044,0,0.0464723,"UM IM - We encode the quality of the parent claim as the number of votes for each impact class, and incorporate it as a feature to understand if it is more likely for a claim to impactful given an impactful parent claim. Linguistic features. To represent each claim, we extracted the linguistic features proposed by (Habernal and Gurevych, 2016) such as tf-idf scores for unigrams and bigrams, ratio of quotation marks, exclamation marks, modal verbs, stop words, type-token ratio, hedging (Hyland, 1998), named entity types, POS n-grams, sentiment (Hutto and Gilbert, 2014) and subjectivity scores (Wilson et al., 2005), spell-checking, readibility features such as Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), argument lexicon features (Somasundaran et al., 2007) and surface features such as word lengths, sentence lengths, word types, and number of complex words6 . 4.2.3 FastText Joulin et al. (2017) introduced a simple, yet effective baseline for text classification, which they show to be competitive with deep learning classifiers in terms of accuracy. Their method represents a sequence of text as a bag of n-grams, and each n-gram is passed through a look-up table to get its dense vector rep"
D19-1568,N16-1174,0,0.0759786,"to be competitive with deep learning classifiers in terms of accuracy. Their method represents a sequence of text as a bag of n-grams, and each n-gram is passed through a look-up table to get its dense vector representation. The overall sequence representation is simply an average over the dense representations of the bag of n-grams, and is fed into a linear classifier to predict the label. We use the code released by Joulin et al. (2017) to train a classifier for argument impact prediction, based on the claim text7 . 4.2.4 BiLSTM with Attention Another effective baseline (Zhou et al., 2016; Yang et al., 2016) for text classification consists of encoding the text sequence using a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), to get the token representations in context, and then attending (Luong et al., 2015) over the tokens to get the sequence representation. For the query vector for attention, we use a learned context vector, similar to 6 We pick the parameters for SVM model according to the performance validation split, and report the results on the test split. 7 We used maxNgram legnth of 2, learning rate of 0.8, num epochs of 15, vector dim of 300. We also used"
D19-1568,N16-1017,0,0.0761183,") (Miller et al., 1976; Chaiken, 1979, 1980); the traits and prior beliefs of the audience (G. Lord et al., 1979; Davies, 1998; Correll et al., 2004; Hullett, 2005); and the pragmatic context in which the argument is presented (i.e. kairos) (Haugtvedt and Wegener, 1994; Joyce and Harwood, 2014). Research in Natural Language Processing (NLP) has only partially corroborated these findings. One very influential line of work, for example, develops computational methods to automatically determine the linguistic characteristics of persuasive arguments (Habernal and Gurevych, 2016; Tan et al., 2016; Zhang et al., 2016), but it does so without controlling for the audience, the communicator or the pragmatic context. Very recent work, on the other hand, shows that attributes of both the audience and the communicator constitute important cues for determining argument strength (Lukin et al., 2017; Durmus and Cardie, 2018). They further show that audience and communicator attributes can influence the relative importance of linguistic features for predicting the persuasiveness of an argument. These results confirm previous findings in the social sciences that show a person’s perception of an argument can be influe"
D19-1568,P16-2034,0,0.0158591,"on, which they show to be competitive with deep learning classifiers in terms of accuracy. Their method represents a sequence of text as a bag of n-grams, and each n-gram is passed through a look-up table to get its dense vector representation. The overall sequence representation is simply an average over the dense representations of the bag of n-grams, and is fed into a linear classifier to predict the label. We use the code released by Joulin et al. (2017) to train a classifier for argument impact prediction, based on the claim text7 . 4.2.4 BiLSTM with Attention Another effective baseline (Zhou et al., 2016; Yang et al., 2016) for text classification consists of encoding the text sequence using a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), to get the token representations in context, and then attending (Luong et al., 2015) over the tokens to get the sequence representation. For the query vector for attention, we use a learned context vector, similar to 6 We pick the parameters for SVM model according to the performance validation split, and report the results on the test split. 7 We used maxNgram legnth of 2, learning rate of 0.8, num epochs of 15, vector dim"
D19-5804,P04-3031,0,0.221819,"l language exams, respectively. Experiments and Discussions Datasets in Section 2.1). We first fine-tune BERTLARGE for five epochs on RACE to get the pre-fine-tuned model and then further fine-tune the model for eight epochs on the target QA datasets in scientific domains. We show the accuracy of the prefine-tuned model on RACE in Table 4. We use the noun phrase chunker in spaCy2 to extract concept mentions. For information retrieval, we use the version 7.4.0 of Lucene (McCandless et al., 2010) and set the maximum number of the retrieved sentences K to 50. We use the stop word list from NLTK (Bird and Loper, 2004). In addition, we design two slightly different settings for information retrieval. In setting 1, the original reference corpus of each dataset is independent. Formally, for each dataset x ∈ D, we perform information retrieval based on the corresponding original reference corpus of x and/or the external corpus generated based on problems in x, where D = {ARC-Easy, ARC-Challenge, OpenBookQA}. In setting 2, all original reference corpora are integrated to further leverage external in-domain knowledge. Formally, for each dataset x ∈ D, we conduct information retrieval based on the given reference"
D19-5804,N19-1423,0,0.365628,"through magnetism” and “iron is always magnetic”}, as well as general world knowledge extracted from an external source such as {“a belt buckle is often made of iron” and “iron is metal”} are required. Thus, these QA tasks provide suitable testbeds for evaluating external knowledge exploitation and intergration. Previous subject-area QA methods (e.g., (Khot et al., 2017; Zhang et al., 2018; Zhong et al., 2018)) explore many ways of exploiting structured knowledge. Recently, we have seen that the framework of fine-tuning a pre-trained language model (e.g., GPT (Radford et al., 2018) and BERT (Devlin et al., 2019)) outperforms previous state-of* Equal contribution. This work was conducted when the two authors were at Tencent AI Lab, Bellevue, WA. 1 Ground truth facts are usually not provided in this kind of question answering tasks. 1 Introduction 27 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 27–37 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics sides, our promising results emphasize the importance of external unstructured knowledge for subject-area QA. We expect there is still much scope for further improvements by exploitin"
D19-5804,P17-1147,0,0.0411808,"activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained language model such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) on QA tasks (Radford et al., 2018; Devlin et al., 2019; Hu et al., 2019; Yang"
D19-5804,K17-1010,0,0.0528847,"et al. (2014)) are trained using pre-defined classes in general domain such as P ERSON, L OCATION, and O RGA NIZATION . However, in ARC and OpenBookQA, the vast majority of mentions are from scientific domains (e.g., “rotation”, “revolution”, “magnet”, and “iron”). Therefore, we simply consider all noun phrases as candidate concept mentions, which are extracted by a noun phrase chunker. For example, in the sample problem in Table 2, we extract concept mentions such as “Mercury”. Then each concept mention is disambiguated and linked to its corresponding concept (page) in Most previous methods (Khashabi et al., 2017; Musa et al., 2018; Ni et al., 2019; Yadav et al., 2019) perform information retrieval on the reference corpus to retrieve relevant sentences to form reference documents. In contrast, we retrieve relevant sentences from the combination of an opendomain resource and the original reference corpus to generate a reference document for each (question, answer option) pair. We still keep up to top K sentences for each reference document (Section 2.1). See the framework overview in Figure 1. 29 2.3 Utilization of External Knowledge from In-Domain Data Since there are a relatively small number of trai"
D19-5804,D18-1260,0,0.475044,"nell University, Ithaca, NY, USA 3 Tencent AI Lab, Bellevue, WA, USA obtained from sources outside of the text (McNamara et al., 2004; Salmer´on et al., 2006). It is perhaps not surprising then, that machine readers also require knowledge external to the text itself to perform well on question answering (QA) tasks. We focus on multiple-choice QA tasks in subject areas such as science, in which facts from the given reference corpus (e.g., a textbook) need to be combined with broadly applicable external knowledge to select the correct answer from the available options (Clark et al., 2016, 2018; Mihaylov et al., 2018). For convenience, we call these subject-area QA tasks. Abstract We focus on multiple-choice question answering (QA) tasks in subject areas such as science, where we require both broad background knowledge and the facts from the given subject-area reference corpus. In this work, we explore simple yet effective methods for exploiting two sources of external knowledge for subject-area QA. The first enriches the original subject-area reference corpus with relevant text snippets extracted from an open-domain resource (i.e., Wikipedia) that cover potentially ambiguous concepts in the question and a"
D19-5804,I17-1097,0,0.0238765,"leading to large forest fires”, instead of the real cause “humanity” supported by “the problem now is with anthropogenic climate change—that is, climate change caused by human activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a"
D19-5804,D18-1055,0,0.0207706,"Missing"
D19-5804,Q19-1026,0,0.0212927,"change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained language model such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) on QA tasks (Radford et al., 2018; Devlin et al., 2019; Hu et al., 2019; Yang et al., 2019). As shown in Table 5, the basel"
D19-5804,N19-1030,0,0.587879,"sks. 1 Introduction 27 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 27–37 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics sides, our promising results emphasize the importance of external unstructured knowledge for subject-area QA. We expect there is still much scope for further improvements by exploiting more sources of external knowledge, and we hope the present empirical study can serve as a new starting point for researchers to identify the remaining challenges in this area. the-art methods (Mihaylov et al., 2018; Ni et al., 2019). However, it is still not clear how to incorporate different sources of external knowledge, especially unstructured knowledge, into this powerful framework to further improve subject-area QA. We investigate two sources of external knowledge (i.e., open-domain and in-domain), which have proven effective for other types of QA tasks, by incorporating them into a pre-trained language model during the fine-tuning stage. First, we identify concepts in question and answer options and link these potentially ambiguous concepts to an open-domain resource that provides unstructured background informatio"
D19-5804,N15-1119,1,0.804061,"tion retrieval; MRC: machine reading comprehension). Q, O, q, oi , di , and n denote the set of all questions, the set of all answer options, a question, one of the answer options associated with question q, the document (formed by retrieved sentences) associated with the (q, oi ) pair, and the number of answer options of q, respectively. Wikipedia. For example, the ambiguous concept mention “Mercury” in Table 2 should be linked to the concept Mercury (planet) rather than Mercury (element) in Wikipedia. For concept disambiguation and linking, we simply adopt an existing unsupervised approach (Pan et al., 2015) that first selects high quality sets of concept collaborators to feed a simple similarity measure (i.e., Jaccard) to link concept mentions. Question: Mercury, the planet nearest to the Sun, has extreme surface temperatures, ranging from 465◦ C in sunlight to −180◦ C in darkness. Why is there such a large range of temperatures on Mercury? A. The planet is too small to hold heat. B. The planet is heated on only one side. C. The planet reflects heat from its dark side. D. The planet lacks an atmosphere to hold heat. X Table 2: A sample problem from the ARC-Challenge dataset (Clark et al., 2018)"
D19-5804,D17-1082,0,0.346799,"sentences using the non-stop words in q and oi as the query and then concatenate the retrieved sentences to form di (Sun et al., 2019). The final prediction for each question is obtained by a linear plus softmax layer over the output of the final hidden state of the first token in each input sequence. By default, we employ the following two-step fine-tuning approach unless explicitly specified. Following previous work (Sun et al., 2019) based on GPT (Radford et al., 2018), we first finetune BERT (Devlin et al., 2019) on a large-scale multiple-choice machine reading comprehension dataset RACE (Lai et al., 2017) collected from English-as-a-foreign-language exams, which provides a ground truth reference document instead of a reference corpus for each question. Then, we further fine-tune the model on the target multiplechoice science QA datasets. For convenience, we call the model obtained after the first fine-tuning phase as a pre-fine-tuned model. We conduct experiments on three challenging multiple-choice science QA tasks where existing methods stubbornly continue to exhibit performance gaps in comparison with humans: ARC-Easy, ARC-Challenge (Clark et al., 2016, 2018), and OpenBookQA (Mihaylov et al"
D19-5804,D18-1053,0,0.0549991,"Missing"
D19-5804,D16-1264,0,0.190821,"Missing"
D19-5804,D15-1236,0,0.106165,"d science QA datasets. As shown in Figure 2, we see that the performance drops dramatically without using pre-fine-tuning on the RACE dataset. 4 4.1 Utilization of External Knowledge for Subject-Area QA Previous studies have explored many ways to leverage structured knowledge to solve questions in subject areas such as science exams. Many researchers investigate how to directly or indirectly use automatically constructed knowledge bases/graphs from reference corpora (Khot et al., 2017; Kwon et al., 2018; Khashabi et al., 2018; Zhang et al., 2018) or existing external general knowledge graphs (Li and Clark, 2015; Sachan et al., 2016; Wang et al., 2018a,c; Zhong et al., 2018; Musa et al., 2018) such as ConceptNet (Speer et al., 2017). However, for subject-area QA, unstructured knowledge is seldom considered in previous studies, and it is still not clear the usefulness of this kind of knowledge. As far as we know, for subject-area QA tasks, this is the first attempt to impart sources of external unstructured knowledge into one state-of-theart pre-trained language model, and we are among the first to investigate the effectiveness of the exRelated Work Subject-Area QA Tasks and Methods As there is not a"
D19-5804,P16-2076,0,0.0294061,"s. As shown in Figure 2, we see that the performance drops dramatically without using pre-fine-tuning on the RACE dataset. 4 4.1 Utilization of External Knowledge for Subject-Area QA Previous studies have explored many ways to leverage structured knowledge to solve questions in subject areas such as science exams. Many researchers investigate how to directly or indirectly use automatically constructed knowledge bases/graphs from reference corpora (Khot et al., 2017; Kwon et al., 2018; Khashabi et al., 2018; Zhang et al., 2018) or existing external general knowledge graphs (Li and Clark, 2015; Sachan et al., 2016; Wang et al., 2018a,c; Zhong et al., 2018; Musa et al., 2018) such as ConceptNet (Speer et al., 2017). However, for subject-area QA, unstructured knowledge is seldom considered in previous studies, and it is still not clear the usefulness of this kind of knowledge. As far as we know, for subject-area QA tasks, this is the first attempt to impart sources of external unstructured knowledge into one state-of-theart pre-trained language model, and we are among the first to investigate the effectiveness of the exRelated Work Subject-Area QA Tasks and Methods As there is not a clear distinction bet"
D19-5804,P18-1161,0,0.0703199,"Missing"
D19-5804,P14-5010,0,0.00309586,"can serve as a reliable piece of evidence to infer the correct answer option D for the question in Table 2. Just as human readers activate their background knowledge related to the text materials (Kendeou and Van Den Broek, 2007), we link concepts identified in questions and answer options to an opendomain resource (i.e., Wikipedia) and provide machine readers with unstructured background information relevant to these concepts, used to enrich the original reference corpus. Concept Identification and Linking: We first extract concept mentions from texts. Most mention extraction systems (e.g., Manning et al. (2014)) are trained using pre-defined classes in general domain such as P ERSON, L OCATION, and O RGA NIZATION . However, in ARC and OpenBookQA, the vast majority of mentions are from scientific domains (e.g., “rotation”, “revolution”, “magnet”, and “iron”). Therefore, we simply consider all noun phrases as candidate concept mentions, which are extracted by a noun phrase chunker. For example, in the sample problem in Table 2, we extract concept mentions such as “Mercury”. Then each concept mention is disambiguated and linked to its corresponding concept (page) in Most previous methods (Khashabi et a"
D19-5804,S18-1120,0,0.138342,"ence corpora are integrated to further leverage external in-domain knowledge. Formally, for each dataset x ∈ D, we conduct information retrieval based on the given reference corpus of D and/or the external corpus generated based on problems in D instead of x.3 . In our experiment, we use RACE (Lai et al., 2017) — the largest existing multiple-choice machine reading comprehension dataset collected from real and practical language exams — in the pre-finetuning stage. Questions in RACE focus on evaluating linguistic knowledge acquisition of participants and are commonly used in previous methods (Wang et al., 2018a; Sun et al., 2019). We evaluate the performance of our methods on three multiple-choice science QA datasets: ARC-Easy, ARC-Challenge, and OpenBookQA. ARC-Challenge and ARC-easy originate from the same set of exam problems collected from multiple sources. ARC-Challenge contains questions answered incorrectly by both a retrieval-based method and a word co-occurrence method, and the remaining questions form ARC-Easy. Questions in OpenBookQA are crowdsourced by turkers and then carefully filtered and modified by experts. See the statistics of these datasets in Table 3. Note that for OpenBookQA,"
D19-5804,W17-4413,0,0.028917,"fires”, instead of the real cause “humanity” supported by “the problem now is with anthropogenic climate change—that is, climate change caused by human activity, which is making the climate change a lot faster than it normally would”. for convenience we call a task in which there is no reference document provided for each instance as a QA task. In this paper, we focus on multiple-choice subject-area QA tasks, where the in-domain reference corpus does not provide sufficient relevant content on its own to answer a significant portion of the questions (Clark et al., 2016; Kobayashi et al., 2017; Welbl et al., 2017; Clark et al., 2018; Mihaylov et al., 2018). In contrast to other types of QA scenarios (Nguyen et al., 2016; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), in this setting: (1) the reference corpus does not reliably contain text spans from which the answers can be drawn, and (2) it does not provide sufficient information on its own to answer a significant portion of the questions. Thus they are suitable for us to study how to exploit external knowledge for QA. Our work follows the general framework of discriminatively fine-tuning a pre-trained languag"
D19-5804,N19-1274,0,0.223526,"to this powerful framework to further improve subject-area QA. We investigate two sources of external knowledge (i.e., open-domain and in-domain), which have proven effective for other types of QA tasks, by incorporating them into a pre-trained language model during the fine-tuning stage. First, we identify concepts in question and answer options and link these potentially ambiguous concepts to an open-domain resource that provides unstructured background information relevant to the concepts and used to enrich the original reference corpus (Section 2.2). In comparison to previous work (e.g., (Yadav et al., 2019)), we perform information retrieval based on the enriched corpus instead of the original one to form a document for answering a question. Second, we increase the amount of training data by appending additional in-domain subject-area QA datasets (Section 2.3). 2 Method In this section, we first introduce our BERT-based QA baseline (Section 2.1). Then, we present how we incorporate external open-domain (Section 2.2) and in-domain (Section 2.3) sources of knowledge into the baseline. 2.1 Baseline Framework Given a question q, an answer option oi , and a reference document di , we concatenate them"
D19-5804,N19-4013,0,0.0363013,"Missing"
D19-5804,P06-4018,0,\N,Missing
D19-5804,P16-1223,0,\N,Missing
D19-5804,P17-2049,0,\N,Missing
D19-5804,N18-1202,0,\N,Missing
D19-5804,S18-1119,0,\N,Missing
D19-5804,D18-1453,0,\N,Missing
D19-5804,N19-1270,1,\N,Missing
H01-1054,H01-1065,0,0.0170363,"ocuments into a coherent event-oriented view, though considerable challenges remain to be addressed in this area. The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al. [7] and Radev et al. [11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary. It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set. As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization. Having also noted this problem, we have focused our efforts in this area on attempting to balance coherence and informativeness in selecting sets of sentences to include in the summary. In ongoing work, we are investigating techniques for improving merging accuracy and summary fluency in the context of summarizing the more than 150 news articles we have collecte"
H01-1054,W00-0405,0,0.0152076,"ortant difference is that SUMMONS sidestepped the problem of comparing reported numbers of varying specificity (e.g. several thousand vs. anywhere from 2000 to 5000 vs. up to 4000 vs. 5000), whereas we have implemented rules for doing so. Finally, we have begun to address some of the difficult issues that arise in merging information from multiple documents into a coherent event-oriented view, though considerable challenges remain to be addressed in this area. The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al. [7] and Radev et al. [11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary. It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set. As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization. Having also n"
H01-1054,J93-2004,0,0.0355149,"o template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89). The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text. The RIPTIDES system for the most part employs a traditional IE architecture [4]. In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations. A preprocessor first finds sentences and tokens. For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems. Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document. The extraction phase of the system identifies domain-specific relations among relevant entities in the text. It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns. Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training — one set comprises texts relevant to the domain of"
H01-1054,J98-3005,0,0.690231,"earch efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques. This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization. (RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization.) Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques. In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system. We conclude with a brief discussion of related and ongoing work. 2. SYSTEM DESIGN Figure 1 depicts the IE-supported summarization system. The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario te"
H01-1054,W98-1428,1,0.601029,"as however, nevertheless, etc. We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid. Figure 4. 200 word summary of actual IE output, with emphasis on Red Cross cases. We then perform a randomized local search for a good set of sentences according to these scoring criteria. 2.2.4 Implementation The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex’s Exemplars Framework [13]. The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations. In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a “rough draft” of the summary text. In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text. In the second generation step, the Exemplars text polishing component makes use of this markup to trigger s"
H01-1054,A00-2018,0,\N,Missing
H05-1045,P98-1013,0,0.00645926,"Missing"
H05-1045,A97-1029,0,0.0683494,"Missing"
H05-1045,C04-1018,1,0.565484,"t in methods for automatically identifying opinions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 355–362, Vancouver, October 2005. 2005 Association for Computational Linguistics S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US officials want the EU to prevail. In S1, the phrase “Taiwan-born voters”"
H05-1045,W03-0430,0,0.0131654,"rpetrator and the person who is the victim. We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent. We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning. In particular, we consider Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a). While CRFs treat source identification as a sequence tagging task, AutoSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence. We hypothesized that a combination of the two techniques would perform better than either one alone. Section 3 describes the CRF approach to identifying opinion sources and the features that the system uses. Section 4 then presents a new variation of Aut"
H05-1045,J05-1004,0,0.0139477,"Missing"
H05-1045,W02-1011,0,0.0421764,"Missing"
H05-1045,P04-1035,0,0.246996,"Missing"
H05-1045,W03-1014,1,0.638262,"Missing"
H05-1045,P02-1053,0,0.0160953,"Missing"
H05-1045,H05-2018,1,0.453659,"troduction In recent years, there has been a great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 355–362, Vancouver, October 2005. 2005 Association for Computational Linguistics S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US offi"
H05-1045,W03-1017,0,0.237929,"Missing"
H05-1045,J03-4003,0,\N,Missing
H05-1045,C98-1013,0,\N,Missing
H05-1068,C04-1018,1,0.77882,"ances (Kubat and Matwin, 1997). Conceivably, an automatic PSF extractor with high precision and mediocre recall could be used to automate the annotation process. For this reason we measure the performance with an unbalanced Fmeasure that emphasizes precision. Specifically, we try β = 0.5 (F0.5) and β = 0.2 (F0.2). Data Set: We use 400 documents from the MPQA corpus (2002), a collection of news stories manually annotated with PSF information. The 400 documents are randomly split to get 320 training, 40 tuning, and 40 testing documents. 4.3 Determining PSF Hierarchy The third task is taken from Breck and Cardie (2004). Explicit PSF’s each have a source that corresponds to the person or entity expressing the subjectivity. In the presence of second-hand reporting, sources are often nested. This has the effect of filtering subjectivity through a chain of sources. Given sentences annotated with PSF information (i.e. which spans are PSF’s), the task is to discover 543 the hierarchy among the PSF’s that corresponds to the nesting of their respective sources. From each sentence, multiple instances are created by pairing every PSF with every other PSF in the sentence.4 Let (P SFparent , P SFtarget ) denote one of"
H05-1068,daelemans-hoste-2002-evaluation,0,0.0639344,"r understood. However, ensemble selection is perhaps trustworthy enough to optimize metrics that are difficult to overfit and could not be easily optimized otherwise — in our case, the task-specific aggregate performance measures. More work is needed to understand when ensemble selection can be safely used for NLL. 6 References Related Work Hoste et al. (2002) and Hoste (2005) study the impact of tuning parameters for k-NN and a rulelearning algorithm on word sense disambiguation and coreference resolution, respectively, and find that parameter settings greatly change results. Similar work by Daelemans and Hoste (2002) shows the fallacy of comparing algorithm performance without first tuning parameters. They find that the best algorithm for a task frequently changes after optimizing parameters. In contrast to our work, these earlier experiments investigate at most two algorithms and only measure performance with one metric per task. 7 Conclusion We evaluate an ensemble selection framework that enables optimizing classifier performance to arbitrary performance metrics without re-training. An important side benefit of the framework is the fully automatic production of base-level models, removing the need for"
H05-1068,P02-1014,1,0.843412,"s for these metrics. 4 Tasks Because of space issues, we necessarily provide only brief descriptions of each NLL task. Readers are referred to the cited papers to obtain detailed descriptions. 4.1 Noun Phrase Coreference Resolution The goal for a standard noun phrase coreference resolution system is to identify the noun phrases in a document and determine which of them refer to the same entity. Entities can be people, places, things, etc. The resulting partitioning of noun phrases creates reference chains with one chain per entity. We use the same problem formulation as Soon et al. (2001) and Ng and Cardie (2002) — a combination of classification and clustering. Briefly, every noun phrase is paired with all preceding noun phrases, creating multiple pairs. For the training data, the pairs are labeled as coreferent or not. A binary classifier is trained to predict the pair labels. During classification, the predicted labels are used 542 to form clusters. Two noun phrases A and B share a cluster if they are either predicted as coreferent by the classifier or if they are transitively predicted as coreferent through one or more other noun phrases. Instance selection (Soon et al., 2001; Ng, 2004) is used to"
H05-1068,J01-4004,0,0.163289,"n only iterates 50 times for these metrics. 4 Tasks Because of space issues, we necessarily provide only brief descriptions of each NLL task. Readers are referred to the cited papers to obtain detailed descriptions. 4.1 Noun Phrase Coreference Resolution The goal for a standard noun phrase coreference resolution system is to identify the noun phrases in a document and determine which of them refer to the same entity. Entities can be people, places, things, etc. The resulting partitioning of noun phrases creates reference chains with one chain per entity. We use the same problem formulation as Soon et al. (2001) and Ng and Cardie (2002) — a combination of classification and clustering. Briefly, every noun phrase is paired with all preceding noun phrases, creating multiple pairs. For the training data, the pairs are labeled as coreferent or not. A binary classifier is trained to predict the pair labels. During classification, the predicted labels are used 542 to form clusters. Two noun phrases A and B share a cluster if they are either predicted as coreferent by the classifier or if they are transitively predicted as coreferent through one or more other noun phrases. Instance selection (Soon et al., 2"
H05-1068,M95-1005,0,0.204617,"f positive instances in the training set.3 We use the learning features described by Ng and Cardie (2002). All learning algorithms are trained with the full set of features. Additionally, the rule learner, SVM, and LR are also trained with a hand-selected subset of the features that Ng and Cardie (2002) find to outperform the full feature set. Essentially this is an additional parameter to set for the learning task. Special Metrics: Rather than focusing on performance at the pairwise coreference classification level, performance for this task is typically reported using either the MUC metric (Vilain et al., 1995) or the BCUBED metric (Bagga and Baldwin, 1998). Both of these metrics measure the degree that predicted coreference chains agree with an answer key. In particular they measure the chain-level precision and recall (and the corresponding F-measure). We abbreviate these metrics MUC-F1, and B3F1. Data Set: For our experiments we use the MUC6 corpus, which contains 60 documents annotated with coreference information. The training, tuning, and test sets consist of documents 1-20, 21-30, and 3 Soon-1 instance selection is used for all algorithms; we also use soon-2 (Ng, 2004) instance selection for"
H05-1116,P04-1035,0,0.241773,"ion 5 briefly describes an opinion annotation scheme used in the experiments. Sections 6 and 7 explore the use of opinion information in the design of MPQA systems. 2 Related Work There is a growing interest in methods for the automatic identification and extraction of opinions, emotions, and sentiments in text. Much of the relevant research explores sentiment classification, a text categorization task in which the goal is to assign to a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Other research has concentrated on analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)). Related work in the area of corpus development includes Wiebe et al.’s (2005) opinion annotation scheme to identify subjective expressions — expressions used to ex"
H05-1116,W02-1011,0,0.028931,"ic issues for handling opinion vs. fact questions. Section 5 briefly describes an opinion annotation scheme used in the experiments. Sections 6 and 7 explore the use of opinion information in the design of MPQA systems. 2 Related Work There is a growing interest in methods for the automatic identification and extraction of opinions, emotions, and sentiments in text. Much of the relevant research explores sentiment classification, a text categorization task in which the goal is to assign to a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Other research has concentrated on analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)). Related work in the area of corpus development includes Wiebe et al.’s (2005) opinion annotation scheme to"
H05-1116,W03-1014,1,0.458677,"text. Much of the relevant research explores sentiment classification, a text categorization task in which the goal is to assign to a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Other research has concentrated on analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)). Related work in the area of corpus development includes Wiebe et al.’s (2005) opinion annotation scheme to identify subjective expressions — expressions used to express opinions, emotions, sentiments and other private states in text. Wiebe et al. have applied the annotation scheme to create the MPQA corpus consisting of 535 documents manually annotated for phrase-level expressions of opinion. In addition, the NIST-sponsored TREC evaluation has begun to develop data focu"
H05-1116,P02-1053,0,0.00514181,"ng opinion vs. fact questions. Section 5 briefly describes an opinion annotation scheme used in the experiments. Sections 6 and 7 explore the use of opinion information in the design of MPQA systems. 2 Related Work There is a growing interest in methods for the automatic identification and extraction of opinions, emotions, and sentiments in text. Much of the relevant research explores sentiment classification, a text categorization task in which the goal is to assign to a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Other research has concentrated on analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)). Related work in the area of corpus development includes Wiebe et al.’s (2005) opinion annotation scheme to identify subjec"
H05-1116,W03-1017,0,0.728753,"on task in which the goal is to assign to a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Other research has concentrated on analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)). Related work in the area of corpus development includes Wiebe et al.’s (2005) opinion annotation scheme to identify subjective expressions — expressions used to express opinions, emotions, sentiments and other private states in text. Wiebe et al. have applied the annotation scheme to create the MPQA corpus consisting of 535 documents manually annotated for phrase-level expressions of opinion. In addition, the NIST-sponsored TREC evaluation has begun to develop data focusing on opinions — the 2003 Novelty Track features a task that requires sys924 tems to identify op"
H05-1116,W03-2102,1,\N,Missing
H05-2018,H05-1045,1,0.340187,"generated from a large corpus of unannotated data by two high-precision, rule-based classifiers. Speech Events and Direct Subjective Expression Classification The second component identifies speech events (e.g., “said,” “according to”) and direct subjective expressions (e.g., “fears,” “is happy”). Speech events include both speaking and writing events. Direct subjective expressions are words or phrases where an opinion, emotion, sentiment, etc. is directly described. A high-precision, rule-based classifier is used to identify these expressions. Related Work Please see (Wiebe and Riloff, 2005; Choi et al., 2005; Wilson et al., 2005) for discussions of related work in automatic opinion and sentiment analysis. 4 Acknowledgments This work was supported by the Advanced Research and Development Activity (ARDA), by the NSF under grants IIS-0208028, IIS-0208798 and IIS0208985, and by the Xerox Foundation. 2.3.2 2.3.3 Opinion Source Identification The third component is a source identifier that combines a Conditional Random Field sequence tagging model (Lafferty et al., 2001) and extraction pattern learning (Riloff, 1996) to identify the sources of speech events and subjective expressions (Choi et al., 2005"
H05-2018,P97-1003,0,0.0192817,"tering out opinionated sentences (Riloff et al., 2005). System Architecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time required for parsing. Finally, a clue-finder is run to identify words and phrases from a large subjective language lexicon. 2.3 Subjectivity Analysis The subjectivity analysis has four components. The first classifier focuses on identifying sentiment"
H05-2018,W03-1014,1,0.673444,"from knowledge of subjective language include systems that summarize the various viewpoints in a document or that mine product reviews. Even typical fact-oriented applications, such as information extraction, can benefit from subjectivity analysis by filtering out opinionated sentences (Riloff et al., 2005). System Architecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time requir"
H05-2018,H05-1044,1,0.169551,"rge corpus of unannotated data by two high-precision, rule-based classifiers. Speech Events and Direct Subjective Expression Classification The second component identifies speech events (e.g., “said,” “according to”) and direct subjective expressions (e.g., “fears,” “is happy”). Speech events include both speaking and writing events. Direct subjective expressions are words or phrases where an opinion, emotion, sentiment, etc. is directly described. A high-precision, rule-based classifier is used to identify these expressions. Related Work Please see (Wiebe and Riloff, 2005; Choi et al., 2005; Wilson et al., 2005) for discussions of related work in automatic opinion and sentiment analysis. 4 Acknowledgments This work was supported by the Advanced Research and Development Activity (ARDA), by the NSF under grants IIS-0208028, IIS-0208798 and IIS0208985, and by the Xerox Foundation. 2.3.2 2.3.3 Opinion Source Identification The third component is a source identifier that combines a Conditional Random Field sequence tagging model (Lafferty et al., 2001) and extraction pattern learning (Riloff, 1996) to identify the sources of speech events and subjective expressions (Choi et al., 2005). The source of a spe"
H05-2018,H01-1014,0,0.0113786,"tecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time required for parsing. Finally, a clue-finder is run to identify words and phrases from a large subjective language lexicon. 2.3 Subjectivity Analysis The subjectivity analysis has four components. The first classifier focuses on identifying sentiment expressions. The second classifier takes the sentiment expressions and iden"
L18-1257,P12-2041,0,0.0338394,"ifies specific premises for the given conclusion, as well as critical questions that can be used to Argument Quality Assessment Measuring the quality of argument has long been a subject of discussion and research, leading to a variety of dimensions of quality (Toulmin, 1958; Perelman et al., 1969; van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006; Wachsmuth et al., 2017). More recently, argument mining research is conducted with specific measures of quality depending on the domain and purpose, such as persuasiveness (Tan et al., 2016), strength (Persing and Ng, 2015), acceptability (Cabrio and Villata, 2012), and convincingness (Habernal and Gurevych, 2016). The measure of quality we are interested in is evaluability (Park et al., 2015). By examining arguments’ evaluability, we aim to identify ways to improve them so that they can be better understood and evaluated. For example, we answer questions like, “Which propositions need additional reasons or evidence supporting them?” This is the type of constructive feedback that can help commenter improve their arguments, unlike quality measures that results in a single numeric score without specifying how an argument can be improved. 3 Annotation Sche"
L18-1257,P16-1150,0,0.0854261,"n, as well as critical questions that can be used to Argument Quality Assessment Measuring the quality of argument has long been a subject of discussion and research, leading to a variety of dimensions of quality (Toulmin, 1958; Perelman et al., 1969; van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006; Wachsmuth et al., 2017). More recently, argument mining research is conducted with specific measures of quality depending on the domain and purpose, such as persuasiveness (Tan et al., 2016), strength (Persing and Ng, 2015), acceptability (Cabrio and Villata, 2012), and convincingness (Habernal and Gurevych, 2016). The measure of quality we are interested in is evaluability (Park et al., 2015). By examining arguments’ evaluability, we aim to identify ways to improve them so that they can be better understood and evaluated. For example, we answer questions like, “Which propositions need additional reasons or evidence supporting them?” This is the type of constructive feedback that can help commenter improve their arguments, unlike quality measures that results in a single numeric score without specifying how an argument can be improved. 3 Annotation Scheme The annotators annotated the elementary units a"
L18-1257,L16-1617,1,0.843227,"he sheer number of argument schemes also causes additional challenges in gathering enough examples for each scheme. In this work, we adopt a model uniquely designed to capture the evaluability of arguments, which is general enough to model diverse argumentative structures that appear in practical argumentation (Park et al., 2015). Argument mining systems also differ in the domain, resulting in datasets consisting of newspaper articles (Reed et al., 2008), legal documents (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014), and eRulemaking user comments (Park and Cardie, 2014; Konat et al., 2016), to name a few. While ours is not the first eRulemaking dataset, the task is different; Park and Cardie (2014) targets elementary unit classification only, and Konat et al. (2016) focuses on identifying divisive issues between commenters by analyzing conflict relations found across multiple comments in a thread. In contrast, we examine support structures within a comment; our dataset contains both elementary unit and support relation annotation without cross-comment conflict annotation. Also, the user comments comprising our dataset are different from those in the aforementioned datasets. 2.2"
L18-1257,P17-1091,1,0.515022,"Missing"
L18-1257,W14-2105,1,0.853684,"matic classification. The sheer number of argument schemes also causes additional challenges in gathering enough examples for each scheme. In this work, we adopt a model uniquely designed to capture the evaluability of arguments, which is general enough to model diverse argumentative structures that appear in practical argumentation (Park et al., 2015). Argument mining systems also differ in the domain, resulting in datasets consisting of newspaper articles (Reed et al., 2008), legal documents (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014), and eRulemaking user comments (Park and Cardie, 2014; Konat et al., 2016), to name a few. While ours is not the first eRulemaking dataset, the task is different; Park and Cardie (2014) targets elementary unit classification only, and Konat et al. (2016) focuses on identifying divisive issues between commenters by analyzing conflict relations found across multiple comments in a thread. In contrast, we examine support structures within a comment; our dataset contains both elementary unit and support relation annotation without cross-comment conflict annotation. Also, the user comments comprising our dataset are different from those in the aforeme"
L18-1257,P15-1053,0,0.0480952,"are in use; each argument scheme specifies specific premises for the given conclusion, as well as critical questions that can be used to Argument Quality Assessment Measuring the quality of argument has long been a subject of discussion and research, leading to a variety of dimensions of quality (Toulmin, 1958; Perelman et al., 1969; van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006; Wachsmuth et al., 2017). More recently, argument mining research is conducted with specific measures of quality depending on the domain and purpose, such as persuasiveness (Tan et al., 2016), strength (Persing and Ng, 2015), acceptability (Cabrio and Villata, 2012), and convincingness (Habernal and Gurevych, 2016). The measure of quality we are interested in is evaluability (Park et al., 2015). By examining arguments’ evaluability, we aim to identify ways to improve them so that they can be better understood and evaluated. For example, we answer questions like, “Which propositions need additional reasons or evidence supporting them?” This is the type of constructive feedback that can help commenter improve their arguments, unlike quality measures that results in a single numeric score without specifying how an a"
L18-1257,reed-etal-2008-language,0,0.0331277,"2001). Having many specific premises, a subset of which may not be present in the text, makes it difficult for manual annotation and automatic classification. The sheer number of argument schemes also causes additional challenges in gathering enough examples for each scheme. In this work, we adopt a model uniquely designed to capture the evaluability of arguments, which is general enough to model diverse argumentative structures that appear in practical argumentation (Park et al., 2015). Argument mining systems also differ in the domain, resulting in datasets consisting of newspaper articles (Reed et al., 2008), legal documents (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014), and eRulemaking user comments (Park and Cardie, 2014; Konat et al., 2016), to name a few. While ours is not the first eRulemaking dataset, the task is different; Park and Cardie (2014) targets elementary unit classification only, and Konat et al. (2016) focuses on identifying divisive issues between commenters by analyzing conflict relations found across multiple comments in a thread. In contrast, we examine support structures within a comment; our dataset contains both elementary unit and support relation"
L18-1257,C14-1142,0,0.208026,"e text, makes it difficult for manual annotation and automatic classification. The sheer number of argument schemes also causes additional challenges in gathering enough examples for each scheme. In this work, we adopt a model uniquely designed to capture the evaluability of arguments, which is general enough to model diverse argumentative structures that appear in practical argumentation (Park et al., 2015). Argument mining systems also differ in the domain, resulting in datasets consisting of newspaper articles (Reed et al., 2008), legal documents (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014), and eRulemaking user comments (Park and Cardie, 2014; Konat et al., 2016), to name a few. While ours is not the first eRulemaking dataset, the task is different; Park and Cardie (2014) targets elementary unit classification only, and Konat et al. (2016) focuses on identifying divisive issues between commenters by analyzing conflict relations found across multiple comments in a thread. In contrast, we examine support structures within a comment; our dataset contains both elementary unit and support relation annotation without cross-comment conflict annotation. Also, the user comments comprisi"
L18-1257,E17-1017,0,0.0529428,"room for multiple interpretations. For example, according to Eemeren et al. (1987), warrant is indistinguishable from data. On the other hand, argument schemes capture specific patterns of argument that are in use; each argument scheme specifies specific premises for the given conclusion, as well as critical questions that can be used to Argument Quality Assessment Measuring the quality of argument has long been a subject of discussion and research, leading to a variety of dimensions of quality (Toulmin, 1958; Perelman et al., 1969; van Eemeren and Grootendorst, 2004; Johnson and Blair, 2006; Wachsmuth et al., 2017). More recently, argument mining research is conducted with specific measures of quality depending on the domain and purpose, such as persuasiveness (Tan et al., 2016), strength (Persing and Ng, 2015), acceptability (Cabrio and Villata, 2012), and convincingness (Habernal and Gurevych, 2016). The measure of quality we are interested in is evaluability (Park et al., 2015). By examining arguments’ evaluability, we aim to identify ways to improve them so that they can be better understood and evaluated. For example, we answer questions like, “Which propositions need additional reasons or evidence"
N03-1023,W01-0501,1,\N,Missing
N03-1023,M95-1005,0,\N,Missing
N03-1023,E03-1008,0,\N,Missing
N03-1023,W02-1008,1,\N,Missing
N03-1023,N01-1023,0,\N,Missing
N03-1023,P01-1005,0,\N,Missing
N03-1023,J01-4004,0,\N,Missing
N03-1023,W99-0613,0,\N,Missing
N03-1023,P02-1046,0,\N,Missing
N03-1023,P02-1045,0,\N,Missing
N07-1009,J96-1002,0,0.0057634,"5), Wellner et al. (2004)). Solutions of the first type replace the computation  of the global normalization factor y p(y|x) with argmaxy p(y|x) during training, since finding an argmax of a probability distribution is often an easier problem than finding the entire probability distribution. Training via the voted perceptron algorithm (Collins, 2002) or using a max-margin criterion also correspond to the first option (e.g. McCallum and Wellner (2004), Finley and Joachims (2005)). But without the global normalization, the maximumlikelihood criterion motivated by the maximum entropy principle (Berger et al., 1996) is no longer a feasible option as an optimization criterion. The second solution simplifies the graph structure for training, and applies complex global inference only for testing. In spite of the discrepancy between the training model and the testing model, it has been empirically shown that (1) performing global inference only during testing can improve performance (e.g. Finkel et al. (2005), Roth and Yih (2005)), and (2) full-blown global training can often perform worse due to insufficient training data (e.g. Punyakanok et al. (2005)). Importantly, however, attempts to reduce the discrepa"
N07-1009,W02-1001,0,0.0180331,"great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)). For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard. Previous approaches for training CRFs have either (1) opted for a training method that no longer maximizes the likelihood, (e.g. McCallum and Wellner (2004), Roth and Yih (2005)) 1 , or (2) opted for a 1 Both McCallum and Wellner (2004) and Roth and Yih (2005) used the voted perceptron algorithm (Collins, 2002) to train intractable CRFs. simplified graph structure to avoid intractable global normalization (e.g. Roth and Yih (2005), Wellner et al. (2004)). Solutions of the first type replace the computation  of the global normalization factor y p(y|x) with argmaxy p(y|x) during training, since finding an argmax of a probability distribution is often an easier problem than finding the entire probability distribution. Training via the voted perceptron algorithm (Collins, 2002) or using a max-margin criterion also correspond to the first option (e.g. McCallum and Wellner (2004), Finley and Joachims (20"
N07-1009,P05-1045,0,0.0259861,"idden variables are used to capture interactions between local inference and global inference. Furthermore, we introduce biased potential functions that empirically drive CRFs towards performance improvements w.r.t. the preferred evaluation measure for the learning task. We report promising experimental results on two coreference data sets using two task-specific evaluation measures. 1 Introduction Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al. (2004), Finkel et al. (2005)). For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard. Previous approaches for training CRFs have either (1) opted for a training method that no longer maximizes the likelihood, (e.g. McCallum and Wellner (2004), Roth and Yih (2005)) 1 , or (2) opted for a 1 Both McCallum and Wellner (2004) and Roth and Yih (2005) used the voted perceptron algorithm (Collins, 2002) to train intractable CRFs. simplified graph structure to avoid intractable global normalization (e."
N07-1009,P02-1014,1,0.743149,"M y ← 1 else i yi ← 0 end for return y h∗ ← argmaxh P (h|x) h ← single-link-clustering(h∗ ) for each hi ∈ h if hi = yi∗ y  ← h∗i else i yi ← yi∗ end for return y Figure 2: Algorithm to find a high confidence labeling y that is close to the true labeling y∗ Figure 1: Algorithm to find the highest confidence labeling y that can be clustered to the true labeling y∗ [Global Model P (y|h)] For the global model, we assume a deterministic clustering algorithm is given. In particular, we focus on single-link clustering, as it has been shown to be effective for coreference resolution (e.g. Ng and Cardie (2002)). With single-link clustering, P (y|h) = 1 if h can be clustered to y, and P (y|h) = 0 if h cannot be clustered to y.5 [Computation of the E-step] The E-step requires computation of the distribution of P (h|y, x, θ(t−1) ), which we will simply denote as P (h|y, x), since all our distributions are implicitly conditioned on the model parameters θ. P (h|y, x) = x, true labeling y∗ , current local model P (h|x) Find a high confidence labeling y that is close to the true labeling y∗ P (h, y|x) ∝ P (y|h) P (h|x) P (y|x) Notice that when computing P (h|y, x), the denominator P (y|x) stays as a cons"
N07-1009,W06-1640,1,0.896293,"Missing"
N13-1053,P12-2034,0,0.287111,"ight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et al. (2011), we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011), whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation on the training data. Results appear in Table 2. Each row lists the sen"
N13-1053,P09-2078,0,0.24052,"agreement computed using Fleiss’ kappa is only slight at 0.07 (Landis and Koch, 1977). Furthermore, based on Cohen’s kappa, the highest pairwise interannotator agreement is only 0.26, between JUDGE 1 and JUDGE 2. These low agreements suggest that while the judges may perform statistically better than chance, they are identifying different reviews as deceptive, i.e., few reviews are consistently identified as deceptive. 3.2 Automated Classifier Performance Standard n-gram–based text categorization techniques have been shown to be effective at detecting deception in text (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Ott et al., 2011; Feng et al., 2012). Following Ott et al. (2011), we evaluate the performance of linear Support Vector Machine (SVM) classifiers trained with unigram and bigram term-frequency features on our novel negative deceptive opinion spam dataset. We employ the same 5-fold stratified cross-validation (CV) procedure as Ott et al. (2011), whereby for each cross-validation iteration we train our model on all reviews for 16 hotels, and test our model on all reviews for the remaining 4 hotels. The SVM cost parameter, C, is tuned by nested cross-validation on the training data. Results app"
N13-1053,P11-1032,1,0.727569,"l.edu jeff.hancock@cornell.edu Abstract The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM—fictitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship. 1 Introduction Consumer’s purchase decisions are increasingly influenced by use"
N15-1112,chang-manning-2012-sutime,0,0.0218669,"r CNN articles via Disqus API. 1 NYT comments come with information on whether a comment is an editor’s-pick. The statistics on the four datasets are displayed in Table 1.2 MH370 Ukraine Israel-Gaza NSA Time Span # Articles # Comments 03/08 - 06/30 955 406,646 03/08 - 06/30 3,779 646,961 07/20 - 09/30 909 322,244 03/23 - 06/30 145 60,481 Table 1: Statistics on the four event datasets. We extract parse trees, dependency trees, and coreference resolution results of articles and comments with Stanford CoreNLP (Manning et al., 2014). Sentences in articles are labeled with timestamps using SUTime (Chang and Manning, 2012). We also collect all articles with comments from NYT in 2013 (henceforth NYT2013) to form a training set for learning importance scoring functions on articles sentences and comments (see Section 3). NYT2013 contains 3, 863 articles and 833, 032 comments. 3 Joint Learning for Importance Scoring We first introduce a joint learning method that uses graph-based regularization to simultaneously learn two functions — a SENTENCE scorer and a COM MENT scorer — that predict the importance of including an individual news article sentence or a particular user comment in the timeline. We train the model"
N15-1112,D11-1142,0,0.0175838,"es. For instance, the following thread connects events about Obama’s action towards the annexation of Crimea by Russia: Day 1: Day 2: Day 3: Day 4: Obama declared sanctions on Russian officials. President Obama warned Russian. Obama urges Russian to move back its troops. Obama condemns Russian aggression in Ukraine. We first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open information extraction system. We retain extractions with confidence scores higher than 0.5. We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns. Each relation contains at least one event-related word (Ritter et al., 2012). The entity-centered event threading algorithm works as follows: on the first day, each sentence in the summary becomes an individual cluster; thereafter, each sentence in the current day’s article summary either gets attached to an existing thread or starts a new thread. The updated threads then become the input to next day’s summary generation process. On day n, we have a set of threads T = {τ : s1 , s2 , · · · , sn−1 } constructed from previous"
N15-1112,P11-1052,0,0.108666,"mi(·, ·) is a word similarity function. We experiment with shortest path based similarity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al., 2013). Systems using the three metrics that optimize Z(S, C; T ) are henceforth called T HREAD +O PTTFIDF , T HREAD +O PTWordNet and T HREAD +O PTWordVec . 4.4 An Alternating Optimization Algorithm To maximize the full objective function Z(S, C; T ), we design a novel alternating optimization algorithm (Alg. 1) where we alternately find better S and C . We initialize S0 by a greedy algorithm (Lin and Bilmes, 2011) with respect to Squal (S; T ). Notice that Squal (S; T ) is a submodular function, so that the greedy solution is a 1 − 1/e approximation to the optimal solution of Squal (S; T ). Fixing S0 , we model the problem of finding C0 that maximizes Cqual (C) + δX (S0 , C) as a maximum-weight bipar1060 tite graph matching problem. This problem can be reduced to a maximum network flow problem, and then be solved by Ford-Fulkerson algorithm (details are discussed in (Kleinberg and Tardos, 2005)). Thereafter, for each iteration, we alternately find a better St with regard to Squal (S; T ) + δX (S, Ct−1"
N15-1112,N03-1020,0,0.146307,"mmaries with no comment information by optimizing Squal (S; T ) using a greedy algorithm: BASIC ignores event threading; T HREAD considers the threads. T HREAD +O PTTFIDF , T HREAD +O PTWordNet and T HREAD +O PTWordVec (see Section 4.3) leverage user comments to generate article summaries as well as comment summaries based on alternating optimization of Equation 3. Although comment summaries are generated, they are not used in the evaluation. For all systems, we generate daily article summaries of at most 100 words, and select 5 comments for the corresponding comment summary. We employ ROUGE (Lin and Hovy, 2003) to automatically evaluate the content coverage (in terms of ngrams) of the article-based timelines vs. goldstandard timelines. ROUGE-2 (measures bigram overlap) and ROUGE-SU4 (measures unigram and skip-bigrams separated by up to four words) scores are reported in Table 4. As can be seen, under the alternating optimization framework, our systems, employing both articles and comments, consistently yield better ROUGE scores than the three baseline systems and our systems that do not leverage comments. Though constructed from single-article abstracts, baseline A BSTRACT is found to contain redund"
N15-1112,P14-5010,0,0.0060203,"ticles. We collect comments for NYT articles through NYT community API, and comments for CNN articles via Disqus API. 1 NYT comments come with information on whether a comment is an editor’s-pick. The statistics on the four datasets are displayed in Table 1.2 MH370 Ukraine Israel-Gaza NSA Time Span # Articles # Comments 03/08 - 06/30 955 406,646 03/08 - 06/30 3,779 646,961 07/20 - 09/30 909 322,244 03/23 - 06/30 145 60,481 Table 1: Statistics on the four event datasets. We extract parse trees, dependency trees, and coreference resolution results of articles and comments with Stanford CoreNLP (Manning et al., 2014). Sentences in articles are labeled with timestamps using SUTime (Chang and Manning, 2012). We also collect all articles with comments from NYT in 2013 (henceforth NYT2013) to form a training set for learning importance scoring functions on articles sentences and comments (see Section 3). NYT2013 contains 3, 863 articles and 833, 032 comments. 3 Joint Learning for Importance Scoring We first introduce a joint learning method that uses graph-based regularization to simultaneously learn two functions — a SENTENCE scorer and a COM MENT scorer — that predict the importance of including an individu"
N15-1112,D12-1048,0,0.0132997,", followed by X (S, C) in Section 4.3. 4.1 Entity-Centered Event Threading We present an event threading process where each thread connects sequential events centered on a set of relevant entities. For instance, the following thread connects events about Obama’s action towards the annexation of Crimea by Russia: Day 1: Day 2: Day 3: Day 4: Obama declared sanctions on Russian officials. President Obama warned Russian. Obama urges Russian to move back its troops. Obama condemns Russian aggression in Ukraine. We first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open information extraction system. We retain extractions with confidence scores higher than 0.5. We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns. Each relation contains at least one event-related word (Ritter et al., 2012). The entity-centered event threading algorithm works as follows: on the first day, each sentence in the summary becomes an individual cluster; thereafter, each sentence in the current day’s article summary either gets attached to an existing thread or starts"
N15-1112,C14-1157,1,0.861638,"Missing"
N15-1112,H05-1044,0,0.0144777,"or sentence importance scoring. Basic Features Readability Features - num of words - Flesch-Kincaid Readability - num of sentences - Gunning-Fog Readability - avg num of words Discourse Features per sentence - num/proportion of connectives - num of NEs - num/proportion of hedge words - num/proportion of Article Features capitalized words - TF/TF-IDF simi with article - avg/sum TF-IDF - TF/TF-IDF simi with comments - contains URL - JS/KL divergence (div) with article - user rating (pos/neg) - JS/KL div with comments Sentiment Features - num /proportion of positive/negative/neutral words (MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966)) - num /proportion of sentiment words Table 3: Features used for comment importance scoring. Ri,j . The interplay between the two types of data is encoded in the following regularizing constraint: Js,c (ws , wc ) = X X λsc · Basic Features - num of words - absolute/relative position - overlaps with headline - avg/sum TF-IDF scores - num of NEs (4) Furthermore, using the following notation,      0   0 ˜ ˜ ˜ ˜ ˜ = Xs 0 Y ˜ = Ys X ˜ 0 = Xs 0 Y ˜ 0 = Ys X 0 ˜ ˜ ˜ ˜ Yc0 0 Xc Yc 0 Xc     0 ˜ = βs Ik 0 ˜ = λs I|X0s | β λ 0 βc I l 0 λc I|X0c |"
N18-1079,W07-1009,0,0.770024,"Missing"
N18-1079,N16-1030,0,0.792375,"with constituents for each named entity in a sentence. Their approach is expensive, i.e., time complexity is cubic in the number of words in the sentence. Lu and Roth (2015) later proposed a mention hypergraph model for nested entity detection with linear time complexity. And recently, Muis and Lu (2017) introduced a multigraph representation based on mention separators for this task. All of these models depend on manually crafted features. In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models — for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) — to handle nested entities. In this paper, we propose a recurrent neural network-based model for nested named entity and nested entity mention recognition. We present a modification to the standard LSTM-based sequence labeling model (Sutskever et al., 2014) that handles both problems and operates linearly in the number of tokens and the number of possible output labels at any token. The proposed neural network approach additionally jointly models entity mention head2 information, a subtask found to be useful for man"
N18-1079,D13-1057,0,0.122522,"Missing"
N18-1079,D15-1102,0,0.140476,"., 2001) and 1 https://catalog.ldc.upenn.edu/ LDC2005T09 (ACE2004) and https://catalog. ldc.upenn.edu/LDC2006T06 (ACE2005) 861 Proceedings of NAACL-HLT 2018, pages 861–871 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics (2007), for example, proposed a cascaded CRF model but it does not identify nested named entities of the same type. Finkel and Manning (2009) proposed building a constituency parser with constituents for each named entity in a sentence. Their approach is expensive, i.e., time complexity is cubic in the number of words in the sentence. Lu and Roth (2015) later proposed a mention hypergraph model for nested entity detection with linear time complexity. And recently, Muis and Lu (2017) introduced a multigraph representation based on mention separators for this task. All of these models depend on manually crafted features. In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models — for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) — to handle nested entities. In this paper, we propose a recurrent neural ne"
N18-1079,W06-1651,1,0.780372,"Missing"
N18-1079,D15-1104,0,0.034151,"y mention detection. How2 This involves identifying the headword of a named entity or entity mention. 862 ever, we will show that our model also outperforms their approach on all tasks. Recently, recurrent neural networks (RNNs) have been widely applied to several sequence labeling tasks achieving state-of-the-art results. Lample et al. (2016) proposed neural models based on long short term memory networks (LSTMs) and CRFs for named entity recognition and another transition-based approach inspired by shift-reduce parsers. Both models achieve performance comparable to a state-of-the-art model (Luo et al., 2015), but neither handles nested named entities. 3 In our encoding of nested entities, a hyperarc is introduced when two or more entity mentions requiring different label types are present at the same position. In Figure 2, for example, the nodes “O” (corresponding to the input token “that”) and the nodes “U PER” and “B PER” (corresponding to the input token “his”) are connected by a hyperarc because three entity mentions start at this time step from the tail “O” node (two of which share the “B PER” tag).4 3.1 Hypergraph Construction Let us first discuss how the problem of nested entity recognitio"
N18-1079,D09-1015,0,0.673653,"Missing"
N18-1079,N04-1001,0,0.402222,"Missing"
N18-1079,P16-1087,1,0.838761,"s expensive, i.e., time complexity is cubic in the number of words in the sentence. Lu and Roth (2015) later proposed a mention hypergraph model for nested entity detection with linear time complexity. And recently, Muis and Lu (2017) introduced a multigraph representation based on mention separators for this task. All of these models depend on manually crafted features. In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models — for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) — to handle nested entities. In this paper, we propose a recurrent neural network-based model for nested named entity and nested entity mention recognition. We present a modification to the standard LSTM-based sequence labeling model (Sutskever et al., 2014) that handles both problems and operates linearly in the number of tokens and the number of possible output labels at any token. The proposed neural network approach additionally jointly models entity mention head2 information, a subtask found to be useful for many information extraction applications. Our model significantly outperforms th"
N18-1079,P09-1113,0,0.254903,"Missing"
N18-1079,W01-1812,0,0.103393,"Finkel and Manning (2009) proposed a CRF-based constituency parser for nested named entities such that each named entity is a constituent in the parse tree. Their model achieved state-of-the-art results on the GENIA dataset. However, the time complexity of their model is O(n3 ), where n is the number of tokens in the sentence, making inference slow. As a result, we do not adopt their parse tree-based representation of nested entities and propose instead a linear time directed hypergraph-based model similar to that of Lu and Roth (2015). Directed hypergraphs were also introduced for parsing by Klein and Manning (2001). While most previous efforts for nested entity recognition were limited to named entities, Lu and Roth (2015) addressed the problem of nested entity mention detection where mentions can either be named, nominal or pronominal. Their hypergraph-based approach is able to represent the potentially exponentially many combinations of nested mentions of different types. They adopted a CRF-like log-linear approach to learn these mention hypergraphs and employed several hand-crafted features defined over the input sentence and the output hypergraph structure. Our approach also learns a similar hypergr"
N18-1079,D17-1276,0,0.146241,"edings of NAACL-HLT 2018, pages 861–871 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics (2007), for example, proposed a cascaded CRF model but it does not identify nested named entities of the same type. Finkel and Manning (2009) proposed building a constituency parser with constituents for each named entity in a sentence. Their approach is expensive, i.e., time complexity is cubic in the number of words in the sentence. Lu and Roth (2015) later proposed a mention hypergraph model for nested entity detection with linear time complexity. And recently, Muis and Lu (2017) introduced a multigraph representation based on mention separators for this task. All of these models depend on manually crafted features. In addition, they cannot be directly applied to extend current state-of-the-art recurrent neural networkbased models — for flat named entity recognition (Lample et al., 2016) or the joint extraction of entities and relations (Katiyar and Cardie, 2016) — to handle nested entities. In this paper, we propose a recurrent neural network-based model for nested named entity and nested entity mention recognition. We present a modification to the standard LSTM-base"
N18-1079,P02-1060,0,0.526396,"fined over the input sentence and the output hypergraph structure. Our approach also learns a similar hypergraph representation with differences in the types of nodes and edges in the hypergraph. It does not depend on any manually crafted features. Also, our model learns the hypergraph greedily and significantly outperforms their approach. Related Work Several methods have been proposed for named entity recognition in the existing literature as summarized by Nadeau and Sekine (2007) in their survey paper. Early techniques in the supervised domain have been based on hidden markov models (e.g., Zhou and Su (2002)) or, later, conditional Recently, Muis and Lu (2017) introduced the notion of mention separators for nested entity mention detection. In contrast to the hypergraph representation that we and Lu and Roth (2015) adopt, they learn a multigraph representation and are able to perform exact inference on their structure. It is an interesting orthogonal possible approach for nested entity mention detection. How2 This involves identifying the headword of a named entity or entity mention. 862 ever, we will show that our model also outperforms their approach on all tasks. Recently, recurrent neural netw"
N18-1094,W16-6209,0,0.04093,"Missing"
N18-1094,P11-1099,0,0.499757,"weak subjectivity. Swear words. # of swear words. Connotation score (Feng and Average # of words with positive, negative and neuHirst, 2011). tral connotation. Personal pronouns. Usage of first, second, and third person pronouns. Modal verbs. Usage of modal verbs. Argument lexicon features. # of phrases corresponding to different argumenta(Somasundaran et al., 2007). tion styles. Spelling. # of spelling errors. Links. # of links. Numbers. # of numbers. Exclamation marks. # of exclamation marks. Questions. # of questions. Table 2: Feature descriptions ment (e.g., showing evidence, connotation (Feng and Hirst, 2011), subjectivity (Wilson et al., 2005), sentiment, swear word features) as well as features that convey different argumentation styles (argument lexicon features (Somasundaran and Wiebe, 2010). Argument lexicon features include the counts for the phrases that match with the regular expressions of argumentation styles such as assessment, authority, conditioning, contrasting, emphasizing, generalizing, empathy, inconsistency, necessity, possibility, priority, rhetorical questions, desire, and difficulty. We then concatenate these features to get a single feature representation for the entire debat"
N18-1094,P16-1150,0,0.0825585,"ost recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs. Persuasion. There has been a tremendous amount of research effort in the social sciences (including computational social science) to understand the characteristics of persuasive text (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016). Most relevant amon"
N18-1094,D14-1083,0,0.0281543,"cuses on persuasion in debates than monologues and forum datasets and accounts for the user-based features. Persuasion in debates. Debates are another resource for studying the different aspects of persuasive arguments. Different from monologues where the audience is exposed to only one side of the opinions about an issue, debates allow the audience to see both sides of a particular issue via a 1042 9 https://www.reddit.com/r/changemyview/ controlled discussion. There has been some work on argumentation and persuasion on online debates. Sridhar et al. (2015), Somasundaran and Wiebe (2010) and Hasan and Ng (2014), for example, studied detecting and modeling stance on online debates. Zhang et al. (2016) found that the side that can adapt to their opponents’ discussion points over the course of the debate is more likely to be the winner. None of these studies investigated the role of prior beliefs in stance detection or persuasion. User effects in persuasion. Persuasion is not independent from the characteristics of the people to be persuaded. Research in psychology has shown that people have biases in the ways they interpret the arguments they are exposed to because of their prior beliefs (Lord et al.,"
N18-1094,W17-5102,0,0.248645,"Missing"
N18-1094,W15-0503,0,0.0343809,"tic features Length Politeness Modal verbs Tf-idf features User-based+linguistic features USER *+ Length USER *+ Tf-idf USER *+ Length + Tf-idf Related Work Below we provide an overview of related work from the multiple disciplines that study persuasion. Argumentation mining. Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs. Persuasion. There has been a tremendous amount of research effort in the social sciences (including computational social s"
N18-1094,P17-1091,1,0.872179,"Missing"
N18-1094,W14-2105,1,0.898292,"nion similarity Linguistic features Length Politeness Modal verbs Tf-idf features User-based+linguistic features USER *+ Length USER *+ Tf-idf USER *+ Length + Tf-idf Related Work Below we provide an overview of related work from the multiple disciplines that study persuasion. Argumentation mining. Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs. Persuasion. There has been a tremendous amount of research effort in the social sciences (includi"
N18-1094,D15-1110,0,0.0343328,"eness Modal verbs Tf-idf features User-based+linguistic features USER *+ Length USER *+ Tf-idf USER *+ Length + Tf-idf Related Work Below we provide an overview of related work from the multiple disciplines that study persuasion. Argumentation mining. Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs. Persuasion. There has been a tremendous amount of research effort in the social sciences (including computational social science) to understand the"
N18-1094,P15-1053,0,0.116853,", exclamation mark, questions, politeness, referring to opponent, showing evidence, modals, links, and numbers as features. 5 Baseline Majority User-based Features Opinion similarity Linguistic features Length Politeness Modal verbs Tf-idf features User-based+linguistic features USER *+ Length USER *+ Tf-idf USER *+ Length + Tf-idf Related Work Below we provide an overview of related work from the multiple disciplines that study persuasion. Argumentation mining. Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of pe"
N18-1094,D17-1261,0,0.746001,"setting that can control for selected user-level factors, in our case, the prior beliefs associated with the political or 1 Variables that affect both the dependent and independent variables causing misleading associations. 1035 Proceedings of NAACL-HLT 2018, pages 1035–1045 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics religious ideology of the debaters and voters. In particular, previous studies focus on predicting the winner of a debate based on the cumulative change in pre-debate vs. post-debate votes for the opposing sides (Zhang et al., 2016; Potash and Rumshisky, 2017). In contrast, we aim to predict which debater an individual user (i.e., reader of the debate) perceives as more successful, given their stated political and religious ideology. Finally, we identify which features appear to be most important for persuasion, considering the selected user-level factors as well as the more traditional linguistic features associated with the language of the debate itself. We hypothesize that the effect of political and religious ideology will be stronger when the debate topic is Politics and Religion, respectively. To test this hypothesis, we experiment with debat"
N18-1094,W15-4625,0,0.0404488,"guistic features USER *+ Length USER *+ Tf-idf USER *+ Length + Tf-idf Related Work Below we provide an overview of related work from the multiple disciplines that study persuasion. Argumentation mining. Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs. Persuasion. There has been a tremendous amount of research effort in the social sciences (including computational social science) to understand the characteristics of persuasive text (Kelman, 1961; Bu"
N18-1094,2007.sigdial-1.5,0,0.639423,"Missing"
N18-1094,W10-0214,0,0.336979,"ge of first, second, and third person pronouns. Modal verbs. Usage of modal verbs. Argument lexicon features. # of phrases corresponding to different argumenta(Somasundaran et al., 2007). tion styles. Spelling. # of spelling errors. Links. # of links. Numbers. # of numbers. Exclamation marks. # of exclamation marks. Questions. # of questions. Table 2: Feature descriptions ment (e.g., showing evidence, connotation (Feng and Hirst, 2011), subjectivity (Wilson et al., 2005), sentiment, swear word features) as well as features that convey different argumentation styles (argument lexicon features (Somasundaran and Wiebe, 2010). Argument lexicon features include the counts for the phrases that match with the regular expressions of argumentation styles such as assessment, authority, conditioning, contrasting, emphasizing, generalizing, empathy, inconsistency, necessity, possibility, priority, rhetorical questions, desire, and difficulty. We then concatenate these features to get a single feature representation for the entire debate. 4 Results and Analysis For each of the tasks, prediction accuracy is evaluated using 5-fold cross validation. We pick the model parameters for each split with 3-fold cross validation on t"
N18-1094,P14-1017,0,0.155126,"Missing"
N18-1094,H05-1044,0,0.0716338,"}. We denote these features as matching political ideology and matching religious ideology. Linguistic features Description Length. Number of tokens. Tf-idf. Unigram, bigram and trigram features. Referring to the opponent. Whether the debater refers to their opponent using words or phrases like “opponent, my opponent”. Politeness cues. Whether the text includes any signs of politeness such as “thank” and “welcome”. Showing evidence. Whether the text has any signs of citing any other sources (e.g., phrases like “according to”), or quotation. Sentiment. Average sentiment polarity. Subjectivity (Wilson et al., 2005). Number of words with negative strong, negative weak, positive strong, and positive weak subjectivity. Swear words. # of swear words. Connotation score (Feng and Average # of words with positive, negative and neuHirst, 2011). tral connotation. Personal pronouns. Usage of first, second, and third person pronouns. Modal verbs. Usage of modal verbs. Argument lexicon features. # of phrases corresponding to different argumenta(Somasundaran et al., 2007). tion styles. Spelling. # of spelling errors. Links. # of links. Numbers. # of numbers. Exclamation marks. # of exclamation marks. Questions. # of"
N18-1094,N16-1017,0,0.398583,"ublic debate forums provide to participants a common platform for expressing their point of view on a topic; they also present to participants the different sides of an argument. The latter can be particularly important: awareness of divergent points of view allows one, in theory, to make a fair and informed decision about an issue; and exposure to new points of view can furthermore possibly persuade a reader to change his overall stance on a topic. Research in natural language processing (NLP) has begun to study persuasive writing and the role of language in persuasion. Tan et al. (2016) and Zhang et al. (2016), for example, have shown that the language of opinion holders or debaters and their patterns of interaction play a key role in changing the mind of a reader. At the same time, research in psychology has shown that prior beliefs can affect our interpretation of an argument even when the argument consists of numbers and empirical studies that would seemingly belie misinterpretation (Lord et al., 1979; Vallone et al., 1985; Chambliss and Garner, 1996). We hypothesize that studying the actual effect of language on persuasion will require a more controlled experimental setting — one that takes int"
N18-1094,P15-1012,0,0.0308051,"-persuasive essays. In contrast to the above, our work focuses on persuasion in debates than monologues and forum datasets and accounts for the user-based features. Persuasion in debates. Debates are another resource for studying the different aspects of persuasive arguments. Different from monologues where the audience is exposed to only one side of the opinions about an issue, debates allow the audience to see both sides of a particular issue via a 1042 9 https://www.reddit.com/r/changemyview/ controlled discussion. There has been some work on argumentation and persuasion on online debates. Sridhar et al. (2015), Somasundaran and Wiebe (2010) and Hasan and Ng (2014), for example, studied detecting and modeling stance on online debates. Zhang et al. (2016) found that the side that can adapt to their opponents’ discussion points over the course of the debate is more likely to be the winner. None of these studies investigated the role of prior beliefs in stance detection or persuasion. User effects in persuasion. Persuasion is not independent from the characteristics of the people to be persuaded. Research in psychology has shown that people have biases in the ways they interpret the arguments they are"
N18-1094,C14-1142,0,0.0545231,"tures. 5 Baseline Majority User-based Features Opinion similarity Linguistic features Length Politeness Modal verbs Tf-idf features User-based+linguistic features USER *+ Length USER *+ Tf-idf USER *+ Length + Tf-idf Related Work Below we provide an overview of related work from the multiple disciplines that study persuasion. Argumentation mining. Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; ?; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017). Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs. Persuasion. There has been a tremendous amount of"
N18-1094,D16-1129,0,\N,Missing
N18-1094,C16-1158,0,\N,Missing
N18-1094,E17-1070,0,\N,Missing
N18-1111,W06-1615,0,\N,Missing
N18-1111,P07-1056,0,\N,Missing
N19-1244,D14-1159,1,0.853396,"inimization for data graph. Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news st"
N19-1244,P16-1223,0,0.0514622,"Missing"
N19-1244,N18-1144,1,0.91825,"agraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems. 1 Figure 1: Fragments from three independent texts about photosynthesis. Although (1) is ambiguous as to whether oxygen is being created or merely moved, evidence from (2) and (3) suggests it is being created, helping to correctly interpret (1). More generally, encouraging consistency between predictions from different paragraphs about the same process/procedure can improve performance. Introduction We address the task of procedural text comprehension, namely tracking how the prope"
N19-1244,W18-2501,0,0.0285905,"Missing"
N19-1244,P18-1075,0,0.0217954,"ork is related to several important branches of work in both NLP and ML, as we now summarize. Leveraging Label Consistency Leveraging information about label consistency (i.e., similar instances should have consistent labels at a certain granularity) is an effective idea. It has been studied in computer vision (Haeusser et al., 2017; Chen et al., 2018) and IR (Clarke et al., 2001; Dumais et al., 2002). Learning by association (Haeusser et al., 2017) establishes implicit cross-modal links between similar descriptions and leverage more unlabeled data during training. Schütze et al. (2018); 2348 Hangya et al. (2018) adapt the similar idea to exploit unlabeled data for the cross-lingual classification. We extend this line of research in two ways: by developing a framework allowing it to be applied to the task of structure prediction; and by incorporating label consistency into the model itself via end-to-end training, rather than enforcing consistency as a post-processing step. Semi-supervised Learning Approaches Besides utilizing the label consistency knowledge, our learning framework is also able to use unlabeled paragraphs, which fits in the literature of semisupervised learning approaches (for NLP). Z"
N19-1244,D15-1114,0,0.0604539,"ces. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news stories about a political meeting, we expect top-level features (e.g., where the meeting took place, who attended) to be similar; for different recipes for the same item, we expect loosely similar ingredients and steps; and for different i"
N19-1244,D16-1032,0,0.0264777,"r) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news stories about a political meeting, we expect top-level features (e.g., where the meeting took place, who attended) to be similar; for different recipes for the same item, we expect loosely similar ingredients and steps; and for different images of the same person, we expect some high-level characteristics (e.g., height, face shape) to be similar. Note"
N19-1244,D14-1162,0,0.0816999,"ncoder ProStruct uses an encoder-decoder architecture that takes procedural text as input and predicts the state changes of entities E in the text as output. During encoding, each step st is encoded using |E |embeddings, one for each entity e j ∈ E. Each embedding represents the action that st describes, applied to ek . The model thus allows the same action to have different effects on different entities (e.g., a transformation destroys one entity, and creates another). For each (st , e j ) ∈ S × E pair, the step is fed into a BiLSTM (Hochreiter and Schmidhuber, 1997), using pretrained GloVe (Pennington et al., 2014) vectors vw for each word wi concatenated with two indicator variables, one indicating whether wi is a word referring to e j , and one indicating whether wi is a verb. A bilinear attention layer then computes attention over the contextualized vectors hi output by the BiLSTM: ai = hi ∗ B∗hev +b , where B and b are learned parameters, and hev is the concatenation of he (the averaged contextualized embedding for the entity words we ) and hv (the averaged contextualized embedding for the verb words wv ). Finally, the output vector ct j is the attentionPI weighted sum of the hi : ct j = i=1 ai ∗ hi"
N19-1244,D08-1061,0,0.0331192,"rediction; and by incorporating label consistency into the model itself via end-to-end training, rather than enforcing consistency as a post-processing step. Semi-supervised Learning Approaches Besides utilizing the label consistency knowledge, our learning framework is also able to use unlabeled paragraphs, which fits in the literature of semisupervised learning approaches (for NLP). Zhou et al. (2003) propose an iterative label propagation algorithm similar to spectral clustering. Zhu et al. (2003) propose a semi-supervised learning framework via harmonic energy minimization for data graph. Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event or"
N19-1244,D18-1006,1,0.889929,"Missing"
N19-1270,D15-1220,0,0.0304878,"Missing"
N19-1270,P16-1223,0,0.0939856,"Missing"
N19-1270,W16-3612,0,0.0118964,", 2017), ROCStories (Mostafazadeh et al., 2016), and MultiRC (Khashabi et al., 2018)) (Section 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of a"
N19-1270,D18-1241,0,0.0296487,"ave been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive human efforts to build such a dataset (e.g., MCTest (Richardson et al., 2013), SemEval2018 Task 11 (Ostermann et al., 2018), MultiRC (K"
N19-1270,N18-1143,0,0.095059,"Missing"
N19-1270,P17-1168,0,0.0607419,"Missing"
N19-1270,S18-1189,0,0.0133255,"answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing (Yang et al., 2017; Yuan et al., 2017), which require a large amount of training data or limited by the number of training instances (Yu et al., 2018). In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document. 6 Conclusions Inspired by previous research on reading strategies for improved comprehension levels of human readers, we propose three strate"
N19-1270,D17-1087,0,0.0148323,"ldom take the rich external knowledge (other than pretrained word embeddings) into considerations. Instead, we investigate different strategies based on an existing pre-trained transformer (Radford et al., 2018) (Section 3.1), which leverages rich linguistic knowledge from external corpora and achieves state-of-the-art performance on a wide range of natural language processing tasks including machine reading comprehension. 5.2 Transfer Learning for Machine Reading Comprehension and Question Answering Transfer learning techniques have been successfully applied to machine reading comprehension (Golub et al., 2017; Chung et al., 2018) and question answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reorde"
N19-1270,P17-1147,0,0.105329,"tion 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extract"
N19-1270,N18-1023,0,0.522404,"oaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of ∗ This work was done when K. S. was an intern at the Tencent AI Lab, Bellevue, WA. Introduction Recent years have seen a growing interest in machine reading comprehension (MRC) (Rajpurkar et al., 2016; Choi et al., 2018; Koˇcisk`y et al., 2018; Reddy et al., 2018). In this paper, we mainly focus on non-extractive MRC (Khashabi et al., 2018; Ostermann et al., 2018; Clark et al., 2018), in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to extractive MRC tasks (Section 2.1), non-extractive MRC (Section 2.2) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams (Lai et al., 2017). Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-"
N19-1270,Q18-1023,0,0.0617034,"Missing"
N19-1270,D17-1082,0,0.740196,"al., 2018; Koˇcisk`y et al., 2018; Reddy et al., 2018). In this paper, we mainly focus on non-extractive MRC (Khashabi et al., 2018; Ostermann et al., 2018; Clark et al., 2018), in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to extractive MRC tasks (Section 2.1), non-extractive MRC (Section 2.2) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams (Lai et al., 2017). Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-tuning a pre-trained generalpurpose language model (Radford et al., 2018; Devlin et al., 2018). However, similar to the process of knowledge accumulation for human readers, it is time-consuming and resource-demanding to impart massive amounts of general domain knowledge from external corpora into a deep language model via pre-training. For example, it takes a month to pre-train a 12-layer transformer on eight P100 GPUs over the BooksCorpus (Zhu et al., 2015; Radford et al., 2018);"
N19-1270,S18-1180,0,0.0280285,", 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing (Yang et al., 2017; Yuan et al., 2017), which require a large amount of training data or limited by the number of training instances (Yu et al., 2018). In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document. 6 Conclusions Inspired by previous research on reading strategies for improved comprehension levels of human readers, we propose three strategies (i.e., back and"
N19-1270,N18-1185,0,0.0109608,"ults indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this se"
N19-1270,D18-1260,0,0.250953,"ectly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive human efforts to build such a dataset (e.g., MCTest (Richardson et al., 2013), SemEval2018 Task 11 (Ostermann et al., 2018), MultiRC (Khashabi et al., 2018), and OpenBookQA (Mihaylov et al., 2018)) by crowdsourcing. Besides crowdsourcing, datasets such as RACE (Lai et al., 2017) and ARC (Clark et al., 2018) are collected from language or science exams designed by educational experts (Penas et al., 2014; Shibuki et al., 2014; Tseng et al., 2016) to evaluate the comprehension level of human participants. Compared to questions in extractive MRC tasks, besides surface matching, there are various types of complicated questions such as math word problems, summarization, logical reasoning, and sentiment analysis, requiring advanced read2634 RACE ARC OpenBookQA MCTest SemEval-2018 Task 11 ROCS"
N19-1270,D16-1241,0,0.0274386,"Task 11 (Yang et al., 2017), ROCStories (Mostafazadeh et al., 2016), and MultiRC (Khashabi et al., 2018)) (Section 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answe"
N19-1270,S18-1119,0,0.402795,"ative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of ∗ This work was done when K. S. was an intern at the Tencent AI Lab, Bellevue, WA. Introduction Recent years have seen a growing interest in machine reading comprehension (MRC) (Rajpurkar et al., 2016; Choi et al., 2018; Koˇcisk`y et al., 2018; Reddy et al., 2018). In this paper, we mainly focus on non-extractive MRC (Khashabi et al., 2018; Ostermann et al., 2018; Clark et al., 2018), in which a significant percentage of candidate answers are not restricted to text spans from the reference document or corpus. In comparison to extractive MRC tasks (Section 2.1), non-extractive MRC (Section 2.2) requires diverse reading skills and, as a result, the performance of machine readers on these tasks more accurately indicates the comprehension ability of machine readers in realistic settings such as exams (Lai et al., 2017). Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-tuning a pre-trained gen"
N19-1270,P17-2081,0,0.0230051,"d embeddings) into considerations. Instead, we investigate different strategies based on an existing pre-trained transformer (Radford et al., 2018) (Section 3.1), which leverages rich linguistic knowledge from external corpora and achieves state-of-the-art performance on a wide range of natural language processing tasks including machine reading comprehension. 5.2 Transfer Learning for Machine Reading Comprehension and Question Answering Transfer learning techniques have been successfully applied to machine reading comprehension (Golub et al., 2017; Chung et al., 2018) and question answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and"
N19-1270,R13-1061,0,0.0293908,"ack and forth reading (BF) (Section 3.2), highlighting (HL) (Section 3.3), and self-assessment (SA) (Section 3.4), respectively. 3.3 Highlighting (HL) In the original implementation (Radford et al., 2018), during the fine-tuning stage of GPT, the text embedding of a document is independent of its associated questions and answer options. Inspired by highlights used in human reading, we aim to make the document encoding aware of the associated question-answer option pair (q, oi ). We focus on the content words in questions and answer options since they appear to provide more useful information (Mirza and Bernardi, 2013), and we identify them via their part of speech (POS) tags, one of: noun, verb, adjective, adverb, numeral, or foreign word. Formally, we let T be the set of POS tags of the content words. We let d denote the sequence of the text embedding of document d. We use dj to represent the j th token in d and dj to denote the text embedding of dj . Given d and a (q, oi ) pair, we define a highlight embedding hji for the j th token in d as: hji =  +  `  `− if the POS tag of dj belongs to T , and dj appears in either q or oi otherwise (2) where `+ and `− are two trainable vectors of the same dimensi"
N19-1270,D16-1264,0,0.0838319,"f our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoi"
N19-1270,D13-1020,0,0.158061,"ed on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer options are not restricted to extractive text spans. Given a question and a reference document/corpus, multiple answer options are provided, and at least one of them is correct. It involves extensive human efforts to build such a dataset (e.g., MCTest (Richardson et al., 2013), SemEval2018 Task 11 (Ostermann et al., 2018), MultiRC (Khashabi et al., 2018), and OpenBookQA (Mihaylov et al., 2018)) by crowdsourcing. Besides crowdsourcing, datasets such as RACE (Lai et al., 2017) and ARC (Clark et al., 2018) are collected from language or science exams designed by educational experts (Penas et al., 2014; Shibuki et al., 2014; Tseng et al., 2016) to evaluate the comprehension level of human participants. Compared to questions in extractive MRC tasks, besides surface matching, there are various types of complicated questions such as math word problems, summarization, logi"
N19-1270,N16-1098,0,0.0733986,"te improvement in accuracy over the previous best result achieved by the same pretrained transformer fine-tuned on RACE without the use of strategies (Section 4.2). We further fine-tune the resulting model on a target MRC task. Experiments show that our method achieves new state-of-the-art results on six representative non-extractive MRC datasets that require a range of reading skills such as commonsense and multi-sentence reasoning (i.e., ARC (Clark et al., 2016, 2018), OpenBookQA (Mihaylov et al., 2018), MCTest (Richardson et al., 2013), SemEval-2018 Task 11 (Yang et al., 2017), ROCStories (Mostafazadeh et al., 2016), and MultiRC (Khashabi et al., 2018)) (Section 4.4). These results indicate the effectiveness of our proposed strategies and the versatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016;"
N19-1270,P17-1075,0,0.0336289,"Missing"
N19-1270,W17-2623,0,0.0527475,"rsatility and generality of our fine-tuned models that incorporate the strategies. 2 Task Introduction We roughly categorize machine reading comprehension tasks into two groups: extractive (Section 2.1) and non-extractive (Section 2.2) based on the expected answer types. 2.1 Extractive MRC Recently large-scale extractive MRC datasets have been constructed (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016; Chen and Choi, 2016; Mostafazadeh et al., 2016; Bajgar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Ma et al., 2018), such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017). Given a reference document and a question, the expected answer is a short span from the document. In contrast, answers in datasets such as SearchQA (Dunn et al., 2017) and NarrativeQA (Koˇcisk`y et al., 2018) are free-form human generated texts based on given documents (Nguyen et al., 2016; Reddy et al., 2018; Choi et al., 2018). However, since annotators tend to directly copy spans as answers, the majority of answers are still extractive (Reddy et al., 2018; Koˇcisk`y et al., 2018). 2.2 Non-Extractive MRC In this section, we primarily discuss multiplechoice MRC datasets, in which answer opt"
N19-1270,S18-1120,0,0.0564461,"ing the joint exact match accuracy (i.e., EM0 reported by the official evaluation (Khashabi et al., 2018))). 4.4 Adaptation to Other Non-Extractive Machine Reading Comprehension Tasks We follow the philosophy of transferring the knowledge from a high-performing model pretrained on a large-scale supervised data of a source task to a target task, in which only a small amount of training data is available (Chung et al., 2018). RACE has been used to pre-train a model for other MRC tasks as it contains the largest number of general domain non-extractive questions (Table 1) (Ostermann et al., 2018; Wang et al., 2018a). In our experiment, we also treat RACE as the source task and regard six representative non-extractive multiple-choice MRC datasets from multiple domains as the target tasks. We require some task-specific modifications considering the different structures of these datasets. In ARC and OpenBookQA, there is no reference document associated with each question. Instead, a reference corpus is provided, which consists of unordered science-related sentences relevant to questions. We therefore first use Lucene (McCandless et al., 2010) to retrieve the top 50 sentences by using the non-stop words in"
N19-1270,P18-2118,0,0.0369884,"ing the joint exact match accuracy (i.e., EM0 reported by the official evaluation (Khashabi et al., 2018))). 4.4 Adaptation to Other Non-Extractive Machine Reading Comprehension Tasks We follow the philosophy of transferring the knowledge from a high-performing model pretrained on a large-scale supervised data of a source task to a target task, in which only a small amount of training data is available (Chung et al., 2018). RACE has been used to pre-train a model for other MRC tasks as it contains the largest number of general domain non-extractive questions (Table 1) (Ostermann et al., 2018; Wang et al., 2018a). In our experiment, we also treat RACE as the source task and regard six representative non-extractive multiple-choice MRC datasets from multiple domains as the target tasks. We require some task-specific modifications considering the different structures of these datasets. In ARC and OpenBookQA, there is no reference document associated with each question. Instead, a reference corpus is provided, which consists of unordered science-related sentences relevant to questions. We therefore first use Lucene (McCandless et al., 2010) to retrieve the top 50 sentences by using the non-stop words in"
N19-1270,K17-1029,0,0.022419,"considerations. Instead, we investigate different strategies based on an existing pre-trained transformer (Radford et al., 2018) (Section 3.1), which leverages rich linguistic knowledge from external corpora and achieves state-of-the-art performance on a wide range of natural language processing tasks including machine reading comprehension. 5.2 Transfer Learning for Machine Reading Comprehension and Question Answering Transfer learning techniques have been successfully applied to machine reading comprehension (Golub et al., 2017; Chung et al., 2018) and question answering (Min et al., 2017; Wiese et al., 2017). Compared to previous work, we simply fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or gener"
N19-1270,P17-1096,0,0.0246747,"fine-tune our model on the source data and then further fine-tune the entire model on the target data. The investigation of methods such as adding additional parameters or an L2 loss and fine-tuning only part of the parameters is beyond the scope of this work. 5.3 Data Augmentation for Machine Reading Comprehension Without Using External Datasets Previous methods augment the training data for extractive machine reading comprehension and question answering by randomly reordering words or shuffling sentences (Ding and Zhou, 2018; Li and Zhou, 2018) or generating questions through paraphrasing (Yang et al., 2017; Yuan et al., 2017), which require a large amount of training data or limited by the number of training instances (Yu et al., 2018). In comparison, our problem (i.e., question and answer options) generation method does not rely on any existing questions in the training set, and the generated questions can involve the content of multiple sentences in a reference document. 6 Conclusions Inspired by previous research on reading strategies for improved comprehension levels of human readers, we propose three strategies (i.e., back and forth reading, highlighting, and self-assessment), aiming at im"
P02-1014,M95-1005,0,\N,Missing
P02-1014,N01-1008,0,\N,Missing
P02-1014,M95-1014,0,\N,Missing
P02-1014,M95-1010,0,\N,Missing
P02-1014,J94-4002,0,\N,Missing
P02-1014,P95-1017,0,\N,Missing
P02-1014,J01-4004,0,\N,Missing
P09-1074,N07-1010,0,0.183205,"resolution classes in a corpus explains (at least partially) why performance varies so much from cor662 P O MUC6 0.59 0.67 MUC7 0.59 0.61 ACE2 0.62 0.66 ACE03 0.65 0.68 ACE04 0.59 0.62 ACE05 0.62 0.67 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (20"
P09-1074,N04-1038,1,0.917126,"Missing"
P09-1074,D08-1031,0,0.459229,"er, we describe the coreference resolver that we use for our study. 3.1 docs 60 50 159 105 128 81 CEs 4232 4297 2630 3106 3037 1991 chains 960 1081 1148 1340 1332 775 CEs/ch 4.4 3.9 2.3 2.3 2.3 2.6 tr/tst split 30/30 (st) 30/20 (st) 130/29 (st) 74/31 90/38 57/24 Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details. Lexical (9): String-based comparisons of the two CEs, such as exact string matching and head noun matching. Proximity (5): Sentence and paragraph-based measures of the distance between two CEs. Grammatical (28): A wide variety of syntactic properties of the CEs, either individually or as a pair. These features are based on part-of-speech tags, parse trees, or dependency relations. For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs"
P09-1074,C02-1139,1,0.861798,"61 ACE2 0.62 0.66 ACE03 0.65 0.68 ACE04 0.59 0.62 ACE05 0.62 0.67 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusi"
P09-1074,P02-1014,1,0.845895,"61 ACE2 0.62 0.66 ACE03 0.65 0.68 ACE04 0.59 0.62 ACE05 0.62 0.67 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusi"
P09-1074,J01-4006,0,0.0863922,"other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers. Table 6: Predicted (P) vs Observed (O) scores. pus to corpus. To explore this issue, we create a Coreference Performance Prediction (CPP) measure to"
P09-1074,de-marneffe-etal-2006-generating,0,0.0108758,"Missing"
P09-1074,N07-1051,0,0.0206299,"Missing"
P09-1074,W04-1217,0,0.0121339,"Missing"
P09-1074,J01-4004,0,0.979775,"Missing"
P09-1074,P04-1018,0,0.531788,"Missing"
P09-1074,P03-1023,0,0.315248,"Missing"
P09-1074,H05-1004,0,0.564694,"s work has shown that resolving coreference between proper names is relatively easy (e.g. Kameyama (1997)) because string matching functions specialized to the type of proper name (e.g. person vs. location) are quite accurate. Thus, we would expect a coreference resolution system to depend critically on its Named Entity (NE) extractor. On the other hand, state-of-the-art NE taggers are already quite good, so improving this component may not provide much additional gain. To study the influence of NE recognition, we replace the system-generated NEs of 4 We also experimented with the CEAF score (Luo, 2005), but excluded it due to difficulties dealing with the extracted, rather than annotated, CEs. CEAF assigns a zero score to each twinless extracted CE and weights all coreference chains equally, irrespective of their size. As a result, runs with extracted CEs exhibit very low CEAF precision, leading to unreliable scores. 5 All experiments sample uniformly from 1000 threshold values. 659 ReconcileACL09 1. DEFAULT THRESHOLD (0.5) 2. BASELINE = THRESHOLD ESTIMATION 3. OPTIMAL THRESHOLD 4. BASELINE with perfect NEs 5. BASELINE with perfect CEs 6. BASELINE with anaphoric CEs MUC B 3 all B30 MUC B 3"
P09-1074,M95-1005,0,\N,Missing
P09-1074,J00-4005,0,\N,Missing
P09-1074,D08-1078,0,\N,Missing
P09-1074,W97-1307,0,\N,Missing
P10-2029,P04-1018,0,0.369513,"array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art learning-based coreference resolution systems; • support experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features;"
P10-2029,H05-1004,0,0.76397,"Missing"
P10-2029,D08-1031,0,0.799412,"2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum an"
P10-2029,P02-1014,1,0.937135,"n be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review related work (Section 2), describe Reconcile’s organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4). 2 3 System Description Reconcile was designed to be a research testbed capable of implementing most current"
P10-2029,P05-1045,0,0.0134027,"Missing"
P10-2029,N07-1051,0,0.00913438,"Missing"
P10-2029,poesio-kabadjov-2004-general,0,0.252363,"on task (e.g., MUC-6 (1995), ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets). However, it is still frustratingly difficult to compare results across different coreference resolution systems. Reported coreference resolution scores vary wildly across data sets, evaluation metrics, and system configurations. • implement the basic underlying software ar156 Proceedings of the ACL 2010 Conference Short Papers, pages 156–161, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengts"
P10-2029,qiu-etal-2004-public,0,0.58709,"the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review rel"
P10-2029,P07-1107,0,0.113384,"rt supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art learning-based coreference resolution systems; • support experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference"
P10-2029,J01-4004,0,0.984178,"hough we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art le"
P10-2029,P09-1074,1,0.696567,"Missing"
P10-2029,P08-4003,0,0.235695,"(2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets). However, it is still frustratingly difficult to compare results across different coreference resolution systems. Reported coreference resolution scores vary wildly across data sets, evaluation metrics, and system configurations. • implement the basic underlying software ar156 Proceedings of the ACL 2010 Conference Short Papers, pages 156–161, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in pe"
P10-2029,P03-1023,0,0.148118,"has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art learning-based coreference resolution systems; • support experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • c"
P10-2029,M95-1005,0,\N,Missing
P10-2029,J94-4002,0,\N,Missing
P10-2050,D08-1083,1,0.564318,"Missing"
P10-2050,W06-1673,0,0.0133806,"Missing"
P10-2050,C04-1200,0,0.0809007,"a sequence tagging task as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f (yi , x, i)+λ′ f ′ (yi−1 , yi , x, i) exp Zx i where Zx is the normalization factor. In order t"
P10-2050,I05-2011,0,0.00936932,"2 Hierarchical Sequential Learning We define the problem of joint extraction of opinion expressions and their attributes as a sequence tagging task as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propag"
P10-2050,H05-1043,0,0.9049,"tial Learning We define the problem of joint extraction of opinion expressions and their attributes as a sequence tagging task as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f"
P10-2050,H05-1044,0,0.114114,"ask as follows. Given a sequence of tokens, x = x1 ... xn , we predict a sequence of labels, y = y1 ... yn , where yi ∈ {0, ..., 9} are defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then the conditional probability p(y|x) for linear-chain CRFs is given as (Lafferty et al., 2001) 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determining their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f (yi , x, i)+λ′ f ′ (yi−1 , yi , x, i) exp Zx i where Zx is the normalization factor. In order to apply a hierarchical"
P10-2050,J09-3003,0,0.0483576,"Missing"
P10-2050,D08-1013,0,0.0134636,"u and Etzioni (2005), Wilson et al. (2006)). Most previous work treats each subtask in isolation: opinion expression extraction (i.e. detecting the boundaries of opinion expressions) and opinion attribute classification (e.g. determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, errors from individual components will propagate in P (y|x) =  X 1 λ f (yi , x, i)+λ′ f ′ (yi−1 , yi , x, i) exp Zx i where Zx is the normalization factor. In order to apply a hierarchical parameter sharing technique (e.g., Cai and Hofmann (2004), Zhao et al. (2008)), we extend parameters as follows. 269 Proceedings of the ACL 2010 Conference Short Papers, pages 269–274, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics                                                                  Figure 1: The hierarchical structure of classes for opinion expressions with polarity (positive, neutral, negative) and intensity (high, medium, low) L ABEL P OLARITY I NTENSITY 0 1 2 3 4 5 6 7 8 9 none positive positive positive neutral"
P10-2050,H05-2017,0,\N,Missing
P10-2062,P07-1056,0,0.195605,"Missing"
P10-2062,P07-1055,0,0.348901,"Missing"
P10-2062,P04-1035,0,0.728953,"evious research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales. 1 Introduction One of the central challenges in sentiment-based text categorization is that not every portion of a given document is equally informative for inferring its overall sentiment (e.g., Pang and Lee (2004)). Zaidan et al. (2007) address this problem by asking human annotators to mark (at least some of) the relevant text spans that support each document-level sentiment decision. The text spans of these “rationales” are then used to construct additional training examples that can guide the learning algorithm toward better categorization models. But could we perhaps enjoy the performance gains of rationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)? We hypothesize that in the area of sentiment analysis, where there has been a"
P10-2062,W02-1011,0,0.0464621,"r this, we rely on the following two assumptions: (1) Regions marked as annotator rationales are more subjective than unmarked regions. (2) The sentiment of each annotator rationale coincides with the document-level sentiment. Note that assumption 1 was not observed in the Zaidan et al. (2007) work: annotators were asked only to mark a few rationales, leaving other (also subjective) rationale sections unmarked. And at first glance, assumption (2) might seem too obvious. But it is important to include as there can be subjective regions with seemingly conflicting sentiment in the same document (Pang et al., 2002). For instance, an author for a movie review might express a positive sentiment toward the movie, while also discussing a negative sentiment toward one of the fictional characters appearing in the movie. This implies that not all subjective regions will be relevant for the documentlevel sentiment classification — rather only those regions whose polarity matches that of the document should be considered. In order to extract regions that satisfy the above assumptions, we first look for subjective regions in each document, then filter out those regions that exhibit a sentiment value (i.e., polari"
P10-2062,W09-3909,0,0.0241034,"Missing"
P10-2062,H05-2018,1,0.942633,"tead, we opt for methods that make use of only the document-level sentiment and off-the-shelf utilities that were trained for slightly different sentiment classification tasks using a corpus from a different domain and of a different genre. Although such utilities might not be optimal for our task, we hoped that these basic resources from the research community would constitute an adequate source of sentiment information for our purposes. We next describe three methods for the automatic acquisition of rationales. 3.1 Contextual Polarity Classification The first approach employs OpinionFinder (Wilson et al., 2005a), an off-the-shelf opinion analysis utility.1 In particular, OpinionFinder identifies phrases expressing positive or negative opinions. Because OpinionFinder models the task as a word-based classification problem rather than a sequence tagging task, most of the identified opinion phrases consist of a single word. In general, such short text spans cannot fully incorporate the contextual information relevant to the detection of subjective language (Wilson et al., 2005a). Therefore, we conjecture that good rationales should extend beyond short phrases.2 For simplicity, we choose to extend Opini"
P10-2062,N07-1033,0,0.092619,"Missing"
P10-2062,H05-1044,0,\N,Missing
P10-2062,D08-1004,0,\N,Missing
P11-1032,W10-0731,0,0.00549796,"Missing"
P11-1032,N10-1096,0,0.0218597,"iews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15 We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the ra"
P11-1032,P96-1041,0,0.0602621,"ximum likelihood classifier (Peng and Schuurmans, 2003): yˆ = arg max Pr(~x |y = c) (2) c Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to estimate individual language models, Pr(~x |y = c), for truthful and deceptive opinions. We consider all three n-gram feature sets, namely UNIGRAMS, BIGRAMS + , and TRIGRAMS+ , with corresponding language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996). We also train Support Vector Machine (SVM) classifiers, which find a high-dimensional separating hyperplane between two groups of data. To simplify feature analysis in Section 5, we restrict our evaluation to linear SVMs, which learn a weight vector w ~ and bias term b, such that a document ~x can be classified by: Text categorization In contrast to the other strategies just discussed, our text categorization approach to deception detection allows us to model both content and context with n-gram features. Specifically, we consider the following three n-gram feature sets, with the correspondi"
P11-1032,W06-1650,0,0.116071,"Missing"
P11-1032,P03-1054,0,0.0211199,"tification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15 We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four"
P11-1032,P09-2078,0,0.703668,"009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.14 Combined with the 400 deceptive reviews gathered in Section 3.1 this yields our final dataset of 800 reviews. 3.3 Human performance Assessing human deception detection performance is important for several reasons. First, there are few other baselines for our classification task; indeed, related studies (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009) have only considered a random guess baseline. Second, assessing human performance is necessary to validate the deceptive opinions gathered in Section 3.1. If human performance is low, then our deceptive opinions are convincing, and therefore, deserving of further attention. Our initial approach to assessing human performance on this task was with Mechanical Turk. Unfortunately, we found that some Turkers selected among the choices seemingly at random, presumably to maximize their hourly earnings by obviating the need to read the review. While a similar effect has been observed previously (Akk"
P11-1032,P08-1105,0,0.00648691,"Missing"
P11-1032,P07-2032,0,0.0078724,"Missing"
P11-1033,C10-1004,0,0.326788,"Missing"
P11-1033,D08-1014,0,0.325093,"which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the lang"
P11-1033,J96-1002,0,0.0285566,"Missing"
P11-1033,N06-1014,0,0.0413855,"and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by considering the noise in parallel data, we get: (5) just the labeled data. Then, in the E-step, the classifiers, based on current values of and , compute for each labeled example and assign probabilistically-weighted class labels to each unlabeled example. Next, in the M-step, the parameters, and , are updated using both the original labeled data ( and ) and the newly labeled data . These last two steps are iterated"
P11-1033,W02-2018,0,0.023691,"t as s, and as (6) where the first term on the right-hand side is the log likelihood of the labeled data from both and the second is the log likelihood of the unlabeled parallel data , multiplied by , a constant that controls the contribution of the unlabeled data; and is a regularization constant that penalizes model complexity or large 3. feature weights. When is 0, the algorithm ignores the unlabeled data and degenerates to two In the M-step, we can optimize the regularized MaxEnt models trained on only the labeled data. joint log likelihood using any gradient-based optimization technique (Malouf, 2002). The 3.3 The EM Algorithm on MaxEnt gradient for Equation 3 based on Equation 4 is To solve the optimization problem for the model, shown in Appendix A; those for Equations 5 and 6 we need to jointly estimate the optimal parameters can be derived similarly. In our experiments, we for the two monolingual classifiers by finding: use the L-BFGS algorithm (Liu et al., 1989) and run EM until the change in regularized joint log (7) likelihood is less than 1e-5 or we reach 100 This can be done with an EM algorithm, whose iterations.3 steps are summarized in Algorithm 1. First, the MaxEnt parameters,"
P11-1033,W06-1615,0,0.0215572,"rom one language (usually English) to other languages with few sentiment resources. Mihalcea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et 321 al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ra"
P11-1033,P07-1123,0,0.847904,"sed learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is av"
P11-1033,J05-4003,0,0.0121558,"ide is the likelihood of labeled data for both and ; and the second term is the likelihood of the unlabeled parallel data . If we assume that parallel sentences are perfect translations, the two sentences in each pair should have the same polarity label, which gives us: (4) where is the unobserved class label for the -th instance in the unlabeled data. This probability directly models the sentiment label agreement between and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by c"
P11-1033,D10-1005,0,0.168458,"Missing"
P11-1033,W10-2906,0,0.0120873,"hods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1"
P11-1033,D08-1092,0,0.032352,"g, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistic"
P11-1033,D08-1083,1,0.0862197,"Missing"
P11-1033,P09-1121,0,0.0337052,"0), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Be"
P11-1033,J94-4003,0,0.0600034,"Missing"
P11-1033,N10-1120,0,0.0150772,"ly for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpu"
P11-1033,J03-1002,0,0.00233983,"ctly models the sentiment label agreement between and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by considering the noise in parallel data, we get: (5) just the labeled data. Then, in the E-step, the classifiers, based on current values of and , compute for each labeled example and assign probabilistically-weighted class labels to each unlabeled example. Next, in the M-step, the parameters, and , are updated using both the original labeled data ( and ) and the newly"
P11-1033,W02-1011,0,0.0339296,"re bootstrapped by adding the most confident predicted examples from the unlabeled data into the training set. We run bootstrapping for 100 iterations. In each iteration, we select the most confidently predicted 50 positive and 50 negative sentences from each of the two classifiers, and take the union of the resulting 200 sentence pairs as the newly labeled training data. (Examples with conflicting labels within the pair are not included.) 5-fold cross-validation and report average accuracy (also MicroF1 in this case) and MacroF1 scores. Unigrams are used as binary features for all models, as Pang et al. (2002) showed that binary features perform better than frequency features for sentiment classification. The weights for unlabeled data and regularization, and , are set to 1 unless otherwise stated. Later, we will show that the proposed approach performs well with a wide range of parameter values.7 5.1 Method Comparison In our experiments, the methods are tested in the two data settings with the corresponding unlabeled parallel corpus as mentioned in Section 4.6 We use We first compare the proposed joint model (Joint) with the baselines in Table 2. As seen from the table, the proposed approach outpe"
P11-1033,P10-1114,0,0.355153,"he appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to si"
P11-1033,schulz-etal-2010-multilingual,0,0.0211262,"er consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpus with the following 3"
P11-1033,D08-1058,0,0.0612854,"a annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under st"
P11-1033,P09-1027,0,0.81898,"et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et 321 al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ratings. Unlike the methods described above, we focus on simultaneously improving th"
P11-1033,P10-1115,0,0.0157409,"gularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Berger et al., 1996; Ratnaparkhi, 1997; Smith, 2006). The models assign the conditional proba"
P11-1033,P09-1007,0,0.0101867,"r semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum en"
P11-1033,P02-1053,0,\N,Missing
P13-1136,P11-1049,0,0.258265,". Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while"
P13-1136,J96-1002,0,0.124803,"Missing"
P13-1136,briscoe-carroll-2002-robust,0,0.0538117,"other systems except the rule-based system). 7 Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). Grayed out words are removed. Queryirrelevant phrases, such as temporal information or source of the news, have been removed. dependency-tree based compressor (Martins and Smith, 2009)8 . We adopt the metrics in Martins and Smith (2009) to measure the unigram-level macro precision, recall, and F1-measure with respect to human annotated compression. In addition, we also compute the F1 scores of grammatical relations which are annotated by RASP (Briscoe and Carroll, 2002) according to Clarke and Lapata (2008). In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p < 0.01) higher precisions (Uni8 Thanks to Andr´e F.T. Martins for system outputs. Conclusion We have presented a framework for query-focused multi-document summarization based on sentence compression. We propose three types of compression approaches. Our tree-based compression method can easily incorporate measures of query relevance, content importance, redundancy and language quality into the compression process. By testing on a standard dat"
P13-1136,P11-1050,0,0.017869,"Missing"
P13-1136,P06-1039,0,0.0127762,"Missing"
P13-1136,W03-0501,0,0.238213,"Missing"
P13-1136,N04-1001,0,0.0120169,"and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40"
P13-1136,P07-2015,0,0.0521182,"Missing"
P13-1136,N07-1023,0,0.0698946,", 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-pr"
P13-1136,W09-1802,0,0.0755725,"gned submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley"
P13-1136,N09-1041,0,0.0486933,"sis applications including openended question answering, recommender systems, and summarization of search engine results. As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster new research on automatic summarization in the context of users’ needs. To date, most top-performing systems for multi-document summarization—whether queryspecific or not—remain largely extractive: their summaries are comprised exclusively of sentences selected directly from the documents to be summarized (Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-T¨ur, 2011). Despite their simplicity, extractive approaches have some disadvantages. First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. In addition, when people write summaries, they tend to abstract the content and seldom use entire sentences taken verbatim from the original documents. In news articles, for example, most sentences are lengthy and contain both potentially useful information for a summary as well as unnecessary"
P13-1136,P11-1052,0,0.00996893,"pervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has bee"
P13-1136,C00-1072,0,0.0769395,"we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the sum"
P13-1136,N03-1020,0,0.164068,"Missing"
P13-1136,W03-1101,0,0.0260883,"efully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust proba"
P13-1136,H05-1083,0,0.0143206,". DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for t"
P13-1136,H05-1115,0,0.0142511,"ments over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (s"
P13-1136,P05-1036,0,0.042728,"candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by u"
P13-1136,P04-1018,0,0.0103922,"or the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt cl"
P13-1136,W09-1801,0,0.781754,"e) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tre"
P13-1136,E06-1038,0,0.216493,"ches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing. First, sentence ranking determines the importance of each sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sen"
P13-1136,N04-1019,0,0.0713979,"edundancy within the summary through compression. Furthermore, our H EAD-driven beam search method with M ULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (p < 0.01). Its R-SU4 score is also significantly (p < 0.01) better than extractive methods, rule-based and sequence-based compression methods on both DUC 2006 and 2007. Moreover, our systems with learning-based compression have considerable compression rates, indicating their capability to remove superfluous words as well as improve summary quality. Human Evaluation. The Pyramid (Nenkova and Passonneau, 2004) evaluation was developed to manually assess how many relevant facts or Summarization Content Units (SCUs) are captured by system summaries. We ask a professional annotator (who is not one of the authors, is highly experienced in annotating for various NLP tasks, and is fluent in English) to carry out a Pyramid evaluation on 10 randomly selected topics from 4 We looked at various beam sizes on the heldout data, and observed that the performance peaks around this value. 5 ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d 6 The system output from Davis et al. (2012) is n"
P13-1137,D10-1049,0,0.0275358,"the meeting transcripts. On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style. 1395 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows u"
P13-1137,N03-1003,0,0.0149501,"us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multiple Sequence Alignment (MSA) (Durbin et al., 1998), to induce domain-independent templates that guide abstract generation. MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al., 1998) and has also been employed for learning paraphrases (Barzilay and Lee, 2003). • Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types o"
P13-1137,W09-3934,0,0.189992,"ng as a whole, they refer to summaries of a specific aspect of a meeting, such as the DECISIONS reached, PROBLEMS discussed, PROGRESS made or AC TION ITEMS that emerged (Carenini et al., 2011). Our goal is to provide an automatic summarization system that can generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner. Existing meeting summarization systems remain largely extractive: their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original co"
P13-1137,P11-1054,0,0.0516102,"er of content words that are also in previous DA indicator/argument only contains stopword? number of new nouns Content Features has capitalized word? has proper noun? TF/IDF/TFIDF min/max/average Discourse Features main speaker or not? is in an adjacency pair (AP)? is in the source/target of the AP? number of source/target DA in the AP is the target of the AP a positive/negative/neutral response? is the source of the AP a question? Syntax Features indicator/argument constituent tag dependency relation of indicator and argument tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, hwant, an LCD display with a spinning wheeli and hpush-buttons, on the outsidei are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicat"
P13-1137,W06-1643,0,0.108062,"or full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Dialogue Acts: C: Looking at what we've got, we we want [an LCD display with a spinning wheel]. B: You have to have some push-buttons, don't you? C: Just spinning and not sc"
P13-1137,N10-1086,0,0.0580777,"2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multipl"
P13-1137,P03-1054,0,0.0576942,"uent tag dependency relation of indicator and argument tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, hwant, an LCD display with a spinning wheeli and hpush-buttons, on the outsidei are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicator to be a noun or verb; the eligible arguments is a noun phrase (NP), prepositional phrase (PP) or adjectival phrase (ADJP). A valid indicator-argument pair should have at least one content word and satisfy one of the following constraints: • When the indicator is a noun, the argument has to be a modifier or complement of the indicator. • When the indicator is a verb, the argument has to be the subject or the object if it is an NP, or a modifier or complement of the indicator if"
P13-1137,P12-1039,0,0.0497661,"pts. On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style. 1395 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformula"
P13-1137,N10-1134,0,0.0793183,"Missing"
P13-1137,N03-1020,0,0.106053,"also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same fea1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4 . Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005; Xie et al., 2008). We report ROUGE-2 (R2) and ROUGE-SU4 (R-SU4) that are shown to correlate with human evaluation reasonably well. In AMI, four meetings of different functions are carried out in each group5 . 35 meetings for “conceptual design” are randomly selected for testing. For ICSI, we reserve 12 meetings for testing. The R-SU4 scores f"
P13-1137,P09-2066,0,0.0267631,"ed exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original conversation written in new language. A user study from Murray et al. (2010b) showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational. Consider, for example, the two types of focused summary along with their associated dialogue snippets in Figure 1. We can see that extracts are likely to include unnecessary and noisy information from the meeting transcripts. On the contrary, the manually compose"
P13-1137,W10-4211,0,0.0247195,"the final summary (see Section 5.3). determined by an external ontology. Liu and Liu (2009) apply sentence compression on extracted summary utterances. Though some of the unnecessary words are dropped, the resulting compressions can still be ungrammatical and unstructured. This work is also broadly related to expert system-based language generation (Reiter and Dale, 2000) and concept-to-text generation tasks (Angeli et al., 2010; Konstas and Lapata, 2012), where the generation process is decomposed into content selection (or text planning) and surface realization. For instance, Angeli et al. (2010) learn from structured database records and parallel textual descriptions. They generate texts based on a series of decisions made to select the records, fields, and proper templates for rendering. Those techniques that are tailored to specific domains (e.g. weather forecasts or sportcastings) cannot be directly applied to the conversational data, as their input is well-structured and the templates learned are domain-specific. 3 Framework Our domain-independent abstract generation framework produces a summarizer that generates a grammatical abstract from a cluster of meeting-element-related di"
P13-1137,N01-1003,0,0.1162,"Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction a"
P13-1137,W11-0503,1,0.882524,"usterings setting, we use the annotations to create perfect partitions of the DAs for input to the system; in the System Figure 4: Content selection evaluation by using ROUGE-SU4 (multiplied by 100). SVM-DA and SVM-T OKEN denotes for supervised extract-based methods with SVMs on utterance- and token-level. Summaries for decision, problem, action item, and progress are generated and evaluated for AMI and ICSI (with names in parentheses). X-axis shows the number of meetings used for training. Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in (Wang and Cardie, 2011). DAs are grouped according to a classifier trained beforehand. Baselines and Comparisons. We compare our system with (1) two unsupervised baselines, (2) two supervised extractive approaches, and (3) an oracle derived from the gold standard abstracts. Baselines. As in Riedhammer et al. (2010), the L ONGEST DA in each cluster is selected as the summary. The second baseline picks the cluster prototype (i.e. the DA with the largest TFIDF similarity with the cluster centroid) as the summary according to Wang and Cardie (2011). Although it is possible that important content is spread over multiple"
P13-1137,W12-1642,1,0.897104,"rom the same system trained on in-domain data, and statistically significantly outperform supervised extractive summarization approaches trained on in-domain data. 2 Related Work Most research on spoken dialogue summarization attempts to generate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming app"
P13-1137,P02-1040,0,0.0912979,"baselines and supervised systems. The learning curve of our system is relatively flat, which means not many training meetings are required to reach a usable performance level. Note that the ROUGE scores are relative low when the reference summaries are human abstracts, even for evaluation among abstracts produced by different annotators (Dang, 2005). The intrinsic difference of styles between dialogue and human abstract further lowers the scores. But the trend is still respected among the systems. Abstract Generation Evaluation. To evaluate the full abstract generation system, the BLEU score (Papineni et al., 2002) (the precision of unigrams and bigrams with a brevity penalty) is computed with human abstracts as reference. BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems (Angeli et al., 2010; Konstas and Lapata, 2012). 4 We use SVMlight (Joachims, 1999) with RBF kernel by default parameters for SVM-based classifiers and regressor. 5 The four types of meetings in AMI are: project kick-off (35 meetings), functional design (35 meetings), conceptual design (35 meetings), and detailed design (34 meetings). Figure 5: Full abstract gen"
P13-1137,W10-2603,0,0.0757588,"ter prototype (i.e. the DA with the largest TFIDF similarity with the cluster centroid) as the summary according to Wang and Cardie (2011). Although it is possible that important content is spread over multiple DAs, both baselines allow us to determine summary quality when summaries are restricted to a single utterance. Supervised Learning. We also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same fea1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4 . Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005"
P13-1137,N10-1132,0,\N,Missing
P13-1161,H05-1045,1,0.7387,"corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relatio"
P13-1161,W06-1651,1,0.18183,"stics, pages 1640–1649, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS - FROM linking relations, and demonstrated the effectiveness of joint inference. Their ILP formulation, however, does not handle implicit linking relations, i.e. opinion expressions with no explicit opinion holder; nor does it consider IS - ABOUT relations. In this paper, we present a model that jointly identifies opinion-related entities, including opinion expressions, opinion targets and opinion holders as well as the associated opinion linking relations, IS - ABOUT and IS - FROM. For each"
P13-1161,S12-1029,0,0.0368286,"e problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opinion entities and opinion relations. Specifically, given a sentence, our goal is to identify spans of opinion expressions, opinion arguments (targets and holders) and their associated linking relations. Training data consists"
P13-1161,C10-1059,0,0.129362,"our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Koba"
P13-1161,W10-2910,0,0.0440603,"our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Koba"
P13-1161,W06-0301,0,0.548807,"ang and Lee, 2008). Nevertheless, much progress has been made in extracting opinion information from text. Sequence labeling models have been successfully employed to identify opinion expressions (e.g. (Breck et al., 1640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS - FROM linking relations, and demonstrated the effectiveness of joint inference. Their ILP formulation, however, does not handle implicit linking relations, i.e. opinion expressions with no expl"
P13-1161,D07-1114,0,0.448837,"Missing"
P13-1161,D12-1123,0,0.0147823,"implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Rot"
P13-1161,J08-2005,0,0.122421,"targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opinion entities and opinion relations. Specifically, given a sentence, our goal is to identify spans of opinion expressions, opinion arguments (targets and holders) and their associate"
P13-1161,J09-3003,0,0.0109422,"elations; and evidence of opinion relations might provide clues to guide the accurate extraction of opinion entities. We evaluate our approach using a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)) and demonstrate that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targe"
P13-1161,D09-1159,0,0.048261,"ets, (2) handles implicit opinion relations (opinion expressions without any associated argument), and (3) uses a simpler ILP formulation. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is th"
P13-1161,D12-1122,1,0.523587,"tes” in S2). Not surprisingly, fine-grained opinion extraction is a challenging task due to the complexity and variety of the language used to express opinions and their components (Pang and Lee, 2008). Nevertheless, much progress has been made in extracting opinion information from text. Sequence labeling models have been successfully employed to identify opinion expressions (e.g. (Breck et al., 1640 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2007; Yang and Cardie, 2012)) and relation extraction techniques have been proposed to extract opinion holders and targets based on their linking relations to the opinion expressions (e.g. Kim and Hovy (2006), Kobayashi et al. (2007)). However, most existing work treats the extraction of different opinion entities and opinion relations in a pipelined manner: the interaction between different extraction tasks is not modeled jointly and error propagation is not considered. One exception is Choi et al. (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS - FROM linking"
P13-1161,C10-2167,0,0.0659611,"Missing"
P13-1161,J11-1002,0,0.0386673,"n. There has also been substantial interest in opinion extraction from product reviews (Liu, 2012). Most existing approaches focus on the extraction of opinion targets and their associated opinion expressions and usually employ a pipeline architecture: generate candidates of opinion expressions and opinion targets first, and then use rule-based or machine-learning-based approaches to identify potential relations between opinions and targets (Hu and Liu, 2004; Wu et al., 2009; Liu et al., 2012). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint in"
P13-1161,W04-2401,0,0.264451,"12). In addition to pipeline approaches, bootstrapping-based approaches were proposed (Qiu et al., 2009; Qiu et al., 2011; Zhang et al., 2010) to identify opinion expressions and targets iteratively; however, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opi"
P13-1161,ruppenhofer-etal-2008-finding,0,0.3607,"nto fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and targets) by identifying opinion expressions first and then searching for the most likely target for each opinion expression via a binary relation classifier. All these methods extract opinion arguments and opinion relations in separate stages instead"
P13-1161,D11-1012,0,0.00938357,"ever, they suffer from the problem of error propagation. There is much work demonstrating the benefit of performing global inference. Roth and Yih 1641 (2004) proposed a global inference approach in the formulation of a linear program (LP) and applied it to the task of extracting named entities and relations simultaneously. Their problem is similar to ours — the difference is that Roth and Yih Roth and Yih (2004) assume that named entity spans are known a priori and only their labels need to be assigned. Joint inference has also been applied to semantic role labeling (Punyakanok et al., 2008; Srikumar and Roth, 2011; Das et al., 2012), where the goal is to jointly identify semantic arguments for given lexical predicates. The problem is conceptually similar to identifying opinion arguments for opinion expressions, however, we do not assume prior knowledge of opinion expressions (unlike in SRL, where predicates are given). 3 Model As proposed in Section 1, we consider the task of jointly identifying opinion entities and opinion relations. Specifically, given a sentence, our goal is to identify spans of opinion expressions, opinion arguments (targets and holders) and their associated linking relations. Trai"
P13-1161,C08-1103,1,0.807587,"e that our model outperforms by a significant margin traditional baselines that do not employ joint inference for extracting opinion entities and different types of opinion relations. 2 Related Work Significant research effort has been invested into fine-grained opinion extraction for open-domain text such as news articles (Wiebe et al., 2005; Wilson et al., 2009). Many techniques were proposed to identify the text spans for opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010b; Yang and Cardie, 2012)), opinion holders (e.g. (Choi et al., 2005)) and topics of opinions (Stoyanov and Cardie, 2008). Some consider extracting opinion targets/holders along with their relation to the opinion expressions. Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Ruppenhofer et al. (2008) argued that semantic role labeling is not sufficient for identifying opinion holders and targets. Johansson and Moschitti (2010a) extract opinion expressions and holders by applying reranking on top of sequence labeling methods. Kobayashi et al. (2007) considered extracting “aspect-evaluation” relations (relations between opinion expressions and target"
P13-1161,J13-3002,0,\N,Missing
P13-2039,P11-1032,1,0.75723,"alities of reviewers could predict spammers, without using any textual features. Li et al. (2011) carefully explored review-related features based on content and sentiment, training a semi-supervised classifier for opinion spam detection. However, the disadvantages of standard supervised learning methods are obvious. First, they do not generally provide readers with a clear probabilistic preIntroduction Consumers rely increasingly on user-generated online reviews to make purchase decisions. Positive opinions can result in significant financial gains. This gives rise to deceptive opinion spam (Ott et al., 2011; Jindal et al., 2008), fake reviews written to sound authentic and deliberately mislead readers. Previous research has shown that humans have difficulty distinguishing fake from truthful reviews, operating for the most part at chance (Ott et al., 2011). Consider, for example, the following two hotel reviews. One is truthful and the other is deceptive1 : 1. My husband and I stayed for two nights at the Hilton Chicago. We were very pleased with the accommodations and enjoyed the service every minute of it! The bedrooms are immaculate, and the linens are very soft. We also appreciated the free w"
P13-2039,D09-1026,0,0.19765,"Missing"
P13-2039,P10-1066,0,0.0510505,"Missing"
P14-1031,D08-1083,1,0.959236,"-level) has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks (Pang and Lee, 2008). In this paper, we focus on the task of sentencelevel sentiment classification in online reviews. Typical approaches to the task employ supervised 325 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 325–335, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al. (2010) uses tree-CRF to model word interactions based on dependency tree structures; Choi and Cardie (2008) applies compositional inference rules to handle polarity reversal; Socher et al. (2011) and Socher et al. (2013) compute compositional vector representations for words and phrases and use them as features in a classifier. However, the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity. Obtaining sentiment labels at the fine-grained level is costly. Semi-supervised techniques have been proposed for sentence-level sentiment classification (T¨ackstr¨om and McDonald, 2011a; Qu et al., 2012). However, they rely on a large amount of docum"
P14-1031,J13-4004,0,0.00699048,"y) = fc,s (xi , yi , yi−1 ) Our coreference relations indicated by opinion targets overlap with the same target relation introduced in (Somasundaran et al., 2009). The differences are: (1) we encode the coreference relations as soft constraints during learning instead of applying them as hard constraints during inference time; (2) our constraints can apply to both polar and non-polar sentences; (3) our identification of coreference relations is automatic without any fine-grained annotations for opinion targets. To extract coreferential opinion targets, we apply Stanford’s coreference system (Lee et al., 2013) to extract coreferential mentions in the document, and then apply a set of syntactic rules to identify opinion targets from the extracted mentions. The syntactic rules correspond to the shortest dependency paths between an opinion word and an extracted mention. We consider the 10 most frequent dependency paths in the training data. Example dependency paths include nsubj(opinion, mention), nobj(opinion, mention), and amod(mention, opinion). For sentences connected by the opinion coreference relation, we expect their sentiment to be consistent. To encode this intuition, we define the following"
P14-1031,D09-1062,1,0.822918,"nd three-way classification results (positive, negative or neutral). We use accuracy as the performance measure. In our tables, boldface numbers are statistically significant by paired t-test for p < 0.05 against the best baseline developed in this paper 7 . We trained our model using a CRF incorporated with the proposed posterior constraints. For the CRF features, we include the tokens, the partof-speech tags, the prior polarities of lexical patterns indicated by the opinion lexicon and the negator lexicon, the number of positive and negative tokens and the output of the vote-flip algorithm (Choi and Cardie, 2009). In addition, we include the discourse connectives as local or transition features and the document-level sentiment labels as features (only available in the MD dataset). We set the CRF regularization parameter σ = 1 and set the posterior regularization parameter β and γ (a trade-off parameter we introduce to balance the supervised objective and the posterior regularizer in 2) by using grid search 8 . For approximation inference with higher-order constraints, we perform 2000 Gibbs sampling iterations where the first 1000 iterations are burn-in iterations. To make the results more stable, we c"
P14-1031,P07-1055,0,0.015912,"that our model outperforms state-ofthe-art methods in both the supervised and semisupervised settings. We also show that discourse knowledge is highly useful for improving sentence-level sentiment classification. 2 The second idea is to exploit sentiment signals at the inter-sentential level. Polanyi and Zaenen (2006) argue that discourse structure is important in polarity classification. Various attempts have been made to incorporate discourse relations into sentiment analysis: Pang and Lee (2004) explored the consistency of subjectivity between neighboring sentences; Mao and Lebanon (2007),McDonald et al. (2007), and T¨ackstr¨om and McDonald (2011a) developed structured learning models to capture sentiment dependencies between adjacent sentences; Kanayama and Nasukawa (2006) and Zhou et al. (2011) use discourse relations to constrain two text segments to have either the same polarity or opposite polarities; Trivedi and Eisenstein (2013) and Lazaridou et al. (2013) encode the discourse connectors as model features in supervised classifiers. Very little work has explored long-distance discourse relations. Somasundaran et al. (2008) define opinion target relations and apply them to constrain the polarit"
P14-1031,N10-1120,0,0.177606,"We hope the expectation of the constraint function takes a small value. In our experiments, we set the expected value to be the empirical estimate of the probability of “conflicting” sentiment in polar documents using the training data. 3.3 exp(θ · f (x, y) + λ · φ(x, y)) Zλ,θ (X) Training and Inference During training, we need to compute the constraint expectations and the feature expectations under the auxiliary distribution q at each gradient step. 6 Available at http://www.cs.uic.edu/˜liub/ FBS/sentiment-analysis.html. 330 Methods CRF CRF-inflex CRF-infdisc PRlex PR Previous work TreeCRF (Nakagawa et al., 2010) Dropout LR (Wang and Manning, 2013) both the available unlabeled data and the test data. For each domain in the MD dataset, we made use of no more than 100 unlabeled documents in which our posterior constraints apply. We adopted the evaluation schemes used in previous work: 10fold cross validation for the CR dataset and 3-fold cross validation for the MD dataset. We also report both two-way classification (positive vs. negative) and three-way classification results (positive, negative or neutral). We use accuracy as the performance measure. In our tables, boldface numbers are statistically si"
P14-1031,D13-1205,0,0.0562514,"h local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning. Specifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization (PR) (Ganchev et al., 2010). As a framework for structured learning with constraints, PR has been successfully applied to many structural NLP tasks (Ganchev et al., 2009; Ganchev et al., 2010; Ganchev and Das, 2013). Our work is the first to explore PR for sentiment analysis. Unlike most previous work, we explore a rich set of structural constraints that cannot be naturally encoded in the feature-label form, and show that such constraints can improve the performance of the CRF model. We evaluate our approach on the sentencelevel sentiment classification task using two standard product review datasets. Experimental results show that our model outperforms state-ofthe-art methods in both the supervised and semisupervised settings. We also show that discourse knowledge is highly useful for improving sentence"
P14-1031,P09-1042,0,0.194171,"ncorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning. Specifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization (PR) (Ganchev et al., 2010). As a framework for structured learning with constraints, PR has been successfully applied to many structural NLP tasks (Ganchev et al., 2009; Ganchev et al., 2010; Ganchev and Das, 2013). Our work is the first to explore PR for sentiment analysis. Unlike most previous work, we explore a rich set of structural constraints that cannot be naturally encoded in the feature-label form, and show that such constraints can improve the performance of the CRF model. We evaluate our approach on the sentencelevel sentiment classification task using two standard product review datasets. Experimental results show that our model outperforms state-ofthe-art methods in both the supervised and semisupervised settings. We also show that discourse kno"
P14-1031,P04-1035,0,0.222794,"e sentencelevel sentiment classification task using two standard product review datasets. Experimental results show that our model outperforms state-ofthe-art methods in both the supervised and semisupervised settings. We also show that discourse knowledge is highly useful for improving sentence-level sentiment classification. 2 The second idea is to exploit sentiment signals at the inter-sentential level. Polanyi and Zaenen (2006) argue that discourse structure is important in polarity classification. Various attempts have been made to incorporate discourse relations into sentiment analysis: Pang and Lee (2004) explored the consistency of subjectivity between neighboring sentences; Mao and Lebanon (2007),McDonald et al. (2007), and T¨ackstr¨om and McDonald (2011a) developed structured learning models to capture sentiment dependencies between adjacent sentences; Kanayama and Nasukawa (2006) and Zhou et al. (2011) use discourse relations to constrain two text segments to have either the same polarity or opposite polarities; Trivedi and Eisenstein (2013) and Lazaridou et al. (2013) encode the discourse connectors as model features in supervised classifiers. Very little work has explored long-distance d"
P14-1031,prasad-etal-2008-penn,0,0.00718842,"atterns along with their sentiment values as feature-label constraints. The constraint function can be written as X φw (x, y) = fw (xi , yi ) Discourse Connectives. Lexical patterns can be limited in capturing contextual information since they only look at interactions between words within an expression. To capture context at the clause or sentence level, we consider discourse connectives, which are cue phrases or words that indicate discourse relations between adjacent sentences or clauses. To identify discourse connectives, we apply a discourse tagger trained on the Penn Discourse Treebank (Prasad et al., 2008) 4 to our data. Discourse connectives are tagged with four senses: Expansion, Contingency, Comparison, Temporal. Discourse connectives can operate at both intrasentential and inter-sentential level. For example, the word “although” is often used to connect two polar clauses within a sentence, while the word “however” is often used to at the beginning of the sentence to connect two polar sentences. It is important to distinguish these two types of discourse connectives. We consider a discourse connective to be intra-sentential if it has the Comparison sense and connects two polar clauses with o"
P14-1031,D12-1014,0,0.248268,"ons based on dependency tree structures; Choi and Cardie (2008) applies compositional inference rules to handle polarity reversal; Socher et al. (2011) and Socher et al. (2013) compute compositional vector representations for words and phrases and use them as features in a classifier. However, the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity. Obtaining sentiment labels at the fine-grained level is costly. Semi-supervised techniques have been proposed for sentence-level sentiment classification (T¨ackstr¨om and McDonald, 2011a; Qu et al., 2012). However, they rely on a large amount of document-level sentiment labels that may not be naturally available in many domains. In this paper, we propose a sentence-level sentiment classification method that can (1) incorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning. Specifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraint"
P14-1031,J00-4006,0,0.00712633,"intuition, we define the following constraint function: X φcoref (x, y) = fcoref (xi , xj , yi , yj ) i where c denotes a discourse connective, s indicates its sense, and fc,s is a penalty function that takes value 1.0 when yi and yi−1 form a contradictory sentiment transition, that is, yi 6=polar yi−1 if s ∈ {Expansion, Contingency}, or yi =polar yi−1 if s = Comparison. The desired value for the constraint expectation is set to 0 so that the model is encouraged to have less constraint violations. Opinion Coreference Sentences in a discourse can be linked by many types of coherence relations (Jurafsky et al., 2000). Coreference is one of the commonly used relations in written text. In this work, we explore coreference in the context of sentence-level sentiment analysis. We consider a set of polar sentences to be linked by the opinion coreference relation if they contain coreferring opinion-related entities. For example, the following sentences express opinions towards “the speaker phone”, “The speaker phone” and “it” respectively. As these opinion targets are coreferential (referring to the same entity “the speaker phone”), they are linked by the opinion coreference relation 5 . i,ant(i)=j,j≥0 My favori"
P14-1031,W06-1642,0,0.0503228,"Missing"
P14-1031,D11-1014,0,0.0549768,"mportance in supporting these opinion analysis tasks (Pang and Lee, 2008). In this paper, we focus on the task of sentencelevel sentiment classification in online reviews. Typical approaches to the task employ supervised 325 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 325–335, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al. (2010) uses tree-CRF to model word interactions based on dependency tree structures; Choi and Cardie (2008) applies compositional inference rules to handle polarity reversal; Socher et al. (2011) and Socher et al. (2013) compute compositional vector representations for words and phrases and use them as features in a classifier. However, the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity. Obtaining sentiment labels at the fine-grained level is costly. Semi-supervised techniques have been proposed for sentence-level sentiment classification (T¨ackstr¨om and McDonald, 2011a; Qu et al., 2012). However, they rely on a large amount of document-level sentiment labels that may not be naturally available in many domains. In this"
P14-1031,P13-1160,0,0.41224,"expresses sentiment towards the same aspect – the music – as the first sentence; the third sentence expands the second sentence with the discourse connective In fact. These discourse-level relations help indicate that sentence 2 and 3 are likely to have positive sentiment as well. The importance of discourse for sentiment analysis has become increasingly recognized. Most existing work considers discourse relations between adjacent sentences or clauses and incorporates them as constraints (Kanayama and Nasukawa, 2006; Zhou et al., 2011) or features in classifiers Trivedi and Eisenstein (2013; Lazaridou et al. (2013). Very little work has explored long-distance discourse relations for sentiment analysis. Somasundaran et al. (2008) defines coreference relations on opinion targets and applies them to constrain the polarity of sentences. Introduction The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization, opinion question answering and opinion retrieval. Accordingly, extracting sentiment at the fine-grained level (e.g. at the sentence- or phrase-level) has received increasing attention recently due to its challenging nature and its importance"
P14-1031,C08-1101,0,0.0931606,"second sentence with the discourse connective In fact. These discourse-level relations help indicate that sentence 2 and 3 are likely to have positive sentiment as well. The importance of discourse for sentiment analysis has become increasingly recognized. Most existing work considers discourse relations between adjacent sentences or clauses and incorporates them as constraints (Kanayama and Nasukawa, 2006; Zhou et al., 2011) or features in classifiers Trivedi and Eisenstein (2013; Lazaridou et al. (2013). Very little work has explored long-distance discourse relations for sentiment analysis. Somasundaran et al. (2008) defines coreference relations on opinion targets and applies them to constrain the polarity of sentences. Introduction The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization, opinion question answering and opinion retrieval. Accordingly, extracting sentiment at the fine-grained level (e.g. at the sentence- or phrase-level) has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks (Pang and Lee, 2008). In this paper, we focus on the task of sentenceleve"
P14-1031,D09-1018,0,0.041093,"arity. A series of sentences connected via a listing tend to have the same polarity. The sentence-level polarity tends to be consistent with the document-level polarity. Lexical patterns Discourse Connectives (clause) Discourse Connectives (sentence) Coreference Listing patterns Global labels Inter-sentential X X X X Table 1: Summarization of Posterior Constraints for Sentence-level Sentiment Classification define the following constraint function: X φc,s (x, y) = fc,s (xi , yi , yi−1 ) Our coreference relations indicated by opinion targets overlap with the same target relation introduced in (Somasundaran et al., 2009). The differences are: (1) we encode the coreference relations as soft constraints during learning instead of applying them as hard constraints during inference time; (2) our constraints can apply to both polar and non-polar sentences; (3) our identification of coreference relations is automatic without any fine-grained annotations for opinion targets. To extract coreferential opinion targets, we apply Stanford’s coreference system (Lee et al., 2013) to extract coreferential mentions in the document, and then apply a set of syntactic rules to identify opinion targets from the extracted mention"
P14-1031,P11-2100,0,0.0529482,"Missing"
P14-1031,N13-1100,0,0.0773255,"see that: the second sentence expresses sentiment towards the same aspect – the music – as the first sentence; the third sentence expands the second sentence with the discourse connective In fact. These discourse-level relations help indicate that sentence 2 and 3 are likely to have positive sentiment as well. The importance of discourse for sentiment analysis has become increasingly recognized. Most existing work considers discourse relations between adjacent sentences or clauses and incorporates them as constraints (Kanayama and Nasukawa, 2006; Zhou et al., 2011) or features in classifiers Trivedi and Eisenstein (2013; Lazaridou et al. (2013). Very little work has explored long-distance discourse relations for sentiment analysis. Somasundaran et al. (2008) defines coreference relations on opinion targets and applies them to constrain the polarity of sentences. Introduction The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization, opinion question answering and opinion retrieval. Accordingly, extracting sentiment at the fine-grained level (e.g. at the sentence- or phrase-level) has received increasing attention recently due to its challenging n"
P14-1031,D13-1097,0,0.0447055,"arning models to capture sentiment dependencies between adjacent sentences; Kanayama and Nasukawa (2006) and Zhou et al. (2011) use discourse relations to constrain two text segments to have either the same polarity or opposite polarities; Trivedi and Eisenstein (2013) and Lazaridou et al. (2013) encode the discourse connectors as model features in supervised classifiers. Very little work has explored long-distance discourse relations. Somasundaran et al. (2008) define opinion target relations and apply them to constrain the polarity of text segments annotated with target relations. Recently, Zhang et al. (2013) explored the use of explanatory discourse relations as soft constraints in a Markov Logic Network framework for extracting subjective text segments. Leveraging both ideas, our approach exploits sentiment signals from both intra-sentential and inter-sentential context. It has the advantages of utilizing rich discourse knowledge at different levels of context and encoding it as soft constraints during learning. Our approach is also semi-supervised. Compared to the existing work on semi-supervised learning for sentence-level sentiment classification (T¨ackstr¨om and McDonald, 2011a; T¨ackstr¨om"
P14-1031,D11-1015,0,0.54685,"sentences within the discourse context, we can see that: the second sentence expresses sentiment towards the same aspect – the music – as the first sentence; the third sentence expands the second sentence with the discourse connective In fact. These discourse-level relations help indicate that sentence 2 and 3 are likely to have positive sentiment as well. The importance of discourse for sentiment analysis has become increasingly recognized. Most existing work considers discourse relations between adjacent sentences or clauses and incorporates them as constraints (Kanayama and Nasukawa, 2006; Zhou et al., 2011) or features in classifiers Trivedi and Eisenstein (2013; Lazaridou et al. (2013). Very little work has explored long-distance discourse relations for sentiment analysis. Somasundaran et al. (2008) defines coreference relations on opinion targets and applies them to constrain the polarity of sentences. Introduction The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization, opinion question answering and opinion retrieval. Accordingly, extracting sentiment at the fine-grained level (e.g. at the sentence- or phrase-level) has receive"
P14-1031,miltsakaki-etal-2004-penn,0,\N,Missing
P14-1031,D13-1170,0,\N,Missing
P14-1147,I13-1039,0,0.0487601,"on the review text, reviewer, and product to identify duplicate opinions, i.e., opinions that appear more than once in the corpus with similar contexts. Wu et al. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguistic differences between them. Ott et al. created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features. In addition to exploring text or linguistic features in deception, some existing work looks into customers’ behavior to identify deception (Mukherjee et al., 2013a). For example, Mukherjee et al. (2011; 2012) delved into group behavior to identify group"
P14-1147,P12-2034,0,0.368025,"Missing"
P14-1147,P13-2039,1,0.838567,"on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general population that generate fake reviews, or in other words, Ott et al.’s data set may correspond to only one type of online deceptive opinion spam — fake reviews generated by people who have never been to offerings or experienced the entities. Specifically, according to their findings (Ott et al., 2011; has updated their guidelines on the use of endorsements and testimonials in advertising to suggest that"
P14-1147,D13-1199,1,0.866691,"on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general population that generate fake reviews, or in other words, Ott et al.’s data set may correspond to only one type of online deceptive opinion spam — fake reviews generated by people who have never been to offerings or experienced the entities. Specifically, according to their findings (Ott et al., 2011; has updated their guidelines on the use of endorsements and testimonials in advertising to suggest that"
P14-1147,P11-1032,1,0.703381,"used on developing supervised learningbased algorithms to help users identify deceptive opinion spam, which are highly dependent upon high-quality gold-standard labeled data (Jindal and Liu, 2008; Jindal et al., 2010; Lim et al., 2010; Wang et al., 2011; Wu et al., 2010). Studies in the literature rely on a couple of approaches for obtaining labeled data, which usually fall into two categories. The first relies on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the ge"
P14-1147,N13-1053,1,0.453732,"Missing"
P14-1147,D13-1113,0,0.00665599,"res from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features. In addition to exploring text or linguistic features in deception, some existing work looks into customers’ behavior to identify deception (Mukherjee et al., 2013a). For example, Mukherjee et al. (2011; 2012) delved into group behavior to identify group of reviewers who work collaboratively to write fake reviews. Qian and Liu (2013) identified multiple user IDs that are generated by the same author, as these authors are more likely to generate deceptive reviews. In the psychological literature, researchers have looked into possible linguistic cues to deception (Newman et al., 2003), such as decreased spatial detail, which is consistent with theories of reality monitoring (Johnson and Raye, 1981), increased negative emotion terms (Newman et al., 2003), or the writing style difference between informative (truthful) and imaginative (deceptive) writings in (Rayson et al., 2001). The former typically consists of more nouns, a"
P14-2113,W11-0707,0,0.428721,"Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation “nsubj(wrong, you)” becomes “nsubj(SentiWordneg , you)”. 4 4.1 Online Dispute Detection Training A Sentiment Classifier Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011) on a 5point scale (i.e. NN, N, O, P, PP). AAWD consists of 221 English Wikipedia discussions with positive and negative alignment annotations. Annotators either label each sentence as positive, negative or neutral, or label the full turn. For instances that have only a turn-level label, we assume all sentences have the same label as the turn. We further transform the labels into the five sentiment labels. Sentences annotated as being a positive alignment by at least two annotators are treated as very positive (PP). If a sentence is only selected as positive by one annotator or obtains the lab"
P14-2113,esuli-sebastiani-2006-sentiwordnet,0,0.0174906,"con). Concretely, we take a lexicon M = Mp ∪Mn , where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume µhσ,wi encodes the weight between label σ and feature w, for each feature w ∈ Mp ; then the isotonic CRF enforces σ ≤ σ 0 ⇒ µhσ,wi ≤ µhσ0 ,wi . For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn . Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber,"
P14-2113,W10-3001,0,0.0252497,"Missing"
P14-2113,P04-1085,0,0.0539025,"ages (3609 disputes, 3609 nondisputes).3 We find that classifiers that employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach to automatically identify online disputes on a dataset of scale. Additional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the ar"
P14-2113,N06-2014,0,0.155289,"a”. Omitted sentences are indicated by ellipsis. Names of editors are in bold. The start of each set of related turns is numbered; “&gt;” is an indicator for the reply structure. presumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 dis"
P14-2113,D10-1121,0,0.162,"ies: C ONTROVERSY, R EQUEST FOR C OM MENT (RFC), and R ESOLVED based on the tags found in discussions (see Table 1). The numbers of discussions for the three types are 42, 3484, and 105, respectively. Note that dispute tags only appear in a small number of articles and talk pages. There may exist other discussions with disputes. Dispute Subcategory Controversy Request for Comment Resolved Wikipedia Tags on Talk pages C ONTROVERSIAL, TOTALLYDISPUTED, D ISPUTED, C ALM TALK, POV RFC Any tag from above + R ESOLVED Table 1: Subcategory for disputes with corresponding tags. 2 A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3 The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. Note that each discussion in the R ESOLVED class has more than one tag. Step 3: Get Discussions without Disputes. Likewise, we collect non-dispute discussions from 4 http://en.wikipedia.org/wiki/Wikipedia: Requests_for_comment 694 Lexical"
P14-2113,J93-3003,0,0.0716698,"built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation “nsubj(wrong, you)” becomes “nsubj(SentiWordneg , you)”. 4 4.1 Online Dispute Detection Training A Sentiment Classifier Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus"
P14-2113,P11-2065,0,0.0555638,"at employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach to automatically identify online disputes on a dataset of scale. Additional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the article to flag it for attention. In this research, we are interested"
P14-2113,H05-1044,0,0.0116014,"l structure and domain knowledge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M = Mp ∪Mn , where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume µhσ,wi encodes the weight between label σ and feature w, for each feature w ∈ Mp ; then the isotonic CRF enforces σ ≤ σ 0 ⇒ µhσ,wi ≤ µhσ0 ,wi . For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn . Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Sentiment Featur"
P14-2113,W12-3710,0,0.175666,"es are indicated by ellipsis. Names of editors are in bold. The start of each set of related turns is numbered; “&gt;” is an indicator for the reply structure. presumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 nondisp"
P14-2113,prasad-etal-2008-penn,0,\N,Missing
P16-1087,D14-1080,1,0.877173,"Missing"
P16-1087,H05-1045,1,0.190361,"Missing"
P16-1087,W06-0301,0,0.0997797,"iwan]T2 an integral part of its territory awaiting reunification, by force if necessary. S2 “[Our agency]T1 ,H2 [seriously needs]O2 [equipment for detecting drugs]T2 ,” [he]H1 [said]O1 . In S1, for example, “infuriated” indicates that there is an (negative) opinion from “Beijing” regarding “the sale.”1 Traditionally, the task of extracting opinion entities and opinion relations was handled in a pipelined manner, i.e., extracting the opinion expressions first and then extracting opinion targets and opinion holders based on their syntactic and semantic associations with the opinion expressions (Kim and Hovy, 2006; Kobayashi et al., 2007). More recently, methods that jointly infer the opinion entity and relation extraction tasks (e.g., using Integer Linear Programming (ILP)) have been introduced (Choi et al., 2006; Yang and Cardie, 2013) and show that the existence of opinion relations provides clues for the identification of opinion entities and vice-versa, and thus results in better performance than a pipelined approach. However, the success of these methods depends critically on the availability of opinion lexicons, dependency parsers, named-entity taggers, etc. Introduction There has been much rese"
P16-1087,W06-1651,1,0.912936,", “infuriated” indicates that there is an (negative) opinion from “Beijing” regarding “the sale.”1 Traditionally, the task of extracting opinion entities and opinion relations was handled in a pipelined manner, i.e., extracting the opinion expressions first and then extracting opinion targets and opinion holders based on their syntactic and semantic associations with the opinion expressions (Kim and Hovy, 2006; Kobayashi et al., 2007). More recently, methods that jointly infer the opinion entity and relation extraction tasks (e.g., using Integer Linear Programming (ILP)) have been introduced (Choi et al., 2006; Yang and Cardie, 2013) and show that the existence of opinion relations provides clues for the identification of opinion entities and vice-versa, and thus results in better performance than a pipelined approach. However, the success of these methods depends critically on the availability of opinion lexicons, dependency parsers, named-entity taggers, etc. Introduction There has been much research in recent years in the area of fine-grained opinion analysis where the goal is to identify subjective expressions in text along with their associated sources and targets. More specifically, fine-grai"
P16-1087,D07-1114,0,0.0193261,"part of its territory awaiting reunification, by force if necessary. S2 “[Our agency]T1 ,H2 [seriously needs]O2 [equipment for detecting drugs]T2 ,” [he]H1 [said]O1 . In S1, for example, “infuriated” indicates that there is an (negative) opinion from “Beijing” regarding “the sale.”1 Traditionally, the task of extracting opinion entities and opinion relations was handled in a pipelined manner, i.e., extracting the opinion expressions first and then extracting opinion targets and opinion holders based on their syntactic and semantic associations with the opinion expressions (Kim and Hovy, 2006; Kobayashi et al., 2007). More recently, methods that jointly infer the opinion entity and relation extraction tasks (e.g., using Integer Linear Programming (ILP)) have been introduced (Choi et al., 2006; Yang and Cardie, 2013) and show that the existence of opinion relations provides clues for the identification of opinion entities and vice-versa, and thus results in better performance than a pipelined approach. However, the success of these methods depends critically on the availability of opinion lexicons, dependency parsers, named-entity taggers, etc. Introduction There has been much research in recent years in t"
P16-1087,J05-1003,0,0.0602187,"Missing"
P16-1087,D15-1168,0,0.146423,"iebe and Cardie, 2005)); • opinion targets, T , which are the entities or topics that the opinion is about; and 1 This paper does not attempt to determine the sentiment, i.e., the positive or negative polarity, of an opinion. 919 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 919–929, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2 Alternatively, neural network-based methods have been employed. In these approaches, the required latent features are automatically learned as dense vectors of the hidden layers. Liu et al. (2015), for example, compare several variations of recurrent neural network methods and find that long short-term memory networks (LSTMs) perform the best in identifying opinion expressions and opinion targets for the specific case of product/service reviews. Related Work LSTM-RNNs (Hochreiter and Schmidhuber, 1997) have recently been applied to many sequential modeling and prediction tasks, such as machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), speech recognition (Graves et al., 2013), NER (Hammerton, 2003). The bi-directional variant of RNNs has been found to perform better a"
P16-1087,W03-0426,0,0.256934,"Missing"
P16-1087,P13-1161,1,0.317198,"cates that there is an (negative) opinion from “Beijing” regarding “the sale.”1 Traditionally, the task of extracting opinion entities and opinion relations was handled in a pipelined manner, i.e., extracting the opinion expressions first and then extracting opinion targets and opinion holders based on their syntactic and semantic associations with the opinion expressions (Kim and Hovy, 2006; Kobayashi et al., 2007). More recently, methods that jointly infer the opinion entity and relation extraction tasks (e.g., using Integer Linear Programming (ILP)) have been introduced (Choi et al., 2006; Yang and Cardie, 2013) and show that the existence of opinion relations provides clues for the identification of opinion entities and vice-versa, and thus results in better performance than a pipelined approach. However, the success of these methods depends critically on the availability of opinion lexicons, dependency parsers, named-entity taggers, etc. Introduction There has been much research in recent years in the area of fine-grained opinion analysis where the goal is to identify subjective expressions in text along with their associated sources and targets. More specifically, fine-grained opinion analysis aim"
P16-1087,D12-1110,0,0.0467994,"een much addressed in previous work and has proven to be difficult for existing methods. 920 outputs a number between 0 and 1 where 0 implies that the information is completely lost and 1 means that the information is completely retained. does not involve identification of relations between opinion entities. Hence, standard LSTMs are applicable in this domain. None of the above neural network based models can jointly model opinion entities and opinion relations. In the relation extraction domain, several neural networks have been proposed for relation classification, such as RNN-based models (Socher et al., 2012) and LSTM-based models (Xu et al., 2015). These models depend on constituent or dependency tree structures for relation classification, and also do not model entities jointly. Recently, Miwa and Bansal (2016) proposed a model to jointly represent both entities and relations with shared parameters, but it is not a joint-inference framework. 3 et = tanh(Wc xt + Uc ht−1 + bc ) C et + ft ∗ Ct−1 Ct = it ∗ C et and previous Thus, the intermediate cell state C cell state Ct−1 are used to update the new cell state Ct . ot = σ(Wo xt + Uo ht−1 + Vo Ct + bo ) ht = ot ∗ tanh(Ct ) Next, we update the hidde"
P16-1087,H05-1044,0,0.0425608,"Missing"
P16-1087,D15-1206,0,0.0232217,"proven to be difficult for existing methods. 920 outputs a number between 0 and 1 where 0 implies that the information is completely lost and 1 means that the information is completely retained. does not involve identification of relations between opinion entities. Hence, standard LSTMs are applicable in this domain. None of the above neural network based models can jointly model opinion entities and opinion relations. In the relation extraction domain, several neural networks have been proposed for relation classification, such as RNN-based models (Socher et al., 2012) and LSTM-based models (Xu et al., 2015). These models depend on constituent or dependency tree structures for relation classification, and also do not model entities jointly. Recently, Miwa and Bansal (2016) proposed a model to jointly represent both entities and relations with shared parameters, but it is not a joint-inference framework. 3 et = tanh(Wc xt + Uc ht−1 + bc ) C et + ft ∗ Ct−1 Ct = it ∗ C et and previous Thus, the intermediate cell state C cell state Ct−1 are used to update the new cell state Ct . ot = σ(Wo xt + Uo ht−1 + Vo Ct + bo ) ht = ot ∗ tanh(Ct ) Next, we update the hidden state ht based on the output gate ot a"
P16-1087,D12-1122,1,0.642312,"Missing"
P17-1085,H05-1091,0,0.25582,"d model bodes well for relation extraction of non-adjacent entities in low-resource languages that lack good parsers. In the sections that follow, we describe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availabili"
P17-1085,P11-1056,0,0.0301762,"anded to include tokens from other related entities, such that we assign equal probability N1 to all tokens3 depending on the number N of these related tokens. The log-probability for the entity part remain the same as in our objective discussed in Section 4, however we modify the relation log-probability as below: X 0 ri,j log p(ri,j |e≤i , r&lt;i , S, θ) development and the remaining 80 documents for the test set. ACE04 has 7 relation types with an additional Discourse (D ISC) type and split O RG -A FF relation type into O RG -A FF and OTHER -A FF. We perform 5-fold cross validation similar to Chan and Roth (2011) for fair comparison with the state-of-theart. 5.2 In order to compare our system with the previous systems, we report micro F1-scores, Precision and Recall on both entities and relations similar to Li and Ji (2014) and Miwa and Bansal (2016). An entity is considered correct if we can identify its head and the entity type correctly. A relation is considered correct if we can identify the head of the argument entities and also the relation type. We also report a combined score when both argument entities and relations are correct. 0 |j:ri,j &gt;0| 0 where, ri is the true distribution over relation"
P17-1085,P16-1223,0,0.0628084,"int models have been argued to perform better than the pipeline models as knowledge of the typed relation can increase the confidence of the model on entity extraction and vice versa. Recurrent networks (RNNs) (Elman, 1990) have recently become very popular for sequence tagging tasks such as entity extraction that involves a set of contiguous tokens. However, their ability to identify relations between non-adjacent tokens in a sequence, e.g., the head nouns of two entities, is less explored. For these tasks, RNNs that make use of tree structures have been deemed more suitable. Miwa and Bansal (2016), for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components. As a result, their model depends critically on access to dependency trees, restricting it to sentencelevel extraction and to languages for which (good) dependency parsers exist. Also, their model does not jointly extract entities and relations; they first extract all entities and then perform relation classification on all pairs of entities in a sentence."
P17-1085,W03-0426,0,0.116786,"Missing"
P17-1085,K15-1027,0,0.0165609,"ctions that follow, we describe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end"
P17-1085,P16-1046,0,0.00545907,"tokenlevel weights as pointers to the input elements. 918 PHYS ORG-AFF PART-WHOLE Martin Geissler , ITV News , Safwan southern Iraq . Entity tags B PER L PER O B ORG L ORG O U GPE O U LOC O Figure 1: Gold standard annotation for an example sentence from ACE05 dataset. relation extraction. Using LSTMs, we can compute the hidden state → − ← − ht in the forward direction and ht in the backward direction for every token as below: → − → − h t = LST M (xt , h t−1 ) ← − ← − h t = LST M (xt , h t+1 ) Zhai et al. (2017), for example, have used these for neural chunking, and Nallapati et al. (2016) and Cheng and Lapata (2016), for summarization. However, to the best of our knowledge, these networks have not been used for joint extraction of entity mentions and relations. We present first such attempt to use these attention models with recurrent neural networks for joint extraction of entity mentions and relations. 3 For every token t in the subsequent layer l, we → − ← − combine the representations h l−1 and h l−1 from t t previous layer l-1 and feed it as an input. In this paper, we only use the hidden state from the last layer L for output layer and compute the top hidden layer representation as below: − (L) ← −"
P17-1085,D14-1080,1,0.909552,". In our previous work (Katiyar and Cardie, 2016), as explained earlier, we proposed a LSTM-based model for joint extraction of opinion entities and relations, but no relation types. This model cannot be directly extended to include relation types as the output space becomes sparse making it difficult for the model to learn. Related Work RNNs (Hochreiter and Schmidhuber, 1997) have been recently applied to many sequential modeling and prediction tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (NER) (Hammerton, 2003), opinion mining (Irsoy and Cardie, 2014). Variants such as adding CRF-like objective on top of LSTMs have been found to produce state-of-the-art results on several sequence prediction NLP tasks (Collobert et al., 2011; Huang et al., 2015; Katiyar and Cardie, 2016). These models assume conditional independence at the output layer whereas the model we propose in this paper does not assume any conditional indepenRecent advances in recurrent neural network has seen the application of attention on recurrent neural networks to obtain a representation weighted by the importance of tokens in the sequence model. Such models have been very fr"
P17-1085,W06-1651,1,0.703751,"Missing"
P17-1085,W10-2924,0,0.0465781,"en widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural networks with attention for extracting semantic relations bet"
P17-1085,J05-1003,0,0.0121965,"ut encoding as well as an objective to learn multiple relations in this paper. However, this presents the challenge of combining predictions from the two directions. We use heuristics in this paper to combine the predictions. We think that using probabilistic methods to combine model predictions from both directions may further improve the performance. We also plan to use Sparsemax (Martins and Astudillo, 2016) instead of Softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels. It would also be interesting to see the effect of reranking (Collins and Koo, 2005) on our joint model. We also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in Lu and Roth (2015). incorporating these co-reference information during both training and evaluation will further improve the performance of both systems. Another source of error that we found was the inability of our system to extract entities (lower recall) as in S3. Our model could not identify the FAC entity “residence”. Hence, we think an improvement on entity performance via methods like pretraining might be helpful in identifying more relations. Fo"
P17-1085,J81-4005,0,0.757491,"Missing"
P17-1085,P16-1087,1,0.642419,"an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components. As a result, their model depends critically on access to dependency trees, restricting it to sentencelevel extraction and to languages for which (good) dependency parsers exist. Also, their model does not jointly extract entities and relations; they first extract all entities and then perform relation classification on all pairs of entities in a sentence. In our previous work (Katiyar and Cardie, 2016), we address the same task in an opinion extraction context. Our LSTM-based formulation explicitly encodes distance between the head of entities into opinion relation labels. The output space of our model is quadratic in size of the entity and relation label set and we do not specifically identify the relation type. Unfortunately, adding relation type makes the output label space very sparse, making it difficult for the model to learn. In this paper, we propose a novel RNN-based model for the joint extraction of entity mentions We present a novel attention-based recurrent neural network for jo"
P17-1085,P15-1061,0,0.0349756,"Missing"
P17-1085,P14-1038,0,0.620921,"4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entitie"
P17-1085,D15-1102,0,0.0194566,"s. We use heuristics in this paper to combine the predictions. We think that using probabilistic methods to combine model predictions from both directions may further improve the performance. We also plan to use Sparsemax (Martins and Astudillo, 2016) instead of Softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels. It would also be interesting to see the effect of reranking (Collins and Koo, 2005) on our joint model. We also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in Lu and Roth (2015). incorporating these co-reference information during both training and evaluation will further improve the performance of both systems. Another source of error that we found was the inability of our system to extract entities (lower recall) as in S3. Our model could not identify the FAC entity “residence”. Hence, we think an improvement on entity performance via methods like pretraining might be helpful in identifying more relations. For distance less than 7, we find that our model has better recall but lower precision, as expected. 8 Conclusion In this paper, we propose a novel attention-bas"
P17-1085,D12-1110,0,0.0296635,"ce languages that lack good parsers. In the sections that follow, we describe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc."
P17-1085,D15-1166,0,0.00921321,"Missing"
P17-1085,P16-1105,0,0.146446,"asaki, 2014). Joint models have been argued to perform better than the pipeline models as knowledge of the typed relation can increase the confidence of the model on entity extraction and vice versa. Recurrent networks (RNNs) (Elman, 1990) have recently become very popular for sequence tagging tasks such as entity extraction that involves a set of contiguous tokens. However, their ability to identify relations between non-adjacent tokens in a sequence, e.g., the head nouns of two entities, is less explored. For these tasks, RNNs that make use of tree structures have been deemed more suitable. Miwa and Bansal (2016), for example, propose an RNN comprised of a sequencebased long short term memory (LSTM) for entity identification and a separate tree-based dependency LSTM layer for relation classification using shared parameters between the two components. As a result, their model depends critically on access to dependency trees, restricting it to sentencelevel extraction and to languages for which (good) dependency parsers exist. Also, their model does not jointly extract entities and relations; they first extract all entities and then perform relation classification on all pairs of entities in a sentence."
P17-1085,D15-1062,0,0.0109923,"escribe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM base"
P17-1085,D14-1200,0,0.385982,"nts on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path"
P17-1085,D15-1206,0,0.0114958,"escribe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); results (Section 6); error analysis (Section 7) and conclusion (Section 8). 2 Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM base"
P17-1085,C10-2160,0,0.206191,"arguments of the relations are known in advance. There have been several models proposed including featurebased models (Bunescu and Mooney, 2005; Zelenko et al., 2003) and neural network based models (Socher et al., 2012; dos Santos et al., 2015; Hashimoto et al., 2015; Xu et al., 2015a,b). For joint-extraction of entities and relations, feature-based structured prediction models (Li and Ji, 2014; Miwa and Sasaki, 2014), joint inference integer linear programming models(Yih and Roth, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010) and probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013) have been proposed. In contrast, we propose a neural network model which does not depend on the availability of any features such as part of speech (POS) tags, dependency trees, etc. Recently, Miwa and Bansal (2016) proposed an end-to-end LSTM based sequence and treestructured model. They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural networks with attention for extracting semantic relations between entity mentions without using any dependency par"
P17-1085,P05-1053,0,0.343028,"es and their relations from text belongs to a very well-studied family of structured prediction tasks in NLP. There are several NLP tasks such as fine-grained opinion mining (Choi et al., 2006), semantic role labeling (Gildea and Jurafsky, 2002), etc., which have a similar structure; thus making it an important and a challenging task. Several methods have been proposed for entity mention and relation extraction at the sentencelevel. These can be broadly categorized into – 1) pipeline models that treat the identification of entity mentions (Nadeau and Sekine, 2007) and relation classification (Zhou et al., 2005) as two separate tasks; and 2) joint models, also the more 917 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 917–928 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1085 dence at the output layer, allowing it to model an arbitrary distribution over output sequences. and relations. Unlike other models, our model does not depend on any dependency tree information. Our RNN-based model is a multi-layer bidirectional LSTM over a sequence. We encode the output sequence from"
P17-1085,J02-3001,0,\N,Missing
P17-1091,S14-2082,0,0.0618804,"programming postprocessing. For insight into the strengths and weaknesses of the proposed models, as well as into the differences between SVM and RNN parameterizations, we perform an error analysis in Section 5.1. To support argument mining research, we also release our Python implementation, Marseille.3 izing adjacent links with higher-order structures (e.g., c → b → a) and enforcing arbitrary constraints on the link structure, not limited to trees. Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by Martins et al. (2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks. Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline. Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation datasets. Furthermore, our model"
P17-1091,P13-2109,0,0.0361475,"is based on integer linear programming postprocessing. For insight into the strengths and weaknesses of the proposed models, as well as into the differences between SVM and RNN parameterizations, we perform an error analysis in Section 5.1. To support argument mining research, we also release our Python implementation, Marseille.3 izing adjacent links with higher-order structures (e.g., c → b → a) and enforcing arbitrary constraints on the link structure, not limited to trees. Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by Martins et al. (2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks. Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline. Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation d"
P17-1091,H05-1066,0,0.0227802,"Missing"
P17-1091,P82-1020,0,0.820561,"Missing"
P17-1091,P16-1087,1,0.831722,"On UKP, for link prediction, the linear baseline can reach good performance when using inference, similar to the approach of Stab and Gurevych (2016), but the improvement in proposition prediction leads to higher overall F1 for the structured models. Meanwhile, on the more difficult CDCP setting, performing inference on the baseline output is not competitive. While feature engineering still outperforms our RNN model, we find that RNNs shine on proposition classification, especially on UKP, and that structured training can make them competitive, reducing their observed lag on link prediction (Katiyar and Cardie, 2016), possibly through mitigating class imbalance. Hyperparameters. We perform grid search using k-fold document-level cross-validation, tuning the dropout probability in the dense MLP layers over {0.05, 0.1, 0.15, 0.2, 0.25} and the optimal number of passes over the training data over {10, 25, 50, 75, 100}. We use 2 layers for the LSTM and the proposition classifier, 128 hidden units in all layers, and a multilinear decomposition with rank r = 16, after preliminary CV runs. 4.5 Baseline models We compare our proposed models to equivalent independent unary classifiers. The unary-only version of a"
P17-1091,Q16-1023,0,0.434943,"nd Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees. This makes the problem computationally easier by enabling the use of maximum spanning tree–style parsing ap1 We describe proposition types (FACT, etc.) in Section 3. 985 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 985–995 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1091 (Tsochantaridis et al., 2005), and recurrent neural networks with structured loss, extending (Kiperwasser and Goldberg, 2016). Interestingly, RNNs perform poorly when trained with classification losses, but become competitive with the featureengineered structured SVMs when trained within our proposed structured learning model. We evaluate our approach on two argument mining datasets. Firstly, on our new Cornell eRulemaking Corpus – CDCP,2 consisting of argument annotations on comments from an eRulemaking discussion forum, where links don’t always form trees (Figure 1 shows an abridged example comment, and Section 3 describes the dataset in more detail). Secondly, on the UKP argumentative essays v2 (henceforth UKP),"
P17-1091,W15-0506,1,0.903437,"ne the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation datasets. Furthermore, our model can enforce the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by Stab and Gurevych (2016), thanks to the AD3 inference algorithm (Martins et al., 2015). Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step. Meanwhile, a new direction in argument mining explores pointer networks (Potash et al., 2016); a promising method, currently lacking support for tree structures and domain-specific constraints. 2 Related work Our factor graph formulation draws from ideas previously used independently in parsing and argument mining. In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by McDonald et al. (2005) and applied to argument mining with mixed results by Peldszus and Stede (2015). As they"
P17-1091,D15-1110,0,0.0650036,"pplications in policy making, summarization, and education, among others. The argument mining task includes the tightly-knit subproblems of classifying propositions into elementary unit types and detecting argumentative relations between the elementary units. The desired output is a document argumentation graph structure, such as the one in Figure 1, where propositions are denoted by letter subscripts, and the associated argumentation graph shows their types and support relations between them. Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees. This makes the problem computationally easier by enabling the use of maximum spanning tree–style parsing ap1 We describe proposition types (FACT, etc.) in Section 3. 985 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 985–995 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1091 (Tsochantaridis et al., 2005), and recurrent neural networks with structured loss, extending (Kiperwasser an"
P17-1091,D14-1162,0,0.095816,"a → b =⇒ a  b CLAIM , MAJOR CLAIM , PREMISE within each paragraph grandparents, co-parents directed forest: • T REE FACTOR over each paragraph • zero-potential “root” links a → ∗ link source must be premise: a → b =⇒ a = PREMISE Table 1: Instantiation of model design choices for each dataset. 4.4 Argument structure RNN lexical (unigrams and dependency tuples), structural (token statistics and proposition location), indicators (from hand-crafted lexicons), contextual, syntactic (subclauses, depth, tense, modal, and POS), probability, discourse (Lin et al., 2014), and average GloVe embeddings (Pennington et al., 2014). Link features are lexical (unigrams), syntactic (POS and productions), structural (token statistics, proposition statistics and location features), hand-crafted indicators, discourse triples, PMI, and shared noun counts. Our proposed higher-order factors for grandparent, co-parent, and sibling structures require features extracted from a proposition triplet a, b, c. In dependency and semantic parsing, higher-order factors capture relationships between words, so sparse indicator features can be efficiently used. In our case, since propositions consist of many words, BOW features may be too no"
P17-1123,P16-1223,0,0.0336036,"Missing"
P17-1123,D14-1179,0,0.0157924,"Missing"
P17-1123,N16-1012,0,0.0176435,"Missing"
P17-1123,W14-3348,0,0.00577951,", ∗∗ (p < 0.001)). 5.4 Sentence 2: free oxygen first appeared in significant quantities during the paleoproterozoic eon -lrb- between 3.0 and 2.3 billion years ago -rrb- . Human: during which eon did free oxygen begin appearing in quantity ? H&S: what first appeared in significant quantities during the paleoproterozoic eon ? Ours: how long ago did the paleoproterozoic exhibit ? Automatic Evaluation We use the evaluation package released by Chen et al. (2015), which was originally used to score image captions. The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts. BLEU measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. BLEU-n is BLEU score that uses up to n-grams for counting co-occurrences. METEOR is a recall-oriented metric, which calculates the similarity between generations and references by considering synonyms, stemming and paraphrases. ROUGE is commonly employed to evaluate n-grams recall of the summaries with goldstandard sentences as references. ROUGEL (measured based on longest common subsequence) results are reported. 5.5 Sentence 3: inf"
P17-1123,P13-2121,0,0.00839682,"Missing"
P17-1123,N10-1086,0,0.845345,"uce oxygen from water. Questions: – What life process produces oxygen in the presence of light? photosynthesis – Photosynthesis uses which energy to form oxygen from water? sunlight – From what does photosynthesis get oxygen? water Figure 1: Sample sentence from the second paragraph of the article Oxygen, along with the natural questions and their answers. Introduction Question generation (QG) aims to create natural questions from a given a sentence or paragraph. One key application of question generation is in the area of education — to generate questions for reading comprehension materials (Heilman and Smith, 2010). Figure 1, for example, shows three manually generated questions that test a user’s understanding of the associated text passage. Question generation systems can also be deployed as chatbot components (e.g., asking questions to start a conversation or to request feedback (Mostafazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971). In addition to the above applications, question generation systems can aid in the development of annotated data sets for natural language processing (NLP) research in reading comprehe"
P17-1123,P16-1195,0,0.0140801,"Missing"
P17-1123,P17-4012,0,0.0171241,"Missing"
P17-1123,P07-2045,0,0.0122736,"ism (Luong et al., 2015a). We investigate several variations of this model, including one that takes into account paragraph- rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings. In evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al., 2007), and the overgenerate-and-rank approach of Heilman and Smith (2010). Human evaluations also rated our generated questions as more grammatical, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-theart Heilman and Smith (2010) system. In the sections below we discuss related work (Section 2), specify the task definition (Section 3) and describe our neural sequence learning based models (Section 4). We explain the experimental setup in Section 5. Lastly, we present the evaluation results as well as a deta"
P17-1123,P15-1086,0,0.343339,"utomatic means. Question Generation has attracted the attention of the natural language generation (NLG) community in recent years, since the work of Rus et al. (2010). Most work tackles the task with a rule-based approach. Generally, they first transform the input sentence into its syntactic representation, which 1343 1 https://stanford-qa.com they then use to generate an interrogative sentence. A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014). Labutov et al. (2015) use crowdsourcing to collect a set of templates and then rank the relevant templates for the text of another domain. Generally, the rule-based approaches make use of the syntactic roles of words, but not their semantic roles. Heilman and Smith (2010) introduce an overgenerate-and-rank approach: their system first overgenerates questions and then ranks them. Although they incorporate learning to rank, their system’s performance still depends critically on the manually constructed generating rules. Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connect"
P17-1123,W04-1013,0,0.0178882,"oxygen first appeared in significant quantities during the paleoproterozoic eon -lrb- between 3.0 and 2.3 billion years ago -rrb- . Human: during which eon did free oxygen begin appearing in quantity ? H&S: what first appeared in significant quantities during the paleoproterozoic eon ? Ours: how long ago did the paleoproterozoic exhibit ? Automatic Evaluation We use the evaluation package released by Chen et al. (2015), which was originally used to score image captions. The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts. BLEU measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. BLEU-n is BLEU score that uses up to n-grams for counting co-occurrences. METEOR is a recall-oriented metric, which calculates the similarity between generations and references by considering synonyms, stemming and paraphrases. ROUGE is commonly employed to evaluate n-grams recall of the summaries with goldstandard sentences as references. ROUGEL (measured based on longest common subsequence) results are reported. 5.5 Sentence 3: inflammation is one of the"
P17-1123,W13-2114,0,0.347228,"for reading comprehension materials, albeit via automatic means. Question Generation has attracted the attention of the natural language generation (NLG) community in recent years, since the work of Rus et al. (2010). Most work tackles the task with a rule-based approach. Generally, they first transform the input sentence into its syntactic representation, which 1343 1 https://stanford-qa.com they then use to generate an interrogative sentence. A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014). Labutov et al. (2015) use crowdsourcing to collect a set of templates and then rank the relevant templates for the text of another domain. Generally, the rule-based approaches make use of the syntactic roles of words, but not their semantic roles. Heilman and Smith (2010) introduce an overgenerate-and-rank approach: their system first overgenerates questions and then ranks them. Although they incorporate learning to rank, their system’s performance still depends critically on the manually constructed generating rules. Mostafazadeh et al. (2016) introduce visual que"
P17-1123,D15-1166,0,0.147766,"we propose here to frame the task of question generation as a sequence-to-sequence learning problem that directly maps a sentence from a text passage to a question. Importantly, our approach is fully data-driven in that it requires no manually generated rules. More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al., 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al., 2015a). We investigate several variations of this model, including one that takes into account paragraph- rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings. In evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al., 2007), an"
P17-1123,P15-1002,0,0.440688,"we propose here to frame the task of question generation as a sequence-to-sequence learning problem that directly maps a sentence from a text passage to a question. Importantly, our approach is fully data-driven in that it requires no manually generated rules. More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al., 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al., 2015a). We investigate several variations of this model, including one that takes into account paragraph- rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings. In evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significantly outperforms a collection of strong baselines, including an information retrieval-based system (Robertson and Walker, 1994), a statistical machine translation approach (Koehn et al., 2007), an"
P17-1123,P14-5010,0,0.00193117,"Missing"
P17-1123,P14-2053,0,0.0816604,"ion materials, albeit via automatic means. Question Generation has attracted the attention of the natural language generation (NLG) community in recent years, since the work of Rus et al. (2010). Most work tackles the task with a rule-based approach. Generally, they first transform the input sentence into its syntactic representation, which 1343 1 https://stanford-qa.com they then use to generate an interrogative sentence. A lot of research has focused on first manually constructing question templates, and then applying them to generate questions (Mostow and Chen, 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014). Labutov et al. (2015) use crowdsourcing to collect a set of templates and then rank the relevant templates for the text of another domain. Generally, the rule-based approaches make use of the syntactic roles of words, but not their semantic roles. Heilman and Smith (2010) introduce an overgenerate-and-rank approach: their system first overgenerates questions and then ranks them. Although they incorporate learning to rank, their system’s performance still depends critically on the manually constructed generating rules. Mostafazadeh et al. (2016) introduce visual question generation task, to e"
P17-1123,W03-0203,0,0.211112,"fazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971). In addition to the above applications, question generation systems can aid in the development of annotated data sets for natural language processing (NLP) research in reading comprehension and question answering. Indeed the creation of such datasets, e.g., SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas. For the most part, question generation has been tackled in the past via rule-based approaches (e.g., Mitkov and Ha (2003); Rus et al. (2010). The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge. To improve over a purely rule-based system, Heilman and Smith (2010) introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them using a supervised learning-based ranker. Although the ranking algorithm helps to produce more ac1342 Proceedings of the 55th Annual Meeting of the Association for Co"
P17-1123,P16-1170,0,0.573571,"Oxygen, along with the natural questions and their answers. Introduction Question generation (QG) aims to create natural questions from a given a sentence or paragraph. One key application of question generation is in the area of education — to generate questions for reading comprehension materials (Heilman and Smith, 2010). Figure 1, for example, shows three manually generated questions that test a user’s understanding of the associated text passage. Question generation systems can also be deployed as chatbot components (e.g., asking questions to start a conversation or to request feedback (Mostafazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971). In addition to the above applications, question generation systems can aid in the development of annotated data sets for natural language processing (NLP) research in reading comprehension and question answering. Indeed the creation of such datasets, e.g., SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas. For the most part, question generation has been tackled in the past via rule-based approaches (e.g., Mitkov and Ha (2003);"
P17-1123,P02-1040,0,0.124075,"is indicated with ∗ (p < 0.005), ∗∗ (p < 0.001)). 5.4 Sentence 2: free oxygen first appeared in significant quantities during the paleoproterozoic eon -lrb- between 3.0 and 2.3 billion years ago -rrb- . Human: during which eon did free oxygen begin appearing in quantity ? H&S: what first appeared in significant quantities during the paleoproterozoic eon ? Ours: how long ago did the paleoproterozoic exhibit ? Automatic Evaluation We use the evaluation package released by Chen et al. (2015), which was originally used to score image captions. The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGEL (Lin, 2004) evaluation scripts. BLEU measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. BLEU-n is BLEU score that uses up to n-grams for counting co-occurrences. METEOR is a recall-oriented metric, which calculates the similarity between generations and references by considering synonyms, stemming and paraphrases. ROUGE is commonly employed to evaluate n-grams recall of the summaries with goldstandard sentences as references. ROUGEL (measured based on longest common subsequence) resul"
P17-1123,D14-1162,0,0.118723,"multiple corresponding questions, and, on average, there are 1.4 questions for each sentence. 5.2 Implementation Details We implement our models 2 in Torch7 3 on top of the newly released OpenNMT system (Klein et al., 2017). For the source side vocabulary V, we only keep the 45k most frequent tokens (including <SOS>, <EOS> and placeholders). For the target side vocabulary U, similarly, we keep the 28k most frequent tokens. All other tokens outside the vocabulary list are replaced by the UNK symbol. We choose word embedding of 300 dimensions and use the glove.840B.300d pre-trained embeddings (Pennington et al., 2014) for initialization. We fix the word representations during training. We set the LSTM hidden unit size to 600 and set the number of layers of LSTMs to 2 in both the encoder and the decoder. Optimization is performed using stochastic gradient descent (SGD), with an initial learning rate of 1.0. We start halving the learning rate at epoch 8. The mini-batch size for the update is set at 64. Dropout with probability 2 The code is available at https://github.com/ xinyadu/nqg. 3 http://torch.ch/ 1346 Model BLEU 1 BLEU 2 BLEU 3 BLEU 4 METEOR ROUGEL IRBM25 IREdit Distance MOSES+ DirectIn H&S Vanilla s"
P17-1123,D16-1264,0,0.602408,"r’s understanding of the associated text passage. Question generation systems can also be deployed as chatbot components (e.g., asking questions to start a conversation or to request feedback (Mostafazadeh et al., 2016)) or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971). In addition to the above applications, question generation systems can aid in the development of annotated data sets for natural language processing (NLP) research in reading comprehension and question answering. Indeed the creation of such datasets, e.g., SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas. For the most part, question generation has been tackled in the past via rule-based approaches (e.g., Mitkov and Ha (2003); Rus et al. (2010). The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge. To improve over a purely rule-based system, Heilman and Smith (2010) introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based"
P17-1123,D13-1020,0,0.00968185,"Missing"
P17-1123,P16-1056,0,0.167278,"et of templates and then rank the relevant templates for the text of another domain. Generally, the rule-based approaches make use of the syntactic roles of words, but not their semantic roles. Heilman and Smith (2010) introduce an overgenerate-and-rank approach: their system first overgenerates questions and then ranks them. Although they incorporate learning to rank, their system’s performance still depends critically on the manually constructed generating rules. Mostafazadeh et al. (2016) introduce visual question generation task, to explore the deep connection between language and vision. Serban et al. (2016) propose generating simple factoid questions from logic triple (subject, relation, object). Their task tackles mapping from structured representation to natural language text, and their generated questions are consistent in terms of format and diverge much less than ours. To our knowledge, none of the previous works has framed QG for reading comprehension in an end-to-end fashion, and nor have them used deep sequence-to-sequence learning approach to generate questions. 3 Task Definition In this section, we define the question generation task. Given an input sentence x, our goal is to generate"
P17-1123,W10-4234,0,0.422586,"or, arguably, as a clinical tool for evaluating or improving mental health (Weizenbaum, 1966; Colby et al., 1971). In addition to the above applications, question generation systems can aid in the development of annotated data sets for natural language processing (NLP) research in reading comprehension and question answering. Indeed the creation of such datasets, e.g., SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016), has spurred research in these areas. For the most part, question generation has been tackled in the past via rule-based approaches (e.g., Mitkov and Ha (2003); Rus et al. (2010). The success of these approaches hinges critically on the existence of well-designed rules for declarative-to-interrogative sentence transformation, typically based on deep linguistic knowledge. To improve over a purely rule-based system, Heilman and Smith (2010) introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them using a supervised learning-based ranker. Although the ranking algorithm helps to produce more ac1342 Proceedings of the 55th Annual Meeting of the Association for Computational Linguis"
P17-1123,D15-1044,0,0.343745,"would seem to require an abstractive approach that can produce fluent phrasings that do not exactly match the text from which they were drawn. As a result, and in contrast to all previous work, we propose here to frame the task of question generation as a sequence-to-sequence learning problem that directly maps a sentence from a text passage to a question. Importantly, our approach is fully data-driven in that it requires no manually generated rules. More specifically, inspired by the recent success in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (Rush et al., 2015; Iyer et al., 2016), and image caption generation (Xu et al., 2015), we tackle question generation using a conditional neural language model with a global attention mechanism (Luong et al., 2015a). We investigate several variations of this model, including one that takes into account paragraph- rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings. In evaluations on the SQuAD dataset (Rajpurkar et al., 2016) using three automatic evaluation metrics, we find that our system significan"
P18-1177,D13-1160,0,0.0529619,"lection subtask. In comparison to the related methods from above that generate questions from raw text, our method is different in its ability to take into account contextual information beyond the sentencelevel by introducing coreference knowledge. 1908 2.2 sentence S that contains candidate ansi such that: Question Answering Datasets and Creation Recently there has been an increasing interest in question answering with the creation of many datasets. Most are built using crowdsourcing; they are generally comprised of fewer than 100,000 QA pairs and are time-consuming to create. WebQuestions (Berant et al., 2013), for example, contains 5,810 questions crawled via the Google Suggest API and is designed for knowledge base QA with answers restricted to Freebase entities. To tackle the size issues associated with WebQuestions, Bordes et al. (2015) introduce SimpleQuestions, a dataset of 108,442 questions authored by English speakers. SQuAD (Rajpurkar et al., 2016) is a dataset for machine comprehension; it is created by showing a Wikipedia paragraph to human annotators and asking them to write questions based on the paragraph. TriviaQA (Joshi et al., 2017) includes 95k question-answer authored by trivia e"
P18-1177,P16-1223,0,0.0763997,"Missing"
P18-1177,P17-1171,0,0.177275,"ith over 100k questions posed by crowdworkers on a set of Wikipedia articles. The answer to each question is a segment of text from the corresponding Wiki passage. The crowdworkers were users of Amazon’s Mechanical Turk located in the US or Canada. To obtain high-quality articles, the authors sampled 500 articles from the top 10,000 articles obtained by Nayuki’s Wikipedia’s internal PageRanks. The question-answer pairs were generated by annotators from a paragraph; and although the dataset is typically used to evaluate reading comprehension, it has also been used in an open domain QA setting (Chen et al., 2017; Wang et al., 2018). For training/testing answer extraction systems, we pair each paragraph in the dataset with the gold answer spans that it contains. For the question generation system, we pair each sentence that contains an answer span with the corresponding gold question as in Du et al. (2017). To quantify the effect of using predicted (rather than gold standard) answer spans on question generation (e.g., predicted answer span boundaries can be inaccurate), we also train the models on an augmented “Training set w/ noisy examples” For question generation evaluation, we use BLEU (Papineni e"
P18-1177,P16-1061,0,0.0196187,"ibed below. It then encodes the textual input using an LSTM unit (Hochreiter and Schmidhuber, 1997). Finally, an attention-copy equipped decoder is used to decode the question. More specifically, given the input sentence S (containing an answer span) and the preceding context C, we first run a coreference resolution system to get the coref-clusters for S and C and use them to create a coreference transformed input sentence: for each pronoun, we append its most representative non-pronominal coreferent mention. Specifically, we apply the simple feedforward network based mention-ranking model of Clark and Manning (2016) to the concatenation of C and S to get the coref-clusters for all entities in C and S. The C&M model produces a score/representation s for each mention pair (m1 , m2 ), 1909 s(m1 , m2 ) = Wm hm (m1 , m2 ) + bm (2) encoder … ... Context Vector Attention Decoder LSTMs Encoder ℎ/ ℎ0 ℎ1 ℎ2 ℎ3 ℎ4 ℎ5 … What team did the Panthers defeat … ? word ? Natural Question answer feature ? , refined coref. position feature ?? ?&quot; coref. position feature ?? MLP ?????&quot; ?&quot; They mention-pair score the Panthers defeated the Arizona Cardinals … coreference transformed sentence S’ coref. gate vector Figure 2: The ga"
P18-1177,W14-3348,0,0.0147983,"training/testing answer extraction systems, we pair each paragraph in the dataset with the gold answer spans that it contains. For the question generation system, we pair each sentence that contains an answer span with the corresponding gold question as in Du et al. (2017). To quantify the effect of using predicted (rather than gold standard) answer spans on question generation (e.g., predicted answer span boundaries can be inaccurate), we also train the models on an augmented “Training set w/ noisy examples” For question generation evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).1 BLEU measures average n-gram precision vs. a set of reference questions and penalizes for overly short sentences. METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases. For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers. Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match. Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partia"
P18-1177,D17-1219,1,0.850825,"esentations (Yao et al., 2012; Olney et al., 2012). With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et al. (2017) generated questions from a knowledge graph; Du et al. (2017) studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentencevs. paragraph-level information. Du and Cardie (2017) proposed a hierarchical neural sentencelevel sequence tagging model for identifying question-worthy sentences in a text passage. Finally, Duan et al. (2017) investigated how to use question generation to help improve question answering systems on the sentence selection subtask. In comparison to the related methods from above that generate questions from raw text, our method is different in its ability to take into account contextual information beyond the sentencelevel by introducing coreference knowledge. 1908 2.2 sentence S that contains candidate ansi such that: Question Answering Datasets"
P18-1177,P17-1123,1,0.89477,"stem generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012). With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et al. (2017) generated questions from a knowledge graph; Du et al. (2017) studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentencevs. paragraph-level information. Du and Cardie (2017) proposed a hierarchical neural sentencelevel sequence tagging model for identifying question-worthy sentences in a text passage. Finally, Duan et al. (2017) investigated how to use question generation to help improve question answering systems on the sentence selection subtask. In comparison to the related methods from above that generate questions from raw text, our method is different in i"
P18-1177,D17-1090,0,0.189177,"on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et al. (2017) generated questions from a knowledge graph; Du et al. (2017) studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentencevs. paragraph-level information. Du and Cardie (2017) proposed a hierarchical neural sentencelevel sequence tagging model for identifying question-worthy sentences in a text passage. Finally, Duan et al. (2017) investigated how to use question generation to help improve question answering systems on the sentence selection subtask. In comparison to the related methods from above that generate questions from raw text, our method is different in its ability to take into account contextual information beyond the sentencelevel by introducing coreference knowledge. 1908 2.2 sentence S that contains candidate ansi such that: Question Answering Datasets and Creation Recently there has been an increasing interest in question answering with the creation of many datasets. Most are built using crowdsourcing; th"
P18-1177,N10-1086,0,0.503859,"omprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6. The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA. 2 2.1 Related Work Question Generation Since the work by Rus et al. (2010), question generation (QG) has attracted interest from both the NLP and NLG communities. Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the application of a sequence of well-designed general rules or templates (Mitkov and Ha, 2003; Labutov et al., 2015). Heilman and Smith (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012). With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et"
P18-1177,D14-1080,1,0.807297,"penalizes for overly short sentences. METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases. For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers. Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match. Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partial credit proportional to the amount of overlap (Johansson and Moschitti, 2010; Irsoy and Cardie, 2014). 5.3 Baselines and Ablation Tests For question generation, we compare to the stateof-the-art baselines and conduct ablation tests as follows: Du et al. (2017)’s model is an attention-based RNN sequence-to-sequence neural network (without using the answer location information feature). Seq2seq + copyw/ answer is the attention-based sequence-to-sequence model augmented with a copy mechanism, with answer features concatenated with the word embeddings during encoding. Seq2seq + copyw/ full context + answer is the same model as the previous one, but we allow access to the full context (i.e., all t"
P18-1177,W10-2910,0,0.0171648,"set of reference questions and penalizes for overly short sentences. METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases. For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers. Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match. Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partial credit proportional to the amount of overlap (Johansson and Moschitti, 2010; Irsoy and Cardie, 2014). 5.3 Baselines and Ablation Tests For question generation, we compare to the stateof-the-art baselines and conduct ablation tests as follows: Du et al. (2017)’s model is an attention-based RNN sequence-to-sequence neural network (without using the answer location information feature). Seq2seq + copyw/ answer is the attention-based sequence-to-sequence model augmented with a copy mechanism, with answer features concatenated with the word embeddings during encoding. Seq2seq + copyw/ full context + answer is the same model as the previous one, but we allow access to the"
P18-1177,P17-1147,0,0.126495,"Questions: – What was Tesla’s reputation in popular culture? mad scientist – How did Tesla finance his work? patents – Where did Tesla live for much of his life? New York hotels Figure 1: Example input from the fourth paragraph of a Wikipedia article on Nikola Tesla, along with the natural questions and their answers from the SQuAD (Rajpurkar et al., 2016) dataset. We show in italics the set of mentions that refer to Nikola Tesla — Tesla, him, his, he, etc. Introduction Recently, there has been a resurgence of work in NLP on reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) with the goal of developing systems that can answer questions about the content of a given passage or document. Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field. And there have been a number of efforts in this direction. Miller et al. (2016), for example, develop a dataset for open-domain question answering; Rajpurkar et al. (2016) and Joshi et al. (2017) do so for reading comprehension (RC); and Hill et al. (2015) and Hermann et al. (2015), for the related task of answering cloze questions (Wino"
P18-1177,P15-1086,0,0.0817431,"ion answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6. The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA. 2 2.1 Related Work Question Generation Since the work by Rus et al. (2010), question generation (QG) has attracted interest from both the NLP and NLG communities. Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the application of a sequence of well-designed general rules or templates (Mitkov and Ha, 2003; Labutov et al., 2015). Heilman and Smith (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012). With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowled"
P18-1177,N16-1030,0,0.0226223,"function, and Wd , We , Wf are weight matrices. 4.2 Answer Span Identification We frame the problem of identifying candidate answer spans from a paragraph as a sequence labeling task and base our model on the BiLSTM-CRF approach for named entity recognition (Huang et al., 2015). Given a paragraph of n tokens, instead of directly feeding the sequence of word vectors x = (x1 , ..., xn ) to the LSTM units, we 0 first construct the feature-rich embedding x for each token, which is the concatenation of the word embedding, an NER feature embedding, and a character-level representation of the word (Lample et al., 2016). We use the concatenated vector 0 as the “final” embedding x for the token, 0 (7) where Wc is a weight matrix and attention distribution αt is a probability distribution over the source sentence words. With αt , we can obtain the context vector h∗t , h∗t = Then, using the context vector h∗t and hidden state st , the probability distribution over the target (question) side vocabulary is calculated as, xi = concat(xi , CharRepi , NERi ) (13) where CharRepi is the concatenation of the last hidden states of a character-based biLSTM. The intuition behind the use of NER features is that SQuAD answe"
P18-1177,D16-1147,0,0.0193234,"urkar et al., 2016) dataset. We show in italics the set of mentions that refer to Nikola Tesla — Tesla, him, his, he, etc. Introduction Recently, there has been a resurgence of work in NLP on reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) with the goal of developing systems that can answer questions about the content of a given passage or document. Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field. And there have been a number of efforts in this direction. Miller et al. (2016), for example, develop a dataset for open-domain question answering; Rajpurkar et al. (2016) and Joshi et al. (2017) do so for reading comprehension (RC); and Hill et al. (2015) and Hermann et al. (2015), for the related task of answering cloze questions (Winograd, 1972; Levesque et al., 2011). To create these datasets, either crowdsourcing or (semi-)synthetic approaches are used. The (semi-)synthetic datasets (e.g., Hermann et al. (2015)) are large in size and cheap to obtain; however, they do not share the same characteristics as explicit QA/RC questions (Rajpurkar et al., 2016). In comparis"
P18-1177,W03-0203,0,0.115206,"es, we obtain a question answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6. The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA. 2 2.1 Related Work Question Generation Since the work by Rus et al. (2010), question generation (QG) has attracted interest from both the NLP and NLG communities. Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the application of a sequence of well-designed general rules or templates (Mitkov and Ha, 2003; Labutov et al., 2015). Heilman and Smith (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012). With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generat"
P18-1177,P02-1040,0,0.101061,"al., 2017; Wang et al., 2018). For training/testing answer extraction systems, we pair each paragraph in the dataset with the gold answer spans that it contains. For the question generation system, we pair each sentence that contains an answer span with the corresponding gold question as in Du et al. (2017). To quantify the effect of using predicted (rather than gold standard) answer spans on question generation (e.g., predicted answer span boundaries can be inaccurate), we also train the models on an augmented “Training set w/ noisy examples” For question generation evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).1 BLEU measures average n-gram precision vs. a set of reference questions and penalizes for overly short sentences. METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases. For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers. Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match. Binary Overlap counts every predicted answer that overlaps with a gold answer span as correc"
P18-1177,E17-1036,0,0.0326919,"th (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012). With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation. Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et al. (2017) generated questions from a knowledge graph; Du et al. (2017) studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentencevs. paragraph-level information. Du and Cardie (2017) proposed a hierarchical neural sentencelevel sequence tagging model for identifying question-worthy sentences in a text passage. Finally, Duan et al. (2017) investigated how to use question generation to help improve question answering systems on the sentence selection subtask. In comparison to the related methods from above that g"
P18-1177,W10-4234,0,0.0890606,"d a model that encodes all preceding context and the input sentence itself. When evaluated on only the portion of SQuAD that requires coreference resolution, the gap between our system and the baseline systems is even larger. By applying our approach to the 10,000 topranking Wikipedia articles, we obtain a question answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6. The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA. 2 2.1 Related Work Question Generation Since the work by Rus et al. (2010), question generation (QG) has attracted interest from both the NLP and NLG communities. Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the application of a sequence of well-designed general rules or templates (Mitkov and Ha, 2003; Labutov et al., 2015). Heilman and Smith (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates. Apart from generating questions from raw text, there has also been research on question generation from symbolic repre"
P18-1177,P16-1056,0,0.404751,"ame characteristics as explicit QA/RC questions (Rajpurkar et al., 2016). In comparison, high-quality crowdsourced datasets are much smaller in size, and the annotation process is quite expensive because the labeled examples require expertise and careful design (Chen et al., 2016). 1907 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1907–1917 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Thus, there is a need for methods that can automatically generate high-quality question-answer pairs. Serban et al. (2016) propose the use of recurrent neural networks to generate QA pairs from structured knowledge resources such as Freebase. Their work relies on the existence of automatically acquired KBs, which are known to have errors and suffer from incompleteness. They are also nontrivial to obtain. In addition, the questions in the resulting dataset are limited to queries regarding a single fact (i.e., tuple) in the KB. Motivated by the need for large scale QA pairs and the limitations of recent work, we investigate methods that can automatically “harvest” (generate) question-answer pairs from raw text/unst"
P18-1177,H89-1033,0,0.628033,"017) with the goal of developing systems that can answer questions about the content of a given passage or document. Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field. And there have been a number of efforts in this direction. Miller et al. (2016), for example, develop a dataset for open-domain question answering; Rajpurkar et al. (2016) and Joshi et al. (2017) do so for reading comprehension (RC); and Hill et al. (2015) and Hermann et al. (2015), for the related task of answering cloze questions (Winograd, 1972; Levesque et al., 2011). To create these datasets, either crowdsourcing or (semi-)synthetic approaches are used. The (semi-)synthetic datasets (e.g., Hermann et al. (2015)) are large in size and cheap to obtain; however, they do not share the same characteristics as explicit QA/RC questions (Rajpurkar et al., 2016). In comparison, high-quality crowdsourced datasets are much smaller in size, and the annotation process is quite expensive because the labeled examples require expertise and careful design (Chen et al., 2016). 1907 Proceedings of the 56th Annual Meeting of the Association for Compu"
P18-1177,D16-1264,0,0.652311,") His patents earned him a considerable amount of money, much of which was used to finance his own projects with varying degrees of success. (3) He lived most of his life in a series of New York hotels, through his retirement. (4) Tesla died on 7 January 1943. ... Questions: – What was Tesla’s reputation in popular culture? mad scientist – How did Tesla finance his work? patents – Where did Tesla live for much of his life? New York hotels Figure 1: Example input from the fourth paragraph of a Wikipedia article on Nikola Tesla, along with the natural questions and their answers from the SQuAD (Rajpurkar et al., 2016) dataset. We show in italics the set of mentions that refer to Nikola Tesla — Tesla, him, his, he, etc. Introduction Recently, there has been a resurgence of work in NLP on reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) with the goal of developing systems that can answer questions about the content of a given passage or document. Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field. And there have been a number of efforts in this direction. Miller et al. (2016"
P19-1057,C16-1324,0,0.174916,"Missing"
P19-1057,N16-1166,0,0.0518861,"Missing"
P19-1057,E17-1070,0,0.0475433,"nd of users is generally unavailable. In this paper, we present a 1 That study is distinct from those presented here. See Section 4 for details. 602 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 602–607 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics The dataset will be made publicly available2 . 2 to the datasets commonly employed in studies of argument strategies (Johnson and Goldman, 2009; Walker et al., 2012; Zhang et al., 2016; Wang et al., 2017; Cano-Basave and He, 2016; Al Khatib et al., 2016). Lukin et al. (2017) is the closest work to ours as it studies the effect of OCEAN personality traits (Roccas et al., 2002; T. Norman, 1963) of the audience on how they perceive the persuasiveness of monologic arguments. Note that, in our dataset, we do not have information about users’ personality traits; however, we have extensive information about their demographics, social interactions, beliefs and language use. Related Work and Datasets There has been a tremendous amount of research effort to understand the important linguistic features for identifying argument structure and determining effective argumentati"
P19-1057,P13-1025,0,0.0855349,"Missing"
P19-1057,N18-1094,1,0.838368,"income, religion) and stance on a variety of controversial debate topics as well as a record of user activity on the debate platform (e.g. debates won and lost). We view this new dataset as a resource that affords the NLP and CSS communities the opportunity to understand the effect of audience characteristics on the efficacy of different debating and persuasion strategies as well as to model changes in user’s opinions and activities on a debate platform over time. (To date, part of our debate.org dataset has been used in one such study to understand the effect of prior beliefs in persuasion1 (Durmus and Cardie, 2018). Here, we focus on the properties of the dataset itself and study a different task.) In the next section, we describe the dataset in the context of existing argumentation datasets. We then provide statistics on key aspects of the collected debates and user profiles (Section 3). Section 4 reports a study in which we investigate the predictive effect of selected user traits (namely, the debaters’ and audience’s experience, prior debate success, social interactions, and demographic information) vs. standard linguistic features. Experimental results show that features of the user traits are signi"
P19-1057,2007.sigdial-1.5,0,0.0975286,"Missing"
P19-1057,P11-1099,0,0.421906,"lable. This paper presents a dataset of 78, 376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles. We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind. 1 Introduction Previous work from Natural Language Processing (NLP) and Computational Social Science (CSS) that studies argumentative text and its persuasive effects has mainly focused on identifying the content and structure of an argument (e.g. Feng and Hirst (2011)) and the linguistic features that are indicative of effective argumentation strategies (e.g. Tan et al. (2016)). The effectiveness of an argument, however, cannot be determined solely by its textual content; rather, it is important to consider characteristics of the reader, listener or participants in the debate or discussion. Does the reader already agree with the argument’s stance? Is she predisposed to changing her mind on the particular topic of the debate? Is the style of the argument appropriate for the individual? To date, existing argumentation datasets have permitted only limited ass"
P19-1057,walker-etal-2012-corpus,0,0.0641815,"ation datasets have permitted only limited assessment of such “user” traits because information on the background of users is generally unavailable. In this paper, we present a 1 That study is distinct from those presented here. See Section 4 for details. 602 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 602–607 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics The dataset will be made publicly available2 . 2 to the datasets commonly employed in studies of argument strategies (Johnson and Goldman, 2009; Walker et al., 2012; Zhang et al., 2016; Wang et al., 2017; Cano-Basave and He, 2016; Al Khatib et al., 2016). Lukin et al. (2017) is the closest work to ours as it studies the effect of OCEAN personality traits (Roccas et al., 2002; T. Norman, 1963) of the audience on how they perceive the persuasiveness of monologic arguments. Note that, in our dataset, we do not have information about users’ personality traits; however, we have extensive information about their demographics, social interactions, beliefs and language use. Related Work and Datasets There has been a tremendous amount of research effort to unders"
P19-1057,Q17-1016,0,0.0192307,"d assessment of such “user” traits because information on the background of users is generally unavailable. In this paper, we present a 1 That study is distinct from those presented here. See Section 4 for details. 602 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 602–607 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics The dataset will be made publicly available2 . 2 to the datasets commonly employed in studies of argument strategies (Johnson and Goldman, 2009; Walker et al., 2012; Zhang et al., 2016; Wang et al., 2017; Cano-Basave and He, 2016; Al Khatib et al., 2016). Lukin et al. (2017) is the closest work to ours as it studies the effect of OCEAN personality traits (Roccas et al., 2002; T. Norman, 1963) of the audience on how they perceive the persuasiveness of monologic arguments. Note that, in our dataset, we do not have information about users’ personality traits; however, we have extensive information about their demographics, social interactions, beliefs and language use. Related Work and Datasets There has been a tremendous amount of research effort to understand the important linguistic features"
P19-1057,H05-1044,0,0.0203019,"Missing"
P19-1057,N16-1017,0,0.567354,"xisting argumentation datasets. We then provide statistics on key aspects of the collected debates and user profiles (Section 3). Section 4 reports a study in which we investigate the predictive effect of selected user traits (namely, the debaters’ and audience’s experience, prior debate success, social interactions, and demographic information) vs. standard linguistic features. Experimental results show that features of the user traits are significantly more predictive of a debater’s success than the linguistic features that are shown to be predictive of debater success by the previous work (Zhang et al., 2016). This suggests that user traits are important to take into account in studying success in online debating. Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content, structure and linguistic features of argumentative text. They have been much less successful in fostering studies of the effect of “user” traits — characteristics and beliefs of the participants — on the debate/argument outcome as this type of user information is generally not available. This paper presents a dataset of 78, 376 debates generated over a 10-yea"
P19-1057,D14-1006,0,\N,Missing
P19-1057,N15-1172,0,\N,Missing
P19-1057,D16-1129,0,\N,Missing
P19-1057,W17-5102,0,\N,Missing
P19-1299,Q17-1010,0,0.156594,"Missing"
P19-1299,N18-1111,1,0.83271,"model is unable to utilize features that are shared only between English and German. To address these shortcomings, we propose a new MLTL model that not only exploits languageinvariant features, but also allows the target language to dynamically and selectively leverage language-specific features through a probabilistic attention-style mixture of experts mechanism (see §3). This allows our model to learn effectively what to share between various languages. Another contribution of this paper is that, when combined with the recent unsupervised cross-lingual word embeddings (Lample et al., 2018; Chen and Cardie, 2018b), our model is able to operate in a zero-resource setting where neither task-specific target language annotations nor general-purpose cross-lingual resources (e.g. parallel corpora or machine translation (MT) systems) are available. This is an advantage over many existing CLTL works, making our model more widely applicable to many lower-resource languages. We evaluate our model on multiple MLTL tasks ranging from text classification to named entity recognition and semantic slot filling, including a real-world industry dataset. Our model beats all baseline models trained, like ours, without c"
P19-1299,D18-1024,1,0.860918,"model is unable to utilize features that are shared only between English and German. To address these shortcomings, we propose a new MLTL model that not only exploits languageinvariant features, but also allows the target language to dynamically and selectively leverage language-specific features through a probabilistic attention-style mixture of experts mechanism (see §3). This allows our model to learn effectively what to share between various languages. Another contribution of this paper is that, when combined with the recent unsupervised cross-lingual word embeddings (Lample et al., 2018; Chen and Cardie, 2018b), our model is able to operate in a zero-resource setting where neither task-specific target language annotations nor general-purpose cross-lingual resources (e.g. parallel corpora or machine translation (MT) systems) are available. This is an advantage over many existing CLTL works, making our model more widely applicable to many lower-resource languages. We evaluate our model on multiple MLTL tasks ranging from text classification to named entity recognition and semantic slot filling, including a real-world industry dataset. Our model beats all baseline models trained, like ours, without c"
P19-1299,N18-1032,1,0.773415,"static weights at the task level, while our model can dynamically select what to share at the instance level. A very recent work (Guo et al., 2018) attempts to model the relation between the target domain and each source domain. Our model combines the strengths of these methods and is able to simul3099 JC JD 1 JD Language Label Language Discriminator D straint and many useful features may be wiped out by adversarial training if they are shared only between the target language and a subset of source languages. Therefore, we propose to use a mixture-of-experts (MoE) model (Shazeer et al., 2017; Gu et al., 2018) to learn the private features. The idea is to have a set of language expert networks, one per source language, each responsible for learning language-specific features for that source language during training. However, instead of hard-switching between the experts, each sample uses a convex combination of all experts, dictated by an expert gate. Thus, at test time, the trained expert gate can decide the optimal expert weights for the unseen target language based on its similarity to the source languages. Figure 1 shows an overview of our MAN-MoE model for multilingual model transfer. The boxe"
P19-1299,D18-1498,0,0.115728,"Missing"
P19-1299,D14-1080,1,0.880726,"Missing"
P19-1299,D17-1302,0,0.0218392,"les from different languages extracted by the same neural net remain divergent, and hence weight sharing is not sufficient for learning a language-invariant feature space that generalizes well across languages. As such, previ2 In contrast, supervised CLTL assumes the availability of annotations in the target language. 3098 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3098–3112 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ous work has explored using language-adversarial training (Chen et al., 2016; Kim et al., 2017) to extract features that are invariant with respect to the shift in language, using only (non-parallel) unlabeled texts from each language. On the other hand, in the MLTL setting, where multiple source languages exist, languageadversarial training will only use, for model transfer, the features that are common among all source languages and the target, which may be too restrictive in many cases. For example, when transferring from English, Spanish and Chinese to German, language-adversarial training will retain only features that are invariant across all four languages, which can be too spars"
P19-1299,D14-1181,0,0.00319016,"dd character-level word embeddings (Dos Santos and Zadrozny, 2014) that captures sub-word information. When character embeddings are used, we add a single CharCNN that is shared across all languages, and the final word representation is the concatenation of the word embedding and the char-level embedding. The CharCNN can then be trained end to end with the rest of the model. MAN Shared Feature Extractor Fs is a multinomial adversarial network (Chen and Cardie, 2018a), which is an adversarial pair of a feature extractor (e.g. LSTM or CNN) and a language discriminator D. D is a text classifier (Kim, 2014) that takes the shared features (extracted by Fs ) of an input sequence and predicts which language it comes from. On the other hand, Fs strives to fool D so that it cannot identify the language of a sample. The hypothesis is that if D cannot recognize the language of the input, the shared features then do not contain language information and are hence language-invariant. Note that D is trained only using unlabeled texts, and can therefore be trained on all languages including the target language. MoE Private Feature Extractor Fp is a key difference from previous work, shown in Figure 2. The f"
P19-1299,C12-1089,0,0.0197063,"16) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2018b). These efforts pave the road for performing CLTL without cross-lingual resources. Finally, a related field to MLTL is multi-source domain adaptation (Mansour et al., 2009), where most prior work relies on the learning of domaininvariant features (Zhao et al., 2018; Chen and Cardie, 2018a). Ruder et al. (2019) pr"
P19-1299,D15-1166,0,0.0454517,"Fp is able to dynamically determine what knowledge to use from each individual source language at a token level. MoE Task-Specific Predictor C is the final module that make predictions for the end task, and may take different forms depending on the task. For instance, for sequence tagging tasks, the shared and private features are first concatenated for each token, and then past through a MoE module similar to Fp (as shown in Figure 6 in the Appendix). It is straightforward to adapt C to work for other tasks. For example, for text classification, a pooling layer such as dot-product attention (Luong et al., 2015) is added at the bottom to fuse token-level features into a single sentence feature vector. C first concatenates the shared and private features to form a single feature vector for each token. It then has another MoE module that outputs a softmax probability over all labels for each token. The idea is that it may be favorable to put different weights between the language-invariant and language-specific features for different target languages. Again consider the example of English, German, Spanish and Chinese. When transferring to Chinese from the other three, the source lan3101 Algorithm 1 MAN"
P19-1299,D17-1269,0,0.127447,"Missing"
P19-1299,D11-1006,0,0.199876,"a target language using annotated data from other languages (source languages) (Yarowsky et al., 2001). In this paper, we concentrate on the more challenging unsupervised CLTL setting, where no target language labeled data is used for training.2 Traditionally, most research on CLTL has been devoted to the standard bilingual transfer (BLTL) case where training data comes from a single source language. In practice, however, it is often the case that we have labeled data in a few languages, and would like to be able to utilize all of the data when transferring to other languages. Previous work (McDonald et al., 2011) indeed showed that transferring from multiple source languages could result in significant performance improvement. Therefore, in this work, we focus on the multi-source CLTL scenario, also known as multilingual transfer learning (MLTL), to further boost the target language performance. One straightforward method employed in CLTL is weight sharing, namely directly applying the model trained on the source language to the target after mapping both languages to a common embedding space. As shown in previous work (Chen et al., 2016), however, the distributions of the hidden feature vectors of sam"
P19-1299,P12-1066,0,0.0388569,"systems or parallel corpora are utilized to replace taskspecific annotated data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2"
P19-1299,P17-1135,0,0.225946,"Missing"
P19-1299,W15-1512,0,0.126077,"on dataset (Prettenhofer and Stein, 2010). The dataset is a binary classification dataset where each review is classified into positive or negative sentiment. It has four languages: English, German, French and Japanese. As shown in Table 5, MT-BOW uses machine translation to translate the bag of words of a target sentence into the source language, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel corpus (translated Amazon reviews) that are difficult to obtain in practice, making the num3105 German Domain music avg"
P19-1299,P10-1114,0,0.66091,"natural language processing. In order to alleviate the need for obtaining annotated data for each task in each language, cross-lingual transfer learning (CLTL) has long been studied (Yarowsky et al., 2001; Bel et al., 2003, inter alia). For unsupervised CLTL in particular, where no target language training data is available, most prior research investigates the bilingual transfer setting. Traditionally, research focuses on resource-based methods, where general-purpose cross-lingual resources such as MT systems or parallel corpora are utilized to replace taskspecific annotated data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), sh"
P19-1299,W02-2024,0,0.765396,"51 2014 1366 480 Domain Navigation Calendar Files Chinese 1497 926 843 1986 1081 970 #Train #Dev #Test #Slot 7472 2056 1289 1114 309 256 1173 390 215 8 4 5 Examples [Driving]transportation type directions to [Walmart]place name in [New York ]location . Add [school meeting]title to my calendar on [Monday]start date at [noon]start time . Search for [notes]data type with [grocery list]keyword . Table 1: Statistics for the Multilingual Semantic Slot Filling dataset with examples from each domain. academic datasets, namely the CoNLL multilingual named entity recognition (sequence tagging) dataset (Sang, 2002; Sang and Meulder, 2003), and the multilingual Amazon reviews (text classification) dataset (Prettenhofer and Stein, 2010). 4.1 Cross-Lingual Semantic Slot Filling As shown in Table 1, we collect data for four languages: English, German, Spanish, and Chinese, over three domains: Navigation, Calendar, and Files. Each domain has a set of pre-determined slots (the slots are the same across languages), and the user utterances in each language and domain are annotated by crowd workers with the correct slots (see the examples in Table 1). We employ the standard BIO tagging scheme to formulate the s"
P19-1299,W03-0419,0,0.615088,"Missing"
P19-1299,K16-1022,0,0.077161,"gical features such as capitalization is crucial for NER. We hence add character-level word embeddings for this task (§3.1) to capture subword fea3104 de es nl avg 59.3 61.0 60.6 65.1 66.0 58.4 64.0 61.6 65.4 64.5 52.7 60.3 56.8 63.0 62.3 Methods without cross-lingual resources MAN-MoE 55.1 59.5 BWE+CharCNN (1-to-1) 51.5 61.0 BWE+CharCNN (3-to-1) 55.8 70.4 Xie et al. (2018)* 56.9 71.0 MAN-MoE+CharCNN 56.7 71.0 MAN-MoE+CharCNN+UMWE 56.0 73.5 61.8 67.3 69.8 71.3 70.9 72.4 58.8 60.0 65.3 66.4 66.2 67.3 Methods with cross-lingual resources T¨ackstr¨om et al. (2012) 40.4 Nothman et al. (2013) 55.8 Tsai et al. (2016) 48.1 Ni et al. (2017) 58.5 Mayhew et al. (2017) 57.5 * Average Gate Weights Target Language 0.40 0.35 0.30 0.25 en fr ja Target Lang: de en de ja Target Lang: fr en de fr Target Lang: ja Figure 3: Average expert gate weights aggregated on a language level for the Amazon Reviews dataset. Contemporaneous work Table 4: F1 scores for the CoNLL NER dataset on German (de), Spanish (es) and Dutch (nl). tures and alleviate the OOV problem. For German, however, all nouns are capitalized, and the capitalization features learned on the other three languages would lead to poor results. Therefore, for Ger"
P19-1299,P09-1027,0,0.0919103,"llenge for natural language processing. In order to alleviate the need for obtaining annotated data for each task in each language, cross-lingual transfer learning (CLTL) has long been studied (Yarowsky et al., 2001; Bel et al., 2003, inter alia). For unsupervised CLTL in particular, where no target language training data is available, most prior research investigates the bilingual transfer setting. Traditionally, research focuses on resource-based methods, where general-purpose cross-lingual resources such as MT systems or parallel corpora are utilized to replace taskspecific annotated data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzil"
P19-1299,D13-1153,0,0.0490677,"w). 4.3 Cross-Lingual Text Classification on Amazon Reviews Finally, we report results on a multilingual text classification dataset (Prettenhofer and Stein, 2010). The dataset is a binary classification dataset where each review is classified into positive or negative sentiment. It has four languages: English, German, French and Japanese. As shown in Table 5, MT-BOW uses machine translation to translate the bag of words of a target sentence into the source language, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel"
P19-1299,D18-1034,0,0.229425,"Missing"
P19-1299,P18-1072,0,0.0561644,"Missing"
P19-1299,D17-1053,0,0.0469431,"ied into positive or negative sentiment. It has four languages: English, German, French and Japanese. As shown in Table 5, MT-BOW uses machine translation to translate the bag of words of a target sentence into the source language, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel corpus (translated Amazon reviews) that are difficult to obtain in practice, making the num3105 German Domain music avg books dvd music avg Methods with general-purpose cross-lingual resources MT-BOW1 79.68 77.92 77.22 78.27 80.76 CL-SCL"
P19-1299,P17-1130,0,0.0670095,"anguage, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel corpus (translated Amazon reviews) that are difficult to obtain in practice, making the num3105 German Domain music avg books dvd music avg Methods with general-purpose cross-lingual resources MT-BOW1 79.68 77.92 77.22 78.27 80.76 CL-SCL1 79.50 76.92 77.79 78.07 78.49 CR-RL2 79.89 77.14 77.27 78.10 78.25 Bi-PV3 79.51 78.60 82.45 80.19 84.25 UMM4 81.65 81.27 81.32 81.41 80.27 78.83 78.80 74.83 79.60 80.27 75.78 77.92 78.71 80.09 79.41 78.46 78.40 77.26 81.31"
P19-1299,N13-1126,0,0.0762296,"Missing"
P19-1299,H01-1035,0,0.696461,"the first author was an intern at Microsoft Research. 1 The code is available at https://github.com/ microsoft/Multilingual-Model-Transfer. large-scale annotated datasets. However, such an advantage is not available to most of the world languages since many of them lack the the labeled data necessary for training deep neural nets for a variety of NLP tasks. As it is prohibitive to obtain training data for all languages of interest, crosslingual transfer learning (CLTL) offers the possibility of learning models for a target language using annotated data from other languages (source languages) (Yarowsky et al., 2001). In this paper, we concentrate on the more challenging unsupervised CLTL setting, where no target language labeled data is used for training.2 Traditionally, most research on CLTL has been devoted to the standard bilingual transfer (BLTL) case where training data comes from a single source language. In practice, however, it is often the case that we have labeled data in a few languages, and would like to be able to utilize all of the data when transferring to other languages. Previous work (McDonald et al., 2011) indeed showed that transferring from multiple source languages could result in s"
P19-1299,N12-1052,0,0.307376,"Missing"
P19-1299,D15-1213,0,0.0272914,"data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2018b). These efforts pave the road for performing CLTL without cross-lingual re"
P19-1299,D13-1141,0,0.0421518,"sarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2018b). These efforts pave the road for performing CLTL without cross-lingual resources. Finally, a related field to MLTL is multi-source domain adaptation (Mansour et al., 2009), where most prior work relies on the learning of domaininvariant features (Zhao et al., 2018; Chen and Cardie, 2018a). Ruder et al. (2019) propose a general fr"
P19-1407,W05-0909,0,0.166761,"arsing, where the logical form is in SPARQL), and WikiSQL (Zhong et al., 2017) (where the logical form is SQL). Both datasets are small, with the former having 3098 training and 1639 testing examples, and the latter being an order of magnitude larger with 56346 training and 15873 testing examples. We evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question). BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) are chosen for precision and recall-based metrics. METEOR (Banerjee and Lavie, 2005) is chosen to deal with stemming and synonyms. We noticed that many tokens that appear in the logical form are also present in the natural language form for each example. In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016), implying that a network with the ability to copy from the input could see significant gains on the task. Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-rel"
P19-1407,N16-1012,0,0.0283807,", we see sharper attention earlier in the sentence that better aligns with word-level translations (e.g. ’hand’ is properly attended to with scratchpad, but not with non-scratchpad). Additionally, some words that are never properly translated (e.g. wahrscheinlich - ’probably’) by the non-scratchpad model are not heavily attended to, whereas with the scratchpad mechanism, they are. be used with any choice of encoder/decoder as long as attention is used. Summarization Since Rush et al. (2015) first applied neural networks to abstractive text summarization, work has focused on augmenting models (Chopra et al., 2016; Nallapati et al., 2016b; Gu et al., 2016), incorporating syntactic and semantic information (Takase et al., 2016), or direct optimization of the metric at hand (Ranzato et al., 2016). Nallapati et al. (2016b) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization and provided the first abstractive and extractive (Nallapati et al., 2016a) models. See et al. (2017) demonstrated that pointer-generator networks can significantly improve the quality of generated summaries. Additionally, work has explored using Reinforcement Learning, often with additional losses"
P19-1407,P16-1004,0,0.0238181,"nd Text Summarization — and obtain stateof-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output. 1 Introduction The sequence-to-sequence neural network framework (seq2seq) (Sutskever et al., 2014) has been successful in a wide range of tasks in natural language processing, from machine translation (Bahdanau et al., 2014) and semantic parsing (Dong and Lapata, 2016) to summarization (Nallapati et al., 2016b; See et al., 2017). Despite this success, seq2seq models are known to often exhibit an overall lack of fluency in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017). Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) ∗ Work performed while at Apple. to deal with r"
P19-1407,P17-1123,1,0.88332,"mework that operates on entity and predicate embeddings trained using TransE (Bordes et al., 2011). Later, Elsahar et al. (2018) extended this approach to support unseen predicates. Both approaches operate on triplets, meaning they have limited capability beyond generating simple questions and cannot generate the far more complex compositional questions that our approach does by operating on the more expressive SPARQL query (logical form). In the question generation domain, 4164 there has been a recent surge in research on generating questions for a given paragraph of text (Song et al., 2017; Du et al., 2017; Tang et al., 2017; Duan et al., 2017; Wang et al., 2018; Yao et al., 2018), with most of the work being a variant of the seq2seq approach. In Song et al. (2017), a seq2seq model with copynet and a coverage mechanism (Tu et al., 2016) is used to achieve state-of-the-art results. We have demonstrated that our Scratchpad outperforms this approach in both quantitative and qualitative evaluations. Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in Tu et al. (2016) and later adapted for summarization in See et al. (2017). Both works"
P19-1407,D17-1090,0,0.210525,"redicate embeddings trained using TransE (Bordes et al., 2011). Later, Elsahar et al. (2018) extended this approach to support unseen predicates. Both approaches operate on triplets, meaning they have limited capability beyond generating simple questions and cannot generate the far more complex compositional questions that our approach does by operating on the more expressive SPARQL query (logical form). In the question generation domain, 4164 there has been a recent surge in research on generating questions for a given paragraph of text (Song et al., 2017; Du et al., 2017; Tang et al., 2017; Duan et al., 2017; Wang et al., 2018; Yao et al., 2018), with most of the work being a variant of the seq2seq approach. In Song et al. (2017), a seq2seq model with copynet and a coverage mechanism (Tu et al., 2016) is used to achieve state-of-the-art results. We have demonstrated that our Scratchpad outperforms this approach in both quantitative and qualitative evaluations. Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in Tu et al. (2016) and later adapted for summarization in See et al. (2017). Both works try to minimize erroneous repetitions"
P19-1407,N18-1020,0,0.0128637,"nto the summary which are used to restrict a second pointer-generator model can reap significant gains. Question Generation Early work on translating SPARQL queries into natural language relied heavily on hand-crafted rules (Ngonga Ngomo et al., 2013a,b) or manually crafted templates to map selected categories of SPARQL queries to questions (Trivedi et al., 2017; Seyler et al., 2017). In (Serban et al., 2016) knowledge base triplets are used to generate questions using encoder-decoder framework that operates on entity and predicate embeddings trained using TransE (Bordes et al., 2011). Later, Elsahar et al. (2018) extended this approach to support unseen predicates. Both approaches operate on triplets, meaning they have limited capability beyond generating simple questions and cannot generate the far more complex compositional questions that our approach does by operating on the more expressive SPARQL query (logical form). In the question generation domain, 4164 there has been a recent surge in research on generating questions for a given paragraph of text (Song et al., 2017; Du et al., 2017; Tang et al., 2017; Duan et al., 2017; Wang et al., 2018; Yao et al., 2018), with most of the work being a varia"
P19-1407,E17-2075,0,0.0133113,"al. (2016), for example, use an extra GRU to keep track of this information, whereas See et al. (2017) keep track of the sum of attention weights and add a penalty to the loss function based on it to discourage repetition. Our approach is much simpler than either solution since it does not require any extra vectors or an additional loss term; rather, the encoder vector itself is being used as scratch memory. Our experiments also show that for the question generation task, the Scratchpad performs better than coverage based approaches. Our idea was influenced by the dialogue generation work of Eric and Manning (2017) in which the entire sequence of interactions is re-encoded every time a response is generated by the decoder. 7 Conclusion In this paper, we introduce the Mechanism, a novel write operator, to the sequence to sequence framework aimed at addressing many of the common issues encountered by sequence to sequence models and evaluate it on a variety of standard conditional natural language generation tasks. By letting the decoder ’keep notes’ on the encoder, or said another way, re-encode the input at every decoding step, the Scratchpad Mechanism effectively guides future generation. The Scratchpad"
P19-1407,D18-1443,0,0.0362218,"Missing"
P19-1407,D15-1013,0,0.0401091,"Missing"
P19-1407,P16-1154,0,0.533789,"specific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) ∗ Work performed while at Apple. to deal with repetition and over-copying in summarization, Hua and Wang (2018) introduced a method of attending over keyphrases to improve argument generation, and Kiddon et al. (2016) introduced a method that attends to an agenda of items to improve recipe generation. Perhaps not surprisingly, general-purpose attention mechanisms targeting individual problems from the list above have also begun to be developed. Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015), for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of Tu et al. (2016) tackle repetition and under-generation. These techniques, however, often require significant hyperparameter tuning and are purposely limited to fixing a specific problem in the generated text. We present here a general-purpose addition to the standard seq2seq framework that aims to simultaneously tackle all of the above issues. In particular, we propose Scratchpad, a novel write mechanism th"
P19-1407,P17-1019,0,0.0271241,"l form are also present in the natural language form for each example. In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016), implying that a network with the ability to copy from the input could see significant gains on the task. Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-related problems. Seq2Seq is the standard approach introduced in Sutskever et al. (2014). The Copynet (He et al., 2017) baseline additionally gives the Seq2Seq model the ability to copy vocabulary from the source to the target. From Table 2 it is clear that our approach, Scratchpad outperforms all baselines on all the metrics. Experimental Details Our encoder is a 2-layer bi-directional GRU where outputs are combined by concatenation, and our decoder is a 2-layer 4160 Rouge Model Meteor 1 2 L exact match +stem/syn/para Pointer-generator See et al. (2017) Scratchpad 36.44 39.53 39.65 15.66 17.28 17.61 33.42 36.38 36.62 15.35 17.32 17.26 16.65 18.72 18.63 CopyTransformer + Coverage Penalty Pointer-Generator + Ma"
P19-1407,P18-1013,0,0.0162096,"corporating syntactic and semantic information (Takase et al., 2016), or direct optimization of the metric at hand (Ranzato et al., 2016). Nallapati et al. (2016b) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization and provided the first abstractive and extractive (Nallapati et al., 2016a) models. See et al. (2017) demonstrated that pointer-generator networks can significantly improve the quality of generated summaries. Additionally, work has explored using Reinforcement Learning, often with additional losses or objective functions to improve performance (Hsu et al., 2018; Paulus et al., 2018; Li et al., 2018; elikyilmaz et al., 2018; Pasunuru and Bansal, 2018). Finally, Gehrmann et al. (2018) demonstrated that a two-stage procedure, where a model first identifies spans in the article that could be copied into the summary which are used to restrict a second pointer-generator model can reap significant gains. Question Generation Early work on translating SPARQL queries into natural language relied heavily on hand-crafted rules (Ngonga Ngomo et al., 2013a,b) or manually crafted templates to map selected categories of SPARQL queries to questions (Trivedi et al.,"
P19-1407,P18-1021,0,0.0193046,"ee et al., 2017). Despite this success, seq2seq models are known to often exhibit an overall lack of fluency in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017). Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) ∗ Work performed while at Apple. to deal with repetition and over-copying in summarization, Hua and Wang (2018) introduced a method of attending over keyphrases to improve argument generation, and Kiddon et al. (2016) introduced a method that attends to an agenda of items to improve recipe generation. Perhaps not surprisingly, general-purpose attention mechanisms targeting individual problems from the list above have also begun to be developed. Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015), for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of Tu et al. (2016) tackle repetition and under-gen"
P19-1407,D16-1032,0,0.0230218,"ncy in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017). Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) ∗ Work performed while at Apple. to deal with repetition and over-copying in summarization, Hua and Wang (2018) introduced a method of attending over keyphrases to improve argument generation, and Kiddon et al. (2016) introduced a method that attends to an agenda of items to improve recipe generation. Perhaps not surprisingly, general-purpose attention mechanisms targeting individual problems from the list above have also begun to be developed. Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015), for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of Tu et al. (2016) tackle repetition and under-generation. These techniques, however, often require significant hyperparameter tuning and are purposely limi"
P19-1407,W04-1013,0,0.145943,": WebQuestionsSP (Yih et al., 2016) (a standard dataset for semantic parsing, where the logical form is in SPARQL), and WikiSQL (Zhong et al., 2017) (where the logical form is SQL). Both datasets are small, with the former having 3098 training and 1639 testing examples, and the latter being an order of magnitude larger with 56346 training and 15873 testing examples. We evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question). BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) are chosen for precision and recall-based metrics. METEOR (Banerjee and Lavie, 2005) is chosen to deal with stemming and synonyms. We noticed that many tokens that appear in the logical form are also present in the natural language form for each example. In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016), implying that a network with the ability to copy from the input could see significant gains on the task. Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and ("
P19-1407,2015.iwslt-evaluation.11,0,0.024505,"rase-level generation is expected. Question Generation from logical forms requires reasoning about the syntax, parse tree, and vocabulary of the input sequence to infer the meaning of the logical form program and utilize copy-mechanism to copy entities. Lastly, sumTranslation IWSLT14 IWSLT15 De→En Es →En En→Vi MIXER AC + LL NPMT 21.83 28.53 29.96 7 7 7 7 7 28.07 Stanford NMT Transformer (6 layer) Layer-Coord (14 layer) Scratchpad (3 layer) 7 32.86 35.07 35.08 7 38.57 40.50 40.92 26.1 7 7 29.59∗ Table 1: Performance for non-scratchpad models are taken from He et al. (2018) except Stanford NMT (Luong and Manning, 2015). ∗: model is 2 layers. Experimental Details For IWSLT14, our encoder is a 3-layer Bi-LSTM (Hochreiter and Schmidhuber, 1997), where outputs are combined by concatenation, and the decoder is a 3-layer LSTM as well. For IWSLT15 the encoder and decoder are 2-layers. We follow Luong et al. (2015), using the ’general’ score function, input feeding, and combining the attentional context and hidden state. Since we use input feeding, Steps (1) and (2) in Section 3 are switched. All our models have a 4159 Per-Sentence WikiSQL WebQSP Model Baseline Copynet Copy + Coverage Copy + Scratchpad Baseline Cop"
P19-1407,D15-1166,0,0.0816172,"Es →En En→Vi MIXER AC + LL NPMT 21.83 28.53 29.96 7 7 7 7 7 28.07 Stanford NMT Transformer (6 layer) Layer-Coord (14 layer) Scratchpad (3 layer) 7 32.86 35.07 35.08 7 38.57 40.50 40.92 26.1 7 7 29.59∗ Table 1: Performance for non-scratchpad models are taken from He et al. (2018) except Stanford NMT (Luong and Manning, 2015). ∗: model is 2 layers. Experimental Details For IWSLT14, our encoder is a 3-layer Bi-LSTM (Hochreiter and Schmidhuber, 1997), where outputs are combined by concatenation, and the decoder is a 3-layer LSTM as well. For IWSLT15 the encoder and decoder are 2-layers. We follow Luong et al. (2015), using the ’general’ score function, input feeding, and combining the attentional context and hidden state. Since we use input feeding, Steps (1) and (2) in Section 3 are switched. All our models have a 4159 Per-Sentence WikiSQL WebQSP Model Baseline Copynet Copy + Coverage Copy + Scratchpad Baseline Copynet Copy + Coverage Copy + Scratchpad Corpus-Level Bleu Meteor Rouge-L Bleu Meteor Rouge-L 7.51 23.9 47.1 17.96 22.9 47.13 6.89 14.55 15.29 27.1 33.7 34.7 52.5 58.9 59.5 17.42 26.78 27.64 26.03 30.86 31.49 52.56 58.91 59.44 9.94 26.71 47.96 17.34 25.34 47.96 8.04 15.76 16.89 24.66 34.04 34.47"
P19-1407,K16-1028,0,0.0611422,"Missing"
P19-1407,P02-1040,0,0.106108,"(question, logical form) pairs: WebQuestionsSP (Yih et al., 2016) (a standard dataset for semantic parsing, where the logical form is in SPARQL), and WikiSQL (Zhong et al., 2017) (where the logical form is SQL). Both datasets are small, with the former having 3098 training and 1639 testing examples, and the latter being an order of magnitude larger with 56346 training and 15873 testing examples. We evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question). BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) are chosen for precision and recall-based metrics. METEOR (Banerjee and Lavie, 2005) is chosen to deal with stemming and synonyms. We noticed that many tokens that appear in the logical form are also present in the natural language form for each example. In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016), implying that a network with the ability to copy from the input could see significant gains on the task. Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq"
P19-1407,N18-2102,0,0.0159774,"ptimization of the metric at hand (Ranzato et al., 2016). Nallapati et al. (2016b) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization and provided the first abstractive and extractive (Nallapati et al., 2016a) models. See et al. (2017) demonstrated that pointer-generator networks can significantly improve the quality of generated summaries. Additionally, work has explored using Reinforcement Learning, often with additional losses or objective functions to improve performance (Hsu et al., 2018; Paulus et al., 2018; Li et al., 2018; elikyilmaz et al., 2018; Pasunuru and Bansal, 2018). Finally, Gehrmann et al. (2018) demonstrated that a two-stage procedure, where a model first identifies spans in the article that could be copied into the summary which are used to restrict a second pointer-generator model can reap significant gains. Question Generation Early work on translating SPARQL queries into natural language relied heavily on hand-crafted rules (Ngonga Ngomo et al., 2013a,b) or manually crafted templates to map selected categories of SPARQL queries to questions (Trivedi et al., 2017; Seyler et al., 2017). In (Serban et al., 2016) knowledge base triplets are used to ge"
P19-1407,D14-1162,0,0.0885952,"or end-to-end models on summarization without Reinforcement Learning on ROUGE, while remaining competitive with See et al. (2017) on METEOR. Additionally, Scratchpad does not use an auxiliary loss as in See et al. (2017) or the middle third of the table. Gehrmann et al. (2018) do not evaluate on METEOR. GRU. We use the attention mechanism from 4.1. We train all models for 75 epochs with a batch size of 32, a hidden size of 512 (for the GRU and any MLP’s), and a word vector size of 300. Dropout is used on every layer except the output layer, with a drop probability of 0.5. Where Glove vectors (Pennington et al., 2014) are used to initialize word vectors, we use 300-dimensional vectors trained on Wikipedia and Gigaword (6B.300D). We use the Adam optimizer with a learning rate of 1e−4 and we do teacher forcing (Williams and Zipser, 1989) with probability 0.5. These hyperparameters were tuned for our Seq2Seq baselines and held constant for the rest of the models. The vocabulary consists of all tokens appearing at least once in the training set. 4.3 Summarization We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016b) as in See et al. (2017). The dataset consists of 287,226 training,"
P19-1407,D15-1044,0,0.0441152,"@@” at the end of a bpe token denotes it should be concatenated with the following token(s) to make a word. With Scratchpad, we see sharper attention earlier in the sentence that better aligns with word-level translations (e.g. ’hand’ is properly attended to with scratchpad, but not with non-scratchpad). Additionally, some words that are never properly translated (e.g. wahrscheinlich - ’probably’) by the non-scratchpad model are not heavily attended to, whereas with the scratchpad mechanism, they are. be used with any choice of encoder/decoder as long as attention is used. Summarization Since Rush et al. (2015) first applied neural networks to abstractive text summarization, work has focused on augmenting models (Chopra et al., 2016; Nallapati et al., 2016b; Gu et al., 2016), incorporating syntactic and semantic information (Takase et al., 2016), or direct optimization of the metric at hand (Ranzato et al., 2016). Nallapati et al. (2016b) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization and provided the first abstractive and extractive (Nallapati et al., 2016a) models. See et al. (2017) demonstrated that pointer-generator networks can significantly improve the"
P19-1407,P17-1099,0,0.124587,"erformance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output. 1 Introduction The sequence-to-sequence neural network framework (seq2seq) (Sutskever et al., 2014) has been successful in a wide range of tasks in natural language processing, from machine translation (Bahdanau et al., 2014) and semantic parsing (Dong and Lapata, 2016) to summarization (Nallapati et al., 2016b; See et al., 2017). Despite this success, seq2seq models are known to often exhibit an overall lack of fluency in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017). Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) ∗ Work performed while at Apple. to deal with repetition and over-copying in summarization, Hua and Wang (20"
P19-1407,P16-1162,0,0.00896783,"94 26.71 47.96 17.34 25.34 47.96 8.04 15.76 16.89 24.66 34.04 34.47 46.82 54.94 55.69 15.11 25.01 26.10 23.53 32.38 32.76 46.82 54.94 55.69 Table 2: Methods allowing the model to keep track of past attention (Coverage, Scratchpad) significantly improve performance when combined with a copy mechanism. The Scratchpad Encoder achieves the best performance. hidden size of 512 (for the LSTM and any MLP’s). The internal layers of the decoder are residual, adding their output to their input and putting it through Layer Normalization (Ba et al., 2016). Sentences were encoded using byte-pair encoding (Sennrich et al., 2016), with a shared source-target vocabulary of 10, 000 for De→En and Es→En (En →Vi uses words as tokens to be comparable to Wu et al. (2016)). Source and target word embeddings are dimension 128. We use dropout (Srivastava et al., 2014) in the encoder and decoder with a probability of 0.1. We use the Adam optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.002.We train for 30/20 epochs for IWSLT14/15, decaying the learning rate by a factor of 0.7 whenever the validation loss does not improve from the last epoch. Each training batch contained at most 2000 source or target tokens. W"
P19-1407,P16-1056,0,0.0546263,"Missing"
P19-1407,D16-1112,0,0.0180623,"properly attended to with scratchpad, but not with non-scratchpad). Additionally, some words that are never properly translated (e.g. wahrscheinlich - ’probably’) by the non-scratchpad model are not heavily attended to, whereas with the scratchpad mechanism, they are. be used with any choice of encoder/decoder as long as attention is used. Summarization Since Rush et al. (2015) first applied neural networks to abstractive text summarization, work has focused on augmenting models (Chopra et al., 2016; Nallapati et al., 2016b; Gu et al., 2016), incorporating syntactic and semantic information (Takase et al., 2016), or direct optimization of the metric at hand (Ranzato et al., 2016). Nallapati et al. (2016b) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization and provided the first abstractive and extractive (Nallapati et al., 2016a) models. See et al. (2017) demonstrated that pointer-generator networks can significantly improve the quality of generated summaries. Additionally, work has explored using Reinforcement Learning, often with additional losses or objective functions to improve performance (Hsu et al., 2018; Paulus et al., 2018; Li et al., 2018; elikyilmaz e"
P19-1407,P16-1008,0,0.160906,"danau et al., 2014) and semantic parsing (Dong and Lapata, 2016) to summarization (Nallapati et al., 2016b; See et al., 2017). Despite this success, seq2seq models are known to often exhibit an overall lack of fluency in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017). Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) ∗ Work performed while at Apple. to deal with repetition and over-copying in summarization, Hua and Wang (2018) introduced a method of attending over keyphrases to improve argument generation, and Kiddon et al. (2016) introduced a method that attends to an agenda of items to improve recipe generation. Perhaps not surprisingly, general-purpose attention mechanisms targeting individual problems from the list above have also begun to be developed. Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015), for example, aim to reduce input-output vocabulary mismatch and, ther"
P19-1407,P18-1204,0,0.0197852,"trained using TransE (Bordes et al., 2011). Later, Elsahar et al. (2018) extended this approach to support unseen predicates. Both approaches operate on triplets, meaning they have limited capability beyond generating simple questions and cannot generate the far more complex compositional questions that our approach does by operating on the more expressive SPARQL query (logical form). In the question generation domain, 4164 there has been a recent surge in research on generating questions for a given paragraph of text (Song et al., 2017; Du et al., 2017; Tang et al., 2017; Duan et al., 2017; Wang et al., 2018; Yao et al., 2018), with most of the work being a variant of the seq2seq approach. In Song et al. (2017), a seq2seq model with copynet and a coverage mechanism (Tu et al., 2016) is used to achieve state-of-the-art results. We have demonstrated that our Scratchpad outperforms this approach in both quantitative and qualitative evaluations. Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in Tu et al. (2016) and later adapted for summarization in See et al. (2017). Both works try to minimize erroneous repetitions generated by a copy"
P19-1407,P16-2033,0,0.0152434,"tor of 0.7 whenever the validation loss does not improve from the last epoch. Each training batch contained at most 2000 source or target tokens. We use label smoothing with ls = 0.1 (Szegedy et al., 2016). We average the last 5 epochs to obtain the final model and run with a beam of size 4. 4.2 Question Generation We use the task of question generation: Given a structured representation of a query against a knowledge base or a database (e.g. a logical form), produce the corresponding natural language question. We use two datasets consisting of (question, logical form) pairs: WebQuestionsSP (Yih et al., 2016) (a standard dataset for semantic parsing, where the logical form is in SPARQL), and WikiSQL (Zhong et al., 2017) (where the logical form is SQL). Both datasets are small, with the former having 3098 training and 1639 testing examples, and the latter being an order of magnitude larger with 56346 training and 15873 testing examples. We evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question). BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) are chosen for precision"
P19-1407,N18-1150,0,0.0169137,"t al., 2016), or direct optimization of the metric at hand (Ranzato et al., 2016). Nallapati et al. (2016b) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization and provided the first abstractive and extractive (Nallapati et al., 2016a) models. See et al. (2017) demonstrated that pointer-generator networks can significantly improve the quality of generated summaries. Additionally, work has explored using Reinforcement Learning, often with additional losses or objective functions to improve performance (Hsu et al., 2018; Paulus et al., 2018; Li et al., 2018; elikyilmaz et al., 2018; Pasunuru and Bansal, 2018). Finally, Gehrmann et al. (2018) demonstrated that a two-stage procedure, where a model first identifies spans in the article that could be copied into the summary which are used to restrict a second pointer-generator model can reap significant gains. Question Generation Early work on translating SPARQL queries into natural language relied heavily on hand-crafted rules (Ngonga Ngomo et al., 2013a,b) or manually crafted templates to map selected categories of SPARQL queries to questions (Trivedi et al., 2017; Seyler et al., 2017). In (Serban et al., 2016) knowledge"
P19-1456,E17-1024,0,0.497582,"pecificity of any claims employed in the proposed argument. Consider, for example, the argument thesis (i.e., the topic) of Figure 1: (T HESIS) Would we like to live in the world of Harry Potter? Construction of an argument in support or in opposition to this thesis necessarily requires knowing the stance of the claims that comprise it: the claim Magic opens a lot of interesting possibilities should be identified as a claim in support of the T HESIS, and The capacity of harm is greater when magic is involved (H ARM), as a claim in opposition. Indeed, previous work has studied this task (e.g., Bar-Haim et al. (2017); Faulkner (2014)). It is not sufficient, however, to determine claim stance only with respect to the argument thesis. Debate and argument generation systems, in general, should also be able to determine whether two claims that address the same line of reasoning represent the same, or the opposing stance: using Defense is also made easier through magic to refute the H ARM claim in Figure 1, for example, requires recognizing that it represents the opposite stance. The issue of claim specificity in argumentation has been much less addressed. Existing work, however, suggests that a high degree of"
P19-1456,P11-1151,0,0.012878,"particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argum"
P19-1456,P18-1058,0,0.0810739,"stance only with respect to the argument thesis. Debate and argument generation systems, in general, should also be able to determine whether two claims that address the same line of reasoning represent the same, or the opposing stance: using Defense is also made easier through magic to refute the H ARM claim in Figure 1, for example, requires recognizing that it represents the opposite stance. The issue of claim specificity in argumentation has been much less addressed. Existing work, however, suggests that a high degree of specificity is correlated with argument quality and persuasiveness (Carlile et al., 2018; Swanson et al., 2015). In terms of argument quality though, it is entirely possible for the presented claims to be coherent and meaningful, yet be too specific within the given discourse, and therefore be logically irrelevant (Dessalles, 2016). As a concrete example, suppose we wanted to assert a claim in support of the argument T HESIS of Figure 1. While The Unforgivable Curses are illegal...and their use is grounds for immediate life imprisonment supports the T HESIS, it is too specific a claim to introduce at this point in the argument. Namely, it doesn’t flow naturally without first intr"
P19-1456,D14-1179,0,0.0520596,"Missing"
P19-1456,N18-1094,1,0.870034,"using on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2017; Faulkner, 2014). Although there has been some work on constructing argument structure datasets using news sources (Reed et al., 2008), microtexts (Peldszus, 2014) and user comments (Park and Cardie, 2018), these structures tend to be shallower and include fewer opposing"
P19-1456,P11-1099,0,0.0273742,"he stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostl"
P19-1456,W17-5006,0,0.0256066,"hly predictive of specificity and more specific claims are longer than more generic claims. Across all settings, the fine-tuned BERT model achieves the best performance. As expected, the performance degrades, for all models, as we control for distance and stance, since the claims get more similar in language, for both cases. Table 4 shows the top weighted words by BOW model for each class. We find that connectives (such as also, but, because, when) are associated more with arguments with higher specificity as they are mostly used to add more specific information to the claims as also found by Lugini and Litman (2017), whereas concept words (such as society, world, gender) have higher association with more generic arguments since these words represents the concepts of the controversial topics that people argue about. We further evaluate our models for the claim pairs with distance values 2 to 5 as shown in Table 3. We find that BERT model is consistently the best performing model for all distance pairs. As we increase the distance, the models achieve higher prediction performance despite having less training examples for higher distance values. 4 Claim Stance Detection It is not sufficient for debate and a"
P19-1456,I13-1191,0,0.106804,"claim, which can enable an argument generation system to generate relevant claims, with a particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representat"
P19-1456,P18-1021,0,0.0225474,"generating a claim with a particular stance or the appropriate level of specificity within the context. Furthermore, these models are trained on social media conversations, which can be noisy, and as noted by Rakshit et al. (2017), many sentences either do not express an argument or cannot be understood out of context. In contrast, our dataset explicitly provides the sequence of claims in an argument path that leads to any particular claim, which can enable an argument generation system to generate relevant claims, with a particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of thes"
P19-1456,W00-1406,0,0.229804,"argument path 4636 9 We measure the significance performing t-test. Number of examples Fine-tuned BERT Fine-tuned BERT with path (simple) Fine-tuned BERT with path (hierarchical) d=1 21,451 74.84 76.77 77.46 d=2 19,940 60.69 65.10 67.74 d=3 14,947 58.34 59.12 62.51 d=4 9,394 55.88 55.80 59.51 Table 6: Accuracy for relative stance at distance 1-4. context as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notion of generating a claim with a particular stance or th"
P19-1456,W03-1601,0,0.0656394,"Number of examples Fine-tuned BERT Fine-tuned BERT with path (simple) Fine-tuned BERT with path (hierarchical) d=1 21,451 74.84 76.77 77.46 d=2 19,940 60.69 65.10 67.74 d=3 14,947 58.34 59.12 62.51 d=4 9,394 55.88 55.80 59.51 Table 6: Accuracy for relative stance at distance 1-4. context as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notion of generating a claim with a particular stance or the appropriate level of specificity within the context. Furthermore, thes"
P19-1456,W18-5215,0,0.0211198,"84 76.77 77.46 d=2 19,940 60.69 65.10 67.74 d=3 14,947 58.34 59.12 62.51 d=4 9,394 55.88 55.80 59.51 Table 6: Accuracy for relative stance at distance 1-4. context as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notion of generating a claim with a particular stance or the appropriate level of specificity within the context. Furthermore, these models are trained on social media conversations, which can be noisy, and as noted by Rakshit et al. (2017), many s"
P19-1456,S16-1003,0,0.0608202,"Missing"
P19-1456,W15-0503,0,0.012633,"be, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Nadere"
P19-1456,P17-1091,1,0.798318,"rected towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose"
P19-1456,W14-2105,1,0.904225,"ument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic doc"
P19-1456,L18-1257,1,0.851773,"iken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2017; Faulkner, 2014). Although there has been some work on constructing argument structure datasets using news sources (Reed et al., 2008), microtexts (Peldszus, 2014) and user comments (Park and Cardie, 2018), these structures tend to be shallower and include fewer opposing claims since they employ existing monologic texts that are rel4637 atively short. In contrast, the dataset we provide is constructed with the goal of providing supporting and opposing claims for each of the claim presented in an argument tree. Therefore, these argument tree structures are deeper and have more balanced number of supporting and opposing claims. 6 Conclusion We present a new dataset of manually curated argument trees, which can open interesting avenues of research in argumentation. We use this dataset to study met"
P19-1456,W14-2112,0,0.0219552,"an, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2017; Faulkner, 2014). Although there has been some work on constructing argument structure datasets using news sources (Reed et al., 2008), microtexts (Peldszus, 2014) and user comments (Park and Cardie, 2018), these structures tend to be shallower and include fewer opposing claims since they employ existing monologic texts that are rel4637 atively short. In contrast, the dataset we provide is constructed with the goal of providing supporting and opposing claims for each of the claim presented in an argument tree. Therefore, these argument tree structures are deeper and have more balanced number of supporting and opposing claims. 6 Conclusion We present a new dataset of manually curated argument trees, which can open interesting avenues of research in argum"
P19-1456,D15-1110,0,0.243665,"simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more s"
P19-1456,reed-etal-2008-language,0,0.0400664,"cs of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2017; Faulkner, 2014). Although there has been some work on constructing argument structure datasets using news sources (Reed et al., 2008), microtexts (Peldszus, 2014) and user comments (Park and Cardie, 2018), these structures tend to be shallower and include fewer opposing claims since they employ existing monologic texts that are rel4637 atively short. In contrast, the dataset we provide is constructed with the goal of providing supporting and opposing claims for each of the claim presented in an argument tree. Therefore, these argument tree structures are deeper and have more balanced number of supporting and opposing claims. 6 Conclusion We present a new dataset of manually curated argument trees, which can open interesting"
P19-1456,W15-4625,0,0.0248314,"sis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2"
P19-1456,P15-4019,0,0.020887,"(simple) Fine-tuned BERT with path (hierarchical) d=1 21,451 74.84 76.77 77.46 d=2 19,940 60.69 65.10 67.74 d=3 14,947 58.34 59.12 62.51 d=4 9,394 55.88 55.80 59.51 Table 6: Accuracy for relative stance at distance 1-4. context as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notion of generating a claim with a particular stance or the appropriate level of specificity within the context. Furthermore, these models are trained on social media conversations, whi"
P19-1456,P09-1026,0,0.152463,"of claims in an argument path that leads to any particular claim, which can enable an argument generation system to generate relevant claims, with a particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we fi"
P19-1456,W10-0214,0,0.0269016,"d to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen a"
P19-1456,W14-2715,0,0.0267589,"ble an argument generation system to generate relevant claims, with a particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument pa"
P19-1456,C14-1142,0,0.0272263,"ask (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al.,"
P19-1456,C18-1203,0,0.0131468,"y rebutting it’s parent, which presents a supporting point for the claim. Results and Analysis We experiment with a feature-based Logistic Regression model and a fine-tuned BERT model (Devlin et al., 2018) using the same strategy to split the data into train, development and test sets as in Section 3.1. Baseline. Our feature-based model employs features shown to be effective in stance detection tasks (Mohammad et al., 2016) such as bag of words, word match, sentiment match, document embedding similarity, and MPQA subjectivity features (Wilson et al., 2005)8 . We cannot evaluate the model from Sun et al. (2018) as a baseline, as that requires additional annotations for argument phrases for the given topics. Similarly, we cannot evaluate the model from Bar-Haim et al. (2017) as a baseline, since it would require additional annotations for target phrases in each claim, polarity towards the target phrases, and consistent/contrastive labels between the target phrases of two claims. Fine-tuned BERT. We feed a pair of claims into a pre-trained BERT model, in the same manner as detailed above for relative specificity detection, and take the output of the [CLS] token from final layer and feed it into a clas"
P19-1456,W15-4631,0,0.0169816,"ect to the argument thesis. Debate and argument generation systems, in general, should also be able to determine whether two claims that address the same line of reasoning represent the same, or the opposing stance: using Defense is also made easier through magic to refute the H ARM claim in Figure 1, for example, requires recognizing that it represents the opposite stance. The issue of claim specificity in argumentation has been much less addressed. Existing work, however, suggests that a high degree of specificity is correlated with argument quality and persuasiveness (Carlile et al., 2018; Swanson et al., 2015). In terms of argument quality though, it is entirely possible for the presented claims to be coherent and meaningful, yet be too specific within the given discourse, and therefore be logically irrelevant (Dessalles, 2016). As a concrete example, suppose we wanted to assert a claim in support of the argument T HESIS of Figure 1. While The Unforgivable Curses are illegal...and their use is grounds for immediate life imprisonment supports the T HESIS, it is too specific a claim to introduce at this point in the argument. Namely, it doesn’t flow naturally without first introducing the concept of"
P19-1456,P14-1017,0,0.0195237,"evych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2017; Faulkner, 2014). Although there has been some work on constructing argument structure datasets using news sources (Reed et al., 2008), microtexts (Peldszus, 2014) and user comments (Park and Cardie, 2018), these structures tend to be shallower and include fewer opposing claims since they employ existing monologic texts that are rel4637 atively sh"
P19-1456,W06-1639,0,0.0941379,"tion system to generate relevant claims, with a particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context,"
P19-1456,E17-1017,0,0.0232338,"on of the argument path, i.e. the context, significantly outperform these baselines. Argument Structure and Quality. There has been tremendous amount of work in computational argumentation mining focusing on determining argumentative components (Mochales and Moens, 2011; Stab and Gurevych, 2014; Nguyen and Litman, 2015) and argument structure in text (Palau and Moens, 2009; Biran and Rambow, 2011; Feng and Hirst, 2011; Lippi and Torroni, 2015; Park and Cardie, 2014; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), and understanding the argument quality dimensions (Wachsmuth et al., 2017; Carlile et al., 2018) and the characteristics of persuasive arguments (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Durmus and Cardie, 2018; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart and Naderer, 2016; Durmus and Cardie, 2019). Existing work on claim specificity and stance detection has mostly employed datasets extracted from monologic documents that include more shallow support/oppose structures (Bar-Haim et al., 2017; Faulkner, 2014). Although there has been some work on constructing arg"
P19-1456,N12-1072,0,0.125124,"ext as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notion of generating a claim with a particular stance or the appropriate level of specificity within the context. Furthermore, these models are trained on social media conversations, which can be noisy, and as noted by Rakshit et al. (2017), many sentences either do not express an argument or cannot be understood out of context. In contrast, our dataset explicitly provides the sequence of claims in an argumen"
P19-1456,walker-etal-2012-corpus,0,0.15068,"ext as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notion of generating a claim with a particular stance or the appropriate level of specificity within the context. Furthermore, these models are trained on social media conversations, which can be noisy, and as noted by Rakshit et al. (2017), many sentences either do not express an argument or cannot be understood out of context. In contrast, our dataset explicitly provides the sequence of claims in an argumen"
P19-1456,N16-1007,0,0.02823,"etter, which further justifies our choice to treat the argument path 4636 9 We measure the significance performing t-test. Number of examples Fine-tuned BERT Fine-tuned BERT with path (simple) Fine-tuned BERT with path (hierarchical) d=1 21,451 74.84 76.77 77.46 d=2 19,940 60.69 65.10 67.74 d=3 14,947 58.34 59.12 62.51 d=4 9,394 55.88 55.80 59.51 Table 6: Accuracy for relative stance at distance 1-4. context as a hierarchical rather than a flat representation. 5 Related Work Argumentation Generation. Previous work in argument generation has focused on generating summaries of opinionated text (Wang and Ling, 2016), rebuttals for a given argument (Jitnah et al., 2000), paraphrases from predicate/argument structure (Kozlowski et al., 2003), generation via sentence retrieval (Sato et al., 2015) and developing argumentative dialogue agents (Le et al., 2018; Rakshit et al., 2017). The work on developing argumentative dialogue agents, in particular, has employed mostly social media data such as IAC (Walker et al., 2012c) to design retrievalbased or generative models to make argumentative responses to the users. These models, however, employ very limited context in generating the claims, and there is no notio"
P19-1456,H05-1044,0,0.492582,"not applicable. We would need human annotation for these cases to be able to make any claims for the relative specificity. path information between a pair of claims, as this would be equivalent to giving the gold label as input to the model, since given the path, the relative specificity is deterministic. 3.1 Results and Analysis Baseline. We experiment with feature-based Logistic Regression (LR) model that incorporates all the features that are shown to be effective in determining sentence specificity (Louis and Nenkova, 2012). For example, this feature list includes polarity of the claims (Wilson et al., 2005), number of personal pronouns in the claims, and length of the claims since (Louis and Nenkova, 2012) shows that generic sentences have stronger polarity, less number of personal pronouns and are shorter in length. While Ko et al. (2019) has also looked at the task of specificity prediction, we cannot directly apply their models to our data, since their annotation scheme requires each sentence to be labelled as general or specific, whereas we argue that specificity is relative. Fine-tuned BERT. We compare our baselines with a fine-tuned BERT model (Devlin et al., 2018). BERT is a pre-trained d"
P19-1456,D10-1102,1,0.73022,"te relevant claims, with a particular stance and at the right level of specificity. Recent work by Hua and Wang (2018) studies the task of generating claims of a different stance for a given statement, however their context is limited to the given statement and they do not take specificity into account. Stance Detection. Previous work on claim stance detection has studied the important linguistic features to determine the stance of a claim relative to a thesis/main claim (Somasundaran and Wiebe, 2009, 2010; Walker et al., 2012a,b; Hasan and Ng, 2013; Sridhar et al., 2014; Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011; Kwon et al., 2007; Faulkner, 2014; BarHaim et al., 2017). Some of these studies have shown that simple linear classifiers with uni-gram and n-gram features are effective for this task (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). However, in our setting, since we try to predict the stance between all pairs of claims on an argument path, rather than simply claims that are directed towards the thesis or the parent claim, we find that the models with a hierarchical representation of the argument path, i.e. the context, significantly outperform"
P92-1028,P91-1034,0,0.0855644,"Missing"
P92-1028,C88-1026,0,0.0128533,"usually include the algorithms used to find relative pronoun antecedents, current high-coverage parsers seem to employ one of 3 approaches for relative pronoun disambiguation. Systems that use a formal syntactic grammar often directly encode information for relative pronoun disambiguation in the grammar. Alternatively, a syntactic filter is applied to the parse tree and any noun phrases for which coreference with the relative pronoun is syntactically legal (or, in some cases, illegal) are passed to a semantic component which determines the antecedent using inference or preference rules (see (Correa, 1988), (Hobbs, 1986), (Ingria, & Stallard, 1989), (Lappin, & McCord, 1990)). The third approach employs handcoded disambiguation heuristics that rely mainly on Figure 1. E x a m p l e s o f R e l a t i v e Pronoun Antecedents &quot;our sponsors&quot; or its appositive &quot;GE and NSF&quot; is a semantically valid antecedent. Because pp-attachment and interpretation of conjunctions and appositives remain difficult for current systems, it is often unreasonable to expect reliable parser output for clauses containing those constructs. Moreover, the parser must access both syntactic and semantic knowledge in finding the a"
P92-1028,P90-1031,0,0.025042,"Missing"
P92-1028,P90-1033,0,0.0680363,"Missing"
P92-1028,P90-1034,0,0.0387369,"ribed an automated approach for the acquisition of relative pronoun disambiguation heuristics that duplicates the performance of handceded rules. Unfortunately, extending the technique for use with unrestricted texts may be difficult. The UMass/MUC-3 parser would clearly need additional mechanisms to handle the ensuing part of speech and 7Other parsing errors occurred throughout the training set, but only those instances where the antecedent was not recognized as a constituent (and the wh-word had an anteceden0 were discarded. 8Interestingly, in work on the automated classification of nouns, (Hindle, 1990) also noted problems with &quot;empty&quot; words that depend on their complements for meaning. 221 automated acquisition of disambiguation rules for other problems in natural language processing. word sense disambiguation problems. However, recent research in these areas indicates that automated approaches for these tasks may be feasible (see, for example, (Brown, Della Pietra, Della Pietra, & Mercer, 1991) and (l-Iindle, 1983)). In addition, although our simple semantic feature set seems adequate for the current relative pronoun disambiguntion task, it is doubtful that a single semantic feature set ca"
P92-1028,M91-1001,0,0.0284877,"Missing"
P92-1028,P91-1030,0,0.0352338,"Missing"
P92-1028,P90-1004,0,0.0129827,"uation rules, is difficult because successful heuristics demand the assimilation of complex syntactic and semantic knowledge. Consider, for example, the problem of prepositional phrase attachment. A number of purely structural solutions have been proposed including the theories of Minimal Attachment (Frazier, 1978) and Right Association (Kimball, 1973). While these models may suggest the existence of strong syntactic preferences in effect during sentence understanding, other studies provide clear evidence that purely syntactic heuristics for prepositional phrase attachment will not work (see (Whittemore, Ferrara, & Brunner, 1990), (Taraban, & McClelland, 1988)). However, computational linguists have found the manual encoding of disarnbiguation rules - especially those that merge syntactic and semantic constraints - - to be difficult, time-consuming, and prone to error. In addition, hand-coded heuristics are often incomplete and perform poorly in new domains comprised of specialized vocabularies or a different genre of text. In the next section, we briefly describe the task of relative pronoun disambiguation. Sections 3 and 4 give the details of the acquisition algorithm and evaluate its performance. Problems with the"
P92-1028,P89-1032,0,0.0275901,"Missing"
P92-1028,P90-1018,0,0.0364872,"Missing"
P92-1028,J93-1005,0,\N,Missing
P92-1028,M91-1033,1,\N,Missing
P92-1028,H91-1067,0,\N,Missing
P92-1028,H91-1059,0,\N,Missing
P92-1028,P91-1027,0,\N,Missing
P98-1034,C92-3150,0,0.0309712,"Missing"
P98-1034,A88-1019,0,0.443278,"Missing"
P98-1034,J93-2004,0,0.0608977,"mponent of any partial parser; in addition, information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms; furthermore, the psycholinguistic studies of Gee and Grosjean (1983) indicate that text chunks like base noun phrases play an important role in human language processing. In this work we define base NPs to be simple, nonrecursive noun phrases - - noun phrases that do not contain other noun phrase descendants. The bracketed portions of Figure 1, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Thus, the string the sunny confines of resort towns like Boca Raton and Hot Springs is too complex to be a base NP; instead, it contains four simpler noun phrases, each of which is considered a base NP: the sunny confines, resort towns, Boca Raton, and Hot Springs. Previous empirical research has addressed the problem of base NP identification. Several algorithms identify &quot;terminological phrases&quot; - - certain Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: Justeson & Katz (1995) look for repeated phrases; Bourigault (1992) uses a handcrafted noun p"
P98-1034,W93-0306,0,0.479502,"Missing"
P98-1034,J95-4004,0,\N,Missing
Q14-1039,D08-1083,1,0.928583,"opinion retrieval. In this paper, we focus on the problem of identifying opinion expressions and classifying their attributes. We consider as an opinion expression Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling approach to extract op"
Q14-1039,P10-2050,1,0.349846,"nary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling approach to extract opinion expressions and label them with polarity and intensity. Their approach treats both expression extraction and attribute classification as token-level se505 Transactions of the Association for Computational Linguistics, vol. 2, pp. 505–516, 2014. Action Editor: Janyce Wiebe. c Submitted 4/2014; Revised 8/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. He demonstrated a bias in favor of medium the rebels despite being severely criticized high . Figure 1: An example sentence annotated with opini"
Q14-1039,W06-1651,1,0.908166,"d attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goal is to identify the spans of opinion"
Q14-1039,P10-1074,0,0.0302981,"individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goa"
Q14-1039,W06-1673,0,0.0384386,"essions, and the word sequence and the POS sequence between the adjacent pairs of extracted opinion expressions. Table 7 shows the reranking performance (F1) for all subtasks. We can see that after reranking, JILOSS still provides the best performance and HJSL achieves comparable performance to P IPELINE. We also found that reranking leads to less performance gain for the joint inference approaches than for the joint learning approaches. This is because the k-best output of JI- PROB and JI- LOSS present less diversity than JSL and HJSL. A similar issue for reranking has also been discussed in Finkel et al. (2006). 5.2.2 Evaluation on Sentence-level Tasks As an additional experiment, we consider a supervised sentence-level sentiment classification task using features derived from the prediction output of different opinion extraction models. As a stanExample Sentences The expression is undoubtedly strong and well JointLearn JointInfer X thought out high . But the Sadc Ministerial Task Force said the election was free and fair medium . The president branded high as the “axis of evil” high in his statement ... well thought out high × X No opinions × of evil medium X × Table 6: Examples of mistakes that ar"
Q14-1039,N10-1112,0,0.0747351,"Missing"
Q14-1039,P11-2018,0,0.649329,"Revised 8/2014; Published November 1, 2014. 2014 Association for Computational Linguistics. He demonstrated a bias in favor of medium the rebels despite being severely criticized high . Figure 1: An example sentence annotated with opinion expressions and their polarity and intensity. We use colored boxes to mark the textual spans of opinion expressions where green (red) denotes positive (negative) polarity, and use subscripts to denote intensity. quence labeling tasks, and thus cannot model the label distribution over expressions even though the annotations are given at the expression level. Johansson and Moschitti (2011) considered a pipeline of opinion extraction followed by polarity classification and propose re-ranking its k-best outputs using global features. One key issue, however, is that the approach enumerates the k-best output in a pipeline manner and thus they do not necessarily correspond to the k-best global decisions. Moreover, as the number of opinion attributes grows, it is not clear how to identify the best k for each attribute. In contrast to existing approaches, we formulate opinion expression extraction as a segmentation problem and attribute classification as segmentlevel attribute labelin"
Q14-1039,J13-3002,0,0.513101,"so utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their interactions at the expression-level. Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features. One major issue with their approach is that the k-best candidates were obtained without global reasoning about the relative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the seg"
Q14-1039,C04-1197,0,0.0772704,"ative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically,"
Q14-1039,W04-2401,0,0.0833456,"soning about the relative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute clas"
Q14-1039,D10-1001,0,0.0332297,"number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goal is to identify th"
Q14-1039,D13-1170,0,0.00550977,"learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 506 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. However, in real-world settings, the spans of opinion expressions within the sentence are not available. In fact, Choi and Cardie (2008) demonstrated that the performance of expression-level polarity classification degrades as more surrounding (but irrelevant) context is considered. This motivates the additiona"
Q14-1039,H05-1044,0,0.758226,"estion answering and opinion retrieval. In this paper, we focus on the problem of identifying opinion expressions and classifying their attributes. We consider as an opinion expression Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling"
Q14-1039,J09-3003,0,0.0177373,"ne and observed that joint learning results in a significant boost in precision while joint inference, with an appropriate objective, can significantly boost both precision and recall and obtain the best overall performance. Error analysis provides additional understanding of the differences between the joint learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 506 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. H"
Q14-1039,D12-1122,1,0.847471,"n is crucial in supporting many opinion-mining applications such as opinion summarization, opinion-oriented question answering and opinion retrieval. In this paper, we focus on the problem of identifying opinion expressions and classifying their attributes. We consider as an opinion expression Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expressi"
Q14-1039,P13-1161,1,0.779206,"it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goal is to identify the spans of opinion expressions, and simulta"
Q14-1039,D11-1016,1,0.904678,"his paper, we focus on the problem of identifying opinion expressions and classifying their attributes. We consider as an opinion expression Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling approach to extract opinion expressions and label the"
Q14-1039,D08-1013,0,0.0188801,"d conditional random fields to assign each token a label indicating whether it belongs to an opinion expression or not. Yang and Cardie (2012) employed a segment-level sequence labeler based on semi-CRFs with rich phrase-level syntactic features. In this work, we also utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their interactions at the expression-level. Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features. One major issue with their approach is that the k-best candidates were obtained without global reasoning about the relative uncertainty in the individual stages. As the number of c"
Q15-1037,W06-0901,0,0.45642,"dropped within just a few moments - {two} {inside the camp itself }, while {the other two} {near the airstrip}. Document 1 The {Yida refugee camp} {in South Sudan} was bombed {on Thursday}. {At least four bombs} were reportedly dropped. {Two bombs} fell {within the Yida camp}, including {one} {close to the school}. Document 2 Figure 1: Examples of event coreference. Mutually coreferent event mentions are underlined and in boldface; participant and spatio-temporal information for the highlighted event is marked by curly brackets. Most previous approaches to event coreference resolution (e.g., Ahn (2006), Chen et al. (2009)) operated by extending the supervised pairwise classi517 Transactions of the Association for Computational Linguistics, vol. 3, pp. 517–528, 2015. Action Editor: Hwee Tou Ng. Submission batch: 4/2015; Revision batch: 7/2015; Published 9/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. fication model that is widely used in entity coreference resolution (e.g., Ng and Cardie (2002)). In this framework, pairwise distances between event mentions are modeled via event-related features (e.g., that indicate event argument compatibility"
Q15-1037,P10-1143,0,0.152903,"ch and Information Engineering Cornell University pf98@cornell.edu In comparison to entity coreference resolution (Ng, 2010), which deals with identifying and grouping noun phrases that refer to the same discourse entity, event coreference resolution has not been extensively studied. This is, in part, because events typically exhibit a more complex structure than entities: a single event can be described via multiple event mentions, and a single event mention can be associated with multiple event arguments that characterize the participants in the event as well as spatio-temporal information (Bejan and Harabagiu, 2010). Hence, the coreference decisions for event mentions usually require the interpretation of event mentions and their arguments in context. See, for example, Figure 1, in which five event mentions across two documents all refer to the same underlying event: Plane bombs Yida camp. We present a novel hierarchical distancedependent Bayesian model for event coreference resolution. While existing generative models for event coreference resolution are completely unsupervised, our model allows for the incorporation of pairwise distances between event mentions — information that is widely used in super"
Q15-1037,J14-2004,0,0.303208,"Missing"
Q15-1037,W99-0611,1,0.495435,"ing of event coreference relations with unsupervised hierarchical modeling of event clustering achieves promising improvements over state-of-theart approaches for within- and cross-document event coreference resolution. 2 Related Work Coreference resolution in general is a difficult natural language processing (NLP) task and typically requires sophisticated inferentially-based knowledgeintensive models (Kehler, 2002). Extensive work in the literature focuses on the problem of entity coreference resolution and many techniques have been developed, including rule-based deterministic models (e.g. Cardie and Wagstaff (1999), Raghunathan et al. (2010), Lee et al. (2011)) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. Stoyanov et al. (2009), Rahman and Ng (2011), Durrett and Klein (2013)) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. Bhattacharya and Getoor (2006), Haghighi and Klein (2007, 2010)) that successfully apply generative modeling to the coreference resolution problem"
Q15-1037,W09-4303,0,0.873689,"hin just a few moments - {two} {inside the camp itself }, while {the other two} {near the airstrip}. Document 1 The {Yida refugee camp} {in South Sudan} was bombed {on Thursday}. {At least four bombs} were reportedly dropped. {Two bombs} fell {within the Yida camp}, including {one} {close to the school}. Document 2 Figure 1: Examples of event coreference. Mutually coreferent event mentions are underlined and in boldface; participant and spatio-temporal information for the highlighted event is marked by curly brackets. Most previous approaches to event coreference resolution (e.g., Ahn (2006), Chen et al. (2009)) operated by extending the supervised pairwise classi517 Transactions of the Association for Computational Linguistics, vol. 3, pp. 517–528, 2015. Action Editor: Hwee Tou Ng. Submission batch: 4/2015; Revision batch: 7/2015; Published 9/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. fication model that is widely used in entity coreference resolution (e.g., Ng and Cardie (2002)). In this framework, pairwise distances between event mentions are modeled via event-related features (e.g., that indicate event argument compatibility), and agglomerative"
Q15-1037,cybulska-vossen-2014-using,0,0.608516,"introduced to incorporate data dependencies into nonparametric clustering models. Here, however, we extend the DDCRP to allow the incorporation of feature-based, learnable distance functions as clustering priors, thus encouraging event mentions that are close in meaning to belong to the same cluster. In addition, we introduce to the DDCRP a representational hierarchy that allows event mentions to be grouped within a document and within-document event clusters to be grouped across documents. To investigate the effectiveness of our approach, we conduct extensive experiments on the ECB+ corpus (Cybulska and Vossen, 2014b), an extension to EventCorefBank (ECB) (Bejan and Harabagiu, 2010) and the largest corpus available that contains event coreference annotations within and across 518 documents. We show that integrating pairwise learning of event coreference relations with unsupervised hierarchical modeling of event clustering achieves promising improvements over state-of-theart approaches for within- and cross-document event coreference resolution. 2 Related Work Coreference resolution in general is a difficult natural language processing (NLP) task and typically requires sophisticated inferentially-based kn"
Q15-1037,D13-1203,0,0.0878315,"processing (NLP) task and typically requires sophisticated inferentially-based knowledgeintensive models (Kehler, 2002). Extensive work in the literature focuses on the problem of entity coreference resolution and many techniques have been developed, including rule-based deterministic models (e.g. Cardie and Wagstaff (1999), Raghunathan et al. (2010), Lee et al. (2011)) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. Stoyanov et al. (2009), Rahman and Ng (2011), Durrett and Klein (2013)) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. Bhattacharya and Getoor (2006), Haghighi and Klein (2007, 2010)) that successfully apply generative modeling to the coreference resolution problem. Event coreference resolution is a more complex task than entity coreference resolution (Humphreys et al., 1997) and also has been relatively less studied. Existing work has adapted similar ideas to those used in entity coreference. Humphreys et al. (1997) first proposed a deterministic clustering"
Q15-1037,P07-1107,0,0.275143,"nd many techniques have been developed, including rule-based deterministic models (e.g. Cardie and Wagstaff (1999), Raghunathan et al. (2010), Lee et al. (2011)) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. Stoyanov et al. (2009), Rahman and Ng (2011), Durrett and Klein (2013)) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. Bhattacharya and Getoor (2006), Haghighi and Klein (2007, 2010)) that successfully apply generative modeling to the coreference resolution problem. Event coreference resolution is a more complex task than entity coreference resolution (Humphreys et al., 1997) and also has been relatively less studied. Existing work has adapted similar ideas to those used in entity coreference. Humphreys et al. (1997) first proposed a deterministic clustering mechanism to group event mentions of prespecified types based on hard constraints. Later approaches (Ahn, 2006; Chen et al., 2009) applied learning-based pairwise classification decisions using event-specific f"
Q15-1037,N10-1061,0,0.079299,"Missing"
Q15-1037,W97-1311,0,0.797538,"text snippets that describe events, and then clustering them such that all event mentions in the same partition refer to the same unique event. Event coreference resolution can be applied within a single document or across multiple documents and is crucial for many natural language processing tasks including topic detection and tracking, information extraction, question answering and textual entailment (Bejan and Harabagiu, 2010). More importantly, event coreference resolution is a necessary component in any reasonable, broadly applicable computational model of natural language understanding (Humphreys et al., 1997). The {Yida refugee camp} was the target of an air strike {in South Sudan} {on Thursday}. {Four bombs} were dropped within just a few moments - {two} {inside the camp itself }, while {the other two} {near the airstrip}. Document 1 The {Yida refugee camp} {in South Sudan} was bombed {on Thursday}. {At least four bombs} were reportedly dropped. {Two bombs} fell {within the Yida camp}, including {one} {close to the school}. Document 2 Figure 1: Examples of event coreference. Mutually coreferent event mentions are underlined and in boldface; participant and spatio-temporal information for the high"
Q15-1037,W11-1902,0,0.0719617,"Missing"
Q15-1037,D12-1045,0,0.670416,", 2006; Chen et al., 2009) applied learning-based pairwise classification decisions using event-specific features to infer event clustering. Bejan and Harabagiu (2010; 2014) proposed several unsupervised generative models for event mention clustering based on the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Our approach is related to both supervised clustering and generative clustering approaches. It is a nonparametric Bayesian model in nature but encodes rich linguistic features in clustering priors. More recent work modeled both entity and event information in event coreference. Lee et al. (2012) showed that iteratively merging entity and event clusters can boost the clustering performance. Liu et al. (2014) demonstrated the benefits of propagating information between event arguments and event mentions during a post-processing step. Other work modeled event coreference as a predicate argument alignment problem between pairs of sentences, and trained classifiers for making alignment decisions (Roth and Frank, 2012; Wolfe et al., 2015). Our model also leverages event argument information into the decisions of event coreference but incorporates it into Bayesian clustering priors. Most ex"
Q15-1037,liu-etal-2014-supervised,0,0.384976,"to infer event clustering. Bejan and Harabagiu (2010; 2014) proposed several unsupervised generative models for event mention clustering based on the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Our approach is related to both supervised clustering and generative clustering approaches. It is a nonparametric Bayesian model in nature but encodes rich linguistic features in clustering priors. More recent work modeled both entity and event information in event coreference. Lee et al. (2012) showed that iteratively merging entity and event clusters can boost the clustering performance. Liu et al. (2014) demonstrated the benefits of propagating information between event arguments and event mentions during a post-processing step. Other work modeled event coreference as a predicate argument alignment problem between pairs of sentences, and trained classifiers for making alignment decisions (Roth and Frank, 2012; Wolfe et al., 2015). Our model also leverages event argument information into the decisions of event coreference but incorporates it into Bayesian clustering priors. Most existing coreference models, both for events and entities, focus on solving the within-document coreference problem."
Q15-1037,H05-1004,0,0.834508,"Missing"
Q15-1037,P02-1014,1,0.628551,"articipant and spatio-temporal information for the highlighted event is marked by curly brackets. Most previous approaches to event coreference resolution (e.g., Ahn (2006), Chen et al. (2009)) operated by extending the supervised pairwise classi517 Transactions of the Association for Computational Linguistics, vol. 3, pp. 517–528, 2015. Action Editor: Hwee Tou Ng. Submission batch: 4/2015; Revision batch: 7/2015; Published 9/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. fication model that is widely used in entity coreference resolution (e.g., Ng and Cardie (2002)). In this framework, pairwise distances between event mentions are modeled via event-related features (e.g., that indicate event argument compatibility), and agglomerative clustering is applied to greedily merge event mentions into clusters. A major drawback of this general approach is that it makes hard decisions on the merging and splitting of clusters based on heuristics derived from the pairwise distances. In addition, it only captures pairwise coreference decisions within a single document and can not account for signals that commonly appear across documents. More recently, Bejan and Har"
Q15-1037,P10-1142,0,0.216136,"Missing"
Q15-1037,P14-2006,0,0.349385,"Missing"
Q15-1037,D10-1048,0,0.127811,"ations with unsupervised hierarchical modeling of event clustering achieves promising improvements over state-of-theart approaches for within- and cross-document event coreference resolution. 2 Related Work Coreference resolution in general is a difficult natural language processing (NLP) task and typically requires sophisticated inferentially-based knowledgeintensive models (Kehler, 2002). Extensive work in the literature focuses on the problem of entity coreference resolution and many techniques have been developed, including rule-based deterministic models (e.g. Cardie and Wagstaff (1999), Raghunathan et al. (2010), Lee et al. (2011)) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. Stoyanov et al. (2009), Rahman and Ng (2011), Durrett and Klein (2013)) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. Bhattacharya and Getoor (2006), Haghighi and Klein (2007, 2010)) that successfully apply generative modeling to the coreference resolution problem. Event coreference resolut"
Q15-1037,P11-1082,0,0.0201956,"cult natural language processing (NLP) task and typically requires sophisticated inferentially-based knowledgeintensive models (Kehler, 2002). Extensive work in the literature focuses on the problem of entity coreference resolution and many techniques have been developed, including rule-based deterministic models (e.g. Cardie and Wagstaff (1999), Raghunathan et al. (2010), Lee et al. (2011)) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. Stoyanov et al. (2009), Rahman and Ng (2011), Durrett and Klein (2013)) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. Bhattacharya and Getoor (2006), Haghighi and Klein (2007, 2010)) that successfully apply generative modeling to the coreference resolution problem. Event coreference resolution is a more complex task than entity coreference resolution (Humphreys et al., 1997) and also has been relatively less studied. Existing work has adapted similar ideas to those used in entity coreference. Humphreys et al. (1997) first proposed"
Q15-1037,S12-1030,0,0.0257331,"onparametric Bayesian model in nature but encodes rich linguistic features in clustering priors. More recent work modeled both entity and event information in event coreference. Lee et al. (2012) showed that iteratively merging entity and event clusters can boost the clustering performance. Liu et al. (2014) demonstrated the benefits of propagating information between event arguments and event mentions during a post-processing step. Other work modeled event coreference as a predicate argument alignment problem between pairs of sentences, and trained classifiers for making alignment decisions (Roth and Frank, 2012; Wolfe et al., 2015). Our model also leverages event argument information into the decisions of event coreference but incorporates it into Bayesian clustering priors. Most existing coreference models, both for events and entities, focus on solving the within-document coreference problem. Cross-document coreference has attracted less attention due to lack of annotated corpora and the requirement for larger model capacity. Hierarchical models (Singh et al., 2010; Wick et al., 2012; Haghighi and Klein, 2007) have been popular choices for cross-document coreference as they can capture coreference"
Q15-1037,P09-1074,1,0.79612,"on in general is a difficult natural language processing (NLP) task and typically requires sophisticated inferentially-based knowledgeintensive models (Kehler, 2002). Extensive work in the literature focuses on the problem of entity coreference resolution and many techniques have been developed, including rule-based deterministic models (e.g. Cardie and Wagstaff (1999), Raghunathan et al. (2010), Lee et al. (2011)) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. Stoyanov et al. (2009), Rahman and Ng (2011), Durrett and Klein (2013)) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. Bhattacharya and Getoor (2006), Haghighi and Klein (2007, 2010)) that successfully apply generative modeling to the coreference resolution problem. Event coreference resolution is a more complex task than entity coreference resolution (Humphreys et al., 1997) and also has been relatively less studied. Existing work has adapted similar ideas to those used in entity coreference. Humphreys et al."
Q15-1037,M95-1005,0,0.960182,"Missing"
Q15-1037,P12-1040,0,0.0220477,"e argument alignment problem between pairs of sentences, and trained classifiers for making alignment decisions (Roth and Frank, 2012; Wolfe et al., 2015). Our model also leverages event argument information into the decisions of event coreference but incorporates it into Bayesian clustering priors. Most existing coreference models, both for events and entities, focus on solving the within-document coreference problem. Cross-document coreference has attracted less attention due to lack of annotated corpora and the requirement for larger model capacity. Hierarchical models (Singh et al., 2010; Wick et al., 2012; Haghighi and Klein, 2007) have been popular choices for cross-document coreference as they can capture coreference at multiple levels of granularities. Our model is also hierarchical, capturing both within- and cross-document coreference. Our model is also closely related to the distance-dependent Chinese Restaurant Process (DDCRP) (Blei and Frazier, 2011). The DDCRP is an infinite clustering model that can account for data dependencies (Ghosh et al., 2011; Socher et al., 2011). But it is a flat clustering model and thus cannot capture hierarchical structure that usually exists in large data"
Q15-1037,N15-1002,0,0.123483,"Missing"
Q15-1037,Q14-1039,1,0.890291,"Missing"
Q18-1039,C10-1004,0,0.0332881,"Missing"
Q18-1039,D08-1014,0,0.0446605,"d as the input representation for all systems to map words from both S OURCE and TARGET into the same feature space. (The only exceptions are the CLDbased CLTC systems of Xu and Yang [2017] explained later in this section, which directly make use of a parallel corpus instead of relying on BWEs.) The same BWEs are adopted in all systems that utilize BWEs. Machine Translation Baselines We then evaluate ADAN against MT baselines (rows 4−5) that (1) translate the TARGET text into English and then (2) use the better of the train-on-S OURCEonly models for sentiment classification. Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment classification for Arabic and European languages claim this MT approach to be very competitive and find that it can sometimes match the state-of-the-art system trained on that language. For Chinese, where translated text was not provided, we use the commercial Google Translate engine,6 which is highly engineered, trained on enormous resources, and arguably one of the best MT systems currently available. As shown in Table 1, our ADAN model substantially outperforms the MT baseline on both languages, indicating that our adversarial model can successfully perf"
Q18-1039,P07-1056,0,0.0508553,"2011) instead uses labeled data from both languages to improve the performance on both. Other papers make direct use of a parallel corpus either to learn a bilingual document representation (Zhou et al., 2016) or to conduct cross-lingual distillation (Xu and Yang, 2017). Zhou et al. (2016) require the translation of the entire English training set, which is prohibitive for our setting, and ADAN outperforms Xu and Yang (2017)’s approach in our experiments. Domain adaptation tries to learn effective classifiers for which the training and test samples are from different underlying distributions (Blitzer et al., 2007; Pan et al., 2011; Glorot et al., 2011; Chen et al., 2012; Liu et al., 2015). This can be thought of as a generalization of cross-lingual text classification. However, one main difference is that, when applied to text classification tasks, most of these domain adaptation work assumes a common feature space such as a bag-of-words representation, which is not available in the crosslingual setting. See Section 3.2 for experiments on this. In addition, most works in domain adaptation evaluate on adapting product reviews across domains (e.g., books to electronics), where the divergence in distribu"
Q18-1039,D14-1080,1,0.606034,"Missing"
Q18-1039,P09-1027,0,0.0395801,"Missing"
Q18-1039,P15-1162,0,0.0574103,"rce-rich source language to low-resource languages where only unlabeled data exist. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems. 1 Introduction Many state-of-the-art models for sentiment classification (Socher et al., 2013; Iyyer et al., 2015; Tai et al., 2015) are supervised learning approaches that rely on the availability of an adequate amount of labeled training data. For a few resource-rich languages, including English, such labeled data are indeed available. For the vast majority of languages, however, it is the norm that only a limited amount of annotated text exists. Worse still, many low-resource languages have no labeled data at all. 1 The source code of ADAN is available at https:// github.com/ccsasuke/adan. To aid the creation of sentiment classification systems in such low-resource languages, an active research direct"
Q18-1039,D14-1181,0,0.00154785,"ess resources and is much faster to train, potentially at the expense of quality. 3.3.4 Feature Extractor Architectures As mentioned in §2.1, the architecture of ADAN’s feature extractor is not limited to a Deep Averaging Network (DAN), and one can choose different feature extractors to suit a particular task or data set. While an extensive study of alternative architectures is beyond the scope of this work, in this section we present a brief experiment illustrating that our adversarial framework works well with other F architectures. In particular, we consider two popular choices: (i) a CNN (Kim, 2014) that has a 1d convolutional layer followed by a single fully-connected layer to extract a fixed-length vector; and (ii) a Bi-LSTM with two variants: one that takes the average of the hidden outputs of each token as the feature vector, and one with the dot attention mechanism (Luong et al., 2015) that learns a weighted linear combination of all hidden outputs. As shown in Table 3, ADAN’s performance can be improved by adopting more sophisticated feature extractors, at the expense of slower running Impact of Bilingual Word Embeddings In this section we discuss the effect of the bilingual word e"
Q18-1039,W10-4116,0,0.0210947,"f 0.0005. Q is trained with another Adam optimizer with the same learning rate. The weights of Q are clipped to [−0.01, 0.01]. We train ADAN for 30 epochs and use early stopping to select the best model on the validation set. ADAN is implemented in PyTorch (Paszke et al., 2017). Related Work Cross-lingual sentiment classification is motivated by the lack of high-quality labeled data in many non-English languages (Bel et al., 2003; Mihalcea et al., 2007; Banea et al., 2008, 2010; Soyer et al., 2015). For Chinese and Arabic in particular, there are several representative works (Wan, 2008, 2009; He et al., 2010; Lu et al., 2011; Mohammad et al., 2016). Our work is comparable to these in objective but very different in method. The work by Wan uses MT to directly convert English training data to Chinese; this is one of our baselines. Lu et al. (2011) instead uses labeled data from both languages to improve the performance on both. Other papers make direct use of a parallel corpus either to learn a bilingual document representation (Zhou et al., 2016) or to conduct cross-lingual distillation (Xu and Yang, 2017). Zhou et al. (2016) require the translation of the entire English training set, which is pro"
Q18-1039,Y15-2030,0,0.0659675,"a set, we map all the rating 4 and 5 English instances to + and the rating 1 and 2 instances to −, and the rating 3 sentences are converted to 0. Data Labeled English Data We use a balanced data set of 700K Yelp reviews from Zhang et al. (2015) with their ratings as labels (scale 1−5). We also adopt their train−validation split: 650K reviews for training and 50K form a validation set. Labeled Chinese Data Because ADAN does not require labeled Chinese data for training, these annotated data are solely used to validate the performance of our model. We use 10K balanced Chinese hotel reviews from Lin et al. (2015) as the validation set for model selection and parameter tuning. The results are reported on a separate test set of another 10K hotel reviews. For Chinese, the data are annotated with five labels (1−5). Unlabeled Arabic Data For Arabic, no additional unlabeled data are used. We only use the text from the validation set (without labels) during training. English−Arabic Bilingual Word Embeddings For Arabic, we train a 300d BilBOWA BWE (Gouws et al., 2015) on the United Nations corpus (Ziemski et al., 2016). Unlabeled Chinese Data For the unlabeled TARGET data used in training ADAN, we use another"
Q18-1039,N15-1078,0,0.0151733,"sentation for all systems to map words from both S OURCE and TARGET into the same feature space. (The only exceptions are the CLDbased CLTC systems of Xu and Yang [2017] explained later in this section, which directly make use of a parallel corpus instead of relying on BWEs.) The same BWEs are adopted in all systems that utilize BWEs. Machine Translation Baselines We then evaluate ADAN against MT baselines (rows 4−5) that (1) translate the TARGET text into English and then (2) use the better of the train-on-S OURCEonly models for sentiment classification. Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment classification for Arabic and European languages claim this MT approach to be very competitive and find that it can sometimes match the state-of-the-art system trained on that language. For Chinese, where translated text was not provided, we use the commercial Google Translate engine,6 which is highly engineered, trained on enormous resources, and arguably one of the best MT systems currently available. As shown in Table 1, our ADAN model substantially outperforms the MT baseline on both languages, indicating that our adversarial model can successfully perform cross-lingual senti"
Q18-1039,P11-1033,1,0.305476,"e research direction is cross-lingual sentiment classification (CLSC), in which the abundant resources of a source language (likely English, denoted as S OURCE) are leveraged to produce sentiment classifiers for a target language (TARGET). In general, CLSC methods make use of generalpurpose bilingual resources—such as hand-crafted bilingual lexica or parallel corpora—to alleviate or eliminate the need for task-specific TARGET annotations. In particular, the bilingual resource of choice for the majority of previous CLSC models is a full-fledged Machine Translation (MT) system (Wan, 2008, 2009; Lu et al., 2011; Zhou et al., 2016), a component that is expensive to obtain. In this work, we propose a language-adversarial training approach that does not need a highly engineered MT system, and requires orders of magnitude less in terms of the size of a parallel corpus. Specifically, we propose an Adversarial Deep Averaging Network (ADAN) that leverages a set of bilingual word embeddings (BWEs; Zou et al., 2013) trained on bitexts, in order to eliminate the need for labeled TARGET training data.2 We introduce the ADAN model in §2, and in §3 evaluate ADAN using English as the S OURCE with two TARGET choic"
Q18-1039,D15-1166,0,0.00707941,"Missing"
Q18-1039,D13-1170,0,0.019834,"beled data on a resource-rich source language to low-resource languages where only unlabeled data exist. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems. 1 Introduction Many state-of-the-art models for sentiment classification (Socher et al., 2013; Iyyer et al., 2015; Tai et al., 2015) are supervised learning approaches that rely on the availability of an adequate amount of labeled training data. For a few resource-rich languages, including English, such labeled data are indeed available. For the vast majority of languages, however, it is the norm that only a limited amount of annotated text exists. Worse still, many low-resource languages have no labeled data at all. 1 The source code of ADAN is available at https:// github.com/ccsasuke/adan. To aid the creation of sentiment classification systems in such low-resource languages, an ac"
Q18-1039,P14-5010,0,0.0182082,"Missing"
Q18-1039,P15-1150,0,0.0448146,"Missing"
Q18-1039,P07-1123,0,0.0138813,"5) is used in each hidden layer in P and Q. F does not use batch normalization. F and P are optimized jointly using Adam (Kingma and Ba, 2015) with a learning rate of 0.0005. Q is trained with another Adam optimizer with the same learning rate. The weights of Q are clipped to [−0.01, 0.01]. We train ADAN for 30 epochs and use early stopping to select the best model on the validation set. ADAN is implemented in PyTorch (Paszke et al., 2017). Related Work Cross-lingual sentiment classification is motivated by the lack of high-quality labeled data in many non-English languages (Bel et al., 2003; Mihalcea et al., 2007; Banea et al., 2008, 2010; Soyer et al., 2015). For Chinese and Arabic in particular, there are several representative works (Wan, 2008, 2009; He et al., 2010; Lu et al., 2011; Mohammad et al., 2016). Our work is comparable to these in objective but very different in method. The work by Wan uses MT to directly convert English training data to Chinese; this is one of our baselines. Lu et al. (2011) instead uses labeled data from both languages to improve the performance on both. Other papers make direct use of a parallel corpus either to learn a bilingual document representation (Zhou et al.,"
Q18-1039,P10-1040,0,0.0154647,"ication score.4 Network Architecture As illustrated in Figure 1, ADAN is a feed-forward network with two branches. There are three main components in the network: a joint feature extractor F that maps an input sequence x to the shared feature space, a sentiment classifier P that predicts the label for x given the feature representation F(x), and a language discriminator Q that also takes F(x) but predicts a scalar score indicating whether x is from S OURCE or TARGET. An input document is modeled as a sequence of words x = w1 , . . . , wn , where each w is represented by its word embedding vw (Turian et al., 2010). For improved performance, pretrained BWEs (Zou et al., 2013; Gouws et al., 2015) can be used to induce bilingual distributed word representations so that similar words are closer in the embedded space regardless of language. A parallel corpus is often required to train highquality BWEs, making ADAN implicitly dependent on the bilingual corpus. However, compared with the MT systems used in other CLSC methods, training BWEs only requires one to two orders of magnitude less parallel data, and some methods only take minutes to train on a consumer CPU (Gouws et al., 2015), whereas state-of-theart"
Q18-1039,D08-1058,0,0.0321841,"nguages, an active research direction is cross-lingual sentiment classification (CLSC), in which the abundant resources of a source language (likely English, denoted as S OURCE) are leveraged to produce sentiment classifiers for a target language (TARGET). In general, CLSC methods make use of generalpurpose bilingual resources—such as hand-crafted bilingual lexica or parallel corpora—to alleviate or eliminate the need for task-specific TARGET annotations. In particular, the bilingual resource of choice for the majority of previous CLSC models is a full-fledged Machine Translation (MT) system (Wan, 2008, 2009; Lu et al., 2011; Zhou et al., 2016), a component that is expensive to obtain. In this work, we propose a language-adversarial training approach that does not need a highly engineered MT system, and requires orders of magnitude less in terms of the size of a parallel corpus. Specifically, we propose an Adversarial Deep Averaging Network (ADAN) that leverages a set of bilingual word embeddings (BWEs; Zou et al., 2013) trained on bitexts, in order to eliminate the need for labeled TARGET training data.2 We introduce the ADAN model in §2, and in §3 evaluate ADAN using English as the S OURC"
Q18-1039,P16-1133,0,0.115293,"ion is cross-lingual sentiment classification (CLSC), in which the abundant resources of a source language (likely English, denoted as S OURCE) are leveraged to produce sentiment classifiers for a target language (TARGET). In general, CLSC methods make use of generalpurpose bilingual resources—such as hand-crafted bilingual lexica or parallel corpora—to alleviate or eliminate the need for task-specific TARGET annotations. In particular, the bilingual resource of choice for the majority of previous CLSC models is a full-fledged Machine Translation (MT) system (Wan, 2008, 2009; Lu et al., 2011; Zhou et al., 2016), a component that is expensive to obtain. In this work, we propose a language-adversarial training approach that does not need a highly engineered MT system, and requires orders of magnitude less in terms of the size of a parallel corpus. Specifically, we propose an Adversarial Deep Averaging Network (ADAN) that leverages a set of bilingual word embeddings (BWEs; Zou et al., 2013) trained on bitexts, in order to eliminate the need for labeled TARGET training data.2 We introduce the ADAN model in §2, and in §3 evaluate ADAN using English as the S OURCE with two TARGET choices: Chinese and Arab"
Q18-1039,L16-1561,0,0.0314468,"Missing"
Q18-1039,P17-1130,0,0.562453,"ch 2 When not using any TARGET annotations, the setting is sometimes referred to as unsupervised (in the target language) in the literature. Similarly, when some labeled data is used, it is called the semi-supervised setting. 557 Transactions of the Association for Computational Linguistics, vol. 6, pp. 557–570, 2018. Action Editor: Trevor Cohn. Submission batch: 2/2018; Revision batch: 5/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 2 −Jq Jp that uses a powerful MT system, and (iv) the crosslingual “distillation” approach of Xu and Yang (2017) that makes direct use of a parallel corpus (see §3.2). In all cases, we find that ADAN achieves statistically significantly better results. We further investigate the semi-supervised setting, where a small amount of annotated TARGET data exists, and show that ADAN continues to outperform the alternatives given the same amount of TARGET supervision (§3.3.1). We provide an analysis and visualization of ADAN (§3.3.2), shedding light on how our approach manages to achieve its strong cross-lingual performance. Additionally, we study the bilingual resource that ADAN depends on, the BWEs, and demons"
Q18-1039,D13-1141,0,0.0817618,"ed for task-specific TARGET annotations. In particular, the bilingual resource of choice for the majority of previous CLSC models is a full-fledged Machine Translation (MT) system (Wan, 2008, 2009; Lu et al., 2011; Zhou et al., 2016), a component that is expensive to obtain. In this work, we propose a language-adversarial training approach that does not need a highly engineered MT system, and requires orders of magnitude less in terms of the size of a parallel corpus. Specifically, we propose an Adversarial Deep Averaging Network (ADAN) that leverages a set of bilingual word embeddings (BWEs; Zou et al., 2013) trained on bitexts, in order to eliminate the need for labeled TARGET training data.2 We introduce the ADAN model in §2, and in §3 evaluate ADAN using English as the S OURCE with two TARGET choices: Chinese and Arabic. ADAN is first compared to two baseline systems: (i) one trained only on labeled S OURCE data, relying on BWEs for cross-lingual generalization; and (ii) a domain adaptation method (Chen et al., 2012) that views the two languages simply as two distinct domains. We then validate ADAN against two state-of-the-art CLSC methods: (iii) an approach 2 When not using any TARGET annotati"
Q18-1039,P17-1179,0,0.0271246,"ilingual word embeddings. We start by initializing the systems with random word embeddings (WEs), shown in Table 2. ADAN with random WEs outperforms the DAN and mSDA baselines using BWEs and matches the performance of the LR+MT baseline (Table 1), suggesting that ADAN successfully extracts features that could be used for crosslingual classification tasks without any bitext. This impressive result vindicates the power of adversarial training to reduce the distance between two complex distributions without any direct supervision, which is also observed in other recent works for different tasks (Zhang et al., 2017; Lample et al., 2018). 565 4 k lambda lambda ADAN without Wasserstein Distance ADAN Figure 4: A grid search on k and lambda for ADAN (right) and the ADAN-GRL variant (left). Numbers indicate the accuracy on the Chinese development set. time. This demonstrates that ADAN’s languageadversarial training framework can be successfully used with other F choices. 3.3.5 ADAN Hyperparameter Stability In this section, we show that the training of ADAN is stable over a large set of hyperparameters, and provides improved performance compared with the standard ADAN-GRL. To verify the superiority of ADAN, w"
Q19-1014,P17-1055,0,0.0523942,"Missing"
Q19-1014,P17-1168,0,0.0194867,"ion to improve the original sliding window baseline, formulated in Expression 8 (Section 4.2). • Stanford Attentive Reader This neural baseline compares each candidate answer (i.e., entity) representation to the question-aware document representation built with attention mechanism (Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford e"
Q19-1014,P04-3031,0,0.108587,"The formal definition of δis is as follows. s (7) To make the final answer option selection, our rule-based method combines Expressions (5) and (7): s IQ δis = (6) s swi Q + swi∗ d Q + d∗i − i 2 2 (5) • Pointwise mutual information (PMI): sQ sQ ∗ ∗ pmimax ,1..3 , pmimax,1..3 , pmimin,1..3, pmimin,1..3, sQ pmiavg,1..3 , and pmi∗avg,1..3 , where pmisf,i is defined as Since a large percentage of questions cannot be solved by word-level matching, we also attempt to incorporate general world knowledge into our rule-based method. We calculate cssi , the  3 We use the list of stop words from NLTK (Bird and Loper, 2004). pmisf,i = 223 j log fk s Oi ,WkD ) Oi s C1 (Wj )C1 (WkD ) C 2 (W j |W Oi | (9) C1 (w) denotes the word frequency of w in external copora (we use Reddit posts [Tan and Lee, 2015]), and C2 (w1 , w2 ) represents the co-occurrence frequency of word w1 and w2 within a distance < K in external copora. We use PMI to evaluate the relatedness between the content of an answer option and the target-speaker-focused context based on co-occurrences of words in external corpora, inspired by previous studies on narrative event chains (Chambers and Jurafsky, 2008). • ConceptNet relations (CR): cr1..3,1..|R |"
Q19-1014,P08-1090,0,0.0431185,", the  3 We use the list of stop words from NLTK (Bird and Loper, 2004). pmisf,i = 223 j log fk s Oi ,WkD ) Oi s C1 (Wj )C1 (WkD ) C 2 (W j |W Oi | (9) C1 (w) denotes the word frequency of w in external copora (we use Reddit posts [Tan and Lee, 2015]), and C2 (w1 , w2 ) represents the co-occurrence frequency of word w1 and w2 within a distance < K in external copora. We use PMI to evaluate the relatedness between the content of an answer option and the target-speaker-focused context based on co-occurrences of words in external corpora, inspired by previous studies on narrative event chains (Chambers and Jurafsky, 2008). • ConceptNet relations (CR): cr1..3,1..|R |. R = {r1 , r2 , . . .} is the set of ConceptNet relation types (e.g., ‘‘CapableOf’’ and ‘‘PartOf’’). cri,j is the number of relation triples (w1 , rj , w2 ) that appear in the ConceptNet (Speer et al., 2017), where w1 represents a word in answer option Oi , w2 represents a word in D, and the relation type rj ∈ R. Similar to the motivation for using PMI, we use CR to capture the association between an answer option and the source dialogue based on raw co-occurrence counts in the commonsense knowledge base. • ConceptNet embeddings (CE): Besides the l"
Q19-1014,P17-1025,1,0.884975,"Missing"
Q19-1014,P16-1223,0,0.0604832,"Missing"
Q19-1014,W16-3612,0,0.28737,"ractive since candidate answers are usually short spans from source documents. State-of-the-art neural models with attention mechanisms already achieve very high performance based on local lexical information. Recently researchers work on the construction of spoken span-based data sets (Lee et al., 2018; Li et al., 2018) by applying text-to-speech technologies or recruiting human speakers based on formal written document-based data sets such as SQuAD (Rajpurkar et al., 2016). Some spanbased conversation data sets are constructed from a relatively small size of dialogues from television shows (Chen and Choi, 2016; Ma et al., 2018). Considering the limitations in extractive data sets, answers in abstractive data sets such as MS MARCO (Nguyen et al., 2016), SearchQA (Dunn et al., 2017), and NarrativeQA (Koˇcisk`y et al., 2018) are human-crowdsourced based on source documents or summaries. Concurrently, there is a growing interest in conversational reading comprehension such as CoQA (Reddy et al., 2018). Because annotators tend to copy spans as answers (Reddy et al., 2018), the majority of answers are still extractive in these data sets (Table 2). Compared to the data sets mentioned above, most of the co"
Q19-1014,C18-1018,0,0.0606449,"Missing"
Q19-1014,D18-1241,1,0.873778,"Missing"
Q19-1014,P17-1147,0,0.0815762,"Missing"
Q19-1014,N18-1023,0,0.133945,"Missing"
Q19-1014,Q18-1023,0,0.115052,"Missing"
Q19-1014,D16-1241,0,0.0576388,"Missing"
Q19-1014,S18-1119,0,0.10702,"Missing"
Q19-1014,P11-2057,0,0.205028,"Missing"
Q19-1014,N18-1185,0,0.441948,"te answers are usually short spans from source documents. State-of-the-art neural models with attention mechanisms already achieve very high performance based on local lexical information. Recently researchers work on the construction of spoken span-based data sets (Lee et al., 2018; Li et al., 2018) by applying text-to-speech technologies or recruiting human speakers based on formal written document-based data sets such as SQuAD (Rajpurkar et al., 2016). Some spanbased conversation data sets are constructed from a relatively small size of dialogues from television shows (Chen and Choi, 2016; Ma et al., 2018). Considering the limitations in extractive data sets, answers in abstractive data sets such as MS MARCO (Nguyen et al., 2016), SearchQA (Dunn et al., 2017), and NarrativeQA (Koˇcisk`y et al., 2018) are human-crowdsourced based on source documents or summaries. Concurrently, there is a growing interest in conversational reading comprehension such as CoQA (Reddy et al., 2018). Because annotators tend to copy spans as answers (Reddy et al., 2018), the majority of answers are still extractive in these data sets (Table 2). Compared to the data sets mentioned above, most of the correct answer optio"
Q19-1014,N18-1202,0,0.0524594,"Missing"
Q19-1014,D18-1260,0,0.067851,"Missing"
Q19-1014,D16-1264,0,0.392475,"Missing"
Q19-1014,D13-1020,0,0.409742,"‘‘really hot,’’ ‘‘really beautiful,’’ ‘‘very bad,’’ and ‘‘very important’’ rather than more appropriate yet more advanced adjectives that might hinder reading comprehension of language learners with smaller vocabularies. According to the explanations provided by the tool, the readability scores for both data sets fall into the same category ‘‘Your text is very simple and easy to read, likely to be understood by an average 5th-grader (age 10).’’ 4 4.2 Rule-Based Approaches We first attempt to incorporate dialogue structure information into sliding window (SW), a rulebased approach developed by Richardson et al. (2013). This approach matches a bag of words constructed from a question Q and one of its answer option Oi with a given document, and calculates the TF-IDF style matching score for each answer option. ˆ , and O ˆ i be the unordered set of ˆ s, Q Let D distinct words (excluding punctuation marks) in Ds , Q, and Oi , respectively. Instead of only regarding dialogue D as a non-conversational text snippet, we also pay special attention to the context that is relevant to the target speaker mentioned in the question. Therefore, given a target speaker sQ , we propose to compute a speaker-focused sliding wi"
Q19-1014,D18-1132,0,0.0646353,"(Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford et al., 2018). • Word Matching This strong baseline (Yih et al., 2013) selects the answer option that has the highest count of overlapping words with the given dialogue. 225 Method Dev Test Random Word Matching (WM) (Yih et al., 2013) Sliding Window (SW) (Richardson et al., 201"
Q19-1014,P18-2118,0,0.297659,"(Hermann et al., 2015; Chen et al., 2016). Lai et al. (2017) add a bilinear operation to compare document and answer option representations to answer multiple-choice questions. • Gated-Attention Reader The baseline models multiplicative question-specific document representations based on a gated-attention mechanism (Dhingra et al., 2017), which are then compared to each answer option (Lai et al., 2017). • Co-Matching This state-of-the-art multiplechoice reading comprehension model explicitly treats question and answer option as two sequences and jointly matches them against a given document (Wang et al., 2018b). Experiment 5.1 Baselines We implement several baselines, including rulebased methods and state-of-the-art neural models. • Finetuned Transformer LM This is a general task-agnostic model introduced in Section 4.4, which achieves the best reported performance on several tasks requiring multisentence reasoning (Radford et al., 2018). • Word Matching This strong baseline (Yih et al., 2013) selects the answer option that has the highest count of overlapping words with the given dialogue. 225 Method Dev Test Random Word Matching (WM) (Yih et al., 2013) Sliding Window (SW) (Richardson et al., 201"
Q19-1014,P13-1171,0,0.114272,"Missing"
R11-1028,H05-1004,0,0.064484,"Missing"
R11-1028,P04-1035,0,0.0283236,"ions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work builds on research on fine-grained opinion extraction by extracting additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources. Several methods for computing opinions from product reviews exist (e.g. Hu and Liu (2004), Popescu and Etzioni (2005)). Due to properties of the limited domain and genre, however, the problem and approaches have been considerably simplified. In the product domain, summaries have are computed by extracting tuples [product attribute, opinion trigger, polarity] (with the product attribute extraction typically performed as a straightforward dictionary lookup) and"
R11-1028,H05-1043,0,0.567734,"the problem of extracting opinions — both at the document level (coarse-grained opinion information) and at the level of sentences, clauses, or individual expressions (fine-grained opinion information). In contrast, our work concerns the consolidation of fine-grained information about opinions to create non-extract-based opinion summaries, a rich, concise and useful representation of the opinions expressed in a document. In particular, the opinion summaries produced by our system combine 1 Several systems for summarizing the opinions expressed in product reviews exist (e.g. Hu and Liu (2004), Popescu and Etzioni (2005)). Due to the limited domain, summarizing opinions in product reviews constitutes a substantially different text-understanding problem; it has proven to be easier than the task addressed here and is handled using a very different set of techniques. 202 Proceedings of Recent Advances in Natural Language Processing, pages 202–209, Hissar, Bulgaria, 12-14 September 2011. applications and uses. In general, we presume the existence of automatically extracted fine-grained opinions, each of which has the following four attributes: [Source American public] opinion has [− turned increasingly against] ["
R11-1028,W03-1014,0,0.0115184,"abstract, graph-based representation of opinions, while the TAC Opinion Summary task aims for extractive summaries. Opinion set summaries support fined-grained information extraction of opinions as well as userdirected exploration of the opinions in a document. 3 Related Work Our works falls in the area of fine-grained subjectivity analysis concerned with analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work builds on research on fine-grained opinion extraction by extracting additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources. Several methods for computing opinions from product reviews exist (e.g. Hu and Liu (2004), Pope"
R11-1028,W06-1640,1,0.763322,"a span of text in the original document. We enhance these fine-grained opinion predictions by using the opinion polarity classifier from Choi and Cardie (2009), which adds polarity predictions as one of three possible values: positive, negative or neutral. This value is added to the opinion tuple to obtain [opinion trigger, source, polarity] triples. Source Coreference Resolution Given the finegrained opinions, our system uses source coreference resolution to decide which opinions should be attributed to the same source. For this task, we rely on the partially supervised learning approach of Stoyanov and Cardie (2006). Following this step, OASIS produces opinion triples grouped according to their sources. Topic Extraction/Coreference Resolution Next, our system labels fine-grained opinions with their topic and decide which opinions are on the same topic. Here, we use the topic coreference resolution approach proposed in Stoyanov and Cardie (2008). As a result of this step, OASIS produces opinion four-tuples [opinion trigger, source, polarity, topic name] that are grouped both 204 Component Fine-grained op. extractor Polarity classifier Source coreference resolver Topic coreference resolver Measure F1 Acc."
R11-1028,P98-1012,0,0.410963,"(possibly conflicting) opinions from a source on the same topic that appear in the opinion set summary. This is done in a straightforward way: the polarity of the aggregate opinion is computed as the average of the polarity of all the opinions from the source on the topic. DLB 3 recalli = (recallisrc + recallitopic )/2 recallisrc = |Risrc ∩ Sisrc |/|Sisrc | Performance of the different subcomponents of our system as it applies to our data (see Section 6) are shown in Table 1. F1 refers to the harmonic average of precision and recall, while the B 3 evaluation metric for coreference resolution (Bagga and Baldwin, 1998) is described in Section 5.2 5 5.2 We propose a novel Opinion Summary Evaluation Metric (OSEM) that combines ideas from the ACE score (ACE, 2006) (used for information extraction) and Luo’s (2005) CEAF score (used for coreference resolution). OSEM can be used for both opinion set and aggregate summaries. The OSEM metric compares two opinion summaries – the key, K, and the response, R, containing a number of “summary opinions”, each of which is comprised of one or more fine-grained opinions. Each summary opinion is characterized by three attributes (the source name, the polarity and the topic n"
R11-1028,C08-1103,1,0.937419,"polarity] triples. Source Coreference Resolution Given the finegrained opinions, our system uses source coreference resolution to decide which opinions should be attributed to the same source. For this task, we rely on the partially supervised learning approach of Stoyanov and Cardie (2006). Following this step, OASIS produces opinion triples grouped according to their sources. Topic Extraction/Coreference Resolution Next, our system labels fine-grained opinions with their topic and decide which opinions are on the same topic. Here, we use the topic coreference resolution approach proposed in Stoyanov and Cardie (2008). As a result of this step, OASIS produces opinion four-tuples [opinion trigger, source, polarity, topic name] that are grouped both 204 Component Fine-grained op. extractor Polarity classifier Source coreference resolver Topic coreference resolver Measure F1 Acc. B3 B3 Score 59.7 65.3 83.2 54.7 of the same items (the key). It is computed as the recall for each item i: Recalli = |Ri ∩ Si |/|Si |, where Ri and Si are the clusters that contains i in the response and the key, respectively. The recall for a document is the average over all items. Precision is computed by switching the roles of the"
R11-1028,P09-2040,0,0.0471842,"Missing"
R11-1028,H05-1116,1,0.916229,"Missing"
R11-1028,D09-1062,1,0.829984,"the Pyramid F-score (Nenkova et al., 2007) borrowed from the field of summarization. Additionally, summaries are man4 Opinion Summarization System In this section we describe the architecture of our system, OASIS. Fine-grained Opinion Extraction OASIS starts with the output of Choi et al.’s (2006) extractor, which recognizes opinion sources and triggers. These predictions can be described as a tuple [opinion trigger, source] with each component representing a span of text in the original document. We enhance these fine-grained opinion predictions by using the opinion polarity classifier from Choi and Cardie (2009), which adds polarity predictions as one of three possible values: positive, negative or neutral. This value is added to the opinion tuple to obtain [opinion trigger, source, polarity] triples. Source Coreference Resolution Given the finegrained opinions, our system uses source coreference resolution to decide which opinions should be attributed to the same source. For this task, we rely on the partially supervised learning approach of Stoyanov and Cardie (2006). Following this step, OASIS produces opinion triples grouped according to their sources. Topic Extraction/Coreference Resolution Next"
R11-1028,H05-1045,1,0.863696,"ve summaries. Opinion set summaries support fined-grained information extraction of opinions as well as userdirected exploration of the opinions in a document. 3 Related Work Our works falls in the area of fine-grained subjectivity analysis concerned with analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work builds on research on fine-grained opinion extraction by extracting additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources. Several methods for computing opinions from product reviews exist (e.g. Hu and Liu (2004), Popescu and Etzioni (2005)). Due to properties of the limited domain and genre, however, the problem"
R11-1028,W03-1017,0,0.0753839,"n Summary task aims for extractive summaries. Opinion set summaries support fined-grained information extraction of opinions as well as userdirected exploration of the opinions in a document. 3 Related Work Our works falls in the area of fine-grained subjectivity analysis concerned with analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work builds on research on fine-grained opinion extraction by extracting additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources. Several methods for computing opinions from product reviews exist (e.g. Hu and Liu (2004), Popescu and Etzioni (2005)). Due to properties of the limited domain and genre, h"
R11-1028,W06-1651,1,0.851844,"rics used for coreference resolution and information extraction. 5.1 Opinion Summary Evaluation Metric Doubly-linked B 3 score Opinion set summaries are similar to the output of coreference resolution – both target grouping a set of items together. Thus, our first evaluation metric is based on a popular coreference resolution measure, the B 3 score (Bagga and Baldwin, 1998). B 3 evaluates the quality of a an automatically generated clustering of items (the system response) as compared to a gold-standard clustering 2 Our scores for fine-grained opinion extraction differ from published results (Choi et al., 2006) because we do not allow the system to extract speech events that do not signal expressions of opinions (i.e. the word “said” when used in objective context: “John said his car is blue.”). 205 Fine-grained opinions Automatic Manual System Baseline OASIS Baseline OASIS OASIS + manual src coref OASIS + manual tpc coref DLB 3 α=0 50.78 49.75 78.67 78.69 82.65 82.40 29.20 31.24 51.12 59.82 79.85 80.80 α = .25 37.32 41.71 60.72 69.04 79.39 78.14 OSEM α = .5 27.90 35.82 47.04 61.47 76.68 74.53 α = .75 21.12 31.52 36.60 55.59 74.61 71.56 α=1 25.47 41.50 28.59 54.80 74.95 71.03 Table 2: Scores for the"
R11-1028,H05-2017,0,\N,Missing
R11-1028,C98-1012,0,\N,Missing
S13-1032,S12-1051,0,0.042451,"verage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System 3.1 Task Setup The STS task consists of labeling one sentence pair at a time, based on the semantic similarity existent between its two component sentences. Human assigned similarity scores range from 0 (no relation) to 5 (semantivally equivalent). The *S EM 2013 STS task did not provide additional labeled data to the training and testing sets released as part of the STS task hosted at S EM E VAL 2012 (Agirre et al., 2012); our system variations were trained on S EM E VAL 2012 data. The test sets (Agirre et al., 2013) consist of text pairs extracted from headlines (headlines, 750 pairs), sense definitions from WordNet and OntoNotes (OnWN, 561 pairs), sense definitions from WordNet and FrameNet (FNWN, 189 pairs), and data used in the evaluation of machine translation systems (SMT, 750 pairs). 3.2 Resources Various subparts of our framework use several resources that are described in more detail below. Wikipedia1 is the most comprehensive encyclopedia to date, and it is an open collaborative effort hosted on-line"
S13-1032,S12-1094,1,0.46486,"Missing"
S13-1032,J90-1003,0,0.122907,"wledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam 222 and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training."
S13-1032,islam-inkpen-2006-second,0,0.0423081,"Missing"
S13-1032,O97-1002,0,0.820124,"he given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and extractive summarization (Salton et al., 1997), in the automatic evaluation of machine translation (Papineni et al., 2002), ∗ † Google Inc. Mountain View, CA √ Language Computer Corp. Richardson, TX Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as latent semantic analysis (Landauer et al., 1997), explicit semantic a"
S13-1032,N03-1020,0,0.295196,"Missing"
S13-1032,E09-1065,1,0.829997,"e relationships are employed by various knowledge-based methods to derive semantic similarity. The MPQA corpus (Wiebe and Riloff, 2005) is a newswire data set that was manually annotated at the expression level for opinion-related content. Some of the features derived by our opinion extraction models were based on training on this corpus. 3.3 Features Our system variations derive the similarity score of a given sentence-pair by integrating information from knowledge, corpus, and opinion-based sources3 . 3.3.1 Knowledge-Based Features Following prior work from our group (Mihalcea et al., 2006; Mohler and Mihalcea, 2009), we employ several WordNet-based similarity metrics for the task of sentence-level similarity. Briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity4 that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this system. The shortest path (P ath) similarity is equal to: Simpath = 1 length length 2∗D Simwup = 2 ∗"
S13-1032,P11-1076,1,0.829995,"X Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as latent semantic analysis (Landauer et al., 1997), explicit semantic analysis (Gabrilovich and Markovitch, 2007), or salient semantic analysis 221 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 221–228, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics (Hassan and Mihalcea, 2011). In this paper, we describe the system variations"
S13-1032,P02-1040,0,0.0966848,"ge processing and related areas. One of the earliest applications of text similarity is perhaps the vector-space model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their angular distance with the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and extractive summarization (Salton et al., 1997), in the automatic evaluation of machine translation (Papineni et al., 2002), ∗ † Google Inc. Mountain View, CA √ Language Computer Corp. Richardson, TX Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score"
S13-1032,J98-1004,0,0.0156291,"o identify it. Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vector-space model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their angular distance with the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and extractive summarization (Salton et al., 1997), in the automatic evaluation of machine translation (Papineni et al., 2002), ∗ † Google Inc. Mountain View, CA √ Language Computer Corp. Richardson, TX Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction m"
S13-1032,H05-2018,1,0.725452,"ating a list ϕ which holds the strongest semantic pairings between the fragments’ terms, such that each term can only belong to one and only one pair. Sim(Ta , Tb ) = (ω + P|ϕ| ϕi ) × (2ab) a+b i=1 (8) where ϕi is the similarity score for the ith pairing. 3.3.3 Opinion Aware Features We design opinion-aware features to capture sentence similarity on the subjectivity level based on the output of three subjectivity analysis systems. Intuitively, two sentences are similar in terms of subjectivity if there exists similar opinion expressions which also share similar opinion holders. OpinionFinder (Wilson et al., 2005) is a publicly available opinion extraction model that annotates the subjectivity of new text based on the presence (or absence) of words or phrases in a large lexicon. The system consists of a two step process, by feeding the sentences identified as subjective or objective by a rule-based high-precision classifier to a highrecall classifier that iteratively learns from the remaining corpus. For each sentence in a STS pair, the two classifiers provide two predictions; a subjectivity similarity score (SUBJSL) is computed as follows. If both sentences are classified as subjective or objective, t"
S13-1032,D12-1122,1,0.841706,"er. We first record how many expressions the two sentences have: feature NUMEX1 and NUMEX2. Then we compare how many tokens these expressions share and we normalize by the total number of expressions (feature EXPR). We compute the difference between the probabilities of the two sentences being subjective (SUBJDIFF), by employing a logistic regression classifier using LIBLINEAR (Fan et al., 2008) trained on the MPQA corpus. The smaller the difference, the more similar the sentences are in terms of subjectivity. We also employ features produced by the opinionextraction model of Yang and Cardie (Yang and Cardie, 2012), which is better suited to process expressions of arbitrary length. Specifically, for each sentence, we extract subjective expressions and generate the following features. SUBJCNT is a binary feature which is equal to 1 if both sentences contain a subjective expression. DSEALGN marks the number of shared words between subjective expressions in two sentences, while DSESIM represents their similarity beyond the word level. We represent the subjective expressions in each sentence as a feature vector, containing unigrams extracted from the expressions, their part-of-speech, their WordNet hypernym"
S13-1032,P94-1019,0,\N,Missing
S14-2010,S14-2085,0,0.0392393,"Missing"
S14-2010,S14-2069,0,0.0326403,"Missing"
S14-2010,S14-2128,0,0.0328229,"Missing"
S14-2010,P13-1024,1,0.0618966,"data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the news title, while the other one represents a Twitter comment on that particular news. They are evenly sampled from string similarity values between 0.5 and 1. Table 1 shows the explanations and values associated with each score between 5 and 0. As in prior years, we used Amazon Mechanical Turk (AMT)3 to crowdsource the annotation of the English pairs.4 Annotators are presented with the Table 2: English subtask: Summary of train (2012 and 2013) and test (2014) datasets. a DARPA sponsored workshop at Columbia University.1 In 2013, STS wa"
S14-2010,S14-2112,0,0.0317369,"Missing"
S14-2010,N06-2015,0,0.0715746,"ferent similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs"
S14-2010,S12-1051,1,0.623306,"r as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline map"
S14-2010,S14-2131,0,0.0336314,"Missing"
S14-2010,S14-2072,0,0.0817436,"Missing"
S14-2010,Q14-1018,0,0.0731,"Missing"
S14-2010,S14-2039,0,0.0993932,"Missing"
S14-2010,S14-2078,0,0.101731,"Missing"
S14-2010,S14-2022,0,0.0306687,"Missing"
S14-2010,S14-2046,0,0.022257,"Missing"
S14-2010,D13-1179,0,0.0175763,"Missing"
S14-2010,S12-1060,0,0.0199826,"Missing"
S14-2010,S14-2093,0,0.0281526,"Missing"
S14-2010,W10-0721,0,0.050451,"d 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the new"
S14-2010,W10-0707,0,\N,Missing
S14-2010,P94-1019,0,\N,Missing
S14-2010,Q14-1017,0,\N,Missing
S14-2010,S14-2138,0,\N,Missing
S14-2098,O97-1002,0,0.055036,"Abstract 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. T"
S14-2098,S12-1051,0,0.0213281,"l language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). 3 ∗ {carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 3.1 System De"
S14-2098,S14-2003,0,0.0210175,"), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). 3 ∗ {carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 3.1 System Description Generic Features Our system employs both knowledge and corpusbased measures as detailed below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlat"
S14-2098,P13-4021,0,0.0236756,"Missing"
S14-2098,N03-1020,0,0.0569777,"ovel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Janyce Wiebe University of Pittsburgh Pittsburgh, PA Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this l"
S14-2098,J90-1003,0,0.0840197,"rm distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. This article pres"
S14-2098,D07-1061,0,0.00911978,"ty has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual inf"
S14-2098,N13-1090,0,0.00492088,"and obtain metrics W T V 1 (by applying Align) and W T V 2 (using VectorSum). paragraph2sentence. At this level, due to the long context that entails one-to-many mappings between the words in the sentence and those in the paragraph, we use a text clustering technique prior to calculating the features’ weights. Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al., 2013b; Mikolov et al., 2013a). A primary advantage of such a model is that, by breaking away from the typical n-gram model that sees individual units with no relationship to each other, it is able to generalize and produce word vectors that are similar for related words, thus encoding linguistic regularities and patterns (Mikolov et al., 2013b). For example, vec(Madrid)-vec(Spain)+vec(France) is closer to vec(Paris) than any other word vector (Mikolov et al., 2013a). We used the pretrained Google News word2vec model (W T V ) built over a 100 billion words corpus, and containing 3 million 300-dimen"
S14-2098,islam-inkpen-2006-second,0,0.0105473,"nomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. This article presents our team’s participating system at SemEval-2014 Task 3. Using"
S14-2098,P02-1040,0,0.0914881,"with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Janyce Wiebe University of Pittsburgh Pittsburgh, PA Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (J"
S14-2098,J98-1004,0,0.0351393,"ith traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Janyce Wiebe University of Pittsburgh Pittsburgh, PA Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEva"
S14-2098,S13-1004,0,\N,Missing
S15-2045,agerri-etal-2014-ixa,1,0.57453,"onal and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple regular expressions. Additionally, this step collects chunk regions coming either from gold standard or from the chunking done by ixa-pipes-chunk (Agerri et al., 2014). This is followed by a lowercased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus ena"
S15-2045,S12-1051,1,0.519741,"TS also differs from both TE and paraphrasing (in as far as both tasks have been defined to date in the literature) in that rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for many NLP tasks such as MT evaluation, information extraction, question answering, summarization. In 2012, we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success (Agirre et al., 2012). In addition, we 252 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity"
S15-2045,S14-2010,1,0.800447,"f the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity in a new language, namely Spanish (Agirre et al., 2014). This year we presented three subtasks: the English subtask, the Spanish subtask and the interpretable pilot subtask. The English subtask comprised pairs from headlines and image descriptions, and it also introduced new genres, including answer pairs from a tutorial dialogue system and from Q&A websites, and pairs from a dataset tagged with committed belief annotations. For the Spanish subtask, additional pairs from news and Wikipedia articles were selected. The annotations for both tasks leveraged crowdsourcing. Finally, with the interpretable STS pilot subtask, we wanted to start exploring"
S15-2045,P14-1023,0,0.0115737,"7070 0.7251 0.7311 0.7250 0.7422 0.6364 0.7775 0.7032 0.7130 0.7189 0.4616 0.7533 0.6111 0.5379 0.5424 0.5672 0.6558 0.4919 0.5912 0.6964 0.7114 0.6364 Table 3: Task 2a: English evaluation results in terms of Pearson correlation. 259 Rank 61 42 29 63 56 57 70 69 71 62 44 49 43 74 72 73 34 28 26 1 3 5 19 18 16 8 9 2 12 13 23 41 59 20 47 22 11 15 33 45 55 24 10 17 50 51 52 4 7 6 46 36 27 39 31 30 32 25 53 14 40 37 35 68 21 58 66 65 64 48 67 60 42 38 54 approach for the top three participants (DLS@CU, ExBThemis, Samsung). They use WordNet (Miller, 1995), Mikolov Embeddings (Mikolov et al., 2013; Baroni et al., 2014) and PPDB (Ganitkevitch et al., 2013). In general, generic NLP tools such as lemmatization, PoS tagging, distributional word embeddings, distributional and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple"
S15-2045,N13-1092,0,0.0796576,"the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,P13-2080,0,0.0179598,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,W10-0721,0,0.0157602,"ns student answers Q&A forum answers commited belief Table 2: English subtask: Summary of train (2012, 2013, 2014) and test (2015) datasets. lines come from a different EMM cluster. Then, we computed the string similarity between those pairs. Accordingly, we sampled 1000 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity as a metric. We sampled another 1000 pairs from the different EMM cluster in the same manner. The Images dataset is a subset of the PASCAL VOC-2008 dataset (Rashtchian et al., 2010), which consists of 1000 images with around 10 descriptions each, and has been used by a number of image description systems. It was also sampled using string similarity, discarding those that had been used in previous years. We organized two bins with 1000 pairs each: one with pairs of descriptions from the same image, and the other one with pairs of descriptions from different images. The source of the Answers-student pairs is the BEETLE corpus (Dzikovska et al., 2010), which is a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system."
S15-2045,W00-0726,0,0.313421,"Missing"
S15-2045,S12-1060,0,0.211502,"Missing"
S15-2045,W10-0707,0,\N,Missing
stoyanov-cardie-2008-annotating,M95-1005,0,\N,Missing
stoyanov-cardie-2008-annotating,passonneau-2004-computing,0,\N,Missing
stoyanov-cardie-2008-annotating,H05-1116,1,\N,Missing
stoyanov-cardie-2008-annotating,W06-0301,0,\N,Missing
stoyanov-cardie-2008-annotating,W03-1017,0,\N,Missing
stoyanov-cardie-2008-annotating,H05-2017,0,\N,Missing
stoyanov-cardie-2008-annotating,H05-1043,0,\N,Missing
stoyanov-cardie-2008-annotating,W03-1014,0,\N,Missing
stoyanov-cardie-2008-annotating,H05-1004,0,\N,Missing
stoyanov-cardie-2008-annotating,P04-1035,0,\N,Missing
stoyanov-cardie-2008-annotating,H05-1045,1,\N,Missing
stoyanov-cardie-2008-annotating,P02-1053,0,\N,Missing
stoyanov-cardie-2008-annotating,W02-1011,0,\N,Missing
W00-0505,M92-1038,0,0.0242643,"ticipant slots plus optional date and location slots.2 We then gathered a small corpus of thirty articles by searching for articles containing ""North Korea"" and one or more of about 15 keywords. The first two sentences (with a few exceptions) were then annotated with the slots to be extracted, leading to a total of 51 sentences containing 47 scenario templates and 89 total 4.2 2 In the end, we did not use the &apos;issue&apos; slot shown in Figure 1, as it contained more complex Idlers than those that typically have been handled in IE systems. For our feasibility study, we chose to follow the AutoSlog (Lehnert et al., 1992; Riloff, 1993) approach to extraction pattern acquisition. In this approach, extraction patterns are acquired 34 Extraction Pattern Learning i. E: &lt;target-np>=&lt;subject> &lt;active voice verb> &lt;participant> MET K: &lt;target-np>=&lt;subject> &lt;active voice verb> &lt;John-i> MANNASSTA &lt;John-nom>&apos;MET 2. E: &lt;target-np>=&lt;subject> &lt;verb> &lt;infinitive> &lt;participant> agreed to MEET K: &lt;target-np>=&lt;subject> &lt;verbl-ki-lo> &lt;verb2> &lt;John-un> MANNA-ki-lo hapuyhayssta &lt;John-nom> MEET-ki-lo agreed (-ki: nominalization ending, -io: an adverbial postposition) Figure 3 via a one-shot general-to-specific learning algorithm d"
W00-0505,1981.tc-1.3,0,0.0738175,"Missing"
W00-0505,W98-1428,1,0.744077,"an Transfer Lexicon is used to map the English keywords to corresponding Korean ones. When the match falls below a user• configurable threshold, the extracted information is filtered out. • The MT component. The MT component (cf. Lavoie et al., 2000) translates the extracted Korean phrases or sentences into corresponding English ones. • The Presentation Generator component. This component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information. It uses existing NLG components, including the Exemplars text planning framework (White and Caldwell, 1998) and the RealPro syntactic realizer (Lavoie and Rainbow, 1997). In our feasibility study, the majority of the effort went towards developing the PIE component, described in the next section. This component was implemented in a general way, i.e. in a way that we would expect to work beyond the specific training/test corpus described below. In contrast, we only implemented initial versions of the User Interface, Ranker and Presentation Generator components, in order to demonstrate the system concept; that is, these initial versions were only intended.to work with our training/test corpus, and wi"
W00-0505,1997.iwpt-1.25,1,0.761584,"Missing"
W00-0505,palmer-etal-1998-rapid,1,\N,Missing
W00-0505,M91-1033,1,\N,Missing
W00-0505,A00-1009,1,\N,Missing
W00-0505,1997.mtsummit-workshop.12,1,\N,Missing
W00-0505,A97-1039,1,\N,Missing
W01-0501,J93-2004,0,\N,Missing
W01-0501,E99-1023,0,\N,Missing
W01-0501,A88-1019,0,\N,Missing
W01-0501,P95-1026,0,\N,Missing
W01-0501,A00-1025,1,\N,Missing
W01-0501,P98-1034,1,\N,Missing
W01-0501,C98-1034,1,\N,Missing
W01-0501,W99-0613,0,\N,Missing
W02-0402,H01-1065,0,0.159675,"Missing"
W02-0402,A97-1029,0,0.0166665,"Missing"
W02-0402,A97-1051,0,0.0232257,"Missing"
W02-0402,W00-0405,0,0.0704563,"Missing"
W02-0402,H01-1054,1,0.878326,"Missing"
W02-0402,A00-2018,0,\N,Missing
W02-1008,W97-0319,0,\N,Missing
W02-1008,M95-1005,0,\N,Missing
W02-1008,N01-1008,0,\N,Missing
W02-1008,P95-1017,0,\N,Missing
W02-1008,P02-1014,1,\N,Missing
W02-1008,J01-4004,0,\N,Missing
W02-1008,W99-0611,1,\N,Missing
W03-1015,P01-1005,0,0.0313355,"Missing"
W03-1015,W99-0613,0,0.240775,"ut a set of views that satisfies two fairly strict conditions. First, each view must be sufficient for learning the target concept. Second, the views must be conditionally independent of each other given the class. Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different lea"
W03-1015,P02-1045,0,0.0479292,". Empirical results on artificial data sets by Muslea et al. (2002) and Nigam and Ghani (2000) confirm that co-training is sensitive to these assumptions. Indeed, although the algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou an"
W03-1015,W02-1008,1,0.57556,"bootstrapping process than Blum and Mitchell’s “rank-by-confidence” method. 2 Noun Phrase Coreference Resolution Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.2 In this section, we give an overview of the coreference resolution system to which the boot2 Concrete examples of the coreference task can be found in MUC-6 (1995) and MUC-7 (1998). strapping algorithms will be applied. The framework underlying the coreference system is a standard combination of classification and clustering (see Ng and Cardie (2002) for details). Coreference resolution is first recast as a classification task, in which a pair of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs. When the system operates within the weakly supervised setting, a weakly supervised algorithm bootstraps the coreference classifier from the given labeled and unlabeled data rather than from a much larger set of labeled instances. The clustering algor"
W03-1015,N03-1023,1,0.717456,"e-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. 100 Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 2 on the 20–30 “formal evaluation” texts for each of the MUC-6 and MUC-7 data sets. Two sets of experiments are conducted, one using naive Bayes as the underlying supervised learning algorithm and the other the decision list learner. All results reported are"
W03-1015,W01-0501,1,0.933763,"Naive Bayes Decision List R P F R P F 50.7 52.6 51.6 17.9 72.0 28.7 33.3 90.7 48.7 19.5 71.2 30.6 53.6 79.0 63.9 40.1 83.1 54.1 48.3 63.5 54.9 18.7 70.8 29.6 MUC-7 Naive Bayes Decision List R P F R P F 40.1 40.2 40.1 32.4 78.3 45.8 32.9 76.3 46.0 32.4 78.3 45.8 43.5 73.2 54.6 38.3 75.4 50.8 40.1 40.2 40.1 32.9 78.1 46.3 Table 2: Results of multi-view co-training, single-view bootstrapping, and self-training. Recall, Precision, and F-measure are provided. Except for the baselines, the best results (F-measure) achieved by the algorithms are shown. ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. 100 Evaluation. We use the MUC-6 (1995) and MUC7 (1998) coreference data sets for evaluation. The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, respectively. Unlike Ng and Cardie (2003) where we choose one of the dryrun texts (contributing approximately 3500–3700 instances) form the labeled data set, however, here we randomly select 1000 instances. The remaining instances are used as unlabeled data. Testing is perf"
W03-1015,N03-1031,0,0.122407,"algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled"
W03-1015,E03-1008,0,0.129526,"algorithm has been applied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classification (Blum and Mitchell, 1998) and named entity classification (Collins and Singer, 1999)), there has been little success, and a number of reported problems, when applying cotraining to NLP data sets for which no natural feature split has been found (e.g. anaphora resolution (Mueller et al., 2002)). As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization. Goldman and Zhou (2000) and Steedman et al. (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training. 1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data. Despite their similarities, the principles underlying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different. In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled"
W03-1015,M95-1005,0,0.28504,"Missing"
W03-1015,P95-1026,0,0.0569571,"d with instances from the unlabeled data and the process is repeated. During testing, each classifier makes an independent decision for a test instance. In this paper, the decision associated with the higher confidence is taken to be the final prediction for the instance. 4 4.2 Experimental Setup P (y |fi = vj ) = N (fi = vj , y) + α N (fi = vj ) + kα Multi-View Co-Training In this section, we describe the Blum and Mitchell (B&M) multi-view co-training algorithm and apply it to coreference resolution. 3 This justifies the use of a decision list as a potential classifier for bootstrapping. See Yarowsky (1995) for details. One of the goals of the experiments is to enable a fair comparison of the multi-view algorithm with our single-view bootstrapping algorithm. Since the B&M co-training algorithm is sensitive not only to the views employed but also to other input parameExperiments Baseline Multi-view Co-Training Single-view Bootstrapping Self-Training MUC-6 Naive Bayes Decision List R P F R P F 50.7 52.6 51.6 17.9 72.0 28.7 33.3 90.7 48.7 19.5 71.2 30.6 53.6 79.0 63.9 40.1 83.1 54.1 48.3 63.5 54.9 18.7 70.8 29.6 MUC-7 Naive Bayes Decision List R P F R P F 40.1 40.2 40.1 32.4 78.3 45.8 32.9 76.3 46."
W06-0302,P02-1014,1,0.313031,"he summaries to be vizualized. Our work also draws on previous work in the area of coreference resolution, which is a relatively well studied NLP problem. Coreference resolution is the problem of deciding what noun phrases in the text (i.e. mentions) refer to the same real-world entities (i.e. are coreferent). Generally, successful approaches have relied machine learning methods trained on a corpus of documents annotated with coreference information (such as the MUC and ACE corpora). Our approach to source coreference resolution is inspired by the state-of-the-art performance of the method of Ng and Cardie (2002). [Target Bulgaria] is criticized by [Source the EU] because of slow reforms in the judiciary branch, the newspaper notes. Stanishev was elected prime minister in 2005. Since then, [Source he] has been a prominent supporter of [Target his country’s accession to the EU]. + + Stanishev + − Delaying Bulgaria Accession − EU Figure 1: Example of text containing opinions (above) and a summary of the opinions (below). In the text, sources and targets of opinions are marked and opinion expressions are shown in italic. In the summary graph, + stands for positive opinion and - for negative. related task"
W06-0302,P04-1035,0,0.185653,"nguistics “ [Target Delaying of Bulgaria’s accession to the EU] would be a serious mistake” [Source Bulgarian Prime Minister Sergey Stanishev] said in an interview for the German daily Suddeutsche Zeitung. “[Target Our country] serves as a model and encourages countries from the region to follow despite the difficulties”, [Source he] added. or individual opinion expression level. Recent work has shown that systems can be trained to recognize opinions, their polarity, and their strength at a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Pang and Lee (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)). Additionally, researchers have been able to effectively identify sources of opinions automatically (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2005). Finally, Liu et al. (2005) summarize automatically generated opinions about products and develop interface that allows the summaries to be vizualized. Our work also draws on previous work in the area of coreference resolution, which is a relatively well studied NLP problem. Coreference resolution is the problem of deciding what noun phrases in the text ("
W06-0302,W02-1011,0,0.0603736,"ains and genres that we target – all documents have occurred in the world press over an 11-month period, between June 2001 and May 2002. Therefore, the Related Work Sentiment analysis has been a subject of much recent research. Several efforts have attempted to automatically extract opinions, emotions, and sentiment from text. The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). In contrast, the problem of fine-grained opinion extraction has concentrated on recognizing opinions at the sentence, clause, 3 The MPQA corpus is available http://nrrc.mitre.org/NRRC/publications.htm. 10 at corpus is suitable for the political and government domains as well as a substantial part of the commercial domain. However, a fair portion of the commercial domain is concerned with opinion extraction from product reviews. Work described in this paper does not target the genre of reviews, which appears to differ significantly from"
W06-0302,W03-1014,0,0.620296,"m Figure 1 performing source coreference resolution amounts to determining that Stanishev, he, and he refer to the same real-world entities. Given the associated opinion expressions and their polarity, this source coreference information is the critical knowledge needed to produce the summary of Figure 1 (although the two target mentions, Bulgaria and our country, would also need to be identified as coreferent). Our work is concerned with fine-grained expressions of opinions and assumes that a system can rely on the results of effective opinion and source extractors such as those described in Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005) and Choi et al. (2005). Presented with sources of opinions, we approach the problem of source coreference resolution as the closely Abstract We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries. In this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution, apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of"
W06-0302,M95-1005,0,0.0592535,"set. The RIPPER runs exhibit the opposite behavior – RIPPER outperforms SVMs on the 200document training set and RIPPER runs trained on the smaller data set exhibit better performance. Overall, the single best performance is observed by RIPPER using the smaller training set. Table 2 lists the results of the best performing runs. The upper half of the table gives the results for the runs that were trained on 400 documents and the lower half contains the results for the 200-document training set. We evaluated using the two widely used performance measures for coreference resolution – MUC score (Vilain et al., 1995) and B 3 (Bagga and Baldwin, 1998). In addition, we used performance metrics (precision, recall and F1) on the identification of the positive class. We compute the latter in two different ways – either by using the pairwise decisions as Another interesting observation is that the B 3 measure correlates well with good “actual” performance on positive class identification. In contrast, good MUC performance is associated with runs that exhibit high recall on the positive class. This confirms some theoretical concerns that MUC score does not reward algorithms that recognize well the absence of lin"
W06-0302,P98-1012,0,0.149756,"he opposite behavior – RIPPER outperforms SVMs on the 200document training set and RIPPER runs trained on the smaller data set exhibit better performance. Overall, the single best performance is observed by RIPPER using the smaller training set. Table 2 lists the results of the best performing runs. The upper half of the table gives the results for the runs that were trained on 400 documents and the lower half contains the results for the 200-document training set. We evaluated using the two widely used performance measures for coreference resolution – MUC score (Vilain et al., 1995) and B 3 (Bagga and Baldwin, 1998). In addition, we used performance metrics (precision, recall and F1) on the identification of the positive class. We compute the latter in two different ways – either by using the pairwise decisions as Another interesting observation is that the B 3 measure correlates well with good “actual” performance on positive class identification. In contrast, good MUC performance is associated with runs that exhibit high recall on the positive class. This confirms some theoretical concerns that MUC score does not reward algorithms that recognize well the absence of links. In addition, the results confi"
W06-0302,H05-1045,1,0.850522,"hat Stanishev, he, and he refer to the same real-world entities. Given the associated opinion expressions and their polarity, this source coreference information is the critical knowledge needed to produce the summary of Figure 1 (although the two target mentions, Bulgaria and our country, would also need to be identified as coreferent). Our work is concerned with fine-grained expressions of opinions and assumes that a system can rely on the results of effective opinion and source extractors such as those described in Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005) and Choi et al. (2005). Presented with sources of opinions, we approach the problem of source coreference resolution as the closely Abstract We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries. In this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution, apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of manually annotated opinions. 1 Introduction Sentiment analysis is conc"
W06-0302,W03-2102,0,0.0249401,"ence resolution developed by Ng and Cardie (2002). Using a corpus of manually annotated opinions, we perform an extensive evaluation and obtain strong initial results for the task of source coreference resolution. 2 3 Data set We begin our discussion by describing the data set that we use for development and evaluation. As noted previously, we desire methods that work with automatically identified opinions and sources. However, for the purpose of developing and evaluating our approaches we rely on a corpus of manually annotated opinions and sources. More precisely, we rely on the MPQA corpus (Wilson and Wiebe, 2003)3 , which contains 535 manually annotated documents. Full details about the corpus and the process of corpus creation can be found in Wilson and Wiebe (2003); full details of the opinion annotation scheme can be found in Wiebe et al. (2005). For the purposes of the discussion in this paper, the following three points suffice. First, the corpus is suitable for the domains and genres that we target – all documents have occurred in the world press over an 11-month period, between June 2001 and May 2002. Therefore, the Related Work Sentiment analysis has been a subject of much recent research. Sev"
W06-0302,W03-1017,0,0.19307,"ysis has been the subject of much recent research interest driven by two primary motivations. First, there is a desire to provide applications that can extract, represent, and allow the exploration of opinions in the commercial, government, and political domains. Second, effective sentiment analysis might be used to enhance and improve existing NLP applications such as information extraction, question answering, summarization, and clustering (e.g. Riloff et al. (2005), Stoyanov et al. (2005)). Several research efforts (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)) have shown that sentiment information can be extracted at the sentence, clause, or individual opinion expression level (fine-grained opinion information). However, little has been done to develop methods for combining fine-grained opinion information to form a summary representation in which expressions of opinions from the 1 We use source to denote an opinion holder and target to denote the entity toward which the opinion is directed. 2 For simplicity, the example summary does not contain any source/target statistics or combination of multiple opinions from the same"
W06-0302,H05-1116,1,\N,Missing
W06-0302,W06-1640,1,\N,Missing
W06-0302,C98-1012,0,\N,Missing
W06-0302,P02-1053,0,\N,Missing
W06-0302,J01-4004,0,\N,Missing
W06-1640,W05-0609,0,0.0937305,"marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, the reference would be omitted from the Moussaoui source chain. Unfortunately, this proper noun phrase might be critical in establishing the coreference of the final source reference he with the other mentions of the source Moussaoui. As mentioned previously, in order to utilize the unlabeled data, our approach differs from traditional coreference resolution, which uses NP pairs as training instances. We instead follow the framework of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) and consider each document as a training example. As in supervised clustering, this framework has the additional advantage that the learning algorithm can consider the clustering algorithm when making decisions about pairwise classification, which could lead to improvements in the classifier. In the next section we describe our approach to classifier construction for step 3 and compare our problem to traditional weakly supervised clustering, characterizing it as an instance of the novel problem of partially supervised clustering. 4 Source coreference resolution might alternatively be approach"
W06-1640,P00-1023,0,0.348933,"ngs of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336–344, c Sydney, July 2006. 2006 Association for Computational Linguistics [Source Zacarias Moussaoui] [− complained] at length today about [Target his own lawyer], telling a federal court jury that [Target he] was [− more interested in achieving fame than saving Moussaoui’s life]. At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of a new machine learning setting that we refer to as partially supervised clustering. In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets). As a r"
W06-1640,P98-1012,0,0.0470628,"d Frank, 2000). The WEKA implementation follows the original RIPPER specification. We changed the implementation to incorporate the modifications suggested by the StRip algorithm; we also modified the underlying data representations and data handling techniques for efficiency. Also due to efficiency considerations, we train StRip only on the 200-document training set. 6.3 6.4 Evaluation In addition to the baselines described above, we evaluate StRip both with and without unlabeled data. That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs. We use the B 3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. B 3 is a measure widely used for evaluating coreference resolution algorithms. The measure computes the precision and recall for each NP mention in a document, and then averages them to produce combined results for the entire output. More precisely, given a mention i that has been assigned to chain ci , the precision for mention i is defined as the number of correctly identified mentions in ci divided by the total number of mentions in ci . Recall for i is defined as the number of correctly"
W06-1640,P02-1014,1,0.324398,"oussaoui. 336 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336–344, c Sydney, July 2006. 2006 Association for Computational Linguistics [Source Zacarias Moussaoui] [− complained] at length today about [Target his own lawyer], telling a federal court jury that [Target he] was [− more interested in achieving fame than saving Moussaoui’s life]. At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of a new machine learning setting that we refer to as partially supervised clustering. In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or t"
W06-1640,P04-1035,0,0.160034,"w, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and Related Work Work relevant to our problem can be split into three main areas – sentiment analysis, traditional noun phrase coreference resolution, and supervised and weakly supervised clustering. Related work in the former two areas is summarized briefly below. Supervised and weakly supervised clustering approaches are discussed in Section 4. Sentiment analysis. Much of the relevant research in sentiment analysis addresses sentiment classification, a text categorization task of extracting opinion at the coarse-grained do"
W06-1640,W02-1011,0,0.0254557,"conduct shows [Target he] is [− not stable mentally], and thus [− undeserving] of [Target the ultimate punishment]. Moussaoui − Zerkin − prison for life −/ + − ultimate punishment Figure 1: Example text containing opinions (above) and a summary of the opinions (below). Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional"
W06-1640,W03-1014,0,0.021084,"in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and Related Work Work relevant to our problem can be split into three main areas – sentiment analysis, traditional noun phrase coreference resolut"
W06-1640,H05-1045,1,0.337874,"d phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and Related Work Work relevant to our problem can be split into three main areas – sentiment analysis, traditional noun phrase coreference resolution, and supervised and weakly supervised clustering. Related work in the former two areas is sum"
W06-1640,W06-0302,1,0.488977,"nce resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). 3 1. Source-to-NP mapping: We preprocess each document by running a tokenizer, sentence splitter, POS tagger, parser, and an NP finder. Subsequently, we augment the set of NPs found by the NP finder with the help of a system for named entity detection. We then map the sources to the NPs. Since there is no one-to-one correspondence, we use a set of heuristics to create the mapping. More details about why heuristics are needed and the process used to map sources to NPs can be found in Stoyanov and Cardie (2006). 2. Feature vector creation: We extract a feature vector for every pair of NPs from the preprocessed corpus. We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. 3. Classifier construction: Using the feature vectors from step 2, we construct a training set containing one training example per document. Each training example consists of the feature vectors for all pairs of NPs in the document, including those that do not map to sources, together with the available coreference information for the source noun phrases (i.e. the noun phrases to which source"
W06-1640,H05-1116,1,0.855016,"te a new algorithm for the task of source coreference resolution that outperforms competitive baselines. 1 Introduction Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text. Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g. Coglianese (2004)). Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al. (2005), Riloff et al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. 1 For simplicity, the example summary does not contain any source/target statistics. 2 In addition, the summary wou"
W06-1640,P02-1053,0,0.00283647,"t he] is [− not stable mentally], and thus [− undeserving] of [Target the ultimate punishment]. Moussaoui − Zerkin − prison for life −/ + − ultimate punishment Figure 1: Example text containing opinions (above) and a summary of the opinions (below). Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information th"
W06-1640,W03-2604,0,0.015685,"t analysis. Much of the relevant research in sentiment analysis addresses sentiment classification, a text categorization task of extracting opinion at the coarse-grained document level. The goal in sentiment classification is to assign to 337 ing marked (manually or automatically) opinion sources. More specifically, the source coreference resolution training phase proceeds through the following steps: characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent). Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering. For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreference classifier is run, followed by a cluster"
W06-1640,W03-1017,0,0.0168028,") or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and Related Work Work relevant to our problem can be split into three main areas – sentiment analysis, traditional noun phrase coreference resolution, and supervised and weakly supervised clustering. Related work in the for"
W06-1640,C98-1012,0,\N,Missing
W06-1651,H05-1045,1,0.554208,"acted from the aforementioned n-best opinion expression and source sequences. The relation classifier is modeled using Markov order-0 CRFs(Lafferty 2 Wiebe et al. (2005) reports human annotation agreement for opinion expression as 82.0 by F1 measure. See Wiebe et al. (2005) for additional details. 432 per sentence, and (4) link relations that span more than one sentence. In addition, the link relation model explicitly exploits mutual dependencies among entities and relations, while Bethard et al. (2004) does not directly capture the potential influence among entities. Kim and Hovy (2005b) and Choi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (20"
W06-1651,I05-2011,0,0.0168719,"and source entities extracted from the aforementioned n-best opinion expression and source sequences. The relation classifier is modeled using Markov order-0 CRFs(Lafferty 2 Wiebe et al. (2005) reports human annotation agreement for opinion expression as 82.0 by F1 measure. See Wiebe et al. (2005) for additional details. 432 per sentence, and (4) link relations that span more than one sentence. In addition, the link relation model explicitly exploits mutual dependencies among entities and relations, while Bethard et al. (2004) does not directly capture the potential influence among entities. Kim and Hovy (2005b) and Choi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (20"
W06-1651,H05-1068,1,0.628041,"im and Hovy (2005b) and Choi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (2005)), omitting source identification altogether. There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies. Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities. This approach however, could not exploit dependencies between relations. Roth and Yih (2004) later formulated global inference using integer linear programming, which is the appr"
W06-1651,W05-0625,0,0.0167612,"Missing"
W06-1651,C04-1197,0,0.182414,"lation extraction, respectively, improving substantially over prior results in the area. Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). Moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently — for semantic role labeling (e.g. Punyakanok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al. (2000), White et al. (2001)). Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al.,"
W06-1651,W04-2401,0,0.374904,"acy. To date, however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them. Without progress on the joint extraction of opinion entities and their relations, the capabilities of opinionbased applications will remain limited. We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis. We identify two types of opinion-related entities — expressions of opinions and sources of opinions — along with the linking relation that exists between them. Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities. Performance further improves when a semantic role labeling system is incorporated. The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area. Fortunately, research in machine learning has produced methods for global inference and joint cl"
W06-1651,C02-1151,0,0.00591837,"ions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (2005)), omitting source identification altogether. There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies. Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities. This approach however, could not exploit dependencies between relations. Roth and Yih (2004) later formulated global inference using integer linear programming, which is the approach that we apply here. In contrast to our work, Roth and Yih (2004) operated in the domain of factual information extraction rather than opinion extraction, and assumed that the exact boundaries of entities from the"
W06-1651,H05-1116,1,0.530822,"nok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al. (2000), White et al. (2001)). Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al., 2005), and opinion-oriented summarization (e.g. Cardie et al. (2004), Liu et al. (2005)). Moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. Choi et al. In this paper, we present a global inference approach (Roth and Yih, 2004) to the extraction of opinion-related entities and relations. In particular, we aim to identify two types of entities (i.e. spans of text): entities that express opinions and entities that denote sources of opinions. More specifically, we use the term opinion expression to de"
W06-1651,H01-1054,1,0.684396,"ependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently — for semantic role labeling (e.g. Punyakanok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al. (2000), White et al. (2001)). Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al., 2005), and opinion-oriented summarization (e.g. Cardie et al. (2004), Liu et al. (2005)). Moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. Choi et al. In this paper, we present a global inference approach (Roth and Yih, 2004) to t"
W06-1651,H05-1044,0,0.196694,"hoi et al. (2005) focus only on the extraction of sources of opinions, without extracting opinion expressions. Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al. (2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources. Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al. (2005) and Wilson et al. (2005)), omitting source identification altogether. There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies. Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities. This approach however, could not exploit dependencies between relations. Roth and Yih (2004) later formulated global inference using integer linear programming, which is the approach that we apply here."
W06-1651,P04-1056,0,0.0158274,"the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities. Performance further improves when a semantic role labeling system is incorporated. The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area. Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). Moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently — for semantic role labeling (e.g. Punyakanok et al. (2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al. (2004)). 1 Introduction Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-speci"
W11-0503,J96-1002,0,0.0152048,"relative position5 and whether they 3 We cannot easily associate each topic with a decision because the number of decisions is not known a priori. 4 Parameter estimation and inference done by GibbsLDA++. 5 Here is the definition for the relative position of pairwise DAs. Suppose there are N DAs in one meeting ordered by time, 19 have the same DA type. We employ Support Vector Machines (SVMs) and Maximum Entropy (MaxEnt) as our learning methods, because SVMs are shown to be effective in text categorization (Joachims, 1998) and MaxEnt has been applied in many natural language processing tasks (Berger et al., 1996). Given an −−→ F Vij , for SVMs, we utilize the decision value of −−→ wT · F Vij + b as the similarity, where w is the weight vector and b is the bias. For MaxEnt, we make use of the probability of P (SameDecision | −−→ F Vij ) as the similarity value. 3.3 Experiments Corpus. We use the AMI meeting Corpus (Carletta et al., 2005), a freely available corpus of multiparty meetings that contains a wide range of annotations. The 129 scenario-driven meetings involve four participants playing different roles on a design team. A short (usually one-sentence) abstract is included that describes each dec"
W11-0503,W09-3934,0,0.433553,"ed summarization setting, and given true clusterings of decisionrelated utterances, we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. In the unsupervised summarization setting,we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23 using SVMs). 1 tle work has focused on decision summarization: Fern´andez et al. (2008a) and Bui et al. (2009) investigate the use of a semantic parser and machine learning methods for phrase- and token-level decision summarization. We believe our work is the first to explore and compare token-level and dialogue act-level approaches — using both unsupervised and supervised learning methods — for summarizing decisions in meetings. Introduction Meetings are a common way for people to share information and discuss problems. And an effective meeting always leads to concrete decisions. As a result, it would be useful to develop automatic methods that summarize not the entire meeting dialogue, but just the"
W11-0503,A00-2004,0,0.046773,"gn team. A short (usually one-sentence) abstract is included that describes each decision, action, or problem discussed in the meeting; and each DA is linked to the abstracts it supports. We use the manually constructed decision abstracts as gold-standard summaries and assume that all decision-related DAs have been identified (but not linked to the decision(s) it supports). Baselines. Two clustering baselines are utilized for comparison. One baseline places all decisionrelated DAs for the meeting into a single partition (A LL I N O NE G ROUP). The second uses the text segmentation software of Choi (2000) to partition the decision-related DAs (ordered according to time) into several topic-based groups (C HOI S EGMENT). Experimental Setup and Evaluation. Results for pairwise supervised clustering were obtained using 3-fold cross-validation. In the current work, stopping conditions for hierarchical agglomerative clustering are selected manually: For the TF-IDF and topic model approaches, we stop when the similarity measure reaches 0.035 and 0.015, respectively; For the SVM and MaxEnt versions, we use 0 and 0.45, respectively. We use the Mallet implementation for MaxEnt and the SVMlight implement"
W11-0503,W08-0125,0,0.053685,"Missing"
W11-0503,D09-1118,0,0.287277,"stering of decision-related DAs. Here we aim to partition the decision-related utterances (DRDAs) according to the decisions each supports. This step is similar in spirit to many standard text summarization techniques (Salton et al., 1997) that begin by grouping sentences according to semantic similarity. Summarization at the DA-level. We select just the important DRDAs in each cluster. Our goal is to eliminate redundant and less informative utterances. The 1 These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008a), Frampton et al. (2009). The latter refer to all DAs that appear in a decision discussion even if they do NOT support any particular decision. 17 selected DRDAs are then concatenated to form the decision summary. Optional token-level summarization of the selected DRDAs. Methods are employed to capture concisely the gist of each decision, discarding any distracting text. Incorporation of the discourse context as needed. We hypothesize that this will produce more interpretable summaries. More specifically, we compare both unsupervised (TFIDF (Salton et al., 1997) and LDA topic modeling (Blei et al., 2003)) and (pairwi"
W11-0503,W06-1643,0,0.439921,"omatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information. Popular supervised methods for summarizing speech — including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) — are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004). Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern´andez et al. (2008b) develop a model of decision-making dialogue structure and detect decision DAs based on it; and Frampton et al. (2009) implement"
W11-0503,C04-1110,0,0.162287,"). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information. Popular supervised methods for summarizing speech — including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) — are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004). Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern´andez et al. (2008b) develop a model of decision-making dialogue structure and detect decision DAs based on it; and Frampton et al. (2009) implement a real-time decision detection system. Fern´andez et al. (2008a) and Bui et al. (2009), however, might be the most relev"
W11-0503,N03-1020,0,0.887363,"partitioning DRDAs according to the decision each supports. We also investigate unsupervised methods and supervised learning for decision summarization at both the DA and token level, with and without the incorporation of discourse context. During training, the supervised decision summarizers are told which DRDAs for each decision are the most informative for constructing the decision abstract. Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. In the supervised summarization setting, our experiments demonstrate that with true clusterings of decision-related DAs, token-level summaries that employ limited discourse context can approach an upper bound for summaries extracted directly from DRDAs2 — 0.4387 ROUGE-F1 vs. 0.5333. When using system-generated DRDA clusterings, the DAlevel summaries always dominate token-level methods in terms of performance. For the unsupervised summarization setting, we investigate the use of both unsupervised and supervised methods for the initial DRDA clustering step."
W11-0503,de-marneffe-etal-2006-generating,0,0.00728913,"0.1727 0.3391 0.3760 0.2903 0.4882 0.2097 0.1427 0.1869 0.1486 0.2349 0.1843 0.2068 0.2056 0.3508 0.2807 0.3583 0.4891 0.1884 0.04968 0.1891 0.0822 0.2197 0.0777 0.2221 0.1288 0.3592 0.3607 0.3418 0.4873 0.2026 0.0885 0.1892 0.0914 0.2348 0.1246 0.2213 0.1393 0.08673 0.1906 0.1957 0.0625 0.0993 0.0868 0.0707 0.1890 0.1979 0.3068 0.0916 0.2057 Table 7: Results for ROUGE-1: Summary Generation Using Supervised Learning et al. (2008a), we design features that encode (a) basic predicate-argument structures involving major phrase types (S, VP, NP, and PP) and (b) additional typed dependencies from Marneffe et al. (2006). We use the Stanford Parser. 5 Experiments Experiments based on supervised learning are performed using 3-fold cross-validation. We train two different types of classifiers for identifying informative DAs or tokens: Conditional Random Fields (CRFs) (via Mallet) and Support Vector Machines (SVMs) (via SVMlight ). We remove function words from DAs before using them as the input of our systems. The AMI decision abstracts are the gold-standard summaries. We use the ROUGE (Lin and Hovy, 2003) evaluation measure. ROUGE is a recall-based method that can identify systems producing succinct and descri"
W11-0503,J02-4003,0,0.517629,"the supervised summarization setting, we observe that including additional discourse context boosts performance only for token-level summaries. 2 The upper bound measures the vocabulary overlap of each gold-standard decision summary with the complete text of all of its associated DRDAs. 2 Related Work There exists much previous research on automatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information. Popular supervised methods for summarizing speech — including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) — are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004). Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in rec"
W11-1920,W11-1901,0,0.0584979,"Missing"
W11-1920,J01-4004,0,0.0896323,"pair generation to refer to the process of creating the CE pairs that the classifier considers. The most straightforward way of generating pairs is by enumerating all possible unique combinations. This approach has two undesirable properties – it re124 quires time in the order of O(n2 ) for a given document (where n is the number of CEs in the document) and it produces highly imbalanced data sets with the number of positive instances (i.e., coreferent CEs) being a small fraction of the number of negative instances. The latter issue has been addressed by a technique named instance generation (Soon et al., 2001): during training, each CE is matched with the first preceding CE with which it corefers and all other CEs that reside in between the two. During testing, a CE is compared to all preceding CEs until a coreferent CE is found or the beginning of the document is reached. This technique reduces class imbalance, but it has the same worst-case runtime complexity of O(n2 ). We employ a new type of pair generation that aims to address both the class imbalance and improves the worst-case runtime. We will use S MART PG to refer to this component. Our pair generator relies on linguistic intuitions and is"
W11-1920,P09-1074,1,0.866166,"scribing the general Reconcile architecture (Section 2), then describe the changes that we incorporated in order to enable Reconcile to work on OntoNotes data (Sections 3 and 4). Finally, we describe our experimental set up and results from running ReconcileCoN LL under different conditions (Section 5). 2 Overview of Reconcile In this section we give a high-level overview of the Reconcile platform. We refer the reader for more details to Stoyanov et al. (2010a) and Stoyanov et al. (2010b). Results from running a Reconcilebased coreference resolution system on different corpora can be found in Stoyanov et al. (2009). Reconcile was developed to be a coreference resolution research platform that allows for quick implementation of coreference resolution systems. The platform abstracts the major processing steps (components) of current state-of-the-art learningbased coreference resolution systems. A description of the steps and the available components can be found in the referenced papers. 3 The ReconcileCoN LL System To participate in the 2011 CoNLL shared task, we configured Reconcile to conform to the OntoNotes general coreference resolution task. We will use the name ReconcileCoN LL , to refer to this p"
W11-1920,P10-2029,1,0.858489,"Missing"
W12-0404,P09-2078,0,0.0587678,"Missing"
W12-0404,P11-1032,1,0.845144,"n Non-gold Standard Approaches 3.1.1 Limitations Manual annotation of deception is problematic for a number of reasons. First, many of the same challenges that face manual annotation efforts in other domains also applies to annotations of deception. For example, manual annotations can be expensive to obtain, especially in large-scale settings, e.g., the web. Most seriously however, is that human ability to detect deception is notoriously poor (Bond and DePaulo, 2006). Indeed, recent studies have confirmed that human agreement and deception detection performance is often no better than chance (Ott et al., 2011); this is especially the Recently, alternative approaches have emerged to study deception in the absence of gold standard deceptive data. These approaches can typically be broken up into three distinct types. In Section 3.1, we discuss approaches to deception corpus creation that rely on the manual annotation of deceptive instances in the data. In Section 3.2, we discuss approaches that rely on heuristic methods for deriving approximate, but non-gold standard deception labels. In Section 3.3, we discuss a recent approach that uses assumptions about the effects of deception to identify examples"
W12-1605,I08-1018,0,0.175291,", 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fern´andez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both 42 utterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010). Mostly, the sentences are ranked according to importance based on latent topic structures, and top ones are selected as the summary. There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011). Different from their work, we further investigate the topic models of fine granularity on sentence level and leverage context information for decision summarization task. Most existing approaches for speech summari"
W12-1605,N10-1122,0,0.20975,"ted as: S ∗ = arg min KL(PC ||PS ) = arg min S:|S|&lt;θ 4 X S:|S|&lt;θ T i P (Ti |C)log P (Ti |C) P (Ti |S) Topic Models In this section, we briefly describe the three finegrained topic models employed to compute the latent topic distributions on utterance level in the meetings. According to the input of Algorithm 1, we are interested in estimating the topic distribution for each DA P (T |DA) and the word distribution for each topic P (w|T ). For MG-LDA, P (T |DA) is computed as the expectation of local topic distributions with respect to the window distribution. 4.1 Local LDA Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), except that it treats each sentence as a separate document2 . Each DA d is generated as follows: 1. For each topic k: (a) Choose word distribution: φk ∼ Dir(β) 2. For each DA d: (a) Choose topic distribution: θd ∼ Dir(α) (b) For each word w in DA d: i. Choose topic: zd,w ∼ θd ii. choose word: w ∼ φzd,w 4.2 Multi-grain LDA Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) can model both the meeting specific topics (e.g. the design of a remote control) and various concrete aspects (e."
W12-1605,W09-3934,0,0.135358,"d be to add complementary knowledge when the DRDAs cannot provide complete information. Therefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky1 These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008), Frampton et al. (2009). 41 ilmaz and Hakkani-Tur, 2010). Thus, topic models appear to be a better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings. In contrast to previous work, we study the unsupervised token-level decision summarization in meetings by identifying a concise set of key words or phrases, w"
W12-1605,P10-1084,0,0.0180247,". (Fern´andez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both 42 utterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010). Mostly, the sentences are ranked according to importance based on latent topic structures, and top ones are selected as the summary. There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011). Different from their work, we further investigate the topic models of fine granularity on sentence level and leverage context information for decision summarization task. Most existing approaches for speech summarization result in a selection of utterances from the dialogue, which"
W12-1605,D09-1118,0,0.0309555,"DRDAs cannot provide complete information. Therefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky1 These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008), Frampton et al. (2009). 41 ilmaz and Hakkani-Tur, 2010). Thus, topic models appear to be a better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings. In contrast to previous work, we study the unsupervised token-level decision summarization in meetings by identifying a concise set of key words or phrases, which can either be output as a compact summary or"
W12-1605,W06-1643,0,0.0883874,"oposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decision Abstracts (Summary) D ECISION 1: The targ"
W12-1605,N09-1041,0,0.397634,"n” is not specified in any of the listed DRDAs supporting it. By looking at the transcript, we find “power button” mentioned in one of the preceding, but not decision-related DAs. Consequently another challenge would be to add complementary knowledge when the DRDAs cannot provide complete information. Therefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky1 These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008), Frampton et al. (2009). 41 ilmaz and Hakkani-Tur, 2010). Thus, topic models appear to be a better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approa"
W12-1605,P10-1009,0,0.017854,"evel summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decision Abstracts (Summary) D ECISION 1: The target group comprises o"
W12-1605,N03-1020,0,0.0186919,"gs. In the True Clusterings setting, we use the AMI annotations to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) clustering method groups DRDAs according to their LDA topic distribution similarity. As better approaches for DRDA clustering become available, they could be employed instead. Evaluation Metric. To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics. We use the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries, same as Riedhammer et al. (2010) do. Inference and Hyperparameters We use the implementation from (Lu et al., 2011) for the three topic models in Section 4. The collapsed Gibbs Sampling approach (Griffiths and Steyvers, 2004) is exploited for inference. Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al., 2010). In LDA and LocalLDA, α and β are both set to 0.1 . For MG-LDA, αgl , αloc an"
W12-1605,N09-1070,0,0.0814448,"Missing"
W12-1605,W10-4211,0,0.0642725,"pproach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decision Abstracts (Summary) D ECISION 1: The target group comprises of individuals who can"
W12-1605,W11-0503,1,0.848553,"teristics of dialogue. Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008). For unsupervised methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010). Gillick et al. (2009) introduce a conceptbased global optimization framework by using integer linear programming (ILP). Only in very recent works has decision summarization been addressed in (Fern´andez et al., 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fern´andez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both 42 utterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009;"
W12-1605,J02-4003,0,0.257571,"the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decisio"
W12-1605,N10-1132,0,\N,Missing
W12-1605,N10-1006,0,\N,Missing
W12-1614,P06-4018,0,0.0174553,"er Stemming, PTB-style tokenization8 , and hand-crafted rules for matching tokens to entries in the polarity and General Inquirer lexicons. Then, feature selection is performed via forward selection, in which we start with the single bestperforming feature and, in each iteration, add the feature that improves the F1 -score the most, until no significant improvement can be made. Once the 6 Some prior work uses a different experimental setting. For instance, Zhou et al. (2010) only considers two of the nonexplicit relations, namely Implicit and NoRel. 7 We use classifiers from the nltk package (Bird, 2006). 8 Stanford Parser (Klein and Manning, 2003). 110 optimal feature set for each relation sense is determined by testing on the validation set, we retrain each classifier using the entire training set and report final performance on the test set. 5 Results and Analysis Table 5 indicates the performance achieved by employing the feature set found to be optimal for each relation sense via forward selection, along with the performance of the individual features that constitute the ideal subset. The two bottom rows show the results reported in two previous papers with the most similar experiment me"
W12-1614,N07-1054,0,0.0268671,"Missing"
W12-1614,P03-1054,0,0.0279844,"Missing"
W12-1614,D09-1036,0,0.706336,"— is much harder. It has been the subject of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.1 Recent studies, for example, explored the utility of various classes of features for the task, including linguistically informed features, context, constituent and dependency parse features, and features that encode entity information or rely on language models (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Zhou et al., 2010). To date, however, there has not been a systematic study of combinations of these features for implicit discourse relation identification. In addition, the results of existing studies are often difficult to compare because of differences in data set creation, feature set choice, or experimental methodology. This paper provides a systematic study of previously proposed features for implicit discourse relation identification and identifies feature combinations that optimize F1 -score using forward selection (John et al., 1994). We report the performance o"
W12-1614,W10-4310,0,0.297268,"It has been the subject of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.1 Recent studies, for example, explored the utility of various classes of features for the task, including linguistically informed features, context, constituent and dependency parse features, and features that encode entity information or rely on language models (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Zhou et al., 2010). To date, however, there has not been a systematic study of combinations of these features for implicit discourse relation identification. In addition, the results of existing studies are often difficult to compare because of differences in data set creation, feature set choice, or experimental methodology. This paper provides a systematic study of previously proposed features for implicit discourse relation identification and identifies feature combinations that optimize F1 -score using forward selection (John et al., 1994). We report the performance of our binary (one vs"
W12-1614,P02-1047,0,0.691062,"ion and identifies feature combinations that optimize F1 -score using forward selection (John et al., 1994). We report the performance of our binary (one vs. rest) classifiers on the PDTB data set for its four top-level discourse relation classes: C OMPARISON, C ONTINGENCY, E XPANSION, and T EMPORAL. In each case, the resulting classifiers achieve the best F1 -scores for the PDTB to date. We 1 Research on implicit discourse relation recognition prior to the release of the PDTB instead relied on synthetic data created by removing explicit connectives from explicit discourse relation instances (Marcu and Echihabi, 2002), but the trained classifiers do not perform as well on real-world data (BlairGoldensohn et al., 2007). 108 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 108–112, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up. Finally, by documenting an easily replicable experimental methodology and making public the code for feature extraction2 , we hope to provide a new set of ba"
W12-1614,J93-2004,0,0.0400324,"relation sense determines the relation that exists between its text span arguments as one of: C OMPARISON, C ONTIN GENCY , E XPANSION , and T EMPORAL . For example, the following shows an explicit C ONTINGENCY relation between argument1 (arg1) and argument2 (arg2), denoted via the connective “because”: The federal government suspended sales of U.S. savings bonds because Congress hasn’t listed the ceiling on government debt. The four relation senses comprise the target classes for our classifiers. A notable feature of the PDTB is that the annotation is done on the same corpus as Penn Treebank (Marcus et al., 1993), which provides parse trees and part-of-speech (POS) tags. This enables the use of gold standard parse information for some features, e.g., the production rules feature, one of the most effective features proposed to date. 3 Features Below are brief descriptions of features whose efficacy have been empirically determined in prior works3 , along with the rationales behind them: 2 These are available from http://www.joonsuk.org. Word Pairs (Marcu and Echihabi, 2002). First-Last-First3 (Wellner et al., 2006). Polarity, Verbs, Inquirer Tags, Modality, Context (Pitler et al., 2009). Production Rul"
W12-1614,C08-2022,0,0.790903,"ul, recently discovered features. Our results constitute a new set of baselines for future studies of implicit discourse relation identification. 1 Introduction The ability to recognize the discourse relations that exist between arbitrary text spans is crucial for understanding a given text. Indeed, a number of natural language processing (NLP) applications rely on it — e.g., question answering, text summarization, and textual entailment. Fortunately, explicit discourse relations — discourse relations marked by explicit connectives — have been shown to be easily identified by automatic means (Pitler et al., 2008): each such connective is generally strongly coupled with a particular relation. The connective “because”, for example, serves as a prominent cue for the C ONTIN GENCY relation. The identification of implicit discourse relations — where such connectives are absent — is much harder. It has been the subject of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.1 Recent studies, for"
W12-1614,P09-1077,0,0.667085,"nnectives are absent — is much harder. It has been the subject of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.1 Recent studies, for example, explored the utility of various classes of features for the task, including linguistically informed features, context, constituent and dependency parse features, and features that encode entity information or rely on language models (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Zhou et al., 2010). To date, however, there has not been a systematic study of combinations of these features for implicit discourse relation identification. In addition, the results of existing studies are often difficult to compare because of differences in data set creation, feature set choice, or experimental methodology. This paper provides a systematic study of previously proposed features for implicit discourse relation identification and identifies feature combinations that optimize F1 -score using forward selection (John et al., 1994). We report"
W12-1614,prasad-etal-2008-penn,0,0.925771,"ation, and textual entailment. Fortunately, explicit discourse relations — discourse relations marked by explicit connectives — have been shown to be easily identified by automatic means (Pitler et al., 2008): each such connective is generally strongly coupled with a particular relation. The connective “because”, for example, serves as a prominent cue for the C ONTIN GENCY relation. The identification of implicit discourse relations — where such connectives are absent — is much harder. It has been the subject of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.1 Recent studies, for example, explored the utility of various classes of features for the task, including linguistically informed features, context, constituent and dependency parse features, and features that encode entity information or rely on language models (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Zhou et al., 2010). To date, however, there has not been a systematic study of combinations of these features"
W12-1614,H05-1044,0,0.00626088,"2009). Production Rules (Lin et al., 2009). 3 109 Word Pairs (cross product of unigrams: arg1 × arg2) — A few of these word pairs may capture information revealing the discourse relation of the target spans. For instance, rain-wet can hint at C ON TINGENCY . First-Last-First3 (the first, last, and first three words of each argument) — The words in this range may be expressions that function as connectives for certain relations. Polarity (the count of words in arg1 and arg2, respectively, that hold negated vs. non-negated positive, negative, and neutral sentiment) according to the MPQA corpus (Wilson et al., 2005)) — The change in sentiment from arg1 to arg2 could be a good indication of C OMPARISON. Inquirer Tags (negated and non-negated finegrained semantic classification tags for the verbs in each argument and their cross product) — The tags are drawn from the General Inquirer Lexicon (Stone et al., 1966)4 , which provides word level relations that might be propagated to the target spans’ discourse relation, e.g., rise:fall. Verbs (count of pairs of verbs from arg1 and arg2 belonging to the same Levin English Verb Class (Levin and Somers, 1993)5 , the average lengths of verb phrases as well as their"
W12-1614,C10-2172,0,0.76479,"ect of much recent research since the release of the Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), which annotates relations between adjacent text spans in Wall Street Journal (WSJ) articles, while clearly distinguishing implicit from explicit discourse relations.1 Recent studies, for example, explored the utility of various classes of features for the task, including linguistically informed features, context, constituent and dependency parse features, and features that encode entity information or rely on language models (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Zhou et al., 2010). To date, however, there has not been a systematic study of combinations of these features for implicit discourse relation identification. In addition, the results of existing studies are often difficult to compare because of differences in data set creation, feature set choice, or experimental methodology. This paper provides a systematic study of previously proposed features for implicit discourse relation identification and identifies feature combinations that optimize F1 -score using forward selection (John et al., 1994). We report the performance of our binary (one vs. rest) classifiers"
W12-1642,P08-1004,0,0.0310767,"Missing"
W12-1642,W09-3934,0,0.658826,"gonna have”), but do not themselves describe the decision. We will refer to this portion of a DRDA (underlined in Figure 1) as the Decision Cue. Moreover, the decision cue is generally directly followed by the actual Decision Content (e.g., “be a little apple”, “have rubber cases”). Decision Content phrases are denoted in Figure 1 via italics and square brackets. Importantly, it is just the decision content portion of the utterance that should be considered for incorporation into the focused summary. 1 These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of (Bui et al., 2009), (Fern´andez et al., 2008), (Frampton et al., 2009). 2 Murray et al. (2010b) show that users much prefer abstractive summaries over extracts when the text to be summarized is a conversation. In particular, extractive summaries drawn from group conversations can be confusing to the reader without additional context; and the noisy, error-prone, disfluent text of speech transcripts is likely to result in extractive summaries with low readability. 304 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304–313, c Seoul, South Korea, 5-6"
W12-1642,P11-1054,0,0.436064,"8), Bui et al. (2009)), we view the problem as an information extraction task and hypothesize that existing methods for domain-specific relation extraction can be modified to identify salient phrases for use in generating abstractive summaries. Very generally, information extraction methods identify a lexical “trigger” or “indicator” that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the “target phrase” or “argument constituent” to be extracted. Relation instances, then, are represented by indicator-argument pairs (Chen et al., 2011). Figure 1 shows some possible indicator-argument pairs for identifying the Decision Content phrases in the dialogue sample. Content indicator words 305 are shown in italics; the Decision Content target phrases are the arguments. For example, in the fourth DRDA, “require” is the indicator, and “rubber buttons” and “rubber case” are both arguments. Although not shown in Figure 1, it is also possible to identify relations that correspond to the Decision Cue phrases.3 Specifically, we focus on the task of decision summarization and, as in previous work in meeting summarization (e.g., Fern´andez e"
W12-1642,de-marneffe-etal-2006-generating,0,0.0146691,"Missing"
W12-1642,D09-1118,0,0.0124431,"he decision. We will refer to this portion of a DRDA (underlined in Figure 1) as the Decision Cue. Moreover, the decision cue is generally directly followed by the actual Decision Content (e.g., “be a little apple”, “have rubber cases”). Decision Content phrases are denoted in Figure 1 via italics and square brackets. Importantly, it is just the decision content portion of the utterance that should be considered for incorporation into the focused summary. 1 These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of (Bui et al., 2009), (Fern´andez et al., 2008), (Frampton et al., 2009). 2 Murray et al. (2010b) show that users much prefer abstractive summaries over extracts when the text to be summarized is a conversation. In particular, extractive summaries drawn from group conversations can be confusing to the reader without additional context; and the noisy, error-prone, disfluent text of speech transcripts is likely to result in extractive summaries with low readability. 304 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304–313, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Lingui"
W12-1642,W06-1643,0,0.588542,"baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogue acts (DRDAs) — utterances associated with o"
W12-1642,D09-1044,0,0.232806,"adopt here the two unsupervised baselines (utterance-level summaries) from that work for use in our evaluation. We further employ their supervised summarization methods as comparison points for token-level summarization, adding additional features for consis306 tency with the other approaches in the evaluation. Murray et al. (2010a) develop an integer linear programming approach for focused summarization at the utterance-level, selecting sentences that cover more of the entities mentioned in the meeting as determined through the use of an external ontology. The most relevant previous work is Hachey (2009), which uses relational representations to facilitate sentence-ranking for multi-document summarization. The method utilizes generic relation extraction to represent the concepts in the documents as relation instances; summaries are generated based on a set cover algorithm that selects a subset of the sentences that best cover the weighted concepts. Thus, the goal of Hachey’s approach is sentence extraction rather than phrase extraction. Although his relation extraction method, like ours (see Section 4), is probabilistic and unsupervised (he uses Latent Dirichelt Allocation (Blei et al., 2003)"
W12-1642,P03-1054,0,0.00469157,"opriate for our decision summarization setting. Nevertheless, we will adapt his approach for comparison with our relation-based summarization technique and include it for evaluation. 3 Focused Summarization as Relation Extraction Given the DRDAs for each meeting grouped (not necessarily correctly) according to the decisions they support, we put each cluster of DRDAs (ordered according to time within the cluster) into one “decision document”. The goal will be to produce one decision abstract for each such decision document. We obtain constituent and dependency parses using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). With the corpus of constituent-parsed decision documents as the input, we will use and modify Chen et al. (2011)’s system to identify decision cue relations and decision content relations for each cluster.4 (Section 6 will make clear how the learned decision cue relations will be used to identify decision content relations.) The salient decision content relation instances will be returned as decision summary com4 Other unsupervised relation learning methods might also be appropriate (e.g., Open IE (Banko et al., 2007)), but they generally model relations between pa"
W12-1642,P10-1009,0,0.0200589,"ell as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogue acts (DRDAs) — utterances associated with one or more decisions."
W12-1642,N03-1020,0,0.193241,"ts. We consider two system input settings. In the True Clusterings setting, we use the AMI annotations to create perfect partitionings of the DRDAs for input to the summarization system; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) clustering method groups DRDAs according to their LDA topic distribution similarity. As better approaches for DRDA clustering become available, they could be employed instead. Evaluation Metrics. We use the widely accepted ROUGE (Lin and Hovy, 2003) evaluation measure. We adopt the ROUGE-1 and ROUGE-SU4 metrics from (Hachey, 2009), and also use ROUGE2. We choose the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries. Training and Parameters. The Dirichlet hyperparameters are set to 0.1 for the priors. When training the model, ten random restarts are performed and each run stops when reaching a convergence threshold (10−5 ). Then we select the posterior with the lowest final free energy. For the parameters used in posterior constraints, we either adopt them fr"
W12-1642,D08-1081,0,0.0194305,"ecision cue relation if any (before, after) Table 3: Additional features for Decision Content relation extraction, inspired by Decision Cue relations. Both indicator and argument use those features. Table 1: Features for Decision Cue and Decision Content relation extraction. All features, except the last type of features, are used for both the indicator and argument. (An Adjacency Pair (AP) is an important conversational analysis concept (Schegloff and Sacks, 1973). In the AMI corpus, an AP pair consists of a source utterance and a target utterance, produced by different speakers.) ley, 2006; Murray and Carenini, 2008; Fern´andez et al., 2008; Wang and Cardie, 2011). Features employed only for argument’s are listed in the last category in Table 1. After applying the features in Table 1 and the global constraints from Section 5 in preliminary experiments, we found that the extracted relation instances are mostly derived from decision cue relations. Sample decision cue relations and instances are displayed in Table 2 and are not necessarily surprising: previous research (Hsueh and Moore, 2007) has observed the important role of personal pronouns, such as “we” and “I”, in decision-making expressions. Notably,"
W12-1642,W10-4211,0,0.54678,"neric relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogue acts (DRDAs) — utterances associated with one or more decisions.1 Each DRDA is labele"
W12-1642,2007.sigdial-1.4,0,0.174245,"I SION 1, 2 or 3. Also shown is the gold-standard (manual) abstract (summary) for each decision. Colors indicate overlapping vocabulary between utterances and the summary. Underlining, italics, and [bracketing] are decscribed in the running text. This paper presents an unsupervised framework for focused meeting summarization that supports the generation of abstractive summaries. (Note that we do not currently generate actual abstracts, but rather aim to identify those Content phrases that should comprise the abstract.) In contrast to existing approaches to focused meeting summarization (e.g., Purver et al. (2007), Fern´andez et al. (2008), Bui et al. (2009)), we view the problem as an information extraction task and hypothesize that existing methods for domain-specific relation extraction can be modified to identify salient phrases for use in generating abstractive summaries. Very generally, information extraction methods identify a lexical “trigger” or “indicator” that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the “target phrase” or “argument constituent” to be extracted. Relation instances, then, are represented by in"
W12-1642,W11-0503,1,0.660663,"ws some possible indicator-argument pairs for identifying the Decision Content phrases in the dialogue sample. Content indicator words 305 are shown in italics; the Decision Content target phrases are the arguments. For example, in the fourth DRDA, “require” is the indicator, and “rubber buttons” and “rubber case” are both arguments. Although not shown in Figure 1, it is also possible to identify relations that correspond to the Decision Cue phrases.3 Specifically, we focus on the task of decision summarization and, as in previous work in meeting summarization (e.g., Fern´andez et al. (2008), Wang and Cardie (2011)), assume that all decision-related utterances (DRDAs) have been identified. We adapt the unsupervised relation learning approach of Chen et al. (2011) to separately identify relations associated with decision cues vs. the decision content within DRDAs by defining a new set of task-specific constraints and features to take the place of the domain-specific constraints and features of the original model. Output of the system is a set of extracted indicator-argument decision content relations (see the “O UR M ETHOD” sample summary of Table 6) that can be used as the basis of the decision abstract"
W12-1642,J02-4003,0,0.146497,"vised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogu"
W12-1642,N10-1132,0,\N,Missing
W14-2105,D12-1091,0,0.0512542,"Missing"
W14-2105,P12-2041,0,0.446307,"articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument. Consider the following examples: The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument.1 This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition as"
W14-2105,J93-2004,0,0.0528511,"3.2 Features The features are binary-valued, and the feature vector for each data point is normalized to have the unit length: “Presence” features are binary features indicating whether the given feature is present in the proposition or not; “Count” features 32 tively verify whether or not the governor verbally expressed his appreciation of peanuts. To realize this intuition, we use syntactic parse trees generated by the Stanford Parser (De Marneffe et al., 2006). In particular, Penn Treebank 2 Tags contain a clause-level tag SBAR denoting a “clause introduced by a subordinating conjunction” (Marcus et al., 1993). The “that” clause in proposition 10 spans a subtree rooted by SBAR, whose left-most child has a lexical value “that.” Similarly, the subordinate (non-italicized) clause in proposition 9 falls in a subtree rooted by SBAR, whose only child is S. Once the main clause of a given proposition is identified, all features set off by the clause are tagged as “core” and the rest are tagged as ”accessory.” If a speech event is present, the tags are flipped. are numeric counts of the occurrence of each feature is converted to a set of three binary features each denoting 0, 1 and 2 or more occurrences. W"
W14-2105,W12-3810,0,0.0267106,"iated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not have adequate support and an automatic means of analysis can be useful. Subjectivity Detection Work to distinguish subjective from objective propositions (e.g.(Wiebe and Riloff, 2005)), often a subtask for sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as an6 Conclusions and Future Work We have proposed a novel task of automatically classifying each proposition as U N V ERIFIABLE, V ERIFIABLE N ON E XPERIENTIAL, or V ERIFI ABLE E XPERIENTIAL , where the appropriate type of support is reason, evidence, and optional evidence, respectively. This classification, once the existing support relations among propositions are identified, can provide an est"
W14-2105,de-marneffe-etal-2006-generating,0,0.0328807,"Missing"
W14-2105,P11-1099,0,0.602001,", USA jpark@cs.cornell.edu Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu Abstract premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument. Consider th"
W14-2105,reed-etal-2008-language,0,0.0531215,"Missing"
W14-2105,W95-0112,0,0.0842147,"hose only child is S. Once the main clause of a given proposition is identified, all features set off by the clause are tagged as “core” and the rest are tagged as ”accessory.” If a speech event is present, the tags are flipped. are numeric counts of the occurrence of each feature is converted to a set of three binary features each denoting 0, 1 and 2 or more occurrences. We first tried a binning method with each digit as its own interval, resulting in binary features of the form featCntn , but the three-interval approach proved to be better empirically, and is consistent with the approach by Riloff and Shoen (1995). The features can be grouped into three categories by purpose: Verifiability-specific (VER), Experientiality-specific (EXP) and Basic Features, each designed to capture the given proposition’s verifiability, experientiality, and both, respectively. Now we discuss the features in more detail. 3.2.1 Basic Features N-gram Presence A set of binary features denote whether a given unigram or bigram occurs in the proposition. The intuition is that by examining the occurrence of words or phrases in V ERIFN ON , V ERIFEXP , and U N V ERIF propositions, the classes that have close ties to certain words"
W14-2105,W10-0214,0,0.0284644,"ormative feaFeature Set UNI+BI+VER+EXP + should,StrSentClue&gt;2 , VB&gt;2 U N V ERIF - StrSentClue0 , VBD&gt;2 , air, since, no one, allergic, not an + die, death, reaction, person, allergen, airV ERIFN ON borne, no one, allergies - PER1st , should + VBD&gt;2 , PER1st , i have, his, he, him, time ! V ERIFEXP - VBZ&gt;2 , PER2nd Table 6: Most Informative Features for UNI+BI+VER+EXP 10 Features with the largest weight (magnitude) with respect to each class ( + : positive weight / - : negative weight). 35 swering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify opinionated propositions expressing beliefs, leveraging from previous work in sentiment analysis and belief tagging. While the class of subjective propositions in subjectivity detection strictly contains U N V ERIF propositions, it also partially overlaps with the V ERIFEXP and V ERIFN ON classes of our work: We want to identify verifiable assertions within propositions, rather than determine the subjectivity of the proposition as a whole (e.g. proposition 8 in Table 1 is classified as a V ERIFN ON , though “Clearly” is subjective.). We also d"
W14-2105,2007.sigdial-1.5,0,0.0369451,"ative Features The most informative feaFeature Set UNI+BI+VER+EXP + should,StrSentClue&gt;2 , VB&gt;2 U N V ERIF - StrSentClue0 , VBD&gt;2 , air, since, no one, allergic, not an + die, death, reaction, person, allergen, airV ERIFN ON borne, no one, allergies - PER1st , should + VBD&gt;2 , PER1st , i have, his, he, him, time ! V ERIFEXP - VBZ&gt;2 , PER2nd Table 6: Most Informative Features for UNI+BI+VER+EXP 10 Features with the largest weight (magnitude) with respect to each class ( + : positive weight / - : negative weight). 35 swering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify opinionated propositions expressing beliefs, leveraging from previous work in sentiment analysis and belief tagging. While the class of subjective propositions in subjectivity detection strictly contains U N V ERIF propositions, it also partially overlaps with the V ERIFEXP and V ERIFN ON classes of our work: We want to identify verifiable assertions within propositions, rather than determine the subjectivity of the proposition as a whole (e.g. proposition 8 in Table 1 is classified as a V ERIFN ON , though “Clea"
W14-2105,W05-0308,0,0.0229049,"n is distinct in imaginative vs. informative writing. We expect this feature to distinguish U N V ERIF propositions from the rest. Sentiment Clue Count Wilson et al. (2005) provides a subjectivity clue lexicon, which is a list of words with sentiment strength tags, either strong or weak, along with additional information, such as the sentiment polarity, Part-of-Speech Count (POS), etc. We suspect that propositions containing more sentiment words is more likely to be U N V ERIF. Speech Event Count We use the 50 most frequent Objective-speech-event text anchors crawled from the MPQA 2.0 corpus (Wilson and Wiebe, 2005) as a speech event lexicon. The speech event text anchors refer to words like “stated” and “wrote” that introduce written or spoken propositions attributed to a source. propositions containing speech events such as proposition 10 in Table 1 are generally V ERIFN ON or V ERIFEXP , since whether the attributed source has indeed made the proposition he allegedly made is objectively verifiable regardless of the subjectivity of the proposition itself. Imperative Expression Count Imperatives, i.e. commands, are generally U N V ERIF (e.g. “Do the homework now!” that is, we expect there to be no objec"
W14-2105,H05-1044,0,0.078957,"Missing"
W14-2617,W11-0702,0,0.854029,") of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a"
W14-2617,P12-1042,0,0.0411988,"Missing"
W14-2617,D10-1121,0,0.119061,"t features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. 3 The Model We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as"
W14-2617,W11-0707,0,0.788166,"ructure, where turns starting with &gt; are response for most previous turn that with one less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always pe"
W14-2617,D12-1006,0,0.40296,"near chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. 1 Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Linguistics Zer0faults: So questions comments feedback welcome"
W14-2617,D09-1062,1,0.31828,"l language used. As an example, consider a snippet of discussion from Wikipedia Talk page for article “Iraq War” where editors argue on the correctness of the information in the opening paragraph (Figure 1). “So what?” should presumably be tagged as a negative sentence as should the sentence “If you’re going to troll, do us all a favor and stick to the guidelines.”. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009). As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an exi"
W14-2617,J93-3003,0,0.0611429,"e Section 4). The parameters can be found by maximizing the likelihood subject to the monotonicity constraints. We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization problem, and refer the readers to Mao and Lebanon (2007) for more details.2 3.3 Table 1: Features used in sentiment prediction. eralizing a word to its POS tag in turn. For instance, “nsubj(wrong, you)” is generlized as the “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations. Discourse Features. Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Hedge words are collected from the CoNLL-2012 shared task (Farkas et al., 2010). Conversation Features. Conversation features encode some useful information regarding the similarity between the current utterance(s) and the sentences uttered by the target participant. TFIDF similarity is computed. We also check if the current utterance(s) quotes targe"
W14-2617,de-marneffe-etal-2006-generating,0,0.0408168,"Missing"
W14-2617,P09-2079,0,0.0426911,"Missing"
W14-2617,esuli-sebastiani-2006-sentiwordnet,0,0.00901566,"N N ] However I agree that the emphasis needs to be on the armaments crisis because it was the reason sold to the public and the major one used to justify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.[P P ] ... agreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide"
W14-2617,W10-3001,0,0.02276,"Missing"
W14-2617,P13-1174,0,0.0140273,"e is no lexicon available for online discussions. Thus, we create from a large-scale corpus via label propagation. The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2. Edge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector ~a using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define “co-occur” as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine similarity between ~a0 and"
W14-2617,W13-4006,0,0.2293,"n the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each to"
W14-2617,E12-1079,0,0.0146493,"ant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. 3 The Model We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as the backbone approach for our sentence- or segment-level agreement and disagreement detection model. We defer the explanation of online discussion lexicon construction in Section 4. 3.1 Problem Description Consider a discussion comprised of sequential turns uttered by the participants; each turn consists of a sequence of text units, where each unit can be a sentence or a segment of several sentences. Our model takes as input the t"
W14-2617,N13-1041,0,0.0179851,"0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. 1 Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Ling"
W14-2617,P04-1085,0,0.106885,"ns derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model"
W14-2617,W12-3710,0,0.418571,"cally inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware o"
W14-2617,P09-1026,0,0.298182,"isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. 1 Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, c Baltimore, Maryland, USA. June 27, 2014. 2"
W14-2617,W10-0214,0,0.0497347,"tify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.[P P ] ... agreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applicat"
W14-2617,W06-1639,0,0.113471,"elements in the partially ordered set O possess an ordinal relation ≤. Here, we differentiate agreement and disagreement with different intensity, because the output of our classifier can be used for other applications, such as dispute detection, where “strongly disagree” (e.g. NN) plays an important role. Meanwhile, fine-grained sentiment labels potentially provide richer context information for the sequential model employed for this task. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressi"
W14-2617,N10-1119,0,0.0103901,"oise. So far as we know, there is no lexicon available for online discussions. Thus, we create from a large-scale corpus via label propagation. The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2. Edge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector ~a using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define “co-occur” as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine simi"
W14-2617,walker-etal-2012-corpus,0,0.048886,"less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) inv"
W14-2617,N12-1072,0,0.0354572,"less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) inv"
W14-2617,P11-2065,0,0.16263,"the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, our predictions are made for each sentence or s"
W14-2617,H05-1044,0,0.0369926,"that the ONLY reason for the war was WMD’s is wrong - because it simply isn’t.[N N ] However I agree that the emphasis needs to be on the armaments crisis because it was the reason sold to the public and the major one used to justify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.[P P ] ... agreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction"
W14-2617,prasad-etal-2008-penn,0,\N,Missing
W14-2617,N06-2014,0,\N,Missing
W14-2703,D13-1181,0,0.0119088,"language techniques are the only option to further investigate possible effects of enrollment. Consequently we focus on the review text posted by Vine members. 4.1 Time Specificity A similar concern as Product Specificity exists for date references. By focusing on syntactic and semantic style, we avoid the use of time specific features. Another concern is that perhaps post enrollment, reviewers receive writing guidelines from Amazon. This does not appear to be the case, as the writing guidelines 9 appear to be for all members.We now turn to the extraction of style features. Approach Following Ashok et al. (2013) and Bergsma et al. (2012) we construct features that represent writing style from each review (discussed in more detail in the next section). We incorporate these features in a classification algorithm that attempts to classify each review as having been written pre or post-enrollment to the Vine program. We report whether the difference in accuracy for this classifier vs. a majority vote classification is statistically significant or not. In order to detect differences in style pre and post-enrollment, we need to address certain confounding factors — Reviewer Specificity , Product Specificit"
W14-2703,N12-1033,0,0.28818,"mputer Science Department of Information Science Cornell University cardie@cs.cornell.edu Dinesh Puranam Samuel Curtis Johnson Graduate School of Management Cornell University dp457@cornell.edu Voices.”3 There could be potential concerns as to whether this enrollment affects the way reviews are written, introducing, for example, a positive bias.4 In this work, we investigate whether enrollment in the Vine program results in changes in the linguistic style used in reviews. We investigate this by looking at reviews by individuals before and after enrollment in the program. Following Feng et al. (2012) and Bergsma et al. (2012), we conduct a stylometric analysis using a number of syntactic and semantic features to detect differences in style. We believe that detecting changes in consumer behavior due to intervention by a firm is a novel natural language processing task. Our approach offers a framework for analyzing text to detect these changes. This work is relevant for social scientists and consumer advocates as research suggests that product reviews are influential (Chevalier and Mayzlin, 2006) and changes in style could potentially influence consumer decisions. Abstract Do rewards from r"
W14-2703,P03-1054,0,0.00463777,"d the number of sentences in each review. Latent Dirichlet Allocation (LDA) We also apply Latent Dirichlet Allocation (Blei et al., 2003) to the production rules extracted from the Probabilistic Context Free Grammar. We use the topics generated as features in our prediction task. Our objective was to determine whether certain co-occurring production rules offered better classification accuracy. Our implementation includes hyper-parameter optimization via maximum likelihood. The number of topics is selected by maximizing the pairwise cosine distance amongst topics. We used the Stanford Parser (Klein and Manning, 2003) to parse each of the reviews and the Natural Language Toolkit (NLTK) (Bird et al., 2009) to post process the results. 5 All experiments use the Fan et al. (2008) implementation of linear Support Vector Machines (Vapnik, 1998). The linear specification allows us to infer feature importance. We learn the penalty parameter via grid search using 5 fold cross-validation and report performance on a heldout balanced sample of reviews from 50 randomly selected users (all of whom were excluded from the training set) from the group of reviewers with at least 25 reviews in pre and post enrollment period"
W14-2703,P11-1032,1,0.798796,"the quality and helpfulness of their reviews as judged by other Amazon customers. Amazon provides Vine members with free products that have been submitted to the program by participating vendors. Vine reviews are the “independent opinions of the Vine Product Reviews Product reviews have received considerable attention in multiple disciplines including Marketing, Computer Science and Information Science. Research has addressed questions such as the influence of product reviews on product sales and on brands (Gopinath et al. (2014); Chevalier and Mayzlin (2006)), detection of deceptive reviews (Ott et al., 2011) and sentiment summarization (Titov and McDonald, 2008). 1 3 A status badge is a special identification usually placed next to a username in online content. 2 http://blog.librarything.com/main/2007/08/amazonvine-and-early-reviewers/ http://www.amazon.com/gp/vine/help, words italicized by authors. 4 http://www.npr.org/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, c Baltimore, Maryland USA, 27 June 2014. 2014 Association for Computational Linguistics Thi"
W14-2703,P08-1036,0,0.0104622,"judged by other Amazon customers. Amazon provides Vine members with free products that have been submitted to the program by participating vendors. Vine reviews are the “independent opinions of the Vine Product Reviews Product reviews have received considerable attention in multiple disciplines including Marketing, Computer Science and Information Science. Research has addressed questions such as the influence of product reviews on product sales and on brands (Gopinath et al. (2014); Chevalier and Mayzlin (2006)), detection of deceptive reviews (Ott et al., 2011) and sentiment summarization (Titov and McDonald, 2008). 1 3 A status badge is a special identification usually placed next to a username in online content. 2 http://blog.librarything.com/main/2007/08/amazonvine-and-early-reviewers/ http://www.amazon.com/gp/vine/help, words italicized by authors. 4 http://www.npr.org/blogs/money/2013/10/29/241372607/topreviewers-on-amazon-get-tons-of-free-stuff. 17 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17–27, c Baltimore, Maryland USA, 27 June 2014. 2014 Association for Computational Linguistics This list is by no means comprehensive, but it is indicati"
W18-1110,W17-4909,0,0.0274874,"ted with the topic. We include the text of debates where a FEMALE and a MALE with opposing stances on ABORTION discusses some aspect • We investigate which linguistic features are important for discriminating between groups with different gender and stance in expressing opinions about ABORTION. 2 69 Related Work There has been a tremendous amount of work on understanding differences in writing styles between gender groups. Argamon et al. (2003) has found that females use many more pronouns whereas males use noun specifiers more frequently in British National Corpus covering a range of genres. Litvinova et al. (2017) has looked at the differences in the frequencies of some parts of speech (POS) between different genders in Russian written texts. Morphological features have shown to be important to discriminate between genders in many European languages (Mikros, 2013; Bortolato, 2016). Schofield and Mehr (2016) has studied significant linguistic and structural features of dialogue to differentiate genders in conversation and analyzed how these Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 69–75 c New Orleans, Louisiana, J"
W18-1110,strapparava-valitutti-2004-wordnet,0,0.110388,"lso find that CON - FEMALE and PRO - MALE uses significantly more existential “there” than any other group (p &lt; 0.001) and CON - MALE uses significantly more existential “there” than PRO FEMALE (p &lt; 0.05). Named Entity Mentions. For each entity type such as PERSON, LOCATION, ORGANIZATION, we look at the ratio of words belonging to these classes to number of tokens. As 2(d) shows, CON MALE has significantly more mentions for entity type PERSON than any other group. We observe that CON - MALE uses PERSON entity type while citing other people’s ideas. Affect and Sentiment. We used WordnetAffect (Valitutti, 2004) to find affective concepts correlated with affective words and compare the affective concepts across these different groups. We look at the words associated with negative, positive and ambiguous emotions. Figure 3 shows the ratio of negative 3(a), positive 3(b) and ambiguous 3(d) affect words to number of tokens for each group. We find that CON - FEMALE uses significantly more (p &lt; 0.05) negative and positive affect words than any other group and PRO FEMALE uses significantly more ambiguous affect words any other group (p &lt; 0.05). We also look at the overall sentiment of the sentences and we"
W18-1110,W11-1709,0,0.421051,"written texts. Morphological features have shown to be important to discriminate between genders in many European languages (Mikros, 2013; Bortolato, 2016). Schofield and Mehr (2016) has studied significant linguistic and structural features of dialogue to differentiate genders in conversation and analyzed how these Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 69–75 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics effects relate to existing literature on gender in film. Mohammad and Yang (2011) has shown that there are marked differences across genders in how they use emotion words in work-place email. They found that women use many words from the joy-sadness axis, whereas men prefer terms from the fear-trust axis. Thelwall et al. (2010) has looked at the comments on MySpace and found that females are more likely to give and receive more positive comments than are males, but there is no difference for negative comments. Although previous work investigates discriminative features to distinguish between different gender groups, the effect of the stance on the linguistic differences ac"
W18-1110,W16-0204,0,0.0852736,"69 Related Work There has been a tremendous amount of work on understanding differences in writing styles between gender groups. Argamon et al. (2003) has found that females use many more pronouns whereas males use noun specifiers more frequently in British National Corpus covering a range of genres. Litvinova et al. (2017) has looked at the differences in the frequencies of some parts of speech (POS) between different genders in Russian written texts. Morphological features have shown to be important to discriminate between genders in many European languages (Mikros, 2013; Bortolato, 2016). Schofield and Mehr (2016) has studied significant linguistic and structural features of dialogue to differentiate genders in conversation and analyzed how these Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 69–75 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics effects relate to existing literature on gender in film. Mohammad and Yang (2011) has shown that there are marked differences across genders in how they use emotion words in work-place email. They found that women use many words from the j"
W19-1503,D18-1122,0,0.0281954,"Missing"
W19-1503,D14-1162,0,0.0807335,"Missing"
W19-1503,N18-1202,0,0.0221537,"ext span, we identify the closest mention of the source that precedes the target. We begin the context span starting at the first word beyond the end of this mention (wi ). Similarly, we conclude the span at the last word preceding the target (wj ). Denoting the context span as [wi , wj ], we compute the context vector c as (where the U maps are learned): usrc = Usrc src; utrg = Utrg trg; ut = Uc ht Implementation Details We use frozen word representations that concatenate noncontextual 300-dimensional GloVe embeddings (Pennington et al., 2014) and contextual 3072-dimensional ELMo embeddings (Peters et al., 2018). We use a 2-layer bidirectional LSTM encoder (Hochreiter and Schmidhuber, 1997) and consistently use both a hidden dimensionality of 128 and tanh nonlinearities throughout the work. Based on results on the development set, we use an attention dimensionality of 32, a dropout (Srivastava et al., 2014) probability in all hidden layers of 0.2, and a batch size of 10 documents. We also find that a single-layer GRU worked best for event argument pooling and interestingly find that a unidirectional GRU is preferable to a bidirectional GRU. The final classifier is a single-layer FFNN. The model is tr"
W19-1503,P16-1087,1,0.827093,"ations in red. document and its EREs (including metadata and mentions) determine the sentiment of each EN TITY towards every RELATION and EVENT in the document.1 Until quite recently, existing approaches to this type of second-order extraction task have relied heavily on domain knowledge and feature engineering (Rambow et al., 2016b; Niculae et al., 2016; Dalton et al., 2016; Gutirrez et al., 2016). And while end-to-end neural network methods that bypass feature engineering have been developed for information extraction problems, they have largely been applied to first-order extraction tasks (Katiyar and Cardie, 2016; Miwa and Bansal, 2016; Katiyar and Cardie, 2017; Orr et al., 2018). Possibly more importantly, these techniques ignore, or have no access to, the internal structure of relations and events. We hypothesize that utilizing the metadataIntroduction Information extraction has long been an active subarea of natural language processing (NLP) (Onyshkevych et al., 1993; Freitag, 2000). A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY , RELATION , and EVENT identified in a given text (Song et al"
W19-1503,P17-1085,1,0.843473,"etadata and mentions) determine the sentiment of each EN TITY towards every RELATION and EVENT in the document.1 Until quite recently, existing approaches to this type of second-order extraction task have relied heavily on domain knowledge and feature engineering (Rambow et al., 2016b; Niculae et al., 2016; Dalton et al., 2016; Gutirrez et al., 2016). And while end-to-end neural network methods that bypass feature engineering have been developed for information extraction problems, they have largely been applied to first-order extraction tasks (Katiyar and Cardie, 2016; Miwa and Bansal, 2016; Katiyar and Cardie, 2017; Orr et al., 2018). Possibly more importantly, these techniques ignore, or have no access to, the internal structure of relations and events. We hypothesize that utilizing the metadataIntroduction Information extraction has long been an active subarea of natural language processing (NLP) (Onyshkevych et al., 1993; Freitag, 2000). A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY , RELATION , and EVENT identified in a given text (Song et al., 2015). In some variants of ERE detection, meta"
W19-1503,W15-0812,0,0.0198524,"rdie, 2016; Miwa and Bansal, 2016; Katiyar and Cardie, 2017; Orr et al., 2018). Possibly more importantly, these techniques ignore, or have no access to, the internal structure of relations and events. We hypothesize that utilizing the metadataIntroduction Information extraction has long been an active subarea of natural language processing (NLP) (Onyshkevych et al., 1993; Freitag, 2000). A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY , RELATION , and EVENT identified in a given text (Song et al., 2015). In some variants of ERE detection, metadata descriptions and specific mentions of the ERE objects also need to be recorded as shown graphically in Figure 1. Subsequent second-order extraction tasks can further build upon first-order ERE information. In this work, in particular, we consider the second-order structured prediction task studied in the 2016/2017 Belief and Sentiment analysis evaluations (BeSt) (Rambow et al., 2016a): given a 1 The BeSt evaluation also requires the identification of sentiment towards entities. For reasons that will become clear later, we do not consider entity-to-"
W19-1503,P16-1105,0,0.0309015,"d its EREs (including metadata and mentions) determine the sentiment of each EN TITY towards every RELATION and EVENT in the document.1 Until quite recently, existing approaches to this type of second-order extraction task have relied heavily on domain knowledge and feature engineering (Rambow et al., 2016b; Niculae et al., 2016; Dalton et al., 2016; Gutirrez et al., 2016). And while end-to-end neural network methods that bypass feature engineering have been developed for information extraction problems, they have largely been applied to first-order extraction tasks (Katiyar and Cardie, 2016; Miwa and Bansal, 2016; Katiyar and Cardie, 2017; Orr et al., 2018). Possibly more importantly, these techniques ignore, or have no access to, the internal structure of relations and events. We hypothesize that utilizing the metadataIntroduction Information extraction has long been an active subarea of natural language processing (NLP) (Onyshkevych et al., 1993; Freitag, 2000). A particularly important class of extraction tasks is ERE detection in which an object, typically an element in a knowledge base, is created for each ENTITY , RELATION , and EVENT identified in a given text (Song et al., 2015). In some varia"
W19-4519,P11-1099,0,0.079169,"hoice (i.e. use of hedging, and particular causal connectors and modal particles) are indicative of the mode of argumentation (Gold et al., 2015; Paxton and Dale, 2014). Style features include length, personal pronouns, referring to opponent, use of citations, and links. Using citations and addressing an opponent’s points are critical components of justification that affect the reception of an argument. Additionally, the length of a speaker’s utterance and the language used when referring to self and the oppoSemantic features include sentiment, subjectivity (Wilson et al., 2005), connotation (Feng and Hirst, 2011), and politeness. The sentiment and subjectivity of an argument impacts the reception of the message, and are predictive of argument stance (Somasundaran and Wiebe, 2010). In addition to these attributes, connotation and politeness cues contribute to the patterns of interaction of debaters, which are critical in predicting persuasiveness (Tan et al., 2016). Argumentation features, as in (Somasundaran et al., 2007), have been shown to predict the stance and opinion of a speaker. These include the following: assessment, authority, conditioning, contrasting, emphasizing, generalizing, empathy, in"
W19-4519,P16-2089,0,0.031303,"and hedging (Paxton and Dale, 2014), and rhetoric qualities such as precision, firmness, energy, and commitment (Jorgensen et al., 1998). These works in psychology highlight the importance of studying linguistic features in arguments and persuasion. Argument mining. Much recent work in argumentation has focused on the automatic detection of argument structures in text (Lippi and Torroni, 2016; Schulz et al., 2018; Stab et al., 2018; Morio and Fujita, 2018). Research has shown promising results on using extracted argument structures as features on tasks that involve predicting convincingness (Ghosh et al., 2016; Yunfan Gu and Huang, 2018; Cano-Basave and He, 2016). Specific to debates, work has been done on detecting the stance of the speaker. Walker et al. (2012), for example, find that structuring the debates in terms of agreement relations between 1 http://www.4forums.com/political/ forum.php/ 168 ROUND 1 PRO: they are not. To the best of our knowledge, this work is most relevant to ours because it studies debate text and considers prior beliefs of both the audience and the debaters. Our work differs from this study in that we separately consider persuasion of audience members who were undecided"
W19-4519,N16-1166,0,0.0246045,"the importance of considering the undecided and decided audiences separately when studying linguistic factors of persuasion. 1 Claire Cardie Cornell University cardie@cs.cornell.edu Introduction Understanding the factors that influence persuasion in the context of argumentation (e.g. debates) has been an important focus in a variety of research areas. Natural language processing (NLP) research on persuasion has focused for the most part on uncovering the linguistic factors that determine and define persuasive arguments — features of the language of the argument itself. For example, Tan et al. (2016) and Zhang et al. (2016) have found that the language used in arguments and the patterns of interaction between debaters are important predictors of persuasiveness. Recently, however, studies have emerged that begin to study the effects of audience characteristics on persuasion, e.g. features that encode demographic information, the prior beliefs, and debate platform 167 Proceedings of the 6th Workshop on Argument Mining, pages 167–176 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics speakers improves prediction. Lexical and syntactic argument features are show"
W19-4519,N18-1094,1,0.832984,"hile also considering audience characteristics such as prior beliefs and decidedness. Prior views of the audience. Persuasion of an audience is not solely dependent on the language used by the speaker. Research in psychology emphasizes the significance of people’s prior views on their perception of new information. The effectiveness of a message depends significantly on the prior beliefs and the strengths of beliefs of the message recipient (Johnson et al., 1995; Lau et al., 1991). Recent work has analyzed the influence of audience characteristics on predicting persuasion (Lukin et al., 2017; Durmus and Cardie, 2018). Lukin et al. (2017) examine the effects of audience factors and argumentation types in belief change. They study dialogs from 4forums.com1 , which contain argument type annotations. Their results show that information on prior beliefs and personality type improves the ability of the model to predict belief change; more conscientious, open, and agreeable people tend to respond more to emotional argument types. The importance of considering audiencespecific prior belief factors is further illustrated in Durmus and Cardie (2018). Using debate and user data from debate.org, they study the effect"
W19-4519,E17-1070,0,0.0769104,"f persuasive text, while also considering audience characteristics such as prior beliefs and decidedness. Prior views of the audience. Persuasion of an audience is not solely dependent on the language used by the speaker. Research in psychology emphasizes the significance of people’s prior views on their perception of new information. The effectiveness of a message depends significantly on the prior beliefs and the strengths of beliefs of the message recipient (Johnson et al., 1995; Lau et al., 1991). Recent work has analyzed the influence of audience characteristics on predicting persuasion (Lukin et al., 2017; Durmus and Cardie, 2018). Lukin et al. (2017) examine the effects of audience factors and argumentation types in belief change. They study dialogs from 4forums.com1 , which contain argument type annotations. Their results show that information on prior beliefs and personality type improves the ability of the model to predict belief change; more conscientious, open, and agreeable people tend to respond more to emotional argument types. The importance of considering audiencespecific prior belief factors is further illustrated in Durmus and Cardie (2018). Using debate and user data from debate."
W19-4519,W18-5202,0,0.0224503,"le, 2014; Jorgensen et al., 1998). These studies find the importance of various language features: lexical qualities such as personal pronoun use, word sentiment, and hedging (Paxton and Dale, 2014), and rhetoric qualities such as precision, firmness, energy, and commitment (Jorgensen et al., 1998). These works in psychology highlight the importance of studying linguistic features in arguments and persuasion. Argument mining. Much recent work in argumentation has focused on the automatic detection of argument structures in text (Lippi and Torroni, 2016; Schulz et al., 2018; Stab et al., 2018; Morio and Fujita, 2018). Research has shown promising results on using extracted argument structures as features on tasks that involve predicting convincingness (Ghosh et al., 2016; Yunfan Gu and Huang, 2018; Cano-Basave and He, 2016). Specific to debates, work has been done on detecting the stance of the speaker. Walker et al. (2012), for example, find that structuring the debates in terms of agreement relations between 1 http://www.4forums.com/political/ forum.php/ 168 ROUND 1 PRO: they are not. To the best of our knowledge, this work is most relevant to ours because it studies debate text and considers prior beli"
W19-4519,N12-1072,0,0.0185711,"psychology highlight the importance of studying linguistic features in arguments and persuasion. Argument mining. Much recent work in argumentation has focused on the automatic detection of argument structures in text (Lippi and Torroni, 2016; Schulz et al., 2018; Stab et al., 2018; Morio and Fujita, 2018). Research has shown promising results on using extracted argument structures as features on tasks that involve predicting convincingness (Ghosh et al., 2016; Yunfan Gu and Huang, 2018; Cano-Basave and He, 2016). Specific to debates, work has been done on detecting the stance of the speaker. Walker et al. (2012), for example, find that structuring the debates in terms of agreement relations between 1 http://www.4forums.com/political/ forum.php/ 168 ROUND 1 PRO: they are not. To the best of our knowledge, this work is most relevant to ours because it studies debate text and considers prior beliefs of both the audience and the debaters. Our work differs from this study in that we separately consider persuasion of audience members who were undecided before the debate from audience members who switched sides. The undecided audience. There has been a substantial amount of research effort in the social and"
W19-4519,H05-1044,0,0.0233466,"and punctuation. A speaker’s word choice (i.e. use of hedging, and particular causal connectors and modal particles) are indicative of the mode of argumentation (Gold et al., 2015; Paxton and Dale, 2014). Style features include length, personal pronouns, referring to opponent, use of citations, and links. Using citations and addressing an opponent’s points are critical components of justification that affect the reception of an argument. Additionally, the length of a speaker’s utterance and the language used when referring to self and the oppoSemantic features include sentiment, subjectivity (Wilson et al., 2005), connotation (Feng and Hirst, 2011), and politeness. The sentiment and subjectivity of an argument impacts the reception of the message, and are predictive of argument stance (Somasundaran and Wiebe, 2010). In addition to these attributes, connotation and politeness cues contribute to the patterns of interaction of debaters, which are critical in predicting persuasiveness (Tan et al., 2016). Argumentation features, as in (Somasundaran et al., 2007), have been shown to predict the stance and opinion of a speaker. These include the following: assessment, authority, conditioning, contrasting, em"
W19-4519,N18-2006,0,0.0294409,"ng the outcome of debates (Paxton and Dale, 2014; Jorgensen et al., 1998). These studies find the importance of various language features: lexical qualities such as personal pronoun use, word sentiment, and hedging (Paxton and Dale, 2014), and rhetoric qualities such as precision, firmness, energy, and commitment (Jorgensen et al., 1998). These works in psychology highlight the importance of studying linguistic features in arguments and persuasion. Argument mining. Much recent work in argumentation has focused on the automatic detection of argument structures in text (Lippi and Torroni, 2016; Schulz et al., 2018; Stab et al., 2018; Morio and Fujita, 2018). Research has shown promising results on using extracted argument structures as features on tasks that involve predicting convincingness (Ghosh et al., 2016; Yunfan Gu and Huang, 2018; Cano-Basave and He, 2016). Specific to debates, work has been done on detecting the stance of the speaker. Walker et al. (2012), for example, find that structuring the debates in terms of agreement relations between 1 http://www.4forums.com/political/ forum.php/ 168 ROUND 1 PRO: they are not. To the best of our knowledge, this work is most relevant to ours because it"
W19-4519,N16-1017,0,0.120736,"nce of considering the undecided and decided audiences separately when studying linguistic factors of persuasion. 1 Claire Cardie Cornell University cardie@cs.cornell.edu Introduction Understanding the factors that influence persuasion in the context of argumentation (e.g. debates) has been an important focus in a variety of research areas. Natural language processing (NLP) research on persuasion has focused for the most part on uncovering the linguistic factors that determine and define persuasive arguments — features of the language of the argument itself. For example, Tan et al. (2016) and Zhang et al. (2016) have found that the language used in arguments and the patterns of interaction between debaters are important predictors of persuasiveness. Recently, however, studies have emerged that begin to study the effects of audience characteristics on persuasion, e.g. features that encode demographic information, the prior beliefs, and debate platform 167 Proceedings of the 6th Workshop on Argument Mining, pages 167–176 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics speakers improves prediction. Lexical and syntactic argument features are shown to improve predictive"
W19-4519,2007.sigdial-1.5,0,0.2155,"ionally, the length of a speaker’s utterance and the language used when referring to self and the oppoSemantic features include sentiment, subjectivity (Wilson et al., 2005), connotation (Feng and Hirst, 2011), and politeness. The sentiment and subjectivity of an argument impacts the reception of the message, and are predictive of argument stance (Somasundaran and Wiebe, 2010). In addition to these attributes, connotation and politeness cues contribute to the patterns of interaction of debaters, which are critical in predicting persuasiveness (Tan et al., 2016). Argumentation features, as in (Somasundaran et al., 2007), have been shown to predict the stance and opinion of a speaker. These include the following: assessment, authority, conditioning, contrasting, emphasizing, generalizing, empathy, inconsistency, necessity, possibility, priority, rhetorical questions, desire, and difficulty. 171 4.2 Accuracy of Models Hypotheses We hypothesize that there are key differences in the linguistic features important for persuasion of an a priori undecided audience member and the persuasion of an a priori seemingly decided audience member to change their mind. Drawing from social and political science studies, we hyp"
W19-4519,W10-0214,0,0.450221,"t the language used in arguments and the patterns of interaction between debaters are important predictors of persuasiveness. Recently, however, studies have emerged that begin to study the effects of audience characteristics on persuasion, e.g. features that encode demographic information, the prior beliefs, and debate platform 167 Proceedings of the 6th Workshop on Argument Mining, pages 167–176 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics speakers improves prediction. Lexical and syntactic argument features are shown to improve predictive performance in Somasundaran and Wiebe (2010). More relevant to our work, recent studies have examined the role of language in predicting persuasion outcomes in debates. For example, Tan et al. (2016) find that the linguistic interaction between an opinion holder and opposing debater are highly predictive of persuasiveness. And Zhang et al. (2016) find that debaters who target and address their opponent’s points are more likely to win the debate. While these studies motivate the linguistic features examined in our study, they do not take factors corresponding to audience characteristics into consideration. Our work aims to study the ling"
W19-4519,D18-1402,0,0.0219527,"ates (Paxton and Dale, 2014; Jorgensen et al., 1998). These studies find the importance of various language features: lexical qualities such as personal pronoun use, word sentiment, and hedging (Paxton and Dale, 2014), and rhetoric qualities such as precision, firmness, energy, and commitment (Jorgensen et al., 1998). These works in psychology highlight the importance of studying linguistic features in arguments and persuasion. Argument mining. Much recent work in argumentation has focused on the automatic detection of argument structures in text (Lippi and Torroni, 2016; Schulz et al., 2018; Stab et al., 2018; Morio and Fujita, 2018). Research has shown promising results on using extracted argument structures as features on tasks that involve predicting convincingness (Ghosh et al., 2016; Yunfan Gu and Huang, 2018; Cano-Basave and He, 2016). Specific to debates, work has been done on detecting the stance of the speaker. Walker et al. (2012), for example, find that structuring the debates in terms of agreement relations between 1 http://www.4forums.com/political/ forum.php/ 168 ROUND 1 PRO: they are not. To the best of our knowledge, this work is most relevant to ours because it studies debate text"
W96-0211,P95-1017,0,0.0542046,"Missing"
W96-0211,E93-1007,0,0.0684471,"Missing"
W96-0211,P87-1022,0,0.0348718,"Missing"
W96-0211,P92-1028,1,0.892425,"Missing"
W96-0211,J94-3007,0,0.0683918,"Missing"
W96-0211,P95-1037,0,0.0271207,"Missing"
W96-0211,J92-4001,0,0.161957,"Missing"
W96-0211,J93-3001,0,\N,Missing
W99-0611,P95-1017,0,0.507919,"ic constraints similar in spirit to those employed in our system. The main advantage of our approach is that all constraints and preferences are represented neatly in the distance metric (and radius r), allowing for simple modification of this measure to incorporate new Work Existing systems for noun phrase coreference resolution can be broadly characterized as learning and non-learning approaches. All previous attempts to view coreference as a learning problem treat coreference resolution as a classification task: the algorithms classify a pair of noun phrases as coreferent or not. Both MLR (Aone and Bennett, 1995) and RESOLVE (McCarthy and Lehnert, 1995), for ex4Whether or not clustering can be considered a ""learning"" approach is unclear. The algorithm uses the existing partitions to process each successive NP, but the partitions generated for a document are not useful for processing subsequent documents. OWe do use training data to tune r, but as noted above, it is likely that r need not be recalculated for new corpora. 87 Algorithm Clustering RESOLVE Best MUC-6 Worst MUC-6 Recall Precision F-measure 53 55 54 44 59 36 51 72 44 ! the evaluation. In contrast to other approaches to coreference resolution"
W99-0611,P98-1011,0,0.0133096,"andling the wide variety of anaphoric expressions that comprise noun phrase coreference. We would also like to make use of cues from centering theory and plan to explore the possibility of learning the weights associated with each term in the distance metric. Our methods for producing the noun phrase feature vector are also overly simplistic. Nevertheless, the relatively strong performance of the technique indicates that clustering constitutes a powerful and natural approach to noun phrase coreference resolution. There is also a growing body of work on the narrower task of pronoun resolution. Azzam et al. (1998), for example, describe a focus-based approach that incorporates discourse information when resolving pronouns. Lappin and Leass (1994) make use of a series of filters to rule out impossible antecedents, many of which are similar to our ooincompatibilities. They also make use of more extensive syntactic information (such as the thematic role each noun phrase plays), and thus require a fuller parse of the input text. Ge et al. (1998) present a supervised probabilistic algorithm that assumes a full parse of the input text. Dagan and Itai (1991) present a hybrid full-parse/unsupervised learning a"
W99-0611,P98-1034,1,0.249354,"h distance greater than a clustering radius r are not placed into the same partition and so are not considered coreferent. The subsections below describe the noun phrase 1The coreference 'relation is symmetric, transitive, and reflexive. of[R: Prime Corp.] since 1986, sg..x~i,[js fiis]..J pay jump 20 , ~ , (a~..[js ~e 37- year-old] also became [pc ~ l services company]'s [Js president]. Figure 1: Coreference System representation, the distance metric, and the clustering algorithm in turn. 3.1 Instance R e p r e s e n t a t i o n Given an input text, we first use the Empire noun phrase finder (Cardie and Pierce, 1998) to locate all noun phrases in the text. Note that Empire identifies only base noun phrases, i.e. simple noun phrases that contain no other smaller noun phrases within them. For example, Chief Financial Officer of Prime Corp. is too complex to be a base noun phrase. It contains two base noun phrases: Chief Financial Officer and Prime Corp. Each noun phrase in the input text is then represented as a set of 11 features as shown in Table 1. This noun phrase representation is a first approximation to the feature vector that would be required for accurate coreference resolution. All feature values"
W99-0611,P92-1028,1,0.722078,"dents, many of which are similar to our ooincompatibilities. They also make use of more extensive syntactic information (such as the thematic role each noun phrase plays), and thus require a fuller parse of the input text. Ge et al. (1998) present a supervised probabilistic algorithm that assumes a full parse of the input text. Dagan and Itai (1991) present a hybrid full-parse/unsupervised learning approach that focuses on resolving ""it"". Despite a large corpus (150 million words), their approach suffers from sparse data problems, but works well when enough relevant data is available. Lastly, Cardie (1992a; 1992b) presents a case-based learning approach for relative pronoun disambiguation. 7 This work was 9624639 and a fellowship. We formatting and Our clustering approach differs from this previous work in several ways. First, because we only require the noun phrases in any input text, we do not require a fifll syntactic parse. Although we would expect increases in performance if complex noun phrases were used, our restriction to base NPs does not reflect a limitation of the clustering algorithm (or the distance metric), but rather a self-imposed limitation on the preprocessing requirements of"
W99-0611,W98-1119,0,0.56613,"Missing"
W99-0611,J94-4002,0,0.376013,"m centering theory and plan to explore the possibility of learning the weights associated with each term in the distance metric. Our methods for producing the noun phrase feature vector are also overly simplistic. Nevertheless, the relatively strong performance of the technique indicates that clustering constitutes a powerful and natural approach to noun phrase coreference resolution. There is also a growing body of work on the narrower task of pronoun resolution. Azzam et al. (1998), for example, describe a focus-based approach that incorporates discourse information when resolving pronouns. Lappin and Leass (1994) make use of a series of filters to rule out impossible antecedents, many of which are similar to our ooincompatibilities. They also make use of more extensive syntactic information (such as the thematic role each noun phrase plays), and thus require a fuller parse of the input text. Ge et al. (1998) present a supervised probabilistic algorithm that assumes a full parse of the input text. Dagan and Itai (1991) present a hybrid full-parse/unsupervised learning approach that focuses on resolving ""it"". Despite a large corpus (150 million words), their approach suffers from sparse data problems, b"
W99-0611,J93-2004,0,0.0413035,"Missing"
W99-0611,M95-1005,0,0.205343,"Missing"
W99-0611,C98-1034,1,\N,Missing
W99-0611,C98-1011,0,\N,Missing
X98-1017,J95-4004,0,0.0172765,"irst extracts an initial grammar from a &quot;treebank&quot; corpus, i.e., a corpus that has been annotated with respect to the linguistic relationship of interest. Consider the base noun phrase relationship - - the identification of simple, non-recursive noun phrases. Accurate identification of base noun phrases is a critical component of any partial parser; in addition, Smart relies on base NPs as its primary source of linguistic phrase information. To extract a grammar for base noun phrase identification, we tag the training text with a part-of-speech tagger (we use Mitre&apos;s version of Brill&apos;s tagger [3]) and then extract as an NP rule every unique part-of-speech sequence that covers a base NP annotation. Next, the grammar is improved by discarding rules that obtain a low precision-based &quot;benefit&quot; score when applied to a held out portion of the training corpus, the pruning corpus. The resulting &quot;grammar&quot; can then be used to identify base NPs in a novel text as follows: 1. Run all lower-level annotators. For base NPs, for example, run the part-of-speech annotator. 2. Proceed through the tagged text from left to right, at each point matching the rules against the remaining input. For base NP re"
X98-1017,J93-2004,0,0.0237712,"Missing"
X98-1017,A97-1004,0,0.0404277,"Missing"
X98-1017,P98-1034,1,\N,Missing
X98-1017,C98-1034,1,\N,Missing
