2020.acl-main.50,P13-2144,0,0.07718,"Missing"
2020.acl-main.50,K19-1096,1,0.467789,"Missing"
2020.acl-main.50,W13-1106,0,0.0531734,"Missing"
2020.acl-main.50,W11-3702,0,0.145937,"Missing"
2020.acl-main.50,I13-1191,0,0.0312558,"cation is aided by the tendency of users to form so-called “echo chambers”, where they engage with like-minded users (Himelboim et al., 2013; Magdy et al., 2016a), and the tendency of users’ beliefs to be persistent over time (Borge-Holthoefer et al., 2015; Magdy et al., 2016a; Pennacchiotti and Popescu, 2011b). Studies have examined the effectiveness of different features for stance detection, including textual features such as word n-grams and hashtags, network interactions such as retweeted accounts and mentions, and profile information such as user location (Borge-Holthoefer et al., 2015; Hasan and Ng, 2013; Magdy et al., 2016a,b; Weber et al., 2013). Network interaction features were shown to yield better results compared to using textual features (Magdy et al., 2016a; Wong et al., 2013). Sridhar et al. (2015) leveraged both user interactions and textual information when modeling stance and disagreement, using a probabilistic programming system that allows models to be specified using a declarative language. 528 Trabelsi and Za¨ıane (2018) described an unsupervised stance detection method that determines the viewpoints of comments and of their authors. It analyzes online forum discussion thread"
2020.acl-main.50,D14-1083,0,0.019639,"vior to infer the ideological leanings of online media sources and popular Twitter accounts. Barber´a and Sood (2015) proposed a statistical model based on the follower relationships to media sources and Twitter personalities in order to estimate their ideological leaning. As for individual users, much recent work focused on stance detection to determine a person’s position on a topic including the deduction of political preferences (Barber´a, 2015; Barber and Rivero, 2015; Borge-Holthoefer et al., 2015; Cohen and Ruths, 2013; Colleoni et al., 2014; Conover et al., 2011b; Fowler et al., 2011; Hasan and Ng, 2014; Himelboim et al., 2013; Magdy et al., 2016a,b; Makazhanov et al., 2014; Trabelsi and Za¨ıane, 2018; Weber et al., 2013). User stance classification is aided by the tendency of users to form so-called “echo chambers”, where they engage with like-minded users (Himelboim et al., 2013; Magdy et al., 2016a), and the tendency of users’ beliefs to be persistent over time (Borge-Holthoefer et al., 2015; Magdy et al., 2016a; Pennacchiotti and Popescu, 2011b). Studies have examined the effectiveness of different features for stance detection, including textual features such as word n-grams and hashtag"
2020.acl-main.50,E17-2068,0,0.0473605,"ering and supervised classification. For the projection and clustering step, we identify clusters of core vocal users using the unsupervised method described in (Darwish et al., 2020). In this step, users are mapped to a lower dimensional space based on their similarity, and then they are clustered. After performing this unsupervised learning step, we train a supervised classifier using the two largest identified clusters in order to tag many more users. For that, we use FastText, a deep neural network text classifier, that has been shown to be effective for various text classification tasks (Joulin et al., 2017). 529 Topic Keywords Date Range Climate change #greendeal, #environment, #climate, #climatechange, #carbonfootprint, #climatehoax, #climategate, #globalwarming, #agw, #renewables #gun, #guns, #weapon, #2a, #gunviolence, #secondamendment, #shooting, #massshooting, #gunrights, #GunReformNow, #GunControl, #NRA IlhanOmarIsATrojanHorse, #IStandWithIlhan, #ilhan, #Antisemitism, #IlhanOmar, #IlhanMN, #RemoveIlhanOmar, #ByeIlhan, #RashidaTlaib, #AIPAC, #EverydayIslamophobia, #Islamophobia, #ilhan #border, #immigration, #immigrant, #borderwall, #migrant, #migrants, #illegal, #aliens midterm, election,"
2020.acl-main.50,S16-1003,0,0.128731,"Missing"
2020.acl-main.50,C14-1019,0,0.0621783,"Missing"
2020.acl-main.50,P15-1012,0,0.0173173,"persistent over time (Borge-Holthoefer et al., 2015; Magdy et al., 2016a; Pennacchiotti and Popescu, 2011b). Studies have examined the effectiveness of different features for stance detection, including textual features such as word n-grams and hashtags, network interactions such as retweeted accounts and mentions, and profile information such as user location (Borge-Holthoefer et al., 2015; Hasan and Ng, 2013; Magdy et al., 2016a,b; Weber et al., 2013). Network interaction features were shown to yield better results compared to using textual features (Magdy et al., 2016a; Wong et al., 2013). Sridhar et al. (2015) leveraged both user interactions and textual information when modeling stance and disagreement, using a probabilistic programming system that allows models to be specified using a declarative language. 528 Trabelsi and Za¨ıane (2018) described an unsupervised stance detection method that determines the viewpoints of comments and of their authors. It analyzes online forum discussion threads, and therefore assumes a certain structure of the posts. It also assumes that users tend to reply to each others’ comments when they are in disagreement, whereas we assume the opposite in this paper. Their"
2020.acl-main.50,P14-1018,0,0.0621415,"Missing"
2020.coling-demos.15,N16-3003,1,0.796987,"rade 6) 1 Buckwalter transliteration and translation are provided. We thank The World Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To"
2020.coling-demos.15,L18-1366,0,0.0587851,"orm that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The tool provides insights into the strengths and"
2020.coling-demos.15,W17-1302,1,0.819228,"translation are provided. We thank The World Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To implement our tool, we used Django4 , a"
2020.coling-demos.15,L18-1039,0,0.0127727,"predict the reading difficulty of texts. Al-Khalifa and Al-Ajlan (2010) proposed a tool for readability analysis and applied this to curricula in Saudi Arabia. Zalmout et al. (2016) described a process to analyze the textbooks of two different English teaching methods for English as a Second Language (ESL) by using readability scoring technique. Al Khalil et al. (2018) presented an Arabic reading corpus that was collected from textbooks from first to twelfth grade from United Arab Emirates and works of fiction to enhance the inadequate resources that effected educational applications. Garc´ıa Salido et al. (2018) proposed a lexical tool for academic writing in Spanish and described the data extraction from a corpus of academic texts. This tool basically provides insight into how to use typical vocabulary for academic genre in order to build an entire text. Arabic is a complex language with rich morphology. Stems are typically derived from a set of roots using predefined stem templates. Affixes can be attached to stems to generate words (surface forms). For . JºJ ð (“wsyktbwnhA” – “and they will write it”)1 has two prefixes (and and will) example, the word AîEñJ and two suffixes (they and it). Furthe"
2020.coling-demos.15,L18-1181,1,0.737646,"rld Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To implement our tool, we used Django4 , a Python web framework for the rapid"
2020.coling-demos.15,zaghouani-etal-2014-large,0,0.031376,"r example by vocabulary level. Developing a platform that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The"
2020.coling-demos.15,W16-4916,0,0.154605,"level. Developing a platform that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The tool provides insight"
2020.wanlp-1.19,2020.osact-1.2,0,0.0195188,"tively dealing with the vanishing gradients problem. Bidirectional GRUs and LSTMs make better use of the training data as the data is traversed twice, typically leading to improved performance (SiamiNamini et al., 2019). Table 2 lists the model parameters. 4.2 Our BERT-based Method Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019b) resulted in major improvements for many NLP classification and language understanding tasks. For our proposed method, we fine-tuned AraBERT (v 0.1) for the classification task (Antoun et al., 2020). AraBERT is pre-trained on an identical architecture to BERT, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. It is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens, and it uses SentencePiece (BP) word segmentation. AraBERT has been shown to yield better results than multilingual BERT from Google, which is trained on Arabic Wikipedia only (Antoun et al., 2020). We 211 Human generated Deepfake YªK ÐñJË@ IK  @P Qj.®Ë@ Q g AîE@ é<Ë@ ZA à@ YÒjÖß. AK ðP . úÎ« éJ ÊJK@ Qå B@ H@  úÎ« C£@    ñ®Ë@ ,Pñ"
2020.wanlp-1.19,N19-1423,0,0.179006,"/story/ai-generated-text-is-the-scariest-deepfake-of-all/ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 207 Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 207–214 Barcelona, Spain (Online), December 12, 2020 Neural Netowrks (RNNs) are now applied to enhance the performance of auto-generated text detection models(Iqbal and Qureshi, 2020). Despite theirs success, they still suffer from the lack of annotated data for training. GPT-2 (Radford et al., 2019), and BERT (Devlin et al., 2019a) are among new approaches recently considered for overcoming this issue. A language understanding model learns contextual and task-independent representations of terms. A huge amount of texts obtained from large corpora is used to build generic (multilingual multi-application) models. Numerous deep learning text detection methods have been proposed to help readers determine if a piece of text has been authored by machine or human. The results of employing such language models are very effective in the detection of misinformation and propaganda by providing mechanisms to reveal the nature of"
2020.wanlp-1.21,L18-1577,0,0.0233125,", 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of Arabic news articles, with the base configur"
2020.wanlp-1.21,S18-1053,0,0.0244389,"orization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized"
2020.wanlp-1.21,2020.osact-1.2,0,0.302997,"7y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of Arabic news articles, with the base configuration of the BERT model. The model showed success for many Arabic NLP downstream tasks. Recently, a Multidialect-Arabic-BERT (Talafh"
2020.wanlp-1.21,Q17-1010,0,0.01627,"l embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model ("
2020.wanlp-1.21,2020.lrec-1.761,1,0.86977,"witter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized embeddings for downstream tasks like text classification, using a small amount of in-domain data. To push the sta"
2020.wanlp-1.21,P18-2089,0,0.0192211,"nclude: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of A"
2020.wanlp-1.21,W19-4621,0,0.0643966,"https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of Arabic news articles, with the base configuration of the BERT model."
2020.wanlp-1.21,2020.osact-1.9,1,0.697172,"uTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized embeddings for downstream tasks like text classification, using a small amount of in-domain data. To push the state-of-the-art performa"
2020.wanlp-1.21,P18-1031,0,0.0314595,"elonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/a"
2020.wanlp-1.21,L16-1147,0,0.0306364,"text): This model is pre-trained on a collection of publicly available corpora including Arabic Wikipedia, the 1.5B words Arabic Corpus (El-Khair, 2016), the OSIAN Corpus (Zeroual et al., 2019), Assafir news articles, and 4 other manually crawled news websites (AlAkhbar, Annahar, AL-Ahram, AL-Wafd) from the Wayback Machine. The final model is trained on approximately 70M sentences containing roughly 3B Arabic tokens (Antoun et al., 2020). Arabic BERT: QARiB (mixed style text): This model is trained on the Arabic GigaWord corpus,15 Abulkhair Arabic Corpus (El-Khair, 2016) , and OpenSubtitles (Lison and Tiedemann, 2016) in addition to 50 million tweets that were collected by issuing the query “lang:ar” against Twitter API. The final training corpus contains 120M sentences and tweets composed of 2.7B Arabic words. Downstream Task Design: For the downstream tasks, we fine-tuned the aforementioned BERT models for our classification task using a learning rate of 2e − 5 with a batch size of 64 and 3 epochs. For the training, we restricted the maximum input length to 128 tokens, with no extra preprocessing of the data. 4.3 Evaluation To asses the categorization effectiveness we used Macro F1, which is computed by"
2020.wanlp-1.21,S13-2053,0,0.0136787,"ntroduce two new Arabic text categorization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted s"
2020.wanlp-1.21,S16-1003,0,0.0233564,"om a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized embeddings for downstream tasks like text classification, using a sm"
2020.wanlp-1.21,D14-1162,0,0.0866959,"ataset include 486k articles with 52 categories. 3. Arabic News Text (ANT) Corpus10 (Chouigui et al., 2017): This dataset was collected from RSS feeds and contains approximately 6k articles belonging to 9 categories Other available datasets include: Khaleej-2004 (5k articles belonging to 4 categories) (Abbas and Smaili, 2005); Watan-2004 (20k articles belonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText m"
2020.wanlp-1.21,N18-1202,0,0.024731,"Smaili, 2005); Watan-2004 (20k articles belonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www."
2020.wanlp-1.21,P10-1040,0,0.06095,"ia8 and Masrawy9 news sites. The released dataset include 486k articles with 52 categories. 3. Arabic News Text (ANT) Corpus10 (Chouigui et al., 2017): This dataset was collected from RSS feeds and contains approximately 6k articles belonging to 9 categories Other available datasets include: Khaleej-2004 (5k articles belonging to 4 categories) (Abbas and Smaili, 2005); Watan-2004 (20k articles belonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arab"
2020.wanlp-1.21,W19-4619,0,0.0937594,"ween 1 and 5. 4.2.2 Pre-trained Bidirectional Encoder Representations from Transformers (BERT) Models We experimented with three different BERT models as follows: Multilingual BERT: mBERT (formal text): The model is pre-trained using a masked language modeling (MLM) objective using Wikipedia articles for 104 languages including Arabic. We used the case sensitive base model (Devlin et al., 2018). Arabic BERT: AraBERT (formal text): This model is pre-trained on a collection of publicly available corpora including Arabic Wikipedia, the 1.5B words Arabic Corpus (El-Khair, 2016), the OSIAN Corpus (Zeroual et al., 2019), Assafir news articles, and 4 other manually crawled news websites (AlAkhbar, Annahar, AL-Ahram, AL-Wafd) from the Wayback Machine. The final model is trained on approximately 70M sentences containing roughly 3B Arabic tokens (Antoun et al., 2020). Arabic BERT: QARiB (mixed style text): This model is trained on the Arabic GigaWord corpus,15 Abulkhair Arabic Corpus (El-Khair, 2016) , and OpenSubtitles (Lison and Tiedemann, 2016) in addition to 50 million tweets that were collected by issuing the query “lang:ar” against Twitter API. The final training corpus contains 120M sentences and tweets c"
2021.eacl-main.227,N19-1423,0,0.00540796,"wnstream tasks lowered the need for extensive manual feature engineering. However, these pre-trained vectors are static and fail to handle polysemous words, where different instances of a word have to share the same representation regardless of context. More recently, different deep neural language models have been introduced to create contextualized word representations that can cope with the issue of polysemy and the context-dependent nature of words. Models such as OpenAi GPT (Radford et al., 2018), ELMo (Peters et al., 2018), BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), and UMLFIT (Howard and Ruder, 2018), to name a few, have achieved groundbreaking results in many NLP classification and language understanding tasks. For this paper, we use BERTbase-multilingual 2 (referred to hereafter simply as BERT), which we fine-tune for stance detection, as this eliminates the need for heavily engineered task-specific architectures. BERT is pre-trained on Wikipedia text from 104 languages and comes with hundreds of millions of parameters. It contains an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. As shown in Fig. 2, We fine-tune"
2021.eacl-main.227,P18-1031,0,0.0178712,"r extensive manual feature engineering. However, these pre-trained vectors are static and fail to handle polysemous words, where different instances of a word have to share the same representation regardless of context. More recently, different deep neural language models have been introduced to create contextualized word representations that can cope with the issue of polysemy and the context-dependent nature of words. Models such as OpenAi GPT (Radford et al., 2018), ELMo (Peters et al., 2018), BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), and UMLFIT (Howard and Ruder, 2018), to name a few, have achieved groundbreaking results in many NLP classification and language understanding tasks. For this paper, we use BERTbase-multilingual 2 (referred to hereafter simply as BERT), which we fine-tune for stance detection, as this eliminates the need for heavily engineered task-specific architectures. BERT is pre-trained on Wikipedia text from 104 languages and comes with hundreds of millions of parameters. It contains an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. As shown in Fig. 2, We fine-tuned BERT by adding a fully-connected de"
2021.eacl-main.227,2021.ccl-1.108,0,0.028989,"Missing"
2021.eacl-main.227,S16-1003,0,0.183739,"Missing"
2021.eacl-main.227,D14-1162,0,0.0884905,"tweets for a user, and we assigned the label with the highest average confidence to the user. As for features, we used all the words in tweets, and we preprocessed tweets in the manner described earlier for Supervised Classification As baselines, we used two different classification methods, namely 2640 1 http://svmlight.joachims.org/ SVM. We opted not use retweeted accounts only as the number of retweeted accounts was arbitrary for each user and fastText is not well suited for long input text. Contextualized Embeddings Over the last several years, pre-trained embedding (Mikolov et al., 2013; Pennington et al., 2014) have helped achieve significant improvements in a wide range of classification tasks in natural language processing. Representing words as vectors in a low-dimensional continuous space and then using them for downstream tasks lowered the need for extensive manual feature engineering. However, these pre-trained vectors are static and fail to handle polysemous words, where different instances of a word have to share the same representation regardless of context. More recently, different deep neural language models have been introduced to create contextualized word representations that can cope"
2021.eacl-main.227,W06-1639,0,0.0716425,"ificantly improve some supervised classification setups. • We conduct error analysis on our best setups to determine the sources of the errors and to guide the choice of classification methods. • We plan to release the tweet IDs of the test set along with the associate gold labels. Further, we plan to release the code that performs classification based on contextualized embeddings. 2 Related Work Over the last few years, much research has focused on user stance detection. The goal of stance detection is to ascertain the positions of users towards some target such as a topic, person, or claim (Thomas et al., 2006; Mohammad et al., 2016a; Barber´a, 2015; Barber´a and Rivero, 2014; BorgeHolthoefer et al., 2015; Cohen and Ruths, 2013; Colleoni et al., 2014; Conover et al., 2011; Fowler et al., 2011; Himelboim et al., 2013; Magdy et al., 2016a,b; Makazhanov et al., 2014; Weber et al., 2013). While stance may easily be detected by humans, machine learning models often fall short, particularly for users who talk about a target sparingly. Several studies have focused on modeling stance by introducing different features ranging from linguistic and structural features (Mohammad et al., 2016a) to network intera"
2021.eacl-main.227,S16-1074,0,0.113029,"fication features, the size of the training sets, the number of available tweets for users in the test set, and the classification algorithm (Borge-Holthoefer et al., 2015). Some common classification features include: lexical, syntactic, and semantics feature; network features such as retweeted accounts and user mentions; content features such as words and hashtags; and user profile information such as name and location (Aldayel and Magdy, 2019; Magdy et al., 2016a,b; Pennacchiotti and Popescu, 2011). Some commonly used classification algorithms include SVMs and deep learning classification (Zarrella and Marsh, 2016). Popat et al. 2638 (2019) presented a neural network model for stance classification by augmenting BERT representations with a novel consistency constraint to determine stance with respect to both a claim and perspective. We extend their work in two ways, namely: we drop the need to have a claim and perspective, and we couple BERT supervised classification with unsupervised classification to effectively tag vocal and non-vocal users. Semi-supervised methods such as label propagation (Barber´a, 2015; BorgeHolthoefer et al., 2015; Weber et al., 2013) often rely on two users retweeting identical"
2021.eacl-main.227,N18-1202,0,0.0104965,"ting words as vectors in a low-dimensional continuous space and then using them for downstream tasks lowered the need for extensive manual feature engineering. However, these pre-trained vectors are static and fail to handle polysemous words, where different instances of a word have to share the same representation regardless of context. More recently, different deep neural language models have been introduced to create contextualized word representations that can cope with the issue of polysemy and the context-dependent nature of words. Models such as OpenAi GPT (Radford et al., 2018), ELMo (Peters et al., 2018), BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), and UMLFIT (Howard and Ruder, 2018), to name a few, have achieved groundbreaking results in many NLP classification and language understanding tasks. For this paper, we use BERTbase-multilingual 2 (referred to hereafter simply as BERT), which we fine-tune for stance detection, as this eliminates the need for heavily engineered task-specific architectures. BERT is pre-trained on Wikipedia text from 104 languages and comes with hundreds of millions of parameters. It contains an encoder with 12 Transformer blo"
2021.eacl-main.227,D19-1675,0,0.0365763,"Missing"
2021.eacl-main.227,N16-3020,0,0.0962094,"Missing"
2021.eacl-main.227,2020.acl-main.50,1,0.748896,"common in the computational social science community, as opposed to tweetbased classification, which is common in the NLP community (Mohammad et al., 2016b). This is motivated by two aspects, namely: 1) tweets often don’t provide sufficient context for proper annotation; and 2) users have durable stances over time. For example, if someone says ”Most important election in history! Vote!”, it is nearly impossible to know if the author’s position without context. 3 Data Sets Topics Our dataset includes tweets on eight polarizing topics that are US-centric, which were graciously provided to us by Stefanov et al. (2020). Table 1 lists all the topics including when the tweets were collected and the number of tweets per topic. The topics include both long-standing issues such as gun control and transient issues such as the nomination of Judge Kavanaugh to the US Supreme Court. There is also a non-political issue, namely vaccination. The tweets were also filtered based on user-stated locations to limit the data to US users. The filtering was done using a gazetteer that includes either US (or its variants) and state names (and their abbreviations). Topic Climate change Gun control Ilhan Omar (remarks on Israeli"
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.wanlp-1.1,L18-1577,0,0.0352448,"Missing"
2021.wanlp-1.1,D14-1154,1,0.786389,"nd feminine forms with and without the definite article È@ (Al – the) such as ú ¯@ Q« (ErAqy - Iraqi (m.)), éJ ¯@ Q« (ErAqyp - Iraqi (f.)), and ú¯@ QªË@ (AlErAqy - the Iraqi (m.)). Arabic Variant Identification The second filter checks if the account mainly tweets in either dialectal Arabic or MSA. Since Arabic users commonly switch between MSA and dialectal Arabic, and we were interested in strictly dialectal tweets, we sought to filter out MSA tweets. There are multiple ways to distinguish between dialectal and MSA text. One such method involves using a list of strictly dialectal words (Darwish et al., 2014). However, constructing such lists across multiple dialects can be challenging. Thus, we opted to train a text classifier using a heuristically labeled tweets. Specifically, given 50 million tweets that we collected between March and September 2018, we assumed that tweets strictly containing the MSA ,úæË@  ,úæË@  YË@ relative pronouns áK ,ø YË@ ,ø YË@ (“Al*y, Al*Y, Alty, AltY, Al*yn” - who/that in masculine, feminine, and plural forms) were MSA, and those strictly containing the dialectal relative pronoun úÎË@ , ú ÎË@ (“Ally, AllY” – who/that) were dialectal. The major advantage of the diale"
2021.wanlp-1.1,2020.wanlp-1.9,0,0.0856536,"Missing"
2021.wanlp-1.1,R15-1015,0,0.0266832,"r tweets contained vulgar words. Removing the tweets of such users was motivated by the fact that their tweets contain strong genre specific signals, which may adversely affect the generalization of dialect identification. 99.5 100 Accuracy 92 90 99.5100 99 97 96.5 96 95 95 94 92 91.5 89.5 87.5 89 89 86.5 84.5 85 84.5 82.582.5 80 Normalization Tweets often contain tokens that are specific to the Twitter platform such as hashtags and user mentions. To improve generalization of the trained models (hopefully beyond tweets), we split hashtags into their semantic constituents (Bansal et al., 2015; Declerck and Lendvai, 2015) and replaced user mentions and URLs with “@USER” and “URL” respectively. 77 75 IQ BH KW SA AE OM QA YE SY JO PL LB EG SD LY TN DZ MA Country Figure 2: Annotation accuracy per country. Second annotators are colored in “Red”. The manually rejected tweets that the annotators classified as not from their dialects were mostly cases where the users interacted with or responded to users from different countries. In such cases, users tend to code-switch or adopt to other users’ dialects. For example, a user identified as Tunisian  ¢Ë@ Im&apos; AÓñÔ« AK@ (Ana EmwmA bHb tweeted ø ð@ éÒÊ . . AlZlmp Awy – I"
2021.wanlp-1.1,N19-1423,0,0.0195153,"SY ا ردن ا ك LB دا ة ز ّٰ ء ي ا وي ة ش ا را ل ا ي ء ا ا ا ا او ل وى د ادا ك اك ره ى ذو وة د ا ا ن ن خ PL م ا ول ن ت رض ف ي وي ا ازاي د ل إ ا د ك JO زول دم ا ّٰ ا ذ ر د ه ا SD زى دا را ن ن EG داك را ا TN ش را ا ا LY د ل ر أ ه MA ا رو د ة ا ن ا ى ا ر Also in other dialects Figure 3: Highest valence words for each country. contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. Both mBERT and AraBERT are pre-trained on identical architectures, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 selfattention heads. However, they differ in one major way. While mBERT is pre-trained on Wikipedia text for 104 languages,AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. For consistency with mBERT, we used Ar"
2021.wanlp-1.1,W19-4621,0,0.0272532,". 5.1 Representations Surface Features: We used two different surfacelevel features, namely word and character n-grams. Specifically, we represented tweets using: i) character n-grams, where we used 2 to 6-grams (C{2-6}); ii) word n-grams, where we used unigrams (W{1}) and unigrams to 6-grams (W{1-6}); and iii) a combination of word and character n-grams. For our dataset and MADAR , we normalized URLs, numbers, and user mentions to URL, NUM, and MENTION respectively. We used tf-idf weighting for character and word n-grams. Static Embeddings: We used Mazajak wordlevel skip-gram embeddings (Abu Farha and Magdy, 2019) that were trained on 250M Arabic tweets with 300-dimensional vectors. Deep Contextualized Embeddings: We also experimented with two pre-trained contextualized embeddings with fine-tuning for down-stream tasks, namely BERTbase-multilingual (mBERT) and AraBERT (Antoun et al., 2020). Recently, deep 5.2 Classification Models For classification, we used an SVM classifier and fine-tuned mBERT and AraBERT. We utilized the SVM classifier when using surface features and static pre-trained Mazajak embeddings. We used the Scikit Learn libsvm implementations of the SVM classifier with a linear kernel. Wh"
2021.wanlp-1.1,L18-1579,0,0.0346953,"Missing"
2021.wanlp-1.1,L18-1573,0,0.0439378,"Missing"
2021.wanlp-1.1,2020.osact-1.2,0,0.0924101,"8). Multiple approaches have been used for dialect ID that exploit a variety of features, such as character or word n-grams (Darwish et al., 2014; Zaidan and Callison-Burch, 2014; Malmasi et al., 2016; Sadat et al., 2014), and techniques such as multiple kernel learning (Ionescu and Popescu, 2016) and distributed representation of dialects (Abdul-Mageed et al., 2018; Zhang and Abdul-Mageed, 2019) to name a few. Zhang and Abdul-Mageed (2019) used semi-supervised learning using multilingual BERT for user-level dialect identification on the MADAR Shared Task. Arabic Tranformers-based approaches (Antoun et al., 2020; Safaya et al., 2020) showed competitive results in NADI (Abdul-Mageed et al., 2020) Shared Task. 3 • All Arab country names written in either Arabic, English, or French,4 such as H. QªÖ Ï @ (Almgrb – Morocco), Morocco, and Maroc respectively. • The names of major cities in these countries in both Arabic and English as specified in  (Alqds – Jerusalem) Wikipedia,5 such as Y®Ë@ and à@ Qëð (whrAn – Oran, Algeria). • Arabic adjectives specifying all nationalities in both masculine and feminine forms with and without the definite article È@ (Al – the) such as ú ¯@ Q« (ErAqy - Iraqi (m.)), éJ"
2021.wanlp-1.1,P13-2081,0,0.0694626,"Missing"
2021.wanlp-1.1,P18-1031,0,0.0182863,"ة ز ّٰ ء ي ا وي ة ش ا را ل ا ي ء ا ا ا ا او ل وى د ادا ك اك ره ى ذو وة د ا ا ن ن خ PL م ا ول ن ت رض ف ي وي ا ازاي د ل إ ا د ك JO زول دم ا ّٰ ا ذ ر د ه ا SD زى دا را ن ن EG داك را ا TN ش را ا ا LY د ل ر أ ه MA ا رو د ة ا ن ا ى ا ر Also in other dialects Figure 3: Highest valence words for each country. contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. Both mBERT and AraBERT are pre-trained on identical architectures, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 selfattention heads. However, they differ in one major way. While mBERT is pre-trained on Wikipedia text for 104 languages,AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. For consistency with mBERT, we used AraBERT with BP. Following Devlin e"
2021.wanlp-1.1,W16-4818,0,0.0376284,"Missing"
2021.wanlp-1.1,W19-4622,1,0.872945,"transcends geographical regions and borders. Automatically distinguishing between the different dialectal variations is valuable for many downstream applications such as machine translations (Diab et al., 2014), POS tagging (Darwish et al., 2020), geo-locating users, and author profiling (Sadat et al., 2014). Though there has been prior work on performing Arabic Dialect Identification (ADI), much of the work was conducted on datasets with significant limitations in terms of genre (Bouamor et al., 2018; Zaidan and Callison-Burch, 2011), number of dialects (Abdul-Mageed et al., 2018), or focus (Bouamor et al., 2019; Zaghouani and Charfi, 2018a), where often the focus was on geo-locating and profiling users as opposed to dialect identification. In this work, we expand beyond these efforts by utilizing tweets from across the MENA region to build a large, non-genre specific, fine-grained, and balanced country-level dialectal Arabic dataset that we use to build effective Arabic Dialect Identification. We rely on two main features to build the dataset. The first feature is the Twitter user profile description, where we identify users who self-declare themselves as belonging to a specific country in different"
2021.wanlp-1.1,W16-4801,0,0.0614661,"Missing"
2021.wanlp-1.1,J14-1006,0,0.0502226,"Missing"
2021.wanlp-1.1,W19-4637,0,0.0331781,"Missing"
2021.wanlp-1.1,W14-3601,1,0.826287,"Missing"
2021.wanlp-1.1,W17-3008,1,0.716641,"Missing"
2021.wanlp-1.1,W14-5904,0,0.0630664,"Missing"
2021.wanlp-1.1,2020.semeval-1.271,0,0.0745377,"Missing"
2021.wanlp-1.1,C18-1113,0,0.0161785,"gorithm treats each dialect as a singleton cluster at the outset and then successively merges (or agglomerates) clusters until all clusters have been merged into a single cluster that contains all dialects. Figure 4 shows the results of hierarchical clustering. The figure reflects the similarity and the geographical proximity of various dialects. At higher levels, dialects are grouped per region, where we can identify the major dialectal groups, namely Gulf, Maghrebi, Egyptian, and Levantine. This is aligned with geographical distribution of the dialects as well as the findings of prior work (Salameh et al., 2018). Corpus Statistics and Analysis Upon constructing the dataset, we attempted to explore its characteristics. First, we extracted features that are distinctive for each dialect. To do so, we computed the so-called valence score for each word in each dialect (Conover et al., 2011). The score helps determine the distinctiveness of a given word in a specific dialect in reference to other dialects. Given N (t, Di ), which is the frequency of the term t in Dialect Di , valence is computed as follows: V (t)i = N (t,Di ) N (Di ) 2 P N (t,D n) n N (Dn ) −1 (1) Where N (Di ) is the total number of occur"
2021.wanlp-1.1,K17-1043,1,0.906165,"Missing"
2021.wanlp-1.1,L18-1111,0,0.119407,"l regions and borders. Automatically distinguishing between the different dialectal variations is valuable for many downstream applications such as machine translations (Diab et al., 2014), POS tagging (Darwish et al., 2020), geo-locating users, and author profiling (Sadat et al., 2014). Though there has been prior work on performing Arabic Dialect Identification (ADI), much of the work was conducted on datasets with significant limitations in terms of genre (Bouamor et al., 2018; Zaidan and Callison-Burch, 2011), number of dialects (Abdul-Mageed et al., 2018), or focus (Bouamor et al., 2019; Zaghouani and Charfi, 2018a), where often the focus was on geo-locating and profiling users as opposed to dialect identification. In this work, we expand beyond these efforts by utilizing tweets from across the MENA region to build a large, non-genre specific, fine-grained, and balanced country-level dialectal Arabic dataset that we use to build effective Arabic Dialect Identification. We rely on two main features to build the dataset. The first feature is the Twitter user profile description, where we identify users who self-declare themselves as belonging to a specific country in different forms such as showing signs"
2021.wanlp-1.1,P11-2007,0,0.0826773,"Missing"
2021.wanlp-1.13,N16-3003,1,0.802265,"“prosti“doorman”) or tute”), and Q« (“ErS” – “pimp”). Figure 5 shows the top words with the highest valance scores for individual words in the offensive tweets. Larger fonts are used to highlight words with highest scores and align as well with the categories mentioned in the breakdown for the offensive languages. We slightly modified the valence score described by (Conover et al., 2011) to magnify its value by multiplying valence with frequency of occurrence. Data Pre-processing We performed several text pre-processing steps. First, we tokenized the text using the Farasa Arabic NLP toolkit (Abdelali et al., 2016). Second, we removed URLs, numbers, and all tweet specific tokens, namely mentions, retweets, and hashtags as they are not part of the language semantic structure, and therefore, not usable in pre-trained embeddings. Third, we performed basic Arabic letter normalization, namely variants of the letter alef to bare alef, ta marbouta to ha, and alef maqsoura to ya. We also separated words that are commonly incorrectly attached such as I . Ê¿ AK (“yAklb” – “O dog”), is split to I . Ê¿ AK (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example,"
2021.wanlp-1.13,W19-4621,0,0.0609864,"tive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented"
2021.wanlp-1.13,2020.osact-1.2,0,0.0282173,"to establish strong Arabic offensive language classification results. Though offensive tweets have finer-grained labels where offensive tweet could also be vulgar and/or hate speech, we conducted coarser-grained classification to determine if a tweet was offensive or not. For classification, we experimented with several tweet representation and classification models. For tweet representations, we used: the count of positive and negative terms, based on a polarity lexicon; static embeddings, namely fastText and SkipGram; and deep contextual embeddings, namely BERTbase-multilingual and AraBERT (Antoun et al., 2020). and 4.1 éJ ë@X ú ¯ hðP (“rwH fy dAhyp” – equivalent to “go to hell”). Name alteration: One common way to insult others is to change a letter or two in their names to produce new offensive words that rhyme with the original names. Some such examples include chang ing èQK Qm .Ì &apos;@ (“Aljzyrp” – “Aljazeera (channel)”) to èQK Q  m Ì &apos;@ (“Alxnzyrp” – “the pig”) and àA ®Ê g (“xl ¯Q k (“xrfAn” fAn” – “Khalfan (person name)”) to àA – “crazed”). Societal stratification: Some insults are associated with: certain jobs such as H . @ñK. (“bwAb” – ÐXAg (“xAdm” – “servant”); and specific societal compon"
2021.wanlp-1.13,Q17-1010,0,0.0340608,"used NileULex (El-Beltagy, 2016), which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dim"
2021.wanlp-1.13,N19-1423,0,0.0375937,"m model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not avail"
2021.wanlp-1.13,L16-1463,0,0.0294886,"ya. We also separated words that are commonly incorrectly attached such as I . Ê¿ AK (“yAklb” – “O dog”), is split to I . Ê¿ AK (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example, the token éêêêë (“hhhhh” – “hahahahaha”) is normalized to éë (“hh”). We also removed Arabic diacritics and word elongations (kashida). 4.2 Representations Lexical Features Since offensive words typically have a negative polarity, we wanted to test the effectiveness of using a polarity lexicon in detecting offensive tweets. For the lexicon, we used NileULex (El-Beltagy, 2016), which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vec"
2021.wanlp-1.13,P18-1031,0,0.0286332,"the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wiki"
2021.wanlp-1.13,E17-2068,0,0.0822972,"an Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classifi"
2021.wanlp-1.13,malmasi-zampieri-2017-detecting,0,0.0532717,"Missing"
2021.wanlp-1.13,W17-3008,1,0.85101,"Missing"
2021.wanlp-1.13,D16-1264,0,0.0502861,"BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages, and AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. Both use identical architectures and come with hundreds of millions of parameters. Both contain an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. These embedding use BP sub-word segments. Following Devlin et al. (2019), the classification consists of introducing a dense layer over the final hidden state h corresponding to fi"
2021.wanlp-1.13,K17-1043,1,0.897502,"Missing"
2021.wanlp-1.13,W18-5446,0,0.0297172,". Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages, and AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. Both use identical architectures and come with hundreds of millions of parameters. Both contain an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. These embedding use BP sub-word segments. Following Devlin et al. (2019), the classification consists of introducing a"
2021.wanlp-1.13,N16-2013,0,0.0892398,"Missing"
2021.wanlp-1.13,S19-2010,0,0.10723,"Missing"
D11-1128,P05-1074,0,0.116412,"Missing"
D11-1128,W10-2407,1,0.706143,"rseness in the training data). Further, resultant mappings may not be observed a sufficient of times and hence their mapping probabilities may be inaccurate. Different methods were proposed to solve these two problems. These methods focused on making training data less sparse by performing some kind of letter conflation. Oh and Choi (2006) used a SOUNDEX like scheme. SOUNDEX is used to convert English words into a simplified phonetic representation, in which vowels are removed and phonetically similar characters are conflated. A variant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity transl"
D11-1128,W07-0711,0,0.0680971,"t of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequences. The alignment was performed using a Bayesian learner that was trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced mappings of source character sequences to target character sequences along with the probability of source given target and vice versa. Source character sequences were restricted to be 1 to 3 characters long. For all the work reported herein, given an English-foreign language transliteration candidate pair, English was treated as the target language and the foreign language as the source. Given a foreign source language word sequence and an English target word sequence , could be a potential transliteration of . An example of word sequences pair is the TamilEnglish pair: (ம"
D11-1128,I08-4002,0,0.0882892,"ngs, which are often erroneous. This positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possibl"
D11-1128,N06-1011,0,0.0379826,"along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probability estimates of obtained mappings may be inaccurate. These two problems affect recall and precision respectively. 1.1 Finding Character Mappings To find character sequence mappings bet"
D11-1128,I08-4003,0,0.142544,"Missing"
D11-1128,W03-0317,0,0.065917,"tively. 1.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Akin to phrasal alignment in machine translation, character sequence alignment is treated as a word alignment problem between parallel sentences, where transliteration pairs are treated as if they are parallel sentences and the characters from which they are composed are treated as if they are words. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transl"
D11-1128,W10-2408,0,0.155149,"tively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been 1386 generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara,"
D11-1128,W09-3407,0,0.0969206,"4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probability estimates of obtained mappings may be inaccurate. These two problems affect recall and precision respectively. 1.1 Finding Character"
D11-1128,I08-6004,0,0.0721402,"is positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source a"
D11-1128,E09-1091,0,0.0362121,"d character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Akin to phrasal alignment in machine translation, character sequence alignment is treated as a word alignment problem between parallel sentences, where transliteration pairs are treated as if they are parallel sentences and the characters from which they are composed are treated as if they are words. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach"
D11-1128,D10-1122,0,0.17722,"Missing"
D11-1128,P08-1089,0,0.0380314,"rs are conflated. A variant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequences. The alignment was performed using a Bayesian learner that was trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced mappings of source character sequences"
D11-1128,W10-2405,0,\N,Missing
D11-1128,D10-1042,0,\N,Missing
D11-1128,W10-2403,0,\N,Missing
D11-1128,P06-1142,0,\N,Missing
D11-1128,N10-1017,0,\N,Missing
D14-1154,cotterell-callison-burch-2014-multi,0,0.4921,"Missing"
D14-1154,N12-1006,0,0.042694,"rb suffixes such as “yn” instead of “wn” and “wA” instead of “wn” respectively. Also, so-called “five nouns”, are used in only one form (ex. “&gt;bw” (father of) instead of “&gt;bA” or “&gt;by”). 4 Detecting Dialectal Peculiarities ARZ is different from MSA lexically, morphologically, phonetically, and syntactically. Here, we present methods to handle such peculiarities. We chose not to handle syntactic differences, because they may be captured using word n-gram models. To capture lexical variations, we extracted and sorted by frequency all the unigrams from the Egyptian side of the LDC2012T09 corpus (Zbib et al., 2012), which has ≈ 38k Egyptian-English parallel sentences. A linguist was tasked with manually reviewing the words from the top until 1,300 dialectal words were found. Some of the words on the list included dialectal words, commonly used foreign words, words that exhibit morphological variations, and others with letter substitution. 1466 For morphological phenomenon, we employed three methods, namely: • Unsupervised Morphology Induction: We employed the unsupervised morpheme segmentation tool, Morfessor (Virpioja et al., 2013). It is a data driven tool that automatically learns morphemes from data"
D14-1154,I11-1062,0,0.0283516,"Missing"
D14-1154,C12-1160,0,0.0521899,"Missing"
D14-1154,P11-2007,0,\N,Missing
D14-1154,J14-1006,0,\N,Missing
D14-1154,darwish-etal-2014-using,1,\N,Missing
D14-1154,P13-2081,0,\N,Missing
D19-3037,W12-2301,0,0.0839234,"Missing"
D19-3037,N07-2014,0,0.106619,"Missing"
D19-3037,N16-3003,1,0.913237,"two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA)"
D19-3037,W17-1305,0,0.0467909,"Missing"
D19-3037,P17-4012,0,0.0277426,"er are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA). However, diacritics are required to properly pronounce words, which makes diacritic restoration (a.k.a. diacritization) essential for language learning and text-to-speech applications. In this paper, we present a system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and Tunisia"
D19-3037,W18-2507,0,0.0275433,"Missing"
D19-3037,D15-1274,0,0.0528553,"Missing"
D19-3037,D17-1151,0,0.0215157,"Missing"
D19-3037,N19-1248,1,0.599941,"ieties of Arabic Hamdy Mubarak Ahmed Abdelali Kareem Darwish Mohamed Eldesouki Younes Samih Hassan Sajjad {hmubarak,aabdelali}@qf.org.qa Qatar Computing Research Institute, HBKU Research Complex, Doha 5825, Qatar Abstract the diacritics, a prerequisite for Language Learning (Asadi, 2017) and Text to Speech (Sherif, 2018) among other applications. In this paper, we present a system that employs a character-based sequence-to-sequence model (seq2seq) (Britz et al., 2017; Cho et al., 2014; Kuchaiev et al., 2018) for diacritizing four different varieties of Arabic. We use the approach described by Mubarak et al. (2019), which they applied to MSA only, to build a system that effectively diacritizes MSA, CA, and and two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles con"
D19-3037,pasha-etal-2014-madamira,0,0.0626661,"Missing"
D19-3037,W17-1302,1,0.833503,"Missing"
D19-3037,W04-1612,0,0.189762,"Missing"
D19-3037,W02-0504,0,0.274384,"Missing"
D19-3038,P15-2072,0,0.0737003,"Missing"
D19-3038,D19-1565,1,0.834147,"Missing"
D19-3038,E17-3016,1,0.8158,"ehow (0.4 ≤ p &lt; 0.6), likely (0.6 ≤ p &lt; 0.8), and very likely (p ≥ 0.8). Crawlers and Translation Our crawlers collect articles from a growing list of sources10 , which currently includes 155 RSS feeds, 82 Twitter accounts and two websites. Once a link to an article has been obtained from any of these sources, we rely on the Newspaper3k Python library to extract its contents.11 After deduplication based on both URL and text content, our crawlers currently download 7k-10k articles per day. As of present, we have more than 700k articles stored in our database. We use QCRI’s Machine Translation (Dalvi et al., 2017) to translate English content into Arabic and vice versa. Since translation is performed offline, we select the most accurate system in Dalvi et al. (2017), i.e., the neural-based one. 8 http://kafka.apache.org http://kubernetes.io 10 http://www.tanbih.org/about 11 http://newspaper.readthedocs.io 9 12 http://github.com/several27/ FakeNewsCorpus 224 2.4 Framing Bias Detection We implemented our stance detection model as fine-tuning of BERT on the FNC-1 dataset from the Fake News Challenge13 . Our model outperformed the best submitted system (Hanselowski et al., 2018), obtaining an F1macro of 75"
D19-3038,C18-1158,0,0.133435,"Missing"
D19-3038,D18-1389,1,0.607642,"Missing"
D19-3038,N18-5006,1,0.812156,"Missing"
D19-3038,N19-1216,1,0.55511,"Missing"
D19-3038,P17-1092,0,0.0313814,"Missing"
D19-3038,D18-1483,0,0.0187346,"11): V (u) = tf (u,C0 ) total(C0 ) 2 tf (u,C ) tf (u,C1 ) 0 total(C0 ) + total(C1 ) −1 (1) Figure 2: The Tanbih main page. where tf (u, C0 ) is the number of times (term frequency) item u is cited by group C0 , and total(C0 ) is the sum of the term frequencies of all items cited by C0 . tf (u, C1 ) and total(C1 ) are defined in a similar fashion. We subdivided the range between -1 and 1 into 5 equal size ranges and we assigned the labels far-left, left, center, right, and far-right to those ranges. 2.9 The model achieved state-of-the-art performance on the testing partition of the corpus from Miranda et al. (2018): an F1 of 98.11 and an F1BCubed of 94.41.15 As a comparison, the best model described in (Miranda et al., 2018) achieved an F1 of 94.1. See Staykovski et al. (2019) for further details. Event Identification / Clustering 3 The clustering module aggregates news articles into stories. The pipeline is divided into two stages: (i) local topic identification and (ii) longterm topic matching for story generation. For step (i), we represent each article as a TF.IDF vector, built from the title and the body concatenated. The pre-processing consists of casefolding, lemmatization, punctuation and stopwo"
darwish-etal-2014-using,N03-1033,0,\N,Missing
darwish-etal-2014-using,P03-1051,0,\N,Missing
darwish-etal-2014-using,E12-1069,0,\N,Missing
darwish-gao-2014-simple,E12-1017,0,\N,Missing
darwish-gao-2014-simple,W10-2417,1,\N,Missing
darwish-gao-2014-simple,W09-1119,0,\N,Missing
darwish-gao-2014-simple,D08-1030,0,\N,Missing
darwish-gao-2014-simple,W07-0803,0,\N,Missing
darwish-gao-2014-simple,P13-1153,1,\N,Missing
darwish-gao-2014-simple,P11-1037,0,\N,Missing
darwish-gao-2014-simple,W03-0430,0,\N,Missing
darwish-gao-2014-simple,D11-1141,0,\N,Missing
darwish-gao-2014-simple,farber-etal-2008-improving,0,\N,Missing
darwish-gao-2014-simple,W09-2208,0,\N,Missing
darwish-gao-2014-simple,P07-1033,0,\N,Missing
K17-1043,N16-3003,1,0.913316,"suffixes for each dialect in comparison to MSA. As the tables show, MGR has the most number of prefixes, while GLF has the most number of suffixes. Further, there are certain prefixes and suffixes that are unique to dialects. While the prefix “Al” (the) leads the list of prefixes for all dialects, the prefix H . “b” in LEV and EGY, where it is either a progressive particle or a preposition, is used more frequently than in MSA, where it is used strictly as a preposition. Similarly, the suffix “kn” (your) is more frequent in LEV than any á» 5.1 We used the SVM-based ranking approach proposed by Abdelali et al. (2016), in which they used SVM based ranking to ascertain the best segmentation for Modern Standard Arabic (MSA), which they show to be fast and of high accuracy. The approach involves generating all possible segmentations of a word and then ranking them. The possible segmentations are generated based on possible prefixes and suffixes that are observed during training. For example, if hypothetically we only had the prefixes ð “w” (and) and È “l” (to) other dialect. The Negation suffix  “$” (not) and feminine suffix marker No. 8 11 11 14 19 Top 5 Al,w,l,b,f Al,b,w,m,h Al,b,w,l,E Al,w,b,l,mA Al,w,l,"
K17-1043,habash-etal-2012-conventional,0,0.0821494,"ained for each dialect and the number of words they contain. Dialect Egyptian Levantine Gulf Maghrebi No of Tokens 6,721 6,648 6,844 5,495 Table 1: Dataset size for the different dialects We manually segmented each word in the corpus while preserving the original characters. This decision was made to allow processing real dialectal words in their original form. Table 2 shows segmented examples from the different dialects. 3.1 Segmentation Convention In some research projects, segmentation of DA is done on a CODA’fied version of the text, where CODA is a standardized writing convention for DA (Habash et al., 2012). CODA guidelines provide directions on to how to normalize words, correct spelling and unify writing. Nonetheless, these guidelines are not available for all dialects. In the absence of such guidelines as well as the dynamic nature of the language, we choose to operate directly on the raw text. As in contrast to MSA, where guidelines for spelling are common and standardized, written DA seems to exhibit a lot of diversity, and hence, segmentation systems need to be robust enough to handle all the variants that might be encountered in such texts. Our segmentation convention is closer to stemmin"
K17-1043,W11-4417,1,0.839746,"Missing"
K17-1043,N13-1044,0,0.142459,"val. Though much work has focused on segmenting Modern Standard Arabic (MSA), recent work began to examine dialectal segmentation in some Arabic dialects. Dialectal segmentation is becoming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHO"
K17-1043,W09-0807,0,0.0760766,"Missing"
K17-1043,bouamor-etal-2014-multidialectal,0,0.0679135,"Missing"
K17-1043,N16-1030,0,0.011125,"A+MSA). 5.2 Figure 2: Architecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last sev"
K17-1043,D14-1154,1,0.904598,"Missing"
K17-1043,W16-4828,1,0.82771,"social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects"
K17-1043,P16-1101,0,0.0180724,"itecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last several years, they have ac"
K17-1043,maamouri-etal-2014-developing,0,0.0662827,"Missing"
K17-1043,mohamed-etal-2012-annotating,0,0.12055,"Missing"
K17-1043,P14-2034,0,0.0769493,"Missing"
K17-1043,W14-3601,1,0.936065,"Missing"
K17-1043,pasha-etal-2014-madamira,0,0.10814,"Missing"
K17-1043,W17-1306,1,0.586168,"Missing"
K17-1043,J14-1006,0,0.0520662,"important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sente"
K17-1043,N12-1006,0,0.0492725,"oming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), wh"
L16-1170,W11-4417,0,0.0705104,"Missing"
L16-1170,C96-1017,0,0.587632,"more accurate and much faster than the current stateof-the-art segmenters. • the introduction of a new test set to evaluate word segmentation. We plan to make this tool and data freely available. A demo of the segmenter is available online at: http://qatsdemo.cloudapp.net/farasa/ 2. Related Work Due to the morphological complexity of the Arabic language, morphological processing such as word segmentation helps recover the units of meaning or their proxies, such as stems (or perhaps roots). Most early Arabic morphological analyzers generally used finite state transducers (Beesley et al., 1989; Beesley, 1996; Kiraz, 1998). Their use is problematic for two reasons. First, they were designed to produce as many analyses as possible without indicating which analysis is most likely. This property of the analyzers complicate subsequent NLP as many applications require the most likely solution. Second, the use of finite state transducers inherently limits coverage, which is the number of words that the analyzer can analyze, to the cases programmed into the transducers. Other similar approaches attempt to find all possible prefix and suffix combinations in a word and then try to match the remaining 1070"
L16-1170,darwish-etal-2014-using,1,0.791867,"Missing"
L16-1170,W02-0506,1,0.22553,"Missing"
L16-1170,N13-1044,0,0.0208869,"Missing"
L16-1170,W98-1009,0,0.167865,"nd much faster than the current stateof-the-art segmenters. • the introduction of a new test set to evaluate word segmentation. We plan to make this tool and data freely available. A demo of the segmenter is available online at: http://qatsdemo.cloudapp.net/farasa/ 2. Related Work Due to the morphological complexity of the Arabic language, morphological processing such as word segmentation helps recover the units of meaning or their proxies, such as stems (or perhaps roots). Most early Arabic morphological analyzers generally used finite state transducers (Beesley et al., 1989; Beesley, 1996; Kiraz, 1998). Their use is problematic for two reasons. First, they were designed to produce as many analyses as possible without indicating which analysis is most likely. This property of the analyzers complicate subsequent NLP as many applications require the most likely solution. Second, the use of finite state transducers inherently limits coverage, which is the number of words that the analyzer can analyze, to the cases programmed into the transducers. Other similar approaches attempt to find all possible prefix and suffix combinations in a word and then try to match the remaining 1070 stem to a list"
L16-1170,P03-1051,0,0.0708448,"Missing"
L16-1170,P14-2034,0,0.0121578,"plifying assumption that word-context is not necessary to find the correct segmentation of a word. Though a word may have multiple valid segmentations, our results show that this assumption minimally impacts our results. We also report evaluation results on a new test set that we developed to measure segmentation accuracy. We also compare to other state-of-the-art segmenters, namely MADAMIRA (Pasha et al., 2014) and QCRI Advanced Tools For ARAbic (QATARA) (Darwish et al., 2014). 1 Buckwalter encoding is used exclusively in the paper. We also wanted to compare to the Stanford Arabic segmenter (Monroe et al., 2014), but its segmentation scheme was different from the one we used in Farasa. Many of the current results reported in the literature are done on subsets of the Penn Arabic Treebank (ATB) (Maamouri et al., 2005). Since the aforementioned tools are trained on the ATB, testing on a subset of the ATB is problematic due to its limited lexical diversity and the similarity between the training and test sets. This generally leads to results that are often artificially high. Our new dataset is composed of recent WikiNews articles from the years 2013 and 2014. The contributions of this paper are: • the de"
L16-1170,pasha-etal-2014-madamira,0,0.262305,"Missing"
L16-1170,2013.iwslt-evaluation.8,0,0.0183226,"Missing"
L18-1015,N16-3003,1,0.699404,"weet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is related more to the linguistic nature of the language rather than the genre. 4. 4.1. would be effective for dialects also, particularly given the overlap between MSA and dialectal Arabic. We used Farasa to determine stem templates (Abdelali et al., 2016). For all the experiments, we trained on the training and dev parts and tested on the test part. As mentioned earlier, we also randomly selected 350 MSA sentences from Arabic Penn Treebank (ATB) and treated MSA as a language variety. Doing so would allow us to observe the divergence of dialects from MSA and the relative effectiveness of using a small dataset compared to much more data. Experiments and Evaluation Experimental Setup For the experiments that we conducted, we used the CRF++ implementation of a CRF sequence labeler with L2 regularization and default value of 10 for the generalizati"
L18-1015,al-sabbagh-girju-2010-mining,0,0.0566271,"Missing"
L18-1015,bouamor-etal-2014-multidialectal,0,0.0463357,"Missing"
L18-1015,J92-4003,0,0.113623,"Missing"
L18-1015,cotterell-callison-burch-2014-multi,0,0.0466115,"Missing"
L18-1015,D14-1154,1,0.889503,"Missing"
L18-1015,W17-1316,1,0.769801,"ur justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouk"
L18-1015,W05-0708,0,0.819563,"Missing"
L18-1015,P06-1086,0,0.314234,"Missing"
L18-1015,habash-etal-2012-conventional,0,0.027441,"prepositions, numbers, and definite articles appear more frequently in MSA than in dialects, while on the other hand dialects show higher frequency of verbs, pronouns and particles. Our justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for"
L18-1015,N13-1044,0,0.508721,"Missing"
L18-1015,N13-1039,0,0.0374619,"Missing"
L18-1015,P08-2030,0,0.65644,"Missing"
L18-1015,W17-1306,1,0.871249,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,K17-1043,1,0.86893,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,W15-1511,0,0.189752,"Missing"
L18-1015,P11-2007,0,0.0736066,"Missing"
L18-1620,N16-3003,1,0.859153,"was considered as a class, and a set of features mentioned at the end of the section were extracted for each clitic and used to train the SVM classifier. In this work we use a combination of features that includes probabilistic, binary, and Arabic-specific features. For probabilistic features we used a combination of bigrams, trigrams, and 4grams of tags and clitics. For binary features we used some features including meta-types of clitics, which indicate if a clitic is a number, a foreign word, a user mention or a URL. For Arabic specific features, we used stem template feature introduced by Abdelali et al. (2016). Where stem template represents the word pattern applied to the root mentioned in section 2.1 . The template for each clitic has been extracted and concatenated to word representation. The set of used features for SVM are: 1. Clitic features: each unique clitic in our training set acted as a feature, and an additional feature is added to represent out-of-vocabulary (OOV) clitics. We experimented with three different values for clitic features. The first value is binary (whether it exists or not). The second is the log of clitic counts in training data. The 3927 third is the Term Frequency-Inv"
L18-1620,W17-1316,1,0.896696,"Approach, Maximum Entropy Approach, Support Vector Machine(SVM) Approach and Neural Network Approach (Wilks, 1996). In this section we present our POS tagging approach; first we describe the set of features we extracted, then we discuss the two machine learning approaches we used, which are SVM and Bi-LSTM. It is worth mentioning that our taggers operate at clitic level instead of word level where a clitic is a word segment that has single POS tag. 3.1. SVM Based POS Tagger SVM is used in many NLP classification tasks including POS tagging and proves to achieve high accuracy results with MSA (Darwish et al., 2017; Gim´enez and M`arquez, 2003). For this work, we used an SVM multi-class, specifically the SVMmulticlass tool developed by Thorsten Joachims (Joachims, 2008). SVMmulticlass uses regularization parameter C to prevent overfitting (Manning et al., 2009). Each tag of POS tags was considered as a class, and a set of features mentioned at the end of the section were extracted for each clitic and used to train the SVM classifier. In this work we use a combination of features that includes probabilistic, binary, and Arabic-specific features. For probabilistic features we used a combination of bigrams"
L18-1620,N07-5003,0,0.0268657,"(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-Subject-Object (VSO) in MSA(Diab and Habash, 2007). is written as l .  [sQ IdZ] in some Gulf dialects variants. Another result of writing words as they are pronounced is that some letters are dropped when pro nounced. For example the word Y«A¯[qa:QId], which ©ËA£ [tQ a:lIQ], which means ’look’ is written as ¨A£ [tQ a:Q] in Kuwaiti Gulf dialect. Éªm.&apos; [najaQl] is the stem and each one of these segments is called clitic. For more explanation see (Darwish and Magdy, 2014; Habash, 2010). Researchers usually consider five main dialects for DA, namely: Egyptian, Iraqi, Levantine, Maghribi, and Gulf (Samih et al., 2017). Although Gulf Arabic is"
L18-1620,W05-0708,0,0.743529,"Missing"
L18-1620,P06-1086,0,0.326157,"Missing"
L18-1620,habash-etal-2012-conventional,0,0.01948,"eir grammars while dialects have no strict rules. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-S"
L18-1620,W12-2301,0,0.345569,"eir grammars while dialects have no strict rules. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-S"
L18-1620,L16-1679,0,0.217841,"s. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-Subject-Object (VSO) in MSA(Diab and Habash, 2007)"
L18-1620,D15-1176,0,0.0960929,"Missing"
L18-1620,pasha-etal-2014-madamira,0,0.305784,"Missing"
L18-1620,P16-2067,0,0.0215024,". È@ [al] determiner. We also include meta-type feature which is an additional information added about the type of clitic i.e. to specify whether it is a number, an adjective number, a prefix, a suffix, a foreign, a punctuation, an Arabic letter and twitter specific types: hashtags, URLs and mentions. 4. 4.1. Bi-LSTM Based POS Tagger Bi-LSTM is a special type of Recurrent Neural Network (RNN). It has proved to be a good choice for sequence modeling tasks (Ling et al., 2015) such as speech processing, POS tagging, phrased based chunking ... etc. It is also less sensitive to training data size (Plank et al., 2016). Moreover, Bi-LSTM can capture the context around source words up to very long sequences in both directions (previous and upfront) (Wang et al., 2015). It also does not need hand crafted features to work well. These characteristics make it a suitable fit for POS tagging of DA. Since there is not much training data available for DA – GA in this case – and since DA lacks standards to design powerful features, a model is needed that auto-fits its features and characteristics. Bi-LSTM structure differs from the classic RNN in that it adds a memory cell to the neural network architecture that lear"
L18-1620,K17-1043,1,0.910025,"ct-Object (VSO) in MSA(Diab and Habash, 2007). is written as l .  [sQ IdZ] in some Gulf dialects variants. Another result of writing words as they are pronounced is that some letters are dropped when pro nounced. For example the word Y«A¯[qa:QId], which ©ËA£ [tQ a:lIQ], which means ’look’ is written as ¨A£ [tQ a:Q] in Kuwaiti Gulf dialect. Éªm.&apos; [najaQl] is the stem and each one of these segments is called clitic. For more explanation see (Darwish and Magdy, 2014; Habash, 2010). Researchers usually consider five main dialects for DA, namely: Egyptian, Iraqi, Levantine, Maghribi, and Gulf (Samih et al., 2017). Although Gulf Arabic is the largest existing dialect in social media, there is very limited attention towards building NLP tools for it. DA is derived from MSA; nevertheless, they differ at many linguistic levels. Some notable differences are in terms of: ¨A¯ [qa:Q], and the the verb h@P [raaè] are used to indicate future tense. In addition, the words I . Ó [mub], H. ñÓ [mob], AÓ [ma:], ñëAÓ [ma:hu] and H . ñëAÓ [ma:hu:b] are used for negation (Khalifa et al., 2016). These differences emphasize the need for specially designed NLP tools for dialects to prevent the performance drop when using"
N12-1025,N06-1011,0,0.117521,"th such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonet"
N12-1025,P07-2045,0,0.00490506,"Missing"
N12-1025,I08-4003,0,0.0196278,"paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach en"
N12-1025,W03-0317,0,0.0347811,"anese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to"
N12-1025,W07-0711,0,0.0323207,"f source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs at character level using an HMM-based aligner akin to that of He (2007). Alignment produced mappings between characters from both languages with associated probabilities. We restricted individual source language character sequences to be 3 characters at most. We always treated English as the target language and Arabic as the source language. Briefly, we produced all possible segmentations of a source word along with their associated mappings into the target language. Valid target sequences were retained and sorted by the product of the constituent mapping probabilities. The candidate with the highest probability was generated given that the product of the mapping"
N12-1025,W11-1209,0,0.0120284,"ow that we can effectively learn the parameters that tune the penalty for two different training sets 244 of varying sizes. In doing so, we achieve better results for graph reinforcement with larger training sets. 2. For large comparable texts, we use contextual clues, namely translations of neighboring words, to constrain TM and to preserve precision. Specifically, we initially extract text segments that are “related” based on cross lingual lexical overlap, and then we perform TM on these segments. Though there have been some papers on extracting sub-sentence alignments from comparable text (Hewavitharana and Vogel, 2011; Munteanu and Marcu, 2006), extracting related (as opposed to parallel) text segments may be preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences may be sufficient to produce high TM precision. Alternate solutions focused on performing TM on extracted named entities only (Udupa et al., 2009b). Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for la"
N12-1025,W10-2408,0,0.0270918,"can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences fr"
N12-1025,W10-2405,0,0.0594258,"a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences fr"
N12-1025,W09-3407,0,0.024924,"reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speec"
N12-1025,I08-4002,0,0.0163428,"as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings between two languages and using these mappings to ascertain if two words are transliterations or not. 2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using autom"
N12-1025,N10-1063,0,0.0288181,"Missing"
N12-1025,E09-1091,0,0.307276,"llel or comparable texts of different languages. For example, given the Arabic-English word sequence pairs: ( ﺍاﻟﻤﻠﻚ ﻫﮬﮪھﺎﻟﻲ ﺳﻼﺳﻲ, Haile Selassie I of Ethiopia), successful TM would mine the transliterations: (ﻫﮬﮪھﺎﻟﻲ, Haile) and (ﺳﻼﺳﻲ, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al., 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem. Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (Udupa and Kumar, 2010b). TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop. In that evaluation, TM was perform"
N12-1025,D10-1122,0,0.137972,"ry words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al., 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem. Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (Udupa and Kumar, 2010b). TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop. In that evaluation, TM was performed using limited training data, namely 1,000 parallel transliteration word-pairs, on short parallel text segments, namely cross-language Wikipedia titles which were typically a few words long. Since TM was performed on very short parallel segments, the chances that two phonetically similar words would appear within such a short text segment in one language were typically very low. Also, since TM training datasets were small, many valid mappings were not obs"
N12-1025,P08-1089,0,0.0462477,"Missing"
N12-1025,W10-2407,1,\N,Missing
N12-1025,D11-1128,1,\N,Missing
N12-1025,D10-1042,0,\N,Missing
N12-1025,W10-2403,0,\N,Missing
N12-1025,J03-3002,0,\N,Missing
N12-1025,P06-1142,0,\N,Missing
N12-1025,P06-1011,0,\N,Missing
N12-1025,P05-1074,0,\N,Missing
N12-1025,N10-1017,0,\N,Missing
N12-1025,I08-6004,0,\N,Missing
N15-1005,W13-4916,0,0.0474771,"Missing"
N15-1005,D12-1133,0,0.0834933,"Missing"
N15-1005,Q13-1034,0,0.0481971,"Missing"
N15-1005,D07-1022,0,0.09079,"n is an appealing alternative for pipeline architectures (Goldberg and Tsarfaty, 2008; Hatori et al., 2012; Habash and Rambow, 2005; GahbicheBraham et al., 2012; Zhang and Clark, 2008; Bohnet and Nivre, 2012). These approaches have been particularly prominent for languages with difficult preprocessing, such as morphologically rich languages (e.g., Arabic and Hebrew) and languages that require word segmentation (e.g., Chinese). For the former, joint prediction models typically rely on a lattice structure to represent alternative morphological analyses (Goldberg and Tsarfaty, 2008; Tratz, 2013; Cohen and Smith, 2007). For instance, transitionbased models intertwine operations on the lattice with operations on a dependency tree. Other joint architectures are more decoupled: in Goldberg and Tsarfaty (2008), a lattice is used to derive the best morphological analysis for each part-of-speech alternative, which is in turn provided to the parsing algorithm. In both cases, tractable inference is achieved by limiting the representation power of the scoring function. Our model also uses a lattice to encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full pat"
N15-1005,W02-1001,0,0.186961,"words. System Variants We also compare against a pipeline variation of our model. In our pipeline model, we predict segmentations and POS tags by the same system that we use to generate candidates. The subsequent standard parsing model then operates on the predicted segmentations and POS tags. 5.5 Experimental Details Following our earlier work (Zhang et al., 2014b), we train a first-order classifier to prune the dependency tree space.10 Following common practice, we average parameters over all iterations after training with passive-aggressive online learning algorithm (Crammer et al., 2006; Collins, 2002). We use the same adaptive random restart strategy as in our earlier work (Zhang et al., 2014b) and set K = 300. In addition, we also apply an aggressive early-stop strategy during training for efficiency. If we have found a violation against the ground truth during the first 50 iterations, we immediately stop and update the parameters based on the current violation. The reasoning behind this early-stop strategy is that weaker violations for some training sentences are already sufficient for separable training sets (Huang et al., 2012). 6 Results Comparison to State-of-the-art Systems Table 4"
N15-1005,darwish-etal-2014-using,1,0.297104,"entence, MADA provides a list of possible morphological analyses and POS tags, each associated with a score. The score of each segmentation or POS tag equals the highest score of the MADA analysis in which it appears. In addition, we associate each segmentation with MADA analyses on gender, number and person. Figure 5 shows an example of MADA output for the token Emlyp and the corresponding lattice structure. Classical Arabic We construct the lattice for this corpus in a similar fashion to the SPMRL dataset with two main departures. First, we use the Arabic morphological analyzer developed by Darwish et al. (2014) because MADA is primarily trained for MSA and performs poorly on classical Arabic. Second, we implement a CRF-based morpheme-level POS tagger and generate the POS tag candidates for each morpheme based on their marginal probabilities, truncated by a probability threshold. CTB5 We first re-train the Stanford Chinese word segmenter on CTB5 and generate a top-10 list for each sentence.7 We treat the word boundaries shared across all the 10 candidates as the confident ones, 7 We use 10-fold cross validation to avoid overfitting on the training set. 47 and construct the lattice as described in Sec"
N15-1005,gahbiche-braham-etal-2012-joint,0,0.0290913,"Missing"
N15-1005,P11-2124,0,0.110668,"Missing"
N15-1005,P08-1043,0,0.381847,"Missing"
N15-1005,P05-1071,0,0.302265,"Missing"
N15-1005,P09-2056,0,0.0516058,"Missing"
N15-1005,I11-1136,0,0.104871,"marginal probabilities, truncated by a probability threshold. CTB5 We first re-train the Stanford Chinese word segmenter on CTB5 and generate a top-10 list for each sentence.7 We treat the word boundaries shared across all the 10 candidates as the confident ones, 7 We use 10-fold cross validation to avoid overfitting on the training set. 47 and construct the lattice as described in Section 3.4. Our model then focuses on disambiguating the rest of the word boundaries in the candidates. To generate POS candidates, we apply a CRF-based tagger with Chinese-specific features used in previous work (Hatori et al., 2011). 5.3 Evaluation Measures Following standard practice in previous work (Hatori et al., 2012; Zhang et al., 2014a), we use Fscore as the evaluation metric for segmentation, POS tagging and dependency parsing. We report the morpheme-level F-score for Arabic and the wordlevel F-score for Chinese. In addition, we use TedEval (Tsarfaty et al., 2012) to evaluate the joint prediction on the SPMRL dataset, because TedEval score is the only evaluation metric used in the official report. We directly use the evaluation tools provided on the SPMRL official website.8 5.4 Baselines State-of-the-Art Systems"
N15-1005,P12-1110,0,0.461808,"to the dependency parse. The search space for the algorithm is a combination of parse trees and lattices that encode alternative morphological and POS analyses. The inference algorithm greedily searches over this space, iteratively making local modifications to POS tags and dependency trees. To overcome local optima, we employ multiple restarts. This simple, yet powerful approach can be easily applied to a range of joint prediction tasks. In prior work, joint models have been designed for a specific language. For instance, joint models for Chinese are designed with word segmentation in mind (Hatori et al., 2012), while algorithms for processing Semitic languages are tailored for morpho42 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 42–52, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics logical analysis (Tratz, 2013; Goldberg and Elhadad, 2011). In contrast, we show that our algorithm can be effortlessly applied to all these distinct languages. Language-specific characteristics drive the lattice construction and the feature selection, while the learning and inference methods are languageagnostic. We ev"
N15-1005,N12-1015,0,0.0242024,"Missing"
N15-1005,D11-1109,0,0.508816,"Missing"
N15-1005,W13-4910,0,0.123985,"ht−1 , t0 i, ht−2 , t0 i, ht−1 , t0 , w−1 i, ht−1 , t0 , w0 i ht−1 , t0 , t1 i, ht−2 , t0 , t1 , i, ht−1 , t0 , t2 i, ht−2 , t0 , t2 i ht−2 , t−1 , t0 , t+1 i, ht−2 , t−1 , t0 , t2 i, ht−2 , t0 , t1 , t2 i ht−2 , t−1 , t0 , t1 , t2 i ht0 , pre1 (w0 )i, ht0 , pre2 (w0 )i, ht0 , suf1 (w0 )i, ht0 , suf2 (w0 )i, ht0 , cn (w0 )i, ht0 , len(w0 )i Experimental Setup Table 2: Statistics of datasets. Morphologically Rich Languages (SPMRL) Shared Task 2013 (Seddah et al., 2013).5 We follow the official split for training, development and testing set. We use the core set of 12 POS categories provided by Marton et al. (2013). In the second Arabic dataset, the training set is a dependency conversion of the Arabic Treebank, which primarily includes Modern Standard Arabic (MSA) text. However, we test on a new corpus, which consists of classical Arabic text obtained from the Comprehensive Islamic Library (CIS).6 A native Arabic speaker with background in computational linguistics annotated the morphological segmentation and POS tags. This corpus is an excellent testbed for a joint model because classical Arabic may use rather different vocabulary from MSA, while their syntactic grammars are very similar to each other"
N15-1005,D12-1046,0,0.0923521,"h has demonstrated that joint prediction alleviates error propagation inherent in pipeline architectures, where mistakes cascade from one task to the next (Bohnet et 1 The source code is available at https://github. com/yuanzh/SegParser. Kareem Darwish ALT Research Group Qatar Computing Research Institute kdarwish@qf.org.qa al., 2013; Tratz, 2013; Hatori et al., 2012; Zhang et al., 2014a). However, jointly modeling all the processing tasks inevitably increases inference complexity. Prior work addressed this challenge by introducing constraints on scoring functions to keep inference tractable (Qian and Liu, 2012). In this paper, we propose a method for joint prediction that imposes no constraints on the scoring function. The method is able to handle high-order and global features for each individual task (e.g., parsing), as well as features that capture interactions between tasks. The algorithm achieves this flexibility by operating over full assignments that specify segmentation, POS tags and dependency tree, moving from one complete configuration to another. Our approach is based on the randomized greedy algorithm from our earlier dependency parsing system (Zhang et al., 2014b). We extend this algor"
N15-1005,W13-4904,0,0.123442,"int prediction is an appealing alternative for pipeline architectures (Goldberg and Tsarfaty, 2008; Hatori et al., 2012; Habash and Rambow, 2005; GahbicheBraham et al., 2012; Zhang and Clark, 2008; Bohnet and Nivre, 2012). These approaches have been particularly prominent for languages with difficult preprocessing, such as morphologically rich languages (e.g., Arabic and Hebrew) and languages that require word segmentation (e.g., Chinese). For the former, joint prediction models typically rely on a lattice structure to represent alternative morphological analyses (Goldberg and Tsarfaty, 2008; Tratz, 2013; Cohen and Smith, 2007). For instance, transitionbased models intertwine operations on the lattice with operations on a dependency tree. Other joint architectures are more decoupled: in Goldberg and Tsarfaty (2008), a lattice is used to derive the best morphological analysis for each part-of-speech alternative, which is in turn provided to the parsing algorithm. In both cases, tractable inference is achieved by limiting the representation power of the scoring function. Our model also uses a lattice to encode alternative analyses. However, we employ this structure in a different way. The model"
N15-1005,P12-2002,0,0.0480518,"nd construct the lattice as described in Section 3.4. Our model then focuses on disambiguating the rest of the word boundaries in the candidates. To generate POS candidates, we apply a CRF-based tagger with Chinese-specific features used in previous work (Hatori et al., 2011). 5.3 Evaluation Measures Following standard practice in previous work (Hatori et al., 2012; Zhang et al., 2014a), we use Fscore as the evaluation metric for segmentation, POS tagging and dependency parsing. We report the morpheme-level F-score for Arabic and the wordlevel F-score for Chinese. In addition, we use TedEval (Tsarfaty et al., 2012) to evaluate the joint prediction on the SPMRL dataset, because TedEval score is the only evaluation metric used in the official report. We directly use the evaluation tools provided on the SPMRL official website.8 5.4 Baselines State-of-the-Art Systems For the SPMRL dataset, we directly compare with Bj¨orkelund et al. (2013). This system achieves the best TedEval score in the track of dependency parsing with predicted information and we directly republish the official result. We also compute the F-score of this system on each task using our own evaluation script.9 For the CTB5 dataset, we dir"
N15-1005,P14-1069,0,0.0737027,"alyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al., 2014b). Our analysis"
N15-1005,P08-1101,0,0.0142698,"also uses a lattice to encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency p"
N15-1005,D10-1082,0,0.360181,"boundaries common across all the top-k candidates as true word boundaries. The remaining tokens (i.e., strings between these boundaries) are treated as words to be further segmented and labeled with POS tags. Figure 3 shows an example of the Chinese word lattice structure we construct. Once the lattice is constructed, the joint prediction model is applied as described above. 4 Features Segmentation Features For both Arabic and Chinese, each segmentation is represented by its score from the preprocessing system, and by the corresponding morphemes (or words in Chinese). Following previous work (Zhang and Clark, 2010), we also add character-based features for Chinese word segmentation, including the first and the last characters in the word, and the length of the word. POS Tag Features Table 1 summarizes the POS tag features employed by the model. First, we use the feature templates proposed in our previous work on Arabic joint parsing and POS correction (Zhang et al., 2014c). In addition, we incorporate character-based features specifically designed for Chinese. These features are mainly inspired by previous transition-based models on Chinese joint POS tagging and word segmentation (Zhang and Clark, 2010)"
N15-1005,P14-1125,0,0.348305,"encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al."
N15-1005,D14-1109,1,0.831276,"encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al."
N15-1005,P14-1019,1,0.85485,"encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al."
N16-3003,W11-4417,0,0.0982575,"Missing"
N16-3003,C96-1017,0,0.288953,"Missing"
N16-3003,N12-1047,0,0.014351,"BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 a"
N16-3003,darwish-etal-2014-using,1,0.880454,"Missing"
N16-3003,W02-0506,1,0.190376,"Missing"
N16-3003,P11-1105,1,0.385324,"olkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013). Results: Table 2 compares the Arabic-to-English SMT systems using the three segmentation tools. Farasa p"
N16-3003,W14-3309,1,0.823799,"ord and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 201"
N16-3003,E14-4029,1,0.444802,"ord and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 201"
N16-3003,N13-1073,0,0.0288181,". We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c"
N16-3003,eisele-chen-2010-multiun,0,0.0434022,"Missing"
N16-3003,D08-1089,0,0.0253633,"ems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013). Results: Table 2 compares the Arabic-to-Englis"
N16-3003,P12-1016,0,0.0135218,"e that each of the segmenters took to process the entire document collection. As can be seen from the results, Farasa outperformed using words, MADAMIRA, and Stanford significantly. Farasa was an order of magnitude faster than Stanford and two orders of magnitude faster than MADAMIRA. 5 Analysis The major advantage of using Farasa is speed, without loss in accuracy. This mainly results from optimization described earlier in the Section 2 which includes caching and limiting the context used for building the features vector. Stanford segmenter uses a third-order (i.e., 4-gram) Markov CRF model (Green and DeNero, 2012) to predict the correct segmentation. On the other hand, MADAMIRA bases its segmentation on the output of a morphological analyzer which provides a list of possible analyses (independent of context) for each word. Both text and analyses are passed to a feature modeling component, which applies SVM and language models to derive predictions for the word segmentation (Pasha et al., 2014). This hierarchy could explain the slowness of MADAMIRA versus other tokenizers. 6 Conclusion In this paper we introduced Farasa, a new Arabic segmenter, which uses SVM for ranking. We compared our segmenter with"
N16-3003,W11-2123,0,0.0161867,"es) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by th"
N16-3003,P07-1019,0,0.0266176,"iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segment"
N16-3003,P07-2045,0,0.0264015,"l Machine Translation (SMT) systems for Arabic↔English, to compare Farasa with Stanford and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Ope"
N16-3003,N04-1022,0,0.0119473,"14-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each ex"
N16-3003,P14-2034,0,0.070743,"Missing"
N16-3003,P02-1040,0,0.119887,"Missing"
N16-3003,pasha-etal-2014-madamira,0,0.166971,"Missing"
N16-3003,2014.iwslt-evaluation.6,1,\N,Missing
N16-3003,2013.iwslt-evaluation.8,1,\N,Missing
N19-1248,D15-1274,0,0.212375,"Missing"
N19-1248,D17-1151,0,0.0558737,"Missing"
N19-1248,W17-1302,1,0.866003,"Missing"
N19-1248,W02-0504,0,0.868168,"Missing"
N19-1248,P05-1071,0,0.215534,"Missing"
N19-1248,N07-2014,0,0.680665,"Missing"
N19-1248,P17-4012,0,0.0215192,"31 2.37 1.99 3.03 2.05 5.97 3.57 3.07 3.93 3.04 7.79 5.49 4.77 6.40 4.77 2.01 1.49 1.30 1.78 1.29 0.00 0.00 0.00 0.00 0.00 12 Combination ∗ 09 +† 11 1.89 2.89 4.49 1.21 0.00 Table 3: Diacritization results: *g represents ngram size e.g. 7g means 7-gram context. Experiment 09 and 11 are comparing NMT models – LSTM-based architecture with attention mechanism and Transformer model Setup and dropout rate = 0.3. The setting for the Transformer were: 6 encoder and 6 decoder layers each of size 512; number of attention heads = 8; feed forward dimension = 2048; and dropout = 0.1. We used the OpenNMT (Klein et al., 2017) implementation with tensorflow for all experiments. System Runs. We conducted a variety of experiments as follows, namely: Word-level experiments where the input is a sequence of words and the output is a sequence of diacritized words: – Baseline Word: uses the full sentences and shows the deficiency of using NMT directly. – Word 7g: uses non-overlapping windows of 7 words to compare to our best character-level model, which also uses a window of length 7. – Word 7g+overlap: uses a sliding window of 7 words. Character-level experiments where the input is represented as a sequence of character"
N19-1248,W18-2507,0,0.137075,"Missing"
N19-1248,W05-0711,0,0.22455,"Missing"
N19-1248,pasha-etal-2014-madamira,0,0.446003,"Missing"
N19-1248,W04-1612,0,0.362427,"Missing"
N19-1248,P06-1073,0,0.232166,"Missing"
N19-1248,E17-2060,0,0.0197172,"not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequences. This leads to a well known limitation of character-based LSTM-based models, namely poor handling of long range dependencies (Sennrich, 2017). An easy fix is to split sentences greater than a certain length into multiple lines. However, boundary words may loose context in the newly created sequences. To handle this, we propose to keep a fixed size context window c for every word. Given a sentence, we use a sliding context window to split it into segments of overlapping windows of size c as in Table 1. This fixes the problems of both long range dependencies and context of neighboring words. We are further aided by the fact that local context can conclusively determine the correct diacritization in the vast majority of cases. Voting."
N19-1248,P16-1162,0,0.0460472,"split into a sequence of subword units each consisting of a letter and its diacritic(s). For example, source word “AlElm” would be represented as “A/l/E/l/m” and its diacritized target “AaloEalamu” as “Aa/lo/Ea/la/mu”. The character-level representation has several benefits, such as reducing the vocabulary size and avoiding OOV words. The splitting of diacritized 2391 words into subword units simplifies the problem as there will be identical number of source and target tokens in a parallel sentence. Later, we support our design decisions with results in the experiments section. Subwords (BPE (Sennrich et al., 2016)) have been used as a defacto standard in building NMT systems. They are a natural choice to handle unknown words. However, BPE does not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequ"
P12-2043,W02-0606,0,0.0878866,"Missing"
P12-2043,W05-0704,1,0.785106,"vational morphology where nouns are transformed into adjectives via the attachment of the suffix ( ﻱيy)2 (ex. ( ﻣﺼﺮmSr) è ( ﻣﺼﺮﻱيmSry)) is desirable as they are semantically related. Matching all stems that are cast from the same root would introduce undesired ambiguity, because a single root can produce up to 1,000 stems. Two general approaches have been shown to improve Arabic retrieval. The first approach involves stemming, which removes clitics, plural and gender markers, and suffixes such as ( ﻱيy). Statistical stemming was reported to be the most effective for Arabic retrieval (Darwish et al., 2005). Though effective, stemming has the following drawbacks: 1. Stemming does not handle infixes and hence cannot conflate singular and broken plural word forms. For example, the plural of the Arabic word for book “( ”ﻛﺘﺎﺏبktAb) is “( ”ﻛﺘﺐktb). 2. Stemming of some named entities, which are important for retrieval, and their inflected forms may produce different stems as word endings may change with the attachment of suffixes. Consider the Arabic words for America ﺃأﻣﺮﻳﯾﻜﺎ (&gt;mrykA) and American &gt;( ﺃأﻣﺮﻳﯾﻜﻲmryky), where the final letter is transformed from “A” to “y”. The second approach in"
P12-2043,N12-1025,1,0.799431,"in}, and the possible mappings for the segments and their probabilities: m = {(m, 0.7), (me, 0.25), (ma, 0.05)} mi = {(mi, 0.5), (me, 0.3), (m, 0.15), (ma, 0.05)} n = {n, 0.7), (nu, 0.2), (an, 0.1)} in = {(in, 0.8), (en, 0.2)} The algorithm would produce the following candidates with the corresponding channel probabilities: (minèmin:0.56): (mèm: 0.7); (inèin: 0.8) (minèmen:0.18): (mèm: 0.7); (inèen: 0.2) 3 The training data can be obtained from: https://github.com/kdarwish/WikiPairs (minèman:0.035): (mièma: 0.05); (nèn: 0.7) The implementation details of the decoder are described in (El-Kahki et al., 2012). 4. Testing Arabic Retrieval Effectiveness 4.1 Experimental Setup We used extrinsic IR evaluation to determine the quality of the related stems that were generated. We performed experiments on the TREC 2001/2002 cross language track collection, which contains 383,872 Arabic newswire articles and 75 topics with their relevance judgments (Oard and Gey, 2002). This is presently the best available large Arabic information retrieval test collection. We used Mean Average Precision (MAP) as the measure of goodness for this retrieval task. Going down from the top a retrieved ranked list, Average Prec"
P12-2043,J01-2001,0,0.0940686,"Although consistency is more important for IR applications than linguistic correctness, perhaps improved correctness would naturally yield great consistency. In this paper, we used a reimplementation of the system proposed by Diab (2009) with the same training set as a baseline. Concerning the automatic induction of morphologically related word-forms, Hammarström (2009) surveyed fairly comprehensively many unsupervised morphology learning approaches. Brent et al. (1995) proposed the use of Minimum Description Length (MDL) to automatically discover suffixes. MDL based approach was improved by: Goldsmith (2001) who applied the EM algorithm to improve the precision of pairing stems prior to suffix induction; and Schone and Jurafsky (2001) who applied latent semantic analysis to determine if two words are semantically related. Jacquemin (1997) used word grams that look similar, i.e. share common stems, to learn suffixes. Baroni (2002) extended his work by incorporating semantic similarity features, via mutual information, and orthographic features, via edit distance. Chen and Gey (2002) utilized a bilingual dictionary to find Arabic words with a common stem that map to the same English stem. Also in t"
P12-2043,W06-3208,0,0.0164348,"l dictionary to find Arabic words with a common stem that map to the same English stem. Also in the cross-language spirit, Snyder and Barzilay (2008) used cross-language mappings to learn morpheme patterns and consequently automatically segment words. They successfully applied their method to Arabic, Hebrew, and Aramaic. Creutz and Lagus (2007) proposed a probabilistic model for automatic word segment discovery. Most of these approaches can discover suffixes and prefixes without human intervention. However, they may not be able to handle infixation and spelling variations. Karagol-Ayan et al. (2006) used approximate string matching to automatically map morphologically similar words in noisy dictionary data. They used the mappings to learn affixation, including infixiation, from noisy data. In this paper, we propose a new technique for finding morphologically related word-forms based on learning character-level mappings. Title: Title: ﺍاﻟﺑﺭرﺗﻐﺎﻝل ﻟﻐﺔ ﺑﺭرﺗﻐﺎﻟﻳﯾﺔ Figure 1. Example hypertexts to Wikipedia titles 3. Character-Level Model 3.1 Training Data In our experiments, we extracted Wikipedia hypertext to page title pairs as in Figure 1. We performed all work on an Arabic Wikipedia d"
P12-2043,P07-2045,0,0.00565189,"Missing"
P12-2043,P03-1051,0,0.0621069,"Missing"
P12-2043,N01-1024,0,0.0123788,"uld naturally yield great consistency. In this paper, we used a reimplementation of the system proposed by Diab (2009) with the same training set as a baseline. Concerning the automatic induction of morphologically related word-forms, Hammarström (2009) surveyed fairly comprehensively many unsupervised morphology learning approaches. Brent et al. (1995) proposed the use of Minimum Description Length (MDL) to automatically discover suffixes. MDL based approach was improved by: Goldsmith (2001) who applied the EM algorithm to improve the precision of pairing stems prior to suffix induction; and Schone and Jurafsky (2001) who applied latent semantic analysis to determine if two words are semantically related. Jacquemin (1997) used word grams that look similar, i.e. share common stems, to learn suffixes. Baroni (2002) extended his work by incorporating semantic similarity features, via mutual information, and orthographic features, via edit distance. Chen and Gey (2002) utilized a bilingual dictionary to find Arabic words with a common stem that map to the same English stem. Also in the cross-language spirit, Snyder and Barzilay (2008) used cross-language mappings to learn morpheme patterns and consequently aut"
P12-2043,P08-1084,0,0.0271332,"thm to improve the precision of pairing stems prior to suffix induction; and Schone and Jurafsky (2001) who applied latent semantic analysis to determine if two words are semantically related. Jacquemin (1997) used word grams that look similar, i.e. share common stems, to learn suffixes. Baroni (2002) extended his work by incorporating semantic similarity features, via mutual information, and orthographic features, via edit distance. Chen and Gey (2002) utilized a bilingual dictionary to find Arabic words with a common stem that map to the same English stem. Also in the cross-language spirit, Snyder and Barzilay (2008) used cross-language mappings to learn morpheme patterns and consequently automatically segment words. They successfully applied their method to Arabic, Hebrew, and Aramaic. Creutz and Lagus (2007) proposed a probabilistic model for automatic word segment discovery. Most of these approaches can discover suffixes and prefixes without human intervention. However, they may not be able to handle infixation and spelling variations. Karagol-Ayan et al. (2006) used approximate string matching to automatically map morphologically similar words in noisy dictionary data. They used the mappings to learn"
P12-2043,W02-0506,1,\N,Missing
P13-1153,W10-2417,1,0.882588,"Missing"
P13-1153,attia-etal-2010-automatically,0,0.0900323,"Missing"
P13-1153,D08-1030,0,0.152007,"agging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA, an Arabic analyzer, to indicate if a word is capitalized in English or not. They reported that it was the second most discriminating feature that they used. However, there seems to be room for improvement because: (1) MADA’s dictionary is relatively small and would have low coverage; and (2) the use of such a binary feature is problematic, because Arabic names are often common Arabic words and hence a word may be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using lar"
P13-1153,W10-2906,0,0.0134089,"eports on their effectiveness; and Section 5 concludes the paper. 2 2.1 Related Work Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA, an Arabic analyzer, to indicate if a word is capitalized in English or not. They reported that it was the"
P13-1153,D11-1128,1,0.88556,"Missing"
P13-1153,farber-etal-2008-improving,0,0.0882554,"Missing"
P13-1153,P09-1042,0,0.00653259,"tion 2 provides related work; Section 3 describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 2.1 Related Work Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dic"
P13-1153,P12-1016,0,0.0211864,"ative names to articles. Of these articles, 254,145 have cross-lingual links to English Wikipedia. We used DBpedia 3.8 which includes 6,157,591 entries of Wikipedia titles and their “types”, such as “person”, “plant”, or “device”, where a title can have multiple types. The phrase table was trained on a set of 3.69 million parallel sentences containing 123.4 million English tokens. The sentences were drawn from the UN parallel data along with a variety of parallel news data from LDC and the GALE project. The Arabic side was stemmed (by removing just prefixes) using the Stanford word segmenter (Green and DeNero, 2012). 4.1 Cross-lingual Capitalization As we mentioned earlier, Arabic lacks capitalization and Arabic names are often common Arabic words. For example, the Arabic name “Hasan” means good. To capture cross-lingual capitalization, we used the aforementioned true-cased phrase table at word and phrase levels as follows: translation was capitalized or not respectively; and the weights were binned because CRF++ only takes nominal features. In essence we tried every subsequence of S of length l or less to see if the translation was capitalized. A subsequence can be 1 word long. We tried longer sequences"
P13-1153,P08-1045,0,0.0227649,"y be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using large bilingual resources, and we incorporate confidences to avoid having a binary feature. Richman and Schone (2008) used English linguis1559 tic tools and cross language links in Wikipedia to automatically annotate text in different languages. Transliteration Mining (TM) has been used to enrich MT phrase tables or to improve cross language search (Udupa et al., 2009). Conversely, people have used NER to determine if a word is to be transliterated or not (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; B"
P13-1153,W03-0429,0,0.0135579,"Missing"
P13-1153,W03-0430,0,0.15115,"Missing"
P13-1153,W02-2020,0,0.110069,"Missing"
P13-1153,P08-1001,0,0.106528,"s capitalized in English or not. They reported that it was the second most discriminating feature that they used. However, there seems to be room for improvement because: (1) MADA’s dictionary is relatively small and would have low coverage; and (2) the use of such a binary feature is problematic, because Arabic names are often common Arabic words and hence a word may be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using large bilingual resources, and we incorporate confidences to avoid having a binary feature. Richman and Schone (2008) used English linguis1559 tic tools and cross language links in Wikipedia to automatically annotate text in different languages. Transliteration Mining (TM) has been used to enrich MT phrase tables or to improve cross language search (Udupa et al., 2009). Conversely, people have used NER to determine if a word is to be transliterated or not (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In o"
P13-1153,W07-0803,0,0.284714,"Missing"
P13-1153,D10-1103,0,0.0221089,"d work; Section 3 describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 2.1 Related Work Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA,"
P13-1153,N01-1026,0,0.0361617,"describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 2.1 Related Work Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA, an Arabic analyzer, to in"
P13-1153,E12-1017,0,\N,Missing
P13-2001,E06-1047,0,0.149589,"012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k sentences for trainin"
P13-2001,P10-1048,1,0.686515,"LM experiments also affirmed the importance of in-domain English LMs. We also showed that a conversion does not imply a straight forward usage of MSA resources and there is a need for adaptation which we fulfilled using phrase-table merging (Nakov and Ng, 2009). 2 2.1 Previous Work Baseline Our work is related to research on MT from a resource poor language (to other languages) by pivoting on a closely related resource rich language. This can be done by either translating between the related languages using word-level translation, character level transformations, and language specific rules (Durrani et al., 2010; Hajiˇc et al., 2000; Nakov and Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to impr"
P13-2001,N13-1044,0,0.12911,"actic challenge in this sentence, since the Egyptian word order in interrogative sentences is normally different from the MSA word order: the interrogative particle appears at the end of the sentence instead of at the beginning. Addressing this problem might have improved translation. The above analysis suggests that incorporating deeper linguistic information in the conversion procedure could improve translation quality. In particular, using a morphological analyzer seeems like a promising possibility. One approach could be to run a morphological analyzer for dialectal Arabic (e.g. MADA-ARZ (Habash et al., 2013)) on the original EG sentence and another analyzer for MSA (such as MADA) on the converted EG0 sentence, and then to compare the morphological features. Discrepancies should be probabilistically incorporated in the conversion. Exploring this approach is left for future work. 4 Conclusion We presented an Egyptian to English MT system. In contrast to previous work, we used an automatic conversion method to map Egyptian close to MSA. The converted Egyptian EG0 had fewer OOV words and spelling mistakes and improved language handling. The MT system built on the adapted parallel data showed an impro"
P13-2001,A00-1002,0,0.0541606,"Missing"
P13-2001,W11-2123,0,0.0145373,"(Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG and AR training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011). In case of more than one LM, we tuned their weights on a development set using Minimum Error Rate Training (Och and Ney, 2003). We built several baseline systems as follows: – B1 used AR for training a translation model and GW for LM. – B2-B4 systems used identical training data, namely EG, with the GW, EGen , or both for B2, B3, and B4 respectively for language modeling. Table 1 reports the baseline results. The system trained on AR (B1) performed poorly compared to the one trained on EG (B2) with a 6.75 BLEU points difference. This highlights the difference between MSA and Egyptian. Using"
P13-2001,N03-1017,0,0.00522573,"g of 200k sentences from LDC4 . We refer to this corpus as the AR corpus. For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5. We refer to this corpus as GW. We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008). Word elongations were already fixed in the corpus. We wordaligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG and AR training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011). In case of more than one LM, we tuned their weights on a development set using Minimum Erro"
P13-2001,P12-2035,0,0.0480451,"ting the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k sentences for training, 2k for development"
P13-2001,N12-1006,0,0.379251,"r, we applied an adaptation method to incorporate MSA/English parallel data. The contributions of this paper are as follows: – We trained an Egyptian/MSA transformation model to make Egyptian look similar to MSA. We publicly released the training data. – We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points. – We used phrase-table merging (Nakov and Ng, 2009) to utilize MSA/English parallel data with the available in-domain parallel data. learnt character mappings from dialect/MSA word pairs. Zbib et al. (2012) explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They used two language models built from the English GigaWord corpus and from a large web crawl. Their best system outperformed manually translating Egyptian to MSA then translating using an MSA/English system. In contrast, we showed that training on in-domain dialectal data irrespective of its small size is better than training on large MSA/English data. Our LM experiments also affirmed the importance of in-domain English LMs. We also showed that a conversion does"
P13-2001,D09-1141,0,0.390382,"using character-level transformations and word n-gram models that handle spelling mistakes, phonological variations, and morphological transformations. Later, we applied an adaptation method to incorporate MSA/English parallel data. The contributions of this paper are as follows: – We trained an Egyptian/MSA transformation model to make Egyptian look similar to MSA. We publicly released the training data. – We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points. – We used phrase-table merging (Nakov and Ng, 2009) to utilize MSA/English parallel data with the available in-domain parallel data. learnt character mappings from dialect/MSA word pairs. Zbib et al. (2012) explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They used two language models built from the English GigaWord corpus and from a large web crawl. Their best system outperformed manually translating Egyptian to MSA then translating using an MSA/English system. In contrast, we showed that training on in-domain dialectal data irrespective of its small size is bett"
P13-2001,P12-2059,0,0.0156256,"e of in-domain English LMs. We also showed that a conversion does not imply a straight forward usage of MSA resources and there is a need for adaptation which we fulfilled using phrase-table merging (Nakov and Ng, 2009). 2 2.1 Previous Work Baseline Our work is related to research on MT from a resource poor language (to other languages) by pivoting on a closely related resource rich language. This can be done by either translating between the related languages using word-level translation, character level transformations, and language specific rules (Durrani et al., 2010; Hajiˇc et al., 2000; Nakov and Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chia"
P13-2001,J03-1002,0,0.00581558,"(2012) to directly compare to their results. - An MSA/English parallel corpus consisting of 200k sentences from LDC4 . We refer to this corpus as the AR corpus. For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5. We refer to this corpus as GW. We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008). Word elongations were already fixed in the corpus. We wordaligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG and AR training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011). In"
P13-2001,P08-2030,0,0.0379788,"is corpus as EG and the English part of it as EGen . We did not have access to the training/test splits of Zbib et al. (2012) to directly compare to their results. - An MSA/English parallel corpus consisting of 200k sentences from LDC4 . We refer to this corpus as the AR corpus. For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5. We refer to this corpus as GW. We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008). Word elongations were already fixed in the corpus. We wordaligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG"
P13-2001,W11-2602,0,0.266595,"ects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k sentences for training, 2k for development and 4k for testing. We henceforth refer to this corpus as EG and the English part of it as EGen . We did not have access to the training/test splits of Zbib et al. (201"
P13-2001,2010.amta-papers.5,0,0.351178,"Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k"
P13-2001,P11-2007,0,0.209222,"Missing"
P13-2001,D11-1128,1,\N,Missing
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
W02-0506,C96-1017,0,0.41565,"Missing"
W02-0506,J00-4006,0,\N,Missing
W02-0506,J01-2001,0,\N,Missing
W05-0704,P03-1051,1,0.948273,"additional analysis, namely to the root “syr” and the stem “syr.” Perhaps such ambiguity can be reduced by using the context in which the word is mentioned. For example, for the word “ksyr” in the sentence “sAr ksyr” (and he walked like), the letter “k” is likely to be a prefix. The problem of coverage is practically eliminated by light stemming. However, light stemming yields greater consistency without regard to correctness. Although consistency is more important for IR applications than linguistic correctness, perhaps improved correctness would naturally yield great consistency. Lee et al. [15] adopted a trigram language model (LM) trained on a portion of the manually segmented LDC Arabic Treebank in developing an Arabic morphology system, which attempts to improve the coverage and linguistic correctness over existing statistical analyzers such as Sebawai [15]. The analyzer of Lee et al. will be henceforth referred to as the IBM-LM analyzer. IBM-LM's analyzer combined the trigram LM (to analyze a word within its context in the sentence) with a prefix-suffix filter (to eliminate illegal prefix suffix combinations, hence improving correctness) and unsupervised stem acquisition (to imp"
W05-0704,W02-0506,1,\N,Missing
W06-1648,P00-1037,0,0.404134,"Missing"
W06-1648,W05-0704,1,0.829543,"d on the same collection after the language modeling based stemming was used on all the tokens in the collection (Lee et al., 2003). The top n generated corrections were subsequently stemmed and the stems were reranked using the language model. The top resulting stem was compared to the condition in which language modeling was used without morphological analysis (as in the previous subsection) and then the top resulting correction were stemmed. This path was pursued to examine the effect of correction on applications where stems are more useful than words such as Arabic information retrieval (Darwish et al., 2005; Larkey et al., 2002). 3.4 Testing the Models The 1:1 and m:n character mapping models were tested while enabling or disabling character position training (CP), smoothing by the assignment of small probabilities to unseen single character substitutions (UP), language modeling (LM), and shallow morphological processing (SM) using the 6-gram model. As mentioned earlier, all models were tested using sentences containing 2,000 words in total. 4 Experimental Results Table 1 reports on the percentage of words for which a proper correction was found in the top n generated corrections using different"
W06-1648,P00-1026,0,0.0150104,"Missing"
W06-1648,P03-1051,0,0.0106297,"a sequence Ω = {ω1 .. ωi .. ωn}, where ωi ∈ Χi, that maximizes:  Π P(χ |χ , χ ) ⋅ P(δ |χ ) ij i −1, j i −2 , j i ij i =1..n , j =1..m 142 4 3 1 44444244444 3 CharacterM odel LanguageModel 3.3 Language Modeling and Shallow Morphological Analysis Two paths were pursued to explore the combined effect of language modeling and shallow morphological analysis. In the first, a 6-gram language model was trained on the same web-mined collection after each of the words in the collection was segmented into its constituent prefix, stem, and suffix (in this order) using language model based stemmer (Lee et al., 2003). For example, “TUPQR – وآwktAbhm” was replaced by “w# ktAb +hm” where # and + were used to mark prefixes and suffixes respectively and to distinguish them from stems. Like before, alef and ya letter normalizations were performed and the language model was built using SRILM toolkit with the same parameters. Formally, the only difference between this model and the one before is that Χi ={χi0 .. χim} are the {prefix, stem, suffix} tuples of the possible corrections of δi (a tuple is treated as a block). Otherwise everything else is identical. In the second, a trigram language model was trained"
W06-1648,J96-1003,0,0.735435,"uments has been reported for many languages, including English (Harding et al., 1997), Chinese (Tseng and Oard, 2001), and Arabic (Darwish and Oard, 2002). 2.2 OCR Error Correction Much research has been done to correct recognition errors in OCR-degraded collections. There are two main categories of determining how to correct these errors. They are word-level and passage-level post-OCR processing. Some of the kinds of word level post-processing include the use of dictionary lookup, probabilistic relaxation, character and word n-gram frequency analysis (Hong, 1995), and morphological analysis (Oflazer, 1996). Passage-level postprocessing techniques include the use of word ngrams, word collocations, grammar, conceptual closeness, passage level word clustering, linguistic context, and visual context. The following introduces some of the error correction techniques. • Dictionary Lookup: Dictionary Lookup, which is the basis for the correction reported in this paper, is used to compare recognized words with words in a term list (Church and Gale, 1991; Hong, 1995; Jurafsky and Martin, 2000). If a word is found in the dictionary, then it is considered correct. Otherwise, a checker attempts to find a di"
W06-1648,P98-1003,0,\N,Missing
W06-1648,C98-1003,0,\N,Missing
W07-0804,W04-0701,0,0.536715,"zation methodology; Section 5 describes the experimental setup; Section 6 reports and discusses experimental results; and Section 7 concludes the paper and provides possible future directions. 2. Background While considerable work has focused on named entity normalization within a single document, little work has focused on the challenges associated with resolving person name references across multiple documents. Most of the work done in cross-document normalization focused on the problem of determining if two instances with the same name from different documents referring to the same person (Fleischman and Hovy, 2004). Fleischman and Hovy (2004) focused on distinguishing between individuals having identical names, but they did not extend normalization to different names referring to the same individual. Their task is a subtask of what is examined in this paper. They used a large number of features to accomplish their work, depending mostly on language specific dictionaries and wordnet. Some these resources are not available for Arabic and many other languages. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. They focused on information extracti"
W07-0804,W04-0705,0,0.0667395,"Missing"
W07-0804,P03-1051,1,0.825356,"ual examination of the training set). 2. Name mentions formed of a single token consisting of less than 3 characters are removed. Such names are almost always misrecognized name entities. 3. Name entities with 10 or more different name mentions are automatically removed. The NERT system often produces entities that include many different name mentions referring to different persons as one. Such entities are errant because they over normalize name mentions. Persons are referred to using a limited number of name mentions. 4. Nominal mentions are stemmed using a context sensitive Arabic stemmer (Lee et al. 2003) to overcome the morphological complexity of Arabic. For example, “JKL= ”ر “president”, “JKLQO“ = ”اthe president”, “JKLQO“ = ”واand the president”, “TUSKL“ = ”رits presidents” … etc are stemmed to “JKL= ”ر “president”. Cross-document entities are compared in a pairwise manner and binary decision is taken on whether they are the same. Therefore, the available 7,184 entities lead to nearly 26 million pairwise comparisons (For N entities, the number of pair wise comparisons = N ( N − 1) ). 2 Entity pairs were chosen to be included in the training set if they match any of the following"
W07-0804,W03-0405,0,0.0618253,"determining if two instances with the same name from different documents referring to the same person (Fleischman and Hovy, 2004). Fleischman and Hovy (2004) focused on distinguishing between individuals having identical names, but they did not extend normalization to different names referring to the same individual. Their task is a subtask of what is examined in this paper. They used a large number of features to accomplish their work, depending mostly on language specific dictionaries and wordnet. Some these resources are not available for Arabic and many other languages. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. They focused on information extraction to build biographical profiles (date of birth, place of birth, etc.), and they wanted to disambiguate biographies belonging to different authors with identical names. Dozier and Zielund (Dozier and Zielund, 2004) reported on cross-document person name normalization in the legal domain. They used a finite state machine that identifies paragraphs in a document containing the names of attorneys, judges, or experts and a semantic parser that extracts from the paragraphs template information"
W07-0804,A97-1028,0,0.0686857,"Missing"
W07-0804,N04-1001,0,0.167317,"Missing"
W07-0804,C02-1127,0,\N,Missing
W07-0804,P05-1051,0,\N,Missing
W07-1012,W06-1659,1,0.852884,"Missing"
W07-1012,W04-3102,0,0.0358198,"Missing"
W07-1012,P04-1025,0,0.0305574,"Missing"
W07-1012,W04-1221,0,0.0349834,"Missing"
W07-1012,W03-1309,0,0.0283401,"Missing"
W07-1012,C00-1030,0,\N,Missing
W07-1012,W02-0301,0,\N,Missing
W10-2407,W03-0317,0,0.0576311,"al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and English-Russian (Klementiev and Roth, 2006). The most common approach for determining letter sequence mapping between two languages is using automatic letter alignment of a training set of transliteration pairs. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or using an HMM aligner (Udupa et al., 2009a; Udupa et al., 2009b). Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set. One example of that is to use a predefined transliteration scheme to transliterate a word in one character set into another character set (Oh and Choi, 2006). Different methods were proposed to ascertain if two words can be transliterations of each o"
W10-2407,W09-3407,0,0.242893,"nts in recall dwarfing drops in precision. Using iterative training improved recall, but often at the cost of significant drops in precision. The best runs typically used both letter conflation and iterative learning. 1 2 Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and English-Russian (Klementiev and Roth, 2006). The most common approach for determining letter sequence mapping between two languages is using automatic letter alignment of a training set of transliteration pairs. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or using an HMM aligner (Udupa et al., 2009a; Udupa et al., 2009b). Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation var"
W10-2407,E09-1091,0,0.195716,"aravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and English-Russian (Klementiev and Roth, 2006). The most common approach for determining letter sequence mapping between two languages is using automatic letter alignment of a training set of transliteration pairs. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or using an HMM aligner (Udupa et al., 2009a; Udupa et al., 2009b). Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set. One example of that is to use a predefined transliteration scheme to transliterate a word in one character set into another character set (Oh and Choi, 2006). Different methods were proposed to ascertain if two words can be transliterations of each other. One such way is to use a generative mo"
W10-2407,W07-0711,0,0.168963,"ion pairs. The training involved automatically aligning character sequences. SOUNDEX like letter conflation and iterative transliterator training was used to improve recall. Akin to phrasal alignment in machine translation, character sequence alignment was treated as a word alignment problem between parallel sentences, where transliterations were treated as if they were sentences and the characters from which they were composed were treated as if they were words. The alignment was performed using a Bayesian learner that trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced a mapping of source character sequence to a target character sequence along with the probability of source given target. For all the work reported herein, given an Englishforeign language transliteration candidate pair, English was treated as the target language and the foreign language as the source. Given a foreign source language word sequence and an English target word sequence , is a potential transliteration of . Given Fi, composed of the character sequence f1 … fo, and Ej, composed of the character sequence e1 … ep, P(Fi|Ej) is calculated using the trained model, as"
W10-2407,I08-4002,0,0.376388,"Missing"
W10-2407,N06-1011,0,\N,Missing
W10-2407,P06-1142,0,\N,Missing
W10-2407,I08-4003,0,\N,Missing
W10-2407,I08-6004,0,\N,Missing
W10-2414,W03-2201,0,0.012545,"d Cairo Microsoft Innovation Faculty of Computers and Information, Cairo University Center Information, Cairo University Cairo, Egypt Cairo, Egypt Cairo, Egypt iman.saleh@fcikareemd@microsoft.com a.fahmy@fcicu.edu.eg cu.edu.eg hierarchical tree of categories and their mappings to articles. Many of the Wikipedia pages provide information about concepts and named entities (NE). Identifying pages that provide information about different NE’s can be of great help in a variety of NLP applications such as named entity recognition, question answering, information extraction, and machine translation (Babych and Hartley, 2003; Dakka and Cucerzan, 2008). This paper attempts to identify multilingual Wikipedia pages that provide information about different types of NE, namely persons, locations, and organizations. The identification is done using a Support Vector Machines (SVM) classifier that is trained on a variety of Wikipedia features such as infobox attributes, tokens in text, and category links for different languages aided by crosslanguage links in pages. Using features from different languages helps in two ways, namely: clues such infobox attributes may exist in one language, but not in the other, and this al"
W10-2414,I08-1071,0,0.315035,"on Faculty of Computers and Information, Cairo University Center Information, Cairo University Cairo, Egypt Cairo, Egypt Cairo, Egypt iman.saleh@fcikareemd@microsoft.com a.fahmy@fcicu.edu.eg cu.edu.eg hierarchical tree of categories and their mappings to articles. Many of the Wikipedia pages provide information about concepts and named entities (NE). Identifying pages that provide information about different NE’s can be of great help in a variety of NLP applications such as named entity recognition, question answering, information extraction, and machine translation (Babych and Hartley, 2003; Dakka and Cucerzan, 2008). This paper attempts to identify multilingual Wikipedia pages that provide information about different types of NE, namely persons, locations, and organizations. The identification is done using a Support Vector Machines (SVM) classifier that is trained on a variety of Wikipedia features such as infobox attributes, tokens in text, and category links for different languages aided by crosslanguage links in pages. Using features from different languages helps in two ways, namely: clues such infobox attributes may exist in one language, but not in the other, and this allows for tagging pages in m"
W10-2414,U08-1016,0,0.37221,"Missing"
W10-2414,P08-1001,0,0.375956,"Missing"
W10-2414,W06-2809,0,0.201528,"Missing"
W10-2414,D07-1068,0,0.190564,"Missing"
W10-2414,wentland-etal-2008-building,0,\N,Missing
W10-2417,D08-1030,0,0.163697,"letters in words and word n-grams. The proposed features help overcome some of the morphological and orthographic complexities of Arabic. In comparing to results in the literature using Arabic specific features such POS tags on the same dataset and same CRF implementation, the results in this paper are lower by 2 F-measure points for locations, but are better by 8 points for organizations and 9 points for persons. 1 Introduction Named entity recognition (NER) continues to be an important part of many NLP applications such as information extraction, machine translation, and question answering (Benajiba et al., 2008). NER is concerned with identifying sequences of words referring to named entities (NE’s) such as persons, locations, and organizations. For example, in the word sequence “Alan Mulally, CEO of Detroit based Ford Motor Company,” Alan Mulally, Detroit, and Ford Motor Company would be identified as a person, a location, and an organization respectively. Arabic is a Semitic language that present interesting morphological and orthographic challenges that may complicate NER. Some of these challenges include:  Coordinating conjunctions, prepositions, possessive pronouns, and determiners are typicall"
W10-2417,farber-etal-2008-improving,0,0.455761,"Missing"
W10-2417,W03-0429,0,0.0859868,"o not follow the same tagging conventions, training and testing were conducted separately for each collection. Each collection was 80/20 split for training and testing. 4.2 Data Processing and Sequence Labeling Training and testing were done using CRF++ which is a CRF sequence label toolkit. The following processing steps of Arabic were performed:  The coordinating conjunctions w ( )وand f ()ف, which always appear as the first prefixes in a word, were optionally stemmed. w and f were stemmed using an in-house Arabic stemmer that is a reimplementation of the stemmer proposed by Lee et al. (2003). However, stemming w or f could have been done by stemming the w or f and searching  4.3 for the stemmed word in a large Arabic corpus. If the stemmed word appears more than a certain count, then stemming was appropriate. The different forms of alef (A ()ا, |()آ, &gt; ()أ, and &lt; ( ))إwere normalized to A ()ا, y ( )يand Y ( )ىwere normalized to y ()ي, and p ( )ةwas mapped to h ()هـ. Evaluation The figures of merit for evaluation were precision, recall, and F-measure ( = 1), with evaluation being conducted at the phrase level. Reporting experiments with all the different comb"
W10-2417,W03-0430,0,0.380772,"Missing"
W10-2417,W02-2020,0,0.172453,"Missing"
W10-2417,N03-1028,0,0.05745,"Missing"
W10-2417,W07-0803,0,0.50189,"Missing"
W10-2417,P03-1051,0,\N,Missing
W13-1608,P11-2103,0,0.578282,"automatic annotation of non-English text that was machine translated into English to automatically or manually translating annotated English text to train a classifier in the target language. In all these cases, they concluded that translation can help avail the need for building language specific resources. In performing both subjectivity and sentiment classification, researchers have used word, phrase, sentence, and topic level fea56 tures. Wilson et al. (2005) report on such features in detail, and we use some of their features in our baseline runs. For Arabic subjectivity classification, Abdul-Mageed et al. (2011) performed sentencelevel binary classification. They used a manually curated subjectivity lexicon and corpus that was drawn from news articles (from Penn Arabic tree bank). They used features that are akin to those developed by Wilson et al. (2005). In later work, AbdulMageed et al. (2012) extended their work to social content including chat sessions, tweets, Wikipedia discussion pages, and online forums. Unfortunately, their tweets corpus is not publicly available. They added social media features such as author information (person vs. organization and gender). They also explored Arabic speci"
W13-1608,W11-0413,0,0.342293,"Missing"
W13-1608,W12-3705,0,0.376001,"Missing"
W13-1608,D08-1014,0,0.00694936,"Mihalcea et al. (2007) translated an existing English subjectivity lexicon from Wiebe and Riloff (2005) using a bilingual dictionary. They also used a subjectivity classifier to automatically annotate the English side of an English-Romanian parallel corpus and then project the annotations to the Romanian side. The projected annotations were used to train a subjectivity classifier. In follow on work, Banea et al. (2010) used MT to exploit annotated SSA English corpora for other languages, including Arabic. They also integrated features from multiple languages to train a combined classifier. In Banea et al. (2008), they compared the automatic annotation of non-English text that was machine translated into English to automatically or manually translating annotated English text to train a classifier in the target language. In all these cases, they concluded that translation can help avail the need for building language specific resources. In performing both subjectivity and sentiment classification, researchers have used word, phrase, sentence, and topic level fea56 tures. Wilson et al. (2005) report on such features in detail, and we use some of their features in our baseline runs. For Arabic subjectivi"
W13-1608,C10-1004,0,0.127529,"aches to Subjectivity, Sentiment and Social Media Analysis, pages 55–64, c Atlanta, Georgia, 14 June 2013. 2013 Association for Computational Linguistics pand SSA lexicons, leading to improvements for SSA for Arabic tweets. The remainder of this paper is organized as follows: Section 2 surveys related work; section 3 introduces some of the challenges associated with Arabic SSA; section 4 describes the lexicons we used; section 5 presents the experimental setup and results; and section 6 concludes the paper and discusses future work. 2 Related Work There has been a fair amount work on SSA. Liu (2010) offers a thorough survey of SSA research. He defines the problem of sentiment analysis including associated SSA terms such as object, opinion, opinion holder, emotions, sentence subjectivity, etc. He also discusses the more popular two stage sentiment and subjectivity classification approach at different granularities (document and sentence levels) using different machine learning approaches (supervised and unsupervised) along with different ways to construct the required data resources (corpora and lexicon). In our work, we classify subjectivity and sentiment in a cascaded fashion following"
W13-1608,C10-2005,0,0.0773567,"sing the scheme described by Darwish et al. (2012). Their work extended the basic Arabic normalization to handle non-Arabic characters that were borrowed from Farsi and Urdu for decoration decorate and words elongation and shortening. After normalization, words were stemmed. - MSA or dialect, which is a binary feature that indicates whether the stem appears in a large MSA stem list (containing 82,380 stems) which was extracted from a large Arabic news corpus from Aljazeera.net. - Stem prior polarity and Stem POS as those for MSA subjectivity classification. Tweets-specific features: Following Barbosa and Feng (2010) and Kothari et al. (2013), we took ad61 vantage of tweet specific features, namely: - Presence of hashtag (#tag). - Presence of user mention (@some user) and position in the tweet (start, end and middle). - Presence of URL and position in the tweet (start, end and middle). - Presence of retweet symbol “RT” and position in the tweet (start, end and middle). “RT” and URL’s usually appear in the beginning and end of tweets respectively, particularly when retweeting news articles. A change in their position may indicate that the person retweeting added text to the tweet, often containing opinions"
W13-1608,P06-4018,0,0.00424106,"ey discussed it to resolve the disagreement. If they couldn’t resolve the disagreement, then the tweet was discarded, which would somewhat affect the SSA effectiveness numbers. They applied one of five possible labels to the tweets, namely: neutral, positive, negative, both, or sarcastic. For subjectivity analysis, all classes other than neutral were considered subjective. As for sentiment analysis, we only considered positive and negative tweets. For both subjectivity and sentiment classification experiments, we used 10-fold cross validation with 90/10 training/test splits. We used the NLTK (Bird, 2006) implementation of the Na¨ıve Bayesian classifier for all our experiments. In offline experiments, the Bayesian classifier performed slightly better than an SVM classifier. The classifier assigned a sentence or 59 argmaxP (c) c∈C n Y P (fi |c) (2) i=1 where f is the feature vector and C is the set of pre-defined classes. As for stemming and POS Tagging, we used an in-house reimplementation of AMIRA (Diab, 2009). We report accuracy as well as precision, recall and F-measure for each class. 5.2 Baseline: SSA for MSA 5.2.1 Subjectivity Classification As mentioned in section 2, we employed some of"
W13-1608,P12-1016,0,0.0133736,"that in turn map to other Arabic words, which are potentially synonymous to the original word. We applied a single graph reinforcement iteration over two phrase tables that were generated using Moses (Koehn et al., 2007). The two phrase tables were: - an English-MSA phrase table, which was trained on a set of 3.69 million parallel sentences containing 123.4 million English tokens. The sentences were drawn from the UN parallel data along with a variety of parallel news data from LDC and the GALE project. The Arabic side was stemmed (by removing just prefixes) using the Stanford word segmenter (Green and DeNero, 2012). - an English-Dialect phrase table, which was trained on 176K short parallel sentences containing 1.8M Egyptian, Levantine, and Gulf dialectal words and 2.1M English words (Zbib et al., 2012). The Arabic side was also stemmed using the Stanford word segmenter. More formally, Arabic seed words and their English translations were represented using a bipartite graph G = (S, T, M), where S was the set of Arabic words, T was the set of English words, and M was the set of mappings (links or edges) between S and T. First, we found all possible English translations T 0 ⊆ T for each Arabic word si ⊆ S"
W13-1608,P07-2045,0,0.00538715,"tionally used graph reinforcement to expand the ArabSenti lexicon using MT phrase tables, which were modeled as a bipartite graph (El-Kahky et al., 2011). As shown in Figure 1, given a seed lexicon, graph reinforcement is then used to enrich the lexicon by inferring additional mappings. Specifically, given the word with the dotted outline, it may map to the words “unfair” and “unjust” in English that in turn map to other Arabic words, which are potentially synonymous to the original word. We applied a single graph reinforcement iteration over two phrase tables that were generated using Moses (Koehn et al., 2007). The two phrase tables were: - an English-MSA phrase table, which was trained on a set of 3.69 million parallel sentences containing 123.4 million English tokens. The sentences were drawn from the UN parallel data along with a variety of parallel news data from LDC and the GALE project. The Arabic side was stemmed (by removing just prefixes) using the Stanford word segmenter (Green and DeNero, 2012). - an English-Dialect phrase table, which was trained on 176K short parallel sentences containing 1.8M Egyptian, Levantine, and Gulf dialectal words and 2.1M English words (Zbib et al., 2012). The"
W13-1608,N10-1017,0,0.0439507,"ation. Abdul-Mageed et al. (2011) performed sentence-level sentiment classification for MSA. They concluded that the appearance of a positive or negative adjective, based on their lexicon, is the most important feature. In later work, Abdul-Mageed et al. (2012) extended their work to social text. They concluded that: (a) POS tags are not as effective in sentiment classification as in the subjectivity classification, and (b) most dialectal Arabic tweets are negative. Lastly, they projected that extending/adapting polarity lexicon to new domains; e.g. social media; would result in higher gains. Kok and Brockett (2010) introduced a random-walk-base approach to generate paraphrases from parallel corpora. They proved to be more effective in generating more paraphrases by traversing paths of lengths longer than 2. El-Kahky et al. (2011) applied graph reinforcement on transliteration mining problem to infer mappings that were unseen in training. We used this graph reinforcement method in our work. 3 Challenges for SSA of Arabic Arabic SSA faces many challenges due to the poorness of language resources and to Arabic-specific linguistic features. Lexicon: Lexicons containing words with prior polarity are crucial"
W13-1608,P07-1123,0,0.0124366,"ence subjectivity, etc. He also discusses the more popular two stage sentiment and subjectivity classification approach at different granularities (document and sentence levels) using different machine learning approaches (supervised and unsupervised) along with different ways to construct the required data resources (corpora and lexicon). In our work, we classify subjectivity and sentiment in a cascaded fashion following Wilson et al. (2005). 2.1 Subjectivity Analysis One of most prominent features for subjectivity analysis is the existence of words in a subjectivity lexicon. Mihalcea et al. (2007) translated an existing English subjectivity lexicon from Wiebe and Riloff (2005) using a bilingual dictionary. They also used a subjectivity classifier to automatically annotate the English side of an English-Romanian parallel corpus and then project the annotations to the Romanian side. The projected annotations were used to train a subjectivity classifier. In follow on work, Banea et al. (2010) used MT to exploit annotated SSA English corpora for other languages, including Arabic. They also integrated features from multiple languages to train a combined classifier. In Banea et al. (2008), t"
W13-1608,pak-paroubek-2010-twitter,0,0.0959599,"). In later work, AbdulMageed et al. (2012) extended their work to social content including chat sessions, tweets, Wikipedia discussion pages, and online forums. Unfortunately, their tweets corpus is not publicly available. They added social media features such as author information (person vs. organization and gender). They also explored Arabic specific features that include stemming, POS tagging, and dialect vs. MSA. Their most notable conclusions are: (a) POS tagging helps and (b) Most dialectal Arabic tweets are subjective. Concerning work on subjectivity classification on English tweets, Pak and Paroubek (2010) created a corpus of tweets for SSA. They made a few fundamental assumptions that do not generalize to Arabic well, namely: - They assumed that smiley and sad emoticons imply positive and negative sentiment respectively. Due to the right-to-left orientation of Arabic text, smiley and sad emoticons can be easily interchanged by mistake in Arabic. - They also assumed that news tweets posted by newspapers Twitter accounts are neutral. This assumption is not valid for Arabic news articles because many Arabic newspapers are overly critical or biased in their reporting of news. Thus, the majority of"
W13-1608,H05-1044,0,0.649569,"offers a thorough survey of SSA research. He defines the problem of sentiment analysis including associated SSA terms such as object, opinion, opinion holder, emotions, sentence subjectivity, etc. He also discusses the more popular two stage sentiment and subjectivity classification approach at different granularities (document and sentence levels) using different machine learning approaches (supervised and unsupervised) along with different ways to construct the required data resources (corpora and lexicon). In our work, we classify subjectivity and sentiment in a cascaded fashion following Wilson et al. (2005). 2.1 Subjectivity Analysis One of most prominent features for subjectivity analysis is the existence of words in a subjectivity lexicon. Mihalcea et al. (2007) translated an existing English subjectivity lexicon from Wiebe and Riloff (2005) using a bilingual dictionary. They also used a subjectivity classifier to automatically annotate the English side of an English-Romanian parallel corpus and then project the annotations to the Romanian side. The projected annotations were used to train a subjectivity classifier. In follow on work, Banea et al. (2010) used MT to exploit annotated SSA Englis"
W13-1608,W03-1017,0,0.0387712,"ssifier performed slightly better than an SVM classifier. The classifier assigned a sentence or 59 argmaxP (c) c∈C n Y P (fi |c) (2) i=1 where f is the feature vector and C is the set of pre-defined classes. As for stemming and POS Tagging, we used an in-house reimplementation of AMIRA (Diab, 2009). We report accuracy as well as precision, recall and F-measure for each class. 5.2 Baseline: SSA for MSA 5.2.1 Subjectivity Classification As mentioned in section 2, we employed some of the SSA features that were shown to be successful in the literature (Wiebe and Riloff, 2005; Wilson et al., 2005; Yu and Hatzivassiloglou, 2003) to construct our baseline objective-subjective classifier. We used the automatically translated MPQA and the ArabSenti lexicons. We tokenized and stemmed all words in the dataset and the lexicon. Part of the tokenization involved performing letter normalization where  the variant forms of alef ( @, @, and @) were normal ized to the bare alef ( @), different forms of hamza (ð' and Zø') were normalized to hamza ( Z), ta marbouta  ( è) was normalized to ha ( è), and alef maqsoura ( ø) was normalized to ya ( ø ). We used the following features: Stem-level features: - Stem is a binary features t"
W13-1608,N12-1006,0,0.0114993,"es (Koehn et al., 2007). The two phrase tables were: - an English-MSA phrase table, which was trained on a set of 3.69 million parallel sentences containing 123.4 million English tokens. The sentences were drawn from the UN parallel data along with a variety of parallel news data from LDC and the GALE project. The Arabic side was stemmed (by removing just prefixes) using the Stanford word segmenter (Green and DeNero, 2012). - an English-Dialect phrase table, which was trained on 176K short parallel sentences containing 1.8M Egyptian, Levantine, and Gulf dialectal words and 2.1M English words (Zbib et al., 2012). The Arabic side was also stemmed using the Stanford word segmenter. More formally, Arabic seed words and their English translations were represented using a bipartite graph G = (S, T, M), where S was the set of Arabic words, T was the set of English words, and M was the set of mappings (links or edges) between S and T. First, we found all possible English translations T 0 ⊆ T for each Arabic word si ⊆ S in the seed lexicon. Then, we found all possible Arabic translations S 0 ⊆ S of the English translations T 0 . The mapping score m(sj ⊆ S 0 |si ) would be computed tweet the class c ∈ C that"
W13-1608,D11-1128,1,\N,Missing
W13-1608,J09-3003,0,\N,Missing
W14-3601,al-sabbagh-girju-2012-yadac,0,0.189531,"Missing"
W14-3601,W12-2301,0,0.0200479,"Missing"
W14-3601,N12-1006,0,0.0434936,"g API. Bo Han et al. (2014) (Han et al., 2014) presented an integrated geolocation prediction framework and investigated what 3 Dialectal Arabic (DA) DA refers to the spoken language used for daily communication in Arab countries. There are considerable geographical distinctions between DAs within countries, across country borders, and even between cities and villages as shown in Figure 1. According to Ethnologue (http://www.ethnologue.com/browse/names), there are 34 variations of spoken Arabic or dialects in Arabic countries in addition to the Modern Standard Arabic (MSA). Some recent works (Zbib et al., 2012; Cotterell et al., 2014) are based on a coarser classification of Arabic dialects into five groups namely: Egyptian (EGY), Gulf (GLF), Maghrebi (MGR), Levantine (LEV), and Iraqi (IRQ). Other dialects are classified as OTHER. Zaidan and Callison-Burch (2014) mentioned that this is one possible breakdown but it is relatively coarse and can be further divided into more dialect groups, especially in large regions such as Maghreb. The goal of this paper is to collect a large, clean corpus for each country and study empirically if some of these dialects can be merged together. We found that there a"
W14-3601,P11-2007,0,\N,Missing
W14-3601,D14-1154,1,\N,Missing
W14-3601,J14-1006,0,\N,Missing
W14-3601,cotterell-callison-burch-2014-multi,0,\N,Missing
W14-3601,W14-3628,0,\N,Missing
W14-3601,P13-2081,0,\N,Missing
W14-3617,J03-1002,0,0.007291,"a generative model that attempts to generate all possible mappings of a source word while restricting the output to words in the target language (El-Kahki et al., 2011; Noeman and Madkour, 2010). Specifically, we used the baseline system of El-Kahky et al. (2011). To train character-level mappings, we extracted all the parallel word-pairs in the original (uncorrected) and corrected versions in the training set. If a word in the original version of the training set was actually correct, the word would be mapped to itself. We then aligned the parallel word pairs at character level using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag132 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 132–136, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics final-and heuristic (Koehn et al., 2007). In all, we aligned a little over one million word pairs. As in the baseline of El-Kahki et al. (2011), given a possibly misspelled word worg , we produced all its possible segmentations along with their associated mappings that we learned during alignment. Valid target sequences were retained and sorted by the product of the"
W14-3617,D11-1128,1,\N,Missing
W14-3617,P07-2045,0,\N,Missing
W14-3617,W14-3605,0,\N,Missing
W14-3617,zaghouani-etal-2014-large,0,\N,Missing
W14-3629,D07-1016,0,0.0654313,"Missing"
W14-3629,I08-1058,0,0.0314654,"Missing"
W14-3629,W10-2408,0,0.0281022,"corpus of Arabic microblogs. TM typically involves using transliteration pairs in two different writing systems or alphabets to learning character (or character-sequence) level mappings between them. The learning can be done using the EM algorithm (Kuo et al., 2006) or HMM alignment (Udupa et al., 2009). Once these mappings are learned, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (El-Kahki et al., 2011; Noeman and Madkour, 2010). Other approaches include the use of locality sensitive hashing (Udupa and Kumar, 2010) and classification (Jiampojamarn et al., 2010). Another dramatically different approaches involves the unsupervised learning of transliteration mappings from a large parallel corpus instead of transliteration pairs (Sajjad et al., 2012). In our work, we used the baseline system of El-Kahky et al. (2011). There are three commercial Input Method Editors (IMEs) that convert from Arabizi to Arabic, namely: Yamli3 , Microsoft Maren4 , and Google t3reeb5 . Since they are IMEs, they only work in an interactive mo"
W14-3629,D10-1122,0,0.0278274,"rent writing systems or alphabets to learning character (or character-sequence) level mappings between them. The learning can be done using the EM algorithm (Kuo et al., 2006) or HMM alignment (Udupa et al., 2009). Once these mappings are learned, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (El-Kahki et al., 2011; Noeman and Madkour, 2010). Other approaches include the use of locality sensitive hashing (Udupa and Kumar, 2010) and classification (Jiampojamarn et al., 2010). Another dramatically different approaches involves the unsupervised learning of transliteration mappings from a large parallel corpus instead of transliteration pairs (Sajjad et al., 2012). In our work, we used the baseline system of El-Kahky et al. (2011). There are three commercial Input Method Editors (IMEs) that convert from Arabizi to Arabic, namely: Yamli3 , Microsoft Maren4 , and Google t3reeb5 . Since they are IMEs, they only work in an interactive mode and don’t allow for batch processing. Thus they are difficult to compare against. Als"
W14-3629,D11-1128,1,\N,Missing
W14-3629,W10-2405,0,\N,Missing
W14-3629,P06-1142,0,\N,Missing
W14-3629,P07-2045,0,\N,Missing
W14-3629,N03-1028,0,\N,Missing
W14-3629,habash-etal-2012-conventional,0,\N,Missing
W15-3201,2014.iwslt-papers.1,1,0.783454,"Missing"
W15-3201,W11-2123,0,0.123578,"s 5 Name Classification Experiments Given the 170K Namesarb and 182K Namestrans that we collected, we randomly split the set into 80/20 training and testing splits. We used word unigrams as features. We also examined giving first and last names different weights and character trigrams as a back-off for unseen words. Further, we trained two classifiers namely a Naive Bayes classifier and an SVM classifier. When using a Naive Bayes classifier and a name was not observed during training in general or for a class, we used KenLM language modeling toolkit to compute the smoothing probability of it (Heafield, 2011). Our baseline involved tagging all test items with the tag of the majority class, which means that every tweep would assigned to SA at country level and the Gulf at region level. Table 4 shows the baseline re3 http://www.geonames.org We use ”ISO 3166-1 alpha-2” for country codes 5 http://thenextweb.com/2010/01/15/twitter-geofail-023tweets-geotagged/ 4 4 Figure 2: Country Distribution for Namesarb sults in term of accuracy. Precision for the majority class would be identical to the overall accuracy and recall would be one. Precision and recall would be zero for all the other classes. Name type"
W15-3201,W14-3601,1,0.88284,"Missing"
W15-3201,N12-1006,0,\N,Missing
W15-3201,cotterell-callison-burch-2014-multi,0,\N,Missing
W15-3218,W14-3605,0,0.0422065,"ion approach that handles specific error types such as dialectal word substitution and word splits and merges with the aid of a language model. We also applied corrections that are specific to second language learners that handle erroneous preposition selection, definiteness, and gender-number agreement. 1 Introduction 2 In This paper, we provide a system description for our submissions to the Arabic error correction shared task (QALB-2015 Shared Task on Automatic Correction of Arabic) as part of the Arabic NLP workshop. The QALB-2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) which addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). The current competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Aljtrain-2014, Alj-dev-2014, Alj-test-2014 texts from QALB-2014. The L2 track includes L2-train-2015 and L2-dev-2015. This data was released for the development of the systems. The systems were scored on blind test sets Alj-test-2015 and"
W15-3218,W14-3617,1,0.791492,"Missing"
W15-3218,P12-1049,0,0.0411773,"Missing"
W15-3218,zaghouani-etal-2014-large,0,\N,Missing
W15-3218,W15-1614,0,\N,Missing
W16-4828,bouamor-etal-2014-multidialectal,0,0.275321,"Missing"
W16-4828,D14-1154,1,0.882221,"Missing"
W16-4828,W16-4801,0,0.043439,"Missing"
W16-4828,W14-3601,1,0.917262,"Missing"
W16-4828,P13-2001,1,0.868736,"rams, bigrams with trigrams, all three n-grams, etc. This resulted in a very high-dimensional feature vector. 3.2.2 Character level features These features are more fine-grained than word level features, which would enable our models to learn morphological and utterance based features. Working with more fine-grained features was also shown to be useful in other natural language processing tasks such as machine translation (Sennrich et al., 2016). Character-based models have also been used in literature to convert Egyptian dialect to MSA in order to aid machine translation of Egyptian dialect (Sajjad et al., 2013; Durrani et al., 2014). Hence, this motivates the use of character level features for this task. Character N-grams: Similar to word level features, we experimented with character-level bigrams, trigrams, 4-grams and 5-grams. The motivation behind this was drawn from word examples from different dialects that only differ in a few characters. The average word length in the dataset for the closed task is around 4.5 characters. Thus, we decided not to try values of n that are higher than 5. Character N-gram combinations: Again, similar to word level features, we noticed that each of the n-gram fe"
W16-4828,P16-1162,0,0.0178969,"unigrams, bigrams and trigrams provides its own advantage over the dataset, we decided to experiment with different combinations of these features, such as unigrams with bigrams, bigrams with trigrams, all three n-grams, etc. This resulted in a very high-dimensional feature vector. 3.2.2 Character level features These features are more fine-grained than word level features, which would enable our models to learn morphological and utterance based features. Working with more fine-grained features was also shown to be useful in other natural language processing tasks such as machine translation (Sennrich et al., 2016). Character-based models have also been used in literature to convert Egyptian dialect to MSA in order to aid machine translation of Egyptian dialect (Sajjad et al., 2013; Durrani et al., 2014). Hence, this motivates the use of character level features for this task. Character N-grams: Similar to word level features, we experimented with character-level bigrams, trigrams, 4-grams and 5-grams. The motivation behind this was drawn from word examples from different dialects that only differ in a few characters. The average word length in the dataset for the closed task is around 4.5 characters. T"
W16-4828,P11-2007,0,0.0968841,"Missing"
W16-4828,J14-1006,0,0.183833,"Missing"
W17-1302,D15-1274,0,0.435692,"Missing"
W17-1302,W05-0711,0,0.852256,"Missing"
W17-1302,pasha-etal-2014-madamira,0,0.231088,"Missing"
W17-1302,L16-1170,1,0.907508,"ring with systems that were trained on the ATB, some preprocessing is required, as we show later, to make sure that we are not unfairly penalizing them. 3 Training and Test Corpora 3.2 Data Preparation Given a word in the diacritized corpus, we produce multiple representations of it. To illustrate the representations, we use the word “wakitAbihimo” (and their book) as our running example. 1. diacritized surface form (“wakitAbihimo”). 2. diacritized surface form without case ending. To remove case endings, we segment each word in the corpus to its underlying clitics using the Farasa segmenter (Darwish and Mubarak, 2016). For example, given the diacritized word “wakitAbihimo” (and their book), it would be segmented to the prefix “wa”, stem “kitAbi”, and suffix “himo”. The Our Diacritizer The diacritizer has two main components. The first component recovers the diacritics for the core word (i.e. word without case ending), and the second only recovers the case ending. In this section we describe: the training and test corpora we used and how we processed them; the training of our system that diacritizes core-words and guesses 11 plates. In our example, the template “wfEAlhm” would be mapped to “wafiEAlihimo” an"
W17-1302,W02-0504,0,0.652492,"Missing"
W17-1302,W04-1612,0,0.738422,"Missing"
W17-1302,N07-2014,0,0.12851,"Missing"
W17-1302,P06-1073,0,0.923617,"Missing"
W17-1302,D11-1128,1,\N,Missing
W17-1302,P07-2045,0,\N,Missing
W17-1306,N16-3003,1,0.794973,"or testing, 75 for development and the remaining 200 for training. The concept We followed in LSTM sequence labeling is that segmentation is one-to-one mapping at the character level where each character is annotated as either beginning a segment (B), continues a previous segment (M), ends a segment (E), or is a segment by itself (S). After the labeling is complete we merge the characters and labels  together, for example @ñËñ®J K. byqwlwA is labeled as “SBMMEBE”, which means that the word is segmented as b+yqwl+wA. We compar results of our two LSTM models (BiLSTM and BiLSTMCRF) with Farasa (Abdelali et al., 2016), an open source segementer for MSA3 , and MADAMIRA for Egyptian dialect. Table 3 shows accuracy for Farasa, MADAMIRA, and both of our models. • ËQK @ñË@ AlwAyrls “the  AlHA$tAj “the hashtag”. h. AJAêË@ AlgTY “the Spelling variation: e.g. ù¢ªË@ cover”, úÎë B l&gt;hly “to Ahly”. wireless”, • Morphological inflection (imperative): e.g.  ¯ fwqwA “wake up”. ø Y $dy “pull”, @ñ¯ñ • Segmentation ambiguity: e.g. éJ Ë lyh meaning mAlnA meaning “our “why” or “to him”, AJËAÓ money” or “what we have”. • Combinations not known to MADAMIRA:  ® ® JÓ mtqflwhA$ “don’t close it”, e.g. AëñÊ @ &gt;wSflkwA “I"
W17-1306,W09-0807,0,0.049808,"instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he"
W17-1306,bouamor-etal-2014-multidialectal,0,0.121902,"tching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of “mA byjy lhA$”. • Some affixes are altered in form from their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emp"
W17-1306,D14-1154,1,0.858589,"cters embedding and stacks them to build a matrix. This latter is then used as the input to the Bi-directional LSTM. On the last layer, an affine transformation function followed by a CRF computes the probability distribution over all labels Early Stopping We also employ early stopping (Caruana et al., 2000; Graves et al., 2013b) to mitigate overfitting by monitoring the model’s performance on development set. 4 ary respectively. The architecture of our segmentation model, shown in Figure 2, is straightforward. It comprises the following three layers: Dataset We used the dataset described in (Darwish et al., 2014). The data was used in a dialect identification task to distinguish between dialectal Egyptian and MSA. It contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs. We manually annotated each word in this corpus to provide: CODA-compliant writing (Habash et al., 2012), segmentation, stem, lemma, and POS, also the corr"
W17-1306,W15-3904,0,0.0407118,"Missing"
W17-1306,W16-4828,1,0.865893,"is paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segment"
W17-1306,N16-1030,0,0.164915,"− x + W← −← − h t−1 + b← −) ht = σ(Wx← h t h h h → − ← − → h + W← − h + by yt = W− hy t hy t set of labels. In our case S ={B, M, E, S, WB}, where B is the beginning of a token, M is the middle of a token, E is the end of a token, S is a single character token, and W B is the word boundary. w ~ is the weight vector for weighting the feature vec~ Training and decoding are performed by the tor Φ. Viterbi algorithm. Note that replacing the softmax with CRF at the output layer in neural networks has proved to be very fruitful in many sequence labeling tasks (Ma and Hovy, 2016; Huang et al., 2015; Lample et al., 2016; Samih et al., 2016) More interpretations about these formulas are found in Graves et al. (2013a). A very important element of the recent success of many NLP applications, is the use of characterlevel representations in deep neural networks. This has shown to be effective for numerous NLP tasks (Collobert et al., 2011; dos Santos et al., 2015) as it can capture word morphology and reduce out-of-vocabulary. This approach has also been especially useful for handling languages with rich morphology and large character sets (Kim et al., 2016). We use pre-trained character embeddings to initialize"
W17-1306,P16-1101,0,0.269252,"(ct ) where σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Lipton et al., 2015). Figure 1 illustrates a single LSTM memory cell (Graves and Schmidhuber, 2005) Arabic Segmentation Model In this section, we will provide a brief description of LSTM, and introduce the different components of our Arabic segmentation model. For all our work, we used the Keras toolkit (Chollet, 2015). The architecture of our model, shown in Figure 2 is similar to Ma and Hovy (2016), Huang et al. (2015), and Collobert et al. (2011) 3.1 Long Short-term Memory A recurrent neural network (RNN) belongs to a family of neural networks suited for modeling sequential data. Given an input sequence x = (x1 , ..., xn ), an RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n: 1 Figure 1: A Long Short-Term Memory Cell. 3.2 Bi-directional LSTM Bi-LSTM networks (Schuster and Paliwal, 1997) are extensions to the single LSTM networks. They MADAMIRA release 20160516 2.1 48 are capable of learning long-term dependencies and maintain contex"
W17-1306,maamouri-etal-2014-developing,0,0.0957441,"Missing"
W17-1306,habash-etal-2012-conventional,0,0.430983,"their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emphasis, e.g. úÍððððððñ«X@ AdEwwwwwwwliy “pray for me”. tAtuw “tattoo”, or coinage, such as the negative particles Ég. @P rAjil “man” Ég. P rajul, and vowel shortening, such as AÖß X dayomA “always” from AÖß @X dAyomA. from • Lack of standard orthography. Many of the words in DA do not follow a standard orthographic system (Habash et al., 2012). “cafe” and H t or  s as in Q J» kvyr Õç' tm → ñK tw. • Some morphological patterns that do not exist in MSA, such as the passive pattern AitofaEal, such as QåºK@ Aitokasar “it broke”. 47 For segmentation, Yao and Huang (2016) successfully used a bi-directional LSTM model for segmenting Chinese text. In this paper, we build on their work and extend it in two ways, namely combining bi-LSTM with CRF and applying on Arabic, which is an alphabetic language. Mohamed et al. (2012) built a segmenter based on memory-based learning. The segmenter has been trained on a small corpus of Egyptian"
W17-1306,mohamed-etal-2012-annotating,0,0.411081,"Missing"
W17-1306,N13-1044,0,0.282692,"neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction"
W17-1306,P14-2034,0,0.310795,"Missing"
W17-1306,pasha-etal-2014-madamira,0,0.190181,"Missing"
W17-1306,P13-2001,1,0.858088,"tures or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object pronoun “hA”, and the post"
W17-1306,W16-5806,1,0.909334,"under lenition, softening of a consonant, or fortition, hardening of a consonant. • Vowel elongation, such as • The use of masculine plural or singular noun forms instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple"
W17-1306,P16-1162,0,0.049716,"reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object prono"
W17-1306,J14-1006,0,0.025968,"ing some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of"
W17-1306,N12-1006,0,0.0419555,"inine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her),"
W17-1316,darwish-etal-2014-using,1,0.942927,"engineering and word embeddigns in Arabic POS tagging. We show that feature engineering improves POS tagging significantly. • We explore the effectiveness of many features including morphological and contextual features for tagging each clitic or each word in-context. 130 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 130–137, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics tagsets such as that of the Penn Arabic Treebank (ATB), which has 70 tags (Maamouri et al., 2004). In our work, we elected to use the tagest proposed by Darwish et al. (2014) which is a simplified version of ATB tagset and uses 18 tags only. • We open-source both Arabic POS taggers, both of which are written entirely in Java. The SVMRank -based system has a load time of 5 seconds and can process about 2,000 word/second on an laptop with Intel i7 processor with 16 GB of RAM. 2 2.1 2.2 Background Arabic POS Tagging Most recent work on Arabic POS tagging has used statistical methods. Diab (2009) used an SVM classifier to ascertain the optimal POS tags. The classifier was trained on the ATB data. Essentially, they treated the problem as a sequence-labeling problem. An"
W17-1316,N12-1015,0,0.0186639,"Missing"
W17-1316,N13-1090,0,0.0296891,"tput i as follows: li = tanh(Lf Sif + Lb Sib + bl ) where Lf , Lb and bl denote the parameters for combining the forward and backward states. We experimented with a number of settings where the clitic sequence was augmented with a subset of features that includes character sequences, word meta type, stem template (Darwish et al., 2014), and also combined with 200 dimension word embeddings learned over the aforementioned collection of text containing 10 years of Al-Jazeera articles1 . To create the embeddings, we used word2vec with continuous skip-gram learning algorithm with an 8 gram window (Mikolov et al., 2013)2 . For the bi-LSTM experiments, we used the Java Neural Network Library3 , which is tuned for POS tagging(Ling et al., 2015). We extended the library to produce the additional aforementioned features. • Word context features: p(P OS|w−1 ), p(P OS|w1 ), p(P OS|w−2 , w−1 ), p(P OS|w−3 , w−2 , w−1 ), and p(P OS|w−4 , w−3 , w−2 , w−1 ) 3.1.3 OOVs and pre-Filtering For both clitic and word tagging, In case we could not compute a feature value during training (e.g., a clitic was never observed with a given POS tag), the feature value is assigned a small  value equal to 10−10 . If the clitic is a p"
W17-1316,pasha-etal-2014-madamira,0,0.226248,"Missing"
W17-1316,P16-2067,0,0.0573499,"Missing"
W17-1316,P09-2056,0,\N,Missing
W17-1316,L16-1170,1,\N,Missing
W17-3008,W16-5618,0,0.105291,"Missing"
W19-4603,J92-4003,0,0.551827,"Missing"
W19-4603,W16-5801,0,0.0290695,"Missing"
W19-4603,P13-2037,0,0.0590321,"Missing"
W19-4603,W16-5812,0,0.0243184,"is relatively less challenging for computational analysis, as each sentence still follows a monolingual model, intrasentential CS poses a bottleneck challenge. It needs a special amount of attention, because it is only this type that involves the lexical and syntactic integration and activation of two language models at the same time. NLP systems trained on monolingual data suffer significantly when trying to process this kind bilingual text or utterance. CS has proved challenging for NLP technologies, not only because current tools are geared toward the processing of one language at a time (AlGhamdi et al., 2016), but also because codeswitched data is typically associated with additional challenges such as the non-conventional orthography, non-canonicity (nonstandard or incomplete) of syntactic structures, and the large number of OOV-words (Çetino˘glu et al., 2016), which suggest the need for larger training data than what is typically used in monolingual models. Unfortunately, shortage of training data has usually been cited as the reason for the under-performance of 19 diglossic code-switching, the shift is more likely to be lexical, morphological, and structural, rather than phonological, unlike th"
W19-4603,C12-2011,1,0.885038,"Missing"
W19-4603,Q16-1026,0,0.0173957,"nsure that we get representations of all the words and reduce the number of OOVs (out of vocabulary words). We find significant improvement using FastText embedding over the traditional word2vec representation (Mikolov et al., 2013). This is probably due to the utilization of sub-word (ex. prefixes or suffixes) information in the former. Character-level CNNs. Although originally designed for image recognition, CNNs have proven effective for various NLP tasks due to their ability to encode character-level representations of words as well as extract sub-word information (Collobert et al., 2011; Chiu and Nichols, 2016; dos Santos and Guimarães, 2015). Bi-LSTM Recurrent neural networks (RNN) are well suited for modeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. Figure 3: DNN Architecture. BCs on our crawled code-"
W19-4603,attia-etal-2010-automatically,1,0.834799,"Missing"
W19-4603,L18-1015,1,0.899707,"Missing"
W19-4603,W14-3901,0,0.0289644,"tagging of CS data and concluded that applying a machine learning framework as a voting mechanism on top of the output of two monolingual POS taggers achieves the best performance. Word-level CS identification for Arabic (along with Spanish–English) has been featured in a couple of shared tasks: the First Shared Task on Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been used to aid with the identification of code-switching points, and in some other instances language identific"
W19-4603,Q17-1010,0,0.0325773,"Missing"
W19-4603,N13-1039,0,0.0935945,"Missing"
W19-4603,C82-1023,0,0.578981,"tactic rules of the two languages involved, sometime adding in or leaving out a determiner, or applying a system of affixation from one language and not the other. 1.2 Definition and Defining Perspectives The definition of CS has varied greatly depending on the different researchers’ attitude and perspectives of the operation involved. While some viewed it as a process where two languages are actively interacting with each other (ultimately creating a new code), other viewed the operation just as two separate languages sitting side-by-side as isolated islands. Following the first perspective, Joshi (1982) defined code-switching as the situation when two languages systematically interact with each other in the production of sentences in a framework which consists of two grammatical systems and a mechanism for switching between the two. Following the second perspective, Muysken (1995) defined CS as “the alternative use by bilinguals of two or more languages in the same conversation”, while other researchers (Auer, 1999; Nilep, 2006) defined it as the “juxtaposition” of elements from two different grammatical systems within the same speech. The juxtaposition definition has been widely cited in th"
W19-4603,N16-1030,0,0.01,"®K A£ É¯ BA« System Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Translit. / Gloss byHbk Fine"
W19-4603,D17-1035,0,0.0140169,"tem Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Translit. / Gloss byHbk Fine Tag prog_part Coarse Tag Ve"
W19-4603,P08-2030,0,0.104649,"Missing"
W19-4603,P16-1101,0,0.0119628,".j K. QÒªË@ð J.Ê¯ ®K A£ É¯ BA« System Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Transli"
W19-4603,W16-5806,1,0.944761,"abels ambiguous unk lang1 lang2 mixed ne other et al. (2016) explored different technique for the POS tagging of CS data and concluded that applying a machine learning framework as a voting mechanism on top of the output of two monolingual POS taggers achieves the best performance. Word-level CS identification for Arabic (along with Spanish–English) has been featured in a couple of shared tasks: the First Shared Task on Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been use"
W19-4603,W16-5805,0,0.0180421,"themselves, but our results show that words still give a stronger signal than POS tags alone. We also notice that Brown Clusters, named entity gazetteers and FastText pre-trained embeddings contribute to incrementally improve the performance of the system. Unfortunately adding information from the spelling word list did not show any improvement on the system, and this is why it is removed from the final system architecture. Now we compare our best model to the state-ofthe-art system of Samih et al. (2016), which won the 2016 Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016) on the MSA–EA dataset. We compare the performance of the two systems in terms of f-score accuracy on both the development and test set, in Table 5 and Table 6 respectively. We also include the number of instances and the ratio percentage for each label. As the tables show, the category lang2 constitutes the majority class for both úÍ@ <ilY “to”, which can equally be used as either lang1 or lang2, depending on the context. 6 Conclusion We have presented a neural network system for conducting word-level code-switching identification. Our system outperforms the current stateof-the-art, and we sh"
W19-4603,W15-3904,0,0.0125713,"tions of all the words and reduce the number of OOVs (out of vocabulary words). We find significant improvement using FastText embedding over the traditional word2vec representation (Mikolov et al., 2013). This is probably due to the utilization of sub-word (ex. prefixes or suffixes) information in the former. Character-level CNNs. Although originally designed for image recognition, CNNs have proven effective for various NLP tasks due to their ability to encode character-level representations of words as well as extract sub-word information (Collobert et al., 2011; Chiu and Nichols, 2016; dos Santos and Guimarães, 2015). Bi-LSTM Recurrent neural networks (RNN) are well suited for modeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. Figure 3: DNN Architecture. BCs on our crawled code-switched corpus of 380 million wo"
W19-4603,N16-1159,0,0.025454,"Missing"
W19-4603,D08-1102,0,0.0258644,"Missing"
W19-4603,D08-1110,0,0.107844,"Missing"
W19-4603,W15-1511,0,0.061289,"Missing"
W19-4603,W15-2902,0,0.0329637,"Missing"
W19-4603,W17-2911,0,0.0157523,"n Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been used to aid with the identification of code-switching points, and in some other instances language identification has been used as an indicator or a feature for POS tagging, showing what (Çetino˘glu et al., 2016) referred to as task inter-relatedness, or the cyclic nature of task dependencies. In our work, we use a POS tagger as a predictor of CS. The POS tagger used has been trained specifically on CS data. 3 Token Count"
W19-4639,W18-3930,0,0.112038,"Missing"
W19-4639,W16-4818,0,0.0429729,"Missing"
W19-4639,E17-2068,0,0.0808269,"Missing"
W19-4639,W14-3601,1,0.945204,"Missing"
W19-4639,C18-1113,0,0.0919411,"Missing"
W19-4639,K17-1043,1,0.900152,"Missing"
W19-4639,L16-1658,1,0.898269,"Missing"
W19-4639,P11-2007,0,0.0851928,"Missing"
W19-4639,J14-1006,0,0.163523,"2016). The wide spread of dialectal use has increased the richness and diversity of the language, requiring greater complexity in dealing with it. Non-standard orthography, increased borrowing and coinage of new terms, and code switching are just a few among a long list of new challenges researchers have to deal with. Studying language varieties in particular is associated with important applications such as Dialect Identification (DID), Machine Translation (MT), and other text mining tasks. Performing DID can be achieved using a variety of features, such as character n-grams (Darwish, 2014; Zaidan and Callison-Burch, 2014; Malmasi et al., 2015), and a myriad of techniques, such as 2 System descriptions For both SubTask 1 and SubTask 2, we employed a hybrid system that incorporates different classifiers and components such DNNs and heuristics to perform sentence level dialectal Arabic identification. The classification strategy is built as a cascaded voting system that tags each sequence based on the decisions from two other underlying classifiers. DNNs: This model uses both Bidirectional Long Short Term Memory (Bi-LSTM) and Convolutional Neural Network (CNN) architectures to jointly learn both word-level and c"
