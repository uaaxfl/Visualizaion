2021.emnlp-tutorials.2,Financial Opinion Mining,2021,-1,-1,3,1,8626,chungchi chen,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"In this tutorial, we will show where we are and where we will be to those researchers interested in this topic. We divide this tutorial into three parts, including coarse-grained financial opinion mining, fine-grained financial opinion mining, and possible research directions. This tutorial starts by introducing the components in a financial opinion proposed in our research agenda and summarizes their related studies. We also highlight the task of mining customers{'} opinions toward financial services in the FinTech industry, and compare them with usual opinions. Several potential research questions will be addressed. We hope the audiences of this tutorial will gain an overview of financial opinion mining and figure out their research directions."
2021.emnlp-main.362,Semantics-Preserved Data Augmentation for Aspect-Based Sentiment Analysis,2021,-1,-1,4,0,9451,tingwei hsu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Both the issues of data deficiencies and semantic consistency are important for data augmentation. Most of previous methods address the first issue, but ignore the second one. In the cases of aspect-based sentiment analysis, violation of the above issues may change the aspect and sentiment polarity. In this paper, we propose a semantics-preservation data augmentation approach by considering the importance of each word in a textual sequence according to the related aspects and sentiments. We then substitute the unimportant tokens with two replacement strategies without altering the aspect-level polarity. Our approach is evaluated on several publicly available sentiment analysis datasets and the real-world stock price/risk movement prediction scenarios. Experimental results show that our methodology achieves better performances in all datasets."
2021.eacl-main.122,Dynamic Graph Transformer for Implicit Tag Recognition,2021,-1,-1,4,0,10692,yiting liou,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Textual information extraction is a typical research topic in the NLP community. Several NLP tasks such as named entity recognition and relation extraction between entities have been well-studied in previous work. However, few works pay their attention to the implicit information. For example, a financial news article mentioned {``}Apple Inc.{''} may be also related to Samsung, even though Samsung is not explicitly mentioned in this article. This work presents a novel dynamic graph transformer that distills the textual information and the entity relations on the fly. Experimental results confirm the effectiveness of our approach to implicit tag recognition."
2020.semeval-1.279,{NTU}{\\_}{NLP} at {S}em{E}val-2020 Task 12: Identifying Offensive Tweets Using Hierarchical Multi-Task Learning Approach,2020,-1,-1,3,0,15380,pochun chen,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper presents our hierarchical multi-task learning (HMTL) and multi-task learning (MTL) approaches for improving the text encoder in Sub-tasks A, B, and C of Multilingual Offensive Language Identification in Social Media (SemEval-2020 Task 12). We show that using the MTL approach can greatly improve the performance of complex problems, i.e. Sub-tasks B and C. Coupled with a hierarchical approach, the performances are further improved. Overall, our best model, HMTL outperforms the baseline model by 3{\%} and 2{\%} of Macro F-score in Sub-tasks B and C of OffensEval 2020, respectively."
2020.lrec-1.76,{MPDD}: A Multi-Party Dialogue Dataset for Analysis of Emotions and Interpersonal Relationships,2020,-1,-1,3,0,16758,yiting chen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"A dialogue dataset is an indispensable resource for building a dialogue system. Additional information like emotions and interpersonal relationships labeled on conversations enables the system to capture the emotion flow of the participants in the dialogue. However, there is no publicly available Chinese dialogue dataset with emotion and relation labels. In this paper, we collect the conversions from TV series scripts, and annotate emotion and interpersonal relationship labels on each utterance. This dataset contains 25,548 utterances from 4,142 dialogues. We also set up some experiments to observe the effects of the responded utterance on the current utterance, and the correlation between emotion and relation types in emotion and relation classification tasks."
2020.lrec-1.128,{C}hinese Discourse Parsing: Model and Evaluation,2020,-1,-1,4,0,16872,lin chuanan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Chinese discourse parsing, which aims to identify the hierarchical relationships of Chinese elementary discourse units, has not yet a consistent evaluation metric. Although Parseval is commonly used, variations of evaluation differ from three aspects: micro vs. macro F1 scores, binary vs. multiway ground truth, and left-heavy vs. right-heavy binarization. In this paper, we first propose a neural network model that unifies a pre-trained transformer and CKY-like algorithm, and then compare it with the previous models with different evaluation scenarios. The experimental results show that our model outperforms the previous systems. We conclude that (1) the pre-trained context embedding provides effective solutions to deal with implicit semantics in Chinese texts, and (2) using multiway ground truth is helpful since different binarization approaches lead to significant differences in performance."
2020.lrec-1.711,{MSD}-1030: A Well-built Multi-Sense Evaluation Dataset for Sense Representation Models,2020,-1,-1,5,0,18054,tingyu yen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Sense embedding models handle polysemy by giving each distinct meaning of a word form a separate representation. They are considered improvements over word models, and their effectiveness is usually judged with benchmarks such as semantic similarity datasets. However, most of these datasets are not designed for evaluating sense embeddings. In this research, we show that there are at least six concerns about evaluating sense embeddings with existing benchmark datasets, including the large proportions of single-sense words and the unexpected inferior performance of several multi-sense models to their single-sense counterparts. These observations call into serious question whether evaluations based on these datasets can reflect the sense model{'}s ability to capture different meanings. To address the issues, we propose the Multi-Sense Dataset (MSD-1030), which contains a high ratio of multi-sense word pairs. A series of analyses and experiments show that MSD-1030 serves as a more reliable benchmark for sense embeddings. The dataset is available at http://nlg.csie.ntu.edu.tw/nlpresource/MSD-1030/."
2020.lrec-1.749,"Issues and Perspectives from 10,000 Annotated Financial Social Media Data",2020,-1,-1,3,1,8626,chungchi chen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we investigate the annotation of financial social media data from several angles. We present Fin-SoMe, a dataset with 10,000 labeled financial tweets annotated by experts from both the front desk and the middle desk in a bank{'}s treasury. These annotated results reveal that (1) writer-labeled market sentiment may be a misleading label; (2) writer{'}s sentiment and market sentiment of an investor may be different; (3) most financial tweets provide unfounded analysis results; and (4) almost no investors write down the gain/loss results for their positions, which would otherwise greatly facilitate detailed evaluation of their performance. Based on these results, we address various open problems and suggest possible directions for future work on financial social media data. We also provide an experiment on the key snippet extraction task to compare the performance of using a general sentiment dictionary and using the domain-specific dictionary. The results echo our findings from the experts{'} annotations."
2020.fnp-1.11,"{NTUNLPL} at {F}in{C}ausal 2020, Task 2:Improving Causality Detection Using {V}iterbi Decoder",2020,-1,-1,4,0,19326,peiwei kao,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,0,"In order to provide an explanation of machine learning models, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage."
2020.coling-main.199,Heterogeneous Recycle Generation for {C}hinese Grammatical Error Correction,2020,-1,-1,3,0,21294,charles hinson,Proceedings of the 28th International Conference on Computational Linguistics,0,"Most recent works in the field of grammatical error correction (GEC) rely on neural machine translation-based models. Although these models boast impressive performance, they require a massive amount of data to properly train. Furthermore, NMT-based systems treat GEC purely as a translation task and overlook the editing aspect of it. In this work we propose a heterogeneous approach to Chinese GEC, composed of a NMT-based model, a sequence editing model, and a spell checker. Our methodology not only achieves a new state-of-the-art performance for Chinese GEC, but also does so without relying on data augmentation or GEC-specific architecture changes. We further experiment with all possible configurations of our system with respect to model composition order and number of rounds of correction. A detailed analysis of each model and their contributions to the correction process is performed by adapting the ERRANT scorer to be able to score Chinese sentences."
2020.acl-main.13,A Complete Shift-Reduce {C}hinese Discourse Parser with Robust Dynamic Oracle,2020,-1,-1,3,0,16873,shyhshiun hung,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance."
W19-4508,Lexicon Guided Attentive Neural Network Model for Argument Mining,2019,0,1,4,0,24132,jianfu lin,Proceedings of the 6th Workshop on Argument Mining,0,"Identification of argumentative components is an important stage of argument mining. Lexicon information is reported as one of the most frequently used features in the argument mining research. In this paper, we propose a methodology to integrate lexicon information into a neural network model by attention mechanism. We conduct experiments on the UKP dataset, which is collected from heterogeneous sources and contains several text types, e.g., microblog, Wikipedia, and news. We explore lexicons from various application scenarios such as sentiment analysis and emotion detection. We also compare the experimental results of leveraging different lexicons."
P19-1635,Numeracy-600{K}: Learning Numeracy for Detecting Exaggerated Information in Market Comments,2019,0,5,4,1,8626,chungchi chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16{\%}, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71{\%}. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task."
S18-1171,{NTU} {NLP} Lab System at {S}em{E}val-2018 Task 10: Verifying Semantic Differences by Integrating Distributional Information and Expert Knowledge,2018,0,2,3,1,5402,yowting shiue,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper presents the NTU NLP Lab system for the SemEval-2018 Capturing Discriminative Attributes task. Word embeddings, pointwise mutual information (PMI), ConceptNet edges and shortest path lengths are utilized as input features to build binary classifiers to tell whether an attribute is discriminative for a pair of concepts. Our neural network model reaches about 73{\%} F1 score on the test set and ranks the 3rd in the task. Though the attributes to deal with in this task are all visual, our models are not provided with any image data. The results indicate that visual information can be derived from textual data."
P18-2122,Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection,2018,0,5,3,1,8627,henhsen huang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection. Furthermore, we apply our model to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data."
L18-1139,Transfer of Frames from {E}nglish {F}rame{N}et to Construct {C}hinese {F}rame{N}et: A Bilingual Corpus-Based Approach,2018,0,2,4,0,12615,tsunghan yang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1541,Learning to Map Natural Language Statements into Knowledge Base Representations for Knowledge Base Construction,2018,0,2,3,0,30112,chinho lin,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-2016,A Unified {R}v{NN} Framework for End-to-End {C}hinese Discourse Parsing,2018,0,0,4,0,16872,lin chuanan,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"This paper demonstrates an end-to-end Chinese discourse parser. We propose a unified framework based on recursive neural network (RvNN) to jointly model the subtasks including elementary discourse unit (EDU) segmentation, tree structure construction, center labeling, and sense labeling. Experimental results show our parser achieves the state-of-the-art performance in the Chinese Discourse Treebank (CDTB) dataset. We release the source code with a pre-trained model for the NLP community. To the best of our knowledge, this is the first open source toolkit for Chinese discourse parsing. The standalone toolkit can be integrated into subsequent applications without the need of external resources such as syntactic parser."
C18-2030,A {C}hinese Writing Correction System for Learning {C}hinese as a Foreign Language,2018,0,0,3,1,5402,yowting shiue,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"We present a Chinese writing correction system for learning Chinese as a foreign language. The system takes a wrong input sentence and generates several correction suggestions. It also retrieves example Chinese sentences with English translations, helping users understand the correct usages of certain grammar patterns. This is the first available Chinese writing error correction system based on the neural machine translation framework. We discuss several design choices and show empirical results to support our decisions."
C18-1141,{G}en{S}ense: A Generalized Sense Retrofitting Model,2018,0,1,5,0,10983,yangyin lee,Proceedings of the 27th International Conference on Computational Linguistics,0,"With the aid of recently proposed word embedding algorithms, the study of semantic similarity has progressed and advanced rapidly. However, many natural language processing tasks need sense level representation. To address this issue, some researches propose sense embedding learning algorithms. In this paper, we present a generalized model from existing sense retrofitting model. The generalization takes three major components: semantic relations between the senses, the relation strength and the semantic strength. In the experiment, we show that the generalized model can outperform previous approaches in three types of experiment: semantic relatedness, contextual word similarity and semantic difference."
C18-1204,Correcting {C}hinese Word Usage Errors for Learning {C}hinese as a Second Language,2018,0,0,3,1,5402,yowting shiue,Proceedings of the 27th International Conference on Computational Linguistics,0,"With more and more people around the world learning Chinese as a second language, the need of Chinese error correction tools is increasing. In the HSK dynamic composition corpus, word usage error (WUE) is the most common error type. In this paper, we build a neural network model that considers both target erroneous token and context to generate a correction vector and compare it against a candidate vocabulary to propose suitable corrections. To deal with potential alternative corrections, the top five proposed candidates are judged by native Chinese speakers. For more than 91{\%} of the cases, our system can propose at least one acceptable correction within a list of five candidates. To the best of our knowledge, this is the first research addressing general-type Chinese WUE correction. Our system can help non-native Chinese learners revise their sentences by themselves."
S17-2144,{NLG}301 at {S}em{E}val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News,2017,0,2,3,1,8626,chungchi chen,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Short length, multi-targets, target relation-ship, monetary expressions, and outside reference are characteristics of financial tweets. This paper proposes methods to extract target spans from a tweet and its referencing web page. Total 15 publicly available sentiment dictionaries and one sentiment dictionary constructed from training set, containing sentiment scores in binary or real numbers, are used to compute the sentiment scores of text spans. Moreover, the correlation coeffi-cients of the price return between any two stocks are learned with the price data from Bloomberg. They are used to capture the relationships between the interesting tar-get and other stocks mentioned in a tweet. The best result of our method in both sub-task are 56.68{\%} and 55.43{\%}, evaluated by evaluation method 2."
S17-2177,{NTU}-1 at {S}em{E}val-2017 Task 12: Detection and classification of temporal events in clinical data with domain adaptation,2017,0,1,5,0,32393,poyu huang,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This study proposes a system to participate in the Clinical TempEval 2017 shared task, a part of the SemEval 2017 Tasks. Domain adaptation was the main challenge this year. We took part in the supervised domain adaption where data of 591 records of colon cancer patients and 30 records of brain cancer patients from Mayo clinic were given and we are asked to analyze the records from brain cancer patients. Based on the THYME corpus released by the organizer of Clinical TempEval, we propose a framework that automatically analyzes clinical temporal events in a fine-grained level. Support vector machine (SVM) and conditional random field (CRF) were implemented in our system for different subtasks, including detecting clinical relevant events and time expression, determining their attributes, and identifying their relations with each other within the document. The results showed the capability of domain adaptation of our system."
P17-2064,Detection of {C}hinese Word Usage Errors for Non-Native {C}hinese Learners with Bidirectional {LSTM},2017,9,2,3,1,5402,yowting shiue,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Selecting appropriate words to compose a sentence is one common problem faced by non-native Chinese learners. In this paper, we propose (bidirectional) LSTM sequence labeling models and explore various features to detect word usage errors in Chinese sentences. By combining CWINDOW word embedding features and POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79{\%} of the test data, the model ranks the ground-truth within the top two at position level."
I17-1098,"Integrating Subject, Type, and Property Identification for Simple Question Answering over Knowledge Base",2017,0,1,3,0,32927,weichuan hsiao,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"This paper presents an approach to identify subject, type and property from knowledge base (KB) for answering simple questions. We propose new features to rank entity candidates in KB. Besides, we split a relation in KB into type and property. Each of them is modeled by a bi-directional LSTM. Experimental results show that our model achieves the state-of-the-art performance on the SimpleQuestions dataset. The hard questions in the experiments are also analyzed in detail."
P16-2004,Implicit Polarity and Implicit Aspect Recognition in Opinion Mining,2016,9,2,2,1,34396,huanyuan chen,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,None
L16-1033,Detecting Word Usage Errors in {C}hinese Sentences for Learning {C}hinese as a Foreign Language,2016,8,2,2,1,5402,yowting shiue,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Automated grammatical error detection, which helps users improve their writing, is an important application in NLP. Recently more and more people are learning Chinese, and an automated error detection system can be helpful for the learners. This paper proposes n-gram features, dependency count features, dependency bigram features, and single-character features to determine if a Chinese sentence contains word usage errors, in which a word is written as a wrong form or the word selection is inappropriate. With marking potential errors on the level of sentence segments, typically delimited by punctuation marks, the learner can try to correct the problems without the assistant of a language teacher. Experiments on the HSK corpus show that the classifier combining all sets of features achieves an accuracy of 0.8423. By utilizing certain combination of the sets of features, we can construct a system that favors precision or recall. The best precision we achieve is 0.9536, indicating that our system is reliable and seldom produces misleading results."
L16-1164,Fine-Grained {C}hinese Discourse Relation Labelling,2016,1,0,4,1,34396,huanyuan chen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper explores several aspects together for a fine-grained Chinese discourse analysis. We deal with the issues of ambiguous discourse markers, ambiguous marker linkings, and more than one discourse marker. A universal feature representation is proposed. The pair-once postulation, cross-discourse-unit-first rule and word-pair-marker-first rule select a set of discourse markers from ambiguous linkings. Marker-Sum feature considers total contribution of markers and Marker-Preference feature captures the probability distribution of discourse functions of a representative marker by using preference rule. The HIT Chinese discourse relation treebank (HIT-CDTB) is used to evaluate the proposed models. The 25-way classifier achieves 0.57 micro-averaged F-score."
L16-1198,Subtask Mining from Search Query Logs for How-Knowledge Acceleration,2016,10,0,2,0,34918,chunglun kuo,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"How-knowledge is indispensable in daily life, but has relatively less quantity and poorer quality than what-knowledge in publicly available knowledge bases. This paper first extracts task-subtask pairs from wikiHow, then mines linguistic patterns from search query logs, and finally applies the mined patterns to extract subtasks to complete given how-to tasks. To evaluate the proposed methodology, we group tasks and the corresponding recommended subtasks into pairs, and evaluate the results automatically and manually. The automatic evaluation shows the accuracy of 0.4494. We also classify the mined patterns based on prepositions and find that the prepositions like {``}on{''}, {``}to{''}, and {``}with{''} have the better performance. The results can be used to accelerate how-knowledge base construction."
C16-2059,{NL}2{KB}: Resolving Vocabulary Gap between Natural Language and Knowledge Base in Knowledge Base Construction and Retrieval,2016,5,0,4,0,35678,shenglun wei,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"Words to express relations in natural language (NL) statements may be different from those to represent properties in knowledge bases (KB). The vocabulary gap becomes barriers for knowledge base construction and retrieval. With the demo system called NL2KB in this paper, users can browse which properties in KB side may be mapped to for a given relational pattern in NL side. Besides, they can retrieve the sets of relational patterns in NL side for a given property in KB side. We describe how the mapping is established in detail. Although the mined patterns are used for Chinese knowledge base applications, the methodology can be extended to other languages."
C16-1085,{C}hinese Preposition Selection for Grammatical Error Diagnosis,2016,0,2,3,1,8627,henhsen huang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Misuse of Chinese prepositions is one of common word usage errors in grammatical error diagnosis. In this paper, we adopt the Chinese Gigaword corpus and HSK corpus as L1 and L2 corpora, respectively. We explore gated recurrent neural network model (GRU), and an ensemble of GRU model and maximum entropy language model (GRU-ME) to select the best preposition from 43 candidates for each test sentence. The experimental results show the advantage of the GRU models over simple RNN and n-gram models. We further analyze the effectiveness of linguistic information such as word boundary and part-of-speech tag in this task."
C16-1178,"Detection, Disambiguation and Argument Identification of Discourse Connectives in {C}hinese Discourse Parsing",2016,0,0,2,0,35781,yongsiang shih,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we investigate four important issues together for explicit discourse relation labelling in Chinese texts: (1) discourse connective extraction, (2) linking ambiguity resolution, (3) relation type disambiguation, and (4) argument boundary identification. In a pipelined Chinese discourse parser, we identify potential connective candidates by string matching, eliminate non-discourse usages from them with a binary classifier, resolve linking ambiguities among connective components by ranking, disambiguate relation types by a multiway classifier, and determine the argument boundaries by conditional random fields. The experiments on Chinese Discourse Treebank show that the F1 scores of 0.7506, 0.7693, 0.7458, and 0.3134 are achieved for discourse usage disambiguation, linking disambiguation, relation type disambiguation, and argument boundary identification, respectively, in a pipelined Chinese discourse parser."
C16-1210,{C}hinese Tense Labelling and Causal Analysis,2016,0,0,3,1,8627,henhsen huang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper explores the role of tense information in Chinese causal analysis. Both tasks of causal type classification and causal directionality identification are experimented to show the significant improvement gained from tense features. To automatically extract the tense features, a Chinese tense predictor is proposed. Based on large amount of parallel data, our semi-supervised approach improves the dependency-based convolutional neural network (DCNN) models for Chinese tense labelling and thus the causal analysis."
W15-3106,Introduction to {SIGHAN} 2015 Bake-off for {C}hinese Spelling Check,2015,4,9,4,1,2391,yuenhsien tseng,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper introduces the SIGHAN 2015 Bake-off for Chinese Spelling Check, including task description, data preparation, performance metrics, and evaluation results. The competition reveals current state-of-the-art NLP techniques in dealing with Chinese spelling checking. All data sets with gold standards and evaluation tool used in this bake-off are publicly available for future research."
W14-6820,Overview of {SIGHAN} 2014 Bake-off for {C}hinese Spelling Check,2014,4,16,4,0,2441,liangchih yu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper introduces a Chinese Spelling Check campaign organized for the SIGHAN 2014 bake-off, including task description, data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques."
P14-5018,{FA}d{R}: A System for Recognizing False Online Advertisements,2014,12,2,2,1,39110,yijie tang,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"More and more product information, including advertisements and user reviews, are presented to Internet users nowadays. Some of the information is false, misleading or overstated, which can cause seriousness and needs to be identified. Authorities, advertisers, website owners and consumers all have the needs to detect such statements. In this paper, we propose a False Advertisements Recognition system called FAdR by using one-class and binary classification models. Illegal advertising lists made public by a government and product descriptions from a shopping website are obtained for training and testing. The results show that the binary SVM models can achieve the highest performance when unigrams with the weighting of log relative frequency ratios are used as features. Comparatively, the benefit of the one-class classification models is the adjustable rejection rate parameter, which can be changed to suit different applications. Verb phrases more likely to introduce overstated information are obtained by mining the datasets. These phrases help find problematic wordings in the advertising texts."
O14-4003,Modeling Human Inference Process for Textual Entailment Recognition,2014,0,0,3,1,8627,henhsen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 3, September 2014",0,None
huang-etal-2014-sentence,Sentence Rephrasing for Parsing Sentences with {OOV} Words,2014,11,2,4,1,8627,henhsen huang,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper addresses the problems of out-of-vocabulary (OOV) words, named entities in particular, in dependency parsing. The OOV words, whose word forms are unknown to the learning-based parser, in a sentence may decrease the parsing performance. To deal with this problem, we propose a sentence rephrasing approach to replace each OOV word in a sentence with a popular word of the same named entity type in the training set, so that the knowledge of the word forms can be used for parsing. The highest-frequency-based rephrasing strategy and the information-retrieval-based rephrasing strategy are explored to select the word to replace, and the Chinese Treebank 6.0 (CTB6) corpus is adopted to evaluate the feasibility of the proposed sentence rephrasing strategies. Experimental results show that rephrasing some specific types of OOV words such as Corporation, Organization, and Competition increases the parsing performances. This methodology can be applied to domain adaptation to deal with OOV problems."
E14-4003,{C}hinese Open Relation Extraction for Knowledge Acquisition,2014,15,19,6,1,2391,yuenhsien tseng,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"This study presents the Chinese Open Relation Extraction (CORE) system that is able to extract entity-relation triples from Chinese free texts based on a series of NLP techniques, i.e., word segmentation, POS tagging, syntactic parsing, and extraction rules. We employ the proposed CORE techniques to extract more than 13 million entity-relations for an open domain question answering application. To our best knowledge, CORE is the first Chinese Open IE system for knowledge acquisition."
D14-1156,Leveraging Effective Query Modeling Techniques for Speech Recognition and Summarization,2014,47,7,7,1,2312,kuanyu chen,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Statistical language modeling (LM) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area. In particular, language modeling for information retrieval (IR) has enjoyed remarkable empirical success; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness. This paper presents a continuation of such a general line of research and the main contribution is threefold. First, we propose a principled framework which can unify the relationships among several widely-used query modeling formulations. Second, on top of the successfully developed framework, we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation. Third, we further adopt and formalize such a framework to the speech recognition and summarization tasks. A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks."
C14-2015,A Sentence Judgment System for Grammatical Error Detection,2014,7,11,6,1,1205,lunghao lee,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"This study develops a sentence judgment system using both rule-based and n-gram statistical methods to detect grammatical errors in Chinese sentences. The rule-based method provides 142 rules developed by linguistic experts to identify potential rule violations in input sentences. The n-gram statistical method relies on the n-gram scores of both correct and incorrect training sentences to determine the correctness of the input sentences, providing learners with improved understanding of linguistic rules and n-gram frequencies."
C14-1028,{C}hinese Word Ordering Errors Detection and Correction for Non-Native {C}hinese Language Learners,2014,17,11,3,0,40227,shukman cheng,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Word Ordering Errors (WOEs) are the most frequent type of grammatical errors at sentence level for non-native Chinese language learners. Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. That makes WOEs detection and correction more challenging. In this paper, we propose methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs) based WOEs detection models identify the sentence segments containing WOEs. Segment point-wise mutual information (PMI), inter-segment PMI difference, language model, tag of the previous segment, and CRF bigram template are explored. Words in the segments containing WOEs are reordered to generate candidates that may have correct word orderings. Ranking SVM based models rank the candidates and suggests the most proper corrections. Training and testing sets are selected from HSK dynamic composition corpus created by Beijing Language and Culture University. Besides the HSK WOE dataset, Google Chinese Web 5gram corpus is used to learn features for WOEs detection and correction. The best model achieves an accuracy of 0.834 for detecting WOEs in sentence segments. On the average, the correct word orderings are ranked 4.8 among 184.48 candidates."
C14-1060,Interpretation of {C}hinese Discourse Connectives for Explicit Discourse Relation Recognition,2014,28,3,4,1,8627,henhsen huang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper addresses the specific features of Chinese discourse connectives, including types (word-pair and single-word), linking directions (forward and backward linking), positions and ambiguous degrees, and discusses how they affect the discourse relation recognition. A semisupervised learning method is proposed to learn the probability distributions of discourse functions of connectives from a small labeled dataset and a big unlabeled dataset. The statistics learned from the dataset demonstrates some interesting linguistic phenomena such as connective synonyms sharing similar distributions, multiple discourse functions of connectives, and couple-linking elements providing strong clues for discourse relation resolution."
C14-1120,{C}hinese Irony Corpus Construction and Ironic Structure Analysis,2014,17,10,2,1,39110,yijie tang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Non-literal expression recognition is a challenging task in natural language processing. An ironic expression implies the opposite of the literal meaning, causing problems in opinion mining and sentiment analysis. In this paper, ironic messages are collected from microblogs to form an irony corpus based on the use of emoticons, linguistic forms, and sentiment polarity. Five linguistic patterns are mined by using the proposed bootstrapping approach. We also analyze the linguistic structure and elements used to convey irony. Based on our observations, ironic words/phrases and contextual information are the necessary elements in irony, while the contextual information can be hidden in linguistic forms. A rhetorical element, which is optional in irony, can also be used to help strengthen the effects and understandability of an ironic expression. The ironic elements in each instance of our irony corpus are labelled based on this structure. This corpus can be used to study the usage of ironic expressions and the identification of ironic elements, and thus improve the performance of irony recognition."
W13-4414,A Study of Language Modeling for {C}hinese Spelling Check,2013,25,10,5,1,2312,kuanyu chen,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"Chinese spelling check (CSC) is still an open problem today. To the best of our knowledge, language modeling is widely used in CSC because of its simplicity and fair predictive power, but most systems only use the conventional n-gram models. Our work in this paper continues this general line of research by further exploring different ways to glean extra semantic clues and Web resources to enhance the CSC performance in an unsupervised fashion. Empirical results demonstrate the utility of our CSC system."
W13-2817,Uses of Monolingual In-Domain Corpora for Cross-Domain Adaptation with Hybrid {MT} Approaches,2013,13,3,3,0,40904,anchang hsieh,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"Resource limitation is challenging for crossdomain adaption. This paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results, and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline MT system. The adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant."
W13-2309,Analyses of the Association between Discourse Relation and Sentiment Polarity with a {C}hinese Human-Annotated Corpus,2013,17,5,5,1,8627,henhsen huang,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks."
P13-2079,Modeling Human Inference Process for Textual Entailment Recognition,2013,15,5,3,1,8627,henhsen huang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"To prepare an evaluation dataset for textual entailment (TE) recognition, human annotators label rich linguistic phenomena on text and hypothesis expressions. These phenomena illustrate implicit human inference process to determine the relations of given text-hypothesis pairs. This paper aims at understanding what human think in TE recognition process and modeling their thinking process to deal with this problem. At first, we analyze a labelled RTE-5 test set which has been annotated with 39 linguistic phenomena of 5 aspects by Mark Sammons et al., and find that the negative entailment phenomena are very effective features for TE recognition. Then, a rule-based method and a machine learning method are proposed to extract this kind of phenomena from text-hypothesis pairs automatically. Though the systems with the machine-extracted knowledge cannot be comparable to the systems with human-labelled knowledge, they provide a new direction to think TE problems. We further annotate the negative entailment phenomena on Chinese text-hypothesis pairs in NTCIR-9 RITE-1 task, and conclude the same findings as that on the English RTE-5 datasets."
W12-1636,Contingency and Comparison Relation Labeling and Structure Prediction in {C}hinese Sentences,2012,10,10,2,1,8627,henhsen huang,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Unlike in English, the sentence boundaries in Chinese are fuzzy and not well-defined. As a result, Chinese sentences tend to be long and consist of complex discourse relations. In this paper, we focus on two important relations, Contingency and Comparison, which occur often inside a sentence. We construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure. A learning based model is evaluated with various features. Experimental results show our model achieves accuracies of 81.63% in the task of relation labeling and 74.8% in the task of relation structure prediction."
O12-5003,"é åç¸éè©å½æ¥µæ§åæåæä»¶æ\
ç·åé¡ä¹ç ç©¶ (Domain Dependent Word Polarity Analysis for Sentiment Classification) [In {C}hinese]",2012,4,3,3,0,42726,hocheng yu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 17, Number 4, {D}ecember 2012-Special Issue on Selected Papers from {ROCLING} {XXIV}",0,"The researches of sentiment analysis aim at exploring the emotional state of writers. The analysis highly depends on the application domains. Analyzing sentiments of the articles in different domains may have different results. In this study, we focus on corpora from three different domains in Traditional and Simplified Chinese, then examine the polarity degrees of vocabularies in these three domains, and propose methods to capture sentiment differences. Finally, we apply the results to sentiment classification with supervised SVM learning. The experiments show that the proposed methods can effectively improve the sentiment classification performance. xc2xac2B3/xc2xb1 xe2x80xa6`B[ xe2x80x93 ` xcexbc xc2xb6 Keywords: Document Sentiment Classification, Word Polarity Analysis, Machine Learning xc2xb7 1oxe2x80x9dxc2xbb#B[TUB[xe2x80xb0xc5xa0`,fHTU()/xc2xb1j xe2x80xa6xc2xa2xc2xa3*1xe2x81x8441xe2x81x842j3xe2x81x844? 1?AA>xc3x82xe2x80x9cxe2x80x9dxc2xbb# TFSO A TFIDF j xc2xa3AA *l1xc5xbd IDF ; SO XAE*C E1 TFSOIDF*? xc2xa3EE*TFSOIDFES?IIH>Ixc2xa2xc2xa7 PIC*xc5xbd xc2xa3Eg#xc3x90xe2x80xa1N> @Oh TFSSIDF ES TFSOIDF*TFSDIDF ES TFIDF>Oxc2xa2* Unigramjxc2xa2xc2xa3A TFSSIDFDOO*TFSOIDF; TFSDIDFxc5x93*AO1 TFIDF*;? IIJ>(xc3x973TF: B[OA*IDF: U:/xc2xb1OA*SO: PaU xc2xa9*SD: BC) xc5xa1xe2x80xba TFIDF TFRF Delta TFSO TFSOIDF TFSDIDF TFSSIDF ]^_ 0.848 0.849 0.853 0.847 0.854 0.852 0.863 ab 0.916 0.906 0.914 0.915 0.924 0.918 0.923 cd 0.861 0.839 0.849 0.854 0.871 0.869 0.875 [1] Bo Pang and Lillian Lee, xe2x80x9cOpinion Mining and Sentiment Analysis,xe2x80x9d Foundations and Trends in Information Retrieval, vol. 2, issue 1-2, pp. 1-135, 2008. [2] Lun-Wei Ku and Hsin-Hsi Chen, xe2x80x9cMining Opinions from the Web: Beyond Relevance Retrieval,xe2x80x9d Journal of American Society for Information Science and Technology, vol. 58, no. 12, pp. 1838-1850, 2007. [3] Man Lan, Sam-Yuan Sung, Hwee-Boon Low, and Chew-Lim Tan, xe2x80x9dA Comparative Study on Term Weighting Schemes for Text Categorization,xe2x80x9d In Proceedings of 2005 IEEE International Joint Conference on Neural Networks, pp. 546-551, 2005. [4] Justin Martineau and Tim Finin, xe2x80x9cDelta TFIDF: An Improved Feature Space for Sentiment Analysis,xe2x80x9d In Proceedings of the Third AAAI International Conference on Weblogs and Social Media, pp. 258-261, 2009. Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)"
O12-3002,å»£ç¾©ç¥ç¶²è©å½æè¦æ¥µæ§çé æ¸¬ (Predicting the Semantic Orientation of Terms in {E}-{H}ow{N}et) [In {C}hinese],2012,0,3,3,0,42738,chengru li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 17, Number 2, June 2012-Specia Issue on Selected Papers from {ROCLING} {XXIII}",0,None
O12-1004,"é åç¸éè©å½æ¥µæ§åæåæä»¶æ\
ç·åé¡ä¹ç ç©¶ (Domain Dependent Word Polarity Analysis for Sentiment Classification) [In {C}hinese]",2012,4,3,3,0,42726,hocheng yu,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,"The researches of sentiment analysis aim at exploring the emotional state of writers. The analysis highly depends on the application domains. Analyzing sentiments of the articles in different domains may have different results. In this study, we focus on corpora from three different domains in Traditional and Simplified Chinese, then examine the polarity degrees of vocabularies in these three domains, and propose methods to capture sentiment differences. Finally, we apply the results to sentiment classification with supervised SVM learning. The experiments show that the proposed methods can effectively improve the sentiment classification performance. xc2xac2B3/xc2xb1 xe2x80xa6`B[ xe2x80x93 ` xcexbc xc2xb6 Keywords: Document Sentiment Classification, Word Polarity Analysis, Machine Learning xc2xb7 1oxe2x80x9dxc2xbb#B[TUB[xe2x80xb0xc5xa0`,fHTU()/xc2xb1j xe2x80xa6xc2xa2xc2xa3*1xe2x81x8441xe2x81x842j3xe2x81x844? 1?AA>xc3x82xe2x80x9cxe2x80x9dxc2xbb# TFSO A TFIDF j xc2xa3AA *l1xc5xbd IDF ; SO XAE*C E1 TFSOIDF*? xc2xa3EE*TFSOIDFES?IIH>Ixc2xa2xc2xa7 PIC*xc5xbd xc2xa3Eg#xc3x90xe2x80xa1N> @Oh TFSSIDF ES TFSOIDF*TFSDIDF ES TFIDF>Oxc2xa2* Unigramjxc2xa2xc2xa3A TFSSIDFDOO*TFSOIDF; TFSDIDFxc5x93*AO1 TFIDF*;? IIJ>(xc3x973TF: B[OA*IDF: U:/xc2xb1OA*SO: PaU xc2xa9*SD: BC) xc5xa1xe2x80xba TFIDF TFRF Delta TFSO TFSOIDF TFSDIDF TFSSIDF ]^_ 0.848 0.849 0.853 0.847 0.854 0.852 0.863 ab 0.916 0.906 0.914 0.915 0.924 0.918 0.923 cd 0.861 0.839 0.849 0.854 0.871 0.869 0.875 [1] Bo Pang and Lillian Lee, xe2x80x9cOpinion Mining and Sentiment Analysis,xe2x80x9d Foundations and Trends in Information Retrieval, vol. 2, issue 1-2, pp. 1-135, 2008. [2] Lun-Wei Ku and Hsin-Hsi Chen, xe2x80x9cMining Opinions from the Web: Beyond Relevance Retrieval,xe2x80x9d Journal of American Society for Information Science and Technology, vol. 58, no. 12, pp. 1838-1850, 2007. [3] Man Lan, Sam-Yuan Sung, Hwee-Boon Low, and Chew-Lim Tan, xe2x80x9dA Comparative Study on Term Weighting Schemes for Text Categorization,xe2x80x9d In Proceedings of 2005 IEEE International Joint Conference on Neural Networks, pp. 546-551, 2005. [4] Justin Martineau and Tim Finin, xe2x80x9cDelta TFIDF: An Improved Feature Space for Sentiment Analysis,xe2x80x9d In Proceedings of the Third AAAI International Conference on Weblogs and Social Media, pp. 258-261, 2009. Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)"
tang-chen-2012-mining,Mining Sentiment Words from Microblogs for Predicting Writer-Reader Emotion Transition,2012,6,15,2,1,39110,yijie tang,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The conversations between posters and repliers in microblogs form a valuable writer-reader emotion corpus. This paper adopts a log relative frequency ratio to investigate the linguistic features which affect emotion transitions, and applies the results to predict writers' and readers' emotions. A 4-class emotion transition predictor, a 2-class writer emotion predictor, and a 2-class reader emotion predictor are proposed and compared."
wang-etal-2012-ntusocialrec,{NTUS}ocial{R}ec: An Evaluation Dataset Constructed from Microblogs for Recommendation Applications in Social Networks,2012,8,0,4,1,37581,chiehjen wang,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper proposes a method to construct an evaluation dataset from microblogs for the development of recommendation systems. We extract the relationships among three main entities in a recommendation event, i.e., who recommends what to whom. User-to-user friend relationships and user-to-resource interesting relationships in social media and resource-to-metadata descriptions in an external ontology are employed. In the experiments, the resources are restricted to visual entertainment media, movies in particular. A sequence of ground truths varying with time is generated. That reflects the dynamic of real world."
yu-etal-2012-development,Development of a Web-Scale {C}hinese Word N-gram Corpus with Parts of Speech Information,2012,6,8,3,1,40228,chihsin yu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Web provides a large-scale corpus for researchers to study the language usages in real world. Developing a web-scale corpus needs not only a lot of computation resources, but also great efforts to handle the large variations in the web texts, such as character encoding in processing Chinese web texts. In this paper, we aim to develop a web-scale Chinese word N-gram corpus with parts of speech information called NTU PN-Gram corpus using the ClueWeb09 dataset. We focus on the character encoding and some Chinese-specific issues. The statistics about the dataset is reported. We will make the resulting corpus a public available resource to boost the Chinese language processing."
C12-3028,An Annotation System for Development of {C}hinese Discourse Corpus,2012,7,6,2,1,8627,henhsen huang,Proceedings of {COLING} 2012: Demonstration Papers,0,"Well-annotated discourse corpora facilitate the discourse researches. Unlike English, the Chinese discourse corpus is not widely available yet. In this paper, we present a webbased annotation system to develop a Chinese discourse corpus with much finer annotation. We first review our previous corpora from the practical point of view, then propose a flexible annotation framework, and finally demonstrate the web-based annotation system. Under the proposed annotation scheme, both the explicit and the implicit discourse relations occurring on various linguistic levels will be captured and labelled with three-level PDTB tags. Besides, the sentiment information of each instance is also annotated for advanced study."
C12-3029,Modeling {P}ollyanna Phenomena in {C}hinese Sentiment Analysis,2012,18,2,3,1,720,tinghao huang,Proceedings of {COLING} 2012: Demonstration Papers,0,"This paper proposes a method to enhance sentiment classification by utilizing the Pollyanna phenomena. The Pollyanna phenomena describe the human tendency to use positive words more frequently than negative words. This word-level linguistic bias can be demonstrated to be strong and universal in many languages. We perform detailed analyses of the Pollyanna phenomena in four Chinese corpora. Quantitative analyses show that for documents with few positive words, the word usages in documents from either the positive or the negative polarities become similar. Qualitative analyses indicate t hat this increase of similarity of word usage could be caused by the concentration of topics. By taking advantage of these results, we propose a partitioning strategy for sentiment classification and significantly improve the F1-score."
C12-3063,{C}hinese Web Scale Linguistic Datasets and Toolkit,2012,10,0,2,1,40228,chihsin yu,Proceedings of {COLING} 2012: Demonstration Papers,0,"The web provides a huge collection of web pages for researchers to study natural languages. However, processing web scale texts is not an easy task and needs many computational and linguistic resources. In this paper, we introduce two Chinese parts-of-speech tagged web-scale datasets and describe tools that make them easy to use for NLP applications. The first is a Chinese segmented and POS-tagged dataset, in which the materials are selected from the ClueWeb09 dataset. The second is a Chinese POS n-gram corpus extracted from the POS-tagged dataset. Tools to access the POS-tagged dataset and the POS n-gram corpus are presented. The two datasets will be released to the public along with their tools."
C12-2119,Advertising Legality Recognition,2012,9,1,3,1,39110,yijie tang,Proceedings of {COLING} 2012: Posters,0,"As online marketing and advertising keep growing on the Internet, a large amount of advertisements are presented to consumers. How consumers, advertisers and the authorities identify false and overstated advertisements becomes a critical issue. In this paper, we address this problem, and propose various classification models to detect illegal advertisements. Illegal advertisement lists announced by the government and legal advertising data crawled from an online shopping website are used for training and testing the classification models. Naive Bayes and SVM classifiers with various feature settings are explored on food and cosmetic datasets to demonstrate their feasibility. The experimental results show that log relative frequency ratio can be used as weights for unigram features to achieve the best accuracy. The accuracies of SVM classifiers on food and cosmetic datasets are 93.433% and 86.037%; the false alarm rates are 0.083 and 0.166; and the missing rates are 0.053 and 0.115, respectively. Log relative frequency ratio is further used to mine verb phrases consisting of a transitive verb and an object noun from the illegal datasets. The mined verb phrases, which form an illegal advertising statement list, can be used as a reference for both the advertisers and the authority."
C12-1034,A Simplification-Translation-Restoration Framework for Cross-Domain {SMT} Applications,2012,28,11,3,1,43741,hanbin chen,Proceedings of {COLING} 2012,0,"Integration of domain specific knowledge into a general purpose statistical machine translation (SMT) system poses challenges due to insufficient bilingual corpora. In this paper we propose a simplification-translation-restoration (STR) framework for domain adaptation in SMT by simplifying domain specific segments of a text. For an in-domain text, we identify the critical segments and modify them to alleviate the data sparseness problem in the out-domain SMT system. After we receive the translation result, these critical segments are then restored according to the provided in-domain knowledge. We conduct experiments on an English-toChinese translation task in the medical domain and evaluate each step of the STR framework. The translation results show significant improvement of our approach over the out-domain and the naive in-domain SMT systems."
C12-1184,Detecting Word Ordering Errors in {C}hinese Sentences for Learning {C}hinese as a Foreign Language,2012,15,16,2,1,40228,chihsin yu,Proceedings of {COLING} 2012,0,"Automatic detection of sentence errors is an important NLP task and is valuable to assist foreign language learners. In this paper, we investigate the problem of word ordering errors in Chinese sentences and propose classifiers to detect this type of errors. Word n-gram features in Google Chinese Web 5-gram corpus and ClueWeb09 corpus, and POS features in the Chinese POStagged ClueWeb09 corpus are adopted in the classifiers. The experimental results show that integrating syntactic features, web corpus features and perturbation features are useful for word ordering error detection, and the proposed classifier achieves 71.64% accuracy in the experimental datasets."
W11-3703,Emotion Modeling from Writer/Reader Perspectives Using a Microblog Dataset,2011,15,8,2,1,39110,yijie tang,Proceedings of the Workshop on Sentiment Analysis where {AI} meets Psychology ({SAAIP} 2011),0,"Most recent studies on emotion analysis and detection focus on how writers express their emotions through textual information. In this paper, we model emotion generation on the Plurk microblogging platform from both writer and reader perspectives. Support Vector Machine (SVM)-based classifiers are used for emotion prediction. To better model emotion generation on such a social network, three types of non-linguistic features are used: social relation, user behavior, and relevance degree, along with textual features. We found that each of the non-linguistic features can be combined with linguistic features to achieve higher performance. In fact, the combination of linguistic, social, and behavioral features performs the best."
R11-1021,Pause and Stop Labeling for {C}hinese Sentence Boundary Detection,2011,14,3,2,1,8627,henhsen huang,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"The fuzziness of Chinese sentence boundary makes discourse analysis more challenging. Moreover, many articles posted on the Internet are even lack of punctuation marks. In this paper, we collect documents written by masters as a reference corpus and propose a model to label the punctuation marks for the given text. Conditional random field (CRF) models trained with the corpus determine the correct delimiter (a comma or a full-stop) between each pair of successive clauses. Different tagging schemes and various features from different linguistic levels are explored. The results show that our segmenter achieves an accuracy of 77.48% for plain text, which is close to the human performance 81.18%. For the rich formatted text, our segmenter achieves an even better accuracy of 82.93%."
O11-4004,Intent Shift Detection Using Search Query Logs,2011,16,1,2,1,37581,chiehjen wang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 16, Number 3-4, September/{D}ecember 2011",0,"Detecting intent shift is fundamental for learning users' behaviors and applying their experiences. In this paper, we propose a search-query-log based system to predict users' intent shifts. We begin with selecting sessions in search query logs for training, extracting features from the selected sessions, and clustering sessions of similar intent. The resulting intent clusters are used to predict intent shift in testing data. The experimental results show that the proposed model achieves an accuracy of 0.5099, which is significantly better than the baselines. Moreover, the miss rate and spurious rate of the model are 0.0954 and 0.0867, respectively."
O11-1010,å»£ç¾©ç¥ç¶²è©å½æè¦æ¥µæ§çé æ¸¬ (Predicting the Semantic Orientation of Terms in {E}-{H}ow{N}et) [In {C}hinese],2011,0,0,3,0,42738,chengru li,Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2011),0,None
I11-1039,Predicting Opinion Dependency Relations for Opinion Analysis,2011,17,5,3,1,4125,lunwei ku,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Syntactic structures have been good features for opinion analysis, but it is not easy to use them. To find these features by supervised learning methods, correct syntactic labels are indispensible. Two possible sources to acquire syntactic structures are parsing trees and dependency trees. For the annotation processing, parsing trees are more readable for annotators, while dependency trees are easier to use by programs. To use syntactic structures as features, this paper tried to annotate on human friendly materials and transform these annotations to the corresponding machine friendly materials. We annotated the gold answers of opinion syntactic structures on the parsing tree from Chinese Treebank, and then proposed methods to find their corresponding dependency relations on the dependency trees generated from the same sentence. With these relations, we could train a model to annotate opinion dependency relations automatically to provide an opinion dependency parser, which is language independent if language resources are incorporated. Experiment results show that the annotated syntactic structures and their corresponding dependency relations improve at least 8% of the performance of opinion analysis."
I11-1170,{C}hinese Discourse Relation Recognition,2011,6,23,2,1,8627,henhsen huang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"The challenging issues of discourse relation recognition in Chinese are addressed. Due to the lack of Chinese discourse corpora, we construct a moderate corpus with humanannotated discourse relations. Based on the corpus, a statistical classifier is proposed, and various features are explored in the experiments. The experimental results show that our method achieves an accuracy of 88.28% and an F-Score of 63.69% in four-class classification and achieves an F-Score of 93.57% in the best case."
2011.mtsummit-papers.31,Identification and Translation of Significant Patterns for Cross-Domain {SMT} Applications,2011,-1,-1,5,1,43741,hanbin chen,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-4103,Classical {C}hinese Sentence Segmentation,2010,9,4,3,1,8627,henhsen huang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
kao-chen-2010-comment,Comment Extraction from Blog Posts and Its Applications to Opinion Mining,2010,5,4,2,0,45830,huanan kao,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Blog posts containing many personal experiences or perspectives toward specific subjects are useful. Blogs allow readers to interact with bloggers by placing comments on specific blog posts. The comments carry viewpoints of readers toward the targets described in the post, or supportive/non-supportive attitude toward the post. Comment extraction is challenging due to that there does not exist a unique template among all blog service providers. This paper proposes methods to deal with this problem. Firstly, the repetitive patterns and their corresponding blocks are extracted from input posts by pattern identification algorithm. Secondly, three filtering strategies, i.e., tag pattern loop filtering, rule overlap filtering, and longest rule first, are used to remove non-comment blocks. Finally, a comment/non-comment classifier is learned to distinguish comment blocks from non-comment blocks with 14 block-level features and 5 rule-level features. In the experiments, we randomly select 600 blog posts from 12 blog service providers. F-measure, recall, and precision are 0.801, 0.855, and 0.780, respectively, by using all of the three filtering strategies together with some selected features. The application of comment extraction to blog mining is also illustrated. We show how to identify the relevant opinionated objects â say, opinion holders, opinions, and targets, from posts."
ku-etal-2010-construction,Construction of a {C}hinese Opinion Treebank,2010,7,1,3,1,4125,lunwei ku,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we base on the syntactic structural Chinese Treebank corpus, construct the Chinese Opinon Treebank for the research of opinion analysis. We introduce the tagging scheme and develop a tagging tool for constructing this corpus. Annotated samples are described. Information including opinions (yes or no), their polarities (positive, neutral or negative), types (expression, status, or action), is defined and annotated. In addition, five structure trios are introduced according to the linguistic relations between two Chinese words. Four of them that are possibly related to opinions are also annotated in the constructed corpus to provide the linguistic cues. The number of opinion sentences together with the number of their polarities, opinion types, and trio types are calculated. These statistics are compared and discussed. To know the quality of the annotations in this corpus, the kappa values of the annotations are calculated. The substantial agreement between annotations ensures the applicability and reliability of the constructed corpus."
huang-etal-2010-predicting,Predicting Morphological Types of {C}hinese Bi-Character Words by Machine Learning Approaches,2010,5,1,3,1,720,tinghao huang,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presented an overview of Chinese bi-character wordsÂ morphological types, and proposed a set of features for machine learning approaches to predict these types based on composite charactersÂ information. First, eight morphological types were defined, and 6,500 Chinese bi-character words were annotated with these types. After pre-processing, 6,178 words were selected to construct a corpus named Reduced Set. We analyzed Reduced Set and conducted the inter-annotator agreement test. The average kappa value of 0.67 indicates a substantial agreement. Second, Bi-character wordsÂ morphological types are considered strongly related with the composite charactersÂ parts of speech in this paper, so we proposed a set of features which can simply be extracted from dictionaries to indicate the charactersÂ ÂtendencyÂ of parts of speech. Finally, we used these features and adopted three machine learning algorithms, SVM, CRF, and Na{\""\i}ve Bayes, to predict the morphological types. On the average, the best algorithm CRF achieved 75{\%} of the annotatorsÂ performance."
O09-6003,Identification of Opinion Holders,2009,18,4,3,1,4125,lunwei ku,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 14, Number 4, {D}ecember 2009",0,"Opinion holder identification aims to extract entities that express opinions in sentences. In this paper, opinion holder identification is divided into two subtasks: author's opinion recognition and opinion holder labeling. Support vector machine (SVM) is adopted to recognize author's opinions, and conditional random field algorithm (CRF) is utilized to label opinion holders. New features are proposed for both methods. Our method achieves an f-score of 0.734 in the NTCIR7 MOAT task on the Traditional Chinese side, which is the best performance among results of machine learning methods proposed by participants, and also it is close to the best performance of this task. In addition, inconsistent annotations of opinion holders are analyzed, along with the best way to utilize the training instances with inconsistent annotations being proposed."
O09-1008,"æè¦ææè\
è¾¨è­ä¹ç ç©¶ (A Study on Identification of Opinion Holders) [In {C}hinese]",2009,0,0,3,0,37390,chiaying lee,Proceedings of the 21st Conference on Computational Linguistics and Speech Processing,0,None
D09-1131,Using Morphological and Syntactic Structures for {C}hinese Opinion Analysis,2009,19,48,3,1,4125,lunwei ku,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper employs morphological structures and relations between sentence segments for opinion analysis on words and sentences. Chinese words are classified into eight morphological types by two proposed classifiers, CRF classifier and SVM classifier. Experiments show that the injection of morphological information improves the performance of the word polarity detection. To utilize syntactic structures, we annotate structural trios to represent relations between sentence segments. Experiments show that considering structural trios is useful for sentence opinion analysis. The best f-score achieves 0.77 for opinion word extraction, 0.62 for opinion word polarity detection, 0.80 for opinion sentence extraction, and 0.54 for opinion sentence polarity detection."
O08-5003,Question Analysis and Answer Passage Retrieval for Opinion Question Answering Systems,2008,16,9,3,1,4125,lunwei ku,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 3, September 2008: Special Issue on Selected Papers from {ROCLING} {XIX}",0,"Question answering systems provide an elegant way for people to access an underlying knowledge base. However, people are interested in not only factual questions, but also opinions. This paper deals with question analysis and answer passage retrieval in opinion QA systems. For question analysis, six opinion question types are defined. A two-layered framework utilizing two question type classifiers is proposed. Algorithms for these two classifiers are described. The performance achieves 87.8% in general question classification and 92.5% in opinion question classification. The question focus is detected to form a query for the information retrieval system and the question polarity is detected to retain relevant sentences which have the same polarity as the question. For answer passage retrieval, three components are introduced. Relevant sentences retrieved are further identified as to whether the focus (Focus Detection) is in a scope of opinion (Opinion Scope Identification) or not, and, if yes, whether the polarity of the scope and the polarity of the question (Polarity Detection) match with each other. The best model achieves an F-measure of 40.59% by adopting partial match for relevance detection at the level of meaningful unit. With relevance issues removed, the F-measure of the best model boosts up to 84.96%."
teng-chen-2008-event,Event Detection and Summarization in Weblogs with Temporal Collocations,2008,10,0,2,0,47979,chunyuan teng,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time."
I08-2083,Analysis of Intention in Dialogues Using Category Trees and Its Application to Advertisement Recommendation,2008,6,5,2,0,48615,hungchi huang,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"We propose an intention analysis system for instant messaging applications. The system adopts Yahoo! directory as category trees, and classifies each dialogue into one of the categories of the directory. Two weighting schemes in information retrieval, i.e., tf and tf-idf, are considered in our experiments. In addition, we also expand Yahoo! directory with the accompanying HTML files and explore different features such as nouns, verbs, hypernym, hyponym, etc. Experiments show that category trees expanded with snippets together with noun features under tf scheme achieves a best Fscore, 0.86, when only 37.46% of utterances are processed on the average. This methodology is employed to recommend advertisements relevant to the dialogue."
D08-1015,Ranking Reader Emotions Using Pairwise Loss Minimization and Emotional Distribution Regression,2008,16,31,2,0.882353,4356,kevin lin,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents two approaches to ranking reader emotions of documents. Past studies assign a document to a single emotion category, so their methods cannot be applied directly to the emotion ranking problem. Furthermore, whereas previous research analyzes emotions from the writer's perspective, this work examines readers' emotional states. The first approach proposed in this paper minimizes pairwise ranking errors. In the second approach, regression is used to model emotional distributions. Experiment results show that the regression method is more effective at identifying the most popular emotion, but the pairwise loss minimization method produces ranked lists of emotions that have better correlations with the correct lists."
P07-2023,Test Collection Selection and Gold Standard Generation for a Multiply-Annotated Opinion Corpus,2007,3,15,3,1,4125,lunwei ku,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"Opinion analysis is an important research topic in recent years. However, there are no common methods to create evaluation corpora. This paper introduces a method for developing opinion corpora involving multiple annotators. The characteristics of the created corpus are discussed, and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed. Under the gold standards, an opinion extraction system is evaluated. The experiment results show some interesting phenomena."
P07-2034,Building Emotion Lexicon from Weblog Corpora,2007,7,101,3,1,49164,changhua yang,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,An emotion lexicon is an indispensable resource for emotion analysis. This paper aims to mine the relationships between words and emotions using weblog corpora. A collocation model is proposed to learn emotion lexicons from weblog articles. Emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness.
O07-1013,Question Analysis and Answer Passage Retrieval for Opinion Question Answering Systems,2007,0,10,3,1,4125,lunwei ku,Proceedings of the 19th Conference on Computational Linguistics and Speech Processing,0,None
O07-1015,"ä»¥é¨è½æ ¼èªæé²è¡æ\
ç·è¶¨å¢åæ (Emotion Trend Analysis Using Blog Corpora) [In {C}hinese]",2007,0,0,3,1,49164,changhua yang,Proceedings of the 19th Conference on Computational Linguistics and Speech Processing,0,None
P06-2011,A High-Accurate {C}hinese-{E}nglish {NE} Backward Translation System Combining Both Lexical Information and Web Statistics,2006,10,9,2,1,49906,conrad chen,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Named entity translation is indispensable in cross language information retrieval nowadays. We propose an approach of combining lexical information, web statistics, and inverse search based on Google to backward translate a Chinese named entity (NE) into English. Our system achieves a high Top-1 accuracy of 87.6%, which is a relatively good performance reported in this area until present."
P06-1127,Novel Association Measures Using Web Search with Double Checking,2006,11,140,1,1,8628,hsinhsi chen,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"A web search with double checking model is proposed to explore the web as a live corpus. Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co-Occurrence Double Check (CODC), are presented. In the experiments on Rubenstein-Goodenough's benchmark data set, the CODC measure achieves correlation coefficient 0.8492, which competes with the performance (0.8914) of the model using WordNet. The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable. Further study on named entity clustering shows that the five measures are quite useful. In particular, CODC measure is very stable on word-word and name-name experiments. The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65% and 14.22% increase compared to the system without community expansion. All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web."
O06-4001,An Approach to Using the Web as a Live Corpus for Spoken Transliteration Name Access,2006,66,2,3,1,48616,mingshun lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 3, September 2006: Special Issue on Selected Papers from {ROCLING} {XVII}",0,"Recognizing transliteration names is challenging due to their flexible formulation and lexical coverage. In our approach, we employ the Web as a giant corpus. The patterns extracted from the Web are used as a live dictionary to correct speech recognition errors. The plausible character strings recognized by an Automated Speech Recognition (ASR) system are regarded as query terms and submitted to Google. The top N snippets are entered into PAT trees. The terms of the highest scores are selected. Our experiments show that the ASR model with a recovery mechanism can achieve 21.54% performance improvement compared with the ASR only model on the character level. The recall rate is improved from 0.20 to 0.42, and the MRR from 0.07 to 0.31. For collecting transliteration names, we propose a named entity (NE) ontology generation engine, called the XNE-Tree engine, which produces relational named entities by a given seed. The engine incrementally extracts high co-occurring named entities with the seed. A total of 7,642 named entities in the ontology were initiated by 100 seeds. When the bi-character language model is combined with the NE ontology, the ASR recall rate and MRR are improved to 0.48 and 0.38, respectively."
O06-1017,"ä»¥é¨è½æ ¼ææ¬é²è¡æ\
ç·åé¡ä¹ç ç©¶ (A Study of Emotion Classification Using Blog Articles) [In {C}hinese]",2006,-1,-1,2,1,49164,changhua yang,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
lin-chen-2006-constructing,Constructing a Named Entity Ontology from Web Corpora,2006,12,3,2,1,48616,mingshun lin,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper proposes a named entity (NE) ontology generation engine, called XNE-Tree engine, which produces relational named entities by given a seed. The engine incrementally extracts high co-occurring named entities with the seed by using a common search engine. In each iterative step, the seed will be replaced by its siblings or descendants, which form new seeds. In this way, XNE-Tree engine will build a tree structure with the original seed as a root incrementally. Two seeds, Chinese transliteration names of Nicole Kidman (a famous actress) and Ernest Hemingway (a famous writer), are experimented to evaluate the performance of the XNE-Tree.!`@!`@For test the applicability of the ontology, we employ it to a phoneme-character conversion system, which convert input phoneme syllable sequences to text strings. Total 100 Chinese transliteration names, including 50 person names and 50 location names are used as test data. We derive an ontology composed of 7,642 named entities. The results of phoneme-character conversion show that both the recall rate and the MRR are improved from 0.79 and 0.50 to 0.84 to 0.55, respectively."
ku-etal-2006-tagging,Tagging Heterogeneous Evaluation Corpora for Opinionated Tasks,2006,12,20,3,1,4125,lunwei ku,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Opinion retrieval aims to tell if a document is positive, neutral or negative on a given topic. Opinion extraction further identifies the supportive and the non-supportive evidence of a document. To evaluate the performance of technologies for opinionated tasks, a suitable corpus is necessary. This paper defines the annotations for opinionated materials. Heterogeneous experimental materials are annotated, and the agreements among annotators are analyzed. How human can monitor opinions of the whole is also examined. The corpus can be employed to opinion extraction, opinion summarization, opinion tracking and opinionated question answering."
E06-2019,Classifying Biological Full-Text Articles for Multi-Database Curation,2006,12,2,3,1,41674,wenjuan hou,Demonstrations,0,"In this paper, we propose an approach for identifying curatable articles from a large document set. This system considers three parts of an article (title and abstract, MeSH terms, and captions) as its three individual representations and utilizes two domain-specific resources (UMLS and a tumor name list) to reveal the deep knowledge contained in the article. An SVM classifier is trained and cross-validation is employed to find the best combination of representations. The experimental results show overall high performance."
O05-1024,An Approach of Using the Web as a Live Corpus for Spoken Transliteration Name Access,2005,-1,-1,3,1,48616,mingshun lin,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
I05-1073,"Integrating Punctuation Rules and Na{\\\\\i}ve {B}ayesian Model for {C}hinese Creation Title Recognition""",2005,6,1,2,1,49906,conrad chen,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Creation titles, i.e. titles of literary and/or artistic works, comprise over 7% of named entities in Chinese documents. They are the fourth large sort of named entities in Chinese other than personal names, location names, and organization names. However, they are rarely mentioned and studied before. Chinese title recognition is challenging for the following reasons. There are few internal features and nearly no restrictions in the naming style of titles. Their lengths and structures are varied. The worst of all, they are generally composed of common words, so that they look like common fragments of sentences. In this paper, we integrate punctuation rules, lexicon, and naive Bayesian models to recognize creation titles in Chinese documents. This pioneer study shows a precision of 0.510 and a recall of 0.685 being achieved. The promising results can be integrated into Chinese segmentation, used to retrieve relevant information for specific titles, and so on."
W04-1209,Support Vector Machine Approach to Extracting Gene References into Function from Biological Documents,2004,12,1,3,1,50614,chih lee,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"In the biological domain, extracting newly discovered functional features from the massive literature is a major challenging issue. To automatically annotate Gene References into Function (GeneRIF) in a new literature is the main goal of this paper. We tried to find GRIF words in a training corpus, and then applied these informative words to annotate the GeneRIFs in abstracts with several different weighting schemes. The experiments showed that the Classic Dice score is at most 50.18%, when the weighting schemes proposed in the paper (Hou et al., 2003) were adopted. In contrast, after employing Support Vector Machines (SVMs) and the definition of classes proposed by Jelier et al. (2003), the score greatly improved to 56.86% for Classic Dice (CD). Adopting the same features, SVMs demonstrated advantage over the Naive Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51% for CD."
W04-1215,Annotating Multiple Types of Biomedical Entities: A Single Word Classification Approach,2004,18,27,3,1,50614,chih lee,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"Named entity recognition is a fundamental task in biomedical data mining. Multiple - class annotation is more challenging than single - class annotation. In this paper, we took a single word classification approach to dealing with the multiple - class annotation problem using Support Vector Machines (SVMs). Word attributes, results of existing gene/protein name taggers, context, and other information are important features for classification. During training, the size of training data and the distribution of named entities are considered. The preliminary results showed that the approach might be feasible when more training data is used to alleviate the data imbalance problem."
W04-0703,Event Clustering on Streaming News Using Co-Reference Chains and Event Words,2004,11,6,2,0,51663,junejei kuo,Proceedings of the Conference on Reference Resolution and Its Applications,0,None
O04-1010,"èªæåº«çµ±è¨å¼èå\
¨çè³è¨ç¶²çµ±è¨å¼ä¹æ¯è¼ï¼ä»¥ä¸­ææ·è©æç¨çºä¾ (Comparison of Corpus Statistics and Web Statistics: An Application to {C}hinese Word Segmentation) [In {C}hinese]",2004,0,0,2,0,51791,hsiaoching lin,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
O04-1031,ä»¥èªæ³åæçºè¼å»ºç«æ°èåè©ç¥è­åº« (Construction of Knowledge Base for News Names by Applying Syntactic Rules) [In {C}hinese],2004,0,0,2,1,49164,changhua yang,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
chen-etal-2004-collocation,Collocation Extraction Using Web Statistics,2004,4,0,1,1,8628,hsinhsi chen,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper mines collocations from two different web usage corpora, NTU proxy log and TTS search log. The precisions for NTU and TTS test data are 61.76% and 57.50%, respectively, by human judgment for 2% sampling of extracted collocations. For automatic evaluation, we submit extracted collocation to Google search engine, and the resulting page counts are used to compute the mutual information of the collocation. Experimental results show that total 43.27% and 42.65% of collocations mined from NTU and TTS corpora passed the examination of MIs."
chen-chu-2004-pattern,Pattern Discovery in Named Organization Corpus,2004,4,3,1,1,8628,hsinhsi chen,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents how to mine formulation rules from a named organization corpus. The TEIRESIAS algorithm, which is widely used in bioinformatics domain, is adopted. The experimental results based on MET2 test bed show that the approach of regarding the morpheme of a keyword as a cluster is the best, the approach of regarding all the keywords as the same cluster is the next, and the approach of regarding each keyword as a cluster is the worse. The performance using morpheme-based approach is a little better than that of hand-crafted approach. The methodology can be easily extended to other types of named entities."
W03-1501,Learning Formulation and Transformation Rules for Multilingual Named Entities,2003,8,27,1,1,8628,hsinhsi chen,Proceedings of the {ACL} 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition,0,"This paper investigates three multilingual named entity corpora, including named people, named locations and named organizations. Frequency-based approaches with and without dictionary are proposed to extract formulation rules of named entities for individual languages, and transformation rules for mapping among languages. We consider the issues of abbreviation and compound keyword at a distance. Keywords specify not only the types of named entities, but also tell out which parts of a named entity should be meaning-translated and which part should be phoneme-transliterated. An application of the results on cross language information retrieval is also shown."
W03-1304,Enhancing Performance of Protein Name Recognizers Using Collocation,2003,20,10,2,1,41674,wenjuan hou,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"Named entity recognition is a fundamental task in biological relationship mining. This paper employs protein collocates extracted from a biological corpus to enhance the performance of protein name recognizers. Yapex and KeX are taken as examples. The precision of Yapex is increased from 70.90% to 81.94% at the low expense of recall rate (i.e., only decrease 2.39%) when collocates are incorporated. We also integrate the results proposed by Yapex and KeX, and employs collocates to filter the merged results. Because the candidates suggested by these two systems may be inconsistent, i.e., overlap in partial, one of them is considered as a basis. The experiments show that Yapex-based integration is better than KeX-based integration."
O03-1013,"ä»¥ç¶²éç¶²è·¯å\
§å®¹çºåºç¤ä¹åç­ç³»çµ± {``}Why{''} åå¥ç ç©¶ (The Study of Why Questions in Web-based Question-Answering Systems) [In {C}hinese]",2003,0,0,3,0,52861,teanzuo shen,Proceedings of Research on Computational Linguistics Conference {XV},0,None
W02-2017,Backward Machine Transliteration by Learning Phonetic Similarity,2002,20,73,2,0.666667,48044,weihao lin,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"In many cross-lingual applications we need to convert a transliterated word into its original word. In this paper, we present a similarity-based framework to model the task of backward transliteration, and provide a learning algorithm to automatically acquire phonetic similarities from a corpus. The learning algorithm is based on Widrow-Hoff rule with some modifications. The experiment results show that the learning algorithm converges quickly, and the method using acquired phonetic similarities remarkably outperforms previous methods using pre-defined phonetic similarities or graphic similarities in a corpus of 1574 pairs of English names and transliterated Chinese names. The learning algorithm does not assume any underlying phonological structures or rules, and can be extended to other language pairs once a training corpus and a pronouncing dictionary are available."
C02-1006,{NLP} and {IR} Approaches to Monolingual and Multilingual Link Detection,2002,14,14,2,0,53605,yingju chen,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper considers several important issues for monolingual and multilingual link detection. The experimental results show that nouns, verbs, adjectives and compound nouns are useful to represent news stories; story expansion is helpful; topic segmentation has a little effect; and a translation model is needed to capture the differences between languages."
O01-3002,A Simple Method for {C}hinese Video {OCR} and Its Application to Question Answering,2001,11,11,3,1,14297,chuanjie lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 6, Number 2, August 2001",0,"Captions in videos contain valuable information for video retrieval. Although texts in captions can be obtained easily in the new image compression formats like MPEG2, there still are many video programs encoded in older formats. Thus, video OCR is indispensable for content-based video retrieval. This paper proposes a simple video OCR method for Chinese captions, including image capturing, caption region deciding, background removing, character segmentation, OCR and post-processing. We employed Discovery Channel films as training and testing corpus. In an outside test, the accuracy of the video OCR was 84.1%. The hardware used in the experiment consisted of a computer with a P4-1.7G CPU, 256MB RAM and a 40G, 7200rpm hard disk. On average, it took 29 minutes and 11 seconds to process a film 495MB in size. We also applied the results of video OCR to video retrieval and question answering."
O01-1008,"ç°¡æå½±çå­å¹æå­è¾¨è­æ³åå\
¶è©¢ç­æç¨ (A Simple Method for Video {OCR} and Its Application on Question Answering) [In {C}hinese]",2001,0,0,3,1,14297,chuanjie lin,Proceedings of Research on Computational Linguistics Conference {XIV},0,None
W00-1202,Sense-Tagging {C}hinese Corpus,2000,10,10,1,1,8628,hsinhsi chen,Second {C}hinese Language Processing Workshop,0,"Contextual information and the mapping from WordNet synsets to Cilin sense tags deal with word sense disambiguation. The average performance is 63.36% when small categories are used, and 1, 2 and 3 candidates are proposed for low, middle and high ambiguous words. The performance of tagging unknown words is 34.35%, which is much better than that of baseline mode. The sense tagger achieves the performance of 76.04%, when unambiguous, ambiguous, and unknown words are tagged."
O00-1005,ååç°æå­é³è­¯ç¸ä¼¼åº¦è©éæ¹æ³èè·¨èªè¨è³è¨æª¢ç´¢ (Similarity Measure in Backward Transliteration between Different Character Sets and Its Application to {CLIR}) [In {C}hinese],2000,0,0,2,0.666667,48044,weihao lin,Proceedings of Research on Computational Linguistics Conference {XIII},0,None
C00-1024,A Muitilingual News Summarizer,2000,-1,-1,1,1,8628,hsinhsi chen,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,None
C00-1025,Mining Tables from Large Scale {HTML} Texts,2000,4,148,1,1,8628,hsinhsi chen,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Table is a very common presentation scheme, but few papers touch on table extraction in text data mining. This paper focuses on mining tables from large-scale HTML texts. Table filtering, recognition, interpretation, and presentation are discussed. Heuristic rules and cell similarities are employed to identify tables. The F-measure of table recognition is 86.50%. We also propose an algorithm to capture attribute-value relationships among table cells. Finally, more structured data is extracted and presented."
P99-1028,Resolving Translation Ambiguity and Target Polysemy in Cross-Language Information Retrieval,1999,15,38,1,1,8628,hsinhsi chen,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"This paper deals with translation ambiguity and target polysemy problems together. Two monolingual balanced corpora are employed to learn word co-occurrence for translation ambiguity resolution, and augmented translation restrictions for target polysemy resolution. Experiments show that the model achieves 62.92% of monolingual information retrieval, and is 40.80% addition to the select-all model. Combining the target polysemy resolution, the retrieval performance is about 10.11% increase to the model resolving translation ambiguity only."
O99-4002,Resolving Translation Ambiguity and Target Polysemy in Cross-Language Information Retrieval,1999,15,38,1,1,8628,hsinhsi chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 4, Number 2, August 1999",0,"This paper deals with translation ambiguity and target polysemy problems together. Two monolingual balanced corpora are employed to learn word co-occurrence for translation ambiguity resolution, and augmented translation restrictions for target polysemy resolution. Experiments show that the model achieves 62.92% of monolingual information retrieval, and is 40.80% addition to the select-all model. Combining the target polysemy resolution, the retrieval performance is about 10.11% increase to the model resolving translation ambiguity only."
O99-3003,A {M}andarin to {T}aiwanese {M}in {N}an Machine Translation System with Speech Synthesis of {T}aiwanese {M}in {N}an,1999,11,5,2,1,14297,chuanjie lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 4, Number 1, {F}ebruary 1999",0,"This paper presents a design of a Mand arin to Taiwanese Min Nan (abbreviated as Taiwanese hereafter) machine translation system. It is the first machine translation system which focuses on these two languages. An input Mandarin sentence is segmented, tagged and translated word by word according to the part of speech of each word. The candidates come from a Mandarin-Taiwanese dictionary. If more than one candidate exists, an example base is consulted. When a Mandarin word is not found in the Mandarin-Taiwanese dictionary, it is translated according to a SingleCharacter dictionary. The output can be in terms of either speech or text. For speech output, we also deal with the tone sa ndhi problem in changing the tone of each Taiwanese syllable. Because the mapping between Taiwanese syllables and Chinese characters is still a subject of disagreement, and the phonetic spelling coding systems are not familiar to everybody, speech output is useful but is also a challenge."
X98-1022,An {NTU}-Approach to Automatic Sentence Extraction for Summary Generation,1998,11,5,4,0,55086,kuanghua chert,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"Automatic summarization and information extraction are two important Internet services. MUC and SUMMAC play their appropriate roles in the next generation Internet. This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1. For categorization task, positive feature vectors and negative feature vectors are used cooperatively to construct generic, indicative summaries. For adhoc task, a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment, to rank relevant sentences, and to generate the user-directed summaries. The result shows that the NormF of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0.447. The NormF of the best summary and that of the fixed summary for categorization task are 0.4090 and 0.4023. Our system outperforms the average system in categorization task but does a common job in adhoc task."
P98-1036,Proper Name Translation in Cross-Language Information Retrieval,1998,14,64,1,1,8628,hsinhsi chen,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Recently, language barrier becomes the major problem for people to search, retrieve, and understand WWW documents in different languages. This paper deals with query translation issue in cross-language information retrieval, proper names in particular. Models for name identification, name translation and name searching are presented. The recall rates and the precision rates for the identification of Chinese organization names, person names and location names under MET data are (76.67%, 79.33%), (87.33%, 82.33%) and (77.00%, 82.00%), respectively. In name translation, only 0.79% and 1.11% of candidates for English person names and location names, respectively, have to be proposed. The name searching facility is implemented on an MT sever for information retrieval on the WWW. Under this system, user can issue queries and read documents with his familiar language."
O98-3005,White Page Construction from Web Pages for Finding People on the {I}nternet,1998,10,6,1,1,8628,hsinhsi chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 1, {F}ebruary 1998: Special Issue on the 10th Research on Computational Linguistics International Conference",0,"This paper proposes a method to extract proper names and their associated information from web pages for Internet/Intranet users automatically. The information extracted from World Wide Web documents includes proper nouns, E-mail addresses and home page URLs. Natural language processing techniques are employed to identify and classify proper nouns, which are usually unknown words. The information (i.e., home pages' URLs or e-mail addresses) for those proper nouns appearing in the anchor parts can be easily extracted using the associated anchor tags. For those proper nouns in the non-anchor pan of a web page, different kinds of clues, such as the spelling method, adjacency principle and HTML tags, are used to relate proper nouns to their corresponding E-mail addresses and/or URLs. Based on the semantics of content and HTML tags, the extracted information is more accurate than the results obtained using traditional search engines. The results can be used to construct white pages for Internet/Intranet users or to build databases for finding people and organizations on the Internet. Such searching services are very useful for human communication and dissemination of information."
M98-1017,Description of the {NTU} System used for {MET}-2,1998,6,43,1,1,8628,hsinhsi chen,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,"Named entities form the major components in a document. When we catch the fundamental entities, we can understand a document to some degree. This paper employs different types of information from different levels of text to extract named entities, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache and n-gram model. In the formal run of MET-2, the F-measures P&R, 2P&R and P&2R are 79.61%, 77.88% and 81.42%, respectively. INTRODUCTION People, affairs, time, places and things are five basic entities in a document. When we catch the fundamental entities, we can understand a document to some degree. Natural Language Processing Laboratory (NLPL) in Department of Computer Science and Information Engineering (CSIE), National Taiwan University (NTU) starts to study named entity extraction problem in 1993. At first, we focus on the extraction of Chinese person names, transliterated person names [1] and organization names [2]. The training data and the testing data in these experiments are selected from three Taiwan newspaper corpora (China Times, Liberty Times News and United Daily News). Chen and Lee [3] reported the precision rate and the recall rate for the extraction of Chinese person names, transliterated person names and organization names are (88.04%, 92.56%), (50.62%, 71.93%) and (61.79%, 54.50%), respectively in the 16th International Conference on Computational Linguistics. We employ these results to several applications. Chen and Wu [4] considered person names as one of clues in sentence alignment. Chen and Lee [3] show its application to anaphora resolution. Chen and Bian [5] proposed a method to construct white pages for Internet/Intranet users automatically. We extract information from World Wide Web documents, including proper nouns, E-mail addresses and home page URLs, and find the relationship among these data. Chen, Ding and Tsai [6,7] dealt with proper noun extraction for information retrieval. In MUC-7 and MET-2, we attend named entity extraction tasks for both English and Chinese. We extend our previous work on this problem to cover more named entity types such as locations, date/time expressions and monetary and percentage expressions. Several issues have to be addressed during extension. One of the major differences between Chinese and English language processing is that segmentation is required for Chinese. That is, we have to identify word boundary in Chinese sentences beforehand. That makes Chinese named entity extraction tasks more changeable. Besides, the vocabulary set and the Chinese coding set used in Taiwan and in China are not the same. The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform simplified Chinese characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing. A word that is known may become unknown due to transformation. For example, the character u in xe2x80xa1u (early morning) is used in traditional Chinese characters. However, D is used in simplified Chinese characters and it is also a legal traditional Chinese character that denotes another meaning. In other words, the mapping from GB to Big5 is xe2x80xa1D, which is an unknown word based on our dictionary. The different vocabulary set between China and Taiwan results in different segmentation. This paper is organized as follows. Section 2 illustrates the flow of named entity extraction and the summary scores of our team in MET-2 formal run. Sections 3, 4 and 5 propose methods to extract named people, organizations and locations. Section 6 deals with the rest of named entities, i.e., date/time expressions and monetary and percentage expressions. After each section, we discuss the sources of errors in the formal run. Section 7 concludes the remarks. FLOW OF NAMED ENTITY EXTRACTION The following shows the flow of named entity extraction in MET-2 formal run. (1) Transform Chinese texts in GB codes into texts in Big-5 codes. (2) Segment Chinese texts into a sequence of tokens."
C98-1036,Proper Name Translation in Cross-Language Information Retrieval,1998,14,64,1,1,8628,hsinhsi chen,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Recently, language barrier becomes the major problem for people to search, retrieve, and understand WWW documents in different languages. This paper deals with query translation issue in cross-language information retrieval, proper names in particular. Models for name identification, name translation and name searching are presented. The recall rates and the precision rates for the identification of Chinese organization names, person names and location names under MET data are (76.67%, 79.33%), (87.33%, 82.33%) and (77.00%, 82.00%), respectively. In name translation, only 0.79% and 1.11% of candidates for English person names and location names, respectively, have to be proposed. The name searching facility is implemented on an MT sever for information retrieval on the WWW. Under this system, user can issue queries and read documents with his familiar language."
bian-chen-1998-integrating,Integrating query translation and document translation in a cross-language information retrieval system,1998,18,21,2,0,47962,guowei bian,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Due to the explosive growth of the WWW, very large multilingual textual resources have motivated the researches in Cross-Language Information Retrieval and online Web Machine Translation. In this paper, the integration of language translation and text processing system is proposed to build a multilingual information system. A distributed English-Chinese system on WWW is introduced to illustrate how to integrate query translation, search engines, and web translation system. Since July 1997, more than 46,000 users have accessed our system and about 250,000 English web pages have been translated to pages in Chinese or bilingual English-Chinese versions. And the average satisfaction degree of users at document level is 67.47{\%}."
O97-4001,Building a Bracketed Corpus Using f2 Statistics,1997,8,0,2,1,45165,yueshi lee,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 2, August 1997",0,"Research based on treebanks is ongoing for many natural language applications. However, the work involved in building a large-scale treebank is laborious and time-consuming. Thus, speeding up the process of building a treebank has become an important task. This paper proposes two versions of probabilistic chunkers to aid the development of a bracketed corpus. The basic version partitions part-of-speech sequences into chunk sequences, which form a partially bracketed corpus. Applying the chunking action recursively, the recursive version generates a fully bracketed corpus. Rather than using a treebank as a training corpus, a corpus, which is tagged with part-of-speech information only, is used. The experimental results show that the probabilistic chunker has a correct rate of more than 94% in producing a partially bracketed corpus and also gives very encouraging results in generating a fully bracketed corpus. These two versions of chunkers are simple but effective and can also be applied to many natural language applications."
O97-1010,Proper Name Extraction from Web Pages for Finding People in {I}nternet,1997,0,7,1,1,8628,hsinhsi chen,Proceedings of the 10th Research on Computational Linguistics International Conference,0,"This paper proposes a method to extract proper names and their associated information from Web pages for Internet/Intranet users automatically. It extracts information from World Wide Web documents, including proper nouns, E-mail addresses and home page URLs, and finds the relationship among these data. Natural language processing techniques are employed to identify and classify proper nouns, which are usually unknown words. Different kinds of clues such as spelling method, adjacency principle and HTML tags are used to relate proper nouns to their corresponding E-mail and/or URL. With the mapping schemes, the extracted information is more accurate than the results from the traditional searching engines. The results can be used as the database of the services for finding people and organizations in Internet. Such searching services are very useful for human communication and dissemination of information."
A97-1010,Applying Repair Processing in {C}hinese Homophone Disambiguation,1997,12,7,2,1,45165,yueshi lee,Fifth Conference on Applied Natural Language Processing,0,"Repair processing plays an important role in spoken language processing systems. This paper proposes a method for correcting Chinese repetition repairs and demonstrates the effects of repair processing in Chinese homophone disambiguation. The experimental results show that the precision rate of 93.87% and the recall rate of 90.65% can be achieved for the repair processing. At the same time, 50% of errors in the repairing segments can be reduced for Chinese homophone disambiguation."
O96-2005,A Hybrid Approach to Machine Translation System Design,1996,22,7,2,1,35855,kuanghua chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 1, Number 1, August 1996",0,"It is difficult for pure statistics-based machine translation systems to process long sentences. In addition, the domain dependent problem is a key issue under such a framework. Pure rule-based machine translation systems have many human costs in formulating rules and introduce inconsistencies when the number of rules increases. Integration of these two approaches reduces the difficulties associated with both. In this paper, an integrated model for machine translation system is proposed. A partial parsing method is adopted, and the translation process is performed chunk by chunk. In the synthesis module, the word order is locally rearranged within chunks via the Markov model. Since the length of a chunk is much shorter than that of a sentence, the disadvantage of the Markov model in dealing with long distance phenomena is greatly reduced. Structural transfer is fulfilled using a set of rules; in contrast, lexical transfer is resolved using bilingual constraints. Qualitative and quantitative knowledge is employed interleavingly and cooperatively, so that the advantages of these two approaches can be retained."
O96-1005,Correcting {C}hinese Repetition Repairs In Spontaneous Speech,1996,9,3,2,1,45165,yueshi lee,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,"Disfluencies involving speech repairs pose serious problems for spoken language processing systems. However, which cues in speech signals may facilitate Chinese repair processing is not known. This paper concerns the acoustic and prosodic analysis for correcting Chinese repetition repairs in spontaneous speech. A large spoken corpus is examined in this study. The experimental results show that our method can achieve the precision rate of 93.87% and the recall rate of 90.65%, without using the lexical information."
C96-1038,A Rule-Based and {MT}-Oriented Approach to Prepositional Phrase Attachment,1996,10,9,2,1,35855,kuanghua chen,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Prepositional Phrase is the key issue in structural ambiguity. Recently, researches in corpora provide the lexical cue of prepositions with other words and the information could be used to partly resolve ambiguity resulted from prepositional phrases. Two possible attachments are considered in the literature; either noun attachment or verb attachment. In this paper, we consider the problem from viewpoint of machine translation. Four different attachments are told out according to their functionality: noun attachment. Both lexical knowledge and semantic knowledge are involved resolving attachment in the proposed mechanism. Experimental results show that considering more types of prepositional phrases is useful in machine translation."
C96-1039,Identification and Classification of Proper Nouns in {C}hinese Texts,1996,14,44,1,1,8628,hsinhsi chen,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Various strategies are proposed to identify and classify three types of proper nouns in Chinese texts. Clues from character, sentence and paragraph levels are employed to resolve Chinese personal names. Character, Syllable and Frequency Conditions are presented to treat transliterated personal names. To deal with organization names, keywords, prefix, word association and parts-of-speech are applied. For fair evaluation, large scale test data are selected from six sections of a newspaper. The precision and the recall for these three types are (88.04%, 92.56%), (50.62%, 71.93%) and (61.79%, 54.50%), respectively. When the former two types are regarded as a category, the performance becomes (81.46%, 91.22%). Compared with other approaches, our approach has better performance and our classification is automatic."
W95-0113,Development of a Partially Bracketed Corpus with Part-of-Speech Information Only,1995,8,4,1,1,8628,hsinhsi chen,Third Workshop on Very Large Corpora,0,"Resea/ch based on a treebank is active for many natural language applications. However, the work to build a large scale treebank is laborious and tedious. This paper proposes a probabilistic chunker to help the development of a partially bracketed corpus. The chunker partitions the part-of-speech sequence into segments called chunks. Rather than using a treebank as our training corpus, a corpus which is tagged with part-of-speech information only is used. The experimental results show the probabilistic chunker has more than 92% correct rate in outside test. The well-formed partially bracketed corpus is a milestone in the development of a treebank. Besides, the simple but effective chunker can also be applied to many natural language applications."
1995.tmi-1.21,Machine Translation: an Integration Approach,1995,10,49,2,1,35855,kuanghua chen,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"A pure statistics-based machine translation system is usually incapable of processing long sentences and is usually domain dependent. A pure rule-based machine translation system involves many costs in formulating rules. In addition, it is easy to introduce inconsistencies in a rule-based system, when the number of rules increases. Integrating both of approaches will get rid of these disadvantages. In this paper, a new model for machine translation system is proposed. A partial parsing method is adopted and the translation process is performed chunk by chunk. In synthesis module, the words are locally rearranged in chunks according to Markov model. Since the length of a chunk is much shorter than that of a sentence, the disadvantage of Markov model in dealing with long distance phenomena is greatly reduced. The structural transfer is fulfilled using a set of rules; in contrast, lexical transfer is resolved using bilingual constraints. The qualitative and quantitative knowledge is applied interleavingly and cooperatively, so that the advantages of both approaches are kept."
1995.iwpt-1.10,A Chunking-and-Raising Partial Parser,1995,-1,-1,1,1,8628,hsinhsi chen,Proceedings of the Fourth International Workshop on Parsing Technologies,0,"Parsing is often seen as a combinatorial problem. It is not due to the properties of the natural languages, but due to the parsing strategies. This paper investigates a Constrained Grammar extracted from a Treebank and applies it in a non-combinatorial partial parser. This parser is a simpler version of a chunking-and-raising parser. The chunking and raising actions can be done in linear time. The short-term goal of this research is to help the development of a partially bracketed corpus, i.e., a simpler version of a treebank. The long-term goal is to provide high level linguistic constraints for many natural language applications."
P94-1032,Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation,1994,14,49,2,1,35855,kuanghua chen,32nd Annual Meeting of the Association for Computational Linguistics,1,"To acquire noun phrases from running texts is useful for many applications, such as word grouping, terminology indexing, etc. The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem. In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism. The test texts are SUSANNE Corpus and the results are evaluated by comparing the parse field of SUSANNE Corpus automatically. The results of this preliminary experiment are encouraging."
O94-1010,ä¸­æææ¬äººåè¾¨è­åé¡ä¹ç ç©¶ (Identification of Personal Names in {C}hinese Texts) [In {C}hinese],1994,-1,-1,3,0,56026,jenchang lee,Proceedings of Rocling {VII} Computational Linguistics Conference {VII},0,None
C94-1026,A Part-of-Speech-Based Alignment Algorithm,1994,12,14,2,1,35855,kuanghua chen,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"To align bilingual texts becomes a crucial issue recently. Rather than using length-based or translation-based criterion, a part-of-speech-based criterion is proposed. We postulate that source texts and target texts should share the same concepts, ideas, entities, and events. Simulated annealing approach is used to implement this alignment algorithm. The preliminary experiments show good performance. Most importantly, the experimental objects are Chinese-English texts, which are selected from different language families."
O93-1002,A Storage Reduction Method For Corpus-Based Language Models,1993,0,0,1,1,8628,hsinhsi chen,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O93-1003,A Probabilistic Chunker,1993,16,9,2,1,35855,kuanghua chen,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O92-1008,A Parallel Augmented Context-Free Parsing System For Natural Language Analysis,1992,0,0,1,1,8628,hsinhsi chen,Proceedings of Rocling V Computational Linguistics Conference V,0,None
C90-2009,A Logic-Based Government-Binding Parser for {M}andarin {C}hinese,1990,6,11,1,1,8628,hsinhsi chen,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"Mandarin Chinese is a highly flexible and context-sensitive language. It is difficult to do the case marking and index assignment during the parsing of Chinese sentences. This paper proposes a logic-based Government-Binding approach to treat this problem. The grammar formalism is specified in a formal way. Uniform treatments of movements, arbitrary number of movement non-terminals, automatic detection of grammar errors beforehand, and clear declarative semantics are its specific features. Many common linguistic phenomena of Chinese sentences are represented with this formalism. For example, topic-comment structures, the ba-constructions, the bei-constructions, relative clause constructions, appositive clause constructions, and serial verb constructions. A simple pronoun resolution is touched upon. The expressive capabilities and the design methodologies show this mechanism is also suitable for other flexible and context-sensitive languages."
O88-1007,The Parsing Environment for {M}andarin Syntax,1988,0,0,3,0,57496,ipeng lin,Proceedings of Rocling {I} Computational Linguistics Conference {I},0,None
C88-1024,A New Design of {P}rolog-Based Bottom-Up Parsing System With {G}overnment-{B}inding Theory,1988,8,8,1,1,8628,hsinhsi chen,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper addresses the problems of movement transformation in Prolog-based bottom-up parsing system. Three principles of Government-Binding theory are employed to deal with these problems. They are Empty Category Principle, C-command Principle, and Subjacency Principle. A formalism based upon them is proposed. Translation algorithms are given to add these linguistic principles to the general grammar rules, the leftward movement grammar rules, and the rightward movement grammar rules respectively. This approach has the following specific features: the uniform treatments of leftward and rightward movements, the arbitrary number of movement non-terminals in the rule body, and automatic detection of grammar errors before parsing. An example in Chinese demonstrates all the concepts."
