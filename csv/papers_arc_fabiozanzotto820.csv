2020.emnlp-main.18,{KERMIT}: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations,2020,-1,-1,1,1,20081,fabio zanzotto,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks"
S18-1076,{S}ynt{NN} at {S}em{E}val-2018 Task 2: is Syntax Useful for Emoji Prediction? Embedding Syntactic Trees in Multi Layer Perceptrons,2018,0,1,1,1,20081,fabio zanzotto,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper, we present SyntNN as a way to include traditional syntactic models in multilayer neural networks used in the task of Semeval Task 2 of emoji prediction. The model builds on the distributed tree embedder also known as distributed tree kernel. Initial results are extremely encouraging but additional analysis is needed to overcome the problem of overfitting."
J15-1010,{S}quibs: When the Whole Is Not Greater Than the Combination of Its Parts: A {``}Decompositional{''} Look at Compositional Distributional Semantics,2015,11,5,1,1,20081,fabio zanzotto,Computational Linguistics,0,"Distributional semantics has been extended to phrases and sentences by means of composition operations. We look at how these operations affect similarity measurements, showing that similarity equations of an important class of composition methods can be decomposed into operations performed on the subparts of the input phrases. This establishes a strong link between these models and convolution kernels."
S14-2049,ha{LF}: Comparing a Pure {CDSM} Approach with a Standard Machine Learning System for {RTE},2014,19,1,2,1,37739,lorenzo ferrone,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, we describe our submission to the Shared Task #1. We tried to follow the underlying idea of the task, that is, evaluating the gap of full-fledged recognizing textual entailment systems with respect to compositional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods."
S14-1013,Compositional Distributional Semantics Models in Chunk-based Smoothed Tree Kernels,2014,14,0,3,0,22647,nghia pham,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"The field of compositional distributional semantics has proposed very interesting and reliable models for accounting the distributional meaning of simple phrases. These models however tend to disregard the syntactic structures when they are applied to larger sentences. In this paper we propose the chunk-based smoothed tree kernels (CSTKs) as a way to exploit the syntactic structures as well as the reliability of these compositional models for simple phrases. We experiment with the recognizing textual entailment datasets. Our experiments show that our CSTKs perform better than basic compositional distributional semantic models (CDSMs) recursively applied at the sentence level, and also better than syntactic tree kernels."
C14-1068,Towards Syntax-aware Compositional Distributional Semantic Models,2014,26,10,2,1,37739,lorenzo ferrone,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Compositional Distributional Semantics Models (CDSMs) are traditionally seen as an entire different world with respect to Tree Kernels (TKs). In this paper, we show that under a suitable regime these two approaches can be regarded as the same and, thus, structural information and distributional semantics can successfully cooperate in CSDMs for NLP tasks. Leveraging on distributed trees, we present a novel class of CDSMs that encode both structure and distributional meaning: the distributed smoothed trees (DSTs). By using DSTs to compute the similarity among sentences, we implicitly define the distributed smoothed tree kernels (DSTKs). Experiment with our DSTs show that DSTKs approximate the corresponding smoothed tree kernels (STKs). Thus, DSTs encode both structural and distributional semantics of text fragments as STKs do. Experiments on RTE and STS show that distributional semantics encoded in DSTKs increase performance over structure-only kernels."
W13-3824,Linear Compositional Distributional Semantics and Structural Kernels,2013,-1,-1,2,1,37739,lorenzo ferrone,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-3205,Transducing Sentences to Syntactic Feature Vectors: an Alternative Way to {``}Parse{''}?,2013,42,5,1,1,20081,fabio zanzotto,Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,0,"Classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors. In this paper, we explore an alternative path to use syntax in feature spaces: the Distributed Representation xe2x80x9cParsersxe2x80x9d(DRP). The core of the idea is straightforward: DRPs directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations. Results show that DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information."
S13-2007,{S}em{E}val-2013 Task 5: Evaluating Phrasal Semantics,2013,17,19,3,0,39671,ioannis korkontzelos,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes the SemEval-2013 Task 5: xe2x80x9cEvaluating Phrasal Semanticsxe2x80x9d. Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length. The second one addresses deciding the compositionality of phrases in a given context. The paper discusses the importance and background of these subtasks and their structure. In succession, it introduces the systems that participated and discusses evaluation results."
W11-1302,Distributed Structures and Distributional Meaning,2011,-1,-1,1,1,20081,fabio zanzotto,Proceedings of the Workshop on Distributional Semantics and Compositionality,0,None
D11-1061,Linguistic Redundancy in {T}witter,2011,32,39,1,1,20081,fabio zanzotto,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance."
W10-3504,Expanding textual entailment corpora from{W}ikipedia using co-training,2010,24,25,1,1,20081,fabio zanzotto,Proceedings of the 2nd Workshop on {T}he {P}eople{'}s {W}eb {M}eets {NLP}: {C}ollaboratively {C}onstructed {S}emantic {R}esources,0,In this paper we propose a novel method to automatically extract large textual entailment datasets homogeneous to existing ones. The key idea is the combination of two intuitions: (1) the use of Wikipedia to extract a large set of textual entailment pairs; (2) the application of semisupervised machine learning methods to make the extracted dataset homogeneous to the existing ones. We report empirical evidence that our method successfully expands existing textual entailment corpora.
N10-1146,Syntactic/Semantic Structures for Textual Entailment Recognition,2010,34,25,3,0,3127,yashar mehdad,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning."
fallucchi-etal-2010-generic,Generic Ontology Learners on Application Domains,2010,24,1,3,1,20086,francesca fallucchi,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In ontology learning from texts, we have ontology-rich domains where we have large structured domain knowledge repositories or we have large general corpora with large general structured knowledge repositories such as WordNet (Miller, 1995). Ontology learning methods are more useful in ontology-poor domains. Yet, in these conditions, these methods have not a particularly high performance as training material is not sufficient. In this paper we present an LSP ontology learning method that can exploit models learned from a generic domain to extract new information in a specific domain. In our model, we firstly learn a model from training data and then we use the learned model to discover knowledge in a specific domain. We tested our model adaptation strategy using a background domain that is applied to learn the isa networks in the Earth Observation Domain as a specific domain. We will demonstrate that our method captures domain knowledge better than other generic models: our model better captures what is expected by domain experts than a baseline method based only on WordNet. This latter is better correlated with non-domain annotators asked to produce the ontology for the specific domain."
C10-1142,Estimating Linear Models for Compositional Distributional Semantics,2010,20,98,1,1,20081,fabio zanzotto,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"In distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics."
W09-0209,{SVD} Feature Selection for Probabilistic Taxonomy Learning,2009,17,6,2,1,20086,francesca fallucchi,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances."
R09-1016,Singular Value Decomposition for Feature Selection in Taxonomy Learning,2009,28,2,2,1,20086,francesca fallucchi,Proceedings of the International Conference {RANLP}-2009,0,"In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances."
D09-1010,Efficient kernels for sentence pair classification,2009,21,10,1,1,20081,fabio zanzotto,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a novel class of graphs, the tripartite directed acyclic graphs (tDAGs), to model first-order rule feature spaces for sentence pair classification. We introduce a novel algorithm for computing the similarity in first-order rewrite rule feature spaces. Our algorithm is extremely efficient and, as it computes the similarity of instances that can be represented in explicit feature spaces, it is a valid kernel function."
W08-2004,Encoding Tree Pair-Based Graphs in Learning Algorithms: The Textual Entailment Recognition Case,2008,18,4,2,0.0544974,4033,alessandro moschitti,Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing,0,"In this paper, we provide a statistical machine learning representation of textual entailment via syntactic graphs constituted by tree pairs. We show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs, which can only be managed using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation."
fallucchi-zanzotto-2008-yet,Yet another Platform for Extracting Knowledge from Corpora,2008,16,0,2,1,20086,francesca fallucchi,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The research field of Âextracting knowledge bases from text collectionsÂ seems to be mature: its target and its working hypotheses are clear. In this paper we propose a platform, YAPEK, i.e., Yet Another Platform for Extracting Knowledge from corpora, that wants to be the base to collect the majority of algorithms for extracting knowledge bases from corpora. The idea is that, when many knowledge extraction algorithms are collected under the same platform, relative comparisons are clearer and many algorithms can be leveraged to extract more valuable knowledge for final tasks such as Textual Entailment Recognition. As we want to collect many knowledge extraction algorithms, YAPEK is based on the three working hypotheses of the area: the basic hypothesis, the distributional hypothesis, and the point-wise assertion patterns. In YAPEK, these three hypotheses define two spaces: the space of the target textual forms and the space of the contexts. This platform guarantees the possibility of rapidly implementing many models for extracting knowledge from corpora as the platform gives clear entry points to model what is really different in the different algorithms: the feature spaces, the distances in these spaces, and the actual algorithm."
W07-1412,Shallow Semantic in Fast Textual Entailment Rule Learners,2007,7,24,1,1,20081,fabio zanzotto,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"In this paper, we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results."
W06-3806,Similarity between Pairs of Co-indexed Trees for Textual Entailment Recognition,2006,9,1,1,1,20081,fabio zanzotto,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers. We defined a kernel function based on this similarity along with a more classical intra-pair similarity. Experiments show an improvement of 4.4 absolute percent points over state-of-the-art methods.
P06-1051,Automatic Learning of Textual Entailments with Cross-Pair Similarities,2006,24,66,1,1,20081,fabio zanzotto,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs). This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods.
P06-1107,Discovering Asymmetric Entailment Relations between Verbs Using Selectional Preferences,2006,17,38,1,1,20081,fabio zanzotto,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we investigate a novel method to detect asymmetric entailment relations between verbs. Our starting point is the idea that some point-wise verb selectional preferences carry relevant semantic information. Experiments using Word-Net as a gold standard show promising results. Where applicable, our method, used in combination with other approaches, significantly increases the performance of entailment detection. A combined approach including our model improves the AROC of 5% absolute points with respect to standard models."
shehata-zanzotto-2006-dependency,A Dependency-based Algorithm for Grammar Conversion,2006,7,1,2,0,50270,alessandro shehata,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we present a model to transfer a grammatical formalism in another. The model is applicable only on restrictive conditions. However, it is fairly useful for many purposes: parsing evaluation, researching methods for truly combining different parsing outputs to reach better parsing performances, and building larger syntactically annotated corpora for data-driven approaches. The model has been tested over a case study: the translation of the Turin Tree Bank Grammar to the Shallow Grammar of the CHAOS Italian parser."
pazienza-etal-2006-mixing,"Mixing {W}ord{N}et, {V}erb{N}et and {P}rop{B}ank for studying verb relations",2006,9,7,3,0,39305,maria pazienza,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we present a novel resource for studying the semantics of verb relations. The resource is created by mixing sense relational knowledge enclosed in WordNet, frame knowledge enclosed in VerbNet and corpus knowledge enclosed in PropBank. As a result, a set of about 1000 frame pairs is made available. A frame pair represents a pair of verbs in a peculiar semantic relation accompanied with specific information, such as: the syntactic-semantic frames of the two verbs, the mapping among their thematic roles and a set of textual examples extracted from the PennTreeBank. We specifically focus on four relations: Troponymy, Causation, Entailment and Antonymy. The different steps required for the mapping are described in detail and statistics on resource mutual coverage are reported. We also propose a practical use of the resource for the task of Textual Entailment acquisition and for Question Answering. A first attempt for automate the mapping among verb arguments is also presented: early experiments show that simple techniques can achieve good results, up to 85{\%} F-Measure."
W05-1207,Discovering Entailment Relations Using {``}Textual Entailment Patterns{''},2005,19,7,1,1,20081,fabio zanzotto,Proceedings of the {ACL} Workshop on Empirical Modeling of Semantic Equivalence and Entailment,0,"In this work we investigate methods to enable the detection of a specific type of textual entailment (strict entailment), starting from the preliminary assumption that these relations are often clearly expressed in texts. Our method is a statistical approach based on what we call textual entailment patterns, prototypical sentences hiding entailment relations among two activities. We experimented the proposed method using the entailment relations of WordNet as test case and the web as corpus where to estimate the probabilities; obtained results will be shown."
W04-2510,Ontological resources and question answering,2004,16,17,5,0,12620,roberto basili,Proceedings of the Workshop on Pragmatics of Question Answering at {HLT}-{NAACL} 2004,0,This paper discusses the possibility of building an ontology-based question answering system in the context of the Semantic Web presenting a proof-of-concept system. The system is under development in the MOSES European Project.
guthrie-etal-2004-large,Large Scale Experiments for Semantic Labeling of Noun Phrases in Raw Text,2004,8,2,3,0,37309,louise guthrie,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,This paper gives a brief overview of the results of our work during the Summer 2003 Workshop of the Center for Language and Speech Processing at the Johns Hopkins University in Baltimore Maryland. The goal of the project was to determine the feasibility of extending named entity recognition to common nouns and determine whether or not it is possible to assign automatically a predetermined set of semantic tags and approach human performance in the task.
basili-etal-2004-a2q,{A}2{Q}: An Agent-based Architecure for Multilingual {Q}{\\&}{A},2004,16,0,4,0,12620,roberto basili,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,In this paper we describe the agent based architecture and extensively report the design of the shallow processing model in it. We present the general model describing the data flow and the expected activities that have to be carried out. The notion of question session will be introduced as a means to control the communication among the different agents. We then present a shallow model mainly based on an IR engine and a passage re-ranking that uses the notion of expanded query. We will report the pilot investigation on the performances of
basili-etal-2004-similarity,A Similarity Measure for Unsupervised Semantic Disambiguation,2004,5,11,3,0,12620,roberto basili,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents an unsupervised method for the resolution of lexical ambiguity of nouns. The method relies on the topological structure of the noun taxonomy of WordNet where a notion of semantic distance is defined. An unsupervised semantic tagger, based on the above measure, is evaluated over an hand-annotated portion of the British National Corpus and compared with a supervised approach based on the Maximum Entropy Model."
C02-1129,Decision Trees as Explicit Domain Term Definitions,2002,4,1,3,0,12620,roberto basili,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,Terminology Acquisition (TA) methods are viable solutions for the knowledge bottleneck problem that confines knowledge-intensive information access systems (such as Information Extraction systems) to restricted application scenarios. TA can be seen as a way to inspect large text collections for extracting concise domain knowledge. In this paper we argue that major insights over the notion of term can be obtained by investigating a more domain-based term definition. We propose a decision tree learning approach as an interesting model of the human TA activity. An incremental model is proposed to study the evolution of the term definition during the TA process over a particular implicit domain model. The experimental apparatus is based on robust text processing tools that support a large scale investigation. The good results suggest that the proposed automatic TA model can support the development of conceptual domain dictionaries as required by knowledge-based information systems.
W01-1013,Multilingual Authoring: the {NAMIC} Approach,2001,11,7,3,0,12620,roberto basili,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"With increasing amounts of electronic information available, and the increase in the variety of languages used to produce documents of the same type, the problem of how to manage similar documents in different languages arises. This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out. This work, funded by the European Union, is applied to the Multilingual Authoring of news agency text. We have applied methods from Natural Language Processing, especially Information Extraction technology, to both monolingual and Multilingual Authoring."
basili-etal-2000-tuning,Tuning Lexicons to New Operational Scenarios,2000,10,0,4,0,12620,roberto basili,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,In this paper the role of the lexicon within typical application tasks based on NLP is analysed. A large scale semantic lexicon is studied within the framework of a NLP application. The coverage of the lexicon with respect the target domain and a (semi)automatic tuning approach have been evaluated. The impact of a corpus-driven inductive architecture aiming to compensate lacks in lexical information are thus measured and discussed.
