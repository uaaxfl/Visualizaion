2021.konvens-1.1,The Impact of Word Embeddings on Neural Dependency Parsing,2021,-1,-1,2,0,5535,benedikt adelmann,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2020.lrec-1.292,{E}ye4{R}ef: A Multimodal Eye Movement Dataset of Referentially Complex Situations,2020,-1,-1,5,0,6056,ozge alacam,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Eye4Ref is a rich multimodal dataset of eye-movement recordings collected from referentially complex situated settings where the linguistic utterances and their visual referential world were available to the listener. It consists of not only fixation parameters but also saccadic movement parameters that are time-locked to accompanying German utterances (with English translations). Additionally, it also contains symbolic knowledge (contextual) representations of the images to map the referring expressions onto the objects in corresponding images. Overall, the data was collected from 62 participants in three different experimental setups (86 systematically controlled sentence{--}image pairs and 1844 eye-movement recordings). Referential complexity was controlled by visual manipulations (e.g. number of objects, visibility of the target items, etc.), and by linguistic manipulations (e.g., the position of the disambiguating word in a sentence). This multimodal dataset, in which the three different sources of information namely eye-tracking, language, and visual environment are aligned, offers a test of various research questions not from only language perspective but also computer vision."
W18-6444,Translation of Biomedical Documents with Focus on {S}panish-{E}nglish,2018,-1,-1,2,1,27723,mirelastefania duma,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and it performed best on Portuguese-English, where a BLEU score of 41.84 placed it third out of seven runs submitted by three institutions. In this paper, we describe our method and results with a special focus on Spanish-English where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast, unsupervised method for selecting domain-specific data for training models which obtain good results using only 10{\%} of the general domain data."
W18-6460,The Benefit of Pseudo-Reference Translations in Quality Estimation of {MT} Output,2018,0,1,2,1,27732,melania duma,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"In this paper, a novel approach to Quality Estimation is introduced, which extends the method in (Duma and Menzel, 2017) by also considering pseudo-reference translations as data sources to the tree and sequence kernels used before. Two variants of the system were submitted to the sentence level WMT18 Quality Estimation Task for the English-German language pair. They have been ranked 4th and 6th out of 13 systems in the SMT track, while in the NMT track ranks 4 and 5 out of 11 submissions have been reached."
W18-3005,Text Completion using Context-Integrated Dependency Parsing,2018,0,0,3,0,17255,amr salama,Proceedings of The Third Workshop on Representation Learning for {NLP},0,"Incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges that a successful communication system has to deal with. In this paper, we study text completion with a data set composed of sentences with gaps where a successful completion cannot be achieved through a uni-modal (language-based) approach. We present a solution based on a context-integrating dependency parser incorporating an additional non-linguistic modality. An incompleteness in one channel is compensated by information from another one and the parser learns the association between the two modalities from a multiple level knowledge representation. We examined several model variations by adjusting the degree of influence of different modalities in the decision making on possible filler words and their exact reference to a non-linguistic context element. Our model is able to fill the gap with 95.4{\%} word and 95.2{\%} exact reference accuracy hence the successful prediction can be achieved not only on the word level (such as mug) but also with respect to the correct identification of its context reference (such as mug 2 among several mug instances)."
S18-2006,Graph Algebraic {C}ombinatory {C}ategorial {G}rammar,2018,-1,-1,2,1,25272,sebastian beschke,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,0,"This paper describes CCG/AMR, a novel grammar for semantic parsing of Abstract Meaning Representations. CCG/AMR equips Combinatory Categorial Grammar derivations with graph semantics by assigning each CCG combinator an interpretation in terms of a graph algebra. We provide an algorithm that induces a CCG/AMR from a corpus and show that it creates a compact lexicon with low ambiguity and achieves a robust coverage of 78{\%} of the examined sentences under ideal conditions. We also identify several phenomena that affect any approach relying either on CCG or graph algebraic approaches for AMR parsing. This includes differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonous construction process. To our knowledge, this paper provides the first analysis of these corpus issues."
L18-1567,"Incorporating Contextual Information for Language-Independent, Dynamic Disambiguation Tasks",2018,0,1,3,0,17256,tobias staron,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-4754,Automatic Threshold Detection for Data Selection in Machine Translation,2017,17,1,2,1,27723,mirelastefania duma,Proceedings of the Second Conference on Machine Translation,0,None
W17-4762,{UHH} Submission to the {WMT}17 Quality Estimation Shared Task,2017,17,0,2,1,27732,melania duma,Proceedings of the Second Conference on Machine Translation,0,None
W17-4766,{UHH} Submission to the {WMT}17 Metrics Shared Task,2017,19,1,2,1,27732,melania duma,Proceedings of the Second Conference on Machine Translation,0,None
S17-2024,{SEF}@{UHH} at {S}em{E}val-2017 Task 1: Unsupervised Knowledge-Free Semantic Textual Similarity via Paragraph Vector,2017,0,0,2,1,27723,mirelastefania duma,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes our unsupervised knowledge-free approach to the SemEval-2017 Task 1 Competition. The proposed method makes use of Paragraph Vector for assessing the semantic similarity between pairs of sentences. We experimented with various dimensions of the vector and three state-of-the-art similarity metrics. Given a cross-lingual task, we trained models corresponding to its two languages and combined the models by averaging the similarity scores. The results of our submitted runs are above the median scores for five out of seven test sets by means of Pearson Correlation. Moreover, one of our system runs performed best on the Spanish-English-WMT test set ranking first out of 53 runs submitted in total by all participants."
W16-2331,Data Selection for {IT} Texts using Paragraph Vector,2016,23,5,2,1,27723,mirelastefania duma,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,This paper presents an overview of the system submitted by the University of Hamburg to the IT domain shared translation task as part of the ACL 2016 First Conference of Machine Translation (WMT 2016). We have chosen data selection as a domain adaptation method. The filtering of the general domain data makes use of paragraph vectors as a novel approach for scoring the sentences. Experiments were conducted for English-German under the constrained condition.
W14-2403,Large-scale {CCG} Induction from the {G}roningen Meaning Bank,2014,7,1,3,1,25272,sebastian beschke,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"In present CCG-based semantic parsing systems, the extraction of a semantic grammar from sentence-meaning examples poses a computational challenge. An important factor is the decomposition of the sentence meaning into smaller parts, each corresponding to the meaning of a word or phrase. This has so far limited supervised semantic parsing to small, specialised corpora. We propose a set of heuristics that render the splitting of meaning representations feasible on a largescale corpus, and present a method for grammar induction capable of extracting a semantic CCG from the Groningen Meaning Bank."
P14-2130,Incremental Predictive Parsing with {T}urbo{P}arser,2014,13,6,2,1,9779,arne kohn,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Most approaches to incremental parsing either incur a degradation of accuracy or they have to postpone decisions, yielding underspecified intermediate output. We present an incremental predictive dependency parser that is fast, accurate, and largely language independent. By extending a state-of-the-art dependency parser, connected analyses for sentence prefixes are obtained, which even predict properties and the structural embedding of upcoming words. In contrast to other approaches, accuracy for complete sentence analyses does not decrease."
foth-etal-2014-size,Because Size Does Matter: The {H}amburg Dependency Treebank,2014,22,18,4,1,39902,kilian foth,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present the Hamburg Dependency Treebank (HDT), which to our knowledge is the largest dependency treebank currently available. It consists of genuine dependency annotations, i. e. they have not been transformed from phrase structures. We explore characteristics of the treebank and compare it against others. To exemplify the benefit of large dependency treebanks, we evaluate different parsers on the HDT. In addition, a set of tools will be described which help working with and searching in the treebank."
R13-1048,Incremental and Predictive Dependency Parsing under Real-Time Conditions,2013,13,3,2,1,9779,arne kohn,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modified to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off."
P13-3019,A New Syntactic Metric for Evaluation of Machine Translation,2013,26,2,3,1,27732,melania duma,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"Machine translation (MT) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation. This comparison can be performed on multiple levels: lexical, syntactic or semantic. In this paper, we propose a new syntactic metric for MT evaluation based on the comparison of the dependency structures of the reference and the candidate translations. The dependency structures are obtained by means of a Weighted Constraints Dependency Grammar parser. Based on experiments performed on English to German translations, we show that the new metric correlates well with human judgments at the system level."
W11-4605,Decision Strategies for Incremental {POS} Tagging,2011,-1,-1,3,0,39903,niels beuck,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,None
W09-3816,Co-Parsing with Competitive Models,2009,17,2,3,0,46835,lidia khmylko,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We present an asymmetric approach to a run-time combination of two parsers where one component serves as a predictor to the other one. Predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions. Previously, the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem. It has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level. Results show that the combined system outperforms its individual components, even though their performance in isolation is already fairly high."
R09-1033,Co-Parsing with Competitive Models,2009,17,2,3,0,46835,lidia khmylko,Proceedings of the International Conference {RANLP}-2009,0,"We present an asymmetric approach to a run-time combination of two parsers where one component serves as a predictor to the other one. Predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions. Previously, the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem. It has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level. Results show that the combined system outperforms its individual components, even though their performance in isolation is already fairly high."
R09-1077,{A}mharic Part-of-Speech Tagger for Factored Language Modeling,2009,13,10,2,0,13994,martha tachbelie,Proceedings of the International Conference {RANLP}-2009,0,"This paper presents Amharic part of speech taggers developed for factored language modeling. Hidden Markov Model (HMM) and Support Vector Machine (SVM) based taggers have been trained using the TnT and SVMTool. The overall accuracy of the best performing TnT- and SVM-based taggers is 82.99% and 85.50%, respectively. Generally, with respect to accuracy SVM-based taggers perform better than TnTbased taggers although TnT-based taggers are more ecient with regard to speed and memory requirement. We have developed factored language models (with two and four parents) for which the estimation of the probability for each word depends on the previous one or two words and their POS. These language models have been used in an Amharic speech recognition task in a lattice rescoring framework and a significant improvement in word recognition accuracy has been observed."
W07-0805,Syllable-Based Speech Recognition for {A}mharic,2007,13,8,2,0,13993,solomon abate,Proceedings of the 2007 Workshop on Computational Approaches to {S}emitic Languages: Common Issues and Resources,0,"Amharic is the Semitic language that has the second large number of speakers after Arabic (Hayward and Richard 1999). Its writing system is syllabic with Consonant-Vowel (CV) syllable structure. Amharic orthography has more or less a one to one correspondence with syllabic sounds. We have used this feature of Amharic to develop a CV syllable-based speech recognizer, using Hidden Markov Modeling (HMM), and achieved 90.43% word recognition accuracy."
W06-2305,Robust Parsing: More with Less,2006,-1,-1,2,1,39902,kilian foth,Proceedings of the Workshop on {ROMAND} 2006:Robust Methods in Analysis of Natural language Data,0,None
P06-2029,The Benefit of Stochastic {PP} Attachment to a Rule-Based Parser,2006,19,10,2,1,39902,kilian foth,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"To study PP attachment disambiguation as a benchmark for empirical methods in natural language processing it has often been reduced to a binary decision problem (between verb or noun attachment) in a particular syntactic configuration. A parser, however, must solve the more general task of deciding between more than two alternatives in many different contexts. We combine the attachment predictions made by a simple model of lexical attraction with a full-fledged parser of German to determine the actual benefit of the subtask to parsing. We show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14% and raise the attachment accuracy for dependency parsing of German to an unprecedented 92%."
P06-1037,Guiding a Constraint Dependency Parser with Supertags,2006,19,17,3,1,39902,kilian foth,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We investigate the utility of supertag information for guiding an existing dependency parser of German. Using weighted constraints to integrate the additionally available information, the decision process of the parser is influenced by changing its preferences, without excluding alternative structural interpretations from being considered. The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy. In addition, an upper bound on the accuracy that can be achieved with perfect supertags is estimated."
P06-1041,Hybrid Parsing: Using Probabilistic Models as Predictors for a Symbolic Parser,2006,18,28,2,1,39902,kilian foth,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar. By including a chunker, a supertagger, a PP attacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accuracy to 91.1% on the German NEGRA corpus. We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner, thus avoiding the problem of error propagation."
P04-3008,Interactive grammar development with {WCDG},2004,3,7,3,1,39902,kilian foth,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"The manual design of grammars for accurate natural language analysis is an iterative process; while modelling decisions usually determine parser behaviour, evidence from analysing more or different input can suggest unforeseen regularities, which leads to a reformulation of rules, or even to a different model of previously analysed phenomena. We describe an implementation of Weighted Constraint Dependency Grammar that supports the grammar writer by providing display, automatic analysis, and diagnosis of dependency analyses and allows the direct exploration of alternative analyses and their status under the current grammar."
daum-etal-2004-automatic,Automatic Transformation of Phrase Treebanks to Dependency Trees,2004,14,29,3,1,51698,michael daum,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Word-to-word dependency structures are useful for consistent representation and comparable evaluation of parsing results. However, most large-scale treebanks contain various variants of phrase structure trees, since automatic parsers usually produce constituent structures. We present a freely available extensible tool for converting phrase structure to dependencies automatically, and discuss its application to the NEGRA treebank of German."
W03-3009,Subtree Parsing to Speed up Deep Analysis,2003,0,4,2,1,39902,kilian foth,Proceedings of the Eighth International Conference on Parsing Technologies,0,"Within a grammar formalism that treats syntax analysis as a global optimization problem, methods are investigated to improve parsing performance by recombining the solutions of smaller and easier subproblems. The robust nature of the formalism allows the application of this technique with little change to the original grammar."
E03-1052,Constraint Based Integration of Deep and Shallow Parsing Techniques,2003,12,25,3,1,51698,michael daum,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"To investigate the contributions of taggers or chunkers to the performance of a deep syntactic parser, Weighted Constraint Dependency Grammars have been extended to also take into consideration information from external sources. Using a weak information fusion scheme based on constraint optimization techniques, a parsing accuracy has been achieved which is comparable to other (stochastic) parsers."
menzel-etal-2000-isle,The {ISLE} Corpus of Non-Native Spoken {E}nglish,2000,4,38,1,1,5536,wolfgang menzel,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"For the purpose of developing pronunciation training tools for second language learning a corpus of non-native speech data has been collected, which consists of almost 18 hours of annotated speech signals spoken by Italian and German learners of English. The corpus is based on 250 utterances selected from typical second language learning exercises. It has been annotated at the word and the phone level, to highlight pronunciation errors such as phone realisation problems and misplaced word stress assignments. The data has been used to develop and evaluate several diagnostic components, which can be used to produce corrective feedback of unprecedented detail to a language learner."
C00-2151,An Experiment On Incremental Analysis Using Robust Parsing Techniques,2000,8,2,2,1,39902,kilian foth,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"The results of an experiment are presented in which an approach for robust parsing has been applied incrementally. They confirm that due to the robust nature of the underlying technology an arbitrary prefix of a sentence can be analysed into an intermediate structural description which is able to direct the further analysis with a high degree of reliability. Most notably, this result can be achieved without adapting the grammar or the parsing algorithms to the case of incremental processing. The resulting incremental parsing procedure is significantly faster if compared to a non-incremental best-first search. Additionally it turns out that longer sentences benefit most from this acceleration."
2000.iwpt-1.11,A Transformation-based Parsing Technique With Anytime Properties,2000,-1,-1,3,1,39902,kilian foth,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"A transformation-based approach to robust parsing is presented, which achieves a strictly monotonic improvement of its current best hypothesis by repeatedly applying local repair steps to a complex multi-level representation. The transformation process is guided by scores derived from weighted constraints. Besides being interruptible, the procedure exhibits a performance profile typical for anytime procedures and holds great promise for the implementation of time-adaptive behaviour."
W98-0509,Decision Procedures for Dependency Parsing Using Graded Constraints,1998,9,32,1,1,5536,wolfgang menzel,Processing of Dependency-Based Grammars,0,None
P98-1086,Eliminative Parsing with Graded Constraints,1998,9,32,3,0,8201,johannes heinecke,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Natural language parsing is conceived to be a procedure of disambiguation, which successively reduces an initially totally ambiguous structural representation towards a single interpretation. Graded constraints are used as means to express well-formedness conditions of different strength and to decide which partial structures are locally least preferred and, hence, can be deleted. This approach facilitates a higher degree of robustness of the analysis, allows to introduce resource adaptivity into the parsing procedure, and exhibits a high potential for parallelization of the computation."
C98-1083,Eliminative Parsing with Graded Constraints,1998,9,32,3,0,8201,johannes heinecke,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Natural language parsing is conceived to be a procedure of disambiguation, which successively reduces an initially totally ambiguous structural representation towards a single interpretation. Graded constraints are used as means to express well-formedness conditions of different strength and to decide which partial structures are locally least preferred and, hence, can be deleted. This approach facilitates a higher degree of robustness of the analysis, allows to introduce resource adaptivity into the parsing procedure, and exhibits a high potential for parallelization of the computation."
C90-3092,Anticipation-Free Diagnosis of Structural Faults,1990,6,4,1,1,5536,wolfgang menzel,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Current attempts to diagnose grammatical faults in natural language utterances are except for agreement errors and certain cases of overgeneralization and interference strongly based on the principles of error anticipation (cf. Yazdani 1988, Schwind 1988, Catt 1988): Rather tiny context free grammars are enhanced by some additional rules which describe selected faulty structures and invoke error messages zf they are needed for a successful parse. The efforts required to compile an at least approximatively comprehensive rule set even for simple domains o f grammar are considerable. Deszdes this, it is the student's risk to fall into the remalninq ~ap of neglected possibilities which seems to be difficult to avoid. Hopefully, an improvemeat of this situation can be achieved by an application of model-based reasoning procedures, where an internal model (of language correctness) is used to simulate and evaluate error hypotheses by investigating their consequences for other parts of the mode]. To a certain degree the diagnostic results are logically determined by the correct remainder of the utterance and useful results require a balanced ratio between correct and incorrect language use within the solutlon of the student. Provided a correct and covering model can be supplied for a limited domain, diagnosis is guaranteed to be precise and robust enouqh and error anticipation eventually may be renounced completely. In order to yield an efficient implementation of the idea into a practical solution a preponderantly data driven procedure instead of a strictly hypotheses driven one seems to be desirable. A procedure of this kind }]as been successfully pursued in an earlier paper on the diagnosis of agreement errors in fixed syntactic environments (Menzel 1 9 8 8 ) . Quite naturally this success raises the question on how much of the experience gathered can be transferred to other types of grammatical regularities as linear ordering principles or dominance regularities, for instance. Up to now the only notable exception to the one-sided orientation on error anticipation has been a fail-soft technique implemented in the error sensitive parsing system Linger (Barehan etal. 1986), an approach which later has been named word soup heuristics by their authors: Whenever the normal parsing process based on a principally anticipation-oriented context-free grammar fails, the system attempts to achieve a successful parse by trying single word form substitutions, insertions, deletions or displacements. Although often being very useful in detecting simple flaws of the student, this heuristics not so infrequently produces rather surprising and sometimes even funny interpretations of the input data. Its main drawback is the basic limitation to only single word form errors. Any extension to the handling of complete constituents, desirable as it may be, seems to be condemned to failure because of efficleney reasons: the whole approach is basically expectation driven and it opens up too vast a search space of possible error hypotheses, where the verification of onl~ a single one is not just a trivial task."
C88-2085,Error Diagnosing and Selection in a Training System for Second Language Learning,1988,4,15,1,1,5536,wolfgang menzel,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A diagnosing procedure to be used in intelligent systems for language instruction is presented. Based on a knowledge representation scheme for a certain class of syntactic correctness conditions the system carries out a thorough analysis of possible error hypotheses and their consequences. A comparison with earlier attempts shows a clearly improved precision of diagnostic results. First of all, the procedure concentrates on an exact localization of rule violations, but - if desired - is able to infer information about factual faults as well."
E87-1008,Automated Reasoning About Natural Language Correctness,1987,2,6,1,1,5536,wolfgang menzel,Third Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Automated Reasoning techniques applied to the problem of natural language correctness allow the design of flexible training aids for the teaching of foreign languages. The approach involves important advantages for both the student and the teacher by detecting possible errors and pointing out their reasons. Explanations may be given on four distinct levels, thus offering differently instructive error messages according to the needs of the student."
C82-2037,A Procedure of an Automatic Grapheme-to-Phoneme Transfornation of {G}erman,1982,0,1,2,0,58597,sabine koch,{C}oling 1982 Abstracts: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics Abstracts,0,"The automatic transformation of texts graphemically stored to the corresponding phonemic symbols will enable the speech synthesizer Rosy 4000 (.developed by VEB Robotron Dres~ den) to extend its field of application (application in information systems, development of reading machines for the blind). The texts for this kind of application cannot be limi~ ted in any way a fact which had to be taken into account concerning the methods suitable for such a procedure. The use of the d_%ctionary method, that means storing the whole vocabulary needed together with the corresponding phonemic strings was impossible for this purpose."
