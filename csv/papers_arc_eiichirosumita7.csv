2021.wnut-1.9,"A Text Editing Approach to Joint {J}apanese Word Segmentation, {POS} Tagging, and Lexical Normalization",2021,-1,-1,4,1,126,shohei higashiyama,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Lexical normalization, in addition to word segmentation and part-of-speech tagging, is a fundamental task for Japanese user-generated text processing. In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency. Our experiments showed that the proposed model achieved better normalization performance when trained on more diverse pseudo-labeled data."
2021.wat-1.4,{NICT}{'}s Neural Machine Translation Systems for the {WAT}21 Restricted Translation Task,2021,-1,-1,3,1,304,zuchao li,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper describes our system (Team ID: nictrb) for participating in the WAT{'}21 restricted machine translation task. In our submitted system, we designed a new training approach for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance."
2021.wat-1.8,{NICT}-2 Translation System at {WAT}-2021: Applying a Pretrained Multilingual Encoder-Decoder Model to Low-resource Language Pairs,2021,-1,-1,2,0,324,kenji imamura,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"In this paper, we present the NICT system (NICT-2) submitted to the NICT-SAP shared task at the 8th Workshop on Asian Translation (WAT-2021). A feature of our system is that we used a pretrained multilingual BART (Bidirectional and Auto-Regressive Transformer; mBART) model. Because publicly available models do not support some languages in the NICT-SAP task, we added these languages to the mBART model and then trained it using monolingual corpora extracted from Wikipedia. We fine-tuned the expanded mBART model using the parallel corpora specified by the NICT-SAP task. The BLEU scores greatly improved in comparison with those of systems without the pretrained model, including the additional languages."
2021.naacl-main.311,Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios,2021,-1,-1,5,1,4177,haipeng sun,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems."
2021.naacl-main.438,User-Generated Text Corpus for Evaluating {J}apanese Morphological Analysis and Lexical Normalization,2021,-1,-1,4,1,126,shohei higashiyama,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT."
2021.emnlp-main.261,Unsupervised Neural Machine Translation with Universal Grammar,2021,-1,-1,3,1,304,zuchao li,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky{'}s Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints. Therefore, in this paper, we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models. Through experiments on multiple typical language pairs, we demonstrate the effectiveness of our proposed approaches."
2021.emnlp-main.299,Smoothing Dialogue States for Open Conversational Machine Reading,2021,-1,-1,5,0.45977,6857,zhuosheng zhang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would be noisy, which results in severe challenges in the information transmission. Existing studies commonly train independent or pipeline systems for the two subtasks. However, those methods are trivial by using hard-label decisions to activate question generation, which eventually hinders the model performance. In this work, we propose an effective gating strategy by smoothing the two dialogue states in only one decoder and bridge decision making and question generation to provide a richer dialogue state reference. Experiments on the OR-ShARC dataset show the effectiveness of our method, which achieves new state-of-the-art results."
2021.emnlp-demo.1,{M}i{SS}: An Assistant for Multi-Style Simultaneous Translation,2021,-1,-1,4,1,304,zuchao li,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"In this paper, we present \textbf{MiSS}, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and grammatical error correction. With this system, we aim to provide a complete translation experience for machine translation users. Our design goals are high translation accuracy, real-time translation, flexibility, and measurable translation quality. Compared with the free commercial translation systems commonly used, our translation assistance system regards the machine translation application as a more complete and fully-featured tool for users. By incorporating additional features and giving the user better control over their experience, we improve translation efficiency and performance. Additionally, our assistant system combines machine translation, grammatical error correction, and interactive edits, and uses a crowdsourcing mode to collect more data for further training to improve both the machine translation and grammatical error correction models. A short video demonstrating our system is available at \url{https://www.youtube.com/watch?v=ZGCo7KtRKd8}."
2020.wmt-1.22,{SJTU}-{NICT}{'}s Supervised and Unsupervised Neural Machine Translation Systems for the {WMT}20 News Translation Task,2020,-1,-1,6,1,304,zuchao li,Proceedings of the Fifth Conference on Machine Translation,0,"In this paper, we introduced our joint team SJTU-NICT {`}s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions."
2020.wat-1.3,Transformer-based Double-token Bidirectional Autoregressive Decoding in Neural Machine Translation,2020,-1,-1,2,0,324,kenji imamura,Proceedings of the 7th Workshop on Asian Translation,0,"This paper presents a simple method that extends a standard Transformer-based autoregressive decoder, to speed up decoding. The proposed method generates a token from the head and tail of a sentence (two tokens in total) in each step. By simultaneously generating multiple tokens that rarely depend on each other, the decoding speed is increased while the degradation in translation quality is minimized. In our experiments, the proposed method increased the translation speed by around 113{\%}-155{\%} in comparison with a standard autoregressive decoder, while degrading the BLEU scores by no more than 1.03. It was faster than an iterative non-autoregressive decoder in many conditions."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,21,0,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.364,A {M}yanmar ({B}urmese)-{E}nglish Named Entity Transliteration Dictionary,2020,-1,-1,6,0,17404,aye mon,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Transliteration is generally a phonetically based transcription across different writing systems. It is a crucial task for various downstream natural language processing applications. For the Myanmar (Burmese) language, robust automatic transliteration for borrowed English words is a challenging task because of the complex Myanmar writing system and the lack of data. In this study, we constructed a Myanmar-English named entity dictionary containing more than eighty thousand transliteration instances. The data have been released under a CC BY-NC-SA license. We evaluated the automatic transliteration performance using statistical and neural network-based approaches based on the prepared data. The neural network model outperformed the statistical model significantly in terms of the BLEU score on the character level. Different units used in the Myanmar script for processing were also compared and discussed."
2020.findings-emnlp.371,Reference Language based Unsupervised Neural Machine Translation,2020,28,0,5,1,304,zuchao li,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a source language to target language parallel corpus. The rise of unsupervised neural machine translation (UNMT) almost completely relieves the parallel corpus curse, though UNMT is still subject to unsatisfactory performance due to the vagueness of the clues available for its core back-translation training. Further enriching the idea of pivot translation by extending the use of parallel corpora beyond the source-target paradigm, we propose a new reference language-based framework for UNMT, RUNMT, in which the reference language only shares a parallel corpus with the source, but this corpus still indicates a signal clear enough to help the reconstruction training of UNMT through a proposed reference agreement mechanism. Experimental results show that our methods improve the quality of UNMT over that of a strong baseline that uses only one auxiliary language, demonstrating the usefulness of the proposed reference language-based UNMT and establishing a good start for the community."
2020.coling-main.374,Robust Unsupervised Neural Machine Translation with Adversarial Denoising Training,2020,27,1,6,1,4177,haipeng sun,Proceedings of the 28th International Conference on Computational Linguistics,0,"Unsupervised neural machine translation (UNMT) has recently attracted great interest in the machine translation community. The main advantage of the UNMT lies in its easy collection of required large training text sentences while with only a slightly worse performance than supervised neural machine translation which requires expensive annotated translation pairs on some translation tasks. In most studies, the UMNT is trained with clean data without considering its robustness to the noisy data. However, in real-world scenarios, there usually exists noise in the collected input sentences which degrades the performance of the translation system since the UNMT is sensitive to the small perturbations of the input sentences. In this paper, we first time explicitly take the noisy data into consideration to improve the robustness of the UNMT based systems. First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios."
2020.coling-main.376,Improving Low-Resource {NMT} through Relevance Based Linguistic Features Incorporation,2020,-1,-1,5,0,373,abhisek chakrabarty,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation."
2020.coling-main.378,Bilingual Subword Segmentation for Neural Machine Translation,2020,-1,-1,5,0,12486,hiroyuki deguchi,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper proposed a new subword segmentation method for neural machine translation, {``}Bilingual Subword Segmentation,{''} which tokenizes sentences to minimize the difference between the number of subword units in a sentence and that of its translation. While existing subword segmentation methods tokenize a sentence without considering its translation, the proposed method tokenizes a sentence by using subword units induced from bilingual sentences; this method could be more favorable to machine translation. Evaluations on WAT Asian Scientific Paper Excerpt Corpus (ASPEC) English-to-Japanese and Japanese-to-English translation tasks and WMT14 English-to-German and German-to-English translation tasks show that our bilingual subword segmentation improves the performance of Transformer neural machine translation (up to +0.81 BLEU)."
2020.coling-main.385,Intermediate Self-supervised Learning for Machine Translation Quality Estimation,2020,-1,-1,2,0,8609,raphael rubino,Proceedings of the 28th International Conference on Computational Linguistics,0,"Pre-training sentence encoders is effective in many natural language processing tasks including machine translation (MT) quality estimation (QE), due partly to the scarcity of annotated QE data required for supervised learning. In this paper, we investigate the use of an intermediate self-supervised learning task for sentence encoder aiming at improving QE performances at the sentence and word levels. Our approach is motivated by a problem inherent to QE: mistakes in translation caused by wrongly inserted and deleted tokens. We modify the translation language model (TLM) training objective of the cross-lingual language model (XLM) to orientate the pre-trained model towards the target task. The proposed method does not rely on annotated data and is complementary to QE methods involving pre-trained sentence encoders and domain adaptation. Experiments on English-to-German and English-to-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders."
2020.acl-srw.37,Pre-training via Leveraging Assisting Languages for Neural Machine Translation,2020,17,0,6,0,12440,haiyue song,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios."
2020.acl-main.34,Content Word Aware Neural Machine Translation,2020,-1,-1,4,1,4178,kehai chen,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT."
2020.acl-main.44,A Three-Parameter Rank-Frequency Relation in Natural Languages,2020,-1,-1,3,1,285,chenchen ding,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present that, the rank-frequency relation in textual data follows $f \propto r^{-\alpha}(r+\gamma)^{-\beta}$, where $f$ is the token frequency and $r$ is the rank by frequency, with ($\alpha$, $\beta$, $\gamma$) as parameters. The formulation is derived based on the empirical observation that $d^2 (x+y)/dx^2$ is a typical impulse function, where $(x,y)=(\log r, \log f)$. The formulation is the power law when $\beta=0$ and the Zipf{--}Mandelbrot law when $\alpha=0$. We illustrate that $\alpha$ is related to the analytic features of syntax and $\beta+\gamma$ to those of morphology in natural languages from an investigation of multilingual corpora."
2020.acl-main.324,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,2020,37,1,5,1,4177,haipeng sun,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs."
W19-7201,Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation,2019,23,0,3,0,1478,junya ono,Proceedings of The 8th Workshop on Patent and Scientific Literature Translation,0,"Reduction of training time is an important issue in many tasks like patent translation involving neural networks. Data parallelism and model parallelism are two common approaches for reducing training time using multiple graphics processing units (GPUs) on one machine. In this paper, we propose a hybrid data-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent neural network (RNN) machine translation. We apply a model parallel approach to the RNN encoder-decoder part of the Seq2Seq model and a data parallel approach to the attention-softmax part of the model. We achieved a speed-up of 4.13 to 4.20 times when using 4 GPUs compared with the training speed when using 1 GPU without affecting machine translation accuracy as measured in terms of BLEU scores."
W19-6601,Online Sentence Segmentation for Simultaneous Interpretation using Multi-Shifted Recurrent Neural Network,2019,0,0,3,1,23611,xiaolin wang,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5313,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 News Translation Task,2019,0,2,7,0.873368,286,raj dabre,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for KazakhâEnglish, GujaratiâEnglish, ChineseâEnglish, and EnglishâFinnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: KazakhâEnglish and GujaratiâEnglish translation. For the ChineseâEnglish translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of ChineseâEnglish. For EnglishâFinnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year{'}s task."
W19-5330,{NICT}{'}s Unsupervised Neural and Statistical Machine Translation Systems for the {WMT}19 News Translation Task,2019,0,4,7,0.762156,8610,benjamin marie,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper presents the NICT{'}s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction: German-Czech. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers ({``}constraint{'}{''}), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions."
W19-5362,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task,2019,0,2,2,0.873368,286,raj dabre,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper we describe our neural machine translation (NMT) systems for JapaneseâEnglish translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for JapaneseâEnglish. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest."
P19-1119,Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation,2019,0,7,5,1,4177,haipeng sun,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT."
P19-1174,Neural Machine Translation with Reordering Embeddings,2019,0,5,4,1,4178,kehai chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT{'}14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer."
P19-1296,Sentence-Level Agreement for Neural Machine Translation,2019,0,3,5,0,25711,mingming yang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance."
N19-1205,Improving Neural Machine Translation with Neural Syntactic Distance,2019,0,3,4,1,10519,chunpeng ma,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The explicit use of syntactic information has been proved useful for neural machine translation (NMT). However, previous methods resort to either tree-structured neural networks or long linearized sequences, both of which are inefficient. Neural syntactic distance (NSD) enables us to represent a constituent tree using a sequence whose length is identical to the number of words in the sentence. NSD has been used for constituent parsing, but not in machine translation. We propose five strategies to improve NMT with NSD. Experiments show that it is not trivial to improve NMT with NSD; however, the proposed strategies are shown to improve translation performance of the baseline model (+2.1 (En{--}Ja), +1.3 (Ja{--}En), +1.2 (En{--}Ch), and +1.0 (Ch{--}En) BLEU)."
N19-1276,Incorporating Word Attention into Character-Based Word Segmentation,2019,0,1,3,1,126,shohei higashiyama,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Neural network models have been actively applied to word segmentation, especially Chinese, because of the ability to minimize the effort in feature engineering. Typical segmentation models are categorized as character-based, for conducting exact inference, or word-based, for utilizing word-level information. We propose a character-based model utilizing word information to leverage the advantages of both types of models. Our model learns the importance of multiple candidate words for a character on the basis of an attention mechanism, and makes use of it for segmentation decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets."
K19-2004,{SJTU}-{NICT} at {MRP} 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing,2019,0,0,6,1,304,zuchao li,Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning,0,"This paper describes our SJTU-NICT{'}s system for participating in the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). Our system uses a graph-based approach to model a variety of semantic graph parsing tasks. Our main contributions in the submitted system are summarized as follows: 1. Our model is fully end-to-end and is capable of being trained only on the given training set which does not rely on any other extra training source including the companion data provided by the organizer; 2. We extend our graph pruning algorithm to a variety of semantic graphs, solving the problem of excessive semantic graph search space; 3. We introduce multi-task learning for multiple objectives within the same framework. The evaluation results show that our system achieved second place in the overall $F_1$ score and achieved the best $F_1$ score on the DM framework."
D19-5603,Recycling a Pre-trained {BERT} Encoder for Neural Machine Translation,2019,0,2,2,0,324,kenji imamura,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same model without pre-training. Additionally, we confirmed that NMT with the BERT encoder is more effective in low-resource settings."
D19-5206,Supervised and Unsupervised Machine Translation for {M}yanmar-{E}nglish and {K}hmer-{E}nglish,2019,0,0,7,0.762156,8610,benjamin marie,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the NICT{'}s supervised and unsupervised machine translation systems for the WAT2019 Myanmar-English and Khmer-English translation tasks. For all the translation directions, we built state-of-the-art supervised neural (NMT) and statistical (SMT) machine translation systems, using monolingual data cleaned and normalized. Our combination of NMT and SMT performed among the best systems for the four translation directions. We also investigated the feasibility of unsupervised machine translation for low-resource and distant language pairs and confirmed observations of previous work showing that unsupervised MT is still largely unable to deal with them."
D19-5207,{NICT}{'}s participation to {WAT} 2019: Multilingualism and Multi-step Fine-Tuning for Low Resource {NMT},2019,0,0,2,0.873368,286,raj dabre,Proceedings of the 6th Workshop on Asian Translation,0,"In this paper we describe our submissions to WAT 2019 for the following tasks: English{--}Tamil translation and Russian{--}Japanese translation. Our team,{``}NICT-5{''}, focused on multilingual domain adaptation and back-translation for Russian{--}Japanese translation and on simple fine-tuning for English{--}Tamil translation . We noted that multi-stage fine tuning is essential in leveraging the power of multilingualism for an extremely low-resource language like Russian{--}Japanese. Furthermore, we can improve the performance of such a low-resource language pair by exploiting a small but in-domain monolingual corpus via back-translation. We managed to obtain second rank in both tasks for all translation directions."
D19-5209,{E}nglish-{M}yanmar Supervised and Unsupervised {NMT}: {NICT}{'}s Machine Translation Systems at {WAT}-2019,2019,0,0,6,0,3690,rui wang,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the NICT{'}s participation (team ID: NICT) in the 6th Workshop on Asian Translation (WAT-2019) shared translation task, specifically Myanmar (Burmese) - English task in both translation directions. We built neural machine translation (NMT) systems for these tasks. Our NMT systems were trained with language model pretraining. Back-translation technology is adopted to NMT. Our NMT systems rank the third in English-to-Myanmar and the second in Myanmar-to-English according to BLEU score."
D19-5217,Long Warm-up and Self-Training: Training Strategies of {NICT}-2 {NMT} System at {WAT}-2019,2019,0,0,2,0,324,kenji imamura,Proceedings of the 6th Workshop on Asian Translation,0,"This paper describes the NICT-2 neural machine translation system at the 6th Workshop on Asian Translation. This system employs the standard Transformer model but features the following two characteristics. One is the long warm-up strategy, which performs a longer warm-up of the learning rate at the start of the training than conventional approaches. Another is that the system introduces self-training approaches based on multiple back-translations generated by sampling. We participated in three tasks{---}ASPEC.en-ja, ASPEC.ja-en, and TDDC.ja-en{---}using this system."
D19-3027,{MY}-{AKKHARA}: A {R}omanization-based {B}urmese ({M}yanmar) Input Method,2019,0,1,3,1,285,chenchen ding,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"MY-AKKHARA is a method used to input Burmese texts encoded in the Unicode standard, based on commonly accepted Latin transcription. By using this method, arbitrary Burmese strings can be accurately inputted with 26 lowercase Latin letters. Meanwhile, the 26 uppercase Latin letters are designed as shortcuts of lowercase letter sequences. The frequency of Burmese characters is considered in MY-AKKHARA to realize an efficient keystroke distribution on a QWERTY keyboard. Given that the Unicode standard has not been extensively used in digitization of Burmese, we hope that MY-AKKHARA can contribute to the widespread use of Unicode in Myanmar and can provide a platform for smart input methods for Burmese in the future. An implementation of MY-AKKHARA running in Windows is released at http://www2.nict.go.jp/astrec-att/member/ding/my-akkhara.html"
D19-1139,Recurrent Positional Embedding for Neural Machine Translation,2019,0,2,4,1,4178,kehai chen,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In the Transformer network architecture, positional embeddings are used to encode order dependencies into the input representation. However, this input representation only involves static order dependencies based on discrete numerical information, that is, are independent of word content. To address this issue, this work proposes a recurrent positional embedding approach based on word vector. In this approach, these recurrent positional embeddings are learned by a recurrent neural network, encoding word content-based order dependencies into the input representation. They are then integrated into the existing multi-head self-attention model as independent heads or part of each head. The experimental results revealed that the proposed approach improved translation performance over that of the state-of-the-art Transformer baseline in WMT{'}14 English-to-German and NIST Chinese-to-English translation tasks."
Y18-3003,{NICT}{'}s Participation in {WAT} 2018: Approaches Using Multilingualism and Recurrently Stacked Layers,2018,0,3,4,0.873368,286,raj dabre,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-3006,{E}nglish-{M}yanmar {NMT} and {SMT} with Pre-ordering: {NICT}{'}s Machine Translation Systems at {WAT}-2018,2018,0,0,4,0,3690,rui wang,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-3007,Combination of Statistical and Neural Machine Translation for {M}yanmar-{E}nglish,2018,0,0,3,0.762156,8610,benjamin marie,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
W18-6419,{NICT}{'}s Neural and Statistical Machine Translation Systems for the {WMT}18 News Translation Task,2018,0,0,5,0.762156,8610,benjamin marie,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper presents the NICT{'}s participation to the WMT18 shared news translation task. We participated in the eight translation directions of four language pairs: Estonian-English, Finnish-English, Turkish-English and Chinese-English. For each translation direction, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems were trained with the transformer architecture using the provided parallel data enlarged with a large quantity of back-translated monolingual data that we generated with a new incremental training framework. Our primary submissions to the task are the result of a simple combination of our SMT and NMT systems. Our systems are ranked first for the Estonian-English and Finnish-English language pairs (constraint) according to BLEU-cased."
W18-6489,{NICT}{'}s Corpus Filtering Systems for the {WMT}18 Parallel Corpus Filtering Task,2018,6,0,4,0,3690,rui wang,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper presents the NICT{'}s participation in the WMT18 shared parallel corpus filtering task. The organizers provided 1 billion words German-English corpus crawled from the web as part of the Paracrawl project. This corpus is too noisy to build an acceptable neural machine translation (NMT) system. Using the clean data of the WMT18 shared news translation task, we designed several features and trained a classifier to score each sentence pairs in the noisy data. Finally, we sampled 100 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance."
W18-2707,Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation,2018,0,12,3,0,324,kenji imamura,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling. By using multiple source sentences, diversity close to that of humans is achieved. Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved."
W18-2713,{NICT} Self-Training Approach to Neural Machine Translation at {NMT}-2018,2018,0,2,2,0,324,kenji imamura,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"This paper describes the NICT neural machine translation system submitted at the NMT-2018 shared task. A characteristic of our approach is the introduction of self-training. Since our self-training does not change the model structure, it does not influence the efficiency of translation, such as the translation speed. The experimental results showed that the translation quality improved not only in the sequence-to-sequence (seq-to-seq) models but also in the transformer models."
P18-2048,Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation,2018,17,0,3,0,3690,rui wang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance."
P18-2078,Simplified Abugidas,2018,0,0,3,1,285,chenchen ding,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"An abugida is a writing system where the consonant letters represent syllables with a default vowel and other vowels are denoted by diacritics. We investigate the feasibility of recovering the original text written in an abugida after omitting subordinate diacritics and merging consonant letters with similar phonetic values. This is crucial for developing more efficient input methods by reducing the complexity in abugidas. Four abugidas in the southern Brahmic family, i.e., Thai, Burmese, Khmer, and Lao, were studied using a newswire 20,000-sentence dataset. We compared the recovery performance of a support vector machine and an LSTM-based recurrent neural network, finding that the abugida graphemes could be recovered with 94{\%} - 97{\%} accuracy at the top-1 level and 98{\%} - 99{\%} at the top-4 level, even after omitting most diacritics (10 - 30 types) and merging the remaining 30 - 50 characters into 21 graphemes."
P18-1116,Forest-Based Neural Machine Translation,2018,0,1,5,1,10519,chunpeng ma,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems."
L18-1545,Multilingual Parallel Corpus for Global Communication Plan,2018,0,1,2,0,324,kenji imamura,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-2023,{C}yton{MT}: an Efficient Neural Machine Translation Open-source Toolkit Implemented in {C}++,2018,18,0,3,1,23611,xiaolin wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"This paper presents an open-source neural machine translation toolkit named CytonMT. The toolkit is built from scratch only using C++ and NVIDIA{'}s GPU-accelerated libraries. The toolkit features training efficiency, code simplicity and translation quality. Benchmarks show that cytonMT accelerates the training speed by 64.5{\%} to 110.8{\%} on neural networks of various sizes, and achieves competitive translation quality."
D18-1511,Exploring Recombination for Efficient Decoding of Neural Machine Translation,2018,0,9,4,0,1041,zhisong zhang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recombination in NMT decoding based on the concept of the {``}equivalence{''} of partial hypotheses. Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient."
W17-5705,{J}apanese to {E}nglish/{C}hinese/{K}orean Datasets for Translation Quality Estimation and Automatic Post-Editing,2017,19,0,2,0.272516,5049,atsushi fujita,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"Aiming at facilitating the research on quality estimation (QE) and automatic post-editing (APE) of machine translation (MT) outputs, especially for those among Asian languages, we have created new datasets for Japanese to English, Chinese, and Korean translations. As the source text, actual utterances in Japanese were extracted from the log data of our speech translation service. MT outputs were then given by phrase-based statistical MT systems. Finally, human evaluators were employed to grade the quality of MT outputs and to post-edit them. This paper describes the characteristics of the created datasets and reports on our benchmarking experiments on word-level QE, sentence-level QE, and APE conducted using the created datasets."
W17-5711,Ensemble and Reranking: Using Multiple Models in the {NICT}-2 Neural Machine Translation System at {WAT}2017,2017,0,0,2,0,324,kenji imamura,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"In this paper, we describe the NICT-2 neural machine translation system evaluated at WAT2017. This system uses multiple models as an ensemble and combines models with opposite decoding directions by reranking (called bi-directional reranking). In our experimental results on small data sets, the translation quality improved when the number of models was increased to 32 in total and did not saturate. In the experiments on large data sets, improvements of 1.59-3.32 BLEU points were achieved when six-model ensembles were combined by the bi-directional reranking."
W17-5712,A Simple and Strong Baseline: {NAIST}-{NICT} Neural Machine Translation System for {WAT}2017 {E}nglish-{J}apanese Translation Task,2017,0,0,5,0,296,yusuke oda,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture."
P17-2089,Sentence Embedding for Neural Machine Translation Domain Adaptation,2017,9,26,4,0.206742,3690,rui wang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT{'}s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points."
I17-2049,Key-value Attention Mechanism for Neural Machine Translation,2017,12,3,3,1,287,hideya mino,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this paper, we propose a neural machine translation (NMT) with a key-value attention mechanism on the source-side encoder. The key-value attention mechanism separates the source-side content vector into two types of memory known as the key and the value. The key is used for calculating the attention distribution, and the value is used for encoding the context representation. Experiments on three different tasks indicate that our model outperforms an NMT model with a conventional attention mechanism. Furthermore, we perform experiments with a conventional NMT framework, in which a part of the initial value of a weight matrix is set to zero so that the matrix is as the same initial-state as the key-value attention mechanism. As a result, we obtain comparable results with the key-value attention mechanism without changing the network structure."
I17-1002,Context-Aware Smoothing for Neural Machine Translation,2017,12,2,4,1,4178,kehai chen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In Neural Machine Translation (NMT), each word is represented as a low-dimension, real-value vector for encoding its syntax and semantic information. This means that even if the word is in a different sentence context, it is represented as the fixed vector to learn source representation. Moreover, a large number of Out-Of-Vocabulary (OOV) words, which have different syntax and semantic information, are represented as the same vector representation of {``}unk{''}. To alleviate this problem, we propose a novel context-aware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems."
D17-1155,Instance Weighting for Neural Machine Translation Domain Adaptation,2017,7,38,5,0.206742,3690,rui wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points."
D17-1304,Neural Machine Translation with Source Dependency Representation,2017,14,17,6,1,4178,kehai chen,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel NMT with source dependency representation to improve translation performance of NMT, especially long sentences. Empirical results on NIST Chinese-to-English translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system."
W16-4606,Global Pre-ordering for Improving Sublanguage Translation,2016,14,2,3,1,33573,masaru fuji,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations. This paper proposes a novel global reordering method with particular focus on long-distance reordering for capturing the global sentence structure of a sublanguage. The proposed method learns global reordering models from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations."
W16-4611,{NICT}-2 Translation System for {WAT}2016: Applying Domain Adaptation to Phrase-based Statistical Machine Translation,2016,9,0,2,0,324,kenji imamura,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper describes the NICT-2 translation system for the 3rd Workshop on Asian Translation. The proposed system employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality."
W16-4613,An Efficient and Effective Online Sentence Segmenter for Simultaneous Interpretation,2016,0,1,4,1,23611,xiaolin wang,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"Simultaneous interpretation is a very challenging application of machine translation in which the input is a stream of words from a speech recognition engine. The key problem is how to segment the stream in an online manner into units suitable for translation. The segmentation process proceeds by calculating a confidence score for each word that indicates the soundness of placing a sentence boundary after it, and then heuristics are employed to determine the position of the boundaries. Multiple variants of the confidence scoring method and segmentation heuristics were studied. Experimental results show that the best performing strategy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation."
W16-4614,Similar {S}outheast {A}sian Languages: Corpus-Based Case Study on {T}hai-{L}aotian and {M}alay-{I}ndonesian,2016,1,1,3,1,285,chenchen ding,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper illustrates the similarity between Thai and Laotian, and between Malay and Indonesian, based on an investigation on raw parallel data from Asian Language Treebank. The cross-lingual similarity is investigated and demonstrated on metrics of correspondence and order of tokens, based on several standard statistical machine translation techniques. The similarity shown in this study suggests a possibility on harmonious annotation and processing of the language pairs in future development."
W16-2711,Target-Bidirectional Neural Models for Machine Transliteration,2016,0,10,4,0,16459,andrew finch,Proceedings of the Sixth Named Entity Workshop,0,None
P16-1120,Bilingual Segmented Topic Model,2016,21,1,2,1,1486,akihiro tamura,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1046,Agreement on Target-bidirectional Neural Machine Translation,2016,19,22,4,1,3591,lemao liu,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
N16-1124,Interlocking Phrases in Phrase-based Statistical Machine Translation,2016,16,0,3,1,312,ye thu,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper presents an study of the use of interlocking phrases in phrase-based statistical machine translation. We examine the effect on translation quality when the translation units used in the translation hypotheses are allowed to overlap on the source side, on the target side and on both sides. A large-scale evaluation on 380 language pairs was conducted. Our results show that overall the use of overlapping phrases improved translation quality by 0.3 BLEU points on average. Further analysis revealed that language pairs requiring a larger amount of re-ordering benefited the most from our approach. When the evaluation was restricted to such pairs, the average improvement increased to up to 0.75 BLEU points with over 97% of the pairs improving. Our approach requires only a simple modification to the decoding algorithm and we believe it should be generally applicable to improve the performance of phrase-based decoders."
L16-1249,Introducing the {A}sian Language Treebank ({ALT}),2016,0,15,5,1,312,ye thu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper introduces the ALT project initiated by the Advanced Speech Translation Research and Development Promotion Center (ASTREC), NICT, Kyoto, Japan. The aim of this project is to accelerate NLP research for Asian languages such as Indonesian, Japanese, Khmer, Laos, Malay, Myanmar, Philippine, Thai and Vietnamese. The original resource for this project was English articles that were randomly selected from Wikinews. The project has so far created a corpus for Myanmar and will extend in scope to include other languages in the near future. A 20000-sentence corpus of Myanmar that has been manually translated from an English corpus has been word segmented, word aligned, part-of-speech tagged and constituency parsed by human annotators. In this paper, we present the implementation steps for creating the treebank in detail, including a description of the ALT web-based treebanking tool. Moreover, we report statistics on the annotation quality of the Myanmar treebank created so far."
L16-1350,{ASPEC}: {A}sian Scientific Paper Excerpt Corpus,2016,0,24,5,0.15787,283,toshiaki nakazawa,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we describe the details of the ASPEC (Asian Scientific Paper Excerpt Corpus), which is the first large-size parallel corpus of scientific paper domain. ASPEC was constructed in the Japanese-Chinese machine translation project conducted between 2006 and 2010 using the Special Coordination Funds for Promoting Science and Technology. It consists of a Japanese-English scientific paper abstract corpus of approximately 3 million parallel sentences (ASPEC-JE) and a Chinese-Japanese scientific paper excerpt corpus of approximately 0.68 million parallel sentences (ASPEC-JC). ASPEC is used as the official dataset for the machine translation evaluation workshop WAT (Workshop on Asian Translation)."
D16-1210,Unsupervised Word Alignment by Agreement Under {ITG} Constraint,2016,25,3,5,1,3633,hidetaka kamigaito,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-2007,A Prototype Automatic Simultaneous Interpretation System,2016,4,1,4,1,23611,xiaolin wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"Simultaneous interpretation allows people to communicate spontaneously across language boundaries, but such services are prohibitively expensive for the general public. This paper presents a fully automatic simultaneous interpretation system to address this problem. Though the development is still at an early stage, the system is capable of keeping up with the fastest of the TED speakers while at the same time delivering high-quality translations. We believe that the system will become an effective tool for facilitating cross-lingual communication in the future."
C16-2008,{M}u{TUAL}: A Controlled Authoring Support System Enabling Contextual Machine Translation,2016,3,0,6,0,10715,rei miyata,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"The paper introduces a web-based authoring support system, MuTUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system."
C16-1291,Neural Machine Translation with Supervised Attention,2016,28,24,4,1,3591,lemao liu,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The attention mechanism is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in alignment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT."
C16-1295,Connecting Phrase based Statistical Machine Translation Adaptation,2016,26,1,5,0.206742,3690,rui wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Although more additional corpora are now available for Statistical Machine Translation (SMT), only the ones which belong to the same or similar domains of the original corpus can indeed enhance SMT performance directly. A series of SMT adaptation methods have been proposed to select these similar-domain data, and most of them focus on sentence selection. In comparison, phrase is a smaller and more fine grained unit for data selection, therefore we propose a straightforward and efficient connecting phrase based adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram adaptation. The proposed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods)."
2016.amta-researchers.7,Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation,2016,-1,-1,2,0,324,kenji imamura,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"Domain adaptation is a major challenge when applying machine translation to practical tasks. In this paper, we present domain adaptation methods for machine translation that assume multiple domains. The proposed methods combine two model types: a corpus-concatenated model covering multiple domains and single-domain models that are accurate but sparse in specific domains. We combine the advantages of both models using feature augmentation for domain adaptation in machine learning. Our experimental results show that the BLEU scores of the proposed method clearly surpass those of single-domain models for low-resource domains. For high-resource domains, the scores of the proposed method were superior to those of both single-domain and corpusconcatenated models. Even in domains having a million bilingual sentences, the translation quality was at least preserved and even improved in some domains. These results demonstrate that state-of-the-art domain adaptation can be realized with appropriate settings, even when using standard log-linear models."
Y15-1030,A Large-scale Study of Statistical Machine Translation Methods for {K}hmer Language,2015,18,0,5,1,312,ye thu,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"This paper contributes the first published evaluation of the quality of automatic translation between Khmer (the official language of Cambodia) and twenty other languages, in both directions. The experiments were carried out using three different statistical machine translation approaches: phrase-based, hierarchical phrase-based, and the operation sequence model (OSM). In addition two different segmentation schemes for Khmer were studied, these were syllable segmentation and supervised word segmentation. The results show that the highest quality machine translation was attained with word segmentation in all of the experiments. Furthermore, with the exception of very distant language pairs the OSM approach gave the highest quality translations when measured in terms of both the BLEU and RIBES scores. For distant languages, our results showed a hierarchical phrase-based approach to be the most effective. An analysis of the experimental results indicated that Kendallxe2x80x99s tau may be directly used as a means of selecting an appropriate machine translation approach for a given language pair."
W15-5001,Overview of the 2nd Workshop on {A}sian Translation,2015,-1,-1,6,0.20857,283,toshiaki nakazawa,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
W15-5004,{NICT} at {WAT} 2015,2015,-1,-1,3,1,285,chenchen ding,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
W15-3909,Neural Network Transduction Models in Transliteration Generation,2015,19,12,4,0,16459,andrew finch,Proceedings of the Fifth Named Entity Workshop,0,"In this paper we examine the effectiveness of neural network sequence-to-sequence transduction in the task of transliteration generation. In this yearxe2x80x99s shared evaluation we submitted two systems into all tasks. The primary system was based on the system used for the NEWS 2012 workshop, but was augmented with an additional feature which was the generation probability from a neural network. The secondary system was the neural network model used on its own together with a simple beam search algorithm. Our results show that adding the neural network score as a feature into the phrase-based statistical machine transliteration system was able to increase the performance of the system. In addition, although the neural network alone was not able to match the performance of our primary system (which exploits it), it was able to deliver a respectable performance for most language pairs which is very promising considering the recency of this technique."
P15-1113,Transition-based Neural Constituent Parsing,2015,44,23,2,0,128,taro watanabe,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters."
D15-1119,Improving fast{\\_}align by Reordering,2015,18,2,3,1,285,chenchen ding,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"fast align is a simple, fast, and efficient approach for word alignment based on the IBM model 2. fast align performs well for language pairs with relatively similar word orders; however, it does not perform well for language pairs with drastically different word orders. We propose a segmenting-reversing reordering process to solve this problem by alternately applying fast align and reordering source sentences during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported."
D15-1128,Hierarchical Phrase-based Stream Decoding,2015,12,0,4,0,16459,andrew finch,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a method for hierarchical phrase-based stream decoding. A stream decoder is able to take a continuous stream of tokens as input, and segments this stream into word sequences that are translated and output as a stream of target word sequences. Phrase-based stream decoding techniques have been shown to be effective as a means of simultaneous interpretation. In this paper we transfer the essence of this idea into the framework of hierarchical machine translation. The hierarchical decoding framework organizes the decoding process into a chart; this structure is naturally suited to the process of stream decoding, leading to an efficient stream decoding algorithm that searches a restricted subspace containing only relevant hypotheses. Furthermore, the decoder allows more explicit access to the word re-ordering process that is of critical importance in decoding while interpreting. The decoder was evaluated on TED talk data for English-Spanish and English-Chinese. Our results show that like the phrase-based stream decoder, the hierarchical is capable of approaching the performance of the underlying hierarchical phrase-based machine translation decoder, at useful levels of latency. In addition the hierarchical approach appeared to be robust to the difficulties presented by the more challenging English-Chinese task."
D15-1143,Hierarchical Back-off Modeling of {H}iero Grammar based on Non-parametric {B}ayesian Model,2015,32,1,5,1,3633,hidetaka kamigaito,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus. As a result, spuriously many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/JapaneseEnglish. When compared against heuristic models, our model achieved comparable translation quality on a full size GermanEnglish language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model."
D15-1209,Leave-one-out Word Alignment without Garbage Collector Effects,2015,29,4,5,1,23611,xiaolin wang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Expectation-maximization algorithms, such as those implemented in GIZA pervade the field of unsupervised word alignment. However, these algorithms have a problem of over-fitting, leading to xe2x80x9cgarbage collector effects,xe2x80x9d where rare words tend to be erroneously aligned to untranslated words. This paper proposes a leave-one-out expectationmaximization algorithm for unsupervised word alignment to address this problem. The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it. This prevents erroneous alignments within a sentence pair from supporting themselves. Experimental results on Chinese-English and Japanese-English corpora show that the F1, precision and recall of alignment were consistently increased by 5.0% xe2x80x90 17.2%, and BLEU scores of end-to-end translation were raised by 0.03 xe2x80x90 1.30. The proposed method also outperformed l0-normalized GIZA and Kneser-Ney smoothed GIZA."
D15-1250,A Binarized Neural Network Joint Model for Machine Translation,2015,17,2,3,1,12715,jingyi zhang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks."
2015.mtsummit-papers.1,Patent claim translation based on sublanguage-specific sentence structure,2015,-1,-1,4,1,33573,masaru fuji,Proceedings of Machine Translation Summit XV: Papers,0,None
2015.mtsummit-papers.4,Learning bilingual phrase representations with recurrent neural networks,2015,-1,-1,3,1,287,hideya mino,Proceedings of Machine Translation Summit XV: Papers,0,None
2015.iwslt-papers.17,Risk-aware distribution of {SMT} outputs for translation of documents targeting many anonymous readers,2015,-1,-1,3,0,260,yo ehara,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
W14-7001,Overview of the 1st Workshop on {A}sian Translation,2014,26,27,5,0.20857,283,toshiaki nakazawa,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
W14-7011,Word Order Does {NOT} Differ Significantly Between {C}hinese and {J}apanese,2014,-1,-1,3,1,285,chenchen ding,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
W14-5503,Integrating Dictionaries into an Unsupervised Model for {M}yanmar Word Segmentation,2014,14,2,3,1,312,ye thu,Proceedings of the Fifth Workshop on South and Southeast {A}sian Natural Language Processing,0,"This paper addresses the problem of word segmentation for low resource languages, with the main focus being on Myanmar language. In our proposed method, we focus on exploiting limited amounts of dictionary resource, in an attempt to improve the segmentation quality of an unsupervised word segmenter. Three models are proposed. In the first, a set of dictionaries (separate dictionaries for different classes of words) are directly introduced into the generative model. In the second, a language model was built from the dictionaries, and the n-gram model was inserted into the generative model. This model was expected to model words that did not occur in the training data. The third model was a combination of the previous two models. We evaluated our approach on a corpus of manually annotated data. Our results show that the proposed methods are able to improve over a fully unsupervised baseline system. The best of our systems improved the F-score from 0.48 to 0.66. In addition to segmenting the data, one proposed method is also able to partially label the segmented corpus with POS tags. We found that these labels were approximately 66% accurate."
P14-2026,Dependency-based Pre-ordering for {C}hinese-{E}nglish Machine Translation,2014,19,13,3,0,38092,jingsheng cai,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In statistical machine translation (SMT), syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders. This paper introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT. We present a set of dependency-based preordering rules which improved the BLEU score by 1.61 on the NIST 2006 evaluation data. We also investigate the accuracy of the rule set by conducting human evaluations."
P14-2122,Empirical Study of Unsupervised {C}hinese Word Segmentation Methods for {SMT} on Large-scale Corpora,2014,25,7,4,1,23611,xiaolin wang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine translation (SMT) without annotated data, and bilingual UWS can even optimize segmentation for alignment. Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity. This paper proposes an efficient unified PYP-based monolingual and bilingual UWS method. Experimental results show that the proposed method is comparable to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain."
P14-1138,Recurrent Neural Networks for Word Alignment Model,2014,40,43,3,1,1486,akihiro tamura,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks."
D14-1019,Syntax-Augmented Machine Translation using Syntax-Label Clustering,2014,20,3,3,1,287,hideya mino,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Recently, syntactic information has helped significantly to improve statistical machine translation. However, the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules, especially when syntax labels are projected from a parser in syntax-augmented machine translation. In this paper, we propose a syntax-label clustering method that uses an exchange algorithm in which syntax labels are clustered together to reduce the number of rules. The proposed method achieves clustering by directly maximizing the likelihood of synchronous rules, whereas previous work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method."
D14-1022,Learning Hierarchical Translation Spans,2014,23,10,3,1,12715,jingyi zhang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We propose a simple and effective approach to learn translation spans for the hierarchical phrase-based translation model. Our model evaluates if a source span should be covered by translation rules during decoding, which is integrated into the translation system as soft constraints. Compared to syntactic constraints, our model is directly acquired from an aligned parallel corpus and does not require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system."
D14-1023,Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation,2014,40,19,5,0.163811,3690,rui wang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus."
D14-1173,Refining Word Segmentation Using a Manually Aligned Corpus for Statistical Machine Translation,2014,26,4,4,1,23611,xiaolin wang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Languages that have no explicit word delimiters often have to be segmented for statistical machine translation (SMT). This is commonly performed by automated segmenters trained on manually annotated corpora. However, the word segmentation (WS) schemes of these annotated corpora are handcrafted for general usage, and may not be suitable for SMT. An analysis was performed to test this hypothesis using a manually annotated word alignment (WA) corpus for Chinese-English SMT. An analysis revealed that 74.60% of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank (CTB) will contain conflicts with the gold WA annotations. We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts. Experimental results show that the refined WS reduced word alignment error rate by 6.82% and achieved the highest BLEU improvement (0.63 on average) on the Chinese-English open machine translation (OpenMT) corpora compared to related work."
2014.iwslt-papers.5,"Empircal dependency-based head finalization for statistical {C}hinese-, {E}nglish-, and {F}rench-to-{M}yanmar ({B}urmese) machine translation",2014,23,5,5,1,285,chenchen ding,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"We conduct dependency-based head finalization for statistical machine translation (SMT) for Myanmar (Burmese). Although Myanmar is an understudied language, linguistically it is a head-final language with similar syntax to Japanese and Korean. So, applying the efficient techniques of Japanese and Korean processing to Myanmar is a natural idea. Our approach is a combination of two approaches. The first is a head-driven phrase structure grammar (HPSG) based head finalization for English-to-Japanese translation, the second is dependency-based pre-ordering originally designed for English-to-Korean translation. We experiment on Chinese-, English-, and French-to-Myanmar translation, using a statistical pre-ordering approach as a comparison method. Experimental results show the dependency-based head finalization was able to consistently improve a baseline SMT system, for different source languages and different segmentation schemes for the Myanmar language."
2014.iwslt-papers.8,An exploration of segmentation strategies in stream decoding,2014,-1,-1,3,0,16459,andrew finch,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"In this paper we explore segmentation strategies for the stream decoder a method for decoding from a continuous stream of input tokens, rather than the traditional method of decoding from sentence segmented text. The behavior of the decoder is analyzed and modifications to the decoding algorithm are proposed to improve its performance. The experimental results show our proposed decoding strategies to be effective, and add support to the original findings that this approach is capable of approaching the performance of the underlying phrase-based machine translation decoder, at useful levels of latency. Our experiments evaluated the stream decoder on a broader set of language pairs than in previous work. We found most European language pairs were similar in character, and report results on English-Chinese and English-German pairs which are of interest due to the reordering required."
2014.iwslt-evaluation.20,The {NICT} translation system for {IWSLT} 2014,2014,-1,-1,5,1,23611,xiaolin wang,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes NICT{'}s participation in the IWSLT 2014 evaluation campaign for the TED Chinese-English translation shared-task. Our approach used a combination of phrase-based and hierarchical statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model."
2014.amta-researchers.9,Document-level re-ranking with soft lexical and semantic features for statistical machine translation,2014,28,0,3,1,285,chenchen ding,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,We introduce two document-level features to polish baseline sentence-level translations generated by a state-of-the-art statistical machine translation (SMT) system. One feature uses the word-embedding technique to model the relation between a sentence and its context on the target side; the other feature is a crisp document-level token-type ratio of target-side translations for source-side words to model the lexical consistency in translation. The weights of introduced features are tuned to optimize the sentence- and document-level metrics simultaneously on the basis of Pareto optimality. Experimental results on two different schemes with different corpora illustrate that the proposed approach can efficiently and stably integrate document-level information into a sentence-level SMT system. The best improvements were approximately 0.5 BLEU on test sets with statistical significance.
P13-1016,Distortion Model Considering Rich Context for Statistical Machine Translation,2013,26,6,3,1,288,isao goto,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models."
P13-1078,Additive Neural Networks for Statistical Machine Translation,2013,35,31,3,1,3591,lemao liu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks."
P13-1079,Hierarchical Phrase Table Combination for Machine Translation,2013,30,2,3,0,20203,conghui zhu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel."
P13-1083,Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation,2013,38,7,3,1,1486,akihiro tamura,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model."
I13-1032,Tuning {SMT} with a Large Number of Features via Online Feature Grouping,2013,17,2,4,1,3591,lemao liu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we consider the tuning of statistical machine translation (SMT) models employing a large number of features. We argue that existing tuning methods for these models suffer serious sparsity problems, in which features appearing in the tuning data may not appear in the testing data and thus those features may be over tuned in the tuning data. As a result, we face an over-fitting problem, which limits the generalization abilities of the learned models. Based on our analysis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods."
2013.mtsummit-papers.3,Inducing {R}omanization Systems,2013,-1,-1,4,0,41906,keiko taguchi,Proceedings of Machine Translation Summit XIV: Papers,0,None
W12-4406,Rescoring a Phrase-based Machine Transliteration System with Recurrent Neural Network Language Models,2012,12,8,3,0,16459,andrew finch,Proceedings of the 4th Named Entity Workshop ({NEWS}) 2012,0,"The system entered into this year's shared transliteration evaluation is implemented within a phrase-based statistical machine transliteration (SMT) framework. The system is based on a joint source-channel model in combination with a target language model and models to control the length of the sequences generated. The joint source-channel model was trained using a many-to-many Bayesian bilingual alignment. The focus of this year's system is on input representation. In order attempt to mitigate data sparseness issues in the joint source-channel model, we augmented the system with recurrent neural network (RNN) models that can learn to project the grapheme set onto a smaller hidden representation. We performed experiments on development data to evaluate the effectiveness of our approach. Our results show that using an RNN language model can improve performance for language pairs with large grapheme sets on the target side."
P12-2061,Post-ordering by Parsing for {J}apanese-{E}nglish Statistical Machine Translation,2012,25,23,3,1,288,isao goto,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Reordering is a difficult task in translating between widely different languages such as Japanese and English. We employ the post-ordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language."
D12-1003,Bilingual Lexicon Extraction from Comparable Corpora Using Label Propagation,2012,47,51,3,1,1486,akihiro tamura,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graph-based label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation."
2012.iwslt-evaluation.15,Minimum {B}ayes-risk decoding extended with similar examples: {NAIST}-{NCT} at {IWSLT} 2012,2012,10,0,3,0,39435,hiroaki shimizu,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes our methods used in the NAIST-NICT submission to the International Workshop on Spoken Language Translation (IWSLT) 2012 evaluation campaign. In particular, we propose two extensions to minimum bayes-risk decoding which reduces a expected loss."
2012.iwslt-evaluation.16,The {NICT} translation system for {IWSLT} 2012,2012,14,2,3,0,16459,andrew finch,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes NICTxe2x80x99s participation in the IWSLT 2012 evaluation campaign for the TED speech translation RussianEnglish shared-task. Our approach was based on a phrasebased statistical machine translation system that was augmented by using transliteration mining techniques. The basic premise behind our approach was to try to use sub-word-level alignments to guide the word-level alignment process used to learn the phrase-table. We did this by first mining a corpus of Russian-English transliterations pairs and cognates from a set of interlanguage link titles from Wikipedia. This corpus was then used to build a manyto-many nonparametric Bayesian bilingual alignment model that could be used to identify the occurrence of transliterations and cognates in the training corpus itself. Alignment counts for these mined pairs were increased in the training corpus to increase the likelihood that these pairs would align in training. Our experiments on the test sets from the 2010 and 2011 shared tasks, showed that an improvement in BLEU score can be gained in translation performance by encouraging the alignment of cognates and transliterations during word alignment."
2012.eamt-1.56,Crowd-based {MT} Evaluation for non-{E}nglish Target Languages,2012,-1,-1,2,0,12388,michael paul,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,None
W11-3203,Integrating Models Derived from non-Parametric {B}ayesian Co-segmentation into a Statistical Machine Transliteration System,2011,14,10,3,0,16459,andrew finch,Proceedings of the 3rd Named Entities Workshop ({NEWS} 2011),0,"The system presented in this paper is based upon a phrase-based statistical machine transliteration (SMT) framework. The SMT systemxe2x80x99s log-linear model is augmented with a set of features specifically suited to the task of transliteration. In particular our model utilizes a feature based on a joint source-channel model, and a feature based on a maximum entropy model that predicts target grapheme sequences using the local context of graphemes and grapheme sequences in both source and target languages. The segmentation for our approach was performed using a non-parametric Bayesian co-segmentation model, and in this paper we present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment."
W11-3208,Using Features from a Bilingual Alignment Model in Transliteration Mining,2011,0,6,4,0,44111,takaaki fukunishi,Proceedings of the 3rd Named Entities Workshop ({NEWS} 2011),0,None
W11-2601,Dialect Translation: Integrating {B}ayesian Co-segmentation Models with Pivot-based {SMT},2011,26,3,4,0,12388,michael paul,Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties,0,"Recent research on multilingual statistical machine translation (SMT) focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. This paper proposes a new method to translate a dialect language into a foreign language by integrating transliteration approaches based on Bayesian co-segmentation (BCS) models with pivot-based SMT approaches. The advantages of the proposed method with respect to standard SMT approaches are three fold: (1) it uses a standard language as the pivot language and acquires knowledge about the relation between dialects and the standard language automatically, (2) it reduces the translation task complexity by using monotone decoding techniques, (3) it reduces the number of features in the log-linear model that have to be estimated from bilingual data. Experimental results translating four Japanese dialects (Kumamoto, Kyoto, Okinawa, Osaka) into four Indo-European languages (English, German, Russian, Hindi) and two Asian languages (Chinese, Korean) revealed that the proposed method improves the translation quality of dialect translation tasks and outperforms standard pivot translation approaches concatenating SMT engines for the majority of the investigated language pairs."
P11-2076,Reordering Constraint Based on Document-Level Context,2011,17,2,3,1,41606,takashi onishi,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the document-level context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in Japanese-English translation and 1.41% BLEU points in English-Japanese translation."
P11-1064,An Unsupervised Model for Joint Phrase Alignment and Extraction,2011,30,61,3,0,834,graham neubig,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We present an unsupervised model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size."
P11-1125,Machine Translation System Combination by Confusion Forest,2011,37,9,2,0.670349,128,taro watanabe,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space."
I11-1091,Translation Quality Indicators for Pivot-based Statistical {MT},2011,11,7,2,0,12388,michael paul,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. This paper provides new insights into what factors make a good pivot language and investigates the impact of these factors on the overall pivot translation performance. Pivot-based SMT experiments translating between 22 IndoEuropean and Asian languages were used to analyze the impact of eight factors (language family, vocabulary, sentence length, language perplexity, translation model entropy, reordering, monotonicity, engine performance) on pivot translation performance. The results showed that 81% of system performance variations can be explained by these factors."
2011.mtsummit-papers.37,Searching Translation Memories for Paraphrases,2011,-1,-1,4,0,127,masao utiyama,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.40,A Comparison of Unsupervised Bilingual Term Extraction Methods Using Phrase-Tables,2011,-1,-1,4,0,44906,masamichi ideue,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.51,A Comparison Study of Parsers for Patent Machine Translation,2011,-1,-1,4,1,288,isao goto,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.iwslt-papers.11,Annotating data selection for improving machine translation,2011,7,0,4,1,4955,keiji yasuda,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"In order to efficiently improve machine translation systems, we propose a method which selects data to be annotated (manually translated) from speech-to-speech translation field data. For the selection experiments, we used data from field experiments conducted during the 2009 fiscal year in five areas of Japan. For the selection experiments, we used data sets from two areas: one data set giving the lowest baseline speech translation performance for its test set, and another data set giving the highest. In the experiments, we compare two methods for selecting data to be manually translated from the field data. Both of them use source side language models for data selection, but in different manners. According to the experimental results, either or both of the methods show larger improvements compared to a random data selection."
2011.iwslt-evaluation.5,The {NICT} translation system for {IWSLT} 2011,2011,20,3,4,0,16459,andrew finch,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes NICT{'}s participation in the IWSLT 2011 evaluation campaign for the TED speech translation ChineseEnglish shared-task. Our approach was based on a phrasebased statistical machine translation system that was augmented in two ways. Firstly we introduced rule-based re-ordering constraints on the decoding. This consisted of a set of rules that were used to segment the input utterances into segments that could be decoded almost independently. This idea here being that constraining the decoding process in this manner would greatly reduce the search space of the decoder, and cut out many possibilities for error while at the same time allowing for a correct output to be generated. The rules we used exploit punctuation and spacing in the input utterances, and we use these positions to delimit our segments. Not all punctuation/spacing positions were used as segment boundaries, and the set of used positions were determined by a set of linguistically-based heuristics. Secondly we used two heterogeneous methods to build the translation model, and lexical reordering model for our systems. The first method employed the popular method of using GIZA++ for alignment in combination with phraseextraction heuristics. The second method used a recentlydeveloped Bayesian alignment technique that is able to perform both phrase-to-phrase alignment and phrase pair extraction within a single unsupervised process. The models produced by this type of alignment technique are typically very compact whilst at the same time maintaining a high level of translation quality. We evaluated both of these methods of translation model construction in isolation, and our results show their performance is comparable. We also integrated both models by linear interpolation to obtain a model that outperforms either component. Finally, we added an indicator feature into the log-linear model to indicate those phrases that were in the intersection of the two translation models. The addition of this feature was also able to provide a small improvement in performance."
2011.eamt-1.17,Rule-based Reordering Constraints for Phrase-based {SMT},2011,-1,-1,3,1,44966,chooiling goh,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,None
W10-3804,Syntactic Constraints on Phrase Extraction for Phrase-Based Machine Translation,2010,30,2,3,0,35773,hailong cao,Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation,0,"A typical phrase-based machine translation (PBMT) system uses phrase pairs extracted from word-aligned parallel corpora. All phrase pairs that are consistent with word alignments are collected. The resulting phrase table is very large and includes many non-syntactic phrases which may not be necessary. We propose to filter the phrase table based on source language syntactic constraints. Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words. Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables."
W10-3508,"Helping Volunteer Translators, Fostering Language Resources",2010,6,0,3,0,127,masao utiyama,Proceedings of the 2nd Workshop on {T}he {P}eople{'}s {W}eb {M}eets {NLP}: {C}ollaboratively {C}onstructed {S}emantic {R}esources,0,"This paper introduces a website called Minna no Honxe2x80x99yaku(MNH, xe2x80x9cTranslation for Allxe2x80x9d), which hosts online volunteer translators. Its core features are (1) a set of translation aid tools, (2) high quality, comprehensive language resources, and (3) the legal sharing of translations. As of May 2010, there are about 1200 users and 4 groups registered to MNH. The groups using it include such major"
W10-2406,Transliteration Using a Phrase-Based Statistical Machine Translation System to Re-Score the Output of a Joint Multigram Model,2010,14,22,2,0.923428,16459,andrew finch,Proceedings of the 2010 Named Entities Workshop,0,"The system presented in this paper uses a combination of two techniques to directly transliterate from grapheme to grapheme. The technique makes no language specific assumptions, uses no dictionaries or explicit phonetic information; the process transforms sequences of tokens in the source language directly into to sequences of tokens in the target. All the language pairs in our experiments were transliterated by applying this technique in a single unified manner. The approach we take is that of hypothesis rescoring to integrate the models of two state-of-the-art techniques: phrase-based statistical machine translation (SMT), and a joint multigram model. The joint multigram model was used to generate an n-best list of transliteration hypotheses that were re-scored using the models of the phrase-based SMT system. The both of the models' scores for each hypothesis were linearly interpolated to produce a final hypothesis score that was used to re-rank the hypotheses. In our experiments on development data, the combined system was able to outperform both of its component systems substantially."
W10-1760,Integration of Multiple Bilingually-Learned Segmentation Schemes into Statistical Machine Translation,2010,24,7,3,0.719478,12388,michael paul,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. The method can be applied to any language pair where the source language is unseg-mented and the target language segmentation is known. First, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available state-of-the-art monolingually-built segmentation tools."
P10-2001,Paraphrase Lattice for Statistical Machine Translation,2010,12,25,3,1,41606,takashi onishi,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets."
P10-2004,Filtering Syntactic Constraints for Statistical Machine Translation,2010,18,2,2,0,35773,hailong cao,Proceedings of the {ACL} 2010 Conference Short Papers,0,Source language parse trees offer very useful but imperfect reordering constraints for statistical machine translation. A lot of effort has been made for soft applications of syntactic constraints. We alternatively propose the selective use of syntactic constraints. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system.
abekawa-etal-2010-community,Community-based Construction of Draft and Final Translation Corpus Through a Translation Hosting Site Minna no Hon{'}yaku ({MNH}),2010,6,6,3,1,35650,takeshi abekawa,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper we report a way of constructing a translation corpus that contains not only source and target texts, but draft and final versions of target texts, through the translation hosting site Minna no Hon'yaku (MNH). We made MNH publicly available on April 2009. Since then, more than 1,000 users have registered and over 3,500 documents have been translated, as of February 2010, from English to Japanese and from Japanese to English. MNH provides an integrated translation-aid environment, QRedit, which enables translators to look up high-quality dictionaries and Wikipedia as well as to search Google seamlessly. As MNH keeps translation logs, a corpus consisting of source texts, draft translations in several versions, and final translations is constructed naturally through MNH. As of 7 February, 764 documents with multiple translation versions are accumulated, of which 110 are edited by more than one translators. This corpus can be used for self-learning by inexperienced translators on MNH, and potentially for improving machine translation."
2010.iwslt-papers.7,A {B}ayesian model of bilingual segmentation for transliteration,2010,0,28,2,0.923428,16459,andrew finch,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,None
2010.iwslt-evaluation.18,The {NICT} translation system for {IWSLT} 2010,2010,16,4,5,1,44966,chooiling goh,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes NICT{'}s participation in the IWSLT 2010 evaluation campaign for the DIALOG translation (Chinese-English) and the BTEC (French-English) translation shared-tasks. For the DIALOG translation, the main challenge to this task is applying context information during translation. Context information can be used to decide on word choice and also to replace missing information during translation. We applied discriminative reranking using contextual information as additional features. In order to provide more choices for re-ranking, we generated n-best lists from multiple phrase-based statistical machine translation systems that varied in the type of Chinese word segmentation schemes used. We also built a model that merged the phrase tables generated by the different segmentation schemes. Furthermore, we used a lattice-based system combination model to combine the output from different systems. A combination of all of these systems was used to produce the n-best lists for re-ranking. For the BTEC task, a general approach that used latticebased system combination of two systems, a standard phrasebased system and a hierarchical phrase-based system, was taken. We also tried to process some unknown words by replacing them with the same words but different inflections that are known to the system."
W09-3510,Transliteration by Bidirectional Statistical Machine Translation,2009,6,9,2,1,16459,andrew finch,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"The system presented in this paper uses phrase-based statistical machine translation (SMT) techniques to directly transliterate between all language pairs in this shared task. The technique makes no language specific assumptions, uses no dictionaries or explicit phonetic information. The translation process transforms sequences of tokens in the source language directly into to sequences of tokens in the target. All language pairs were transliterated by applying this technique in a single unified manner. The machine translation system used was a system comprised of two phrase-based SMT decoders. The first generated from the first token of the target to the last. The second system generated the target from last to first. Our results show that if only one of these decoding strategies is to be chosen, the optimal choice depends on the languages involved, and that in general a combination of the two approaches is able to outperform either approach."
W09-2309,Reordering Model Using Syntactic Information of a Source Tree for Statistical Machine Translation,2009,19,3,4,0,46627,kei hashimoto,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"This paper presents a reordering model using syntactic information of a source tree for phrase-based statistical machine translation. The proposed model is an extension of ISTITG (imposing source tree on inversion transduction grammar) constraints. In the proposed method, the target-side word order is obtained by rotating nodes of the source-side parse-tree. We modeled the node rotation, monotone or swap, using word alignments based on a training parallel corpus and sourceside parse-trees. The model efficiently suppresses erroneous target word orderings, especially global orderings. Furthermore, the proposed method conducts a probabilistic evaluation of target word reorderings. In English-to-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model."
W09-0418,{NICT}@{WMT}09: Model Adaptation and Transliteration for {S}panish-{E}nglish {SMT},2009,18,7,3,0.813769,12388,michael paul,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,This paper describes the NICT statistical machine translation (SMT) system used for the WMT 2009 Shared Task (WMT09) evaluation. We participated in the Spanish-English translation task. The focus of this year's participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrase-based SMT system.
N09-2056,On the Importance of Pivot Language Selection for Statistical Machine Translation,2009,6,26,3,0.813769,12388,michael paul,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Recent research on multilingual statistical machine translation focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. Due to the richness of available language resources, English is in general the pivot language of choice. In this paper, we investigate the appropriateness of languages other than English as pivot languages. Experimental results using state-of-the-art statistical machine translation techniques to translate between twelve languages revealed that the translation quality of 61 out of 110 language pairs improved when a non-English pivot language was chosen."
D09-1117,Bidirectional Phrase-based Statistical Machine Translation,2009,14,16,2,1,16459,andrew finch,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper investigates the effect of direction in phrase-based statistial machine translation decoding. We compare a typical phrase-based machine translation decoder using a left-to-right decoding strategy to a right-to-left decoder. We also investigate the effectiveness of a bidirectional decoding strategy that integrates both mono-directional approaches, with the aim of reducing the effects due to language specificity. Our experimental evaluation was extensive, based on 272 different language pairs, and gave the surprising result that for most of the language pairs, it was better decode from right-to-left than from left-to-right. As expected the relative performance of left-to-right and right-to-left strategies proved to be highly language dependent. The bidirectional approach outperformed the both the left-to-right strategy and the right-to-left strategy, showing consistent improvements that appeared to be unrelated to the specific languages used for translation. Bidirectional decoding gave rise to an improvement in performance over a left-to-right decoding strategy in terms of the BLEU score in 99% of our experiments."
2009.tc-1.4,"Minna no Hon{'}yaku: a website for hosting, archiving, and promoting translations",2009,-1,-1,3,0.403169,127,masao utiyama,Proceedings of Translating and the Computer 31,0,None
2009.mtsummit-posters.10,Development of a {J}apanese-{E}nglish Software Manual Parallel Corpus,2009,-1,-1,3,0,45044,tatsuya ishisaka,Proceedings of Machine Translation Summit XII: Posters,0,None
2009.mtsummit-posters.22,Hosting Volunteer Translators,2009,-1,-1,3,0.403169,127,masao utiyama,Proceedings of Machine Translation Summit XII: Posters,0,None
2009.mtsummit-papers.18,Mining Parallel Texts from Mixed-Language Web Pages,2009,-1,-1,4,0.403169,127,masao utiyama,Proceedings of Machine Translation Summit XII: Papers,0,None
2009.iwslt-papers.6,Network-based speech-to-speech translation,2009,0,0,7,0,40303,chiori hori,Proceedings of the 6th International Workshop on Spoken Language Translation: Papers,0,"This demo shows the network-based speech-to-speech translation system. The system was designed to perform realtime, location-free, multi-party translation between speakers of different languages. The spoken language modules: automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS), are connected through Web servers that can be accessed via client applications worldwide. In this demo, we will show the multiparty speech-to-speech translation of Japanese, Chinese, Indonesian, Vietnamese, and English, provided by the NICT server. These speech-to-speech modules have been developed by NICT as a part of A-STAR (Asian Speech Translation Advanced Research) consortium project1."
2009.iwslt-evaluation.12,Two methods for stabilizing {MERT},2009,5,6,3,0.403169,127,masao utiyama,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes the NICT SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2009 evaluation campaign. We participated in the Challenge Task. Our system was based on a fairly common phrase-based machine translation system. We used two methods for stabilizing MERT.
W08-0401,Imposing Constraints from the Source Tree on {ITG} Constraints for {SMT},2008,22,12,3,1,47323,hirofumi yamamoto,Proceedings of the {ACL}-08: {HLT} Second Workshop on Syntax and Structure in Statistical Translation ({SSST}-2),0,"In current statistical machine translation (SMT), erroneous word reordering is one of the most serious problems. To resolve this problem, many word-reordering constraint techniques have been proposed. The inversion transduction grammar (ITG) is one of these constraints. In ITG constraints, target-side word order is obtained by rotating nodes of the source-side binary tree. In these node rotations, the source binary tree instance is not considered. Therefore, stronger constraints for word reordering can be obtained by imposing further constraints derived from the source tree on the ITG constraints. For example, for the source word sequence { a b c d }, ITG constraints allow a total of twenty-two target word orderings. However, when the source binary tree instance ((a b) (c d)) is given, our proposed imposing source tree on ITG (IST-ITG) constraints allow only eight word orderings. The reduction in the number of word-order permutations by our proposed stronger constraints efficiently suppresses erroneous word orderings. In our experiments with IST-ITG using the NIST MT08 English-to-Chinese translation track's data, the proposed method resulted in a 1.8-points improvement in character BLEU-4 (35.2 to 37.0) and a 6.2% lower CER (74.1 to 67.9%) compared with our baseline condition."
W08-0334,Dynamic Model Interpolation for Statistical Machine Translation,2008,15,36,2,1,16459,andrew finch,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper presents a technique for class-dependent decoding for statistical machine translation (SMT). The approach differs from previous methods of class-dependent translation in that the class-dependent forms of all models are integrated directly into the decoding process. We employ probabilistic mixture weights between models that can change dynamically on a segment-by-segment basis depending on the characteristics of the source segment. The effectiveness of this approach is demonstrated by evaluating its performance on travel conversation data. We used the approach to tackle the translation of questions and declarative sentences using class-dependent models. To achieve this, our system integrated two sets of models specifically built to deal with sentences that fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built to handle the general class. The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial."
W08-0335,Improved Statistical Machine Translation by Multiple {C}hinese Word Segmentation,2008,12,33,3,1,46412,ruiqiang zhang,Proceedings of the Third Workshop on Statistical Machine Translation,0,"Chinese word segmentation (CWS) is a necessary step in Chinese-English statistical machine translation (SMT) and its performance has an impact on the results of SMT. However, there are many settings involved in creating a CWS system such as various specifications and CWS methods. This paper investigates the effect of these settings to SMT. We tested dictionary-based and CRF-based approaches and found there was no significant difference between the two in the qualty of the resulting translations. We also found the correlation between the CWS F-score and SMT BLEU score was very weak. This paper also proposes two methods of combining advantages of different specifications: a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated. We found these approaches were very effective in improving quality of translations."
I08-8003,Phrase-based Machine Transliteration,2008,4,34,2,1,16459,andrew finch,Proceedings of the Workshop on Technologies and Corpora for Asia-Pacific Speech Translation ({TCAST}),0,None
I08-4033,{A}chilles: {N}i{CT}/{ATR} {C}hinese Morphological Analyzer for the Fourth Sighan Bakeoff,2008,12,2,2,1,46412,ruiqiang zhang,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"We created a new Chinese morphological analyzer, Achilles, by integrating rule-based, dictionary-based, and statistical machine learning method, conditional random fields (CRF). The rulebased method is used to recognize regular expressions: numbers, time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while outof-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus. In spite of an unexpected file encoding errors, the system exhibited a top level performance. A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus. Achilles uses a feature combined approach for partof-speech tagging. Our post-evaluation results prove the effectiveness of this approach for POS tagging."
I08-2088,Method of Selecting Training Data to Build a Compact and Efficient Translation Model,2008,12,34,4,1,4955,keiji yasuda,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Target task matched parallel corpora are required for statistical translation model training. However, training corpora sometimes include both target task matched and unmatched sentences. In such a case, training set selection can reduce the size of the translation model. In this paper, we propose a training set selection method for translation model training using linear translation model interpolation and a language model technique. According to the experimental results, the proposed method reduces the translation model size by 50% and improves BLEU score by 1.76% in comparison with a baseline training corpus usage."
I08-1030,{C}hinese Unknown Word Translation by Subword Re-segmentation,2008,13,6,2,1,46412,ruiqiang zhang,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We propose a general approach for translating Chinese unknown words (UNK) for SMT. This approach takes advantage of the properties of Chinese word composition rules, i.e., all Chinese words are formed by sequential characters. According to the proposed approach, the unknown word is re-split into a subword sequence followed by subword translation with a subwordbased translation model. xe2x80x9cSubwordxe2x80x9d is a unit between character and long word. We found the proposed approach significantly improved translation quality on the test data of NIST MT04 and MT05. We also found that the translation quality was further improved if we applied named entity translation to translate parts of unknown words before using the subword-based translation."
C08-3006,Multilingual Mobile-Phone Translation Services for World Travelers,2008,5,13,4,1,12388,michael paul,Coling 2008: Companion volume: Demonstrations,0,This demonstration introduces two new multilingual translation services for mobile phones. The first translation service provides state-of-the-art text-to-text translations of Japanese as well as English conversational spoken language in the travel domain into 17 languages using statistical machine translation technologies trained automatically from a large-scale multilingual corpus. The second demonstration is a speech translation service between Japanese and English for real environments. It is based on distributed speech recognition with noise suppression. Flexible interfaces between internal and external speech translation resources ease the portability of the system to other languages and enable real-time location-free communication world-wide.
2008.iwslt-evaluation.11,The {NICT}/{ATR} speech translation system for {IWSLT} 2008.,2008,13,14,8,0.403169,127,masao utiyama,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the National Institute of Information and Communications Technology/Advanced Telecommunications Research Institute International (NICT/ATR) statistical machine translation (SMT) system used for the IWSLT 2008 evaluation campaign. We participated in the Chinese{--}English (Challenge Task), English{--}Chinese (Challenge Task), Chinese{--}English (BTEC Task), Chinese{--}Spanish (BTEC Task), and Chinese{--}English{--}Spanish (PIVOT Task) translation tasks. In the English{--}Chinese translation Challenge Task, we focused on exploring various factors for the English{--}Chinese translation because the research on the translation of English{--}Chinese is scarce compared to the opposite direction. In the Chinese{--}English translation Challenge Task, we employed a novel clustering method, where training sentences similar to the development data in terms of the word error rate formed a cluster. In the pivot translation task, we integrated two strategies for pivot translation by linear interpolation."
P07-2007,{NICT}-{ATR} Speech-to-Speech Translation System,2007,13,6,1,1,129,eiichiro sumita,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"This paper describes the latest version of speech-to-speech translation systems developed by the team of NICT-ATR for over twenty years. The system is now ready to be deployed for the travel domain. A new noise-suppression technique notably improves speech recognition performance. Corpus-based approaches of recognition, translation, and synthesis enable coverage of a wide variety of topics and portability to other languages."
P07-2046,Boosting Statistical Machine Translation by Lemmatization and Linear Interpolation,2007,7,6,2,1,46412,ruiqiang zhang,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"Data sparseness is one of the factors that degrade statistical machine translation (SMT). Existing work has shown that using morpho-syntactic information is an effective solution to data sparseness. However, fewer efforts have been made for Chinese-to-English SMT with using English morpho-syntactic analysis. We found that while English is a language with less inflection, using English lemmas in training can significantly improve the quality of word alignment that leads to yield better translation performance. We carried out comprehensive experiments on multiple training data of varied sizes to prove this. We also proposed a new effective linear interpolation method to integrate multiple homologous features of translation models."
O07-5005,Multilingual Spoken Language Corpus Development for Communication Research,2007,0,16,4,0.493927,43353,toshiyuki takezawa,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 12, Number 3, September 2007: Special Issue on Invited Papers from {ISCSLP} 2006",0,None
D07-1054,Bilingual Cluster Based Models for Statistical Machine Translation,2007,13,14,2,1,47323,hirofumi yamamoto,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We propose a domain specific model for statistical machine translation. It is wellknown that domain specific language models perform well in automatic speech recognition. We show that domain specific language and translation models also benefit statistical machine translation. However, there are two problems with using domain specific models. The first is the data sparseness problem. We employ an adaptation technique to overcome this problem. The second issue is domain prediction. In order to perform adaptation, the domain must be provided, however in many cases, the domain is not known or changes dynamically. For these cases, not only the translation target sentence but also the domain must be predicted. This paper focuses on the domain prediction problem for statistical machine translation. In the proposed method, a bilingual training corpus, is automatically clustered into sub-corpora. Each sub-corpus is deemed to be a domain. The domain of a source sentence is predicted by using its similarity to the sub-corpora. The predicted domain (sub-corpus) specific language and translation models are then used for the translation decoding. This approach gave an improvement of 2.7 in BLEU (Papineni et al., 2002) score on the IWSLT05 Japanese to English evaluation corpus (improving the score from 52.4 to 55.1). This is a substantial gain and indicates the validity of the proposed bilingual cluster based models."
2007.tmi-papers.19,Reducing human assessment of machine translation quality to binary classifiers,2007,-1,-1,3,1,12388,michael paul,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
2007.mtsummit-ucnlg.5,Method of selecting training sets to build compact and efficient language model,2007,-1,-1,3,1,4955,keiji yasuda,Proceedings of the Workshop on Using corpora for natural language generation,0,None
2007.mtsummit-papers.48,Introducing translation dictionary into phrase-based {SMT},2007,-1,-1,3,1,44962,hideo okuma,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.iwslt-1.15,The {NICT}/{ATR} speech translation system for {IWSLT} 2007,2007,18,12,8,1,16459,andrew finch,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"This paper describes the NiCT-ATR statistical machine translation (SMT) system used for the IWSLT 2007 evaluation campaign. We participated in three of the four language pair translation tasks (CE, JE, and IE). We used a phrase-based SMT system using log-linear feature models for all tracks. This year we decoded from the ASR n-best lists in the JE track and found a gain in performance. We also applied some new techniques to facilitate the use of out-of-domain external resources by model combination and also by utilizing a huge corpus of n-grams provided by Google Inc.. Using these resources gave mixed results that depended on the technique also the language pair however, in some cases we achieved consistently positive results. The results from model-interpolation in particular were very promising."
P06-2028,Using Lexical Dependency and Ontological Knowledge to Improve a Detailed Syntactic and Semantic Tagger of {E}nglish,2006,21,3,4,1,16459,andrew finch,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"This paper presents a detailed study of the integration of knowledge from both dependency parses and hierarchical word ontologies into a maximum-entropy-based tagging model that simultaneously labels words with both syntax and semantics. Our findings show that information from both these sources can lead to strong improvements in overall system accuracy: dependency knowledge improved performance over all classes of word, and knowledge of the position of a word in an on-tological hierarchy increased accuracy for words not seen in the training data. The resulting tagger offers the highest reported tagging accuracy on this tagset to date."
P06-2123,Subword-Based Tagging for Confidence-Dependent {C}hinese Word Segmentation,2006,13,23,3,1,46412,ruiqiang zhang,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging. The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods. We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments. In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971)."
N06-2029,Exploiting Variant Corpora for Machine Translation,2006,4,0,2,1,12388,michael paul,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper proposes the usage of variant corpora, i.e., parallel text corpora that are equal in meaning but use different ways to express content, in order to improve corpus-based machine translation. The usage of multiple training corpora of the same content with different sources results in variant models that focus on specific linguistic phenomena covered by the respective corpus. The proposed method applies each variant model separately resulting in multiple translation hypotheses which are selectively combined according to statistical models. The proposed method outperforms the conventional approach of merging all variants by reducing translation ambiguities and exploiting the strengths of each variant model."
N06-2041,Using the Web to Disambiguate Acronyms,2006,6,6,1,1,129,eiichiro sumita,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper proposes an automatic method for disambiguating an acronym with multiple definitions, considering the context surrounding the acronym. First, the method obtains the Web pages that include both the acronym and its definitions. Second, the method feeds them to the machine learner. Cross-validation tests results indicate that the current accuracy of obtaining the appropriate definition for an acronym is around 92% for two ambiguous definitions and around 86% for five ambiguous definitions."
N06-2042,Word Pronunciation Disambiguation using the Web,2006,3,7,1,1,129,eiichiro sumita,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper proposes an automatic method of reading proper names with multiple pronunciations. First, the method obtains Web pages that include both the proper name and its pronunciation. Second, the method feeds them to the learner for classification. The current accuracy is around 90% for open data."
N06-2049,Subword-based Tagging by Conditional Random Fields for {C}hinese Word Segmentation,2006,5,54,3,1,46412,ruiqiang zhang,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005."
2006.iwslt-papers.8,Development of client-server speech translation system on a multi-lingual speech communication platform,2006,11,7,3,0,48707,tohru shimizu,Proceedings of the Third International Workshop on Spoken Language Translation: Papers,0,"This paper describes a client-server speech-to-speech translation system developed on a multi-lingual speech communication platform. This platform enables easy assembly of speech communication system from the corresponding software modules (e.g. speech recognition, spoken language machine-translation, speech synthesis). This client-server speech translation system is designed for use at mobile terminals. Terminals and servers are connected via a 3G public mobile phone networks, and speech translation services are available at various places with thin client. This system realizes hands-free communication and robustness for real use of speech translation in noisy environments. A microphone array and new noise suppression technique improves speech recognition performance, and a corpus-based approach enables wide coverage, robustness and portability to new languages and domains. Recent evaluation of the overall system showed that the utterance correctness of speech recognition output achieved 83%, and more than 88% of the utterances are correctly translated for Japanse-English and JapaneseChinese."
2006.iwslt-evaluation.12,The {N}i{CT}-{ATR} statistical machine translation system for {IWSLT} 2006,2006,9,38,10,1,46412,ruiqiang zhang,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the NiCT-ATR statistical machine translation (SMT) system used for the IWSLT 2006 evaluation compaign. We participated in all four language pair translation tasks (CE, JE, AE and IE) and all two tracks (OPEN and CSTAR). We used a phrase-based SMT in the OPEN track and a hybrid multiple translation engine in the CSTAR track. We also equipped our system with some of new preprocessing and post-processing techniques for Chinese word segmentation, named entity translation, punctuation and capitalization, sentence splitting, and language model adaptation. Our experiments show these features significantly improved our system."
W05-0210,Measuring Non-native Speakers{'} Proficiency of {E}nglish by Using a Test with Automatically-Generated Fill-in-the-Blank Questions,2005,14,67,1,1,129,eiichiro sumita,Proceedings of the Second Workshop on Building Educational Applications Using {NLP},0,"This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency. First, the proposal generates an FBQ from a given sentence in English. The position of a blank in the sentence is determined, and the word at that position is considered as the correct choice. The candidates for incorrect choices for the blank are hypothesized through a thesaurus. Then, each of the candidates is verified by using the Web. Finally, the blanked sentence, the correct choice and the incorrect choices surviving the verification are together laid out to form the FBQ. Second, the proficiency of non-native speakers who took the test consisting of such FBQs is estimated through IRT.n n Our experimental results suggest that: (1) the generated questions plus IRT estimate the non-native speakers' English proficiency; (2) while on the other hand, the test can be completed almost perfectly by English native speakers; and (3) the number of questions can be reduced by using item information in IRT.n n The proposed method provides teachers and testers with a tool that reduces time and expenditure for testing English proficiency."
I05-5003,Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence,2005,10,75,3,1,16459,andrew finch,Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005),0,"The task of machine translation (MT) evaluation is closely related to the task of sentence-level semantic equivalence classification. This paper investigates the utility of applying standard MT evaluation methods (BLEU, NIST, WER and PER) to building classifiers to predict semantic equivalence and entailment. We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence. Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment. Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments."
I05-1021,Acquiring Synonyms from Monolingual Comparable Texts,2005,14,10,2,1,51054,mitsuo shimohata,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper presents a method for acquiring synonyms from monolingual comparable text (MCT). MCT denotes a set of monolingual texts whose contents are similar and can be obtained automatically. Our acquisition method takes advantage of a characteristic of MCT that included words and their relations are confined. Our method uses contextual information of surrounding one word on each side of the target words. To improve acquisition precision, prevention of outside appearance is used. This method has advantages in that it requires only part-of-speech information and it can acquire infrequent synonyms. We evaluated our method with two kinds of news article data: sentence-aligned parallel texts and document-aligned comparable texts. When applying the former data, our method acquires synonym pairs with 70.0% precision. Re-evaluation of incorrect word pairs with source texts indicates that the method captures the appropriate parts of source texts with 89.5% precision. When applying the latter data, acquisition precision reaches 76.0% in English and 76.3% in Japanese."
2005.mtsummit-papers.35,Practical Approach to Syntax-based Statistical Machine Translation,2005,-1,-1,3,1,324,kenji imamura,Proceedings of Machine Translation Summit X: Papers,0,"This paper presents a practical approach to statistical machine translation (SMT) based on syntactic transfer. Conventionally, phrase-based SMT generates an output sentence by combining phrase (multiword sequence) translation and phrase reordering without syntax. On the other hand, SMT based on tree-to-tree mapping, which involves syntactic information, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process."
2005.mtsummit-ebmt.7,Graph-based Retrieval for Example-based Machine Translation Using Edit-distance,2005,22,10,3,1,51229,takao doi,Workshop on example-based machine translation,0,"An Example-Based Machine Translation (EBMT) system, whose translation example unit is a sentence, can produce an accurate and natural translation if translation examples similar enough to an input sentence are retrieved. Such a system, however, suffers from the problem of narrow coverage. To reduce the problem, a large-scale parallel corpus is required and, therefore, an efficient method is needed to retrieve translation examples from a large-scale corpus. The authors propose an efficient retrieval method for a sentence-wise EBMT using edit-distance. The proposed retrieval method efficiently retrieves the most similar sentences using the measure of edit-distance without omissions. The proposed method employs search-space division, word graphs, and an A* search algorithm. The performance of the EBMT was evaluated through Japanese-to-English translation experiments using a bilingual corpus comprising hundreds of thousands of sentences from a travel conversation domain. The EBMT system achieved a high-quality translation ability by using a large corpus and also achieved efficient processing by using the proposed retrieval method."
2005.mtsummit-ebmt.15,A Machine Learning Approach to Hypotheses Selection of Greedy Decoding for {SMT},2005,-1,-1,2,1,12388,michael paul,Workshop on example-based machine translation,0,"This paper proposes a method for integrating example-based and rule-based machine translation systems with statistical methods. It extends a greedy decoder for statistical machine translation (SMT), which searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In order to reduce local optima problems inherent in the search, the outputs generated by multiple translation engines, such as rule-based (RBMT) and example-based (EBMT) systems, are utilized as the initial translation hypotheses. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text corpus. However, the decoding of multiple initial translation hypotheses is computationally expensive. This paper proposes a method to select a single initial translation hypothesis before decoding based on a machine learning approach that judges the appropriateness of multiple initial translation hypotheses and selects the most confident one for decoding. Our approach is evaluated for the translation of dialogues in the travel domain, and the results show that it drastically reduces computational costs without a loss in translation quality."
2005.iwslt-1.5,Nobody is perfect: {ATR}{'}s hybrid approach to spoken language translation,2005,15,22,6,1,12388,michael paul,Proceedings of the Second International Workshop on Spoken Language Translation,0,"This paper describes ATRxe2x80x99s hybrid approach to spoken language translation and itxe2x80x99s application to the IWSLT 2005 translation task. Multiple corpus-based translation engines are used to translate the same input, whereby the best translation among the element MT outputs is selected according to statistical models. The evaluation results of the Japanese-to-English and Chinese-to-English translation tasks for different training data conditions showed the potential of the proposed hybrid approach and revealed new directions in how to improve the current system performance."
W04-1708,Automatic Measuring of {E}nglish Language Proficiency using {MT} Evaluation Technology,2004,6,3,3,1,4955,keiji yasuda,Proceedings of the Workshop on e{L}earning for Computational Linguistics and Computational Linguistics for e{L}earning,0,"Assisting in foreign language learning is one of the major areas in which natural language processing technology can contribute. This paper proposes a computerized method of measuring communicative skill in English as a foreign language. The proposed method consists of two parts. The first part involves a test sentence selection part to achieve precise measurement with a small test set. The second part is the actual measurement, which has three steps. Step one asks proficiency-known human subjects to translate Japanese sentences into English. Step two gauges the match between the translations of the subjects and correct translations based on the n-gram overlap or the edit distance between translations. Step three learns the relationship between proficiency and match. By regression it finds a straight-line fitting for the scatter plot representing the proficiency and matches of the subjects. Then, it estimates proficiency of proficiency-unknown users by using the line and the match. Based on this approach, we conducted experiments on estimating the Test of English for International Communication (TOEIC) score. We collected two sets of data consisting of English sentences translated from Japanese. The first set consists of 330 sentences, each translated to English by 29 subjects with varied English proficiency. The second set consists of 510 sentences translated in a similar manner by a separate group of 18 subjects. We found that the estimated scores correlated with the actual scores."
N04-4003,Example-based Rescoring of Statistical Machine Translation Output,2004,8,2,2,1,12388,michael paul,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"Conventional statistical machine translation (SMT) approaches might not be able to find a good translation due to problems in its statistical models (due to data sparseness during the estimation of the model parameters) as well as search errors during the decoding process. This paper1 presents an example-based rescoring method that validates SMT translation candidates and judges whether the selected decoder output is good or not. Given such a validation filter, defective translations can be rejected. The experiments show a drastic improvement in the overall system performance compared to translation selection methods based on statistical scores only."
shimohata-etal-2004-building,Building a Paraphrase Corpus for Speech Translation,2004,7,9,2,1,51054,mitsuo shimohata,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"When a machine translation (MT) system receives input sentences of spoken language, the following two types of sentences are difficult to translate: (1) long sentences and (2) sentences having redundant expressions often seen in spoken language. To reduce these difficulties, we are developing methods to paraphrase input sentences into more translatable ones. In this paper, we report a preliminary Japanese paraphrase corpus. The corpus consists of original sentences derived from travel conversation and versions of them paraphrased by humans. We use three paraphrasing methods: plain, segment, and summary paraphrasing. Plain paraphrasing is applied to short sentences, where redundant expressions are replaced with plain ones. Segment and summary paraphrasing is applied to long sentences, where long sentences are converted into one or several short sentences. We also report a comparison of machine translation quality between the original sentences and the paraphrased sentences. We use two corpus-based machine translation systems in the experiment."
akiba-etal-2004-incremental,Incremental Methods to Select Test Sentences for Evaluating Translation Ability,2004,4,0,2,1,51985,yasuhiro akiba,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper addresses the problem of selecting test sentences for automatically evaluating language learnersxe2x80x99 translation ability within a smaller error. In this paper, the ability to translate is measured as a TOEIC score. The existing selection methods only check whether an individual test sentence contributes to the estimation of the ability to translate or that of more general academic abilities, although combinations of test sentences may be used to contribute the estimation. This paper proposes two methods that solve the selection problem. The first method selects test sentences to minimize the estimation errors of learnersxe2x80x99 TOEIC scores. The second method selects test sentences to maximize the correlation coefficient between the number of correct translations and learnersxe2x80x99 estimated TOEIC scores. The optimization technique used in both of the proposed methods is the gradient technique in mathematical programming. The proposed methods proved to be more accurate than any of the existing methods we tested, and they estimated each TOEIC score within a permissible error of 69 points. This paper addresses the problem of selecting test sentences for automatically evaluating language learnersxe2x80x99 translation ability within a smaller error. This problem is regarded as an important issue in Test Theory (Wright and Stone, 1997) for precisely measuring learner ability. This paper attempts to solve the problem in the case of estimating a learnerxe2x80x99s TOEIC score based on other learnersxe2x80x99 scores. TOEIC (http://www.toeic.com) is the abbreviation of the xe2x80x9cTest of English for International Communicationxe2x80x9d , which was created by ETS (Educational Testing Service) as TOEFL was."
finch-etal-2004-automatic,How Does Automatic Machine Translation Evaluation Correlate with Human Scoring as the Number of Reference Translations Increases?,2004,8,10,3,1,16459,andrew finch,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
C04-1015,Example-based Machine Translation Based on Syntactic Transfer with Statistical Models,2004,16,24,4,1,324,kenji imamura,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation. Example-based MT sometimes generates invalid translations because it selects similar examples to the input sentence based only on source language similarity. The method proposed in this paper selects the best translation by using a language model and a translation model in the same manner as statistical MT, and it can improve MT quality over that of 'pure' example-based MT. A feature of this method is that the statistical models are applied after word re-ordering is achieved by syntactic transfer. This implies that MT quality is maintained even when we only apply a lexicon model as the translation model. In addition, translation speed is improved by bottom-up generation, which utilizes the tree structure that is output from the syntactic transfer."
C04-1017,Splitting Input Sentence for Machine Translation Using Language Model with Sentence Similarity,2004,15,13,2,1,51229,takao doi,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In order to boost the translation quality of corpus-based MT systems for speech translation, the technique of splitting an input sentence appears promising. In previous research, many methods used N-gram clues to split sentences. In this paper, to supplement N-gram based splitting methods, we introduce another clue using sentence similarity based on edit-distance. In our splitting method, we generate candidates for sentence splitting based on N-grams, and select the best one by measuring sentence similarity. We conducted experiments using two EBMT systems, one of which uses a phrase and the other of which uses a sentence as a translation unit. The translation results on various conditions were evaluated by objective measures and a subjective measure. The experimental results show that the proposed method is valuable for both systems."
C04-1030,Reordering Constraints for Phrase-Based Statistical Machine Translation,2004,17,105,4,0,30618,richard zens,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We investigate different reordering constraints for phrase-based statistical machine translation, namely the IBM constraints and the ITG constraints. We present efficient dynamic programming algorithms for both constraints. We evaluate the constraints with respect to translation quality on two Japanese-English tasks. We show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield statistically significant improvements compared to the unconstrained search."
C04-1047,Using a Mixture of N-Best Lists from Multiple {MT} Systems in Rank-Sum-Based Confidence Measure for {MT} Outputs,2004,14,7,2,1,51985,yasuhiro akiba,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper addressees the problem of eliminating unsatisfactory outputs from machine translation (MT) systems. The authors intend to eliminate unsatisfactory MT outputs by using confidence measures. Confidence measures for MT outputs include the rank-sum-based confidence measure (RSCM) for statistical machine translation (SMT) systems. RSCM can be applied to non-SMT systems but does not always work well on them. This paper proposes an alternative RSCM that adopts a mixture of the N-best lists from multiple MT systems instead of a single-system's N-best list in the existing RSCM. In most cases, the proposed RSCM proved to work better than the existing RSCM on two non-SMT systems and to work as well as the existing RSCM on an SMT system."
2004.tmi-1.12,Method for retrieving a similar sentence and its application to machine translation,2004,0,1,2,1,51054,mitsuo shimohata,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"xe8xa9xb1xe3x81x97xe8xa8x80xe8x91x89xe3x81xa7xe3x81x82xe3x82x8bxe7x99xbaxe8xa9xb1xe3x82x92xe5xafxbexe8xb1xa1xe3x81xa8xe3x81x97xe3x81xa6xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x82x92xe8xa1x8cxe3x81xa3xe3x81x9fxe5xa0xb4xe5x90x88, xe8xa9xb1xe3x81x97xe8xa8x80xe8x91x89xe7x89xb9xe6x9cx89xe3x81xaexe6x80xa7xe8xb3xaaxe3x81x8cxe4xb8x80xe5x9bxa0xe3x81xa8xe3x81xaaxe3x81xa3xe3x81xa6xe9x81xa9xe5x88x87xe3x81xaaxe7xbfxbbxe8xa8xb3xe6x96x87xe3x81x8cxe5xbex97xe3x82x89xe3x82x8cxe3x81xaaxe3x81x84xe5xa0xb4xe5x90x88xe3x81x8cxe3x81x82xe3x82x8b.xe6x9cxacxe8xabx96xe6x96x87xe3x81xa7xe3x81xaf, xe9x81xa9xe5x88x87xe3x81xaaxe7xbfxbbxe8xa8xb3xe6x96x87xe3x81x8cxe5xbex97xe3x82x89xe3x82x8cxe3x81xaaxe3x81x8bxe3x81xa3xe3x81x9fxe5xa0xb4xe5x90x88xe3x81xabxe9xa1x9exe4xbcxbcxe6x96x87xe6xa4x9cxe7xb4xa2xe6x8ax80xe8xa1x93xe3x82x92xe7x94xa8xe3x81x84xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7xe9x81xa9xe5x88x87xe3x81xaaxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe6x96xb9xe6xb3x95xe3x82x92xe6x8fx90xe6xa1x88xe3x81x99xe3x82x8b.xe4xb8x8exe3x81x88xe3x82x89xe3x82x8cxe3x81x9fxe5x85xa5xe5x8ax9bxe6x96x87xe3x81x8cxe9x81xa9xe5x88x87xe3x81xabxe7xbfxbbxe8xa8xb3xe3x81xa7xe3x81x8dxe3x81xaaxe3x81x84xe3x81xa8xe5x88xa4xe6x98x8exe3x81x97xe3x81x9fxe5xa0xb4xe5x90x88xe3x81xab, xe7xbfxbbxe8xa8xb3xe5x8fxafxe8x83xbdxe3x81xaaxe6x96x87xe3x82x92xe9x9bx86xe3x82x81xe3x81x9fxe3x82xb3xe3x83xbcxe3x83x91xe3x82xb9xe3x81x8bxe3x82x89xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe9xa1x9exe4xbcxbcxe6x96x87xe3x82x92xe6xa4x9cxe7xb4xa2xe3x81x99xe3x82x8b.xe6xa4x9cxe7xb4xa2xe3x81x95xe3x82x8cxe3x81x9fxe9xa1x9exe4xbcxbcxe6x96x87xe3x82x92xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x81xabxe4xb8x8exe3x81x88xe3x81xa6xe7xbfxbbxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7, xe9x81xa9xe5x88x87xe3x81xaaxe7xbfxbbxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe3x81x93xe3x81xa8xe3x81x8cxe3x81xa7xe3x81x8dxe3x82x8b.xe6xa4x9cxe7xb4xa2xe5xafxbexe8xb1xa1xe3x81xa8xe3x81xaaxe3x82x8bxe6x96x87 (xe5x80x99xe8xa3x9cxe6x96x87) xe3x81xa8xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe9x96x93xe3x81xaexe9xa1x9exe4xbcxbcxe5xbaxa6xe3x81xaf, xe5x80x99xe8xa3x9cxe6x96x87xe3x81xa8xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe9x96x93xe3x81xa7xe5x85xb1xe9x80x9axe3x81x99xe3x82x8bN-gramxe3x81xaexe6xafx94xe7x8ex87xe3x81xabxe5x9fxbaxe3x81xa5xe3x81x84xe3x81xa6xe7xaex97xe5x87xbaxe3x81x99xe3x82x8b.xe3x81x95xe3x82x89xe3x81xab, xe5x85xa5xe5x8ax9bxe6x96x87xe3x81xabxe3x81xaaxe3x81x84xe5x86x85xe5xaexb9xe8xaax9exe3x82x92xe5x90xabxe3x82x80xe5x80x99xe8xa3x9cxe6x96x87xe3x81xafxe5xafxbexe8xb1xa1xe5xa4x96xe3x81xa8xe3x81x99xe3x82x8bxe3x81x93xe3x81xa8xe3x82x84xe6xa9x9fxe8x83xbdxe8xaax9exe3x81xaexe9x87x8dxe3x81xbfxe3x82x92xe6xb8x9bxe3x82x89xe3x81x99xe3x81xa8xe3x81x84xe3x81xa3xe3x81x9fxe4xbbx98xe5x8axa0xe6x9dxa1xe4xbbxb6xe3x82x92xe5x8axa0xe3x81x88xe3x82x8bxe3x81x93xe3x81xa8xe3x81xa7xe7xb2xbexe5xbaxa6xe5x90x91xe4xb8x8axe3x82x92xe5x9bxb3xe3x81xa3xe3x81x9f.xe6x97xa5xe6x9cxacxe8xaax9exe3x81xabxe3x81x8axe3x81x91xe3x82x8bxe9xa1x9exe4xbcxbcxe6x96x87xe6xa4x9cxe7xb4xa2xe3x81xaexe5xaex9fxe9xa8x93xe3x81xa7xe3x81xaf, xe4xb8x8exe3x81x88xe3x81x9fxe5x85xa5xe5x8ax9bxe6x96x87xe3x81xaexe5x86x8587T2%xe3x81xabxe3x81xa4xe3x81x84xe3x81xa6xe6xa4x9cxe7xb4xa2xe6x96x87xe3x82x92xe5x87xbaxe5x8ax9bxe3x81x97, xe3x81x9dxe3x82x8cxe3x82x89xe3x81xaexe6xa4x9cxe7xb4xa2xe6x96x87xe3x81xaexe5x86x8560.4%xe3x81xafxe9x81xa9xe5x88x87xe3x81xaaxe9xa1x9exe4xbcxbcxe6x96x87xe3x81xa7xe3x81x82xe3x81xa3xe3x81x9f.xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x81xa8xe7xb5x84xe3x81xbfxe5x90x88xe3x82x8fxe3x81x9bxe3x81x9fxe5xaex9fxe9xa8x93xe3x81xa7xe3x81xaf, xe7xbfxbbxe8xa8xb3xe4xb8x8dxe8x83xbdxe3x81xa8xe3x81xaaxe3x81xa3xe3x81x9fxe5x85xa5xe5x8ax9bxe6x96x87xe3x81xabxe3x81xa4xe3x81x84xe3x81xa6xe9xa1x9exe4xbcxbcxe6x96x87xe3x82x92xe6xa4x9cxe7xb4xa2xe3x81x95xe3x81x9b, xe3x81x9dxe3x82x8cxe3x82x89xe3x82x92xe6xa9x9fxe6xa2xb0xe7xbfxbbxe8xa8xb3xe3x81xabxe3x81x8bxe3x81x91xe3x81x9fxe3x81xa8xe3x81x93xe3x82x8d, xe7xbfxbbxe8xa8xb3xe4xb8x8dxe8x83xbdxe6x96x87xe3x81xaexe5x86x8525.9%xe3x81xabxe3x81xa4xe3x81x84xe3x81xa6xe9x81xa9xe5x88x87xe3x81xaaxe8xa8xb3xe6x96x87xe3x82x92xe5xbex97xe3x82x8bxe3x81x93xe3x81xa8xe3x81x8cxe3x81xa7xe3x81x8dxe3x81x9f."
2004.iwslt-evaluation.2,"{EBMT}, {SMT}, hybrid and more: {ATR} spoken language translation system",2004,24,14,1,1,129,eiichiro sumita,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper introduces ATRxe2x80x99s project named Corpus-Centered Computation (C3), which aims at developing a translation technology suitable for spoken language translation. C3 places corpora at the center of its technology. Translation knowledge is extracted from corpora, translation quality is gauged by referring to corpora, the best translation among multiple-engine outputs is selected based on corpora, and the corpora themselves are paraphrased or filtered by automated processes to improve the data quality on which translation engines are based. In particular, this paper reports the hybridization architecture of different machine translation systems, our technologies, their performance on the IWSLT04 task, and paraphrasing methods."
W03-0311,Retrieving Meaning-equivalent Sentences for Example-based Rough Translation,2003,17,2,2,1,51054,mitsuo shimohata,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"Example-based machine translation (EBMT) is a promising translation method for speech-to-speech translation because of its robustness. It retrieves example sentences similar to the input and adjusts their translations to obtain the output. However, it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving meaning-equivalent sentences to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to rough translations. The retrieval is based on content words, modality, and tense."
W03-0318,Input Sentence Splitting and Translating,2003,8,11,2,1,51229,takao doi,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"We propose a method to split and translate input sentences for speech translation in order to overcome the long sentence problem. This approach is based on three criteria used to judge the goodness of translation results. The criteria utilize the output of an MT system only and assumes neither a particular language nor a particular MT approach. In an experiment with an EBMT system, in which prior methods cannot work or work badly, the proposed split-and-translate method achieves much better results in translation quality."
P03-1039,Chunk-Based Statistical Translation,2003,12,31,2,1,128,taro watanabe,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation. The translation model suggested here first performs chunking. Then, each word in a chunk is translated. Finally, translated chunks are reordered. Under this scenario of translation modeling, we have experimented on a broad-coverage Japanese-English traveling corpus and achieved improved performance."
P03-1057,Feedback Cleaning of Machine Translation Rules Using Automatic Evaluation,2003,11,51,2,1,324,kenji imamura,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"When rules of transfer-based machine translation (MT) are automatically acquired from bilingual corpora, incorrect/redundant rules are generated due to acquisition errors or translation variety in the corpora. As a new countermeasure to this problem, we propose a feedback cleaning method using automatic evaluation of MT quality, which removes incorrect/redundant rules as a way to increase the evaluation score. BLEU is utilized for the automatic evaluation. The hill-climbing algorithm, which involves features of this task, is applied to searching for the optimal combination of rules. Our experiments show that the MT quality improves by 10% in test sentences according to a subjective evaluation. This is considerable improvement over previous methods."
N03-2006,Adaptation Using Out-of-Domain Corpus within {EBMT},2003,7,3,2,1,51229,takao doi,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"In order to boost the translation quality of EBMT based on a small-sized bilingual corpus, we use an out-of-domain bilingual corpus and, in addition, the language model of an in-domain monolingual corpus. We conducted experiments with an EBMT system. The two evaluation measures of the BLEU score and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model."
N03-2013,Automatic Expansion of Equivalent Sentence Set Based on Syntactic Substitution,2003,4,2,3,1,324,kenji imamura,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"In this paper, we propose an automatic quantitative expansion method for a sentence set that contains sentences of the same meaning (called an equivalent sentence set). This task is regarded as paraphrasing. The features of our method are: 1) The paraphrasing rules are dynamically acquired by Hierarchical Phrase Alignment from the equivalent sentence set, and 2) A large equivalent sentence set is generated by substituting source syntactic structures. Our experiments show that 561 sentences on average are correctly generated from 8.48 equivalent sentences."
E03-1029,Automatic Construction of Machine Translation Knowledge Using Translation Literalness,2003,10,14,2,1,324,kenji imamura,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"When machine translation (MT) knowledge is automatically constructed from bilingual corpora, redundant rules are acquired due to translation variety. These rules increase ambiguity or cause incorrect MT results. To overcome this problem, we constrain the sentences used for knowledge extraction to the appropriate bilingual sentences for the MT. In this paper, we propose a method using translation literalness to select appropriate sentences or phrases. The translation correspondence rate (TCR) is defined as the literalness measure.Based on the TCR, two automatic construction methods are tested. One is to filter the corpus before rule acquisition. The other is to split the acquisition process into two phases, where a bilingual sentence is divided into literal parts and the other parts before different generalizations are applied. The effects are evaluated by the MT quality, and about 4.9% of MT results were improved by the latter method."
E03-1048,A corpus-centered approach to spoken language translation,2003,21,14,1,1,129,eiichiro sumita,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper reports the latest performance of components and features of a project named Corpus-Centered Computation (C3), which targets a translation technology suitable for spoken language translation. C3 places corpora at the center of the technology. Translation knowledge is extracted from corpora by both EBMT and SMT methods, translation quality is gauged by referring to corpora, the best translation among multiple-engine outputs is selected based on corpora and the corpora themselves are paraphrased or filtered by automated processes."
2003.mtsummit-papers.1,Experimental comparison of {MT} evaluation methods: {RED} vs.{BLEU},2003,-1,-1,2,1,51985,yasuhiro akiba,Proceedings of Machine Translation Summit IX: Papers,0,"This paper experimentally compares two automatic evaluators, RED and BLEU, to determine how close the evaluation results of each automatic evaluator are to average evaluation results by human evaluators, following the ATR standard of MT evaluation. This paper gives several cautionary remarks intended to prevent MT developers from drawing misleading conclusions when using the automatic evaluators. In addition, this paper reports a way of using the automatic evaluators so that their results agree with those of human evaluators."
2003.mtsummit-papers.47,Example-based rough translation for speech-to-speech translation,2003,-1,-1,2,1,51054,mitsuo shimohata,Proceedings of Machine Translation Summit IX: Papers,0,"Example-based machine translation (EBMT) is a promising translation method for speech-to-speech translation (S2ST) because of its robustness. However, it has two problems in that the performance degrades when input sentences are long and when the style of the input sentences and that of the example corpus are different. This paper proposes example-based rough translation to overcome these two problems. The rough translation method relies on {``}meaning-equivalent sentences,{''} which share the main meaning with an input sentence despite missing some unimportant information. This method facilitates retrieval of meaning-equivalent sentences for long input sentences. The retrieval of meaning-equivalent sentences is based on content words, modality, and tense. This method also provides robustness against the style differences between the input sentence and the example corpus."
2003.mtsummit-papers.54,Example-based decoding for statistical machine translation,2003,-1,-1,2,1,128,taro watanabe,Proceedings of Machine Translation Summit IX: Papers,0,"This paper presents a decoder for statistical machine translation that can take advantage of the example-based machine translation framework. The decoder presented here is based on the greedy approach to the decoding problem, but the search is initiated from a similar translation extracted from a bilingual corpus. The experiments on multilingual translations showed that the proposed method was far superior to a word-by-word generation beam search algorithm."
W02-1611,Identifying Synonymous Expressions from a Bilingual Corpus for Example-Based Machine Translation,2002,6,4,2,1,51054,mitsuo shimohata,{COLING}-02: Machine Translation in Asia,0,"Example-based machine translation (EBMT) is based on a bilingual corpus. In EBMT, sentences similar to an input sentence are retrieved from a bilingual corpus and then output is generated from translations of similar sentences. Therefore, a similarity measure between the input sentence and each sentence in the bilingual corpus is important for EBMT. If some similar sentences are missed from retrieval, the quality of translations drops. In this paper, we describe a method to acquire synonymous expressions from a bilingual corpus and utilize them to expand retrieval of similar sentences. Synonymous expressions are acquired from differences in synonymous sentences. Synonymous sentences are clustered by the equivalence of translations. Our method has the advantage of not relying on rich linguistic knowledge, such as sentence structure and dictionaries. We demonstrate the effect on applying our method to a simple EBMT."
W02-0701,Corpus-Centered Computation,2002,22,4,1,1,129,eiichiro sumita,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"To achieve translation technology that is adequate for speech-to-speech translation (S2S), this paper introduces a new attempt named Corpus-Centered Computation, (abbreviated to C3 and pronounced c-cube). As opposed to conventional approaches adopted by machine translation systems for written language, C3 places corpora at the center of the technology. For example, translation knowledge is extracted from corpora, translation quality is gauged by referring to corpora and the corpora themselves are normalized by paraphrasing or filtering. High-quality translation has been demonstrated in the domain of travel conversation, and the prospects of this approach are promising due to the benefits of synergistic effects."
shimohata-sumita-2002-automatic,Automatic paraphrasing based on parallel corpus for normalization,2002,6,23,2,1,51054,mitsuo shimohata,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Abstract There are various ways to express the same meaning in natural language. This diversity causes difficulty in many fields of natural language processing. It can be reduced by normalization of synonymous expressions, which is done by replacing various synonymous expressions with a standard one. In this paper, we propose a method for extracting paraphrases from a parallel corpus automatically and utilizing them for normalization. First, synonymous sentences are grouped by the equivalence of translation. Then, synonymous expressions are extracted by the differences between synonymous sentences. Synonymous expressions contain not only interchangeable words but also surrounding words in order to consider contextual condition. Our method has two advantages: 1) only a parallel corpus is required, and 2) various types of paraphrases can be acquired."
watanabe-etal-2002-statistical,Statistical Machine Translation on Paraphrased Corpora,2002,7,5,3,1,128,taro watanabe,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents a statistical machine translation trained on normalized corpora. The automatic paraphrasing is carried out by inducing paraphrasing expressions from a bilingual corpus. Then, the normalization is treated as a specific paraphrase of a given input determined by the frequency in a corpus. The experimental results on Japanese-to-English translation with normalized English corpus exhibited the reduction of word-error-rate by 8% and the improvement of subjective evaluation from 70% into 72.5%."
takezawa-etal-2002-toward,Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World,2002,7,212,2,0.493927,43353,toshiyuki takezawa,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Abstract At ATR Spoken Language Translation Research Laboratories, we are building a broad-coverage bilingual corpus to study corpus-based speech translation technologies for the real world. There are three important points to consider in designing and constructing a corpus for future speech translation research. The first is to have a variety of speech samples, with a wide range of pronunciations and speakers. The second is to have data for a variety of situations. The third is to have a variety of expressions. This paper reports our trials and discusses the methodology. First, we introduce a bilingual travel conversation (TC) corpus of spoken languages and a broad-coverage bilingual basic expression (BE) corpus. TC and BE are designed to be complementary. TC is a collection of transcriptions of bilingual spoken dialogues, while BE is a collection of Japanese sentences and their English translations. Whereas TC covers a small domain, BE covers a wide variety of domains. We compare the characteristics of vocabulary and expressions between these two corpora and suggest that we need a much greater variety of expressions. One promising approach might be to collect paraphrases representing various different expressions generated by many people for similar concepts."
C02-1017,Corpus-based Generation of Numeral Classifier using Phrase Alignment,2002,17,4,2,1,12388,michael paul,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"A severe problem for NLP applications dealing with multilingual language resources is the acquisition of knowledge that is obligatory in one language but not explicitly expressed in another language. In this paper, we focus on numeral classifiers, which are required in languages like Japanese but are usually not explicitly used in languages like English, which don't have such a classifier system.We propose a uniform method to assign the numeral classifiers of languages that have a numeral classifier system to the numerals of non-classifier languages. The omitted classifier information is extracted from a bilingual corpus based on phrasal correspondences in the contexts of the respective sentences."
C02-1050,Bidirectional Decoding for Statistical Machine Translation,2002,10,34,2,1,128,taro watanabe,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes the right-to-left decoding method, which translates an input string by generating in right-to-left direction. In addition, presented is the bidirectional decoding method, that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions. The experimental results on Japanese and English translation showed that the right-to-left was better for Englith-to-Japanese translation, while the left-to-right was suitable for Japanese-to-English translation. It was also observed that the bidirectional method was better for English-to-Japanese translation."
C02-1076,Using Language and Translation Models to Select the Best among Outputs from Multiple {MT} Systems,2002,12,35,3,1,51985,yasuhiro akiba,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper addresses the problem of automatically selecting the best among outputs from multiple machine translation (MT) systems. Existing approaches select the output assigned the highest score according to a target language model. In some cases, the existing approaches do not work well. This paper proposes two methods to improve performance. The first method is based on a multiple comparison test and checks whether a score from language and translation models is significantly higher than the others. The second method is based on probability that a translation is not inferior to the others, which is predicted from the above scores. Experimental results show that the proposed methods achieve an improvement of 2 to 6% in performance."
2002.tmi-tutorials.1,Example-based machine translation,2002,11,12,1,1,129,eiichiro sumita,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Tutorials,0,"Translation is a repetitive activity. The attempt to automate such a difficult task has been a long-term scientific dream; in the past years research in this field has acquired a growing interest, making some forms of Machine Translation (MT) a reality. Among the several types of approaches in MT, one of the most promising paradigms is MAHT and, in particular, example-Based Machine Translation (EBMT). An EBMT system translates by analogy, using past translations to translate other, similar sourcelanguage sentences into the target language. The basic premise is that, if a previously translated sentence occurs again, the same translation is likely to be correct. In this paper, we propose a solution based on a purely syntactic approach for searching similar sentences and parts of them in an EBMT system; the underlying similarity measure is based on the similarity between sequence of terms such that the sentences most close to a given one are those who maintain most of the original form and contents. The system efficiently retrieves and ranks the most similar sentences available and, when no useful suggestion exists, it proceeds with the retrieval of similar parts. We opted for a design that would require minimal changes to existing databases and whose similarity measure and search algorithms are completely independent from the involved languages. This work has been developed as a joint work with LOGOS S.p.A., a worldwide leader in multilingual document translation."
2002.tmi-papers.20,Statistical machine translation based on hierarchical phrase alignment,2002,11,43,3,1,128,taro watanabe,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,"This paper describes statistical machine translation improved by applying hierarchical phrase alignment. The hierarchical phrase alignment is a method to align bilingual sentences phrase-by-phrase employing the partial parse results. Based on the hierarchical phrase alignment, a translation model is trained on a chunked corpus by converting hierarchically aligned phrases into a sequence of chunks. The second method transforms the bilingual correspondence of the phrase alignments into that of translation model. Both of our approaches yield better quality of the translaiton model."
W01-1615,Integration of Referential Scope Limitations into {J}apanese Pronoun Resolution,2001,6,0,2,1,12388,michael paul,Proceedings of the Second {SIG}dial Workshop on Discourse and Dialogue,0,"We propose a practical approach to the anaphora resolution of Japanese pronouns incorporating knowledge about referential scope limitations extracted from an annotated corpus. A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates. The resolution scope of each pronoun is limited according to the relative distance distribution of the training data, resulting in increases in the classification accuracy and analysis speed by causing only a minor decrease in the recall performance."
W01-1401,Example-based machine translation using {DP}-matching between work sequences,2001,17,44,1,1,129,eiichiro sumita,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"We propose a new approach under the example-based machine translation paradigm. First, the proposed approach retrieves the most similar example by carrying out DP-matching of the input sentence and example sentences while measuring the semantic distance of the words. Second, the approach adjusts the gap between the input and the most similar example by using a bilingual dictionary. We show the results of a computational experiment."
2001.mtsummit-papers.3,Using multiple edit distances to automatically rank machine translation output,2001,9,55,3,1,51985,yasuhiro akiba,Proceedings of Machine Translation Summit VIII,0,"This paper addresses the challenging problem of automatically evaluating output from machine translation (MT) systems in order to support the developers of these systems. Conventional approaches to the problem include methods that automatically assign a rank such as A, B, C, or D to MT output according to a single edit distance between this output and a correct translation example. The single edit distance can be differently designed, but changing its design makes assigning a certain rank more accurate, but another rank less accurate. This inhibits improving accuracy of rank assignment. To overcome this obstacle, this paper proposes an automatic ranking method that, by using multiple edit distances, encodes machine-translated sentences with a rank assigned by humans into multi-dimensional vectors from which a classifier of ranks is learned in the form of a decision tree (DT). The proposed method assigns a rank to MT output through the learned DT. The proposed method is evaluated using transcribed texts of real conversations in the travel arrangement domain. Experimental results show that the proposed method is more accurate than the single-edit-distance-based ranking methods, in both closed and open tests. Moreover, the proposed method could estimate MT quality within 3{\%} error in some cases."
P00-1054,Lexical Transfer Using a Vector-Space Model,2000,6,8,1,1,129,eiichiro sumita,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"Building a bilingual dictionary for transfer in a machine translation system is conventionally done by hand and is very time-consuming. In order to overcome this bottleneck, we propose a new mechanism for lexical transfer, which is simple and suitable for learning from bilingual corpora. It exploits a vector-space model developed in information retrieval research. We present a preliminary result from our computational experiment."
A00-1006,Translation using Information on Dialogue Participants,2000,10,7,2,0,42063,setsuo yamada,Sixth Applied Natural Language Processing Conference,0,"This paper proposes a way to improve the translation quality by using information on dialogue participants that is easily obtained from outside the translation component. We incorporated information on participants' social roles and genders into transfer rules and dictionary entries. An experiment with 23 unseen dialogues demonstrated a recall of 65% and a precision of 86%. These results showed that our simple and easy-to-implement method is effective, and is a key technology enabling smooth conversation with a dialogue translation system."
W99-0207,Corpus-Based Anaphora Resolution Towards Antecedent Preference,1999,13,8,3,1,12388,michael paul,Coreference and Its Applications,0,"In this paper we propose a corpus-based approach to anaphora resolution combining a machine learning method and statistical information. First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the number of potential candidates. In the second step, preference selection is achieved by taking into account the frequency information of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current discourse. Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%."
1999.mtsummit-1.34,Solutions to problems inherent in spoken-language translation: the {ATR}-{MATRIX} approach,1999,8,39,1,1,129,eiichiro sumita,Proceedings of Machine Translation Summit VII,0,"ATR has built a multi-language speech translation system called ATR-MATRIX. It consists of a spoken-language translation subsystem, which is the focus of this paper, together with a highly accurate speech recognition subsystem and a high-definition speech synthesis subsystem. This paper gives a road map of solutions to the problems inherent in spoken-language translation. Spoken-language translation systems need to tackle difficult problems such as ungrammaticality. contextual phenomena, speech recognition errors, and the high-speeds required for real-time use. We have made great strides towards solving these problems in recent years. Our approach mainly uses an example-based translation model called TDMT. We have added the use of extra-linguistic information, a decision tree learning mechanism, and methods dealing with recognition errors."
P98-2233,Feasibility Study for Ellipsis Resolution in Dialogues by Machine-Learning Technique,1998,5,8,2,1,27519,kazuhide yamamoto,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"A method for resolving the ellipses that appear in Japanese dialogues is proposed. This method resolves not only the subject ellipsis, but also those in object and other grammatical cases. In this approach, a machine-learning algorithm is used to select the attributes necessary for a resolution. A decision tree is built, and used as the actual ellipsis resolver. The results of blind tests have shown that the proposed method was able to provide a resolution accuracy of 91.7% for indirect objects, and 78.7% for subjects with a verb predicate. By investigating the decision tree we found that topic-dependent attributes are necessary to obtain high performance resolution, and that indispensable attributes vary according to the grammatical case. The problem of data size relative to decision-tree training is also discussed."
P98-1107,A Method for Correcting Errors in Speech Recognition using the Statistical Features of Character Co-occurrence,1998,5,24,2,0,55360,satoshi kaki,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system. This paper proposes a method for correcting errors using the statistical features of character co-occurrence, and evaluates the method.The proposed method comprises two successive correcting processes. The first process uses pairs of strings: the first string is an erroneous substring of the utterance predicted by speech recognition, the second string is the corresponding section of the actual utterance. Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs. The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors.The results of our evaluation show that the use of our proposed method as a post-processor for speech recognition is likely to make a significant contribution to the performance of speech translation systems."
C98-2228,Feasibility Study for Ellipsis Resolution in Dialogues by Machine-Learning Technique,1998,5,8,2,1,27519,kazuhide yamamoto,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"A method for resolving the ellipses that appear in Japanese dialogues is proposed. This method resolves not only the subject ellipsis, but also those in object and other grammatical cases. In this approach, a machine-learning algorithm is used to select the attributes necessary for a resolution. A decision tree is built, and used as the actual ellipsis resolver. The results of blind tests have shown that the proposed method was able to provide a resolution accuracy of 91.7% for indirect objects, and 78.7% for subjects with a verb predicate. By investigating the decision tree we found that topic-dependent attributes are necessary to obtain high performance resolution, and that indispensable attributes vary according to the grammatical case. The problem of data size relative to decision-tree training is also discussed."
C98-1103,A Method for Correcting Errors in Speech Recognition Using the Statistical Features of Character Co-occurrence,1998,5,24,2,0,55360,satoshi kaki,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system. This paper proposes a method for correcting errors using the statistical features of character co-occurrence, and evaluates the method.The proposed method comprises two successive correcting processes. The first process uses pairs of strings: the first string is an erroneous substring of the utterance predicted by speech recognition, the second string is the corresponding section of the actual utterance. Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs. The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors.The results of our evaluation show that the use of our proposed method as a post-processor for speech recognition is likely to make a significant contribution to the performance of speech translation systems."
C96-2191,Spoken-Language Translation Method Using Examples,1996,7,12,2,0,48354,hitoshi iida,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,None
1995.tmi-1.20,Heterogeneous Computing for Example-Based Translation of Spoken Language,1995,-1,-1,1,1,129,eiichiro sumita,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
A94-1017,Real-Time Spoken Language Translation Using Associative Processors,1994,12,6,2,0,56560,kozo oi,Fourth Conference on Applied Natural Language Processing,0,"This paper proposes a model using associative processors (APs) for real-time spoken language translation. Spoken language translation requires (1) an accurate translation and (2) a realtime response. We have already proposed a model, TDMT (Transfer-Driven Machine Translation), that translates a sentence utilizing examples effectively and performs accurate structural disambiguation and target word selection. This paper will concentrate on the second requirement. In TDMT, example-retrieval (ER), i.e., retrieving examples most similar to an input expression, is the most dominant part of the total processing time. Our study has concluded that we only need to implement the ER for expressions including a frequent word on APs. Experimental results show that the ER can be drastically speeded up. Moreover, a study on communications between APs demonstrates the scalability against vocabulary size by extrapolation. Thus, our model, TDMT on APs, meets the vital requirements of spoken language translation."
1993.tmi-1.7,An Example-Based Disambiguation of Prepositional Phrase Attachment,1993,-1,-1,1,1,129,eiichiro sumita,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
P91-1024,Experiments and Prospects of Example-Based Machine Translation,1991,8,137,1,1,129,eiichiro sumita,29th Annual Meeting of the Association for Computational Linguistics,1,"EBMT (Example-Based Machine Translation) is proposed. EBMT retrieves similar examples (pairs of source phrases, sentences, or texts and their translations) from a database of examples, adapting the examples to translate a new input. EBMT has the following features: (1) It is easily upgraded simply by inputting appropriate examples to the database; (2) It assigns a reliability factor to the translation result; (3) It is accelerated effectively by both indexing and parallel computing; (4) It is robust because of best-match reasoning; and (5) It well utilizes translator expertise. A prototype system has been implemented to deal with a difficult translation problem for conventional Rule-Based Machine Translation (RBMT), i.e., translating Japanese noun phrases of the form N1 no N2 into English. The system has achieved about a 78% success rate on average. This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the broad applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT."
1988.tmi-1.13,A translation aid system using flexible text retrieval based on syntax-matching,1988,-1,-1,1,1,129,eiichiro sumita,Proceedings of the Second Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
