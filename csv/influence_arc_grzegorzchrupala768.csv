2020.acl-main.1,N19-1423,0,0.104147,"al context accompanying an utterance, as in visual grounding, we train it to imagine the semantic content. Note that since the semantic embeddings are based on the transcriptions of the sentences themselves, they have a much closer relation to the sentences than visual context representations would have. The semantic sentence representations were obtained using SBERT, a BERT-based architecture that yields sentence embeddings, which was finetuned on the STS benchmark of SemEval (Reimers and Gurevych, 2019). This particular encoding was chosen because it harnesses the semantic strength of BERT (Devlin et al., 2019) in an encoding of the sentence as a whole. Speech is converted Melfrequency cepstrum coefficients. Speech recognition with non-linguistic supervision In recent years, several studies have worked on machine learning tasks in which models directly extract semantic information from speech, without feedback on the word, character, or phoneme level. Most prominently, work on ‘weakly supervised’ speech recognition includes work in which accompanying visual information is used as a proxy for semantic information. By grounding speech in visual information accompanying it, models can learn 4 4.1 Data"
2020.acl-main.1,P17-1047,0,0.0315837,"t both ADS and CDS trained systems perform best on the matching test set, but CDS trained systems perform better on ADS than systems trained on ADS peform on CDS. They show that this is likely caused by phonetic classes have larger overlaps in CDS. To the authors’ knowledge, the current work is the first to computationally explore learnability differences between ADS and CDS considering the process of speech comprehension as a whole: from audio to semantic information. 2.2 to extract visually relevant semantic information from speech, without needing symbolic annotation (Harwath et al., 2016; Harwath and Glass, 2017; Chrupała et al., 2017; Merkx et al., 2019). The topic is of interest for automatic speech recognition, as it provides potential ways of training speech recognition without the need for vast amounts of annotation. The utilization of nonlinguistic information as supervision is particularly useful for low-resource languages. For the purpose of this study, however, we are interested in this set of problems because of the parallel to human language acquisition. A language learning child does not receive explicit feedback on the words or phonemes it perceives. Rather, they learn to infer these str"
2020.acl-main.1,D19-1410,0,0.0567509,"tation, we use semantic sentence embeddings of the transcriptions. Rather than training our model to imagine the visual context accompanying an utterance, as in visual grounding, we train it to imagine the semantic content. Note that since the semantic embeddings are based on the transcriptions of the sentences themselves, they have a much closer relation to the sentences than visual context representations would have. The semantic sentence representations were obtained using SBERT, a BERT-based architecture that yields sentence embeddings, which was finetuned on the STS benchmark of SemEval (Reimers and Gurevych, 2019). This particular encoding was chosen because it harnesses the semantic strength of BERT (Devlin et al., 2019) in an encoding of the sentence as a whole. Speech is converted Melfrequency cepstrum coefficients. Speech recognition with non-linguistic supervision In recent years, several studies have worked on machine learning tasks in which models directly extract semantic information from speech, without feedback on the word, character, or phoneme level. Most prominently, work on ‘weakly supervised’ speech recognition includes work in which accompanying visual information is used as a proxy for"
2020.acl-main.1,P17-1057,1,\N,Missing
2020.acl-main.381,D19-1593,0,0.0157822,"ns are: • Probing techniques, or diagnostic classifiers, i.e. methods which use the activations from different layers of a deep learning architecture as input to a prediction model (e.g., Adi et al., 2017; Alishahi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018); 1 See https://github.com/gchrupala/analyzing-analyticalmethods. • Representational Similarity Analysis (RSA) borrowed from neuroscience (Kriegeskorte et al., 2008) and used to correlate similarity structures of two different representation spaces (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Abnar et al., 2019; Abdou et al., 2019). We use both techniques in our experiments to systematically compare their output. 2.2 Analyzing random representations Research on the analysis of neural encodings of language has shown that in some cases, substantial information can be decoded from activation patterns of randomly initialized, untrained recurrent networks. It has been suggested that the dynamics of the network together with the characteristics of the input signal can result in non-random activation patterns (Zhang and Bowman, 2018). Using activations generated by randomly initialized recurrent networks has a history in speec"
2020.acl-main.381,Q19-1004,0,0.0969545,"ce. We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent results and we recommend their use as a complement to local-scope diagnostic methods. 1 Introduction As end-to-end architectures based on neural networks became the tool of choice for processing speech and language, there has been increased interest in techniques for analyzing and interpreting the representations emerging in these models. A large array of analytical techniques have been proposed and applied to diverse tasks and architectures (Belinkov and Glass, 2019; Alishahi et al., 2019). Given the fast development of analysis techniques for NLP and speech processing systems, relatively few systematic studies have been conducted to compare the strengths and weaknesses of each methodology and to assess the reliability and explanatory power of their outcomes in controlled settings. This paper reports a step in this direction: as a case study, we examine the representation of phonology in neural network models of spoken language. We choose three different models that process speech signal as input, and analyze their learned neural representations. We use"
2020.acl-main.381,D18-1119,0,0.0177559,"the focus of our paper. Two commonly used approaches to analyzing representations are: • Probing techniques, or diagnostic classifiers, i.e. methods which use the activations from different layers of a deep learning architecture as input to a prediction model (e.g., Adi et al., 2017; Alishahi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018); 1 See https://github.com/gchrupala/analyzing-analyticalmethods. • Representational Similarity Analysis (RSA) borrowed from neuroscience (Kriegeskorte et al., 2008) and used to correlate similarity structures of two different representation spaces (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Abnar et al., 2019; Abdou et al., 2019). We use both techniques in our experiments to systematically compare their output. 2.2 Analyzing random representations Research on the analysis of neural encodings of language has shown that in some cases, substantial information can be decoded from activation patterns of randomly initialized, untrained recurrent networks. It has been suggested that the dynamics of the network together with the characteristics of the input signal can result in non-random activation patterns (Zhang and Bowman, 2018). Using activations gener"
2020.acl-main.381,P19-1647,1,0.848981,"property of an analytical method is that it is sensitive to the learning effect, and that the scores on trained versus randomly initialized models are clearly separated. Coefficient of partial determination Correlation between similarity structures of two representational spaces can, in principle, be partly due to the fact that both these spaces are correlated to a third space. For example, were we to get a high value for global RSA for one of the top layers of the RNNVGS model, we might suspect that this is due to the 3 Note that the visually grounded speech models of Chrupała et al. (2017); Chrupała (2019); Merkx et al. (2019) use similar mechanisms to aggregate the activations of the final RNN layer; here we use it as part of the analytical method to pool any sequential representation of interest. A further point worth noting is that we use scalar weights αt and apply a linear model for learning them in order to keep the analytic model simple and easy to train consistently. eY ∼Z − eY ∼X+Z eY ∼Z (5) where eY ∼X+Z is the sum squared error of the model with all variables, and eY ∼Z is the sum squared error of the model with X removed. Given the scenario above with the confounding space being vis"
2020.acl-main.381,P19-1283,1,0.883982,"use two commonly applied analytical techniques: (i) diagnostic models and (ii) representational similarity analysis to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. In our experiments, we manipulate two important factors that can affect the outcome of analysis. One pitfall not always successfully avoided in work on neural representation analysis is the role of learning. Previous work has shown that sometimes non-trivial representations can be found in the activation patterns of randomly initialized, untrained neural networks (Zhang and Bowman, 2018; Chrupała and Alishahi, 2019). Here we investigate the representations of phonology in neural models of spoken language in light of this fact, as extant studies have not properly controlled for role of learning in these representations. The second manipulated factor in our experiments is the scope of the extracted neural activations. We control for the temporal scope, probing both local activations corresponding to a few milliseconds of the speech signal, as well as global 4146 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4146–4156 c July 5 - 10, 2020. 2020 Association for"
2020.acl-main.381,P17-1057,1,0.843947,"posit that a desirable property of an analytical method is that it is sensitive to the learning effect, and that the scores on trained versus randomly initialized models are clearly separated. Coefficient of partial determination Correlation between similarity structures of two representational spaces can, in principle, be partly due to the fact that both these spaces are correlated to a third space. For example, were we to get a high value for global RSA for one of the top layers of the RNNVGS model, we might suspect that this is due to the 3 Note that the visually grounded speech models of Chrupała et al. (2017); Chrupała (2019); Merkx et al. (2019) use similar mechanisms to aggregate the activations of the final RNN layer; here we use it as part of the analytical method to pool any sequential representation of interest. A further point worth noting is that we use scalar weights αt and apply a linear model for learning them in order to keep the analytic model simple and easy to train consistently. eY ∼Z − eY ∼X+Z eY ∼Z (5) where eY ∼X+Z is the sum squared error of the model with all variables, and eY ∼Z is the sum squared error of the model with X removed. Given the scenario above with the confoundin"
2020.acl-main.381,D18-2012,0,0.0645751,"Missing"
2020.acl-main.381,P18-1198,0,0.0175047,"ndividual or groups of neurons to an incoming trigger (e.g., Nagamine et al., 2015; Krug et al., 2018). In contrast, a larger body of work is dedicated to determining what type of linguistic information is encoded in the learned representations. This type of analysis is the focus of our paper. Two commonly used approaches to analyzing representations are: • Probing techniques, or diagnostic classifiers, i.e. methods which use the activations from different layers of a deep learning architecture as input to a prediction model (e.g., Adi et al., 2017; Alishahi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018); 1 See https://github.com/gchrupala/analyzing-analyticalmethods. • Representational Similarity Analysis (RSA) borrowed from neuroscience (Kriegeskorte et al., 2008) and used to correlate similarity structures of two different representation spaces (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Abnar et al., 2019; Abdou et al., 2019). We use both techniques in our experiments to systematically compare their output. 2.2 Analyzing random representations Research on the analysis of neural encodings of language has shown that in some cases, substantial information can be decoded from"
2020.acl-main.381,W18-5448,0,0.132168,"ral representations. We use two commonly applied analytical techniques: (i) diagnostic models and (ii) representational similarity analysis to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. In our experiments, we manipulate two important factors that can affect the outcome of analysis. One pitfall not always successfully avoided in work on neural representation analysis is the role of learning. Previous work has shown that sometimes non-trivial representations can be found in the activation patterns of randomly initialized, untrained neural networks (Zhang and Bowman, 2018; Chrupała and Alishahi, 2019). Here we investigate the representations of phonology in neural models of spoken language in light of this fact, as extant studies have not properly controlled for role of learning in these representations. The second manipulated factor in our experiments is the scope of the extracted neural activations. We control for the temporal scope, probing both local activations corresponding to a few milliseconds of the speech signal, as well as global 4146 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4146–4156 c July 5 -"
2020.acl-main.381,K17-1037,1,\N,Missing
2020.findings-emnlp.244,D17-1303,0,0.0213547,"pe-ind and pipe-seq) Image Encoder Shared encoder Speech Dedicated encoder Dedicated encoder Methodology The architecture and the training procedure used in this paper are inspired by the improved version of the visually-grounded spoken language understanding system of Merkx et al. (2019). Appendix A.1 provides details on the choice of hyperparameters. Decoder Text (d) mtl-transcribe/mtl-translate model Image Encoder Multilingual data The idea of using multilingual data is not new in the literature: existing work focuses on using the same modality for the two languages, either text or speech. Gella et al. (2017) and K´ad´ar et al. (2018) show that textual descriptions of images in different languages in the Multi30K dataset (Elliott et al., 2016) can be used in conjunction to improve the performance of a visually-grounded model. Harwath et al. (2018) focus on speech, exploring how spoken captions in two languages can be used simultaneously to improve performance in an English-Hindi parallel subset of the Places-205 dataset (Zhou et al., 2014). In contrast, our experiments concern the setting where speech data from a low-resource language is used in conjunction with corresponding translated written ca"
2020.findings-emnlp.244,Q14-1006,0,0.0288808,"vely a translated version of these transcripts. We obtain these elements from a set of related datasets: • Flickr8K (Hodosh et al., 2013) offers 8,000 images of everyday situations gathered from the website flickr.com together with English written captions (5 per image) that were obtained through crowd sourcing. • The Flickr Audio Caption Corpus (Harwath and Glass, 2015), augments Flickr8K with spoken captions read aloud by crowd workers. • F30kEnt-JP (Nakayama et al., 2020) provides Japanese translations of the captions (generated by humans). It covers the images and captions from Flickr30k (Young et al., 2014), a superset of Flickr8K, but only provides the translations of two captions per image.1 In all experiments, we use English as the source language for our models. While English is not a low-resource language, it is the only one for which we have spoken captions. The low-resource setting with translations is thus a simulated setting. To summarize, we have 8,000 images with 40,000 captions (five per image), in both English written and spoken form (amounting to ∼34 hours of speech). In addition, we have Japanese translations for two captions per image. Validation and test sets are composed of 1,0"
2020.findings-emnlp.244,2020.lrec-1.518,0,0.0317323,"ining. For our experiments on textual supervision, we additionally need the transcriptions corresponding to those spoken captions, or alternatively a translated version of these transcripts. We obtain these elements from a set of related datasets: • Flickr8K (Hodosh et al., 2013) offers 8,000 images of everyday situations gathered from the website flickr.com together with English written captions (5 per image) that were obtained through crowd sourcing. • The Flickr Audio Caption Corpus (Harwath and Glass, 2015), augments Flickr8K with spoken captions read aloud by crowd workers. • F30kEnt-JP (Nakayama et al., 2020) provides Japanese translations of the captions (generated by humans). It covers the images and captions from Flickr30k (Young et al., 2014), a superset of Flickr8K, but only provides the translations of two captions per image.1 In all experiments, we use English as the source language for our models. While English is not a low-resource language, it is the only one for which we have spoken captions. The low-resource setting with translations is thus a simulated setting. To summarize, we have 8,000 images with 40,000 captions (five per image), in both English written and spoken form (amounting"
2021.blackboxnlp-1.11,2020.findings-emnlp.244,1,0.0897838,"otion or speaker identity. metrics used to quantify it. The present study aims 163 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 163–176 Online, November 11, 2021. ©2021 Association for Computational Linguistics to fill this gap. We study two approaches to modeling spoken language: learning driven by language-internal structure, and learning driven by grounding in the extra-linguistic world. These two approaches are exemplified by two types of models with VQ layers: the self-supervised model for unit discovery of van Niekerk et al. (2020), and a visually grounded model similar to Harwath et al. (2020a). The datasets used to train each model, Zerospeech 2020 (Dunbar et al., 2020) and Flickr8K (Harwath and Glass, 2015; Rashtchian et al., 2010) are also typical of the task they are used for. Using these two models as our test cases, we systematically investigate the impact of the following factors: (i) the codebook size for the VQ layer, and (ii) the level of placement of the VQ layer. Furthermore, we apply and check the consistency across four different metrics for evaluating the correspondence of the representations with phonem"
2021.blackboxnlp-1.11,2020.acl-main.420,0,0.0276321,"lso known as a probe, is a classifier or regressor trained to predict some information of interest (such as a linguistic annotation) given a neural representation. To the extent that the model successfully predicts the annotation, we conclude that the neural representation encodes this information. Informally, such a diagnostic classifier can be seen as quantifying the amount of easily-accessible – or in the extreme case, linearly decodable – information about the target annotation (Adi et al., 2017; Alishahi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018, among others). As argued by Pimentel et al. (2020), without the qualification that information be easily accessible, probing should aim to approximate the mutual information between the neural representation and the target annotation, and thus should use the bestperforming probe possible. Furthermore, it is not possible for the neural representation to contain more information about the target annotation than the source utterance itself, due to the information processing inequality, and thus, in the general case, probing with an unrestricted classifier is not a wellfounded exercise. In the special case of probing a discrete-valued variable (a"
2021.blackboxnlp-1.11,W10-0721,0,0.136963,"s 163–176 Online, November 11, 2021. ©2021 Association for Computational Linguistics to fill this gap. We study two approaches to modeling spoken language: learning driven by language-internal structure, and learning driven by grounding in the extra-linguistic world. These two approaches are exemplified by two types of models with VQ layers: the self-supervised model for unit discovery of van Niekerk et al. (2020), and a visually grounded model similar to Harwath et al. (2020a). The datasets used to train each model, Zerospeech 2020 (Dunbar et al., 2020) and Flickr8K (Harwath and Glass, 2015; Rashtchian et al., 2010) are also typical of the task they are used for. Using these two models as our test cases, we systematically investigate the impact of the following factors: (i) the codebook size for the VQ layer, and (ii) the level of placement of the VQ layer. Furthermore, we apply and check the consistency across four different metrics for evaluating the correspondence of the representations with phonemes. Findings The self-supervised model shows high variability, but with some of the evaluation metrics (especially ABX and RSA, see Section 3.3 for the definition of the metrics) there is a trend for better"
2021.blackboxnlp-1.11,D07-1043,0,0.167725,"the authors’ implementation available at github.com/bshall/ZeroSpeech. cal estimate. Given discrete random variables X 166 with image X and Y with image Y (i.e. frame-wise codes and phoneme labels in our case), the mutual information I(X; Y ) is I(X; Y ) = XX P (x, y) log x∈X y∈Y P (x, y) (1) P (x)P (y) It is often more informative to use mutual information normalized by the arithmetic mean of the entropies of the two random variables: NMI(X; Y ) = 2 I(X; Y ) H(X) + H(Y ) (2) where H(X) is the entropy of X. This definition of normalized mutual information (NMI) is equivalent to the V-measure (Rosenberg and Hirschberg, 2007). Diagnostic Classifier (DC) A diagnostic model, also known as a probe, is a classifier or regressor trained to predict some information of interest (such as a linguistic annotation) given a neural representation. To the extent that the model successfully predicts the annotation, we conclude that the neural representation encodes this information. Informally, such a diagnostic classifier can be seen as quantifying the amount of easily-accessible – or in the extreme case, linearly decodable – information about the target annotation (Adi et al., 2017; Alishahi et al., 2017; Hupkes et al., 2018;"
2021.eacl-main.203,P14-2030,0,0.0607897,"Missing"
2021.eacl-main.203,P19-1104,0,0.0230892,"n of obfuscation, but all natural language generation research (Novikova et al., 2017)—placing an emphasis on human evaluation (van der Lee et al., 2019). It is perhaps for this reason that most obfuscation work uses heuristically-driven, controlled changes such as splitting or merging words or sentences, removing stop words, changing spelling, punctuation, or casing (see e.g., Karadzhov et al., 2017; Eger et al., 2019). These specific attacks are typically easier to mitigate through preprocessing (Juola and Vescovi, 2011). Obfuscation through lexical substitution (Mansoorizadeh et al., 2016; Bevendorff et al., 2019, 2020) provides a middle ground of control, semantic preservation and attack effectiveness; however, they might prove less effective against models relying on deeper stylistic features (e.g. word order, part-of-speech (POS) tags, or reading complexity scores). End-to-end systems have been employed for similar purposes (Shetty et al., 2018; Saedi and Dras, 2020), or to rewrite entire phrases (Emmery et al., 2018; Bo et al., 2019) using (adversarially-driven) autoencoders. Such attacks seem less common, and provide less control over the perturbations and semantic consistency. Our work does not"
2021.eacl-main.203,D11-1120,0,0.0496269,"ssumed to a large extent, and, most importantly, iii) Beller et al. (2014) and Emmery et al. (2017) have shown that through distant labeling, a representative corpus for this task can be collected in under a day. This allows us to measure transferability of attacks fitted using realistically collected distant corpora to models using high-quality hand labeled corpora. As for the attacks, we focus on lexical substitution of content words strongly related to a given label, as those have been shown to explain a significant portion of the accuracy of stylometric models (see e.g., Rao et al., 2000; Burger et al., 2011; Sap et al., 2014; Rangel et al., 2016). To that effect, we extend the substitution attack of Jin et al. (2020) and apply it to author attribute obfuscation. Specifically, we explore the potential of training a simple (as to meet the speed criterion), non-neural substitute model f 0 to indicate relevant words to perturb, where retaining the original meaning is prioritized. .342 .122 .059 .012 .010 f &apos;(D) f (DADV) knowledgeable about the target architecture, nor to have access to suitable training data (as the target could have been trained on any domain). Hence, we cannot optimally tailor att"
2021.eacl-main.203,D18-2029,0,0.0650145,"Missing"
2021.eacl-main.203,N19-1423,0,0.0183647,"ches to perturb a target word t ∈ T are considered in our experiments. These operations are referred to as candidates in Algorithm 1. Synonym Substitution (WS) This TF-based substitution embeds t as t using a pre-trained embedding matrix V . Ct is selected by computing the cosine similarity between t and all available wordembeddings w ∈ V . We denote cosine similarity with Λ(t, w). A threshold δ is used to keep only reliable candidates Λ(t, w) &gt; δ. Masked Substitution (MB) The embeddingbased substitutions can be replaced by a language model predicting the contextually most likely token. BERT (Devlin et al., 2019)—a bi-directional encoder (Vaswani et al., 2017) trained through masked language modeling and next-sentence prediction—makes this fairly trivial. By replacing t with a mask, BERT produces a top-k most likely Ct for that position. Implementing this in TF does imply each previous substitution of t might be included in the context of the current one. This method of contextual replacement has two drawbacks: i) semantic consistency with the original word is not guaranteed (as the model has no knowledge of t), and ii) the replaced context means semantic drift can occur, as all subsequent substitutio"
2021.eacl-main.203,C18-1055,0,0.0225451,"their writing style. This can prove particularly harmful to individuals in a vulnerable position regarding e.g., race, political affiliation, or mental health. Privacy-preserving defenses against such inferences can be found in the field of adversarial1 stylometry. Our research2 concerns the obfuscation subtask, where the aim is to rewrite an input text such that the style changes, and stylometric predictions fail. It is part of a growing body of research into adversarial attacks on NLP (Smith, 2012), which various modern models have proven vulnerable to; e.g., in neural machine translation (Ebrahimi et al., 2018), summarization (Cheng et al., 2020), and text classification (Liang et al., 2018). Adversarial attacks on NLP are predominantly aimed at demonstrating vulnerabilities in existing algorithms or models, such that they might be fixed, or explicitly improved through adversarial training. Consequently, most related work focuses on white or black-box settings, where all or part of the target model is accessible (e.g., its predictions, data, parameters, gradients, or probability distribution) to fit an attack. The current research, however, does not intend to improve the targeted models; rather, we"
2021.eacl-main.203,P11-1137,0,0.021275,"plethora of author information—either consciously shared or inferable through stylometric analysis (Rao et al., 2000; Adams, 2006). This characteristic is fundamental to author profiling (Koppel et al., 2002), and while the field’s main interest pertains to the study of sociolinguistic and stylometric features that underpin our language use (Daelemans, 2013), herein simultaneously lie its dual-use problems. Author profiling can, often with high accuracy, infer an extensive set of (sensitive) personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Grzegorz Chrupała CSAI, Tilburg University g.a.chrupala@uvt.nl Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). It therefore potentially exposes anyone sharing written online content to unauthorized information collection through their writing style. This can prove particularly harmful to individuals in a vulnerable position regarding e.g., race, political affiliation, or mental health. Privacy-preserving defenses against such inferences can be found in the field of adversarial1 stylometry. Our research2 concerns the obfuscation subtask, where t"
2021.eacl-main.203,W17-4407,1,0.911447,"This implies the substitute models should be fitted locally, and therefore need to meet two criteria: reliable access to labeled data, and being relatively fast and easy to train. To meet the first criterion, the current research focuses on gender prediction, as: i) Twitter corpora annotated with this variable are by far the largest (and most common), ii) author profiling methods typically use similar architectures for different attributes; therefore, the generalization of attacks to other author attributes can be assumed to a large extent, and, most importantly, iii) Beller et al. (2014) and Emmery et al. (2017) have shown that through distant labeling, a representative corpus for this task can be collected in under a day. This allows us to measure transferability of attacks fitted using realistically collected distant corpora to models using high-quality hand labeled corpora. As for the attacks, we focus on lexical substitution of content words strongly related to a given label, as those have been shown to explain a significant portion of the accuracy of stylometric models (see e.g., Rao et al., 2000; Burger et al., 2011; Sap et al., 2014; Rangel et al., 2016). To that effect, we extend the substitu"
2021.eacl-main.203,C18-1084,1,0.823814,"019). These specific attacks are typically easier to mitigate through preprocessing (Juola and Vescovi, 2011). Obfuscation through lexical substitution (Mansoorizadeh et al., 2016; Bevendorff et al., 2019, 2020) provides a middle ground of control, semantic preservation and attack effectiveness; however, they might prove less effective against models relying on deeper stylistic features (e.g. word order, part-of-speech (POS) tags, or reading complexity scores). End-to-end systems have been employed for similar purposes (Shetty et al., 2018; Saedi and Dras, 2020), or to rewrite entire phrases (Emmery et al., 2018; Bo et al., 2019) using (adversarially-driven) autoencoders. Such attacks seem less common, and provide less control over the perturbations and semantic consistency. Our work does not assume the attacks to run end-to-end, but with a hypothetical human in the loop. We further opt for techniques that are more likely to find strong semantic mirrors to the original text while making minimal changes. A substitute model (the algorithm, hyper-parameters, and output of which an author can manipulate as desired) is employed to indicate candidate replacement words, and our attacks suggest and rank thos"
2021.eacl-main.203,J15-4004,0,0.0467292,"plying a maximum of 25 instances per author (some contain one, 2,500 is the API history limit). From the test set, the last6 200 instances were sampled for the attack (110 male, 90 female). While fairly small, this sample does reflect a realistic attack duration and timeline size, as they would be executed for a single profile. 4.2 Attacks For the extension of TF, we re-implemented code7 by Jin et al. (2020) to work with Scikit-learn8 (Pedregosa et al., 2011). For their synonym substitution component, we similarly used counter-fitted embeddings by Mrkˇsi´c et al. (2016) trained on Simlex-999 (Hill et al., 2015). The USE (Cer et al., 2018) implementation uses TensorFlow9 (Abadi et al., 2016a) as back-end, and all BERTvariants were implemented in Hugging Face’s10 Transformers library (Wolf et al., 2020) with PyTorch11 (Paszke et al., 2019) as back-end. We adopt the same parameter settings as Jin et al. (2020) throughout our TF experiments: they set N (considered synonyms) and δ (cosine similarity minimum) empirically to 50 and 0.7 respectively. For MB and DB, we capped T at 50 and top-k at 10 (to improve speed). For DB, we follow Zhou et al. (2019) and set the dropout probability to 0.3. 5 https://spa"
2021.eacl-main.203,2020.lrec-1.180,0,0.026016,"Missing"
2021.eacl-main.203,W19-8643,0,0.0283695,"Missing"
2021.eacl-main.203,W16-6010,0,0.025246,"r identity (Koppel and Schler, 2004), and author profiling (Argamon et al., 2005); e.g., predicting demographic attributes. Adversarial stylometry (as conceptualized by Brennan et al., 2012) intends to subvert these inferences by changing an author’s text through imitation, or, as pertains to our research, the obfuscation of writing style (Kacmarcik and Gamon, 2006; Caliskan et al., 2018; Le et al., 2015; Xu et al., 2019). These changes, or perturbations, can be produced in several ways, and the task is therefore of2389 ten conflated with paraphrasing (Reddy and Knight, 2016), style transfer (Kabbara and Cheung, 2016), and generating adversarial samples or triggers (Zhang et al., 2020b). Regardless of the employed method, the main challenge of obfuscation lies in retaining the original meaning of an input text; its written language medium limits any perturbations to discrete outputs, and unnatural discrepancies are significantly better discernible by humans than, say, a few pixel changes in an image. An additional, persistent limitation is the absence of evaluation metrics that guarantee complete preservation of the original meaning of the input whilst changes remain unnoticed (Potthast et al., 2016). This"
2021.eacl-main.203,P18-2005,0,0.0282887,"target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks. 1 Introduction The widespread use of machine learning on consumer devices and its application to their data has sparked investigation of security and privacy researchers alike in correctly handling sensitive information (Edwards and Storkey, 2016; Abadi et al., 2016b). Natural Language Processing (NLP) is no exception (Fernandes et al., 2019; Li et al., 2018); written text can contain a plethora of author information—either consciously shared or inferable through stylometric analysis (Rao et al., 2000; Adams, 2006). This characteristic is fundamental to author profiling (Koppel et al., 2002), and while the field’s main interest pertains to the study of sociolinguistic and stylometric features that underpin our language use (Daelemans, 2013), herein simultaneously lie its dual-use problems. Author profiling can, often with high accuracy, infer an extensive set of (sensitive) personal information, such as age, gender, education, socio-economic statu"
2021.eacl-main.203,P06-2058,0,0.108189,"models (see surveys by Holmes, 1998; Neal et al., 2017) and machine learning (e.g., Matthews and Merriam, 1993; Merriam and Matthews, 1994). Computational stylometry distinguishes several subtasks such as determining (Baayen et al., 2002) and verifying author identity (Koppel and Schler, 2004), and author profiling (Argamon et al., 2005); e.g., predicting demographic attributes. Adversarial stylometry (as conceptualized by Brennan et al., 2012) intends to subvert these inferences by changing an author’s text through imitation, or, as pertains to our research, the obfuscation of writing style (Kacmarcik and Gamon, 2006; Caliskan et al., 2018; Le et al., 2015; Xu et al., 2019). These changes, or perturbations, can be produced in several ways, and the task is therefore of2389 ten conflated with paraphrasing (Reddy and Knight, 2016), style transfer (Kabbara and Cheung, 2016), and generating adversarial samples or triggers (Zhang et al., 2020b). Regardless of the employed method, the main challenge of obfuscation lies in retaining the original meaning of an input text; its written language medium limits any perturbations to discrete outputs, and unnatural discrepancies are significantly better discernible by hu"
2021.eacl-main.203,2020.acl-main.203,0,0.0489402,"Missing"
2021.eacl-main.203,N16-1018,0,0.0653402,"Missing"
2021.eacl-main.203,D17-1238,0,0.0292513,"hod, the main challenge of obfuscation lies in retaining the original meaning of an input text; its written language medium limits any perturbations to discrete outputs, and unnatural discrepancies are significantly better discernible by humans than, say, a few pixel changes in an image. An additional, persistent limitation is the absence of evaluation metrics that guarantee complete preservation of the original meaning of the input whilst changes remain unnoticed (Potthast et al., 2016). This not only inhibits automatic evaluation of obfuscation, but all natural language generation research (Novikova et al., 2017)—placing an emphasis on human evaluation (van der Lee et al., 2019). It is perhaps for this reason that most obfuscation work uses heuristically-driven, controlled changes such as splitting or merging words or sentences, removing stop words, changing spelling, punctuation, or casing (see e.g., Karadzhov et al., 2017; Eger et al., 2019). These specific attacks are typically easier to mitigate through preprocessing (Juola and Vescovi, 2011). Obfuscation through lexical substitution (Mansoorizadeh et al., 2016; Bevendorff et al., 2019, 2020) provides a middle ground of control, semantic preservat"
2021.eacl-main.203,W15-2913,0,0.018908,"his characteristic is fundamental to author profiling (Koppel et al., 2002), and while the field’s main interest pertains to the study of sociolinguistic and stylometric features that underpin our language use (Daelemans, 2013), herein simultaneously lie its dual-use problems. Author profiling can, often with high accuracy, infer an extensive set of (sensitive) personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Grzegorz Chrupała CSAI, Tilburg University g.a.chrupala@uvt.nl Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). It therefore potentially exposes anyone sharing written online content to unauthorized information collection through their writing style. This can prove particularly harmful to individuals in a vulnerable position regarding e.g., race, political affiliation, or mental health. Privacy-preserving defenses against such inferences can be found in the field of adversarial1 stylometry. Our research2 concerns the obfuscation subtask, where the aim is to rewrite an input text such that the style changes, and stylometric predictions fail. It is part of a growing body of"
2021.eacl-main.203,W16-5603,0,0.026431,"(Baayen et al., 2002) and verifying author identity (Koppel and Schler, 2004), and author profiling (Argamon et al., 2005); e.g., predicting demographic attributes. Adversarial stylometry (as conceptualized by Brennan et al., 2012) intends to subvert these inferences by changing an author’s text through imitation, or, as pertains to our research, the obfuscation of writing style (Kacmarcik and Gamon, 2006; Caliskan et al., 2018; Le et al., 2015; Xu et al., 2019). These changes, or perturbations, can be produced in several ways, and the task is therefore of2389 ten conflated with paraphrasing (Reddy and Knight, 2016), style transfer (Kabbara and Cheung, 2016), and generating adversarial samples or triggers (Zhang et al., 2020b). Regardless of the employed method, the main challenge of obfuscation lies in retaining the original meaning of an input text; its written language medium limits any perturbations to discrete outputs, and unnatural discrepancies are significantly better discernible by humans than, say, a few pixel changes in an image. An additional, persistent limitation is the absence of evaluation metrics that guarantee complete preservation of the original meaning of the input whilst changes rem"
2021.eacl-main.203,2020.starsem-1.19,0,0.0245211,"ing (see e.g., Karadzhov et al., 2017; Eger et al., 2019). These specific attacks are typically easier to mitigate through preprocessing (Juola and Vescovi, 2011). Obfuscation through lexical substitution (Mansoorizadeh et al., 2016; Bevendorff et al., 2019, 2020) provides a middle ground of control, semantic preservation and attack effectiveness; however, they might prove less effective against models relying on deeper stylistic features (e.g. word order, part-of-speech (POS) tags, or reading complexity scores). End-to-end systems have been employed for similar purposes (Shetty et al., 2018; Saedi and Dras, 2020), or to rewrite entire phrases (Emmery et al., 2018; Bo et al., 2019) using (adversarially-driven) autoencoders. Such attacks seem less common, and provide less control over the perturbations and semantic consistency. Our work does not assume the attacks to run end-to-end, but with a hypothetical human in the loop. We further opt for techniques that are more likely to find strong semantic mirrors to the original text while making minimal changes. A substitute model (the algorithm, hyper-parameters, and output of which an author can manipulate as desired) is employed to indicate candidate repla"
2021.eacl-main.203,D14-1121,0,0.0288872,"ent, and, most importantly, iii) Beller et al. (2014) and Emmery et al. (2017) have shown that through distant labeling, a representative corpus for this task can be collected in under a day. This allows us to measure transferability of attacks fitted using realistically collected distant corpora to models using high-quality hand labeled corpora. As for the attacks, we focus on lexical substitution of content words strongly related to a given label, as those have been shown to explain a significant portion of the accuracy of stylometric models (see e.g., Rao et al., 2000; Burger et al., 2011; Sap et al., 2014; Rangel et al., 2016). To that effect, we extend the substitution attack of Jin et al. (2020) and apply it to author attribute obfuscation. Specifically, we explore the potential of training a simple (as to meet the speed criterion), non-neural substitute model f 0 to indicate relevant words to perturb, where retaining the original meaning is prioritized. .342 .122 .059 .012 .010 f &apos;(D) f (DADV) knowledgeable about the target architecture, nor to have access to suitable training data (as the target could have been trained on any domain). Hence, we cannot optimally tailor attacks to the target"
2021.eacl-main.203,P16-1148,0,0.0124184,"fundamental to author profiling (Koppel et al., 2002), and while the field’s main interest pertains to the study of sociolinguistic and stylometric features that underpin our language use (Daelemans, 2013), herein simultaneously lie its dual-use problems. Author profiling can, often with high accuracy, infer an extensive set of (sensitive) personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Grzegorz Chrupała CSAI, Tilburg University g.a.chrupala@uvt.nl Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). It therefore potentially exposes anyone sharing written online content to unauthorized information collection through their writing style. This can prove particularly harmful to individuals in a vulnerable position regarding e.g., race, political affiliation, or mental health. Privacy-preserving defenses against such inferences can be found in the field of adversarial1 stylometry. Our research2 concerns the obfuscation subtask, where the aim is to rewrite an input text such that the style changes, and stylometric predictions fail. It is part of a growing body of research into adversarial att"
2021.eacl-main.203,P14-1018,0,0.0418607,"Missing"
2021.eacl-main.203,D19-1221,0,0.022592,"nt limitations of semantic consistency metrics, could realistically optimize for both obfuscation and inconspicuousness. As such, we would argue that future work should focus on making as few perturbations as possible, retaining only the minimum amount of required obfuscation success. Given this, the other constraints become less relevant; one could generate short sentences (e.g., a single tweet) that might be semantically or contextually incorrect, but if it is a message in a long post history, it will hardly be detectable or intrusive. This would require certain triggers (as demonstrated by Wallace et al. (2019) for example), and ascertaining how well they transfer. 7 Conclusion In our work, we argued realistic adversarial stylometry should be tested on transferability in settings where there is no access to the target model’s data or architecture. We extended previous adversarial text classification work with two transformer-based models, and studied their obfuscation success in such a setting. We showed them to reliably drop target model performance below chance, though human detectability of the attacks remained above chance. Future work could focus on further minimizing this detection under our r"
2021.eacl-main.203,W16-5618,0,0.043724,"Missing"
2021.eacl-main.203,N16-2013,0,0.0659691,"Missing"
2021.eacl-main.203,D19-3003,0,0.0469611,"Missing"
2021.eacl-main.203,P19-1328,0,0.411283,"he document after deleting Di , and oy (D) the logit score by f 0 . The omission score is then given by oy (D) − oy (Di ), and used in an importance score I of token Di , as:   oy (D) − oy (Di ),     if f 0 (D) = f 0 (Di ) = y. IDi = oy (D) − oy (Di ) + oy¯(D) − oy¯(Di ),     if f 0 (D) = y, f 0 (Di ) = y¯, y 6= y¯. (1) With IDi calculated for all words in D, the top k ranked tokens are chosen as target words T . 2390 ALGORITHM 1: Dropout Substitution (DB) A method to circumvent the former (i.e., BERT’s masked prediction limitations for lexical substitution), was presented by Zhou et al. (2019). They apply dropout (Srivastava et al., 2014) to BERT’s internal embedding of target word t before it is passed to the transformer—zeroing part of the weights with some probability. The assumption is that Ct (BERT’s top-k) will contain candidates closer to the original t than the masked suggestions. Obfuscation by lexical replacement. 0 Input : f – substitute model D = {w0 , w1 , . . . , wn } – document y – target label checks – apply checks (bool) k – target max k-amount words Output : DADV – obfuscated document 2 for Di ∈ D do // via Equation 1 IDi ← omission score(f 0 , y) 3 T ← top k(args"
C16-1124,W14-4012,0,0.0733052,"Missing"
C16-1124,P15-2019,1,0.873262,"Missing"
C16-1124,P16-1160,0,0.0340201,"rns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. As both Harwath and Glass (2015) and Synnaeve et al. (2014) work with word-sized chunks of speech, they bypass the segmentation problem that human language learners face. Character-level input representations have recently gained attention in NLP. Ling et al. (2015) and Plank et al. (2016) use bidirectional LSTMs to compose characters into word embeddings, while Chung et al. (2016) propose machine translation model with character level output. These approaches exploit character-level information but crucially they assume that word boundaries are available in the input. Character-level neural NLP without explicit word boundaries in the input is studied in cases where fixed vocabularies are inherently problematic, e.g. with combined natural and programming language 1310 input (Chrupała, 2013) or when specifically dealing with misspelled words in automatic writing feedback (Xie et al., 2016). Character-level language models are analyzed in Hermans and Schrauwen (2013) and"
C16-1124,N15-1016,0,0.0116835,"s simplified in a variety of ways, for example: • distributional learning from pure word-word co-occurrences with no perceptual grounding (Landauer et al., 1998; Kiros et al., 2015); • cross-situational learning with word sequences and sets of symbols representing sensory input (Siskind, 1996; Fazly et al., 2010); • cross-situational learning using sensory audio and visual input, but with extremely limited sets of words and objects (Roy and Pentland, 2002; Iwahashi, 2003). Some recent models have used more naturalistic, larger-scale inputs, for example in cross-modal distributional semantics (Lazaridou et al., 2015) or in implementations of the acquisition process trained on images paired with their descriptions (Chrupała et al., 2015). While in these works the representation of the visual scene consists of pixel-level perceptual data, the linguistic input consists of sentences segmented into discrete word symbols. In this paper we take a step towards addressing this major limitation, by using the phonetic transcription of input utterances. While this type of input is symbolic rather than perceptual, it goes a long way toward making the setting more naturalistic, and the acquisition problem more challeng"
C16-1124,N16-1043,1,0.876397,"Missing"
C16-1124,D15-1176,0,0.0637278,"Missing"
C16-1124,P16-2067,0,0.0213577,"ject recognition model and a word recognition model, and an embedding alignment model that learns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. As both Harwath and Glass (2015) and Synnaeve et al. (2014) work with word-sized chunks of speech, they bypass the segmentation problem that human language learners face. Character-level input representations have recently gained attention in NLP. Ling et al. (2015) and Plank et al. (2016) use bidirectional LSTMs to compose characters into word embeddings, while Chung et al. (2016) propose machine translation model with character level output. These approaches exploit character-level information but crucially they assume that word boundaries are available in the input. Character-level neural NLP without explicit word boundaries in the input is studied in cases where fixed vocabularies are inherently problematic, e.g. with combined natural and programming language 1310 input (Chrupała, 2013) or when specifically dealing with misspelled words in automatic writing feedback (Xie et"
C18-1084,W05-0909,0,0.0511263,"inimize the performance of such classifiers perform what we will refer to as obfuscation-by-transfer. To illustrate, the adversary is easily fooled when a sentence looks strongly female even though it was written by a male. As such, the easiest route to obfuscation from this perspective is a form of styletransfer: swapping a few strongly target-associated content words for their contrastive variant (wife to husband, school to wedding). When such variants are also close in semantic spaces that sequence models make use of, any reconstruction metrics—such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), embedding distances, etc.—might become an inaccurate indication of the change in meaning. Our contributions In this work we propose a different approach to automatic obfuscation that we hypothesize partly overcomes the limitations to preserving meaning of the input: obfuscation-byinvariance. Here, the objective shifts towards maximizing adversary’s uncertainty, implying its accuracy on the protected variable should be as close to chance level as possible. Fixing the adversary’s performance around chance involves making the input text devoid of stylistic features that strongly correlate with"
C18-1084,P14-2030,0,0.30147,"Missing"
C18-1084,P11-1137,0,0.0645164,"ved quality of the output in terms of meaning preservation and grammaticality. 1 Introduction The fact that writing style uniquely characterizes a person, and can be leveraged for automatic author identification (Holmes, 1998; Stamatatos et al., 2000), has been well-studied in the field of (computational) stylometry (Neal et al., 2017). Similarly, work on author profiling (Koppel et al., 2002) has demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-reports as a distant supervision signal (Beller et al., 2014; Emmery et al., 2017; Yates et al., 2017). While these efforts have been greatly beneficial to various research fields such as computational sociolinguistics (Daelemans, 2013), they potentially expose users of such media to attacks where this information can be abused unbekn"
C18-1084,W17-4407,1,0.634227,"work on author profiling (Koppel et al., 2002) has demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-reports as a distant supervision signal (Beller et al., 2014; Emmery et al., 2017; Yates et al., 2017). While these efforts have been greatly beneficial to various research fields such as computational sociolinguistics (Daelemans, 2013), they potentially expose users of such media to attacks where this information can be abused unbeknownst to them. This is particularly harmful to individuals in a vulnerable position regarding race, political affiliation, mental health, or any other personal information made explicitly unavailable. Adversarial stylometry, or style obfuscation, is one of the proposed methods aimed at protecting users against such attacks. Its objective is to"
C18-1084,W17-4912,0,0.0691963,"Missing"
C18-1084,D17-1256,0,0.0204117,"using a sequence-to-sequence MT model inspired by zeroshot translation (Johnson et al., 2016). Their work demonstrates successful translation between many different versions of the English Bible. 2.2 Gradient Reversal The use of a Gradient Reversal Layer (GRL) for learning domain invariant feature representations was proposed by Ganin and Lempitsky (2015), who demonstrated its viability for learning lightningcondition invariance in computer vision. Since then, it has been applied to several language tasks: e.g. textual feature extraction (Pryzant et al., 2017), POS tagging (Kim et al., 2017; Gui et al., 2017), image captioning (Chen et al., 2017), and document classification (Liu et al., 2017; Xu and Yang, 2017). Most importantly, Xie et al. (2017) demonstrate the GRL module can be used to implement an adversarial setting, and to improve performance for a number of language tasks, including generation. These results bode well for its application to obfuscation-by-invariance. 3 Models Our base architecture is a neural encoder-decoder (Sutskever et al., 2014) model similar to that of Wu et al. (2016), implemented in PyTorch (Paszke et al., 2017). Given an input sequence of one-hot encoded words, the"
C18-1084,W02-0808,0,0.0575771,"fed into the architecture at each step. In contrast to the simple autoencoder, the conditional autoencoder allows to choose a desired style at test time. We suspect that by encouraging the decoder to target a certain style, the output would be have more linguistic consistency without fully recovering the targeted style. Token Transfer (TT) It can be argued that an MT system relying on a parallel corpus of styles (be it attributes or authors) would perform obfuscation-by-transfer. Moreover, it would likely preserve the original meaning as translation is largely a meaning-preserving operation (Ide et al., 2002; Dyvik, 2004). However, such parallel corpora are generally not available and have very high associated compilation costs, as it would require large amounts of identical information (ideally on sentence-level) to be written by e.g. teens and adults. Textual style transfer by MT is therefore not a plausible use-case for obfuscation. However, it does provide a good indication of the performance of an obfuscation model under the framework of obfuscation-by-transfer. For this, we apply a sequence-to-sequence translation model trained on style as discussed by (Carlson et al., 2017). Following the"
C18-1084,E17-2068,0,0.0306441,"Missing"
C18-1084,P06-2058,0,0.689057,"ce: obfuscation-by-transfer in a parallel setting works well using a many-to-many translation model, but scores worse in the human evaluation than our style-invariant model. As such, we pose that there is potential in an style-invariant approach to obfuscation, and it deservers further investigation. 2 Related Work 2.1 Adversarial Stylometry The idea that computational stylometry might be used to compromise anonymity was first explored by Rao et al. (2000). They saw potential in concealing style information using machine translation (MT), but noted that it was not powerful enough at the time. Kacmarcik and Gamon (2006) continued the proposed line of work by informing users regarding characteristic features and deeper linguistic cues in their writing style. Recent related studies can be found in (Caliskan-Islam et al., 2015; Le et al., 2015). Brennan et al. (2012) explicitly frame obfuscation as an adversarial task and use MT (round-trip translation), similar to (Caliskan and Greenstadt, 2012). Rule-based perturbations (Juola and Vescovi, 2011) and mixtures of both (Karadzhov et al., 2017) have also been applied for fully automatic obfuscation. Closest to our approach is recent work by Shetty et al. (2017),"
C18-1084,D17-1302,0,0.0320782,"hot style-transfer using a sequence-to-sequence MT model inspired by zeroshot translation (Johnson et al., 2016). Their work demonstrates successful translation between many different versions of the English Bible. 2.2 Gradient Reversal The use of a Gradient Reversal Layer (GRL) for learning domain invariant feature representations was proposed by Ganin and Lempitsky (2015), who demonstrated its viability for learning lightningcondition invariance in computer vision. Since then, it has been applied to several language tasks: e.g. textual feature extraction (Pryzant et al., 2017), POS tagging (Kim et al., 2017; Gui et al., 2017), image captioning (Chen et al., 2017), and document classification (Liu et al., 2017; Xu and Yang, 2017). Most importantly, Xie et al. (2017) demonstrate the GRL module can be used to implement an adversarial setting, and to improve performance for a number of language tasks, including generation. These results bode well for its application to obfuscation-by-invariance. 3 Models Our base architecture is a neural encoder-decoder (Sutskever et al., 2014) model similar to that of Wu et al. (2016), implemented in PyTorch (Paszke et al., 2017). Given an input sequence of one-hot"
C18-1084,P15-1107,0,0.0300173,"997). The resulting sequence of processed vectors is then merged into a single dense representation using an inner-attention mechanism that will be described below. After encoding, a neural language-model is trained to decode the output sequence conditioned on the sentence embedding (a so called context vector) resulting from the encoder. Training is accomplished by minimizing the locally-normalized perword cross-entropy of the target sequence. In an autoencoder setting, the goal of the network is to simply reconstruct the original sentence based on the encoded context vector (Lauly et al., ; Li et al., 2015). This set-up can be combined with the GRL to encourage the encoder to produce attribute-invariant context vectors. The target is the input itself in the case of an autoencoder (AE) architecture, or a paired sentence in the case of a sequence-to-sequence (S2S) architecture. See Figure 1 for a visual representations of the base architecture. 986 3.1 Architecture Components In addition to the architecture described above, we introduce a few extra components: Gradient Reversal Layer (GRL) The GRL (Sutskever et al., 2014) is applied on top of the context vector. During the forward pass the GRL com"
C18-1084,P17-1001,0,0.0322753,"., 2016). Their work demonstrates successful translation between many different versions of the English Bible. 2.2 Gradient Reversal The use of a Gradient Reversal Layer (GRL) for learning domain invariant feature representations was proposed by Ganin and Lempitsky (2015), who demonstrated its viability for learning lightningcondition invariance in computer vision. Since then, it has been applied to several language tasks: e.g. textual feature extraction (Pryzant et al., 2017), POS tagging (Kim et al., 2017; Gui et al., 2017), image captioning (Chen et al., 2017), and document classification (Liu et al., 2017; Xu and Yang, 2017). Most importantly, Xie et al. (2017) demonstrate the GRL module can be used to implement an adversarial setting, and to improve performance for a number of language tasks, including generation. These results bode well for its application to obfuscation-by-invariance. 3 Models Our base architecture is a neural encoder-decoder (Sutskever et al., 2014) model similar to that of Wu et al. (2016), implemented in PyTorch (Paszke et al., 2017). Given an input sequence of one-hot encoded words, the encoder first embeds the words into dense vectors which are then processed by one or"
C18-1084,P02-1040,0,0.101119,"at models explicitly tasked to minimize the performance of such classifiers perform what we will refer to as obfuscation-by-transfer. To illustrate, the adversary is easily fooled when a sentence looks strongly female even though it was written by a male. As such, the easiest route to obfuscation from this perspective is a form of styletransfer: swapping a few strongly target-associated content words for their contrastive variant (wife to husband, school to wedding). When such variants are also close in semantic spaces that sequence models make use of, any reconstruction metrics—such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), embedding distances, etc.—might become an inaccurate indication of the change in meaning. Our contributions In this work we propose a different approach to automatic obfuscation that we hypothesize partly overcomes the limitations to preserving meaning of the input: obfuscation-byinvariance. Here, the objective shifts towards maximizing adversary’s uncertainty, implying its accuracy on the protected variable should be as close to chance level as possible. Fixing the adversary’s performance around chance involves making the input text devoid of stylistic fea"
C18-1084,W15-2913,0,0.0593914,"icality. 1 Introduction The fact that writing style uniquely characterizes a person, and can be leveraged for automatic author identification (Holmes, 1998; Stamatatos et al., 2000), has been well-studied in the field of (computational) stylometry (Neal et al., 2017). Similarly, work on author profiling (Koppel et al., 2002) has demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-reports as a distant supervision signal (Beller et al., 2014; Emmery et al., 2017; Yates et al., 2017). While these efforts have been greatly beneficial to various research fields such as computational sociolinguistics (Daelemans, 2013), they potentially expose users of such media to attacks where this information can be abused unbeknownst to them. This is particularly harmful to individuals in a vul"
C18-1084,D14-1121,0,0.0603432,"en from Shetty et al. (2017). License details: http: 984 Proceedings of the 27th International Conference on Computational Linguistics, pages 984–996 Santa Fe, New Mexico, USA, August 20-26, 2018. We propose that this observed shift in meaning is to some extent a by-product of the formulation of the obfuscation task. Content words that are strongly related to a particular attribute often play a significant role in the accuracy of a potential adversary. There is ample evidence for this phenomenon in age and gender classification work (Koppel et al., 2002; Rao et al., 2010; Burger et al., 2011; Sap et al., 2014, inter alia). Taking examples from Sap et al. (2014) specifically, features with strong coefficient weights for gender include e.g. boxers, shaved, girlfriend, beard, fightin for males, and purse, blueberry, pedicure, hubby, earrings for females. It is therefore not a surprising result that models explicitly tasked to minimize the performance of such classifiers perform what we will refer to as obfuscation-by-transfer. To illustrate, the adversary is easily fooled when a sentence looks strongly female even though it was written by a male. As such, the easiest route to obfuscation from this pe"
C18-1084,J00-4001,0,0.30274,"rallel settings, and compare automatic and human evaluations on the obfuscated sentences. Our experiments show that the performance of a style classifier can be reduced to chance level, while the output is evaluated to be of equal quality to models applying style-transfer. Additionally, human evaluation indicates a trade-off between the level of obfuscation and the observed quality of the output in terms of meaning preservation and grammaticality. 1 Introduction The fact that writing style uniquely characterizes a person, and can be leveraged for automatic author identification (Holmes, 1998; Stamatatos et al., 2000), has been well-studied in the field of (computational) stylometry (Neal et al., 2017). Similarly, work on author profiling (Koppel et al., 2002) has demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal"
C18-1084,P16-1148,0,0.104389,"n The fact that writing style uniquely characterizes a person, and can be leveraged for automatic author identification (Holmes, 1998; Stamatatos et al., 2000), has been well-studied in the field of (computational) stylometry (Neal et al., 2017). Similarly, work on author profiling (Koppel et al., 2002) has demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-reports as a distant supervision signal (Beller et al., 2014; Emmery et al., 2017; Yates et al., 2017). While these efforts have been greatly beneficial to various research fields such as computational sociolinguistics (Daelemans, 2013), they potentially expose users of such media to attacks where this information can be abused unbeknownst to them. This is particularly harmful to individuals in a vulnerable position regarding ra"
C18-1084,P14-1018,0,0.0857715,"Missing"
C18-1084,P17-1130,0,0.047899,"Missing"
C18-1084,D17-1322,0,0.0217338,"ling (Koppel et al., 2002) has demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-reports as a distant supervision signal (Beller et al., 2014; Emmery et al., 2017; Yates et al., 2017). While these efforts have been greatly beneficial to various research fields such as computational sociolinguistics (Daelemans, 2013), they potentially expose users of such media to attacks where this information can be abused unbeknownst to them. This is particularly harmful to individuals in a vulnerable position regarding race, political affiliation, mental health, or any other personal information made explicitly unavailable. Adversarial stylometry, or style obfuscation, is one of the proposed methods aimed at protecting users against such attacks. Its objective is to rewrite an input tex"
C18-1084,D11-1120,0,\N,Missing
C18-1272,K17-1037,1,0.851191,"m Memory (LSTM) language models to encode subjectverb agreement (Linzen et al., 2016), analyzed the translation quality of character-level sequenceto-sequence models in terms of several morpho-syntactic grammaticality tests (Sennrich, 2016), Li et al. (2016) introduce a representation erasure technique to measure the amount of contribution of input words and specific phrases to the decision of RNN-based sentiment classifiers, K´ad´ ar et al. (2017) examine the linguistic representations of Gated Recurrent Unit-based architectures 3216 for image-sentence ranking, and Chrupala et al. (2017) and Alishahi et al. (2017) analyze the linguistic structure learned by Recurrent Highway Network models of visually grounded speech understanding. As an alternative approach, a number of white-box architectures have been proposed which learn explicit representations of linguistic structure. For example, Dyer et al. (2016) examine Recurrent Neural Network grammars and analyze its learned syntax (Kuncoro et al., 2016). They conclude that their architecture learns a similar notion of headedness as established headrule sets and the model learns structure similar to traditional nonterminal categories. Williams et al. (2017)"
C18-1272,P16-1139,0,0.0219425,"ed speech understanding. As an alternative approach, a number of white-box architectures have been proposed which learn explicit representations of linguistic structure. For example, Dyer et al. (2016) examine Recurrent Neural Network grammars and analyze its learned syntax (Kuncoro et al., 2016). They conclude that their architecture learns a similar notion of headedness as established headrule sets and the model learns structure similar to traditional nonterminal categories. Williams et al. (2017) explore the learned grammars of two state-of-the-art natural language inference models: SPINN (Bowman et al., 2016) and Gumbel Tree-LSTM (Choi et al., 2017). They find that both SPINN and Gumbel Tree-LSTM have close to or worse than chance-level parsing performance on standard benchmarks. Furthermore, they note that in the case of the SPINN architecture the learned structure depends on the tuning of the model and that their findings are different from the SPINN implementation of Yogatama et al. (2016). They further note that consistency of the parses produced by Gumbel Tree-LSTM across multiple runs is not far off chance level. The Hierarchical Multi-scale LSTM architecture (Chung et al., 2016a) is in the"
C18-1272,P17-1057,1,0.825313,"e ability of Long Short-Term Memory (LSTM) language models to encode subjectverb agreement (Linzen et al., 2016), analyzed the translation quality of character-level sequenceto-sequence models in terms of several morpho-syntactic grammaticality tests (Sennrich, 2016), Li et al. (2016) introduce a representation erasure technique to measure the amount of contribution of input words and specific phrases to the decision of RNN-based sentiment classifiers, K´ad´ ar et al. (2017) examine the linguistic representations of Gated Recurrent Unit-based architectures 3216 for image-sentence ranking, and Chrupala et al. (2017) and Alishahi et al. (2017) analyze the linguistic structure learned by Recurrent Highway Network models of visually grounded speech understanding. As an alternative approach, a number of white-box architectures have been proposed which learn explicit representations of linguistic structure. For example, Dyer et al. (2016) examine Recurrent Neural Network grammars and analyze its learned syntax (Kuncoro et al., 2016). They conclude that their architecture learns a similar notion of headedness as established headrule sets and the model learns structure similar to traditional nonterminal categor"
C18-1272,P16-1160,0,0.0731489,"ce brought by modern neural network architectures often comes at the cost of our understanding of the representations and structural information the system learns. However, for models to be generalizable to new domains, it is important to move towards analyzing such structural representations, and investigating their impact on the final performance of the model. In the current study, we examine the reproducibility of a language model with the ability to learn explicit linguistic structure: the Hierarchical Multiscale Recurrent Neural Network (HMLSTM) model. This architecture was introduced by Chung et al. (2016a) and set a new This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3215 Proceedings of the 27th International Conference on Computational Linguistics, pages 3215–3227 Santa Fe, New Mexico, USA, August 20-26, 2018. state of the art on language-modeling benchmarks Text8, Hutter Prize and character-level Penn Treebank. Additionally, the paper features examples where the lowest layer of the model recovers word-level segmentation and in some cases performs interpretable morphological analysis. With this"
C18-1272,N16-1024,0,0.0212606,"ue to measure the amount of contribution of input words and specific phrases to the decision of RNN-based sentiment classifiers, K´ad´ ar et al. (2017) examine the linguistic representations of Gated Recurrent Unit-based architectures 3216 for image-sentence ranking, and Chrupala et al. (2017) and Alishahi et al. (2017) analyze the linguistic structure learned by Recurrent Highway Network models of visually grounded speech understanding. As an alternative approach, a number of white-box architectures have been proposed which learn explicit representations of linguistic structure. For example, Dyer et al. (2016) examine Recurrent Neural Network grammars and analyze its learned syntax (Kuncoro et al., 2016). They conclude that their architecture learns a similar notion of headedness as established headrule sets and the model learns structure similar to traditional nonterminal categories. Williams et al. (2017) explore the learned grammars of two state-of-the-art natural language inference models: SPINN (Bowman et al., 2016) and Gumbel Tree-LSTM (Choi et al., 2017). They find that both SPINN and Gumbel Tree-LSTM have close to or worse than chance-level parsing performance on standard benchmarks. Furthe"
C18-1272,D16-1166,0,0.023397,"o shed light on the potential bottlenecks involved in re-purposing a complex deep learning sequence modeling architecture for computational linguistics studies. 1.2 The importance of interpretability Language data, both in the form of speech and text, exhibits hierarchical structure: characters or phonemes form morphemes and words, which form phrases, which in turn form whole sentences. At each level, the units are composed to build up the meaning of the sentence at the highest level. Modern RNN architectures have been very successful in solving character-level NLP tasks (Chung et al., 2016b; Golub and He, 2016; Mikolov et al., 2012). However, they do not make the learned linguistic structure explicit: rather it can be presumed to be cryptically encoded in the states of the hidden layers. The higher performance brought by modern neural network architectures often comes at the cost of our understanding of the representations and structural information such systems learn. However, for a model to be generalizable to new domains, it is important to move towards analyzing such structural representations, and investigating their impact on the final performance of the model. Understanding the structural in"
C18-1272,D17-1076,0,0.029609,"s other than language modeling. Crucially, we investigate whether the performance of the model depends on the acquisition of high-quality linguistic structure. 1.1 The importance of reproducibility There is no established tradition of reproduction studies in computational linguistics; however, scattered attempts at reproducing a number of studies have highlighted the importance of investigating the dataset, the model architecture and the evaluation scheme used in experimental designs. For example, Mieskes (2017) quantifies the availability of non-benchmark datasets underlying the experiments. Horsmann and Zesch (2017) re-run coarse-grained multilingual part-of-speech tagging experiments by Plank et al. (2016) and confirm the superior performance of LSTM-based architectures on fine-grained tagsets. Marrese-Taylor and Matsuo (2017) fail to reproduce the results of three articles in the domain of aspect-based opinion mining, with the conclusion that repeating experiments without the availability of source code is hindered due to lack of details on pre-processing, model architecture specification and exact parameter settings. Morey et al. (2017) replicate the results of 9 discourse parsers trained on the RST D"
C18-1272,Q16-1037,0,0.0255235,"s learn. However, for a model to be generalizable to new domains, it is important to move towards analyzing such structural representations, and investigating their impact on the final performance of the model. Understanding the structural information encoded in black-box neural network architectures has been a desired target since Elman (1990) and several studies have used indirect post-analysis techniques and auxiliary tasks to probe the acquired structure. For example, recent studies have assessed the ability of Long Short-Term Memory (LSTM) language models to encode subjectverb agreement (Linzen et al., 2016), analyzed the translation quality of character-level sequenceto-sequence models in terms of several morpho-syntactic grammaticality tests (Sennrich, 2016), Li et al. (2016) introduce a representation erasure technique to measure the amount of contribution of input words and specific phrases to the decision of RNN-based sentiment classifiers, K´ad´ ar et al. (2017) examine the linguistic representations of Gated Recurrent Unit-based architectures 3216 for image-sentence ranking, and Chrupala et al. (2017) and Alishahi et al. (2017) analyze the linguistic structure learned by Recurrent Highway"
C18-1272,J93-2004,0,0.0607616,"the exact details in the paper. Our implementation follows Mikolov et al. (2012): we use batch size of 1 and carry the states of the recurrent model over to the following batch of 100 characters until the whole test sequence is processed. The experimental results reported in Section 4 are all using this revised implementation, manipulating various ablation factors as described in Section 3.2. Following Chung et al. (2016a), we report results on the following two datasets. Character-level Penn Treebank – The smaller-scale experiments apply variations of the model on the Penn Treebank dataset (Marcus et al., 1993) using the splits and preprocessing from Mikolov et al. (2012). All models are trained with sequence lengths of 100 and batch size of 64. Before each epoch the dataset is randomly cropped to be divisible by 100. The parameters are optimized with Adam (Kingma and Ba, 2014) with initial learning rate of 0.002, which is divided by 50 if no improvement on the validation data is observed after a full epoch. The norm of the gradient is clipped at 1.0. For all models we use 512 units for all layers and 128 dimension character embeddings as reported in the paper. Text8 – The Text8 dataset (Mahoney, 20"
C18-1272,E17-4003,0,0.0186917,"established tradition of reproduction studies in computational linguistics; however, scattered attempts at reproducing a number of studies have highlighted the importance of investigating the dataset, the model architecture and the evaluation scheme used in experimental designs. For example, Mieskes (2017) quantifies the availability of non-benchmark datasets underlying the experiments. Horsmann and Zesch (2017) re-run coarse-grained multilingual part-of-speech tagging experiments by Plank et al. (2016) and confirm the superior performance of LSTM-based architectures on fine-grained tagsets. Marrese-Taylor and Matsuo (2017) fail to reproduce the results of three articles in the domain of aspect-based opinion mining, with the conclusion that repeating experiments without the availability of source code is hindered due to lack of details on pre-processing, model architecture specification and exact parameter settings. Morey et al. (2017) replicate the results of 9 discourse parsers trained on the RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and show that most of the recent gains in the domain are due to non-trivial differences in evaluation methodology. We propose one further step in this direction: in a"
C18-1272,W17-1603,0,0.0149713,"which can facilitate re-using and further developing its architecture and repurposing it for tasks other than language modeling. Crucially, we investigate whether the performance of the model depends on the acquisition of high-quality linguistic structure. 1.1 The importance of reproducibility There is no established tradition of reproduction studies in computational linguistics; however, scattered attempts at reproducing a number of studies have highlighted the importance of investigating the dataset, the model architecture and the evaluation scheme used in experimental designs. For example, Mieskes (2017) quantifies the availability of non-benchmark datasets underlying the experiments. Horsmann and Zesch (2017) re-run coarse-grained multilingual part-of-speech tagging experiments by Plank et al. (2016) and confirm the superior performance of LSTM-based architectures on fine-grained tagsets. Marrese-Taylor and Matsuo (2017) fail to reproduce the results of three articles in the domain of aspect-based opinion mining, with the conclusion that repeating experiments without the availability of source code is hindered due to lack of details on pre-processing, model architecture specification and exa"
C18-1272,D17-1136,0,0.0304702,"ility of non-benchmark datasets underlying the experiments. Horsmann and Zesch (2017) re-run coarse-grained multilingual part-of-speech tagging experiments by Plank et al. (2016) and confirm the superior performance of LSTM-based architectures on fine-grained tagsets. Marrese-Taylor and Matsuo (2017) fail to reproduce the results of three articles in the domain of aspect-based opinion mining, with the conclusion that repeating experiments without the availability of source code is hindered due to lack of details on pre-processing, model architecture specification and exact parameter settings. Morey et al. (2017) replicate the results of 9 discourse parsers trained on the RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and show that most of the recent gains in the domain are due to non-trivial differences in evaluation methodology. We propose one further step in this direction: in addition to reproducing reported results, it is important to investigate the role the components of complex models play and examine their impact on the behavior of the model. The HMLSTM model that we choose as our case study has many desirable properties. However, it is fairly complex and allows for a considerable deg"
C18-1272,P16-2067,0,0.0310029,"nds on the acquisition of high-quality linguistic structure. 1.1 The importance of reproducibility There is no established tradition of reproduction studies in computational linguistics; however, scattered attempts at reproducing a number of studies have highlighted the importance of investigating the dataset, the model architecture and the evaluation scheme used in experimental designs. For example, Mieskes (2017) quantifies the availability of non-benchmark datasets underlying the experiments. Horsmann and Zesch (2017) re-run coarse-grained multilingual part-of-speech tagging experiments by Plank et al. (2016) and confirm the superior performance of LSTM-based architectures on fine-grained tagsets. Marrese-Taylor and Matsuo (2017) fail to reproduce the results of three articles in the domain of aspect-based opinion mining, with the conclusion that repeating experiments without the availability of source code is hindered due to lack of details on pre-processing, model architecture specification and exact parameter settings. Morey et al. (2017) replicate the results of 9 discourse parsers trained on the RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and show that most of the recent gains in t"
chrupala-etal-2008-learning,W96-0213,0,\N,Missing
chrupala-etal-2008-learning,A00-2013,0,\N,Missing
chrupala-etal-2008-learning,N06-1042,0,\N,Missing
chrupala-etal-2008-learning,P05-1071,0,\N,Missing
chrupala-etal-2008-learning,P98-1080,0,\N,Missing
chrupala-etal-2008-learning,C98-1077,0,\N,Missing
chrupala-etal-2008-learning,tufis-dragomirescu-2004-tiered,0,\N,Missing
chrupala-etal-2008-learning,erjavec-2004-multext,0,\N,Missing
chrupala-klakow-2010-named,N04-1043,0,\N,Missing
chrupala-klakow-2010-named,D07-1073,0,\N,Missing
chrupala-klakow-2010-named,W02-1001,0,\N,Missing
chrupala-klakow-2010-named,J92-4003,0,\N,Missing
chrupala-klakow-2010-named,P08-1068,0,\N,Missing
chrupala-klakow-2010-named,W03-0419,0,\N,Missing
chrupala-klakow-2010-named,doddington-etal-2004-automatic,0,\N,Missing
chrupala-klakow-2010-named,C96-1079,0,\N,Missing
chrupala-klakow-2010-named,W09-3301,0,\N,Missing
chrupala-klakow-2010-named,P05-1045,0,\N,Missing
D12-1059,P97-1054,0,0.0350035,"information about finer-grained categories (e.g. verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the over651 all word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syn"
D12-1059,J92-4003,0,0.44394,"reviewers for suggesting this condition as an informed baseline. drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as our approach to learning lexical categories. In Section 3.1 we describe the LDA model for word classes; in Section 3.2 we discuss the online Gibbs sampler we use for inference. 3.1 Word class learning with LDA Latent Dirichlet Allocation (LDA) was introduced by Blei et al. (2003) and is most commonly used for modeling the topic structure in document collections. It is a generative, pro"
D12-1059,I11-1041,1,0.927776,"5). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1 We thank an anonymous reviewers for suggesting this condition as an informed baseline. drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as our approach to learning lexical categories. In Section 3.1 we describe the LDA model for word classes; in Section 3.2 we discuss the online Gibbs sampler we use for inference. 3.1 Word class learning with"
D12-1059,W10-2922,1,0.880854,"tion of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1 We thank an anonymous reviewers for suggesting this condition as an informed baseline. drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. Here we adopt this model as ou"
D12-1059,E12-1024,0,0.061606,". verbs and nouns) can indeed help word learning in a more naturalistic incremental setting. On the other hand, the model of Alishahi and Fazly (2010) integrates manually annotated part-ofspeech tags into an incremental word learning algorithm, and shows that these tags boost the over651 all word learning performance, especially for infrequent words. In a different line of research, a number of models have been proposed which study the acquisition of the link between syntax and semantics within the Combinatory Categorial Grammar (CCG) framework (Briscoe 1997, Villavicencio 2002, Buttery 2006, Kwiatkowski et al. 2012). These approaches set the parameters of a semantic parser on a corpus of utterances paired with a logical form as their meaning. These models bring in extensive and detailed prior assumptions about the nature of the syntactic representation (i.e. atomic categories such as S and NP, and built-in rules which govern their combination), as well as about the representation of meaning via the formalism of lambda calculus. This is fundamentally different than the approach taken in this paper, which in comparison only assumes very simple syntactic and semantic representations of syntax. We view word"
D12-1059,W08-2112,0,0.0664821,"dition. 3 Online induction of word classes with LDA Empirical findings suggest that young children form their knowledge of abstract categories, such as verbs, nouns, and adjectives, gradually (e.g. Gelman and Taylor 1984, Kemp et al. 2005). In addition, several unsupervised computational models have been proposed for inducing categories of words which resemble part-of-speech categories, by 1 We thank an anonymous reviewers for suggesting this condition as an informed baseline. drawing on distributional properties of their context (see for example Redington et al. 1998, Clark 2000, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010). However, explicit accounts of how such categories can be integrated in a crosssituational model of word learning have been rare. Here we adopt an online version of the model proposed in Chrupała (2011), a method of soft word class learning using Latent Dirichlet Allocation. The approach is much more efficient than the commonly used alternative (Brown clustering, (Brown et al. 1992)) while at the same time matching or outperforming it when the word classes are used as automatically learned features for supervised learning of various language understanding tasks. H"
D12-1059,W00-0717,0,\N,Missing
D13-1146,basile-etal-2012-developing,1,0.726779,", as e.g. in did|n’t. An example is given in Figure 1. Table 1: Datasets characteristics. Name Language Domain Sentences Tokens GMB English Newswire 2,886 64,443 TNC Dutch Newswire 49,537 860,637 PAI Italian Web/various 42,674 869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Figure 1: Example of IOB-labeled characters Datasets In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (Basile et al., 2012), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 1423 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditional Random Fields (Lafferty et al., 2001), using as features the output label of each character, combined with 1) the character itself, 2) the output label on the previous character, 3) characters and/or their Unicode categories from context windows of varying sizes. For example, with a context size of 3, in Figure 1, features for the E in Eighty-three with the output label S woul"
D13-1146,P12-2074,0,0.0190705,"kenization, there is still room for improvement, in particular on the methodological side of the task. We are particularly interested in the following questions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? An Elephant in the Room Tokenization, the task of segmenting a text into words and sentences, is often regarded as a solved problem in natural language processing (Dridan and Oepen, 2012), probably because many corpora are already in tokenized format. But like an elephant in the living room, it is a problem that is impossible to overlook whenever new raw datasets need to be processed or when tokenization conventions are reconsidered. It is moreover an important problem, because any errors occurring early in the NLP pipeline affect further analysis negatively. And even though current tokenizers reach high performance, there are three issues that we feel haven’t been addressed satisfactorily so far: • Most tokenizers are rule-based and therefore hard to maintain and hard to adap"
D13-1146,J06-4003,0,0.0968001,"ner, 2004); • Word and sentence segmentation are often seen as separate tasks, but they obviously inform each other and it could be advantageous to view them as a combined task; 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations"
D13-1146,P10-1052,0,0.0419322,"Missing"
D13-1146,J02-3002,0,0.0248183,"-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hear"
D13-1146,J97-2002,0,0.0325392,"Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 3.1 Method IOB Tokenization IOB tagging is widely used in ta"
D13-1146,A97-1004,0,0.112885,"afted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection i"
D13-1146,H89-2048,0,0.0341909,"ilar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 3.1 Method IO"
D13-1146,J00-4006,0,\N,Missing
deng-chrupala-2014-semantic,J93-2003,0,\N,Missing
deng-chrupala-2014-semantic,D09-1092,0,\N,Missing
deng-chrupala-2014-semantic,P11-1060,0,\N,Missing
deng-chrupala-2014-semantic,P10-1129,0,\N,Missing
E12-1063,voorhees-tice-2000-trec,0,0.0555818,"where a human triager is presented with a ranked list of possible outputs (component labels or developer IDs). As such it is important to evaluate not only accuracy of the top ranking suggesting, but rather the quality of the whole ranked list. Previous research (Bhattacharya and Neamtiu 2010, Tamrawi et al. 2011) made an attempt at approximating this criterion by reporting scores which indicate whether the true output is present in the top n elements of the ranking, for several values of n. Here we suggest borrowing the mean reciprocal rank (MRR) metric from the information retrieval domain (Voorhees 2000). It is defined as the mean of the reciprocals of the rank at which the true output is found: MRR = 1 N N X rank(i)−1 i=1 where rank(i) indicates the rank of the ith true output. MRR has the advantage of providing a single number which summarizes the quality of 4.3 Input representation Since in this paper we focus on the issues related to concept drift and online learning, we kept the feature set relatively simple. We preprocess the text in the issue report title and description fields by removing HTML markup, tokenizing, lowercasing and removing most punctuation. We then extracted the followi"
E14-2023,D12-1042,0,0.0530024,"5000 LE Tilburg, The Netherlands † {beroth|tbarth|mgropp|dietrich.klakow}@lsv.uni-saarland.de * g.chrupala@uvt.nl Abstract does an organization have). A perfect system would have to return all relevant information (and only this) contained in the text corpus. TAC KBP aims at giving a realistic picture of not only precision but also recall of relation extraction systems on big corpora, and is therefore an advancement over many other evaluations done for relation extraction that are often precision oriented (Suchanek et al., 2007) or restrict the gold key to answers from a fixed candidate set (Surdeanu et al., 2012) or to answers contained in a data base (Riedel et al., 2010). Similar to the classical TREC evaluation campaigns in document retrieval, TAC KBP aims at approaching a true recall estimate by pooling, i.e. merging the answers of a timed-out manual search with the answers of all participating systems. The pooled answers are then evaluated by human judges. It is a big advantage of TAC KBP that the endto-end setup (from the query, through retrieval of candidate contexts and judging whether a relation is expressed, to normalizing answers and putting them into a knowledge base) is realistic. At the"
I11-1041,2005.mtsummit-papers.11,0,0.0149077,"Missing"
I11-1041,E09-1013,0,0.0166033,"Missing"
I11-1041,P08-1068,0,0.134684,"blem with inefficiency. Very fine-grained Brown classes are typically needed for good performance as shown by Turian et al.’s results. Our model for word class induction addresses both of the weaknesses. Using word representations Our main motivation for studying word class induction methods is to use them in a semisupervised learning scenario, where word representations are induced from a large unlabeled corpus and subsequently used as a source of features for a supervised model. Turian et al. (2010) compare the effect of using representations based on Brown classes, the Collobert and Weston (2008) embeddings and the Mnih and Hinton (2009) embeddings in learning English syntactic chunking (CoNLL 2000) and English coarse-grained Named Entity Recognition (CoNLL 2003). For both tasks the best representation is fine-grained Brown classes (3200 and 1000 classes respectively). Combining the Brown features with distributed embeddings further improves performance on NER but not on chunking. Lin and Wu (2009) use induce word and phrase classes and report results on NER which are higher than Turian et al. (2010)’s Brown scores, but this research used 700 billion words of web text and needed a clo"
I11-1041,J92-4003,0,0.588882,"gh quality implementations. We evaluate the model’s usefulness on finegrained Named Entity Recognition (NER), Morphological Analysis (MA) and semantic Relation Classification (RC) and show that • while the word classes obtained perform better than Brown classes, • they can be induced in a fraction of the time necessary to run the equivalent Brown model. 2 Inducing word representations There is a variety of approaches to inducing word representations from distributional information. In this section we briefly review the research most relevant to our proposed approach. Introduction Hard classes Brown et al. (1992) introduced an early model which induces a mapping from word types to classes. It is an agglomerative clustering algorithm which starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information between class bigrams. The result is a class hierarchy with word types at the leaves. The overall runtime of the algorithm is O(K 2 V ) where K is the number of classes and V the number of word types. Lin and Wu (2009) use a distributed ver"
I11-1041,P10-2040,0,0.364483,"nt reason: it involves highly abstract semantic relations, often not obviously inferable from surface lexical clues. 4 We propose an LDA-based model for word class induction and contrast its structure, efficiency, and performance to those exhibited by the Brown model. Other approaches Other approaches to inducing word representations do not rely on the notion of word class. Distributed word embeddings can be learned using a neural network-bases language models (Bengio et al. 2006, Collobert and Weston 2008, Mnih and Hinton 2009). Dimensionality reduction techniques such as SVD (Sch¨utze 1995, Lamar et al. 2010) and LSA (Deerwester et al. 1990) have also been found useful for generating word representations. 3 LDA model for word class induction 4.1 Weaknesses of Brown Here we address what we see as two related weaknesses of the Brown model. The algorithm’s quadratic dependence on K makes it inconvenient to induce more than a few hundred classes: running a 1.000 class model with a 400.000 vocabulary took over 100 hours. Second, the induced clustering is hard, and the only way to model ambiguous word types is to have a separate class for each kind of ambiguity. This in turn means that we need to learn"
I11-1041,W09-3821,0,0.0805035,"Missing"
I11-1041,chrupala-etal-2008-learning,1,0.36646,"Missing"
I11-1041,chrupala-klakow-2010-named,1,0.919376,"guity. This in turn means that we need to learn a large number of classes, which exacerbates the problem with inefficiency. Very fine-grained Brown classes are typically needed for good performance as shown by Turian et al.’s results. Our model for word class induction addresses both of the weaknesses. Using word representations Our main motivation for studying word class induction methods is to use them in a semisupervised learning scenario, where word representations are induced from a large unlabeled corpus and subsequently used as a source of features for a supervised model. Turian et al. (2010) compare the effect of using representations based on Brown classes, the Collobert and Weston (2008) embeddings and the Mnih and Hinton (2009) embeddings in learning English syntactic chunking (CoNLL 2000) and English coarse-grained Named Entity Recognition (CoNLL 2003). For both tasks the best representation is fine-grained Brown classes (3200 and 1000 classes respectively). Combining the Brown features with distributed embeddings further improves performance on NER but not on chunking. Lin and Wu (2009) use induce word and phrase classes and report results on NER which are higher than Turian"
I11-1041,P09-1116,0,0.140263,"uction Hard classes Brown et al. (1992) introduced an early model which induces a mapping from word types to classes. It is an agglomerative clustering algorithm which starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information between class bigrams. The result is a class hierarchy with word types at the leaves. The overall runtime of the algorithm is O(K 2 V ) where K is the number of classes and V the number of word types. Lin and Wu (2009) use a distributed version of K-Means to assign words and phrases to hard classes, and successfully use them as features in a NER task and in query classification. Word classes automatically induced from distributional evidence have proved useful in a variety of tasks, including Named Entity Recognition (Miller et al. 2004, Ratinov and Roth 2009, Chrupała and Klakow 2010, Turian et al. 2010), parsing (Koo et al. 2008, Suzuki et al. 2009, Candito and Crabb´e 2009) and sentence retrieval (Momtazi and Klakow 2009). Brown et al. (1992) introduced an algorithm which assigns word types to disjoint c"
I11-1041,W02-1001,0,0.0235703,"sk. We followed previous work when available and ran exploratory experiments with different feature combinations on the development data. The details of the final feature sets are given in the respective sections but in general we make use of the hierarchical nature of Brown classes by using them at several levels of granularity. For LDA classes we exploit their probabilistic softness by including feature probability or rank, and include classes inferred from context words when appropriate. 5.2.1 Table 2: Meaning of feature functions Baseline As a baseline we use a sequenceperceptron labeler (Collins 2002) with the following features: {w−2 , w−1 , w0 , lowercase(w0 ), wordshape(w0 ), suffix1 (w0 ), suffix2 (w0 ), suffix3 (w0 ), w1 , w2 }. For the explanation of the feature functions see Table 2. For inducing classes for this task we use the North American News Text Corpus described in section 5.1. When evaluating word classes we add to this feature set the Brown or LDA word class features: Named entity recognition Brown Class IDs encode the path in the class hierarchy, we thus use ID prefixes of different lengths to include classes at several levels of granularity. We also add feature conjuncti"
I11-1041,N04-1043,0,0.440582,"Missing"
I11-1041,D10-1113,0,0.014372,"Missing"
I11-1041,D08-1036,0,0.0419019,"tractive properties compared to Brown: 363 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 363–372, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Soft classes A limitation of the Brown model is that it performs hard clustering of word types, and cannot be used to disambiguate word occurrences based on context. Hidden Markov Models have been used to induce probabilistic (soft) word classes: training an HMM on unlabeled data one obtains classes which correspond to multinomial distributions over the vocabulary (Goldwater and Griffiths 2007, Gao and Johnson 2008). Griffiths et al. (2005) propose a model factored into an HMM which generates function words and an LDA topic models which generates content words. Learning the parameters of a bigram HMM takes O(K 2 N ) time where N is the number of word tokens in the corpus. nearly 20 years after it was proposed. We thus compare the performance of the LDA word class model to the Brown model on three NLP tasks: fine grained Named Entity Recognition, Morphological Analysis, and Relation Classification. The first two tasks are difficult due to the large number of labels and high potential for ambiguity. The th"
I11-1041,P07-1094,0,0.0442115,"Blei et al. 2003) which has attractive properties compared to Brown: 363 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 363–372, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Soft classes A limitation of the Brown model is that it performs hard clustering of word types, and cannot be used to disambiguate word occurrences based on context. Hidden Markov Models have been used to induce probabilistic (soft) word classes: training an HMM on unlabeled data one obtains classes which correspond to multinomial distributions over the vocabulary (Goldwater and Griffiths 2007, Gao and Johnson 2008). Griffiths et al. (2005) propose a model factored into an HMM which generates function words and an LDA topic models which generates content words. Learning the parameters of a bigram HMM takes O(K 2 N ) time where N is the number of word tokens in the corpus. nearly 20 years after it was proposed. We thus compare the performance of the LDA word class model to the Brown model on three NLP tasks: fine grained Named Entity Recognition, Morphological Analysis, and Relation Classification. The first two tasks are difficult due to the large number of labels and high potentia"
I11-1041,W09-1119,0,0.107046,"result in the least decrease in the mutual information between class bigrams. The result is a class hierarchy with word types at the leaves. The overall runtime of the algorithm is O(K 2 V ) where K is the number of classes and V the number of word types. Lin and Wu (2009) use a distributed version of K-Means to assign words and phrases to hard classes, and successfully use them as features in a NER task and in query classification. Word classes automatically induced from distributional evidence have proved useful in a variety of tasks, including Named Entity Recognition (Miller et al. 2004, Ratinov and Roth 2009, Chrupała and Klakow 2010, Turian et al. 2010), parsing (Koo et al. 2008, Suzuki et al. 2009, Candito and Crabb´e 2009) and sentence retrieval (Momtazi and Klakow 2009). Brown et al. (1992) introduced an algorithm which assigns word types to disjoint clusters and it remains a common choice when a simple way to automatically obtain word classes is needed. We present a word class induction method using Latent Dirichlet Allocation (Blei et al. 2003) which has attractive properties compared to Brown: 363 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 3"
I11-1041,S10-1057,0,0.0292902,"Missing"
I11-1041,W10-1410,1,0.891235,"Missing"
I11-1041,D09-1058,0,0.194342,"classes Brown et al. (1992) introduced an early model which induces a mapping from word types to classes. It is an agglomerative clustering algorithm which starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information between class bigrams. The result is a class hierarchy with word types at the leaves. The overall runtime of the algorithm is O(K 2 V ) where K is the number of classes and V the number of word types. Lin and Wu (2009) use a distributed version of K-Means to assign words and phrases to hard classes, and successfully use them as features in a NER task and in query classification. Word classes automatically induced from distributional evidence have proved useful in a variety of tasks, including Named Entity Recognition (Miller et al. 2004, Ratinov and Roth 2009, Chrupała and Klakow 2010, Turian et al. 2010), parsing (Koo et al. 2008, Suzuki et al. 2009, Candito and Crabb´e 2009) and sentence retrieval (Momtazi and Klakow 2009). Brown et al. (1992) introduced an algorithm which assigns word types to disjoint c"
I11-1041,taule-etal-2008-ancora,0,0.0138636,"Missing"
I11-1041,P10-1040,0,0.217644,"Missing"
I11-1041,S10-1047,0,0.0210411,"Missing"
I11-1041,N07-4013,0,0.010274,"Missing"
I11-1041,E95-1020,0,\N,Missing
J17-4003,W14-4012,0,0.0225363,"Missing"
J17-4003,P15-2019,1,0.860315,"Missing"
J17-4003,D14-1181,0,0.011298,"Missing"
J17-4003,N16-1082,0,0.0612046,"Missing"
J17-4003,Q16-1037,0,0.016413,"Missing"
J17-4003,P15-1150,0,0.061447,"Missing"
K17-1037,P06-4018,0,0.00699785,"Missing"
K17-1037,P17-1057,1,0.335159,"Missing"
K17-1037,Q16-1037,0,0.0076623,"ounts of transcribed speech, and these recent models come closer to the way humans acquire language in a grounded setting. It is thus especially interesting to investigate to what extent the traditional levels of linguistic analysis such as phonology, morphology, syntax and semantics are encoded in the activations of the hidden layers of these models. There are a small number of studies which focus on the syntax and/or semantics in the context of neural models of written language (e.g. Elman, 1991; Frank et al., 2013; K´ad´ar et al., 2016; Li et al., 2016a; Adi et al., 2016; Li et al., 2016b; Linzen et al., 2016). Taking it a step further, Gelderloos and Chrupała (2016) and Chrupała et al. (2017a) investigate the levels of representations in models which learn language from phonetic transcriptions and from the speech signal, respectively. Neither of these tackles the We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is"
K17-1037,C16-1124,1,0.329101,"dels come closer to the way humans acquire language in a grounded setting. It is thus especially interesting to investigate to what extent the traditional levels of linguistic analysis such as phonology, morphology, syntax and semantics are encoded in the activations of the hidden layers of these models. There are a small number of studies which focus on the syntax and/or semantics in the context of neural models of written language (e.g. Elman, 1991; Frank et al., 2013; K´ad´ar et al., 2016; Li et al., 2016a; Adi et al., 2016; Li et al., 2016b; Linzen et al., 2016). Taking it a step further, Gelderloos and Chrupała (2016) and Chrupała et al. (2017a) investigate the levels of representations in models which learn language from phonetic transcriptions and from the speech signal, respectively. Neither of these tackles the We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech si"
K17-1037,P17-1047,0,0.0802223,"jad et al., 2017). This type of approach is relevant also when the goal is the understanding of the dynamics in complex neural network models of speech understanding. Firstly because similar techniques are often applicable, but more importantly because the knowledge of how the workings of artificial and biological neural networks are similar or different is valuable for the general enterprise of cognitive science. Recent studies have implemented models which learn to understand speech in a weakly and indirectly supervised fashion from correlated audio and visual signal: Harwath et al. (2016); Harwath and Glass (2017); Chrupała et al. (2017a). This is a departure from typical Automatic Speech Recognition (ASR) systems which rely on large amounts of transcribed speech, and these recent models come closer to the way humans acquire language in a grounded setting. It is thus especially interesting to investigate to what extent the traditional levels of linguistic analysis such as phonology, morphology, syntax and semantics are encoded in the activations of the hidden layers of these models. There are a small number of studies which focus on the syntax and/or semantics in the context of neural models of written"
K17-1037,N16-1082,0,0.0164698,"peech Recognition (ASR) systems which rely on large amounts of transcribed speech, and these recent models come closer to the way humans acquire language in a grounded setting. It is thus especially interesting to investigate to what extent the traditional levels of linguistic analysis such as phonology, morphology, syntax and semantics are encoded in the activations of the hidden layers of these models. There are a small number of studies which focus on the syntax and/or semantics in the context of neural models of written language (e.g. Elman, 1991; Frank et al., 2013; K´ad´ar et al., 2016; Li et al., 2016a; Adi et al., 2016; Li et al., 2016b; Linzen et al., 2016). Taking it a step further, Gelderloos and Chrupała (2016) and Chrupała et al. (2017a) investigate the levels of representations in models which learn language from phonetic transcriptions and from the speech signal, respectively. Neither of these tackles the We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number"
K18-1039,S15-2045,0,0.0356114,"Missing"
K18-1039,S14-2010,0,0.0354622,"ccessfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com baseline, and it improves performance on semantic textual similarity benchmarks (Agirre et al., 2014, 2015). These findings suggest that it may be beneficial to consider another language as another modality in a monolingual grounded language learning model. In the grounded learning scenario, descriptions of an image in multiple languages can be considered as multiple views of the same or closely related data. These additional views can help overcome the problems of data sparsity, and have practical implications for efficiently collecting imagetext datasets in different languages. In real-life applications, many tasks and domains can involve code switching (Barman et al., 2014), which is easi"
K18-1039,Q16-1031,0,0.152459,"epresentation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com baseline, and it improves performance on semantic textual similarity benchmarks (Agirre et al., 2014, 2015). These findings suggest that it may be beneficial to consider another language as another modality in a monolingual grounded langu"
K18-1039,P16-1160,0,0.0242586,"f perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com baseline, and it improves performance on semantic textual similarity benchmarks (Agirre et al., 2014, 2015). These findings suggest that it may be beneficial to consider another lan"
K18-1039,C04-1051,0,0.116917,"et of images in multiple language enables further improvements via an additional caption-caption ranking objective. 1 Introduction Multimodal representation learning is largely motivated by evidence of perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal m"
K18-1039,W17-4718,1,0.89966,"Missing"
K18-1039,W16-3210,1,0.832014,"Missing"
K18-1039,D16-1044,0,0.0473835,"age–caption pairs further from each other, in a joint embedding space. while not stopping criterion do T ∼ Bern(p) if T = 1 then Dn ∼ Dc2i &lt; c, i >∼ Dn a ← φ(c, θφ ) b ← ψ(i, θψ ) else &lt; ca , cb >∼ Dc2c a ← φ(ca , θφ ) b ← φ(cb , θφ ) end if [θφ ; θψ ] ← SGD(∇[θφ ;θψ ] J (a, b)) end while In addition to learning grounded representations for image-sentence ranking, joint vision and language systems have been proposed to solve a wide range of tasks across modalities such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015; Fukui et al., 2016; Jabri et al., 2016), text-toimage synthesis (Reed et al., 2016) and multimodal machine translation (Libovicky and Helcl, 2017; Elliott and Kádár, 2017). Our work is also closely related to multilingual joint representation learning. In this scenario, a single model is trained to solve a task across multiple languages. Ammar et al. (2016) train a multilingual dependency parser on the Universal Dependencies treebank (Nivre et al., 2015) and show that on average the single multilingual model outperforms the monolingual baselines. Johnson et al. (2016) present a zero-shot neural machine translat"
K18-1039,D15-1070,0,0.0234017,"ble pair Figure 1: An example taken from the Translation and Comparable portions of the Multi30K dataset. The translation portion (a) contains professional translations of the English captions into German, French, and Czech. The comparable portion (b) consists of five independently crowdsourced English and German descriptions, given only the image. Note that the sentences in (b) convey different information from the English–German translation pair in (a). captions are collected in different languages. Such disjoint settings have been explored in pivot-based multimodal representation learning (Funaki and Nakayama, 2015; Rajendran et al., 2015) or zero-shot multi-modal machine translation (Nakayama and Nishida, 2017). We compare translated vs. independently collected captions in Sections 5.2 and 6.1, and overlapping vs. disjoint images in Section 5.3. High-to-low resource transfer: In Section 6.2 we investigate whether low-resource languages benefit from jointly training on larger data sets from higher-resource languages. This type of transfer has previously been shown to be effective in machine translation (e.g., Zoph et al., 2016). Training objective: In addition to learning to map images to sentences, we"
K18-1039,D14-1005,0,0.0253874,"et al., 2001) or SimLex999 (Hill et al., 2015) compared to uni-modal representations (Kádár et al., 1 Gloss: Three men and two women with a South-East Asian appearance eat out of bowls at a black table, on which there are, among other things, paper cups and a bag; in the background there are other people and tables. 403 Require: p: task switching probability. Dc2i : datasets D1 . . . Dk of image-caption pairs &lt; c, i > for all k languages. Dc2c : data set of all possible caption pairs &lt; ca , cb > for all k languages. φ(c, θφ ): caption encoder ψ(i, θφ ): image encoder 2015; Bruni et al., 2014; Kiela and Bottou, 2014). Grounded representations of sentences that are learned from image–caption data sets also improve performance on a number of sentence-level tasks (Kiela et al., 2017; Yoo et al., 2017) when used as additional features to skip-thought vectors (Kiros et al., 2015). The model architectures used for these studies have the same overall structure as our model and coincide with image–sentence retrieval systems (Kiros et al., 2014; Karpathy and Fei-Fei, 2015): a pre-trained CNN is fixed or fine-tuned as image feature extractor, followed by a learned transformation, while sentence representations are"
K18-1039,P14-2135,0,0.0264095,"ow-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective. 1 Introduction Multimodal representation learning is largely motivated by evidence of perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using ima"
K18-1039,D17-1303,0,0.188901,"Missing"
K18-1039,Q17-1026,0,0.045102,"r work is also closely related to multilingual joint representation learning. In this scenario, a single model is trained to solve a task across multiple languages. Ammar et al. (2016) train a multilingual dependency parser on the Universal Dependencies treebank (Nivre et al., 2015) and show that on average the single multilingual model outperforms the monolingual baselines. Johnson et al. (2016) present a zero-shot neural machine translation model that is jointly trained on language pairs A ↔ B and B ↔ C and show that the model is capable of performing well on the unseen language pair A ↔ C. Lee et al. (2017) find that jointly training a many-languages-to-one translation model on unsegmented character sequences improves BLEU scores compared to monolingual training. They also show evidence that the model can handle intrasentence code-switching. Peters et al. (2017) train a multilingual sequence-to-sequence translation architecture on grapheme-to-phoneme conversion using more than 300 languages. They report better performance when adding multiple languages, even those which are not present in the test data. Finally, Figure 2: Pseudo-code of the training procedure used to train our multilingual multi"
K18-1039,J15-4004,0,0.0325193,"multilingual multimodal sentence embeddings. Finally, we recommend to collect captions for the same set of images in multiple languages, due to the benefits of the additional caption–caption ranking objective function. 2 Related work Learning visually grounded word-representations has been an active area of research in the fields of multi-modal semantics and cross-situational wordlearning. Such perceptually-grounded word representations have been shown to lead to higher correlation with human judgements on word-similarity benchmarks such as WordSim353 (Finkelstein et al., 2001) or SimLex999 (Hill et al., 2015) compared to uni-modal representations (Kádár et al., 1 Gloss: Three men and two women with a South-East Asian appearance eat out of bowls at a black table, on which there are, among other things, paper cups and a bag; in the background there are other people and tables. 403 Require: p: task switching probability. Dc2i : datasets D1 . . . Dk of image-caption pairs &lt; c, i > for all k languages. Dc2c : data set of all possible caption pairs &lt; ca , cb > for all k languages. φ(c, θφ ): caption encoder ψ(i, θφ ): image encoder 2015; Bruni et al., 2014; Kiela and Bottou, 2014). Grounded representati"
K18-1039,P17-2031,0,0.0182567,"= 1 then Dn ∼ Dc2i &lt; c, i >∼ Dn a ← φ(c, θφ ) b ← ψ(i, θψ ) else &lt; ca , cb >∼ Dc2c a ← φ(ca , θφ ) b ← φ(cb , θφ ) end if [θφ ; θψ ] ← SGD(∇[θφ ;θψ ] J (a, b)) end while In addition to learning grounded representations for image-sentence ranking, joint vision and language systems have been proposed to solve a wide range of tasks across modalities such as image captioning (Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015; Fukui et al., 2016; Jabri et al., 2016), text-toimage synthesis (Reed et al., 2016) and multimodal machine translation (Libovicky and Helcl, 2017; Elliott and Kádár, 2017). Our work is also closely related to multilingual joint representation learning. In this scenario, a single model is trained to solve a task across multiple languages. Ammar et al. (2016) train a multilingual dependency parser on the Universal Dependencies treebank (Nivre et al., 2015) and show that on average the single multilingual model outperforms the monolingual baselines. Johnson et al. (2016) present a zero-shot neural machine translation model that is jointly trained on language pairs A ↔ B and B ↔ C and show that the model is capable of performing well on th"
K18-1039,D17-1268,0,0.0278376,"de-switching. Peters et al. (2017) train a multilingual sequence-to-sequence translation architecture on grapheme-to-phoneme conversion using more than 300 languages. They report better performance when adding multiple languages, even those which are not present in the test data. Finally, Figure 2: Pseudo-code of the training procedure used to train our multilingual multi-task model. massively multilingual language representations trained on over 900 languages have been shown to resemble language families (Östling and Tiedemann, 2016) and can successfully predict linguistic typology features (Malaviya et al., 2017). In the vision and language domain, multilingualmultimodal sentence representation learning has been limited so far to two languages. The joint training of models on English and German data has been shown to outperform monolingual baselines on image-sentence ranking and semantic textual similarity tasks (Gella et al., 2017; Calixto et al., 2017). Recently Harwath et al. (2018) also showed the benefit of joint bilingual training in the domain of speech-to-image and image-to-speech retrieval using English and Hindi data. 3 Multilingual grounded learning We train a standard model of grounded lan"
K18-1039,marelli-etal-2014-sick,0,0.0251852,"iple language enables further improvements via an additional caption-caption ranking objective. 1 Introduction Multimodal representation learning is largely motivated by evidence of perceptual grounding in human concept acquisition and representation (Barsalou et al., 2003). It has been shown that visually grounded word and sentence-representations (Kiela et al., 2014; Baroni, 2016; Elliott and Kádár, 2017; Kiela et al., 2017; Yoo et al., 2017) improve performance on the downstream tasks of paraphrase identification, semantic entailment, and multimodal machine translation (Dolan et al., 2004; Marelli et al., 2014; Specia et al., 2016). Multilingual sentence representations have also been successfully applied to many-languages-to-one character-level machine translation (Chung et al., 2016) and multilingual dependency parsing (Ammar et al., 2016). Recently, Gella et al. (2017) proposed to learn both bilingual and multimodal sentence representations using images paired with captions independently collected in English and German. Their results show that bilingual training improves imagesentence ranking performance over a monolingual ∗ Marc-Alexandre Côté Microsoft Research Montreal macote@microsoft.com ba"
K18-1039,P16-1168,0,0.0209183,"dditional language, it is better to collect captions for the existing images because we can exploit the caption–caption objective. Our results lead to several directions for future work. We would like to pin down the mechanism via which multilingual training contributes to improved performance for image-sentence ranking. Additionally, we only consider four languages and show the gain of multilingual over bilingual training only for the English-German language pair. In future work we will incorporate more languages from data sets such as the Chinese Flickr8K (Li et al., 2016) or Japanese COCO (Miyazaki and Shimizu, 2016). 409 Acknowledgements Desmond Elliott was supported by an Amazon Research Award. 100 English 80 R@10 56.3 German 61 61.9 71.9 67.6 60 Image → Text Text → Image 56 65.1 49.1 40.1 40 52.6 39.5 20.9 20 0 Monolingual Bilingual Multilingual Monolingual Bilingual Multilingual Figure 3: Comparing models from the Monolingual, Bilingual and Multilingual settings. The Monolingual and Bilingual models are trained on the downsampled English and German comparable sets with additional c2c objective. The Multilingual model uses the French and Czech translation pairs as additional data. The results are repor"
K18-1039,W17-5403,0,0.0636155,"Missing"
K18-1039,W16-2346,1,0.912102,"Missing"
K18-1039,D16-1163,0,0.064418,"Missing"
N16-1043,W07-0607,1,0.711698,"eing able to associate a word with a visual extension, our model is simultaneously learning word representations that allow us to deal with a variety of other tasks—for example, as mentioned above, guessing the appearance of the object denoted by a new word from a purely verbal description, grouping concepts into categories by their similarity, or having both abstract and concrete words represented in the same space. 6 Related Work While there is work on learning from multimodal data (Roy, 2000; Yu, 2005, a.o.) as well as work on learning distributed representations from childdirected speech (Baroni et al., 2007; Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed representations from multimodal child-directed data. For example, in comparison to Yu (2005)’s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multi"
N16-1043,P15-1029,0,0.0199639,"d multimodal features, while in Yu’s approach words are simply distributions over categories. It is therefore not clear how Yu’s approach could capture phenomena such as predicting appearance from a verbal description or representing abstract words–all tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007)’s Bayesian model we compare against could be extended to include realistic visual data in a similar vein to Yu’s, but it would then have the same limitations. Our work is also related to research on reference resolution in dialogue systems, such as Kennington and Schlangen (2015). However, unlike Kennington 391 Conclusion Our very encouraging results suggest that multimodal distributed models are well-suited to simulating human word learning. We think the most pressing issue to move ahead in this direction is to construct larger corpora recording the linguistic and visual environment in which children acquire language, in line with the efforts of the Human Speechome Project (Roy, 2009; Roy et al., 2015). Having access to such data will enable us to design agents that acquire semantic knowledge by leveraging all available cues present in multimodal communicative setups"
N16-1043,N15-1016,1,0.851398,"e objects present in a communicative episode. Inspired by recent computational models of meaning (Bruni et al., 2014; Kiros et al., 2014; Silberer 1 See K´ad´ar et al. (2015) for a recent review of this line of work, and another learning model using, like ours, real visual input. 2 Attentive Social MSG Model Like the original MSG, our model learns multimodal word embeddings by reading an utterance sequentially and making, for each word, two sets of predictions: (a) the preceding and following words, and (b) the visual representations of objects co-occurring with the utterance. However, unlike Lazaridou et al. (2015), we do not assume we know the right object to be associated with a word. We consider instead a more realistic scenario where multiple words in an utterance co-occur with multiple objects in the corresponding scene. Under this referential uncertainty, the model needs to induce word-object associations as part of learning, relying on current knowledge about word-object affinities as well as on any social clues present in the scene. Similar to the standard skipgram, the model’s parameters are context word embeddings W0 and tar387 Proceedings of NAACL-HLT 2016, pages 387–392, c San Diego, Califor"
N16-1043,P14-1068,0,0.221684,"Missing"
P06-2018,P04-1041,1,0.846859,"Missing"
P06-2018,H05-1100,0,0.0655456,"Missing"
P06-2018,J96-1002,0,0.0126634,"Missing"
P06-2018,A00-2031,0,0.662248,"Missing"
P06-2018,P04-1040,0,0.282875,"Missing"
P06-2018,Y04-1016,1,0.905065,"Missing"
P14-2111,D12-1039,0,0.663643,"lang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which 2 Methods Many approaches to text normalization adopt the noisy channel setting, where the model normalizing source string s into target canonical form t is factored into two parts: tˆ = arg maxt P (t)P (s|t). The error term P (s|t) models how canonical strings are transformed into variants such as e.g. misspellings, emphatic lengthenings or abbreviations. The language model P (t) encodes which target strings are probable. We think this decomposition is less appropriate 680 Proceedings of the 52nd Annual Meeting of the A"
P14-2111,chrupala-etal-2008-learning,1,0.126164,"Missing"
P14-2111,P10-1052,0,0.0584809,"Missing"
P14-2111,R13-1024,0,0.0224146,"Missing"
P14-2111,P12-1109,0,0.470724,"Missing"
P14-2111,D08-1113,0,0.0316467,"Missing"
P14-2111,P11-2013,0,0.332186,"egisters ranging from formal to internet slang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which 2 Methods Many approaches to text normalization adopt the noisy channel setting, where the model normalizing source string s into target canonical form t is factored into two parts: tˆ = arg maxt P (t)P (s|t). The error term P (s|t) models how canonical strings are transformed into variants such as e.g. misspellings, emphatic lengthenings or abbreviations. The language model P (t) encodes which target strings are probable. We think this decomposition is less appropriate 680 Proce"
P14-2111,D13-1146,1,0.878022,"ed to be tuned on some annotated data. In this work we suggest a simple, supervised characterlevel string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources. Our model learns sequences of edit operations from labeled data using a Conditional Random Field (Lafferty et al., 2001). Unlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation (Chrupała, 2013), and word and sentence boundary detection (Evang et al., 2013). We train a recurrent neural network language model (Mikolov et al., 2010; Mikolov, 2012b) on a large collection of tweets. When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model. The principal contributions of our work are: (i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that characterlevel neural text embeddings can be used to eff"
P14-2111,W11-2210,0,0.143141,"om formal to internet slang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which 2 Methods Many approaches to text normalization adopt the noisy channel setting, where the model normalizing source string s into target canonical form t is factored into two parts: tˆ = arg maxt P (t)P (s|t). The error term P (s|t) models how canonical strings are transformed into variants such as e.g. misspellings, emphatic lengthenings or abbreviations. The language model P (t) encodes which target strings are probable. We think this decomposition is less appropriate 680 Proceedings of the 52nd An"
P14-2111,P11-1038,0,0.840524,"nd writing systems, in registers ranging from formal to internet slang. Substantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content. One approach has been text normalization, i.e. transforming tweet text into a more canonical form which standard NLP tools expect. A multitude of resources and approaches have been used to deal with normalization: handcrafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof. Methods such as those of Han and Baldwin (2011), Liu et al. (2011), Gouws et al. (2011) or Han et al. (2012) are unsupervised but they typically use many adjustable parameters which 2 Methods Many approaches to text normalization adopt the noisy channel setting, where the model normalizing source string s into target canonical form t is factored into two parts: tˆ = arg maxt P (t)P (s|t). The error term P (s|t) models how canonical strings are transformed into variants such as e.g. misspellings, emphatic lengthenings or abbreviations. The language model P (t) encodes which target strings are probable. We think this decomposition is less ap"
P15-2019,D12-1059,1,0.861694,"investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupała, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artiﬁcially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang"
P15-2019,W05-0614,0,0.0345025,"Language Processing (Short Papers), pages 112–118, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artiﬁcially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupała, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from u"
P15-2019,J15-4004,0,0.0459104,"Missing"
P15-2019,W14-4012,0,0.111822,"Missing"
P15-2019,E12-1024,0,0.0156421,"videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupała, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artiﬁcially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input. Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most rel"
P15-2019,N15-1016,0,0.0695508,"8, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artiﬁcially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words. A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupała, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artiﬁcially generated"
P15-2019,Q14-1017,0,0.128564,"Missing"
P15-2019,Q14-1006,0,0.116224,"Missing"
P15-2019,N15-1173,0,\N,Missing
P17-1057,S12-1051,0,0.00467298,"/site pic/pick sun/son wears/wares pause/paws tied/tide ware/wear sales/sails boarder/border plane/plain lapse/laps rose/rows stares/stairs log(mincount) 4 5 6 seen/scene plains/planes see/sea main/mane rains/reins tea/tee stair/stare waist/waste hole/whole suite/sweet pairs/pears cole/coal sale/sail 7 Figure 7: Disambiguation performance per layer. Points #0 and #6 (connected via dotted lines) represent the input vectors and utterance embeddings, respectively. The black line shows the overall mean RER. Flickr8K and video captions from the SemEval 2012 STS MSRVideo Description data set (STS) (Agirre et al., 2012). Captions were paired at random, as well as modified to obtain semantically similar and contrasting counterparts, and the resulting pairs were rated for semantic similarity. For all sentence pairs in SICK, we generate synthetic spoken sentences and feed them to the COCO Speech RHN, and calculate the cosine similarity between the averaged MFCC input vectors, the averaged hidden layer activation vectors, and the sentence embeddings. Z-score transformation was applied before calculating the cosine similarities. We then correlate these cosine similarities with • semantic relatedness according to"
P17-1057,P15-2019,1,0.850784,"Missing"
P17-1057,C16-1124,1,0.661477,"In contrast, several studies presented models that learn from sensory rather than symbolic input, which is rich with regards to the signal itself, but very limited in scale and variation (e.g. Roy and Pentland, 2002; Yu and Ballard, 2004; Lazaridou et al., 2016). 2.1 Multimodal language acquisition Chrupała et al. (2015) introduce a model that learns to predict the visual context from image captions. The model is trained on image-caption pairs from MSCOCO (Lin et al., 2014), capturing both rich visual input as well as larger scale input, but the language input still consists of word symbols. Gelderloos and Chrupała (2016) propose a similar architecture that instead takes phonemelevel transcriptions as language input, thereby incorporating the word segmentation problem into the learning task. In this work, we introduce an architecture that learns from continuous speech and images directly. This work is related to research on visual grounding of language. The field is large and growing, with most work dedicated to the ground2.2 Analysis of neural representations While analysis of neural methods in NLP is often limited to evaluation of the performance on the training task, recently methods have been introduced to"
P17-1057,P17-1047,0,0.409101,"(2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes. The use of such moderately large and low-level data allows the authors to train a multi-layer recurrent neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process. Furthermore, in a series of recent studies Harwath and Glass (2015); Harwath et al. (2016); Harwath and Glass (2017) use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural network models. Finally, there is a small but growing body of work dedicated to elucidating the nature of representations learned by neural networks from language data (see Section 2.2 for a brief overview). In the current work we build on these three strands of research and contribute the following advances: • We use a multi-layer gated recurrent neural network to properly model the temporal nature of speech signal and substantially improve performance compared to the c"
P17-1057,N16-1043,1,0.862673,"Missing"
P17-1057,marelli-etal-2014-sick,0,0.00536903,"ty of a word being positive or negative, we use each positive target as a negative target for another utterance in the validation Figure 5: Mean accuracy values for predicting the presence of a word in an utterance for Flickr8K and COCO. Layers 1–5 represent the (normalized) average unit activations, whereas the first (#0) and last point represent average input vectors and utterance embeddings, respectively. 4.5 Sentence similarity Next we explore to what extent the model’s representations correspond to those of humans. We employ the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014). SICK consists of image descriptions taken from 619 1.00 RER 0.75 0.50 0.25 Figure 6: Pearson’s r of cosine similarities of averaged input MFCCs and COCO Speech RHN hidden layer activation vectors and embeddings of sentence pairs with relatedness scores from SICK, cosine similarity of COCO Text RHN embeddings, and edit similarity. 0.00 0 2 4 6 layer words peaking/peeking great/grate mantle/mantel peer/pier tale/tail wit/whit weight/wait isle/aisle sight/site pic/pick sun/son wears/wares pause/paws tied/tide ware/wear sales/sails boarder/border plane/plain lapse/laps rose/rows stares/stairs lo"
P19-1283,K17-1037,1,0.850186,"easing interest in techniques to analyze these models and gain insight into how they encode linguistic information. For an overview of analysis techniques, see Belinkov and Glass (2019). The most widespread family of techniques are diagnostic models, which use the internal activations of neural networks trained on a particular task as input to another predictive model. The success of such a predictive model is then interpreted as evidence that the predicted information has been encoded by the original neural model. The approach has also been called auxiliary task (Adi et al., 2017), decoding (Alishahi et al., 2017), diagnostic classifier (Hupkes et al., 2018) or probing (Conneau et al., 2018). Diagnostic models have used a range of predictive tasks, but since their main purpose is to help us better understand the dynamics of a complex model, they themselves need to be kept simple and interpretable. This means that the predicted information in these techniques is typically limited to simple class labels or values, as opposed to symbolic, structured representations of interest to linguists such as syntactic trees. In order to work around this limitation Tenney et al. (2019) present a method for probing co"
P19-1283,Q19-1004,0,0.0296077,"imple bag-of-words representations; we also show that according to our metrics syntax is most salient in the intermediate layers of BERT. 2952 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2952–2962 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 2.1 Related work Analytic methods The dominance of deep learning models in NLP has brought an increasing interest in techniques to analyze these models and gain insight into how they encode linguistic information. For an overview of analysis techniques, see Belinkov and Glass (2019). The most widespread family of techniques are diagnostic models, which use the internal activations of neural networks trained on a particular task as input to another predictive model. The success of such a predictive model is then interpreted as evidence that the predicted information has been encoded by the original neural model. The approach has also been called auxiliary task (Adi et al., 2017), decoding (Alishahi et al., 2017), diagnostic classifier (Hupkes et al., 2018) or probing (Conneau et al., 2018). Diagnostic models have used a range of predictive tasks, but since their main purp"
P19-1283,Q17-1010,0,0.0288708,"f questions as the diagnostic model. It has the added advantage of being also easily applicable to structured symbolic representations, while the RSA scores and the full RSA correlation pattern provides a complementary source of insight into neural representations. Encouraged by these findings, we next apply both RSA and RSAREGRESS to representations of natural language sentences. 5 Natural language Infersent This is the supervised model described in Conneau et al. (2017) based on a bidirectional LSTM trained on natural language inference. We use the infersent2 model with pretrained fastText (Bojanowski et al., 2017) word embeddings.3 We also test a randomly initialized version of this model, including random word embeddings. BERT This is an unsupervised model based on the Transformer architecture (Vaswani et al., 2017) trained on a cloze-task and next-sentence prediction (Devlin et al., 2018). We use the Pytorch version of the large 24-layer model (bert-large-uncased).4 We also test a randomly initialized version of this model. 5.2 Here we use our proposed RSA-based techniques to compare tree-structure representations of natural language sentences with their neural representations captured by sentence em"
P19-1283,D18-1119,0,0.121083,"t al. (2008) present RSA as a variant of pattern-information analysis, to be applied for understanding neural activation patterns in human brains, for example syntactic computations (Tyler et al., 2013) or sensory cortical processing (Yamins and DiCarlo, 2016). The core idea is to find connections between data from neuroimaging, behavioral experiments and computational modeling by correlating representations of stimuli in each of these representation spaces via their pairwise (dis)similarities. RSA has also been used for measuring similarities between neuralnetwork representation spaces (e.g. Bouchacourt and Baroni, 2018; Chrupała, 2019). 2.3 Tree kernels For extending RSA to a structured representation space, we need a metric for measuring (dis)similarity between two structured representations. Kernels provide a suitable framework for this purpose: Collins and Duffy (2002) introduce convolutional kernels for syntactic parse trees as a metric which quantifies similarity between trees as the number of overlapping tree fragments between them, and introduce a polynomial time algorithm to compute these kernels; Moschitti (2006) propose an efficient algorithm for computing tree kernels in linear average running ti"
P19-1283,P19-1647,1,0.501359,"variant of pattern-information analysis, to be applied for understanding neural activation patterns in human brains, for example syntactic computations (Tyler et al., 2013) or sensory cortical processing (Yamins and DiCarlo, 2016). The core idea is to find connections between data from neuroimaging, behavioral experiments and computational modeling by correlating representations of stimuli in each of these representation spaces via their pairwise (dis)similarities. RSA has also been used for measuring similarities between neuralnetwork representation spaces (e.g. Bouchacourt and Baroni, 2018; Chrupała, 2019). 2.3 Tree kernels For extending RSA to a structured representation space, we need a metric for measuring (dis)similarity between two structured representations. Kernels provide a suitable framework for this purpose: Collins and Duffy (2002) introduce convolutional kernels for syntactic parse trees as a metric which quantifies similarity between trees as the number of overlapping tree fragments between them, and introduce a polynomial time algorithm to compute these kernels; Moschitti (2006) propose an efficient algorithm for computing tree kernels in linear average running time. 2.4 Synthetic"
P19-1283,D17-1070,0,0.190152,"metric to compute the proportion of common substructures between trees. This approach enables straightforward comparison of neural and symbolic-linguistic representations. Furthermore, we introduce RSAREGRESS , a similarity-based analytic method which combines features of RSA and of diagnostic models. We validate both techniques on neural models which process a synthetic language for arithmetic expressions with a simple syntax and semantics and show that they behave as expected in this controlled setting. We further apply our techniques to two neural models trained on English text, Infersent (Conneau et al., 2017) and BERT (Devlin et al., 2018), and show that both models encode a substantial amount of syntactic information compared to random models and simple bag-of-words representations; we also show that according to our metrics syntax is most salient in the intermediate layers of BERT. 2952 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2952–2962 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 2.1 Related work Analytic methods The dominance of deep learning models in NLP has brought an increasing interest i"
P19-1283,P18-1198,0,0.142003,"Missing"
P19-1283,P17-1032,0,0.0298351,"a. For a discussion of this issue in the context of diagnostic models see Zhang and Bowman (2018). Alternative approaches have been proposed to analyzing neural models of language. For example, Saphra and Lopez (2019) train a language model and parallel recurrent models for POS, semantic and topic tagging, and measure the correlation between the neural representations of the language model and the taggers. Others modify the neural architecture itself to make it more interpretable: Croce et al. (2018) adapt layerwise relevance propagation (Bach et al., 2015) to Kernel-based Deep Architectures (Croce et al., 2017) in order to retrieve examples which motivate model decisions. A vector representation for a given structured symbolic input is built based on kernel evaluations between the input and a subset of training examples known as landmarks, and the network decision is then traced back to the landmarks which had most influence on it. In our work we also use kernels between symbolic structures, but rather than building a particular interpretable model we propose a general analytical framework. 2.2 Representation Similarity Analysis Kriegeskorte et al. (2008) present RSA as a variant of pattern-informat"
P19-1283,W18-5403,0,0.0161779,"s on each particular case, but may involve the dynamics of the network itself as well as features of the input data. For a discussion of this issue in the context of diagnostic models see Zhang and Bowman (2018). Alternative approaches have been proposed to analyzing neural models of language. For example, Saphra and Lopez (2019) train a language model and parallel recurrent models for POS, semantic and topic tagging, and measure the correlation between the neural representations of the language model and the taggers. Others modify the neural architecture itself to make it more interpretable: Croce et al. (2018) adapt layerwise relevance propagation (Bach et al., 2015) to Kernel-based Deep Architectures (Croce et al., 2017) in order to retrieve examples which motivate model decisions. A vector representation for a given structured symbolic input is built based on kernel evaluations between the input and a subset of training examples known as landmarks, and the network decision is then traced back to the landmarks which had most influence on it. In our work we also use kernels between symbolic structures, but rather than building a particular interpretable model we propose a general analytical framewo"
P19-1283,E06-1015,0,0.0884904,"r measuring similarities between neuralnetwork representation spaces (e.g. Bouchacourt and Baroni, 2018; Chrupała, 2019). 2.3 Tree kernels For extending RSA to a structured representation space, we need a metric for measuring (dis)similarity between two structured representations. Kernels provide a suitable framework for this purpose: Collins and Duffy (2002) introduce convolutional kernels for syntactic parse trees as a metric which quantifies similarity between trees as the number of overlapping tree fragments between them, and introduce a polynomial time algorithm to compute these kernels; Moschitti (2006) propose an efficient algorithm for computing tree kernels in linear average running time. 2.4 Synthetic languages When developing techniques for analyzing neural network models of language, several studies have used synthetic data from artificial languages. Using synthetic language has the advantage that its structure is well-understood and the complexity of the language and the statistical characteristics of the generated data can be carefully con2953 trolled. The tradition goes back to the first generation of connectionist models of language (Elman, 1990; Hochreiter and Schmidhuber, 1997)."
P19-1283,W18-5456,0,0.0161662,"of the language and the statistical characteristics of the generated data can be carefully con2953 trolled. The tradition goes back to the first generation of connectionist models of language (Elman, 1990; Hochreiter and Schmidhuber, 1997). More recently, Sennhauser and Berwick (2018) and Skachkova et al. (2018) both use contextfree grammars to generate data, and train RNNbased models to identify matching numbers of opening and closing brackets (so called Dyck languages). The task can be learned, but Sennhauser and Berwick (2018) report that the models fail to generalize to longer sentences. Paperno (2018) also show that with extensive training and the appropriate curriculum, LSTMs trained on synthetic language can learn compositional interpretation rules. Nested arithmetic languages are also appealing choices since they have an unambiguous hierarchical structure and a clear compositional semantic interpretation (i.e. the value of the arithmetic expression). Hupkes et al. (2018) train RNNs to calculate the value of such expressions and show that they perform and generalize well to unseen strings. They apply diagnostic classifiers to analyze the strategy employed by the RNN model. 3 Similarity-b"
P19-1283,N18-1202,0,0.0827073,"model based on the Transformer architecture (Vaswani et al., 2017) trained on a cloze-task and next-sentence prediction (Devlin et al., 2018). We use the Pytorch version of the large 24-layer model (bert-large-uncased).4 We also test a randomly initialized version of this model. 5.2 Here we use our proposed RSA-based techniques to compare tree-structure representations of natural language sentences with their neural representations captured by sentence embeddings. Such embeddings are often provided by NLP systems trained on unlabeled text, using variants of a language modeling objective (e.g. Peters et al., 2018), next and previous sentence prediction (Kiros et al., 2015; Logeswaran and Lee, 2018), or discourse based objectives (Nie et al., 2017; Jernite et al., 2017). Alternatively they can be either fully trained or fine-tuned on annotated data using a task such as natural language inference (Conneau et al., 2017). In our experiments we use one of each type of encoders. 5.1 any words and use raw, unweighted word counts. Encoders Bag of words As a baseline we use a classic bag of words model where a sentence is represented by a vector of word counts. We do not exclude Experimental settings Data We us"
P19-1283,W18-5448,0,0.0490182,"a method for probing complex structures via a formulation named edge probing, where classifiers are trained to predict various lexical, syntactic and semantic relations between representation of word spans within a sentence. Another important consideration when analyzing neural encodings is the fact that a randomly initialized network will often show non-random activation patterns. The reason for this depends on each particular case, but may involve the dynamics of the network itself as well as features of the input data. For a discussion of this issue in the context of diagnostic models see Zhang and Bowman (2018). Alternative approaches have been proposed to analyzing neural models of language. For example, Saphra and Lopez (2019) train a language model and parallel recurrent models for POS, semantic and topic tagging, and measure the correlation between the neural representations of the language model and the taggers. Others modify the neural architecture itself to make it more interpretable: Croce et al. (2018) adapt layerwise relevance propagation (Bach et al., 2015) to Kernel-based Deep Architectures (Croce et al., 2017) in order to retrieve examples which motivate model decisions. A vector repres"
P19-1283,N19-1329,0,0.0611099,"various lexical, syntactic and semantic relations between representation of word spans within a sentence. Another important consideration when analyzing neural encodings is the fact that a randomly initialized network will often show non-random activation patterns. The reason for this depends on each particular case, but may involve the dynamics of the network itself as well as features of the input data. For a discussion of this issue in the context of diagnostic models see Zhang and Bowman (2018). Alternative approaches have been proposed to analyzing neural models of language. For example, Saphra and Lopez (2019) train a language model and parallel recurrent models for POS, semantic and topic tagging, and measure the correlation between the neural representations of the language model and the taggers. Others modify the neural architecture itself to make it more interpretable: Croce et al. (2018) adapt layerwise relevance propagation (Bach et al., 2015) to Kernel-based Deep Architectures (Croce et al., 2017) in order to retrieve examples which motivate model decisions. A vector representation for a given structured symbolic input is built based on kernel evaluations between the input and a subset of tr"
P19-1283,W18-5414,0,0.0259314,"icient algorithm for computing tree kernels in linear average running time. 2.4 Synthetic languages When developing techniques for analyzing neural network models of language, several studies have used synthetic data from artificial languages. Using synthetic language has the advantage that its structure is well-understood and the complexity of the language and the statistical characteristics of the generated data can be carefully con2953 trolled. The tradition goes back to the first generation of connectionist models of language (Elman, 1990; Hochreiter and Schmidhuber, 1997). More recently, Sennhauser and Berwick (2018) and Skachkova et al. (2018) both use contextfree grammars to generate data, and train RNNbased models to identify matching numbers of opening and closing brackets (so called Dyck languages). The task can be learned, but Sennhauser and Berwick (2018) report that the models fail to generalize to longer sentences. Paperno (2018) also show that with extensive training and the appropriate curriculum, LSTMs trained on synthetic language can learn compositional interpretation rules. Nested arithmetic languages are also appealing choices since they have an unambiguous hierarchical structure and a cle"
P19-1283,silveira-etal-2014-gold,0,0.0503853,"Missing"
P19-1283,W18-5425,0,0.0260202,"e kernels in linear average running time. 2.4 Synthetic languages When developing techniques for analyzing neural network models of language, several studies have used synthetic data from artificial languages. Using synthetic language has the advantage that its structure is well-understood and the complexity of the language and the statistical characteristics of the generated data can be carefully con2953 trolled. The tradition goes back to the first generation of connectionist models of language (Elman, 1990; Hochreiter and Schmidhuber, 1997). More recently, Sennhauser and Berwick (2018) and Skachkova et al. (2018) both use contextfree grammars to generate data, and train RNNbased models to identify matching numbers of opening and closing brackets (so called Dyck languages). The task can be learned, but Sennhauser and Berwick (2018) report that the models fail to generalize to longer sentences. Paperno (2018) also show that with extensive training and the appropriate curriculum, LSTMs trained on synthetic language can learn compositional interpretation rules. Nested arithmetic languages are also appealing choices since they have an unambiguous hierarchical structure and a clear compositional semantic in"
P19-1647,K17-1037,1,0.878792,"words in the speech-image pairs, and Harwath et al. (2018b) which constructs a three-dimensional tensor encoding affinities between image regions and speech segments. The work of Chrupała et al. (2017) is similar in that it exploits datasets of images with spoken captions, but their grounded speech model is based around multi-layer Recurrent Highway Networks, and focuses on quantitative analyses of the learned representations. They show that the encoding of meaning tends to become richer in higher layers, whereas encoding of form tends to initially increase and then stay constant or decrease. Alishahi et al. (2017a) further analyze the representations of the same model and show that phonological form is reliably encoded in the lower recurrent layers of the network but becomes substantially attenuated in the higher layers. Drexler and Glass (2017) also analyze the representations of a visually grounded speech model with view of using such representations for unsupervised speech recognition, and show that they contain more linguistic and less speaker information than filterbank features. Kamper et al. (2017) use images as a pivot to learn to associate textual labels with spoken utterances, by mapping utt"
P19-1647,E17-2026,0,0.0155398,"speech retrieval. 2.2 Multi-task learning for speech and language The concept of multi-task learning (MTL) was introduced by Caruana (1997). Neural architectures 6453 widely used in the fields of speech and language processing make it easy to define parametersharing architectures and exploit MTL, and thus there has been a recent spurt of reports on its impact. Within Natural Language Processing (NLP), Luong et al. (2016) explore sharing encoders and decoders in a sequence-to-sequence architecture for translation, syntactic parsing, and image captioning, and show gains on some configurations. Bingel and Søgaard (2017) investigate which particular pairs of NLP tasks lead to gains, concluding that learning curves and label entropy of the tasks may be used as predictors. McCann et al. (2018) propose a 10-task NLP challenge, and a single MTL model which performs reasonably well on all tasks. Søgaard and Goldberg (2016) show that which parameters are shared in a multi-task architecture matters a lot: they find that when sharing parameters between syntactic chunking or supertagging and POS tagging as an auxiliary task, it was consistently better to only share the lower-layers of the model. Relatedly, Hashimoto e"
P19-1647,D17-1303,0,0.0197094,"nd how to schedule and weight the tasks. Some recent works have suggested specific approaches to deal with this complexity: Ruder et al. (2017) propose to learn from data which parameters to share in MTL with sluice networks and show some gains on NLP tasks. Kiperwasser and Ballesteros (2018) investigate how to interleave learning syntax and translation and how to schedule these tasks. Several works show that exploiting MTL via the use of multiple language versions of the same or comparable data leads to performance gains (e.g. Lee et al., 2017; Johnson et al., 2017; de Lhoneux et al., 2018). Gella et al. (2017) and Kádár et al. (2018) learn visual semantic embeddings from textual-visual datasets and show gains from additional languages which reuse the same encoder. Kádár et al. (2018) additionally show that an extra objective linking the languages directly rather than only via the visual modality provides additional performance gains. In the context of audio-visual data, Harwath et al. (2018a) applies a type of MTL in the setting where there are images paired with descriptions in English and Hindi. They project the images, English speech and Hindi speech into a joint semantic space, and show that tr"
P19-1647,P17-1047,0,0.0298924,"ng tools based on neural networks. Harwath and Glass (2015) collect spoken descriptions for the Flick8K captioned image dataset and present a model which is able to map presegmented spoken words to aspects of visual context. Harwath et al. (2016) describe a larger dataset of images paired with spoken captions (Places Audio Caption Corpus) and present an architecture that learns to project images and unsegmented spoken captions to the same embedding space. The sentence representation is obtained by feeding the spectrogram to a convolutional network. Further elaborations on this setting include Harwath and Glass (2017), which shows a clustering-based method to identify grounded words in the speech-image pairs, and Harwath et al. (2018b) which constructs a three-dimensional tensor encoding affinities between image regions and speech segments. The work of Chrupała et al. (2017) is similar in that it exploits datasets of images with spoken captions, but their grounded speech model is based around multi-layer Recurrent Highway Networks, and focuses on quantitative analyses of the learned representations. They show that the encoding of meaning tends to become richer in higher layers, whereas encoding of form ten"
P19-1647,D17-1206,0,0.0244986,"aard (2017) investigate which particular pairs of NLP tasks lead to gains, concluding that learning curves and label entropy of the tasks may be used as predictors. McCann et al. (2018) propose a 10-task NLP challenge, and a single MTL model which performs reasonably well on all tasks. Søgaard and Goldberg (2016) show that which parameters are shared in a multi-task architecture matters a lot: they find that when sharing parameters between syntactic chunking or supertagging and POS tagging as an auxiliary task, it was consistently better to only share the lower-layers of the model. Relatedly, Hashimoto et al. (2017) propose a method of training NLP tasks at multiple levels of complexity by growing the depth of the model to solve increasingly more difficult tasks. Swayamdipta et al. (2018) use similar ideas and show that syntactic information can be incorporated in a semantic task with MTL, using auxiliary syntactic tasks without building full-fledged syntactic structure at prediction time. MTL can lead to a bewildering number of choices regarding which tasks to combine, which parameters to share and how to schedule and weight the tasks. Some recent works have suggested specific approaches to deal with th"
P19-1647,K18-1039,1,0.936982,"of speech and language processing make it easy to define parametersharing architectures and exploit MTL, and thus there has been a recent spurt of reports on its impact. Within Natural Language Processing (NLP), Luong et al. (2016) explore sharing encoders and decoders in a sequence-to-sequence architecture for translation, syntactic parsing, and image captioning, and show gains on some configurations. Bingel and Søgaard (2017) investigate which particular pairs of NLP tasks lead to gains, concluding that learning curves and label entropy of the tasks may be used as predictors. McCann et al. (2018) propose a 10-task NLP challenge, and a single MTL model which performs reasonably well on all tasks. Søgaard and Goldberg (2016) show that which parameters are shared in a multi-task architecture matters a lot: they find that when sharing parameters between syntactic chunking or supertagging and POS tagging as an auxiliary task, it was consistently better to only share the lower-layers of the model. Relatedly, Hashimoto et al. (2017) propose a method of training NLP tasks at multiple levels of complexity by growing the depth of the model to solve increasingly more difficult tasks. Swayamdipta"
P19-1647,Q18-1017,0,0.0192793,"Swayamdipta et al. (2018) use similar ideas and show that syntactic information can be incorporated in a semantic task with MTL, using auxiliary syntactic tasks without building full-fledged syntactic structure at prediction time. MTL can lead to a bewildering number of choices regarding which tasks to combine, which parameters to share and how to schedule and weight the tasks. Some recent works have suggested specific approaches to deal with this complexity: Ruder et al. (2017) propose to learn from data which parameters to share in MTL with sluice networks and show some gains on NLP tasks. Kiperwasser and Ballesteros (2018) investigate how to interleave learning syntax and translation and how to schedule these tasks. Several works show that exploiting MTL via the use of multiple language versions of the same or comparable data leads to performance gains (e.g. Lee et al., 2017; Johnson et al., 2017; de Lhoneux et al., 2018). Gella et al. (2017) and Kádár et al. (2018) learn visual semantic embeddings from textual-visual datasets and show gains from additional languages which reuse the same encoder. Kádár et al. (2018) additionally show that an extra objective linking the languages directly rather than only via th"
P19-1647,Q17-1026,0,0.0335206,"ices regarding which tasks to combine, which parameters to share and how to schedule and weight the tasks. Some recent works have suggested specific approaches to deal with this complexity: Ruder et al. (2017) propose to learn from data which parameters to share in MTL with sluice networks and show some gains on NLP tasks. Kiperwasser and Ballesteros (2018) investigate how to interleave learning syntax and translation and how to schedule these tasks. Several works show that exploiting MTL via the use of multiple language versions of the same or comparable data leads to performance gains (e.g. Lee et al., 2017; Johnson et al., 2017; de Lhoneux et al., 2018). Gella et al. (2017) and Kádár et al. (2018) learn visual semantic embeddings from textual-visual datasets and show gains from additional languages which reuse the same encoder. Kádár et al. (2018) additionally show that an extra objective linking the languages directly rather than only via the visual modality provides additional performance gains. In the context of audio-visual data, Harwath et al. (2018a) applies a type of MTL in the setting where there are images paired with descriptions in English and Hindi. They project the images, English"
P19-1647,D18-1543,0,0.0226392,"Missing"
P19-1647,P16-2038,0,0.0284394,"d thus there has been a recent spurt of reports on its impact. Within Natural Language Processing (NLP), Luong et al. (2016) explore sharing encoders and decoders in a sequence-to-sequence architecture for translation, syntactic parsing, and image captioning, and show gains on some configurations. Bingel and Søgaard (2017) investigate which particular pairs of NLP tasks lead to gains, concluding that learning curves and label entropy of the tasks may be used as predictors. McCann et al. (2018) propose a 10-task NLP challenge, and a single MTL model which performs reasonably well on all tasks. Søgaard and Goldberg (2016) show that which parameters are shared in a multi-task architecture matters a lot: they find that when sharing parameters between syntactic chunking or supertagging and POS tagging as an auxiliary task, it was consistently better to only share the lower-layers of the model. Relatedly, Hashimoto et al. (2017) propose a method of training NLP tasks at multiple levels of complexity by growing the depth of the model to solve increasingly more difficult tasks. Swayamdipta et al. (2018) use similar ideas and show that syntactic information can be incorporated in a semantic task with MTL, using auxil"
P19-1647,D18-1412,0,0.0369927,"Missing"
R15-1005,P79-1022,0,0.616423,"tigating the relationship between the response variable and the predictor variables regularized regression models are preferred, because they solve highly variable estimates of the regression coefficients when there is multicollinearity or when the number of predictors is very large in connection to the number of observations (Hartmann et al., 2009). In programming languages a lot of terms appear together what can lead to multicollinearity. As the number of terms used in this study is extremely large (36,865) and in order to avoid overfitting, a regularized regression model, Ridge regression (Hoerl and Kennard, 1970b; Hoerl and Table 2: Descriptive statistics The independent variables title length, body length and user reputation are normalized by the logarithmic transformation using the natural logarithm, and for question score and number of answers we use percentile normalization. Most questions receive a small number of answers. On average a question receives a relatively low score. Question titles and bodies consisting of only one word may be questions where only a code snippet was posted. The high mean value of the user reputation suggests that many SO users have a high user reputation. As it has be"
W04-2415,W04-2412,1,0.635591,"Missing"
W04-2415,W02-1001,0,0.108027,"We describe a system for the CoNLL-2004 Shared Task on Semantic Role Labeling (Carreras and M`arquez, 2004a). The system implements a two-layer learning architecture to recognize arguments in a sentence and predict the role they play in the propositions. The exploration strategy visits possible arguments bottom-up, navigating through the clause hierarchy. The learning components in the architecture are implemented as Perceptrons, and are trained simultaneously online, adapting their behavior to the global target of the system. The learning algorithm follows the global strategy introduced in (Collins, 2002) and adapted in (Carreras and M`arquez, 2004b) for partial parsing tasks. 2 Semantic Role Labeling Strategy The strategy for recognizing propositional arguments in sentences is based on two main observations about argument structure in the data. The first observation is the relation of the arguments of a proposition with the chunk and clause hierarchy: a proposition places its arguments in the clause directly containing the verb (local clause), or in one of the ancestor clauses. Given a clause, we define the sequence of top-most syntactic elements as the words, chunks or clauses which are dire"
W10-1410,P04-1041,1,0.868231,"Missing"
W10-1410,W09-3821,1,0.89873,"Missing"
W10-1410,W09-1008,1,0.89151,"Missing"
W10-1410,A00-2018,0,0.735055,"rrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original F TB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (B KY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the F TB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the B KY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the B KY parser trained on the F TB. Candito et al. (2009) showed that using gold lemmatization provides a significant increase in performance. Obviously, less sparse lexical data which retains critical pieces of information can only help a model to perform better. This was shown in (Candito and Crabbé, 2009) where distributional word clusters were acquired from a 125 mill"
W10-1410,chrupala-etal-2008-learning,1,0.815209,"Missing"
W10-1410,Y09-1013,0,0.14934,"Missing"
W10-1410,C94-2149,0,0.129478,"Missing"
W10-1410,E09-1038,0,0.139265,"Missing"
W10-1410,P06-1055,0,0.345326,"Missing"
W10-1410,D07-1066,1,0.886037,"Missing"
W10-1410,sagot-etal-2006-lefff,0,0.0720109,"Missing"
W10-1410,W09-3820,0,0.130258,"Missing"
W10-1410,J93-2004,0,\N,Missing
W10-1410,J03-4003,0,\N,Missing
W10-2804,E09-1025,1,0.833372,"are identified by DIRT but not CurveS2 and the other way around. This seems to indicate that these algorithms do capture different aspects of the data and can be combined for better results. An important aspect here is the fact that obtaining highly accurate paraphrases at the subj dobj ←−−− represent −−−→ subj prp pobj ←−−− show −−→ in −−−→ subj dobj ←−−− display −−−→ subj dobj ←−−− bring −−−→ pobj prp dobj ←−−− with ←−− show −−−→ subj dobj Table 3: Top 10 paraphrases for X ←−−− show −−−→ Y cost of losing coverage is not particularly difficult4 however not very useful. Previous work such as (Dinu and Wang, 2009) has shown that for these resources, the coverage is a rather important aspect, since they have to capture the great variety of ways in which a meaning can be expressed in different contexts. DIRT CurveS2 subj dobj ←−−− show −−−→ pobj prp dobj ←−−− in ←−− indicate −−−→ pobj prp dobj ←−−− in ←−− ref lect −−−→ dobj prp pobj ←−−− interpret −−→ as −−−→ subj subj dobj ←−−− display −−−→ subj dobj ←−−− conf irm −−−→ subj prp pobj ←−−− point −−→ to −−−→ dobj ←−−− win −−−→ subj prp pobj ←−−− vie −−→ f or −−−→ subj prp pobj ←−−− compete −−→ f or −−−→ subj dobj pos prp subj dobj nn ←−−− win −−−→ title −−"
W10-2804,P07-1058,0,\N,Missing
W10-2804,E09-1000,0,\N,Missing
W10-2922,J92-4003,0,0.695998,"text of a word have been thoroughly studied. It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and 1.1 Unsupervised Models of Category Induction Several computational models have used distributional information for categorizing words (e.g. Brown et al., 1992; Redington et al., 1998; Clark, 2000; Mintz, 2002). The majority of these mod182 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182–191, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 1.2 els partition the vocabulary into a set of optimum clusters (e.g., Brown et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a use"
W10-2922,W00-0717,0,0.92068,"It has been shown that child-directed speech provides informative co-occurrence cues, which can be reliably used to form lexical categories (Redington et al., 1998; Mintz, 2003). The process of learning lexical categories by children is necessarily incremental. Human language acquisition is bounded by memory and processing limitations, and it is implausible that humans process large volumes of text at once and 1.1 Unsupervised Models of Category Induction Several computational models have used distributional information for categorizing words (e.g. Brown et al., 1992; Redington et al., 1998; Clark, 2000; Mintz, 2002). The majority of these mod182 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182–191, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 1.2 els partition the vocabulary into a set of optimum clusters (e.g., Brown et al., 1992; Clark, 2000). The generated clusters are intuitive, and can be used in different tasks such as word prediction and parsing. Moreover, these models confirm the learnability of abstract word categories, and show that distributional cues are a useful source of information for this pu"
W10-2922,E03-1009,0,0.0224869,"provide no account of words belonging to more than one category, and (ii) the batch algorithms used by these systems make them implausible for modeling human category induction. Unsupervised models of PoS tagging such as Goldwater & Griffiths (2007) do assign labels to wordtokens, but they still typically use batch processing, and what is even more problematic, they hardwire important aspects of the model, such as the final number of categories. Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatic"
W10-2922,W09-2112,0,0.0210475,"Missing"
W10-2922,W08-2112,0,0.457556,"ing, and what is even more problematic, they hardwire important aspects of the model, such as the final number of categories. Evaluation of the Induced Categories There is no standard and straightforward method for evaluating the unsupervised models of category learning (see Clark, 2003, for discussion). Many unsupervised models of lexical category acquisition treat the traditional part of speech (PoS) tags as the gold standard, and measure the accuracy and completeness of their induced categories based on how closely they resemble the PoS categories (e.g. Redington et al., 1998; Mintz, 2003; Parisien et al., 2008). However, it is not at all clear whether humans form the same types of categories. In fact, many language tasks might benefit from finer-grained categories than the traditional PoS tags used for corpus annotation. Frank et al. (2009) propose a different, automatically generated set of gold standard categories for evaluating an unsupervised categorization model. The gold-standard categories are formed according to “substitutability”: if one word can be replaced by another and the resulting sentence is still grammatical, then there is a good chance that the two words belong to the same category"
W10-2922,D07-1012,0,0.0325422,"Missing"
W10-2922,P07-1094,0,\N,Missing
W12-1914,J92-4003,0,0.743417,"tively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word tokens to a small set of d"
W12-1914,D11-1059,0,0.243575,"n Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rat"
W12-1914,I11-1041,1,0.945575,"categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labels. For feature learning, there are is no such limit"
W12-1914,W10-2922,1,0.848193,"n divergence between the probability distributions over classes associated with each word-type. When assigning POS tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag. This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets. 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representa"
W12-1914,P10-2040,0,0.0473855,"tasets. 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the le"
W12-1914,D10-1083,0,0.0466722,"s on 9 out of 10 datasets. 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the"
W12-1914,P09-1116,0,0.0530596,"cremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labels. For feature l"
W12-1914,N04-1043,0,0.0340545,"nstraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labe"
W12-1914,W08-2112,0,0.115145,"ed as the JensenShannon divergence between the probability distributions over classes associated with each word-type. When assigning POS tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag. This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets. 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an int"
W12-1914,P09-1057,0,0.0682214,"e based on Brown clusters on 9 out of 10 datasets. 1 Introduction Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is"
W12-1914,D07-1043,0,0.394073,"h tokenlevel tagging were not successful, here we focus exclusively on type-level tagging. 1 Given the tree and a word-type paired with a class We ran 200 Gibbs sampling passes, and set the LDA hyperdistribution, we generate a path to a leaf in the tree parameters to α = 10 and β = 0.1. K 102 0.20 ● 0.15 HCD 51.4 48.3 42.4 56.8 54.8 67.8 60.2 52.4 46.6 56.1 ● da ● en−ch ● sv ● L 13 16 19 20 12 12 14 11 20 17 Brown 42.2 38.5 45.3 49.2 49.4 66.0 62.0 52.9 45.8 51.8 HCD 52.9 54.4 46.8 63.6 53.4 78.2 61.3 54.7 51.9 56.1 Table 2: Evaluation of coarse-grained POS tagging on test data the V-measure (Rosenberg and Hirschberg 2007) against gold part-of-speech tags. We choose the best-performing pair of K and L and use this setting to label the test set. We tune separately for coarsegrained and fine-grained POS tags. Other than using the development set labels to tune these two parameters our system is unsupervised and uses no data other than the sentences in the provided data files. Table 1 and Table 2 show the best settings for the coarse- and fine-grained POS tagging for all the datasets, and the V-measure scores on the test set achieved by our labeler (HCD for Hierarchy over Class Distributions). Also included are th"
W12-1914,P10-1040,0,0.0850064,"g. Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al. (2010), Lamar et al. (2010), Christodoulopoulos et al. (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, T¨ackstr¨om et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labels. For feature learning, there are i"
W12-1914,N12-1052,0,0.0680003,"Missing"
W14-3915,C82-1023,0,0.223069,"Switched Data shared task in the Workshop on Computational Approaches to Code Switching. Wordlevel classification experiments were carried out using a simple dictionary-based method, linear kernel support vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various"
W14-3915,N13-1131,0,0.0403813,"ing (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, Introduction This paper describes DCU-UVT’s participation in the shared task Language Identification in Code-Switched Data (Solorio et al., 2014) at the Workshop on Computational Approac"
W14-3915,W14-3902,1,0.705154,"textual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), usi"
W14-3915,li-etal-2012-mandarin,0,0.0390339,"h, we use string edit distance, charactern-gram overlap and context similarity to make predictions. For the SVM approach, we experiment with context-independent (word, charactern-grams, length of a word and capitalisation information) and context-sensitive (adding the pre127 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Resource BNC LexNorm TrainingData TrainingData+BNC+LexNorm 2013), modern standard Arabic-Egyptian dialect (Elfardy et al., 2013), Mandarin-English (Li et al., 2012; Lyu et al., 2010), and English-HindiBengali (Barman et al., 2014). 3 Table 2: Average cross-validation accuracy of dictionary-based prediction for Nepali-English Data Statistics The training data provided for this task consists of tweets. Unfortunately, because of deleted tweets, the full training set could not be downloaded. Out of 9,993 Nepali-English training tweets, we were able to download 9,668 and out of 11,400 SpanishEnglish training tweets, we were able to download 11,353. Table 1 shows the token-level statistics of the two datasets. Label lang1 (en) lang2 (ne/es) ne ambiguous mixed"
W14-3915,P14-2111,1,0.88229,"Missing"
W14-3915,D13-1084,0,0.141574,"Missing"
W14-3915,C12-2029,0,0.0353294,"ied over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turki"
W14-3915,W13-2249,1,0.877549,"Missing"
W14-3915,D08-1102,0,0.0294762,"rt vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as"
W14-3915,D12-1039,0,0.0456464,"Missing"
W14-3915,D08-1110,0,0.0265957,"rt vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as"
W14-3915,W14-3907,0,0.041397,"ndom Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, Introduction This paper describes DCU-UVT’s participation in the shared task Language Identification in Code-Switched Data (Solorio et al., 2014) at the Workshop on Computational Approaches to Code Switching, EMNLP, 2014. The task is to make word-level predictions (six labels: lang1, lang2, ne, mixed, ambiguous and other) for mixedlanguage user generated content. We submit predictions for Nepali-English and Spanish-English data and perform experiments using dictionaries, a k-nearest neighbour (k-NN) classifier and a linearkernel SVM classifier. In our dictionary-based approach, we investigate the use of different English dictionaries as well as the training data. In the k-NN based approach, we use string edit distance, charactern-gram"
W14-3915,S14-2036,1,0.884374,"Missing"
W15-2804,N16-1082,0,\N,Missing
W15-2804,P13-2109,0,\N,Missing
W17-4407,P14-1016,0,0.0219217,"vely influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy cues for annotators, thereby omitting profiles that would be costly to annotate. In contrast, our method only has to be repeated once a week, and includes a different set of users where sampling is not influenced by external resources. 3 N hand 1,456 1,109 1,059 1,091 1,045 F .806 .873 .882 .887 .885 F+R .806 .887 .896 .891 .900 Table 1: Several f"
W17-4407,D11-1120,0,0.296945,"iling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy cues for annotators, thereby omitting profiles that would be costly to annotate. In contrast, our method only has to be repeated once a week, and includes a different set of users where sampling is not influenced by external resources. 3 N hand 1,456 1,109 1,059 1,091 1,045 F .806 .873 .882 .887 .885 F+R .806 .887 .896 .891 .9"
W17-4407,W11-1515,0,0.0413131,"Missing"
W17-4407,D13-1114,0,0.0215634,"to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators"
W17-4407,W15-1201,0,0.0177231,"ng, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy cues for annotators, thereby omitting profiles that would be costly to annotate. In contrast, our method only has to be repeated once a week, and includes a different set of users where sampling is not influenced by external resources. 3 N hand 1,456 1,109 1,059 1,091 1,045 F .806 .873 .882 .8"
W17-4407,W15-2913,0,0.64479,""" + : Related Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and men"
W17-4407,P11-1137,0,0.160197,"ion for Computational Linguistics 2 filter none rt rt + "" rt + : rt + "" + : Related Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textu"
W17-4407,P16-1080,0,0.207354,"ffective at this task using predictive models trained on manual annotations, the process of hand-labelling profiles is costly. Even for the ostensibly straight-forward task of annotating gender, a large portion of Twitter users purposefully avoids providing simple indicators such as real names or profile photos including a face. Consequently, this forces annotators to either dive deep into the user’s timeline in search for linguistic cues, or to make decisions based on some personal interpretation, for which they have shown to often incorrectly apply stereotypical biases (Nguyen et al., 2014; Flekova et al., 2016). We show that running a small collection of adhoc queries for self-reports of gender once (“I’m a male, female, man, woman” etc.) — provides distant labels for 6,610 profiles with high confidence in one week worth of data. Employing these for distant supervision, we demonstrate them to be an accurate signal for gender classification, and form a reliable, cheap method that has competitive performance with models trained on costly humanlabelled profiles. Our contributions are as follows: The majority of research on extracting missing user attributes from social media profiles use costly hand-an"
W17-4407,P11-1077,0,0.0467139,"reproduce the experiments is made available open-source at https://github.com/ cmry/simple-queries. 50 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 50–55 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics 2 filter none rt rt + "" rt + : rt + "" + : Related Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or"
W17-4407,D15-1162,0,0.0355146,"Missing"
W17-4407,D14-1121,0,0.422442,"ess than 200 tweets were excluded, as well as any consecutive tweets that would not exactly fit into a batch of 200. The corpora were divided between a (gender 4 https://github.com/Mimino666/ langdetect 5 https://github.com/facebookresearch/ fastText 52 Test Volkova Plank Query Average Majority .556 .659 .674 .630 Lexicon .796 .740 .668 .735 Volkova .822 (0.001) .741 (0.005) .730 (0.007) .764 Train Plank .701 (0.007) .723 (0.003) .689 (0.005) .704 Query .771 (0.007) .724 (0.009) .756 (0.002) .750 Table 4: Individual accuracy scores and averages for majority baseline (Majority), the lexicon of Sap et al. (2014), and the three models (trained on Volkova, Plank, and our dataset respectively) evaluated on the test set for each corpus. Standard deviation is reported after repeating the same experiment 20 times. for parallelising Stochastic Gradient Descent, randomness in the vector representations cannot be controlled using a seed. To estimate the standard deviation in the results, we ran each experiment 20 times. To evaluate how our distantly supervised model compares to using manual annotations, we trained all models in this same configuration for all three corpora. Each model was then evaluated on th"
W17-4407,P15-1073,0,0.325516,"ted Work Author profiling applies machine learning to linguistic features within a piece of writing to make inferences regarding its author. The ability to make such inferences was first discussed for gender by Koppel et al. (2002), and initially applied to blogs (Argamon et al., 2007; Rosenthal and McKeown, 2011; Nguyen et al., 2011). Later, the work extended to social media — encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and men"
W17-4407,L16-1258,1,0.842556,"encompassing a wide variety of attributes such as gender, age, personality, location, education, income, religion, and political polarity (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Apart from relevancy in marketing, security and forensics, author profiling has shown to positively influence several text classification tasks (Hovy, 2015). Gender profiling research on Twitter generally takes a data-driven, open-vocabulary approach using bag of words, or bag of n-gram features (Alowibdi et al., 2013; Ciot et al., 2013; Verhoeven et al., 2016), applying supervised classification using manually annotated profiles. However, distant supervision has as of yet only looked at non-textual cues for this task, unlike for example age, personality, and mental health (e.g. Al Zamal et al., 2012; Plank and Hovy, 2015; Coppersmith et al., 2015). For gender, Burger et al. (2011) and Li et al. (2014) collect links to external profiles, whereas Al Zamal et al. (2012) and Li et al. (2015) use a list with gender-associated names. Both of these approaches rely on continuous monitoring of streaming data, and utilize indicators that are typically easy c"
W17-4407,P14-1018,0,0.258116,"Missing"
W19-0117,P17-1047,0,0.0131849,"e units arise as a by-product in end-to-end tasks such as speech-tospeech translation. In the current work, the aim is to directly extract semantic, rather than word form information from speech. Semantic information encoded in speech is used in studies that ground speech to the visual context. Datasets of images paired with spoken captions can be used to train multimodal models that extract visually salient semantic information from speech, without access to textual information (Harwath and Glass, 2015; Harwath et al., 2016; Kamper et al., 2017b; Chrupała et al., 2017; Alishahi et al., 2017; Harwath and Glass, 2017). This form of semantic supervision, through contextual information from another modality, has its limits: it can only help to learn to understand speech describing the here and now. On the other hand, the success of word embeddings derived by distributional semantic principles has shown how rich the semantic information within the structure of language itself is. Semantic representations of words obtained through Latent Semantic Analysis have proven to closely resemble human semantic knowledge (Blei et al., 2003; Landauer et al., 1998). Word2vec models produce semantically rich word embedding"
W19-0117,D14-1181,0,0.00695339,"Missing"
W19-0117,D13-1170,0,0.00349047,"ributional semantic representations on unrestricted natural spoken language. 1 Introduction In the realm of NLP for written language, unsupervised approaches to inducing semantic representations of words have a long pedigree and a history of substantial success (Landauer et al., 1998; Blei et al., 2003; Mikolov et al., 2013b). The core idea behind these models is to build word representations that can predict their surrounding context. In search for similarly generic and versatile representations of whole sentences, various composition operators have been applied on word representations (e.g. Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Zhao et al., 2015). Alternatively, sentence representations are induced via Lieke Gelderloos Tilburg University l.j.gelderloos@uvt.nl Afra Alishahi Tilburg University a.alishahi@uvt.nl the objective to predict the surrounding sentences (e.g. Le and Mikolov, 2014; Kiros et al., 2015; Arora et al., 2016; Jernite et al., 2017; Logeswaran and Lee, 2018). Such representations capture aspects of the meaning of the encoded sentences, which can be used in a variety of tasks such as semantic entailment or text understanding. In the case of spoken language, unsupe"
