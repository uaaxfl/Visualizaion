2020.emnlp-demos.2,N13-1037,0,0.0467183,"Missing"
2020.emnlp-demos.2,P11-2008,0,0.274639,"Missing"
2020.emnlp-demos.2,W17-4419,0,0.0290252,"Missing"
2020.emnlp-demos.2,D18-1275,0,0.171469,"Missing"
2020.emnlp-demos.2,D17-1256,0,0.0812864,"al., 2011), ARK-Twitter4 (Gimpel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 25 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018). For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al. (2017).6 ARK-Twitter contains two • We first download the general Twitter Stream grabbed by the Archive Team,1 containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification component of fastText (Joulin et al., 2017). We tokenize those English Tweets using “TweetTokenizer” from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token).2 We also normalize the Tweets by converting user mentions and web/url links into special to"
2020.emnlp-demos.2,S17-2126,0,0.103393,"Missing"
2020.emnlp-demos.2,2020.acl-main.740,0,0.080544,"Missing"
2020.emnlp-demos.2,D19-1371,0,0.0330665,"e research and applications on Tweet data. Our BERTweet is available at: https://github.com/ VinAIResearch/BERTweet. 1 Introduction The language model BERT (Devlin et al., 2019)— the Bidirectional Encoder Representations from Transformers (Vaswani et al., 2017)—and its variants have successfully helped produce new stateof-the-art performance results for various NLP tasks. Their success has largely covered the common English domains such as Wikipedia, news and books. For specific domains such as biomedical or scientific, we could retrain a domainspecific model using the BERTology architecture (Beltagy et al., 2019; Lee et al., 2019; Gururangan et al., 2020). Twitter has been one of the most popular microblogging platforms where users can share realtime information related to all kinds of topics and events. The enormous and plentiful Tweet data has been proven to be a widely-used and real-time source of information in various important analytic tasks (Ghani et al., 2019). Note that the characteristics of Tweets are generally different from • We present the first large-scale pre-trained language model for English Tweets. • Our model does better than its competitors RoBERTabase and XLM-Rbase and outperfor"
2020.emnlp-demos.2,D12-1039,0,0.0237061,"e characteristics of Tweets are generally different from • We present the first large-scale pre-trained language model for English Tweets. • Our model does better than its competitors RoBERTabase and XLM-Rbase and outperforms previous SOTA models on three downstream Tweet NLP tasks of POS tagging, NER and text classification, thus confirming the effectiveness of the large-scale and domain-specific language model pre-trained for English Tweets. • We also provide the first set of experiments investigating whether a commonly used approach of applying lexical normalization dictionaries on Tweets (Han et al., 2012) would help im∗ Most of the work done when Thanh Vu was at the Australian e-Health Research Centre, CSIRO, Australia. † Work done during internship at VinAI Research. 9 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 9–14 c November 16-20, 2020. 2020 Association for Computational Linguistics • We also stream Tweets related to the COVID-19 pandemic, available from 01/2020 to 03/2020.3 We apply the same data pre-process step as described above, thus resulting in the second corpus of 5M English Tweets. prove the performance of the pre-trained language models on the downstream tasks."
2020.emnlp-demos.2,S17-2094,0,0.0533307,"Missing"
2020.emnlp-demos.2,E17-2068,0,0.0666588,"). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018). For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al. (2017).6 ARK-Twitter contains two • We first download the general Twitter Stream grabbed by the Archive Team,1 containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification component of fastText (Joulin et al., 2017). We tokenize those English Tweets using “TweetTokenizer” from the NLTK toolkit (Bird et al., 2009) and use the emoji package to translate emotion icons into text strings (here, each icon is referred to as a word token).2 We also normalize the Tweets by converting user mentions and web/url links into special tokens @USER and HTTPURL, respectively. We filter out retweeted Tweets and the ones shorter than 10 or longer than 64 word tokens. This pre-process results in the first corpus of 845M English Tweets. 3 We collect Tweets containing at least one of 11 COVID19 related keywords, e.g. covid19,"
2020.emnlp-demos.2,P16-1162,0,0.0110828,"utational Linguistics • We also stream Tweets related to the COVID-19 pandemic, available from 01/2020 to 03/2020.3 We apply the same data pre-process step as described above, thus resulting in the second corpus of 5M English Tweets. prove the performance of the pre-trained language models on the downstream tasks. • We publicly release our model under the name BERTweet which can be used with fairseq (Ott et al., 2019) and transformers (Wolf et al., 2019). We hope that BERTweet can serve as a strong baseline for future research and applications of Tweet analytic tasks. 2 We then apply fastBPE (Sennrich et al., 2016) to segment all 850M Tweets with subword units, using a vocabulary of 64K subword types. On average there are 25 subword tokens per Tweet. BERTweet In this section, we outline the architecture, and describe the pre-training data and optimization setup that we use for BERTweet. Optimization We utilize the RoBERTa implementation in the fairseq library (Ott et al., 2019). We set a maximum sequence length at 128, thus generating 850M × 25 / 128 ≈ 166M sequence blocks. Following Liu et al. (2019), we optimize the model using Adam (Kingma and Ba, 2014), and use a batch size of 7K across 8 V100 GPUs"
2020.emnlp-demos.2,silveira-etal-2014-gold,0,0.0668403,"Missing"
2020.emnlp-demos.2,W16-3920,0,0.0588149,"Missing"
2020.emnlp-demos.2,W16-3919,0,0.0953362,"Missing"
2020.emnlp-demos.2,P12-1109,0,0.052148,"Missing"
2020.emnlp-demos.2,S18-1005,0,0.0512273,"Missing"
2020.emnlp-demos.2,N18-1088,0,0.118325,"B pre-training dataset of uncompressed texts, containing 850M Tweets (16B word tokens). Here, each Tweet consists of at least 10 and at most 64 word tokens. In particular, this dataset is a concatenation of two corpora: Experimental setup We evaluate and compare the performance of BERTweet with strong baselines on three downstream NLP tasks of POS tagging, NER and text classification, using benchmark Tweet datasets. Downstream task datasets For POS tagging, we use three datasets Ritter11T-POS (Ritter et al., 2011), ARK-Twitter4 (Gimpel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 25 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018). For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al. (2017).6 ARK-Twitter contains two • We first download the general Twitter Stream grabbed by the A"
2020.emnlp-demos.2,2021.ccl-1.108,0,0.329294,"Missing"
2020.emnlp-demos.2,P16-1101,0,0.0472463,"+b] 93.2 [+b] 94.6 [+c] 92.5 [+c] Table 1: POS tagging accuracy results on the Ritter11-T-POS (Ritter11), ARK-Twitter (ARK) and T WEEBANK - V 2 (TB-v2) test sets. Result of ARKtagger (Owoputi et al., 2013) on Ritter11 is reported in the TPANN paper (Gui et al., 2017). Note that Ritter11 uses Twitter-specific POS tags for retweeted (RT), user-account, hashtag and url word tokens which can be tagged perfectly using some simple regular expressions. Therefore, we follow Gui et al. (2017) and Gui et al. (2018) to tag those words appropriately for all models. Results of ARKtagger and BiLSTM-CNNCRF (Ma and Hovy, 2016) on TB-v2 are reported by Liu et al. (2018). Also note that “+a”, “+b” and “+c” denote the additional use of extra training data, i.e. models trained on bigger training data. “+a”: additional use of the POS annotated data from the English WSJ Penn treebank sections 00-24 (Marcus et al., 1993). “+b”: the use of both training and validation sets for learning models. “+c”: additional use of the POS annotated data from the UD English-EWT training set (Silveira et al., 2014). Fine-tuning Following Devlin et al. (2019), for POS tagging and NER, we append a linear prediction layer on top of the last"
2020.emnlp-demos.2,S18-1006,0,0.0496305,"Missing"
2020.emnlp-demos.2,J93-2004,0,0.0743238,"s Twitter-specific POS tags for retweeted (RT), user-account, hashtag and url word tokens which can be tagged perfectly using some simple regular expressions. Therefore, we follow Gui et al. (2017) and Gui et al. (2018) to tag those words appropriately for all models. Results of ARKtagger and BiLSTM-CNNCRF (Ma and Hovy, 2016) on TB-v2 are reported by Liu et al. (2018). Also note that “+a”, “+b” and “+c” denote the additional use of extra training data, i.e. models trained on bigger training data. “+a”: additional use of the POS annotated data from the English WSJ Penn treebank sections 00-24 (Marcus et al., 1993). “+b”: the use of both training and validation sets for learning models. “+c”: additional use of the POS annotated data from the UD English-EWT training set (Silveira et al., 2014). Fine-tuning Following Devlin et al. (2019), for POS tagging and NER, we append a linear prediction layer on top of the last Transformer layer of BERTweet with regards to the first subword of each word token, while for text classification we append a linear prediction layer on top of the pooled output. We employ the transformers library (Wolf et al., 2019) to independently fine-tune BERTweet for each task and each"
2020.emnlp-demos.2,P19-1336,0,0.0420877,"Missing"
2020.emnlp-demos.2,N19-4009,0,0.0275183,"ch Centre, CSIRO, Australia. † Work done during internship at VinAI Research. 9 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 9–14 c November 16-20, 2020. 2020 Association for Computational Linguistics • We also stream Tweets related to the COVID-19 pandemic, available from 01/2020 to 03/2020.3 We apply the same data pre-process step as described above, thus resulting in the second corpus of 5M English Tweets. prove the performance of the pre-trained language models on the downstream tasks. • We publicly release our model under the name BERTweet which can be used with fairseq (Ott et al., 2019) and transformers (Wolf et al., 2019). We hope that BERTweet can serve as a strong baseline for future research and applications of Tweet analytic tasks. 2 We then apply fastBPE (Sennrich et al., 2016) to segment all 850M Tweets with subword units, using a vocabulary of 64K subword types. On average there are 25 subword tokens per Tweet. BERTweet In this section, we outline the architecture, and describe the pre-training data and optimization setup that we use for BERTweet. Optimization We utilize the RoBERTa implementation in the fairseq library (Ott et al., 2019). We set a maximum sequence l"
2020.emnlp-demos.2,N13-1039,0,0.175643,"Missing"
2020.emnlp-demos.2,D11-1141,0,0.119137,"e. See Devlin et al. (2019) and Liu et al. (2019) for more details. 3 Pre-training data We use an 80GB pre-training dataset of uncompressed texts, containing 850M Tweets (16B word tokens). Here, each Tweet consists of at least 10 and at most 64 word tokens. In particular, this dataset is a concatenation of two corpora: Experimental setup We evaluate and compare the performance of BERTweet with strong baselines on three downstream NLP tasks of POS tagging, NER and text classification, using benchmark Tweet datasets. Downstream task datasets For POS tagging, we use three datasets Ritter11T-POS (Ritter et al., 2011), ARK-Twitter4 (Gimpel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 25 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018). For Ritter11-T-POS, we employ a 70/15/15 training/validation/test pre-split available from Gui et al."
2020.emnlp-demos.2,S17-2088,0,0.111372,"Missing"
2020.emnlp-demos.2,W17-4418,0,\N,Missing
2020.emnlp-demos.2,S18-1100,0,\N,Missing
2020.emnlp-demos.2,N19-1423,0,\N,Missing
2020.findings-emnlp.364,D19-1378,0,0.0408939,"Missing"
2020.findings-emnlp.364,2020.acl-main.747,0,0.529919,"g et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020). 1 Introduction Semantic parsing is the task of converting natural language sentences into meaning representations such as logical forms or standard SQL database queries (Mooney, 2007), which serves as an important component in many NLP systems such as Question answering and Task-oriented dialogue (Androutsopoulos et al., 1995; Moldovan et al., 2003; Guo et al., 2018). The significant availability of the world’s knowledge stored in relational databases leads to the creation of large-scale Text-to-SQL datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018), which help boost"
2020.findings-emnlp.364,N19-1423,0,0.0329364,"from dependency parse trees, and inspired by Zhang et al. (2017)’s relation extraction work, we investigate whether latent syntactic features, extracted from the BiLSTM-based dependency parser jPTDP (Nguyen and Verspoor, 2018) pre-trained for Vietnamese, would help improve Vietnamese Text-to-SQL parsing. In particular, our approach is that we dump latent feature representations from jPTDP’s BiLSTM encoder given our word-level inputs, and directly use them as part of input embeddings of EditSQL and IRNet. Pre-trained language models: Zhang et al. (2019) and Guo et al. (2019) make use of BERT (Devlin et al., 2019) to improve their model performances. Thus we also extend EditSQL and IRNet with the use of pre-trained language models XLMR-base (Conneau et al., 2020) and PhoBERT-base (Nguyen and Nguyen, 2020) for the syllable- and word-level settings, respectively. XLM-R is the recent best multi-lingual model, based on RoBERTa (Liu et al., 2019), pre-trained on a 2.5TB multilingual corpus which contains 137GB of syllable-level Vietnamese texts. PhoBERT is a monolingual variant of RoBERTa for Vietnamese, pre-trained on a 20GB of word-level Vietnamese texts. 4 4.1 See Zhang et al. (2019) and Guo et al. (2019"
2020.findings-emnlp.364,L18-1410,1,0.782095,"complex questions, we will rephrase them based on the semantic meaning of the corresponding SQL queries to obtain the most natural language questions in Vietnamese. Following Yu et al. (2018) and Min et al. (2019), we split our dataset into training, development and test sets such that no database overlaps between them, as detailed in Table 1. Examples of question and SQL query pairs from our dataset are presented in Table 2. Note that translated question and SQL query pairs in our dataset are written at the syllable level. To obtain a word-level version of the dataset, we apply RDRSegmenter (Nguyen et al., 2018) from VnCoreNLP (Vu et al., 2018) to perform automatic Vietnamese word segmentation. Original (Easy question–involving one table in one database): What is the number of cars with more than 4 cylinders? SELECT count(*) FROM CARS_DATA WHERE Cylinders > 4 Translated: Cho biết số lượng những chiếc xe có nhiều hơn 4 xi lanh. SELECT count(*) FROM [dữ liệu xe] WHERE [số lượng xi lanh] > 4 Original (Hard question–with a nested SQL query): Which countries in europe have at least 3 car manufacturers? SELECT T1.CountryName FROM COUNTRIES AS T1 JOIN CONTINENTS AS T2 ON T1.Continent = T2.ContId JOIN CAR_MA"
2020.findings-emnlp.364,P19-1444,0,0.257438,"s for Vietnamese include a corpus of 5460 sentences for assigning semantic roles (Phuong et al., 2017) and a small Textto-SQL dataset of 1258 simple structured questions over 3 databases (Vuong et al., 2019). However, these two datasets are not publicly available for research community. In this paper, we introduce the first public largescale Text-to-SQL dataset for the Vietnamese semantic parsing task. In particular, we create this dataset by manually translating the Spider dataset into Vietnamese. We empirically evaluate strong seq2seq baseline parsers EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. Extending the baselines, we extensively investigate key configurations and find that: (1) Our human-translated dataset is far more reliable than a dataset consisting of machine-translated questions, and the overall result obtained for Vietnamese is comparable to that for English. (2) Automatic Vietnamese word segmentation improves the performances of the baselines. (3) The NPMI score (Bouma, 2009) is useful for linking a cell value mentioned in a question to a column in the database schema. (4) Latent syntactic features, which are dumped from a neural dependency parser pre-tra"
2020.findings-emnlp.364,K18-2008,1,0.920587,"nding the baselines, we extensively investigate key configurations and find that: (1) Our human-translated dataset is far more reliable than a dataset consisting of machine-translated questions, and the overall result obtained for Vietnamese is comparable to that for English. (2) Automatic Vietnamese word segmentation improves the performances of the baselines. (3) The NPMI score (Bouma, 2009) is useful for linking a cell value mentioned in a question to a column in the database schema. (4) Latent syntactic features, which are dumped from a neural dependency parser pre-trained for Vietnamese (Nguyen and Verspoor, 2018), also help improve the performances. (5) Highest improvements are accounted for the use of pre-trained language models, where PhoBERT (Nguyen and Nguyen, 2020) helps produce higher results than XLM-R (Conneau et al., 2020). We hope that our dataset can serve as a starting point for future Vietnamese semantic parsing research and applications. We publicly release our dataset at: https://github.com/ VinAIResearch/ViText2SQL. 2 all train dev test #SQL 5263 3493 589 1193 #DB 166 99 25 42 #T/D 5.3 5.4 4.2 5.7 #Easy 2233 1559 249 425 #Med. 3439 2255 405 779 #Hard 2095 1502 191 402 #ExH 1924 1515 10"
2020.findings-emnlp.364,D18-1265,0,0.0249876,"rammar-based LSTM decoder (Yin and Neubig, 2017). Finally, IRNet deterministically uses the synthesized SemQL query to infer the SQL query with domain knowledge. and ‘related terms’. However, these two ConceptNet categories are not available for Vietnamese. Thus we propose a novel use of the NPMI collocation score (Bouma, 2009) for the schema linking in IRNet, which ranks the NPMI scores between the cell values and column names to match a cell value to its column. Latent syntactic features: Previous works have shown that syntactic features help improve semantic parsing (Monroe and Wang, 2014; Jie and Lu, 2018). Unlike these works that use handcrafted syntactic features extracted from dependency parse trees, and inspired by Zhang et al. (2017)’s relation extraction work, we investigate whether latent syntactic features, extracted from the BiLSTM-based dependency parser jPTDP (Nguyen and Verspoor, 2018) pre-trained for Vietnamese, would help improve Vietnamese Text-to-SQL parsing. In particular, our approach is that we dump latent feature representations from jPTDP’s BiLSTM encoder given our word-level inputs, and directly use them as part of input embeddings of EditSQL and IRNet. Pre-trained languag"
2020.findings-emnlp.364,2021.ccl-1.108,0,0.123479,"Missing"
2020.findings-emnlp.364,D19-1377,0,0.0187582,"nd SQL query pair from the same database is first translated by one student and then cross-checked and corrected by the second student; and finally the NLP researcher verifies the original and corrected versions and makes further revisions if needed. Note that in case we have literal translation for a question, we stick to the style of the original English question as much as possible. Otherwise, for complex questions, we will rephrase them based on the semantic meaning of the corresponding SQL queries to obtain the most natural language questions in Vietnamese. Following Yu et al. (2018) and Min et al. (2019), we split our dataset into training, development and test sets such that no database overlaps between them, as detailed in Table 1. Examples of question and SQL query pairs from our dataset are presented in Table 2. Note that translated question and SQL query pairs in our dataset are written at the syllable level. To obtain a word-level version of the dataset, we apply RDRSegmenter (Nguyen et al., 2018) from VnCoreNLP (Vu et al., 2018) to perform automatic Vietnamese word segmentation. Original (Easy question–involving one table in one database): What is the number of cars with more than 4 cy"
2020.findings-emnlp.364,N03-1022,0,0.254391,"Missing"
2020.findings-emnlp.364,N18-5012,1,0.55855,"hem based on the semantic meaning of the corresponding SQL queries to obtain the most natural language questions in Vietnamese. Following Yu et al. (2018) and Min et al. (2019), we split our dataset into training, development and test sets such that no database overlaps between them, as detailed in Table 1. Examples of question and SQL query pairs from our dataset are presented in Table 2. Note that translated question and SQL query pairs in our dataset are written at the syllable level. To obtain a word-level version of the dataset, we apply RDRSegmenter (Nguyen et al., 2018) from VnCoreNLP (Vu et al., 2018) to perform automatic Vietnamese word segmentation. Original (Easy question–involving one table in one database): What is the number of cars with more than 4 cylinders? SELECT count(*) FROM CARS_DATA WHERE Cylinders > 4 Translated: Cho biết số lượng những chiếc xe có nhiều hơn 4 xi lanh. SELECT count(*) FROM [dữ liệu xe] WHERE [số lượng xi lanh] > 4 Original (Hard question–with a nested SQL query): Which countries in europe have at least 3 car manufacturers? SELECT T1.CountryName FROM COUNTRIES AS T1 JOIN CONTINENTS AS T2 ON T1.Continent = T2.ContId JOIN CAR_MAKERS AS T3 ON T1.CountryId = T3.C"
2020.findings-emnlp.364,2020.acl-main.677,0,0.0238086,"200 databases. However, only 9691 questions and their corresponding 5263 SQL queries over 166 databases, which are used for training and development, are publicly available. Thus we could only translate those available ones. The translation work is performed by 1 NLP researcher and 2 computer science students (IELTS #Qu. 9691 6831 954 1906 Table 2: Syllable-level examples. Word segmentation outputs are not shown for simplification. 4080 3 3.1 Baseline Models and Extensions Baselines Recent state-of-the-art results on the Spider dataset are reported for RYANSQL (Choi et al., 2020) and RAT-SQL (Wang et al., 2020), which are based on the seq2seq encoder-decoder architectures. However, their implementations are not published at the time of our empirical investigation.1 Thus we select seq2seq based models EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) with publicly available implementations as our baselines, which produce near state-of-the-art scores on Spider. We briefly describe the baselines EditSQL and IRNet as follows: • EditSQL is developed for a context-dependent Text-to-SQL parsing task, consisting of: (1) a BiLSTM-based question-table encoder to explicitly encode the question and tabl"
2020.findings-emnlp.364,P17-1041,0,0.0220491,"decoder with attention, taking into account the outputs of both encoders to generate a SQL query. • IRNet first performs an n-gram matching-based schema linking to identify the columns and the tables mentioned in a question. Then it takes the question, a database schema and the schema linking results as input to synthesize a tree-structured SemQL query—an intermediate representation bridging the input question and a target SQL query. This synthesizing process is performed by using a BiLSTM-based question encoder and an attention-based schema encoder together with a grammar-based LSTM decoder (Yin and Neubig, 2017). Finally, IRNet deterministically uses the synthesized SemQL query to infer the SQL query with domain knowledge. and ‘related terms’. However, these two ConceptNet categories are not available for Vietnamese. Thus we propose a novel use of the NPMI collocation score (Bouma, 2009) for the schema linking in IRNet, which ranks the NPMI scores between the cell values and column names to match a cell value to its column. Latent syntactic features: Previous works have shown that syntactic features help improve semantic parsing (Monroe and Wang, 2014; Jie and Lu, 2018). Unlike these works that use h"
2020.findings-emnlp.364,D18-1425,0,0.0981937,"0+). Every question and SQL query pair from the same database is first translated by one student and then cross-checked and corrected by the second student; and finally the NLP researcher verifies the original and corrected versions and makes further revisions if needed. Note that in case we have literal translation for a question, we stick to the style of the original English question as much as possible. Otherwise, for complex questions, we will rephrase them based on the semantic meaning of the corresponding SQL queries to obtain the most natural language questions in Vietnamese. Following Yu et al. (2018) and Min et al. (2019), we split our dataset into training, development and test sets such that no database overlaps between them, as detailed in Table 1. Examples of question and SQL query pairs from our dataset are presented in Table 2. Note that translated question and SQL query pairs in our dataset are written at the syllable level. To obtain a word-level version of the dataset, we apply RDRSegmenter (Nguyen et al., 2018) from VnCoreNLP (Vu et al., 2018) to perform automatic Vietnamese word segmentation. Original (Easy question–involving one table in one database): What is the number of ca"
2020.findings-emnlp.364,2020.findings-emnlp.92,1,0.767447,"Missing"
2020.findings-emnlp.364,D17-1182,0,0.050154,"Missing"
2020.findings-emnlp.92,2020.acl-main.747,0,0.0926445,"hn@nvidia.com Abstract models except XLM-R. It is worth noting that Wikipedia data is not representative of a general language use, and the Vietnamese Wikipedia data is relatively small (1GB in size uncompressed), while pre-trained language models can be significantly improved by using more pretraining data (Liu et al., 2019). We present PhoBERT with two versions— PhoBERTbase and PhoBERTlarge —the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at: https://github. com/VinAIResearch/PhoBERT. 1 Introduction Pre-trained language models, especially BERT (Devlin et al., 2019)—the Bidirectional Encoder Representations from Transformers (Vaswani et al., 2017), have recently become extremely popular and helped to produce signific"
2020.findings-emnlp.92,D18-1269,0,0.0381832,"g and Dependency parsing test sets. “Acc.”, “LAS” and “UAS” abbreviate the Accuracy, the Labeled Attachment Score and the Unlabeled Attachment Score, respectively (here, all these evaluation metrics are computed on all word tokens, including punctuation). [♣] and [F] denote results reported by Nguyen et al. (2017) and Nguyen (2019), respectively. treebank v1.1 (Nguyen et al., 2014b) with POS tags predicted by VnCoreNLP and the VLSP 2016 NER dataset (Nguyen et al., 2019a). For NLI, we use the manually-constructed Vietnamese validation and test sets from the crosslingual NLI (XNLI) corpus v1.0 (Conneau et al., 2018) where the Vietnamese training set is released as a machine-translated version of the corresponding English training set (Williams et al., 2018). Unlike the POS tagging, Dependency parsing and NER datasets which provide the gold word segmentation, for NLI, we employ RDRSegmenter to segment the text into words before applying BPE to produce subwords from word tokens. Fine-tuning Following Devlin et al. (2019), for POS tagging and NER, we append a linear prediction layer on top of the PhoBERT architecture (i.e. to the last Transformer layer of PhoBERT) w.r.t. the first subword of each word token"
2020.findings-emnlp.92,N19-1423,0,0.234364,"models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at: https://github. com/VinAIResearch/PhoBERT. 1 Introduction Pre-trained language models, especially BERT (Devlin et al., 2019)—the Bidirectional Encoder Representations from Transformers (Vaswani et al., 2017), have recently become extremely popular and helped to produce significant improvement gains for various NLP tasks. The success of pre-trained BERT and its variants has largely been limited to the English language. For other languages, one could retrain a language-specific model using the BERT architecture (Cui et al., 2019; de Vries et al., 2019; Vu et al., 2019; Martin et al., 2020) or employ existing pre-trained multilingual BERT-based models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 202"
2020.findings-emnlp.92,N19-1419,0,0.0703568,"Missing"
2020.findings-emnlp.92,P19-1356,0,0.0555433,"Missing"
2020.findings-emnlp.92,D18-2012,0,0.0305208,": • All publicly released monolingual and multilingual BERT-based language models are not aware of the difference between Vietnamese syllables and word tokens. This ambiguity comes from the fact that the white space is also used to separate syllables that constitute words when written in Vietnamese.1 For example, a 6-syllable written text “Tôi là một nghiên cứu viên” (I am a researcher) forms 4 words “TôiI làam mộta nghiên_cứu_viênresearcher ”. Without doing a pre-process step of Vietnamese word segmentation, those models directly apply Byte-Pair encoding (BPE) methods (Sennrich et al., 2016; Kudo and Richardson, 2018) to the syllable-level Vietnamese pre-training data.2 Intuitively, for word-level Vietnamese NLP tasks, those models pre-trained on syllable-level data might not perform as good as language models pre-trained on word-level data. To handle the two concerns above, we train the first large-scale monolingual BERT-based “base” and “large” models using a 20GB word-level Vietnamese corpus. We evaluate our models on four downstream Vietnamese NLP tasks: the common word-level ones of Part-of-speech (POS) tagging, Dependency parsing and Named-entity recogni1 • The Vietnamese Wikipedia corpus is the only"
2020.findings-emnlp.92,2021.ccl-1.108,0,0.237321,"Missing"
2020.findings-emnlp.92,P16-1101,0,0.111606,"Missing"
2020.findings-emnlp.92,P18-1130,0,0.0336249,"gging, Dependency parsing and NER datasets which provide the gold word segmentation, for NLI, we employ RDRSegmenter to segment the text into words before applying BPE to produce subwords from word tokens. Fine-tuning Following Devlin et al. (2019), for POS tagging and NER, we append a linear prediction layer on top of the PhoBERT architecture (i.e. to the last Transformer layer of PhoBERT) w.r.t. the first subword of each word token.5 For dependency parsing, following Nguyen (2019), we employ a reimplementation of the state-of-the-art Biaffine dependency parser (Dozat and Manning, 2017) from Ma et al. (2018) with default optimal hyperparameters. We then extend this parser by replacing the pre-trained word embedding of each word in an input sentence by the corresponding contextualized embedding (from the last layer) computed for the first subword token of the word. For POS tagging, NER and NLI, we employ transformers (Wolf et al., 2019) to fine-tune PhoBERT for each task and each dataset independently. We use AdamW (Loshchilov and Hutter, 5 In our preliminary experiments, using the average of contextualized embeddings of subword tokens of each word to represent the word produces slightly lower per"
2020.findings-emnlp.92,U19-1004,1,0.869438,"released as a machine-translated version of the corresponding English training set (Williams et al., 2018). Unlike the POS tagging, Dependency parsing and NER datasets which provide the gold word segmentation, for NLI, we employ RDRSegmenter to segment the text into words before applying BPE to produce subwords from word tokens. Fine-tuning Following Devlin et al. (2019), for POS tagging and NER, we append a linear prediction layer on top of the PhoBERT architecture (i.e. to the last Transformer layer of PhoBERT) w.r.t. the first subword of each word token.5 For dependency parsing, following Nguyen (2019), we employ a reimplementation of the state-of-the-art Biaffine dependency parser (Dozat and Manning, 2017) from Ma et al. (2018) with default optimal hyperparameters. We then extend this parser by replacing the pre-trained word embedding of each word in an input sentence by the corresponding contextualized embedding (from the last layer) computed for the first subword token of the word. For POS tagging, NER and NLI, we employ transformers (Wolf et al., 2019) to fine-tune PhoBERT for each task and each dataset independently. We use AdamW (Loshchilov and Hutter, 5 In our preliminary experiments"
2020.findings-emnlp.92,E14-2005,1,0.776068,"during 3 weeks, and then PhoBERTlarge during 5 weeks. 3 Experimental setup We evaluate the performance of PhoBERT on four downstream Vietnamese NLP tasks: POS tagging, Dependency parsing, NER and NLI. Downstream task datasets Table 1 presents the statistics of the experimental datasets that we employ for downstream task evaluation. For POS tagging, Dependency parsing and NER, we follow the VnCoreNLP setup (Vu et al., 2018), using standard benchmarks of the VLSP 2013 POS tagging dataset,4 the VnDT dependency 1038 4 https://vlsp.org.vn/vlsp2013/eval POS tagging (word-level) Model RDRPOSTagger (Nguyen et al., 2014a) [♣] BiLSTM-CNN-CRF (Ma and Hovy, 2016) [♣] VnCoreNLP-POS (Nguyen et al., 2017) [♣] jPTDP-v2 (Nguyen and Verspoor, 2018) [F] jointWPD (Nguyen, 2019) [F] XLM-Rbase (our result) XLM-Rlarge (our result) PhoBERTbase PhoBERTlarge Acc. 95.1 95.4 95.9 95.7 96.0 96.2 96.3 96.7 96.8 Dependency parsing (word-level) Model LAS / UAS _ _ VnCoreNLP-DEP (Vu et al., 2018) [F] 71.38 / 77.35 jPTDP-v2 [F] 73.12 / 79.63 jointWPD [F] 73.90 / 80.12 Biaffine (Dozat and Manning, 2017) [F] 74.99 / 81.19 Biaffine w/ XLM-Rbase (our result) 76.46 / 83.10 Biaffine w/ XLM-Rlarge (our result) 75.87 / 82.70 Biaffine w/ Pho"
2020.findings-emnlp.92,L18-1410,1,0.886228,"cles and duplication from a 50GB Vietnamese news corpus.3 To 3 https://github.com/binhvq/news-corpus, crawled from a wide range of news websites and topics. Task POS tagging† Dep. parsing† NER† NLI‡ #training 27,000 8,977 14,861 392,702 #valid 870 200 2,000 2,490 #test 2,120 1,020 2,831 5,010 Table 1: Statistics of the downstream task datasets. “#training”, “#valid” and “#test” denote the size of the training, validation and test sets, respectively. † and ‡ refer to the dataset size as the numbers of sentences and sentence pairs, respectively. solve the second concern, we employ RDRSegmenter (Nguyen et al., 2018) from VnCoreNLP (Vu et al., 2018) to perform word and sentence segmentation on the pre-training dataset, resulting in ∼145M word-segmented sentences (∼3B word tokens). Different from RoBERTa, we then apply fastBPE (Sennrich et al., 2016) to segment these sentences with subword units, using a vocabulary of 64K subword types. On average there are 24.4 subword tokens per sentence. Optimization: We employ the RoBERTa implementation in fairseq (Ott et al., 2019). We set a maximum length at 256 subword tokens, thus generating 145M × 24.4 / 256 ≈ 13.8M sentence blocks. Following Liu et al. (2019), we"
2020.findings-emnlp.92,K18-2008,1,0.816751,"Missing"
2020.findings-emnlp.92,U17-1013,1,0.920885,"Missing"
2020.findings-emnlp.92,N19-4009,0,0.149849,"R and NLI, thus showing the effectiveness of large-scale BERT-based monolingual language models for Vietnamese. • To the best of our knowledge, we also perform the first set of experiments to compare monolingual language models with the recent best multilingual model XLM-R in multiple (i.e. four) different language-specific tasks. The experiments show that our models outperform XLM-R on all these tasks, thus convincingly confirming that dedicated language-specific models still outperform multilingual ones. • We publicly release our models under the name PhoBERT which can be used with fairseq (Ott et al., 2019) and transformers (Wolf et al., 2019). We hope that PhoBERT can serve as a strong baseline for future Vietnamese NLP research and applications. 2 PhoBERT This section outlines the architecture and describes the pre-training data and optimization setup that we use for PhoBERT. Architecture: Our PhoBERT has two versions, PhoBERTbase and PhoBERTlarge , using the same architectures of BERTbase and BERTlarge , respectively. PhoBERT pre-training approach is based on RoBERTa (Liu et al., 2019) which optimizes the BERT pre-training procedure for more robust performance. Pre-training data: To handle th"
2020.findings-emnlp.92,P16-1162,0,0.282579,"ain concerns as follows: • All publicly released monolingual and multilingual BERT-based language models are not aware of the difference between Vietnamese syllables and word tokens. This ambiguity comes from the fact that the white space is also used to separate syllables that constitute words when written in Vietnamese.1 For example, a 6-syllable written text “Tôi là một nghiên cứu viên” (I am a researcher) forms 4 words “TôiI làam mộta nghiên_cứu_viênresearcher ”. Without doing a pre-process step of Vietnamese word segmentation, those models directly apply Byte-Pair encoding (BPE) methods (Sennrich et al., 2016; Kudo and Richardson, 2018) to the syllable-level Vietnamese pre-training data.2 Intuitively, for word-level Vietnamese NLP tasks, those models pre-trained on syllable-level data might not perform as good as language models pre-trained on word-level data. To handle the two concerns above, we train the first large-scale monolingual BERT-based “base” and “large” models using a 20GB word-level Vietnamese corpus. We evaluate our models on four downstream Vietnamese NLP tasks: the common word-level ones of Part-of-speech (POS) tagging, Dependency parsing and Named-entity recogni1 • The Vietnamese"
2020.findings-emnlp.92,N18-5012,1,0.704838,"namese news corpus.3 To 3 https://github.com/binhvq/news-corpus, crawled from a wide range of news websites and topics. Task POS tagging† Dep. parsing† NER† NLI‡ #training 27,000 8,977 14,861 392,702 #valid 870 200 2,000 2,490 #test 2,120 1,020 2,831 5,010 Table 1: Statistics of the downstream task datasets. “#training”, “#valid” and “#test” denote the size of the training, validation and test sets, respectively. † and ‡ refer to the dataset size as the numbers of sentences and sentence pairs, respectively. solve the second concern, we employ RDRSegmenter (Nguyen et al., 2018) from VnCoreNLP (Vu et al., 2018) to perform word and sentence segmentation on the pre-training dataset, resulting in ∼145M word-segmented sentences (∼3B word tokens). Different from RoBERTa, we then apply fastBPE (Sennrich et al., 2016) to segment these sentences with subword units, using a vocabulary of 64K subword types. On average there are 24.4 subword tokens per sentence. Optimization: We employ the RoBERTa implementation in fairseq (Ott et al., 2019). We set a maximum length at 256 subword tokens, thus generating 145M × 24.4 / 256 ≈ 13.8M sentence blocks. Following Liu et al. (2019), we optimize the models using Adam ("
2020.findings-emnlp.92,R19-1147,0,0.195237,"NLP. Our PhoBERT models are available at: https://github. com/VinAIResearch/PhoBERT. 1 Introduction Pre-trained language models, especially BERT (Devlin et al., 2019)—the Bidirectional Encoder Representations from Transformers (Vaswani et al., 2017), have recently become extremely popular and helped to produce significant improvement gains for various NLP tasks. The success of pre-trained BERT and its variants has largely been limited to the English language. For other languages, one could retrain a language-specific model using the BERT architecture (Cui et al., 2019; de Vries et al., 2019; Vu et al., 2019; Martin et al., 2020) or employ existing pre-trained multilingual BERT-based models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). In terms of Vietnamese language modeling, to the best of our knowledge, there are two main concerns as follows: • All publicly released monolingual and multilingual BERT-based language models are not aware of the difference between Vietnamese syllables and word tokens. This ambiguity comes from the fact that the white space is also used to separate syllables that constitute words when written in Vietnamese.1 For example, a 6-syllable writte"
2020.findings-emnlp.92,N18-1101,0,0.0443128,"Score, respectively (here, all these evaluation metrics are computed on all word tokens, including punctuation). [♣] and [F] denote results reported by Nguyen et al. (2017) and Nguyen (2019), respectively. treebank v1.1 (Nguyen et al., 2014b) with POS tags predicted by VnCoreNLP and the VLSP 2016 NER dataset (Nguyen et al., 2019a). For NLI, we use the manually-constructed Vietnamese validation and test sets from the crosslingual NLI (XNLI) corpus v1.0 (Conneau et al., 2018) where the Vietnamese training set is released as a machine-translated version of the corresponding English training set (Williams et al., 2018). Unlike the POS tagging, Dependency parsing and NER datasets which provide the gold word segmentation, for NLI, we employ RDRSegmenter to segment the text into words before applying BPE to produce subwords from word tokens. Fine-tuning Following Devlin et al. (2019), for POS tagging and NER, we append a linear prediction layer on top of the PhoBERT architecture (i.e. to the last Transformer layer of PhoBERT) w.r.t. the first subword of each word token.5 For dependency parsing, following Nguyen (2019), we employ a reimplementation of the state-of-the-art Biaffine dependency parser (Dozat and M"
2020.findings-emnlp.92,D19-1077,0,0.0393879,"Missing"
2020.findings-emnlp.92,dinh-etal-2008-word,0,\N,Missing
2020.findings-emnlp.92,D19-1098,0,\N,Missing
2020.findings-emnlp.92,Q19-1038,0,\N,Missing
2020.textgraphs-1.1,D19-1522,0,0.313857,"ntly. Such quadratic forms are also used to model entities and relations in KG2E (He et al., 2015), TATEC (Garc´ıa-Dur´an et al., 2016), TransG (Xiao et al., 2016), RSTE (Tay et al., 2017), ANALOGY (Liu et al., 2017) and Dihedral (Xu and Li, 2019). SME-bilinear (Bordes et al., 2012) is proposed to first separately combine entity-relation pairs (h, r) and (r, t) and then semantically match these combinations, using tensor product. HolE (Nickel et al., 2016b) uses circular correlation– a compositional operator–which can be interpreted as a compression of the tensor product. In addition, TuckER (Balazevic et al., 2019) is a linear model based on the Tucker tensor decomposition of the binary tensor representation of KG triples. Neural network-based models: The neural tensor network (NTN) model (Socher et al., 2013) also uses a bilinear tensor operator to represent each relation while ProjE (Shi and Weninger, 2017) can be viewed as simplified versions of NTN. The ER-MLP model (Dong et al., 2014) represents each triple by a vector obtained from concatenating head, relation and tail embeddings, then feeds this vector into a single-layer MLP with one-node output layer. ConvE (Dettmers et al., 2018) and ConvKB (N"
2020.textgraphs-1.1,D13-1160,0,0.0275581,"Missing"
2020.textgraphs-1.1,N18-1133,0,0.0197822,"or from the set of relation-dependent entities. The “Bernoulli” trick (Wang et al., 2014) is widely used to set different probabilities for generating head or tail entities: For each relation type r, we calculate the averaged number ar,1 of heads h for a pair (r, t) and the averaged number ar,2 of tails t for a pair (h, r). We then define a Bernoulli distribution with success probability ar,1 λr = for sampling: given a correct triple (h, r, t), we corrupt this triple by replacing head ar,1 + ar,2 entity with probability λr while replacing the tail entity with probability (1 − λr ). Recently, Cai and Wang (2018) and Sun et al. (2019) proposed adversarial learning-based strategies for sampling incorrect triples. However, they did not provide a comparison between the adversarial learning-based strategies and the “Bernoulli” trick. 3 Specific Models 3.1 Triple-based Embedding Models Translation-based models: The Unstructured model (Bordes et al., 2012) assumes that the head and tail entity vectors are similar. As the Unstructured model does not take the relationship into account, it cannot distinguish different relation types. The Structured Embedding (SE) model (Bordes et al., 2011) assumes that the he"
2020.textgraphs-1.1,N18-1165,0,0.0172977,"tween two entities as well. For example, in the KG NELL (Carlson et al., 2010), we have information such as if a person works for an organization and this person also leads that organization, then it is likely that this person is the CEO of that organization. Recent research has also shown that relation paths between entities in KGs provide richer context information and improve the performance of embedding models for KG completion (Luo et al., 2015; Liang and Forbus, 2015; Garc´ıa-Dur´an et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Dur´an and Niepert, 2018; Takahashi et al., 2018; Chen et al., 2018). In particular, Luo et al. (2015) constructed relation paths between entities and, viewing entities and relations in the path as pseudo-words, then applied Word2Vec (Mikolov et al., 2013) to produce pre-trained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of models TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). Liang and Forbus (2015) used the plausibility score produced by SME to compute the weights of relation paths. PTransE-RNN (Lin et al., 2015a) mode"
2020.textgraphs-1.1,E17-1013,0,0.0186646,"ties and, viewing entities and relations in the path as pseudo-words, then applied Word2Vec (Mikolov et al., 2013) to produce pre-trained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of models TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). Liang and Forbus (2015) used the plausibility score produced by SME to compute the weights of relation paths. PTransE-RNN (Lin et al., 2015a) models relation paths by using a recurrent neural network (RNN). In addition, Das et al. (2017)’s model and ROPs (Yin et al., 2018) also apply RNN to model the path between an entity pair, however, in contrast to PTransE-RNN, they additionally take the intermediate entities present in the path into account. IRN (Shen et al., 2017) uses a shared memory and RNN-based controller to implicitly model multi-step structured relationships. RTransE (Garc´ıa-Dur´an et al., 2015), PTransE-ADD (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) extend TransE to represent a relation path by a vector which is the sum of the vectors of all relations in the path. In Bilinear-COMP (Guu et al., 2015)"
2020.textgraphs-1.1,N19-1423,0,0.00754795,"al rules for KG reasoning, producing competitive link prediction performances. Feldman 6 Dataset FB15k (Bordes et al., 2013) WN18 (Bordes et al., 2013) FB15k-237 (Toutanova and Chen, 2015) WN18RR Dettmers et al. (2018) |E | 14,951 40,943 14,541 40,943 |R| 1,345 18 237 11 #Triples in train/valid/test 483,142 50,000 59,071 141,442 5,000 5,000 272,115 17,535 20,466 86,835 3,034 3,134 Table 2: Statistics of benchmark experimental datasets. et al. (2019) presented an approach to generate sentences from triples via hand-craft templates, and then use the likelihoods produced by the pre-trained BERT (Devlin et al., 2019) for these generated sentences to score the plausibility of the corresponding triples. See other methods for learning from KGs and multi-relational data in Nickel et al. (2016a) and Wang et al. (2017). 4 Evaluation Task The standard evaluation task of entity prediction, i.e. the link prediction task (Bordes et al., 2013), is proposed to evaluate embedding models for KG completion.3 Datasets: Information about benchmark datasets for KG completion evaluation is given in Table 2. FB15k and WN18 are derived from the large real-world KG Freebase (Bollacker et al., 2008) and the large lexical KG Wor"
2020.textgraphs-1.1,Q15-1002,0,0.049375,"Missing"
2020.textgraphs-1.1,D19-1109,0,0.0598695,"Missing"
2020.textgraphs-1.1,C16-1062,0,0.0367501,"Missing"
2020.textgraphs-1.1,D15-1034,0,0.0521813,"Missing"
2020.textgraphs-1.1,D15-1173,0,0.0148216,"apply DISTMULT, Conv-TransE and ConvKB to compute triple scores, respectively. 3.3 Other KG Completion Models The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KGs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KG. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KG used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KGs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order logic in the discriminative Gaifman model (Niepert, 2016). In addition, Neelakantan et al. (2015) used a RNN to learn vector representations of PRA-style relation paths between entities in the KG. Other random-walk based learning algorithms for KG completion can be also found in Feng et al. (2016b), Liu et al. (20"
2020.textgraphs-1.1,D14-1044,0,0.0327901,"neighboring entities through a normalized sum with different relational weights. For link prediction, R-GCN, SACN and KBGAT apply DISTMULT, Conv-TransE and ConvKB to compute triple scores, respectively. 3.3 Other KG Completion Models The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KGs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KG. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KG used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KGs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order logic in the discriminative Gaifman model (Niepert, 2016). In addition, Neelakantan et al. (2015) used a RNN to learn vector representations of PRA-style relation paths between entities"
2020.textgraphs-1.1,D16-1019,0,0.0220335,"017) applied STransE (Nguyen et al., 2016b) for search personalization to re-rank the search documents returned by a search engine for users’ submitted queries. Other application examples can be also found for recommender systems (Zhang et al., 2016; He et al., 2017; Cao et al., 2019), social relation extraction (Tu et al., 2017) and visual relation detection (Zhang et al., 2017). Future research directions might also include: (i) Combining logical rules which contain rich background information and KG triples in a unified KG completion framework, e.g. jointly embedding KGs and logical rules (Guo et al., 2016; Yang et al., 2017). (ii) Recent embedding models for KG completion hold a closed-world assumption where the KGs are fixed (i.e. new entities might not be added easily), therefore it would be worth exploring open-world KG completion models to connect unseen entities to the existing KGs (Shi and Weninger, 2018). (iii) Investigating efficient approaches which can be applied to large-scale KGs of millions of entities and relations (Zhang et al., 2020). In this paper, we have presented a comprehensive survey of embedding models of entity and relationships for knowledge graph completion. This pape"
2020.textgraphs-1.1,D15-1038,0,0.329421,"o, neighborhood information of entities could be useful for predicting the relationship 5 between two entities as well. For example, in the KG NELL (Carlson et al., 2010), we have information such as if a person works for an organization and this person also leads that organization, then it is likely that this person is the CEO of that organization. Recent research has also shown that relation paths between entities in KGs provide richer context information and improve the performance of embedding models for KG completion (Luo et al., 2015; Liang and Forbus, 2015; Garc´ıa-Dur´an et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Dur´an and Niepert, 2018; Takahashi et al., 2018; Chen et al., 2018). In particular, Luo et al. (2015) constructed relation paths between entities and, viewing entities and relations in the path as pseudo-words, then applied Word2Vec (Mikolov et al., 2013) to produce pre-trained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of models TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). Liang and Forbus (2015) used the plausibility score"
2020.textgraphs-1.1,P15-1067,0,0.134481,"esent each relation type, TransE is not well-suited for Many-to-1, 1-to-Many and Many-to-Many relationships,2 such as for relation types “born in”, “place of birth” and “research fields.” For example in Figure 2, using one vector representing the relation type “born in” cannot capture both the translating direction from “Patti” to “Miami” and its inverse direction from “Mom” to “Austin.” To overcome those issues of TransE, TransH (Wang et al., 2014) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend TransH by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. Similar to TransR, TransR-FT (Feng et al., 2016a) also uses a matrix to project head and tail entity vectors. 2 A relation type r is classified Many-to-1 if multiple head entities can be connected by r to at most one tail entity. A relation type r is classified 1-to-Many if multiple tail entities can be linked by r from at most one head entity. A relation type r is classified Many-to-Many if multiple head entities can be conn"
2020.textgraphs-1.1,D12-1069,0,0.0216539,"Missing"
2020.textgraphs-1.1,D11-1049,0,0.0340454,"ling with highly multi-relational data, e.g. KGs. For computing the final representation of an entity, they make use of layer-wise propagation to accumulate linearly-transformed embeddings of its neighboring entities through a normalized sum with different relational weights. For link prediction, R-GCN, SACN and KBGAT apply DISTMULT, Conv-TransE and ConvKB to compute triple scores, respectively. 3.3 Other KG Completion Models The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) is a random walk inference technique which was proposed to predict a new relationship between two entities in KGs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KG. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KG used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KGs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction wit"
2020.textgraphs-1.1,D15-1082,0,0.291775,"ot well-suited for Many-to-1, 1-to-Many and Many-to-Many relationships,2 such as for relation types “born in”, “place of birth” and “research fields.” For example in Figure 2, using one vector representing the relation type “born in” cannot capture both the translating direction from “Patti” to “Miami” and its inverse direction from “Mom” to “Austin.” To overcome those issues of TransE, TransH (Wang et al., 2014) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend TransH by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. Similar to TransR, TransR-FT (Feng et al., 2016a) also uses a matrix to project head and tail entity vectors. 2 A relation type r is classified Many-to-1 if multiple head entities can be connected by r to at most one tail entity. A relation type r is classified 1-to-Many if multiple tail entities can be linked by r from at most one head entity. A relation type r is classified Many-to-Many if multiple head entities can be connected by r to a tail entity and vice"
2020.textgraphs-1.1,D15-1191,0,0.010683,"indicate a relationship “nationality” between the h and t entities. Also, neighborhood information of entities could be useful for predicting the relationship 5 between two entities as well. For example, in the KG NELL (Carlson et al., 2010), we have information such as if a person works for an organization and this person also leads that organization, then it is likely that this person is the CEO of that organization. Recent research has also shown that relation paths between entities in KGs provide richer context information and improve the performance of embedding models for KG completion (Luo et al., 2015; Liang and Forbus, 2015; Garc´ıa-Dur´an et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Dur´an and Niepert, 2018; Takahashi et al., 2018; Chen et al., 2018). In particular, Luo et al. (2015) constructed relation paths between entities and, viewing entities and relations in the path as pseudo-words, then applied Word2Vec (Mikolov et al., 2013) to produce pre-trained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of models TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bor"
2020.textgraphs-1.1,P19-1466,0,0.0623915,"takes into account the neighborhood entity and relation information of both head and tail entities in each triple. ReInceptionE (Xie et al., 2020) employs the Inception network (Szegedy et al., 2016) to increase the interactions between head and relation embeddings for obtaining better representations of the head and relation pairs and then uses a relation-aware attention mechanism to enrich these pair representations with the local neighborhood and global entity information. Neighborhood information is also exploited in R-GCN (Schlichtkrull et al., 2018), SACN (Shang et al., 2019) and KBGAT (Nathani et al., 2019), which generalize graph convolutional networks (Kipf and Welling, 2017) and graph attention networks (Velikovi et al., 2018) for dealing with highly multi-relational data, e.g. KGs. For computing the final representation of an entity, they make use of layer-wise propagation to accumulate linearly-transformed embeddings of its neighboring entities through a normalized sum with different relational weights. For link prediction, R-GCN, SACN and KBGAT apply DISTMULT, Conv-TransE and ConvKB to compute triple scores, respectively. 3.3 Other KG Completion Models The Path Ranking Algorithm (PRA) (Lao"
2020.textgraphs-1.1,P15-1016,0,0.0245164,"random walks that follow different paths linking the head entity and tail entity in the KG. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KG used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KGs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order logic in the discriminative Gaifman model (Niepert, 2016). In addition, Neelakantan et al. (2015) used a RNN to learn vector representations of PRA-style relation paths between entities in the KG. Other random-walk based learning algorithms for KG completion can be also found in Feng et al. (2016b), Liu et al. (2016), Wei et al. (2016), Mazumder and Liu (2017) and Das et al. (2018). Yang et al. (2017) proposed a Neural Logic Programming (LP) framework to learning probabilistic first-order logical rules for KG reasoning, producing competitive link prediction performances. Feldman 6 Dataset FB15k (Bordes et al., 2013) WN18 (Bordes et al., 2013) FB15k-237 (Toutanova and Chen, 2015) WN18RR De"
2020.textgraphs-1.1,K16-1005,1,0.791578,"relation type r is classified Many-to-1 if multiple head entities can be connected by r to at most one tail entity. A relation type r is classified 1-to-Many if multiple tail entities can be linked by r from at most one head entity. A relation type r is classified Many-to-Many if multiple head entities can be connected by r to a tail entity and vice versa. 4 TEKE H (Wang and Li, 2016) extends TransH to incorporate rich context information in an external text corpus. lppTransD (Yoon et al., 2016) extends TransD to additionally use two projection vectors for representing each relation. STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016) can be viewed as direct extensions of TransR, where head and tail entities are associated with their own projection matrices. Unlike STransE, TranSparse uses adaptive sparse matrices, whose sparse degrees are defined based on the number of entities linked by relations. TranSparse-DT (Chang et al., 2017) is an extension of TranSparse with a dynamic translation. ITransF (Xie et al., 2017) can be considered as a generalization of STransE, which allows the sharing of statistic regularities between relation projection matrices and alleviates data sparsity issue."
2020.textgraphs-1.1,N16-1054,1,0.799366,"relation type r is classified Many-to-1 if multiple head entities can be connected by r to at most one tail entity. A relation type r is classified 1-to-Many if multiple tail entities can be linked by r from at most one head entity. A relation type r is classified Many-to-Many if multiple head entities can be connected by r to a tail entity and vice versa. 4 TEKE H (Wang and Li, 2016) extends TransH to incorporate rich context information in an external text corpus. lppTransD (Yoon et al., 2016) extends TransD to additionally use two projection vectors for representing each relation. STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016) can be viewed as direct extensions of TransR, where head and tail entities are associated with their own projection matrices. Unlike STransE, TranSparse uses adaptive sparse matrices, whose sparse degrees are defined based on the number of entities linked by relations. TranSparse-DT (Chang et al., 2017) is an extension of TranSparse with a dynamic translation. ITransF (Xie et al., 2017) can be considered as a generalization of STransE, which allows the sharing of statistic regularities between relation projection matrices and alleviates data sparsity issue."
2020.textgraphs-1.1,N18-2053,1,0.910173,") is a linear model based on the Tucker tensor decomposition of the binary tensor representation of KG triples. Neural network-based models: The neural tensor network (NTN) model (Socher et al., 2013) also uses a bilinear tensor operator to represent each relation while ProjE (Shi and Weninger, 2017) can be viewed as simplified versions of NTN. The ER-MLP model (Dong et al., 2014) represents each triple by a vector obtained from concatenating head, relation and tail embeddings, then feeds this vector into a single-layer MLP with one-node output layer. ConvE (Dettmers et al., 2018) and ConvKB (Nguyen et al., 2018) are based on convolutional neural networks. ConvE uses a convolution layer directly over 2D reshaping of head-entity and relation embeddings, while ConvKB applies a convolution layer over the embedding triples (here each triple (h, r, t) is represented as a 3-column matrix where each column vector represents a triple element). HypER (Balaˇzevi´c et al., 2019) simplifies ConvE by using a hypernetwork to produce 1D convolutional filters for each relation, then extracts relation-specific features from head entity embeddings. Conv-TransE (Shang et al., 2019) extends ConvE to keep the translationa"
2020.textgraphs-1.1,N19-1226,1,0.906858,"-column matrix where each column vector represents a triple element). HypER (Balaˇzevi´c et al., 2019) simplifies ConvE by using a hypernetwork to produce 1D convolutional filters for each relation, then extracts relation-specific features from head entity embeddings. Conv-TransE (Shang et al., 2019) extends ConvE to keep the translational characteristic between entities and relations. InteractE (Vashishth et al., 2020) uses a circular convolution operator and a checkered reshaping function instead of the standard convolution operator and 2D stack reshaping function in ConvE. The CapsE model (Nguyen et al., 2019) extends ConvKB by stacking a capsule network layer (Sabour et al., 2017) on top of the convolution layer. Complex vector-based models: Instead of embedding entities and relations in the real-valued vector space, ComplEx (Trouillon et al., 2016) is an extension of DISTMULT in the complex vector space. ComplEx-N3 (Lacroix et al., 2018) extends ComplEx with weighted nuclear 3-norm. Also in the complex vector space, RotatE (Sun et al., 2019) defines each relation as a rotation from the head entity to the tail entity. QuatE (Zhang et al., 2019) represents entities by quaternion embeddings (i.e. hy"
2020.textgraphs-1.1,D14-1162,0,0.112986,"of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions. 1 Introduction Let us revisit the classic Word2Vec example of a “royal” relationship between “king” and “man”, and between “queen” and “woman”. As illustrated in this example: v king − v man ≈ v queen − v woman , word vectors learned from a large corpus can model relational similarities or linguistic regularities between pairs of words as translations in the projected vector space (Mikolov et al., 2013; Pennington et al., 2014). Figure 1 shows another example of a relational similarity between word pairs of countries and capital cities: v Japan − v T okyo ≈ v Germany − v Berlin v Germany − v Berlin ≈ v P ortugal − v Lisbon Assume that we consider the country and capital pairs in Figure 1 to be pairs of entities rather than word types. That is, we now represent country and capital entities by low-dimensional and dense vectors. The relational similarity between word pairs is presumably to capture a “is capital of” relationship between country and capital entities. Also, we represent this relationship by a translation"
2020.textgraphs-1.1,N06-1025,0,0.0903345,"Missing"
2020.textgraphs-1.1,W17-2608,0,0.203173,"initialization helps to improve the performance of models TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). Liang and Forbus (2015) used the plausibility score produced by SME to compute the weights of relation paths. PTransE-RNN (Lin et al., 2015a) models relation paths by using a recurrent neural network (RNN). In addition, Das et al. (2017)’s model and ROPs (Yin et al., 2018) also apply RNN to model the path between an entity pair, however, in contrast to PTransE-RNN, they additionally take the intermediate entities present in the path into account. IRN (Shen et al., 2017) uses a shared memory and RNN-based controller to implicitly model multi-step structured relationships. RTransE (Garc´ıa-Dur´an et al., 2015), PTransE-ADD (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) extend TransE to represent a relation path by a vector which is the sum of the vectors of all relations in the path. In Bilinear-COMP (Guu et al., 2015) and PRUNED-PATHS (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. Dur´an and Niepert (2018) proposed the KBLRN framework to combine relational paths with latent and nume"
2020.textgraphs-1.1,P18-1200,0,0.0121896,"ng the relationship 5 between two entities as well. For example, in the KG NELL (Carlson et al., 2010), we have information such as if a person works for an organization and this person also leads that organization, then it is likely that this person is the CEO of that organization. Recent research has also shown that relation paths between entities in KGs provide richer context information and improve the performance of embedding models for KG completion (Luo et al., 2015; Liang and Forbus, 2015; Garc´ıa-Dur´an et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Dur´an and Niepert, 2018; Takahashi et al., 2018; Chen et al., 2018). In particular, Luo et al. (2015) constructed relation paths between entities and, viewing entities and relations in the path as pseudo-words, then applied Word2Vec (Mikolov et al., 2013) to produce pre-trained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of models TransE (Bordes et al., 2013), SME (Bordes et al., 2012) and SE (Bordes et al., 2011). Liang and Forbus (2015) used the plausibility score produced by SME to compute the weights of relation paths. PTransE-RNN (Lin"
2020.textgraphs-1.1,W15-4007,0,0.336802,"ction 3 summarizes different prominent score functions f (h, r, t). To learn model parameters (i.e. entity vectors, relation vectors or matrices), the embedding models minimize an objective loss L. A conventional objective loss is the margin-based pairwise ranking loss (Bordes et al., 2013): X LMargin = [γ − f (h, r, t) + f (h0 , r, t0 )]+ (h,r,t)∈G 0 (h0 ,r,t0 )∈G(h,r,t) 0 where [x]+ = max(0, x); γ is the margin hyper-parameter; and G(h,r,t) is the set of incorrect triples generated by corrupting the correct triple (h, r, t) ∈ G. Also, the negative log-likelihood (NLL) of softmax regression (Toutanova and Chen, 2015) and the NLL of logistic regression (Trouillon et al., 2016) are commonly used in recent KG completion research:1  X exp f (h, r, t)  P LSoftmax = − exp f (h, r, t0 ) (h,r,t)∈G t0 ∈ E{t} !  exp f (h, r, t)  P exp f (h0 , r, t) + h0 ∈ E{h} LLogistic = X  log 1 + exp −I(h,r,t) · f (h, r, t) (h,r,t)∈{G∪G 0 }  with: I(h,r,t) = 1 for (h, r, t) ∈ G −1 for (h, r, t) ∈ G 0 To corrupt the head or tail entities, a common strategy is to uniformly replace the entities when sampling incorrect triples (Bordes et al., 2013), however it results in many false negative labels (Wang et al., 2014). Domai"
2020.textgraphs-1.1,P16-1124,0,0.0170494,"walk inference technique which was proposed to predict a new relationship between two entities in KGs. Lao et al. (2011) used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KG. Gardner et al. (2014) made use of an external text corpus to increase the connectivity of the KG used as the input to PRA. Gardner and Mitchell (2015) improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KGs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order logic in the discriminative Gaifman model (Niepert, 2016). In addition, Neelakantan et al. (2015) used a RNN to learn vector representations of PRA-style relation paths between entities in the KG. Other random-walk based learning algorithms for KG completion can be also found in Feng et al. (2016b), Liu et al. (2016), Wei et al. (2016), Mazumder and Liu (2017) and Das et al. (2018). Yang et al. (2017) proposed a Neural Logic Programming (LP) framework to learning probabilistic f"
2020.textgraphs-1.1,D16-1145,0,0.020342,"oved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KGs more efficient and expressive, while Wang et al. (2016) extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order logic in the discriminative Gaifman model (Niepert, 2016). In addition, Neelakantan et al. (2015) used a RNN to learn vector representations of PRA-style relation paths between entities in the KG. Other random-walk based learning algorithms for KG completion can be also found in Feng et al. (2016b), Liu et al. (2016), Wei et al. (2016), Mazumder and Liu (2017) and Das et al. (2018). Yang et al. (2017) proposed a Neural Logic Programming (LP) framework to learning probabilistic first-order logical rules for KG reasoning, producing competitive link prediction performances. Feldman 6 Dataset FB15k (Bordes et al., 2013) WN18 (Bordes et al., 2013) FB15k-237 (Toutanova and Chen, 2015) WN18RR Dettmers et al. (2018) |E | 14,951 40,943 14,541 40,943 |R| 1,345 18 237 11 #Triples in train/valid/test 483,142 50,000 59,071 141,442 5,000 5,000 272,115 17,535 20,466 86,835 3,034 3,134 Table 2: Statistics of benchmark experimental datasets"
2020.textgraphs-1.1,P16-1219,0,0.167356,"us to handle TransE’s regularization problem which forces entity embeddings to be on a sphere in the embedding vector space. Bilinear- & Tensor-based models: DISTMULT (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Jenatton et al., 2012) where each relation is represented by a diagonal matrix rather than a full matrix. SimplE (Kazemi and Poole, 2018) extends DISTMULT to allow two embeddings of each entity to be learned dependently. Such quadratic forms are also used to model entities and relations in KG2E (He et al., 2015), TATEC (Garc´ıa-Dur´an et al., 2016), TransG (Xiao et al., 2016), RSTE (Tay et al., 2017), ANALOGY (Liu et al., 2017) and Dihedral (Xu and Li, 2019). SME-bilinear (Bordes et al., 2012) is proposed to first separately combine entity-relation pairs (h, r) and (r, t) and then semantically match these combinations, using tensor product. HolE (Nickel et al., 2016b) uses circular correlation– a compositional operator–which can be interpreted as a compression of the tensor product. In addition, TuckER (Balazevic et al., 2019) is a linear model based on the Tucker tensor decomposition of the binary tensor representation of KG triples. Neural network-based models:"
2020.textgraphs-1.1,P17-1088,0,0.19022,"n (Trouillon et al., 2016) are commonly used in recent KG completion research:1  X exp f (h, r, t)  P LSoftmax = − exp f (h, r, t0 ) (h,r,t)∈G t0 ∈ E{t} !  exp f (h, r, t)  P exp f (h0 , r, t) + h0 ∈ E{h} LLogistic = X  log 1 + exp −I(h,r,t) · f (h, r, t) (h,r,t)∈{G∪G 0 }  with: I(h,r,t) = 1 for (h, r, t) ∈ G −1 for (h, r, t) ∈ G 0 To corrupt the head or tail entities, a common strategy is to uniformly replace the entities when sampling incorrect triples (Bordes et al., 2013), however it results in many false negative labels (Wang et al., 2014). Domain sampling (Krompaß et al., 2015; Xie et al., 2017) generates corrupted triples by sampling entities from the same domain or from the set of relation-dependent entities. The “Bernoulli” trick (Wang et al., 2014) is widely used to set different probabilities for generating head or tail entities: For each relation type r, we calculate the averaged number ar,1 of heads h for a pair (r, t) and the averaged number ar,2 of tails t for a pair (h, r). We then define a Bernoulli distribution with success probability ar,1 λr = for sampling: given a correct triple (h, r, t), we corrupt this triple by replacing head ar,1 + ar,2 entity with probability λr"
2020.textgraphs-1.1,2020.acl-main.526,0,0.130026,"is the sum of the vectors of all relations in the path. In Bilinear-COMP (Guu et al., 2015) and PRUNED-PATHS (Toutanova et al., 2016), each relation is a matrix and so it represents the relation path by matrix multiplication. Dur´an and Niepert (2018) proposed the KBLRN framework to combine relational paths with latent and numerical features. The neighborhood mixture model TransE-NMM (Nguyen et al., 2016a) can be also viewed as a threerelation path model because it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. ReInceptionE (Xie et al., 2020) employs the Inception network (Szegedy et al., 2016) to increase the interactions between head and relation embeddings for obtaining better representations of the head and relation pairs and then uses a relation-aware attention mechanism to enrich these pair representations with the local neighborhood and global entity information. Neighborhood information is also exploited in R-GCN (Schlichtkrull et al., 2018), SACN (Shang et al., 2019) and KBGAT (Nathani et al., 2019), which generalize graph convolutional networks (Kipf and Welling, 2017) and graph attention networks (Velikovi et al., 2018)"
2020.textgraphs-1.1,P19-1026,0,0.018839,"sphere in the embedding vector space. Bilinear- & Tensor-based models: DISTMULT (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Jenatton et al., 2012) where each relation is represented by a diagonal matrix rather than a full matrix. SimplE (Kazemi and Poole, 2018) extends DISTMULT to allow two embeddings of each entity to be learned dependently. Such quadratic forms are also used to model entities and relations in KG2E (He et al., 2015), TATEC (Garc´ıa-Dur´an et al., 2016), TransG (Xiao et al., 2016), RSTE (Tay et al., 2017), ANALOGY (Liu et al., 2017) and Dihedral (Xu and Li, 2019). SME-bilinear (Bordes et al., 2012) is proposed to first separately combine entity-relation pairs (h, r) and (r, t) and then semantically match these combinations, using tensor product. HolE (Nickel et al., 2016b) uses circular correlation– a compositional operator–which can be interpreted as a compression of the tensor product. In addition, TuckER (Balazevic et al., 2019) is a linear model based on the Tucker tensor decomposition of the binary tensor representation of KG triples. Neural network-based models: The neural tensor network (NTN) model (Socher et al., 2013) also uses a bilinear ten"
2020.textgraphs-1.1,C18-1200,0,0.034224,"Missing"
2020.textgraphs-1.1,N16-1105,0,0.0903696,"ly. Similar to TransR, TransR-FT (Feng et al., 2016a) also uses a matrix to project head and tail entity vectors. 2 A relation type r is classified Many-to-1 if multiple head entities can be connected by r to at most one tail entity. A relation type r is classified 1-to-Many if multiple tail entities can be linked by r from at most one head entity. A relation type r is classified Many-to-Many if multiple head entities can be connected by r to a tail entity and vice versa. 4 TEKE H (Wang and Li, 2016) extends TransH to incorporate rich context information in an external text corpus. lppTransD (Yoon et al., 2016) extends TransD to additionally use two projection vectors for representing each relation. STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016) can be viewed as direct extensions of TransR, where head and tail entities are associated with their own projection matrices. Unlike STransE, TranSparse uses adaptive sparse matrices, whose sparse degrees are defined based on the number of entities linked by relations. TranSparse-DT (Chang et al., 2017) is an extension of TranSparse with a dynamic translation. ITransF (Xie et al., 2017) can be considered as a generalization of STransE, which"
2020.wnut-1.41,2020.emnlp-demos.2,1,0.853385,"Missing"
2020.wnut-1.41,N19-1423,0,0.0579393,"Missing"
2020.wnut-1.41,2020.wnut-1.68,0,0.0947695,"Missing"
2020.wnut-1.41,2020.wnut-1.47,0,0.0724719,"Missing"
2020.wnut-1.41,E17-2068,0,0.658979,"llowing example presents an informative Tweet: In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task. 1 Introduction As of late-September 2020, the COVID-19 Coronavirus pandemic has led to about 1M deaths and 33M infected patients from 213 countries and territories, creating fear and panic for people all around the world.1 Recently, much attention has been paid to building monitoring systems (e.g. The Johns Hopkins Coronavirus Dashboard) to track the development of the pandemic and to provide users the information related to the virus,2 e.g. any new suspicio"
2020.wnut-1.41,2020.wnut-1.57,0,0.0605608,"Missing"
2020.wnut-1.41,2021.ccl-1.108,0,0.10091,"Missing"
2021.emnlp-main.369,W18-6319,0,0.017477,"parison between the wellknown automatic translation engines (here, Google Translate and Bing Translator) and strong neural MT baselines, and (ii) the usefulness of pre-trained sequence-to-sequence denoising autoencoder. In particular, we use the baseline models: Transformer-base, Transformer-big (Vaswani et al., 2017), and the pre-trained denoising auto-encoder mBART (Liu et al., 2020). We report standard metrics TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), in which lower TER and higher BLEU indicate better performances. We compute the case-sensitive BLEU score using SacreBLEU (Post, 2018). See the Appendix for implementation details. Here, we select the model checkpoint that obtains the highest BLEU score on the validation set to apply to the test set. Model News Google Translate 34.33 Bing Translator 35.05 Transformer-base 34.94 Transformer-big 36.14 mBART 37.04 BloS 26.71 25.91 27.13 28.26 29.26 Vi-to-En TedT MedW 34.03 48.81 32.03 50.29 33.46 50.56 34.46 51.20 36.43 53.93 WikH 28.76 24.78 28.37 28.86 30.63 OpeS 28.63 30.56 32.29 32.69 34.13 Table 3: BLEU scores for each resource domain on the test set. Here, “BloS”, “TedT”, “MedW”, “WikH” and “OpeS” abbreviate Blogspot, TED"
2021.emnlp-main.369,2020.emnlp-main.365,0,0.0542947,"y Vietnamese-English parallel corpora are either not publicly available or smallscale. Ngo et al. (2013) and Phan-Vu et al. (2019) present two corpora each comprising of 800K sentence pairs, however, these two corpora are not publicly available. Thus, the VietnameseEnglish parallel corpus IWSLT15 (Cettolo et al., 2015) of 133K sentence pairs extracted from TED-Talks transcripts is still considered as the standard benchmark for MT when it comes to Vietnamese. Recently, the OPUS project (Tiedemann, 2012) provides 300K+ sentence pairs extracted from the TED2020 v1 corpus of TEDTalks transcripts (Reimers and Gurevych, 2020). Vietnam has achieved rapid economic growth in the last two decades (Baum, 2020). It is now an attractive destination for trade and investment. Due to the language barrier, foreigners usually rely on automatic machine translation (MT) systems to translate Vietnamese texts into their native language or another language they are familiar with, e.g. the global language English, so they could quickly catch up with ongoing events in Vietnam. Thus the demand for high-quality Vietnamese-English MT has rapidly increased. However, state-of-theart MT models require high-quality and large-scale corpora"
2021.emnlp-main.369,2021.eacl-main.115,0,0.0825035,"Missing"
2021.emnlp-main.369,P16-1162,0,0.117048,"Missing"
2021.emnlp-main.369,N18-5012,1,0.855803,"Missing"
2021.naacl-demos.1,2020.findings-emnlp.364,1,0.800185,"AIResearch/PhoNLP. 1 Introduction Vietnamese NLP research has been significantly explored recently. It has been boosted by the success of the national project on Vietnamese language and speech processing (VLSP) KC01.01/20062010 and VLSP workshops that have run shared tasks since 2013.1 Fundamental tasks of POS tagging, NER and dependency parsing thus play important roles, providing useful features for many downstream application tasks such as machine translation (Tran et al., 2016), sentiment analysis (Bang and Sornlertlamvanich, 2018), relation extraction (To and Do, 2020), semantic parsing (Nguyen et al., 2020), open information extraction (Truong et al., 2017) and question answering 1 https://vlsp.org.vn/ 1 Proceedings of NAACL-HLT 2021: Demonstrations, pages 1–7 June 6–11, 2021. ©2021 Association for Computational Linguistics Dependency parsing sub vmod BIAFFINE BIAFFINE FFNN CRF NER O O B-LOC FFNN FFNN FFNN PRON POS Tagging FFNN FFNN softmax softmax softmax FFNN BERT-based encoder NOUN VERB FFNN FFNN Pre-trained BERT-based LM ĐâyThis ID 1 2 3 Form ĐâyThis làis Hà_NộiHa_Noi Hà_NộiHa_Noi làis POS PRON VERB NOUN NER O O B-LOC Head 2 0 2 DepRel sub root vmod Figure 1: Illustration of our PhoNLP model"
2021.naacl-demos.1,N19-1423,0,0.181323,", VnCoreNLP is now no longer considered state-of-the-art because its performance results are significantly outperformed by ones obtained when fine-tuning PhoBERT—the current state-of-the-art monolingual pre-trained language model for Vietnamese (Nguyen and Nguyen, 2020). Note that there are no publicly available fine-tuned BERT-based models for the three Vietnamese tasks. Assuming that there would be, a potential drawback might be that an NLP package wrapping such fine-tuned BERTbased models would take a large storage space, i.e. three times larger than the storage space used by a BERT model (Devlin et al., 2019), thus it would not be suitable for practical applications that require a smaller storage space. Jointly multi-task learning is a promising solution as it might help reduce the storage space. In addition, POS tagging, NER and dependency parsing are related tasks: POS tags are essential input features used for dependency parsing and POS tags are also used as additional features for NER. Jointly multi-task learning thus might also help improve the performance results against the single-task learning (Ruder, 2019). In this paper, we present a new multi-task learning model—named PhoNLP—for joint P"
2021.naacl-demos.1,U19-1004,1,0.780731,": (1) vi = ei ◦ ti (3) where following Hashimoto et al. (2017), the “soft” (1) POS tag embedding ti is computed by multiplying a label weight matrix W(1) with the corresponding probability vector pi : (1) ti 2.5 = W(1) pi The dependency parsing layer creates vectors z1:n in which each zi is resulted in by concatenating ei (2) and another “soft” POS tag embedding ti : (2) ti =W (2) (8) (9) Joint multi-task learning Discussion: Our PhoNLP can be viewed as an extension of previous joint POS tagging and dependency parsing models (Hashimoto et al., 2017; Li et al., 2018; Nguyen and Verspoor, 2018; Nguyen, 2019; Kondratyuk and Straka, 2019), where we additionally incorporate a CRF-based prediction layer for NER. Unlike Hashimoto et al. (2017), Nguyen and Verspoor (2018), Li et al. (2018) and Nguyen (2019) that use BiLSTM-based encoders to extract contextualized feature embeddings, we use a BERT-based encoder. Kondratyuk and Straka (2019) also employ a BERT-based encoder. However, different from PhoNLP where we construct a hierarchical architecture over the POS tagging and dependency parsing layers, Kondratyuk and Straka (2019) do not make use of POS tag embeddings for dependency parsing.2 Dependency"
2021.naacl-demos.1,2020.findings-emnlp.92,1,0.86557,"Missing"
2021.naacl-demos.1,D17-1206,0,0.0304558,"ce labeling 2 ( A- H ) task (Devlin et al., 2019), the POS tagging layer is a linear prediction layer that is appended on top of the encoder. In particular, the POS tagging layer feeds the contextualized word embeddings ei into a feed-forward network (FFNNPOS ) followed by a softmax predictor for POS tag prediction:  pi = softmax FFNNPOS ei (2) hi ( A- D ) hi (L -H) hi (L -D) hi NER The NER layer creates a sequence of vectors v1:n in which each vi is resulted in by concatenating the contextualized word embedding ei and a “soft” (1) POS tag embedding ti : (1) vi = ei ◦ ti (3) where following Hashimoto et al. (2017), the “soft” (1) POS tag embedding ti is computed by multiplying a label weight matrix W(1) with the corresponding probability vector pi : (1) ti 2.5 = W(1) pi The dependency parsing layer creates vectors z1:n in which each zi is resulted in by concatenating ei (2) and another “soft” POS tag embedding ti : (2) ti =W (2) (8) (9) Joint multi-task learning Discussion: Our PhoNLP can be viewed as an extension of previous joint POS tagging and dependency parsing models (Hashimoto et al., 2017; Li et al., 2018; Nguyen and Verspoor, 2018; Nguyen, 2019; Kondratyuk and Straka, 2019), where we additiona"
2021.naacl-demos.1,D19-1279,0,0.0164157,"◦ ti (3) where following Hashimoto et al. (2017), the “soft” (1) POS tag embedding ti is computed by multiplying a label weight matrix W(1) with the corresponding probability vector pi : (1) ti 2.5 = W(1) pi The dependency parsing layer creates vectors z1:n in which each zi is resulted in by concatenating ei (2) and another “soft” POS tag embedding ti : (2) ti =W (2) (8) (9) Joint multi-task learning Discussion: Our PhoNLP can be viewed as an extension of previous joint POS tagging and dependency parsing models (Hashimoto et al., 2017; Li et al., 2018; Nguyen and Verspoor, 2018; Nguyen, 2019; Kondratyuk and Straka, 2019), where we additionally incorporate a CRF-based prediction layer for NER. Unlike Hashimoto et al. (2017), Nguyen and Verspoor (2018), Li et al. (2018) and Nguyen (2019) that use BiLSTM-based encoders to extract contextualized feature embeddings, we use a BERT-based encoder. Kondratyuk and Straka (2019) also employ a BERT-based encoder. However, different from PhoNLP where we construct a hierarchical architecture over the POS tagging and dependency parsing layers, Kondratyuk and Straka (2019) do not make use of POS tag embeddings for dependency parsing.2 Dependency parsing (2) = FFNNLabel-Head"
2021.naacl-demos.1,K18-2008,1,0.746857,"” (1) POS tag embedding ti : (1) vi = ei ◦ ti (3) where following Hashimoto et al. (2017), the “soft” (1) POS tag embedding ti is computed by multiplying a label weight matrix W(1) with the corresponding probability vector pi : (1) ti 2.5 = W(1) pi The dependency parsing layer creates vectors z1:n in which each zi is resulted in by concatenating ei (2) and another “soft” POS tag embedding ti : (2) ti =W (2) (8) (9) Joint multi-task learning Discussion: Our PhoNLP can be viewed as an extension of previous joint POS tagging and dependency parsing models (Hashimoto et al., 2017; Li et al., 2018; Nguyen and Verspoor, 2018; Nguyen, 2019; Kondratyuk and Straka, 2019), where we additionally incorporate a CRF-based prediction layer for NER. Unlike Hashimoto et al. (2017), Nguyen and Verspoor (2018), Li et al. (2018) and Nguyen (2019) that use BiLSTM-based encoders to extract contextualized feature embeddings, we use a BERT-based encoder. Kondratyuk and Straka (2019) also employ a BERT-based encoder. However, different from PhoNLP where we construct a hierarchical architecture over the POS tagging and dependency parsing layers, Kondratyuk and Straka (2019) do not make use of POS tag embeddings for dependency parsin"
2021.naacl-demos.1,W09-3035,0,0.11597,"Missing"
2021.naacl-demos.1,K18-2016,0,0.396496,"predictor for NER label prediction (Lafferty et al., 2001). A cross-entropy loss LNER is calculated for NER during training while the Viterbi algorithm is used for inference. zi = ei ◦ ti (7)  The final training objective loss L of our model PhoNLP is the weighted sum of the POS tagging loss LPOS , the NER loss LNER and the dependency parsing loss LDEP : The NER layer then passes each vector vi into a FFNN (FFNNNER ):  hi = FFNNNER vi (4) 2.4 (6) To predict potential dependency arcs, based on ( A- H ) ( A- D ) input vectors hi and hj , the parsing layer uses a Biaffine classifier’s variant (Qi et al., 2018) that additionally takes into account the distance and relative ordering between two words to produce a probability distribution of arc heads for each word. For inference, the Chu–Liu/Edmonds’ algorithm is used to find a maximum spanning tree (Chu and Liu, 1965; Edmonds, 1967). The parsing layer also uses another Biaffine classifier to label (L -H) the predicted arcs, based on input vectors hi (L -D) and hj . An objective loss LDEP is computed by summing a cross entropy loss for unlabeled dependency parsing and another cross entropy loss for dependency label prediction during training based on"
2021.naacl-demos.1,2020.acl-demos.14,0,0.0222906,"the dependency parsing layer uses FFNNs to split zi into head and dependent representations: 2 In our preliminary experiments, not feeding the POS tag embeddings into the dependency parsing layer decreases the performance. 3 Task POS tagging (leakage) POS tagging (re-split) NER Dependency parsing #train 27000 23906 14861 8977 #valid 870 2009 2000 200 #test 2120 3481 2831 1020 3.1.2 PhoNLP is implemented based on PyTorch (Paszke et al., 2019), employing the PhoBERT encoder implementation available from the transformers library (Wolf et al., 2020) and the Biaffine classifier implementation from Qi et al. (2020). We set both the label weight matrices W(1) and W(2) to have 100 rows, resulting in 100-dimensional soft POS tag embeddings. In addition, following Qi et al. (2018, 2020), FFNNs in equations 6–9 use 400dimensional output layers. We use the AdamW optimizer (Loshchilov and Hutter, 2019) and a fixed batch size at 32, and train for 40 epochs. The sizes of training sets are different, in which the POS tagging training set is the largest, consisting of 23906 sentences. Thus for each training epoch, we repeatedly sample from the NER and dependency parsing training sets to fill the gaps between the t"
2021.naacl-demos.1,N19-5004,0,0.0216731,"space, i.e. three times larger than the storage space used by a BERT model (Devlin et al., 2019), thus it would not be suitable for practical applications that require a smaller storage space. Jointly multi-task learning is a promising solution as it might help reduce the storage space. In addition, POS tagging, NER and dependency parsing are related tasks: POS tags are essential input features used for dependency parsing and POS tags are also used as additional features for NER. Jointly multi-task learning thus might also help improve the performance results against the single-task learning (Ruder, 2019). In this paper, we present a new multi-task learning model—named PhoNLP—for joint POS tagging, NER and dependency parsing. In particular, given an input sentence of words to PhoNLP, an encoding layer generates contextualized word embeddings that represent the input words. These contextualized word embeddings are fed into a POS tagging layer that is in fact a linear prediction layer (Devlin et al., 2019) to predict POS tags for the corresponding input words. Each predicted POS We present the first multi-task learning model—named PhoNLP—for joint Vietnamese part-of-speech (POS) tagging, named e"
2021.naacl-demos.1,P16-1162,0,0.00592851,"pendency parsing for Vietnamese. Given an input sentence consisting of n word tokens w1 , w2 , ..., wn , the encoding layer employs PhoBERT to generate contextualized latent feature embeddings ei each representing the ith word wi :  ei = PhoBERTbase w1:n , i (1) • We discuss a data leakage issue in the Vietnamese benchmark datasets, that has not yet been pointed out before. Experiments show that PhoNLP obtains state-of-the-art performance results, outperforming the PhoBERT-based single task learning. In particular, the encoding layer employs the PhoBERTbase version. Because PhoBERT uses BPE (Sennrich et al., 2016) to segment the input sentence with subword units, the encoding layer in fact represents the ith word wi by using the contextualized embedding of its first subword. • We publicly release PhoNLP as an open-source toolkit that is simple to setup and efficiently run from both the command-line and Python API. We hope that PhoNLP can serve as a strong baseline 2.2 POS tagging Following a common manner when fine-tuning a pre-trained language model for a sequence labeling 2 ( A- H ) task (Devlin et al., 2019), the POS tagging layer is a linear prediction layer that is appended on top of the encoder."
2021.naacl-demos.1,N18-5012,1,0.688676,"m the input file “input.txt” in Figure 2. The output is formatted with 6 columns representing word index, word form, POS tag, NER label, head index of the current word and its dependency relation type. parameter tuning and model selection strategy that we use for PhoNLP. Note that PhoBERT helps produce state-of-theart results for multiple Vietnamese NLP tasks (including but not limited to POS tagging, NER and dependency parsing in a single-task training strategy), and obtains higher performance results than VnCoreNLP. However, in both the PhoBERT and VnCoreNLP papers (Nguyen and Nguyen, 2020; Vu et al., 2018), results for POS tagging and dependency parsing are reported w.r.t. the data leakage issue. Our “Single-task” results in Table 2 regarding “Re-spl” (i.e. the data re-split and duplication removal for POS tagging to avoid the data leakage issue) can be viewed as new PhoBERT results for a proper experimental setup. Table 2 shows that in both setups “Leak.” and “Re-spl”, our joint multi-task training approach PhoNLP performs better than the PhoBERT-based single-task training approach, thus resulting in state-of-theart performances for the three tasks of Vietnamese POS tagging, NER and dependency"
2021.naacl-demos.1,2020.emnlp-demos.6,0,0.0466334,"Missing"
2021.naacl-main.173,2020.acl-main.747,0,0.519133,"P 2016 and find that: automatic Vietnamese word segmentation helps improve the NER results and and 2018 NER shared tasks (Huyen and Luong, the highest performances are obtained by fine2016; Nguyen et al., 2018b). Here, the VLSP-2018 tuning pre-trained language models where the NER dataset is an extension of the VLSP-2016 monolingual model PhoBERT for Vietnamese NER dataset with more data. These two datasets (Nguyen and Nguyen, 2020) produces higher only focus on recognizing generic entities of person results than the multilingual model XLM-R names, organizations, and locations in online news (Conneau et al., 2020). We publicly release articles. Thus, making them difficult to adapt to the our dataset at: https://github.com/ context of extracting key entity information related VinAIResearch/PhoNER_COVID19. to COVID-19 patients. This leads to our work’s 1 Introduction main goals that are: (i) To develop a NER task in the COVID-19 specified domain, that potentially As of early November 2020, the total number of 1 impacts research and downstream applications, and COVID-19 cases worldwide has surpassed 50M. (ii) To provide the research community with a new The world is once again hit by a new wave of dataset"
2021.naacl-main.173,W18-2501,0,0.0120114,"Optimizer Learning rate Mini-batch size LSTM hidden state size Number of BiLSTM layers Dropout Character embedding size Filter length, i.e. window size Number of filters Value Adam 0.001 36 200 2 [0.25, 0.25] 50 3 30 Table 3: Hyper-parameters for BiLSTM-CNN-CRF. Nguyen, 2020). XLM-R is a multi-lingual variant of RoBERTa (Liu et al., 2019), pre-trained on a 2.5TB multilingual dataset that contains 137GB of syllable-level Vietnamese texts. PhoBERT is a monolingual variant of RoBERTa, pre-trained on a 20GB word-level Vietnamese dataset. We employ the BiLSTM-CNN-CRF implementation from AllenNLP (Gardner et al., 2018). Training BiLSTM-CNN-CRF requires input pretrained syllable- and word-level embeddings for the syllable- and word-level settings, respectively. Thus we employ the pre-trained Word2Vec syllable and word embeddings for Vietnamese from Nguyen et al. (2020a). These embeddings are fixed during training. Optimal hyper-parameters that we gridsearched for BiLSTM-CNN-CRF are presented in Table 3. We utilize the transformers library (Wolf et al., 2020) to fine-tune XLM-R and PhoBERT for the syllable- and word-level settings, respectively, using Adam (Kingma and Ba, 2014) with a fixed learning rate of 5"
2021.naacl-main.173,W16-3919,0,0.0191922,"in the future. annotated for the named entity recognition One of the first steps to develop such systems is to (NER) task with newly-defined entity types that can be used in other future epidemics. recognize relevant named entities mentioned in the Our dataset also contains the largest number texts, which is also known as the NER task. of entities compared to existing Vietnamese Compared to other languages, data resources NER datasets. We empirically conduct experfor the Vietnamese NER task are limited, includiments using strong baselines on our dataset, ing only two public datasets from the VLSP 2016 and find that: automatic Vietnamese word segmentation helps improve the NER results and and 2018 NER shared tasks (Huyen and Luong, the highest performances are obtained by fine2016; Nguyen et al., 2018b). Here, the VLSP-2018 tuning pre-trained language models where the NER dataset is an extension of the VLSP-2016 monolingual model PhoBERT for Vietnamese NER dataset with more data. These two datasets (Nguyen and Nguyen, 2020) produces higher only focus on recognizing generic entities of person results than the multilingual model XLM-R names, organizations, and locations in online news (Connea"
2021.naacl-main.173,2020.nlpcovid19-2.1,0,0.036872,"mprove the NER results, and (ii) The highest results are obtained by fine-tuning the pre-trained language models, where PhoBERT does better than XLM-R. • We publicly release our dataset for research or educational purposes. We hope that our dataset can serve as a starting point for future COVID-19 related Vietnamese NLP research and applications. 2020), that help facilitate many types of research works, such as building search engines to retrieve relevant information from scholarly articles (Esteva et al., 2020; Zhang et al., 2020; Verspoor et al., 2020), question answering and summarization (Lee et al., 2020; Su et al., 2020). Recently, Colic et al. (2020) fine-tune a BERT-based NER model on the CRAFT corpus (Verspoor et al., 2012) to recognize and then normalize biomedical ontology and terminology entities in LitCovid. The second type is social media data, particularly Tweets. COVID-19 related Tweet datasets are built for many analytic tasks such as identification of informative Tweets (Nguyen et al., 2020b), and disinformation detection and fact-checking (Shahi and Nandini, 2020; Alam et al., 2020; Alsudias and Rayson, 2020). The most relevant work to ours is proposed by Zong et al. (2020), tha"
2021.naacl-main.173,2021.ccl-1.108,0,0.0742268,"Missing"
2021.naacl-main.173,P16-1101,0,0.289929,"and are not as reliable as official news sources. We then empirically evaluate strong baseline models on our dataset. Our contributions are summarized as follows: • We introduce the first manually annotated Vietnamese dataset in the COVID-19 domain. Our dataset is annotated with 10 different named entity types related to COVID19 patients in Vietnam. Compared to the VLSP-2016 and VLSP-2018 Vietnamese NER datasets, our dataset has the largest number of entities, consisting of 35K entities over 10K sentences. • We empirically investigate strong baselines on our dataset, including BiLSTM-CNN-CRF (Ma and Hovy, 2016) and the pre-trained language models XLM-R (Conneau et al., 2020) and PhoBERT (Nguyen and Nguyen, 2020). We find that: (i) Automatic Vietnamese word segmentation helps improve the NER results, and (ii) The highest results are obtained by fine-tuning the pre-trained language models, where PhoBERT does better than XLM-R. • We publicly release our dataset for research or educational purposes. We hope that our dataset can serve as a starting point for future COVID-19 related Vietnamese NLP research and applications. 2020), that help facilitate many types of research works, such as building search"
2021.naacl-main.173,2020.findings-emnlp.364,1,0.901945,"s, such as building search engines to retrieve relevant information from scholarly articles (Esteva et al., 2020; Zhang et al., 2020; Verspoor et al., 2020), question answering and summarization (Lee et al., 2020; Su et al., 2020). Recently, Colic et al. (2020) fine-tune a BERT-based NER model on the CRAFT corpus (Verspoor et al., 2012) to recognize and then normalize biomedical ontology and terminology entities in LitCovid. The second type is social media data, particularly Tweets. COVID-19 related Tweet datasets are built for many analytic tasks such as identification of informative Tweets (Nguyen et al., 2020b), and disinformation detection and fact-checking (Shahi and Nandini, 2020; Alam et al., 2020; Alsudias and Rayson, 2020). The most relevant work to ours is proposed by Zong et al. (2020), that aims to extract COVID-19 events reporting test results, death cases, cures and prevention from English Tweets. As Twitter is rarely used by Vietnamese people, we could not use it for data collection. 3 3.1 Our dataset Entity types We define 10 entity types with the aim of extracting key information related to COVID-19 patients, which are especially useful in downstream applications. In general, these e"
2021.naacl-main.173,2020.findings-emnlp.92,1,0.880835,"Missing"
2021.naacl-main.173,L18-1410,1,0.902533,"mics. recognize relevant named entities mentioned in the Our dataset also contains the largest number texts, which is also known as the NER task. of entities compared to existing Vietnamese Compared to other languages, data resources NER datasets. We empirically conduct experfor the Vietnamese NER task are limited, includiments using strong baselines on our dataset, ing only two public datasets from the VLSP 2016 and find that: automatic Vietnamese word segmentation helps improve the NER results and and 2018 NER shared tasks (Huyen and Luong, the highest performances are obtained by fine2016; Nguyen et al., 2018b). Here, the VLSP-2018 tuning pre-trained language models where the NER dataset is an extension of the VLSP-2016 monolingual model PhoBERT for Vietnamese NER dataset with more data. These two datasets (Nguyen and Nguyen, 2020) produces higher only focus on recognizing generic entities of person results than the multilingual model XLM-R names, organizations, and locations in online news (Conneau et al., 2020). We publicly release articles. Thus, making them difficult to adapt to the our dataset at: https://github.com/ context of extracting key entity information related VinAIResearch/PhoNER_CO"
2021.naacl-main.173,N18-5012,1,0.854888,"pe is briefly described in Table 1. tific publications, including the datasets CORDSee the Appendix for entity examples as well as 19 (Wang et al., 2020) and LitCovid (Chen et al., some notices over the entity types. 2147 3.2 We first crawl articles tagged with &quot;COVID-19&quot; or &quot;COVID&quot; keywords from the reputable Vietnamese online news sites, including VnExpress,2 ZingNews,3 BaoMoi4 and ThanhNien.5 These articles are dated between February 2020 and August 2020. We then segment the crawled news articles’ primary text content into sentences using RDRSegmenter (Nguyen et al., 2018a) from VnCoreNLP (Vu et al., 2018). To retrieve informative sentences about COVID19 patients, we employ BM25Plus (Trotman et al., 2014) with search queries of common keywords appearing in sentences that report confirmed, suspected, recovered, or death cases as well as the travel history or location of the cases. From the top 15K sentences ranked by BM25Plus, we manually filter out sentences that do not contain information related to patients in Vietnam, thus resulting in a dataset of 10027 raw sentences. 3.3 Annotation process We develop an initial version of our annotation guidelines and then randomly sample a pilot set of 1K"
2021.naacl-main.173,2020.nlpcovid19-acl.1,0,0.0364311,"Missing"
2021.naacl-main.173,2020.emnlp-demos.6,0,0.0477677,"Missing"
E14-2005,J93-2004,0,0.0606227,"Missing"
E14-2005,W09-3035,0,\N,Missing
E14-2005,J95-4004,0,\N,Missing
E14-2005,A00-1031,0,\N,Missing
I13-1114,W02-1011,0,0.0325527,"ssification accuracy. 1 Introduction The rapid growth of the Web supports human users to easily express their reviews about such entities as products, services, events and their properties as well as to find and evaluate the others’ opinions. This brings new challenges for building systems to categorize and understand the sentiments in those reviews. In particular, document-level sentiment classification systems aim to determine either a positive or negative opinion in a given opinionated document (Turney, 2002; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010"
I13-1114,J11-2001,0,0.0639927,"these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems, Naive Bayes (NB) and Support Vector Machine (SVM) are often applied for training learning models as they are frequently used as baseline methods in task of text classification (Wang and Manning, 2012). Although NBs are very fast classifiers requiring a small amount training data, there is a loss of accuracy due to the NBs’ conditional independence assumption. On the other hand, SVMs 2 Our approach This section is to describe our two-stage system for sentiment classification. Figure 1 details an overview of our system’s architectu"
I13-1114,P12-2066,0,0.4276,"Missing"
I13-1114,P02-1053,0,0.0198645,"ents show that our classifier requires less training data while still maintaining reasonable classification accuracy. 1 Introduction The rapid growth of the Web supports human users to easily express their reviews about such entities as products, services, events and their properties as well as to find and evaluate the others’ opinions. This brings new challenges for building systems to categorize and understand the sentiments in those reviews. In particular, document-level sentiment classification systems aim to determine either a positive or negative opinion in a given opinionated document (Turney, 2002; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wi"
I13-1114,P11-1015,0,0.0671729,"products, services, events and their properties as well as to find and evaluate the others’ opinions. This brings new challenges for building systems to categorize and understand the sentiments in those reviews. In particular, document-level sentiment classification systems aim to determine either a positive or negative opinion in a given opinionated document (Turney, 2002; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems, Naive Bayes (NB) and Support Vector Machine (SVM) are often applied for training learning models as they are frequently"
I13-1114,P12-2018,0,0.0457498,"properties as well as to find and evaluate the others’ opinions. This brings new challenges for building systems to categorize and understand the sentiments in those reviews. In particular, document-level sentiment classification systems aim to determine either a positive or negative opinion in a given opinionated document (Turney, 2002; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems, Naive Bayes (NB) and Support Vector Machine (SVM) are often applied for training learning models as they are frequently used as baseline methods in task of text c"
I13-1114,W04-3253,0,0.0603855,"e rapid growth of the Web supports human users to easily express their reviews about such entities as products, services, events and their properties as well as to find and evaluate the others’ opinions. This brings new challenges for building systems to categorize and understand the sentiments in those reviews. In particular, document-level sentiment classification systems aim to determine either a positive or negative opinion in a given opinionated document (Turney, 2002; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems, Naive Bayes (NB) and Supp"
I13-1114,H05-1044,0,0.112056,"02; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems, Naive Bayes (NB) and Support Vector Machine (SVM) are often applied for training learning models as they are frequently used as baseline methods in task of text classification (Wang and Manning, 2012). Although NBs are very fast classifiers requiring a small amount training data, there is a loss of accuracy due to the NBs’ conditional independence assumption. On the other hand, SVMs 2 Our approach This section is to describe our two-stage system for sentiment classification. Figure 1 detail"
I13-1114,P06-2079,0,0.174783,"der to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems, Naive Bayes (NB) and Support Vector Machine (SVM) are often applied for training learning models as they are frequently used as baseline methods in task of text classification (Wang and Manning, 2012). Although NBs are very fast classifiers requiring a small amount training data, there is a loss of accuracy due to the NBs’ conditional independence assumption. On the other hand, SVMs 2 Our approach This section is to describe our two-stage system for sentiment classification. Figure 1 details an overview of"
I13-1114,P04-1035,0,0.190659,"y. 1 Introduction The rapid growth of the Web supports human users to easily express their reviews about such entities as products, services, events and their properties as well as to find and evaluate the others’ opinions. This brings new challenges for building systems to categorize and understand the sentiments in those reviews. In particular, document-level sentiment classification systems aim to determine either a positive or negative opinion in a given opinionated document (Turney, 2002; Liu, 2010). In order to construct these systems, classification-based approaches (Pang et al., 2002; Pang and Lee, 2004; Mullen and Collier, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012) utilizing machine learning to automatically identify document-level sentiment polarity are still mainstream methods obtaining state-of-the-art performances. It is because of possibly combining various features such as: bag of words, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Taboada et al., 2011) like SentiWordNet (Baccianella et al., 2010). In these systems,"
I13-1114,baccianella-etal-2010-sentiwordnet,0,\N,Missing
I17-2007,P16-1004,0,0.0136662,"r1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@mq.edu.au 3 Max Planck Institute for Informatics, Germany cxchu@mpi-inf.mpg.de Abstract ploy sequence-to-sequence learning (S EQ 2S EQ) for this task. S EQ 2S EQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, S EQ 2S EQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of S EQ 2S EQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of S EQ 2S EQ allow us to approach the event prediction task. S EQ 2S EQ has been applied to text prediction by Kiros et al. (2015)"
I17-2007,D15-1166,0,0.0288107,"Missing"
I17-2007,K16-1028,0,0.0186085,"tional Linguistics, Saarland University, Germany {daiquocn, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@mq.edu.au 3 Max Planck Institute for Informatics, Germany cxchu@mpi-inf.mpg.de Abstract ploy sequence-to-sequence learning (S EQ 2S EQ) for this task. S EQ 2S EQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, S EQ 2S EQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of S EQ 2S EQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of S EQ 2S EQ allow us to approach the event prediction task. S EQ 2S EQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016). We also use S"
I17-2007,P02-1040,0,0.100146,"the timestep i. We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016) where they only use a single layer. So we in fact experiment with Bidirectional-LSTM multi-layer RNN (BiLSTM) and Bidirectional-GRU multilayer RNN (BiGRU). • Pichotta and Mooney (2016) use the BLEU score (Papineni et al., 2002) for evaluation (i.e., the standard evaluation metric used in machine translation), which measures surface similarity between predicted and actual sentences. We complement this evaluation by measuring prediction accuracy on the semantic level. To this purpose, we use the gold paraphrase sets of event descriptions in the D E S CRIPT corpus, e.g., “Remove cake”, “Remove from oven” and “Take the cake out of oven” belong to the same gold paraphrase set of taking out oven. The gold paraphrase sets allow us to access the correctness of the prediction which could not be attained by using the BLEU mea"
I17-2007,D14-1179,0,0.010377,"Missing"
I17-2007,W15-4915,0,0.0483245,"Missing"
I17-2007,2015.eamt-1.16,0,\N,Missing
I17-2007,P16-1027,0,\N,Missing
K16-1005,D15-1034,0,0.352519,"Missing"
K16-1005,D15-1173,0,0.138573,"the TransR model, where head and tail entities are associated with their own projection matrices. The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc´ıa-Dur´an et al., 2016). Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc´ıa-Dur´an et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), A"
K16-1005,D15-1082,0,0.493604,"of Computing, Macquarie University, Sydney, Australia dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au 2 Data61 & Australian National University lizhen.qu@data61.csiro.au Abstract et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al., 2015). Most embedding models for KB completion learn only from triples and by doing so, ignore lots of information implicitly provided by the structure of the knowledge graph. Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc´ıaDur´an et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion. For instance, a three-relation path Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE—a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information signifi"
K16-1005,P15-1009,0,0.142407,"onal data. 4 available datasets WN11, FB13 and NELL186. For all of them, the validation and test sets containing both correct and incorrect triples have already been constructed. Statistical information about these datasets is given in Table 2. The two benchmark datasets1 , WN11 and FB13, were produced by Socher et al. (2013) for triple classification. WN11 is derived from the large lexical KB WordNet (Miller, 1995) involving 11 relation types. FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types. The NELL186 dataset2 was introduced by Guo et al. (2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al., 2010). 4.2 We evaluate our model on three commonly used benchmark tasks: triple classification, entity prediction and relation prediction. This subsection describes those tasks in detail. Triple classification: The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding models. The aim of the task is to predict whether a triple (h, r, t) is c"
K16-1005,D15-1038,0,0.302974,"Missing"
K16-1005,D15-1191,0,0.602284,"and tail entities are associated with their own projection matrices. The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc´ıa-Dur´an et al., 2016). Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc´ıa-Dur´an et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 20"
K16-1005,N13-1090,0,0.15165,"o project entity and relation vectors into a subspace. The TransH model (Wang et al., 2014) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. STransE 43 Luo et al. (2015) constructed relation paths between entities and viewing entities and relations in the path as pseudo-words applied Word2Vec algorithms (Mikolov et al., 2013) to produce pretrained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE, SME and SE models. RTransE (Garc´ıa-Dur´an et al., 2015), PTransE (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multip"
K16-1005,P15-1067,0,0.350831,"(Garc´ıa-Dur´an et al., 2016; Nickel et al., 2016). In TransE, both entities e and relations r are represented with k-dimensional vectors v e ∈ Rk and v r ∈ Rk , respectively. These vectors are chosen such that for each triple (h, r, t) ∈ G: vh + vr ≈ vt (h,r,t)∈G 0 (h0 ,r,t0 )∈G(h,r,t) + ∪ {(h, r, t0 ) |t0 ∈ E, (h, r, t0 ) ∈ / G} is the set of incorrect triples generated by corrupting the correct triple (h, r, t) ∈ G. We applied the “Bernoulli” trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016). We use Stochastic Gradient Descent (SGD) with RMSProp adaptive learning rate to minimize L, and impose the following hard constraints during training: kv e k2 6 1 and kv r k2 6 1. We employ alternating optimization to minimize L. We first initialize the entity and relation-specific mixing parameters α and β to zero and only learn the randomly initialized entity and relation vectors v e and v r . Then we fix the learned vectors and only optimize the mixing parameters. In the final step, we fix again the mixing parameters and fine-tune the vectors. In all experiments presente"
K16-1005,P15-1016,0,0.0526464,"l., 2016) are extensions of the TransR model, where head and tail entities are associated with their own projection matrices. The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc´ıa-Dur´an et al., 2016). Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc´ıa-Dur´an et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, Ad"
K16-1005,N16-1054,1,0.648098,"us the corresponding mixing coefficient can be close to zero, whereas it could be relevant for predicting some other relationship, such as parent or spouse, in which case the relation-specific mixing coefficient for the child of relationship could be high. The primary contribution of this paper is introducing and formalizing the neighborhood mixture model. We demonstrate its usefulness by applying it to the well-known TransE model (Bordes et al., 2013). However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al., 2016). While relation path models exploit extra information using longer paths existing in the KB, the neighborhood mixture model effectively incorporates information about many paths simultaneously. Our extensive Ne = {(e0 , r)|r ∈ R ∪ R−1 , e0 ∈ Ne,r } is the set of all entity and relation pairs that are neighbors for entity e. Each entity e is associated with a k-dimensional vector v e ∈ Rk and relation-dependent vectors ue,r ∈ Rk , r ∈ R ∪ R−1 . Now we can define the neighborhood-based entity representation ϑe,r for an entity e ∈ E for predicting the relation r ∈ R as follows: ϑe,r = ae v e + X"
K16-1005,D14-1162,0,0.115248,"orted results on the WN11 and FB13 datasets. The first five rows report the performance of models that use TransE to initialize the entity and relation vectors. The last eight rows present the accuracy of models with randomly initialized parameters. Table 4 shows that our TransE-NMM model obtains the highest accuracy on WN11 and achieves the second highest result on FB13. Note that there are higher results reported for NTN (Socher et al., 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful. Table 5 compares the accuracy for triple classification, the raw mean rank and raw Hits@10 scores for entity prediction on the NELL186 dataset. The"
K16-1005,N16-1105,0,\N,Missing
K16-1005,E17-1013,0,\N,Missing
K16-1005,P16-1136,0,\N,Missing
K16-1005,P16-1124,0,\N,Missing
K16-1005,P16-1219,0,\N,Missing
K16-1005,D16-1145,0,\N,Missing
K17-3014,P16-1231,0,0.0892725,"Missing"
K17-3014,D15-1041,0,0.0586343,"ny external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art. 2 BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1 , w2 , ..., wn , we represent each word wi in (•) s by an embedding ewi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTMseq ) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1 c2 ...ck , the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the j th character cj in w; and the output (∗) is the character-based embedding ew of the word type w, computed as: Our joint model e(∗) w = BiLSTMseq (c1:k ) In this section, we descr"
K17-3014,D12-1133,0,0.0393744,"ation for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project"
K17-3014,D15-1159,0,0.273183,"cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially ou"
K17-3014,A00-1031,0,0.217047,"0 73.4 75.4 79.2 82.5 10.6 81.3 79.9 79.5 81.3 80.4 79.3 3.4 80.2 80.1 79.4 80.7 81.8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when t"
K17-3014,W06-2920,0,0.112905,"to achieve the best accuracy. Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e"
K17-3014,D14-1082,0,0.0290936,"Missing"
K17-3014,P11-1089,0,0.0526243,"142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Univers"
K17-3014,D16-1238,0,0.0563387,"Missing"
K17-3014,P14-1130,0,0.0298879,"k et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when the character-based representations of words are not taken into account. B’15 denotes the character-based stack LSTM model for transition-based dependency parsing (Ballesteros et al., 2015). PipelinePtag refers to a greedy version of the approach proposed by Alberti et al. (2015). RBGParser refers to the graph-based dependency parser with tensor decomposition, presented in Lei et al. (2014). [*]: Results are reported in Zhang and Weiss (2016). 3.2 Implementation details ment set is when using 64-dimensional character embeddings, 128-dimensional word embeddings, 128-dimensional BiLSTM states, 2 BiLSTM layers and 100 hidden nodes in MLPs with one hidden layer.4 We then apply those hyper-parameters to all 18 remaining languages. Our jPTDP is implemented using DYNET v2.0 (Neubig et al., 2017).3 We optimize the objective function using Adam (Kingma and Ba, 2014) with default DYNET parameter settings and no mini-batches. We use a fixed random seed, and we do not utilize pre-trained em"
K17-3014,P13-1104,0,0.0177322,"Missing"
K17-3014,D11-1109,0,0.0242839,"to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental r"
K17-3014,I17-1007,0,0.0423582,"Missing"
K17-3014,P15-1033,0,0.0179515,"Missing"
K17-3014,P13-2109,0,0.0304432,"Missing"
K17-3014,C96-1058,0,0.755432,"g performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed int"
K17-3014,P05-1012,0,0.124355,"and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer percept"
K17-3014,J11-1007,0,0.0469794,"tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustrati"
K17-3014,I11-1136,0,0.040234,"endencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 language"
K17-3014,E06-1011,0,0.360014,"Missing"
K17-3014,P82-1020,0,0.835619,"Missing"
K17-3014,P16-2091,0,0.0520545,"Missing"
K17-3014,Q16-1032,0,0.0276542,"Missing"
K17-3014,U16-1017,1,0.896512,"Missing"
K17-3014,Q16-1023,0,0.513708,"07b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013). Recent work shows that using deep learning in dependency parsing has obtained state-of-the-art performances. Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016). In addition, others propose novel neural architectures for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017). We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural networkbased Stack-propagation model for joint POS tagging and transition-based dependency parsing, result"
K17-3014,W03-3017,0,0.1546,"Missing"
K17-3014,P10-1001,0,0.0222126,"ic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden laye"
K17-3014,P14-1069,0,0.0119884,"r, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs bett"
K17-3014,P15-1032,0,0.0279264,"Missing"
K17-3014,W03-3023,0,0.190146,"Missing"
K17-3014,P15-1031,0,0.0311799,"Missing"
K17-3014,P17-1186,0,0.0153844,"Missing"
K17-3014,petrov-etal-2012-universal,0,0.0164754,"t of all possible dependency trees for the input sentence s while scorearc (h, m) measures the score of the arc between the head hth word and the modifier mth word in s. Following Kiperwasser and Goldberg (2016b), we score an arc by using a MLP with one-node output layer (MLParc ) on top of the BiLSTMctx : 3 scorearc (h, m) = MLParc (v h ◦ v m ) Experiments 3.1 Experimental setup Following Zhang and Weiss (2016) and Plank et al. (2016), we conduct multilingual experiments on 19 languages from the Universal Dependencies (UD) treebanks1 v1.2 (Nivre et al., 2015), using the universal POS tagset (Petrov et al., 2012) instead of the language specific POS tagset.2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS). LAS is the percentage of words which are correctly assigned both dependency arc and relation type. where v h and v m are the shared BiLSTM-based feature vectors representing the hth and mth words in s, respectively. We then compute a marginbased hinge loss Larc with loss-augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree (Kiperwasser and Goldberg, 2016b). Dependency relation types are predicted i"
K17-3014,C14-1168,0,0.0255266,"8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when the character-based representations of words are not taken into account. B’15 denotes the c"
K17-3014,N15-1005,0,0.0152957,"4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong basel"
K17-3014,P16-2067,0,0.17323,"uber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art. 2 BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1 , w2 , ..., wn , we represent each word wi in (•) s by an embedding ewi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTMseq ) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1 c2 ...ck , the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the j th character cj in w; and the output (∗) is the character-based embedding ew of the word type w, computed as: Our joint model e(∗) w = BiLSTMseq (c1"
K17-3014,P16-1147,0,0.0149578,"Missing"
K17-3014,D12-1046,0,0.015649,"Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint"
K17-3014,P11-2033,0,0.142463,"Missing"
K17-3014,P16-1131,0,0.0218431,"Missing"
K17-3014,L16-1680,0,0.129015,"3 69.5 70.3 72.4 73.6 66.8 5.4 84.5 82.4 83.6 83.9 84.7 84.9 2.3 79.4 78.0 73.4 75.4 79.2 82.5 10.6 81.3 79.9 79.5 81.3 80.4 79.3 3.4 80.2 80.1 79.4 80.7 81.8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5"
K17-3014,P16-1218,0,0.0176882,"Missing"
K18-2008,D15-1159,0,0.0496596,"Missing"
K18-2008,K17-3002,0,0.0999746,"Missing"
K18-2008,P16-1231,0,0.0582228,"Missing"
K18-2008,P15-1033,0,0.0752376,"Missing"
K18-2008,D15-1041,0,0.219879,"re and combined features (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011), while recent stateof-the-art models propose neural network architectures to handle feature-engineering (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017). Most traditional and neural network-based parsing models use automatically predicted POS tags as essential features. However, POS taggers are not perfect, resulting in error propagation problems. Some work has attempted to avoid using POS tags for dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015; de Lhoneux et al., 2017), however, to achieve the strongest parsing scores these methods still require automatically assigned POS tags. Alternatively, joint POS tagging and dependency parsing has also attracted a lot of attention in NLP community as it could help improve both tagging and parsing results over independent modeling (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Zhang et al., 2015; Zhang and Weiss, 2016; Yang et al., 2018). In this paper, we present a novel neural network-based model for jointly learning POS tagging and dependency paring. Our jo"
K18-2008,C96-1058,0,0.666084,") = LSTMf (c1:k ) ◦ LSTMr (ck:1 ) 2.2 scorearc (i, j) Tagging component = MLParc We feed the sequence of vectors e1:n with an additional context position index i into another BiLSTM (BiLSTMpos ), resulting in latent feature vec(pos) tors v i each representing the ith word wi in s: (pos) vi = BiLSTMpos (e1:n , i) where (v i ∗ v j ) and |v i − v j |denote the elementwise product and the absolute element-wise difference, respectively; and v i and v j are correspondingly the latent feature vectors associating to the ith and j th words in s, computed by Equation 5. Given the arc scores, we use the Eisner (1996)’s decoding algorithm to find the highest scoring projective parse tree: X score(s) = argmax scorearc (h, m) (7) (2) We use a MLP with softmax output (MLPpos ) on top of the BiLSTMpos to predict POS tag of each word in s. The number of nodes in the output layer of this MLPpos is the number of POS tags. (pos) Given v i , we compute an output vector as: ϑi = (pos) MLPpos (v i ) yˆ∈Y(s) (h,m)∈ˆ y where Y(s) is the set of all possible dependency trees for the input sentence s while scorearc (h, m) measures the score of the arc between the head hth word and the modifier mth word in s. Following Kip"
K18-2008,K18-2002,0,0.041237,"Missing"
K18-2008,P09-1087,0,0.158772,"Missing"
K18-2008,D12-1133,0,0.0664697,"Missing"
K18-2008,W06-2920,0,0.235373,"Missing"
K18-2008,H05-1091,0,0.364375,"Missing"
K18-2008,D14-1082,0,0.0650247,"computational resource, for experiments presented in Section 3, we perform a minimal grid search of hyper-parameters to select the number of BiLSTMpos and BiLSTMdep layers from {1, 2} and the size of LSTM hidden states in each layer from {128, 256}. For experiments presented in sections 4 and 5, we fix the number of BiLSTM layers at 2 and the size of hidden states at 128. 2.5 Experimental setup: We evaluate our model using the English WSJ Penn treebank (Marcus et al., 1993). We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test (Chen and Manning, 2014), employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (de Marneffe and Manning, 2008). Word embeddings are initialized by 100dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword (Pennington et al., 2014).2 As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256-dimensional LSTM hidden states (in Table 1, we present scores obtained on the development set when using 2 BiLSTM layers). 3 I"
K18-2008,D16-1257,0,0.0553598,"Missing"
K18-2008,D17-1206,0,0.0633592,"Missing"
K18-2008,P04-1054,0,0.263556,"Missing"
K18-2008,K17-3022,0,0.0964793,"Missing"
K18-2008,I11-1136,0,0.0682876,"Missing"
K18-2008,P15-1162,0,0.0672387,"Missing"
K18-2008,K17-3014,1,0.812718,"Missing"
K18-2008,Q16-1023,0,0.59467,"Australia {dqnguyen, karin.verspoor}@unimelb.edu.au Abstract (Reddy et al., 2016) and machine translation (Galley and Manning, 2009). In general, dependency parsing models can be categorized as graph-based (McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). Most traditional graph- or transition-based models define a set of core and combined features (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011), while recent stateof-the-art models propose neural network architectures to handle feature-engineering (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017). Most traditional and neural network-based parsing models use automatically predicted POS tags as essential features. However, POS taggers are not perfect, resulting in error propagation problems. Some work has attempted to avoid using POS tags for dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015; de Lhoneux et al., 2017), however, to achieve the strongest parsing scores these methods still require automatically assigned POS tags. Alternatively, joint POS tagging and dependency parsing has also attracted a lot of attention in NLP com"
K18-2008,W03-3017,0,0.364583,"Missing"
K18-2008,D16-1180,0,0.02384,"Missing"
K18-2008,P11-1089,0,0.126896,"Missing"
K18-2008,D11-1109,0,0.0792604,"Missing"
K18-2008,I17-1007,0,0.0345348,"Missing"
K18-2008,D14-1162,0,0.0813878,"n sections 4 and 5, we fix the number of BiLSTM layers at 2 and the size of hidden states at 128. 2.5 Experimental setup: We evaluate our model using the English WSJ Penn treebank (Marcus et al., 1993). We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test (Chen and Manning, 2014), employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (de Marneffe and Manning, 2008). Word embeddings are initialized by 100dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword (Pennington et al., 2014).2 As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256-dimensional LSTM hidden states (in Table 1, we present scores obtained on the development set when using 2 BiLSTM layers). 3 Implementation details Our model is released as jPTDP v2.0, available at https://github.com/datquocnguyen/ jPTDP. Our jPTDP v2.0 is implemented using DY N ET v2.0 (Neubig et al., 2017) with a fixed random seed.1 Word embeddings are initialized either randomly or by pre-trained w"
K18-2008,J93-2004,0,0.0605241,"dimensional character embeddings and 100dimensional POS tag embeddings. We also fix the number of hidden nodes in MLPs at 100. Due to limited computational resource, for experiments presented in Section 3, we perform a minimal grid search of hyper-parameters to select the number of BiLSTMpos and BiLSTMdep layers from {1, 2} and the size of LSTM hidden states in each layer from {128, 256}. For experiments presented in sections 4 and 5, we fix the number of BiLSTM layers at 2 and the size of hidden states at 128. 2.5 Experimental setup: We evaluate our model using the English WSJ Penn treebank (Marcus et al., 1993). We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test (Chen and Manning, 2014), employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (de Marneffe and Manning, 2008). Word embeddings are initialized by 100dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword (Pennington et al., 2014).2 As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiL"
K18-2008,P16-2067,0,0.09422,"e s consisting of n word tokens w1 , w2 , ..., wn , we represent each ith word wi in s by a vector ei . We obtain ei by concate(W) nating word embedding ewi and character-level (C) word embedding ewi : ei = e(wWi ) ◦ e(wCi) Our joint model (1) Here, each word type w in the training data is rep(W) resented by a real-valued word embedding ew . Given the word type w consisting of k characters w = c1 c2 ...ck where each jth character in w is represented by a character embedding cj , we use a sequence BiLSTM (BiLSTMseq ) to learn its character-level vector representation (Ballesteros et al., 2015; Plank et al., 2016). The input to BiLSTMseq is the sequence of k character embeddings c1:k , and the output is a concatenation of outputs of a forward LSTM (LSTMf ) reading This section presents our model for joint POS tagging and graph-based dependency parsing. Figure 1 illustrates the architecture of our joint model which can be viewed as a two-component mixture of a tagging component and a parsing component. Given word tokens in an input sentence, the tagging component uses a BiLSTM to learn “latent” feature vectors representing these word tokens. Then the tagging component feeds these feature vectors into a"
K18-2008,P05-1012,0,0.198355,"n, using only the gold labeled parse tree. Our parsing component can be viewed as an extension of the BIST graph-based dependency model (Kiperwasser and Goldberg, 2016), where we additionally incorporate the character-level vector representations of words. (4) We feed the sequence of vectors x1:n with an additional index i into a BiLSTM (BiLSTMdep ), resulting in latent feature vectors v i as follows: v i = BiLSTMdep (x1:n , i) (6)  v i ◦ v j ◦ (v i ∗ v j ) ◦ |v i − v j | (5) 2.4 Based on latent feature vectors v i , we follow a common arc-factored parsing approach to decode dependency arcs (McDonald et al., 2005). In particular, a dependency tree can be formalized as a directed graph. An arc-factored parsing approach learns the scores of the arcs in the graph (K¨ubler Joint model training The training objective loss of our joint model is the sum of the POS tagging loss LPOS , the structure loss LARC and the relation labeling loss LREL : L = LPOS + LARC + LREL 83 (9) The model parameters, including word embeddings, character embeddings, POS embeddings, three one-hidden-layer MLPs and three BiLSTMs, are learned to minimize the sum L of the losses. Most neural network-based joint models for POS tagging a"
K18-2008,E06-1011,0,0.117076,"Missing"
K18-2008,N15-1005,0,0.207378,"Missing"
K18-2008,P16-1147,0,0.0779833,"Missing"
K18-2008,P11-2033,0,0.139561,"Missing"
K18-2008,K17-3009,0,0.0588227,"Missing"
K18-2008,I05-2038,0,0.0287423,"odel is trained using a fixed set of hyper-parameters as mentioned in Section 2.5. 5 UniMelb in the EPE 2018 campaign scores. General background can be also found in the first EPE edition 2017 (Oepen et al., 2017). Unlike EPE 2017, the EPE 2018 campaign limited the training data to the English UD treebanks only. We unfortunately were unaware of this restriction during development of our model. Thus, we trained a jPTDP v2.0 model on dependency trees generated with the Stanford basic dependencies on a combination of the WSJ treebank, sections 02-21, and the training split of the GENIA treebank (Tateisi et al., 2005). We used the fixed set of hyper-parameters as used for the CoNLL 2018 shared task as mentioned in Section 2.5.11 We then submitted the parsing outputs by runOur UniMelb team also participated with jPTDP v2.0 in the 2018 Extrinsic Parser Evaluation (EPE) campaign (Fares et al., 2018).10 The EPE 2018 campaign runs in collaboration with the CoNLL 2018 shared task, which aims to evaluate dependency parsers by comparing their performance on three downstream tasks: biomedical event extraction (Bj¨orne et al., 2017), negation resolution (Lapponi et al., 2017) and opinion analysis (Johansson, 2017)."
K18-2008,N03-1033,0,0.344113,"Missing"
K18-2008,P15-1032,0,0.0383261,"Missing"
K18-2008,W03-3023,0,0.434999,"Missing"
K18-2008,K18-2001,0,0.0668916,"Missing"
K18-2008,K17-3001,0,0.0683429,"Missing"
K18-2008,E17-1063,0,0.0398596,"Missing"
L18-1410,neubig-mori-2010-word,0,0.024758,"ag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables. In this paper, we propose a novel method to Vietnamese word segmentation. Our method automatically constructs a Single Classification Ripple Down Rules (SCRDR) tree (Compton and Jansen, 1990) to correct wrong segmentations given by a longest matchingbased word segmenter. On the benchmark Vietnamese treebank (Nguyen et al., 2009), experimental results show that our method obtains better accuracy and performance speed than the previous state-of-the-art methods JVnSegmenter (Nguyen et"
L18-1410,Y06-1028,0,0.813957,"elves (Thang et al., 2008; Le et al., 2008), thus creating challenges in Vietnamese word segmentation (Nguyen et al., 2012). Many approaches are proposed for the Vietnamese word segmentation task. Le et al. (2008), Pham et al. (2009) and Tran et al. (2012) applied the maximum matching strategy (NanYuan and YanBin, 1991) to generate all possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) used n-gram language models while Pham et al. (2009) employed part-ofspeech (POS) information from an external POS tagger. In addition, Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b)"
L18-1410,W09-3035,0,0.228624,"ll possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) used n-gram language models while Pham et al. (2009) employed part-ofspeech (POS) information from an external POS tagger. In addition, Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables."
L18-1410,R11-1056,1,0.803088,"OS tag of the word “anticipate” instead of the initial POS tag “VB.” To correct a wrong conclusion returned for a given case, a new node containing a new exception rule may be attached to the last node in the evaluation path. If the last node’s rule is the last satisfied rule given the case, the new node is added as its child with the “except” edge; otherwise, the new node is attached with the “if-not” edge. SCRDR has been successfully applied in NLP tasks for temporal relation extraction (Pham and Hoffmann, 2006), word lemmatization (Plisson et al., 2008), POS tagging (Xu and Hoffmann, 2010; Nguyen et al., 2011b; Nguyen et al., 2014; Nguyen et al., 2016), named entity recognition (Nguyen and Pham, 2012) and question answering (Nguyen et al., 2011a; Nguyen et al., 2013; Nguyen et al., 2017a). The works by Plisson et al. (2008), Nguyen et al. (2011b), Nguyen et al. (2014) and Nguyen et al. (2016) build the tree autoFigure 2: Diagram of our approach. matically, while others manually construct the tree. 3. Our approach This section describes our new error-driven approach to automatically construct a SCRDR tree to correct wrong segmentations produced by an initial word segmenter. Following Nguyen et al."
L18-1410,W12-5005,0,0.0366239,"Missing"
L18-1410,E14-2005,1,0.884766,"ticipate” instead of the initial POS tag “VB.” To correct a wrong conclusion returned for a given case, a new node containing a new exception rule may be attached to the last node in the evaluation path. If the last node’s rule is the last satisfied rule given the case, the new node is added as its child with the “except” edge; otherwise, the new node is attached with the “if-not” edge. SCRDR has been successfully applied in NLP tasks for temporal relation extraction (Pham and Hoffmann, 2006), word lemmatization (Plisson et al., 2008), POS tagging (Xu and Hoffmann, 2010; Nguyen et al., 2011b; Nguyen et al., 2014; Nguyen et al., 2016), named entity recognition (Nguyen and Pham, 2012) and question answering (Nguyen et al., 2011a; Nguyen et al., 2013; Nguyen et al., 2017a). The works by Plisson et al. (2008), Nguyen et al. (2011b), Nguyen et al. (2014) and Nguyen et al. (2016) build the tree autoFigure 2: Diagram of our approach. matically, while others manually construct the tree. 3. Our approach This section describes our new error-driven approach to automatically construct a SCRDR tree to correct wrong segmentations produced by an initial word segmenter. Following Nguyen et al. (2006) and Tran et al."
L18-1410,U17-1013,1,0.664336,"Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables. In this paper, we propose a novel method to Vietnamese word segmentation. Our method automatically constructs a Single Classification Ripple Down Rules (SCRDR) tree (Compton and Jansen, 1990) to correct wrong segmentations given by a longest matchingbased word s"
L18-1410,dinh-etal-2008-word,0,0.574669,"Missing"
N07-2032,N06-1038,0,0.0334578,"ity pairs from Wikipedia’s English version. A binary relation is deﬁned as a triple (ep , rel, es ) in which ep and es are entities and rel indicates a directed relationship of ep and es . Current experiment limits entities and relations to a reasonable size in that an entity is classiﬁable as person, organization, location, artifact, year, month or date; and a relation can be founder, chairman, CEO, COO, president, director, vice chairman, spouse, birth date, birth place, foundation, product and location. To our knowledge, only one recent work has attempted relation extraction on Wikipedia: (Culotta et al., 2006) presents a probabilistic model to integrate extraction and mining tasks performed on biographical text of Wikipedia. Some other works (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) rely on the abundance of web data to obtain easy patterns and learn such patterns based mostly on lexical information. Rather than analyzing dependency path between entity pair proposed in (Bunescu and Mooney, 2006; Cui et al., Due to the encyclopedic style, each Wikipedia article mainly provides information for a speciﬁc entity and further mentions other entities related to it. Culotta et a"
N07-2032,P00-1023,0,0.00901959,"ofounder, founder headquartered, based, locate, headquarter, base, location, situate, located product, include, release, produce, service, operate, provide, market, manage, development, focus, manufacture, provider, launch, make, sell, introduce, producer, supplier, possess, retailer, design, involve, production, offering, serve, sale, supply marry, wife, married, husband, marriage 3.1 Principal Entity Detector This module detects all referring expressions of the principal entity in an article. All occurrences of identiﬁed expressions are labeled as mentions of the principal entity. We adopt (Morton, 2000) to classify the expressions into three types: (1) personal pronoun (2) proper noun (3) common nouns. Based on chunking information, we propose a simple technique to identify a set of referring expressions of the principal entity, denoted as F: (i) Start with F = {}. (ii) Select the ﬁrst two chunks for F: the proper chunk (nounphase with at least one proper noun) of the article title and the ﬁrst proper chunk in the ﬁrst sentence of the article, if any. If F is still empty, stop. (iii) For each remaining proper chunk p in the article, if p is derived from any expressions selected in (ii), then"
N07-2032,P02-1006,0,0.0418399,"xperiment limits entities and relations to a reasonable size in that an entity is classiﬁable as person, organization, location, artifact, year, month or date; and a relation can be founder, chairman, CEO, COO, president, director, vice chairman, spouse, birth date, birth place, foundation, product and location. To our knowledge, only one recent work has attempted relation extraction on Wikipedia: (Culotta et al., 2006) presents a probabilistic model to integrate extraction and mining tasks performed on biographical text of Wikipedia. Some other works (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) rely on the abundance of web data to obtain easy patterns and learn such patterns based mostly on lexical information. Rather than analyzing dependency path between entity pair proposed in (Bunescu and Mooney, 2006; Cui et al., Due to the encyclopedic style, each Wikipedia article mainly provides information for a speciﬁc entity and further mentions other entities related to it. Culotta et al. (2006) deﬁnes the entities as principal entity and secondary entity respectively. We predict only relationships between the principal entity and each mentioned secondary entity that contains a link to i"
N07-2032,P05-1053,0,\N,Missing
N07-2032,C02-1139,0,\N,Missing
N07-2032,P04-1054,0,\N,Missing
N07-2032,J01-4004,0,\N,Missing
N07-2032,P06-1104,0,\N,Missing
N16-1054,W13-3515,0,0.0697858,"Missing"
N16-1054,D15-1034,0,0.611959,"t) and the algorithms used to optimize the margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012) and L-BFGS (Liu and Nocedal, 1989). DISTMULT (Yang et al., 2015) is based on a Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b). The TransH model (Wang et al., 2014b) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model"
N16-1054,D15-1173,0,0.0381607,"n extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets is given in Table 2. 4.1 Task and evaluation protocol The"
N16-1054,D15-1038,0,0.0696033,"Missing"
N16-1054,P15-1067,0,0.418981,"ed on a Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b). The TransH model (Wang et al., 2014b) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142"
N16-1054,D11-1049,0,0.0156437,"as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets i"
N16-1054,D15-1082,0,0.544678,", 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b). The TransH model (Wang et al., 2014b) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,0"
N16-1054,D15-1191,0,0.118236,"el, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets is given in Table 2. 4.1 Task and evaluation protocol The link prediction ta"
N16-1054,N13-1090,0,0.0119687,"t are represented by vectors h and t ∈ Rk respectively. The Unstructured model (Bordes et al., 2012) assumes that h ≈ t. As the Unstructured model does not take the relationship r into account, it cannot distinguish different relation types. The Structured Embedding (SE) model (Bordes et al., 2011) extends the unstructured model by assuming that h and t are similar only in a relation-dependent subspace. It represents each relation r with two matrices Wr,1 and Wr,2 ∈ Rk×k , which are chosen so that Wr,1 h ≈ Wr,2 t. The TransE model (Bordes et al., 2013) is inspired by models such as Word2Vec (Mikolov et al., 2013) where relationships between words often correspond to translations in latent feature space. The TransE model represents each relation r by a translation vector r ∈ Rk , which is chosen so that h + r ≈ t. The primary contribution of this paper is that two very simple relation-prediction models, SE and TransE, can be combined into a single model, which we call STransE. Specifically, we use relationspecific matrices Wr,1 and Wr,2 as in the SE model to identify the relation-dependent aspects of both h and t, and use a vector r as in the TransE model to describe the relationship between h and t in"
N16-1054,P15-1016,0,0.0842811,"STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets is given in Table 2. 4.1 Tas"
N16-1054,N13-1008,0,0.190991,"Missing"
N16-1054,W15-4007,0,0.168876,", γ = 5, and k = 50 for WN18, and λ = 0.0001, γ = 1, and k = 100 for FB15k. 4.2 Main results Table 3 compares the link prediction results of our STransE model with results reported in prior work, using the same experimental setup. The first twelve rows report the performance of models that do not exploit information about alternative paths between head and tail entities. The next two rows report results of the RTransE and PTransE models, which are extensions of the TransE model that exploit information about relation paths. The last row presents results for the log-linear model Node+LinkFeat (Toutanova and Chen, 2015) which makes use of textual mentions derived from the large external ClueWeb-12 corpus. It is clear that Node+LinkFeat with the additional external corpus information obtained best results. In future work we plan to extend the STransE model to incorporate such additional information. Table 3 also shows that models RTransE and PTransE employing path information achieve better results than models that do not use such information. In terms of models not exploiting path information or external information, the STransE model scores better than 463 WN18 MR H10 SE (Bordes et al., 2011) 985 80.5 Unstr"
N16-1054,D15-1174,0,0.0642098,"Missing"
N16-1054,D14-1167,0,0.613922,"using additional information. Third, the more complex models that exploit external information are typically extensions of these simpler models, and are often initialized with parameters estimated by such simpler models, so improvements to the simpler models should yield corresponding improvements to the more complex models as well. Embedding models for KB completion associate entities and/or relations with dense feature vectors or matrices. Such models obtain state-of-the-art performance (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014b; Guu et al., 2015) and generalize to large KBs (Krompa et al., 2015). Table 1 summarizes a number of prominent embedding models for KB completion. Let (h, r, t) represent a triple. In all of the models 460 Proceedings of NAACL-HLT 2016, pages 460–466, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Model Score function fr (h, t) Opt. k×k SE kWr,1 h − Wr,2 tk`1/2 ; Wr,1 , Wr,2 ∈ R SGD Unstructured kh − tk`1/2 SGD TransE kh + r − tk`1/2 ; r ∈ Rk > SGD k×k DISTMULT h Wr t ; Wr is a diagonal matrix ∈ R NTN > d k×k×d u> ; Wr,1 , Wr,2 ∈ Rd×k r tanh(h Mr t"
N16-1054,P16-1136,0,\N,Missing
N16-1054,N16-1105,0,\N,Missing
N16-1054,K16-1005,1,\N,Missing
N16-1054,P16-1124,0,\N,Missing
N16-1054,D16-1145,0,\N,Missing
N16-1054,C16-1062,0,\N,Missing
N16-1054,E17-1013,0,\N,Missing
N18-2053,N18-1133,0,0.2349,"Missing"
N18-2053,D15-1038,0,0.0405912,"Missing"
N18-2053,P17-1021,0,0.00839514,"hmark datasets WN18RR and FB15k-237. 1 Introduction Large-scale knowledge bases (KBs), such as YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), are usually databases of triples representing the relationships between entities in the form of fact (head entity, relation, tail entity) denoted as (h, r, t), e.g., (Melbourne, cityOf, Australia). These KBs are useful resources in many applications such as semantic searching and ranking (Kasneci et al., 2008; Schuhmacher and Ponzetto, 2014; Xiong et al., 2017), question answering (Zhang et al., 2016; Hao et al., 2017) and machine reading (Yang and Mitchell, 2017). However, the KBs are 327 Proceedings of NAACL-HLT 2018, pages 327–333 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics has shown that using relation paths between entities in the KBs could help to get contextual information for improving KB completion performance (Lin et al., 2015a; Luo et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016a). See other embedding models for KB completion in Nguyen (2017). Model The score function f (h, r, t) TransE DISTMULT ComplEx ConvE kv h + v r -"
N18-2053,K16-1005,1,0.807601,"r and t respectively). That is, a TransE score kv h + v r − v t kp of the valid triple (h, r, t) should be close to 0 and smaller than a score kv h0 + v r0 − v t0 kp of an invalid triple (h’, r’, t’). The transitional characteristic in TransE also implies the global relationships among same dimensional entries of v h , v r and v t . Other transition-based models extend TransE to additionally use projection vectors or matrices to translate head and tail embeddings into the relation vector space, such as: TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransD (Ji et al., 2015), STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016). Furthermore, DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) use a tri-linear dot product to compute the score for each triple. Recent research In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3c"
N18-2053,P15-1067,0,0.376345,"d v t are embeddings of h, r and t respectively). That is, a TransE score kv h + v r − v t kp of the valid triple (h, r, t) should be close to 0 and smaller than a score kv h0 + v r0 − v t0 kp of an invalid triple (h’, r’, t’). The transitional characteristic in TransE also implies the global relationships among same dimensional entries of v h , v r and v t . Other transition-based models extend TransE to additionally use projection vectors or matrices to translate head and tail embeddings into the relation vector space, such as: TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransD (Ji et al., 2015), STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016). Furthermore, DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) use a tri-linear dot product to compute the score for each triple. Recent research In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail"
N18-2053,N16-1054,1,0.877135,"r and t respectively). That is, a TransE score kv h + v r − v t kp of the valid triple (h, r, t) should be close to 0 and smaller than a score kv h0 + v r0 − v t0 kp of an invalid triple (h’, r’, t’). The transitional characteristic in TransE also implies the global relationships among same dimensional entries of v h , v r and v t . Other transition-based models extend TransE to additionally use projection vectors or matrices to translate head and tail embeddings into the relation vector space, such as: TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransD (Ji et al., 2015), STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016). Furthermore, DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) use a tri-linear dot product to compute the score for each triple. Recent research In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3c"
N18-2053,D14-1181,0,0.00218773,"∗ Ω)) · w Table 1: The score functions in previous SOTA models and in our ConvKB model. kvkp denotes the p-norm P of v. hv h , v r , v t i = i v hi v ri v ti denotes a tri-linear dot product. g denotes a non-linear function. ∗ denotes a convolution operator. · denotes a dot product. concat b denotes a 2D redenotes a concatenation operator. v shaping of v. Ω denotes a set of filters. Recently, convolutional neural networks (CNNs), originally designed for computer vision (LeCun et al., 1998), have significantly received research attention in natural language processing (Collobert et al., 2011; Kim, 2014). CNN learns non-linear features to capture complex relationships with a remarkably less number of parameters compared to fully connected neural networks. Inspired from the success in computer vision, Dettmers et al. (2018) proposed ConvE—the first model applying CNN for the KB completion task. In ConvE, only v h and v r are reshaped and then concatenated into an input matrix which is fed to the convolution layer. Different filters of the same 3 × 3 shape are operated over the input matrix to output feature map tensors. These feature map tensors are then vectorized and mapped into a vector via"
N18-2053,D15-1082,0,0.620943,"r ≈ v t (here, v h , v r and v t are embeddings of h, r and t respectively). That is, a TransE score kv h + v r − v t kp of the valid triple (h, r, t) should be close to 0 and smaller than a score kv h0 + v r0 − v t0 kp of an invalid triple (h’, r’, t’). The transitional characteristic in TransE also implies the global relationships among same dimensional entries of v h , v r and v t . Other transition-based models extend TransE to additionally use projection vectors or matrices to translate head and tail embeddings into the relation vector space, such as: TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransD (Ji et al., 2015), STransE (Nguyen et al., 2016b) and TranSparse (Ji et al., 2016). Furthermore, DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) use a tri-linear dot product to compute the score for each triple. Recent research In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple ("
N18-2053,W15-4007,0,0.280479,"mputed with a weight vector via a dot product to produce a score for the triple (h, r, t). This score is used to infer whether the triple (h, r, t) is valid or not. Our contributions in this paper are as follows: • We introduce ConvKB—a novel embedding model of entities and relationships for knowledge base completion. ConvKB models the relationships among same dimensional entries of the embeddings. This implies that ConvKB generalizes transitional characteristics in transition-based embedding models. • We evaluate ConvKB on two benchmark datasets: WN18RR (Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015). Experimental results show that ConvKB obtains better link prediction performance than previous SOTA embedding models. In particular, ConvKB obtains the best mean rank and the highest Hits@10 on WN18RR, and produces the highest mean reciprocal rank and highest Hits@10 on FB15k-237. In this paper, we present ConvKB—an embedding model which proposes a novel use of CNN for the KB completion task. In ConvKB, each entity or relation is associated with an unique kdimensional embedding. Let v h , v r and v t denote k-dimensional embeddings of h, r and t, respectively. For each triple (h, r, t), the"
N18-2053,D15-1191,0,0.0142285,". These KBs are useful resources in many applications such as semantic searching and ranking (Kasneci et al., 2008; Schuhmacher and Ponzetto, 2014; Xiong et al., 2017), question answering (Zhang et al., 2016; Hao et al., 2017) and machine reading (Yang and Mitchell, 2017). However, the KBs are 327 Proceedings of NAACL-HLT 2018, pages 327–333 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics has shown that using relation paths between entities in the KBs could help to get contextual information for improving KB completion performance (Lin et al., 2015a; Luo et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016a). See other embedding models for KB completion in Nguyen (2017). Model The score function f (h, r, t) TransE DISTMULT ComplEx ConvE kv h + v r - v t kp hv h , v r , v t i Re (hv h , v r , v t i) br ) ∗ Ω)) W ) · v t g (vec (g (concat (b vh, v ConvKB concat (g ([v h , v r , v t ] ∗ Ω)) · w Table 1: The score functions in previous SOTA models and in our ConvKB model. kvkp denotes the p-norm P of v. hv h , v r , v t i = i v hi v ri v ti denotes a tri-linear dot product. g denotes a non-linear function. ∗ denotes a convolution operat"
N18-2053,P17-1132,0,0.0220779,"Introduction Large-scale knowledge bases (KBs), such as YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), are usually databases of triples representing the relationships between entities in the form of fact (head entity, relation, tail entity) denoted as (h, r, t), e.g., (Melbourne, cityOf, Australia). These KBs are useful resources in many applications such as semantic searching and ranking (Kasneci et al., 2008; Schuhmacher and Ponzetto, 2014; Xiong et al., 2017), question answering (Zhang et al., 2016; Hao et al., 2017) and machine reading (Yang and Mitchell, 2017). However, the KBs are 327 Proceedings of NAACL-HLT 2018, pages 327–333 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics has shown that using relation paths between entities in the KBs could help to get contextual information for improving KB completion performance (Lin et al., 2015a; Luo et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016a). See other embedding models for KB completion in Nguyen (2017). Model The score function f (h, r, t) TransE DISTMULT ComplEx ConvE kv h + v r - v t kp hv h , v r , v t i Re (hv h , v r , v t"
N18-5012,Y06-1028,0,0.124858,"Missing"
N18-5012,N16-1031,0,0.0230142,"Missing"
N18-5012,P15-1038,0,0.0700121,"Missing"
N18-5012,U16-1017,1,0.928447,"is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file, so users do not have to install external dependencies. Users can run processing pipelines from either the command-line or the Java API. • Fast – VnCoreNLP is fast, so it can be used for dealing with large-scale data. Also it benefits users suffering from limited computation resources (e.g. users from Vietnam). • Accurate – VnCoreNLP components obtain higher results than all previous published results on the same benchmark datasets. 56 Proceedings of NAACL-HLT 2018: Demonstrations, pages 56–60 c New Orleans, Louis"
N18-5012,K17-3014,1,0.886035,"Missing"
N18-5012,Q16-1023,0,0.121699,"Missing"
N18-5012,N16-1030,0,0.0236039,"ns present evaluations for the NER (ner) and dependency parsing (parse) components. 4.1 Models: We make an empirical comparison between the VnCoreNLP’s NER component and the following neural network-based models: • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of BiLSTM-CRF, using CNN to derive character-based word representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is an extension of BiLSTM-CRF, using BiLSTM to derive the character-based word representations (Lample et al., 2016). Named entity recognition We make a comparison between SOTA featurebased and neural network-based models, which, to the best of our knowledge, has not been done in any prior work on Vietnamese NER. • BiLSTM-CRF+POS is another extension to BiLSTM-CRF, incorporating embeddings of automatically predicted POS tags (Reimers and Gurevych, 2017). Dataset: The NER shared task at the 2016 VLSP workshop provides a set of 16,861 manually annotated sentences for training and development, and a set of 2,831 manually annotated sentences for test, with four NER labels PER, LOC, ORG and MISC. Note that in bo"
N18-5012,L18-1410,1,0.651006,"Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội.""); pipeline.annotate(annotation); String annotatedStr = annotation. toString(); • wseg – Unlike English where white space is a strong indicator of word boundaries, when written in Vietnamese white space is also used to separate syllables that constitute words. So word segmentation is referred to as the key first step in Vietnamese NLP. We have proposed a transformation rule-based learning model for Vietnamese word segmentation, which obtains better segmentation accuracy and speed than all previous word segmenters. See details in Nguyen et al. (2018). Listing 1: Minimal code for an analysis pipeline. In addition, Listing 2 provides a more realistic and complete example code, presenting key components of the toolkit. Here an annotation pipeline can be used for any text rather than just a single sentence, e.g. for a paragraph or entire news story. 3 Components This section briefly describes each component of VnCoreNLP. Note that our goal is not to develop 57 • pos – To label words with their POS tag, we apply MarMoT which is a generic CRF framework and a SOTA POS and morphological tagger (Mueller et al., 2013).1 gold POS tags are not availa"
N18-5012,U17-1013,1,0.89339,"Missing"
N18-5012,W09-3035,0,0.236133,"Missing"
N18-5012,2010.jeptalnrecital-long.36,0,0.116472,"dependency treebank was published in 2014 (Nguyen et al., 2014); and an NER dataset was released for the second VLSP campaign in 2016. So there is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file, so users do not have to install external dependencies. Users can run processing pipelines from either the command-line or the Java API. • Fast – VnCoreNLP is fast, so it can be used for dealing with large-scale data. Also it benefits users suffering from limited computation resources (e.g. users from Vietnam). • Accurate – VnCoreNLP components obtain higher results than all"
N18-5012,P16-1101,0,0.0801222,"LP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https:// github.com/vncorenlp/VnCoreNLP. 1 Figure 1: In pipeline architecture of VnCoreNLP, annotations are performed on an Annotation object. Pham et al. (2017) built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). However, Pham et al. (2017) did not make a comparison to SOTA traditional feature-based models. In addition, NNVLP is slow with a processing speed at about 300 words per second, which is not practical for real-world application such as dealing with large-scale data. In this paper, we present a Java NLP toolkit for Vietnamese, namely VnCoreNLP, which aims to facilitate Vietnamese NLP research by providing rich linguistic annotations through key NLP components of word segmentation, POS tagging, NER and dependency parsing. Figure 1 describes the overall system architecture. The following items"
N18-5012,P14-5010,0,0.0129689,"tively explored in the last decade, boosted by the successes of the 4-year KC01.01/2006-2010 national project on Vietnamese language and speech processing (VLSP). Over the last 5 years, standard benchmark datasets for key Vietnamese NLP tasks are publicly available: datasets for word segmentation and POS tagging were released for the first VLSP evaluation campaign in 2013; a dependency treebank was published in 2014 (Nguyen et al., 2014); and an NER dataset was released for the second VLSP campaign in 2016. So there is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file,"
N18-5012,I17-3010,0,0.0466753,"ely VnCoreNLP—a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https:// github.com/vncorenlp/VnCoreNLP. 1 Figure 1: In pipeline architecture of VnCoreNLP, annotations are performed on an Annotation object. Pham et al. (2017) built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). However, Pham et al. (2017) did not make a comparison to SOTA traditional feature-based models. In addition, NNVLP is slow with a processing speed at about 300 words per second, which is not practical for real-world application such as dealing with large-scale data. In this paper, we present a Java NLP toolkit for Vietnamese, namely VnCoreNLP, which aims to facilitate Vietnamese NLP research by providing rich linguistic annotations through key NLP components of word segmentat"
N18-5012,P05-1012,0,0.395501,"Missing"
N18-5012,D17-1035,0,0.0299672,"Missing"
N18-5012,D13-1032,0,0.135735,"Missing"
N19-1226,N18-1133,0,0.0536009,"Missing"
N19-1226,P15-1067,0,0.539459,"learning rate at 5e−5 . significant with p &lt; 0.05 using the paired t-test). To illustrate our training progress, we plot performances of CapsE on the validation set over epochs in Figure 4. We observe that the performance is improved with the increase in the number of filters since capsules can encode more useful properties for a large embedding size. 5 Related work Other transition-based models extend TransE to additionally use projection vectors or matrices to translate embeddings of s and o into the vector space of r, such as: TransH (Wang et al., 2014), TransR (Lin et al., 2015b), TransD (Ji et al., 2015) and STransE (Nguyen et al., 2016b). Furthermore, DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016) use a tri-linear dot product to compute the score for each triple. Moreover, ConvKB (Nguyen et al., 2018) applies convolutional neural network, in which feature maps are concatenated into a single feature vector which is then computed with a weight vector via a dot For search tasks, unlike classical methods, personalized search systems utilize the historical interactions between the user and the search system, such as submitted queries and clicked documents to tailor returned res"
N19-1226,D15-1082,0,0.524113,"s et al., 2013), i.e., not taking any corrupted triples that appear in the KG into accounts. We rank the valid test triple and corrupted triples in descending order of their scores. We employ evaluation metrics: mean rank (MR), mean reciprocal rank (MRR) and Hits@10 (i.e., the proportion of the valid test triples ranking in top 10 predictions). Lower MR, higher MRR or higher Hits@10 indicate better performance. Final scores on the test set are reported for the model obtaining the highest Hits@10 on the validation set. Training protocol: We use the common Bernoulli strategy (Wang et al., 2014; Lin et al., 2015b) when sampling invalid triples. For WN18RR, Pinter and Eisenstein (2018)2 found a strong evidence to support the necessity of a WordNet-related semantic setup, in which they averaged pre-trained word embeddings for word surface forms within the WordNet to create synset embeddings, and then used these synset embeddings to initialize entity embeddings for training their TransE association model. We follow this evidence in using the pre-trained 100-dimensional Glove word embeddings (Pennington et al., 2014) to train a TransE model on WN18RR. 2 Pinter and Eisenstein (2018) considered WN18RR and"
N19-1226,N18-2053,1,0.331292,"by Vu et al. (2017). Previous studies have shown the effectiveness of modeling triple for either KG completion or search personalization. However, there has been no single study investigating the performance on both tasks. Conventional embedding models, such as TransE (Bordes et al., 2013), DISTMULT (Yang et al., 2015) and ComplEx (Trouillon et al., 2016), use addition, subtraction or simple multiplication operators, thus only capture the linear relationships between entities. Recent research has raised interest in applying deep neural networks to triplebased prediction problems. For example, Nguyen et al. (2018) proposed ConvKB—a convolutional neural network (CNN)-based model for KG completion and achieved state-of-the-art results. Most of KG embedding models are constructed to modeling entries at the same dimension of the given triple, where presumably each dimension captures some relation-specific attribute of entities. To the 2180 Proceedings of NAACL-HLT 2019, pages 2180–2189 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics best of our knowledge, however, none of the existing models has a “deep” architecture for modeling the entries in a triple at th"
N19-1226,K16-1005,1,0.898244,"word embeddings (Pennington et al., 2014) to train a TransE model on WN18RR. 2 Pinter and Eisenstein (2018) considered WN18RR and evaluated their M3GM model only for 7 relations as they employed the inverse rule model (Dettmers et al., 2018) for 4 remaining symmetric relations. Regarding a fair comparison to other models, we use the M3GM implementation released by Pinter and Eisenstein (2018) to re-train and re-evaluate the M3GM model for all 11 relations. We thank Pinter and Eisenstein (2018) for their assistance running their code. We employ the TransE and ConvKB implementations provided by Nguyen et al. (2016b) and Nguyen et al. (2018). For ConvKB, we use a new process of training up to 100 epochs and monitor the Hits@10 score after every 10 training epochs to choose optimal hyper-parameters with the Adam initial learning rate in {1e−5 , 5e−5 , 1e−4 } and the number of filters N in {50, 100, 200, 400}. We obtain the highest Hits@10 scores on the validation set when using N= 400 and the initial learning rate 5e−5 on WN18RR; and N= 100 and the initial learning rate 1e−5 on FB15k-237. Like in ConvKB, we use the same pre-trained entity and relation embeddings produced by TransE to initialize entity an"
N19-1226,N16-1054,1,0.915774,"word embeddings (Pennington et al., 2014) to train a TransE model on WN18RR. 2 Pinter and Eisenstein (2018) considered WN18RR and evaluated their M3GM model only for 7 relations as they employed the inverse rule model (Dettmers et al., 2018) for 4 remaining symmetric relations. Regarding a fair comparison to other models, we use the M3GM implementation released by Pinter and Eisenstein (2018) to re-train and re-evaluate the M3GM model for all 11 relations. We thank Pinter and Eisenstein (2018) for their assistance running their code. We employ the TransE and ConvKB implementations provided by Nguyen et al. (2016b) and Nguyen et al. (2018). For ConvKB, we use a new process of training up to 100 epochs and monitor the Hits@10 score after every 10 training epochs to choose optimal hyper-parameters with the Adam initial learning rate in {1e−5 , 5e−5 , 1e−4 } and the number of filters N in {50, 100, 200, 400}. We obtain the highest Hits@10 scores on the validation set when using N= 400 and the initial learning rate 5e−5 on WN18RR; and N= 100 and the initial learning rate 1e−5 on FB15k-237. Like in ConvKB, we use the same pre-trained entity and relation embeddings produced by TransE to initialize entity an"
N19-1226,D14-1162,0,0.100801,"the validation set. Training protocol: We use the common Bernoulli strategy (Wang et al., 2014; Lin et al., 2015b) when sampling invalid triples. For WN18RR, Pinter and Eisenstein (2018)2 found a strong evidence to support the necessity of a WordNet-related semantic setup, in which they averaged pre-trained word embeddings for word surface forms within the WordNet to create synset embeddings, and then used these synset embeddings to initialize entity embeddings for training their TransE association model. We follow this evidence in using the pre-trained 100-dimensional Glove word embeddings (Pennington et al., 2014) to train a TransE model on WN18RR. 2 Pinter and Eisenstein (2018) considered WN18RR and evaluated their M3GM model only for 7 relations as they employed the inverse rule model (Dettmers et al., 2018) for 4 remaining symmetric relations. Regarding a fair comparison to other models, we use the M3GM implementation released by Pinter and Eisenstein (2018) to re-train and re-evaluate the M3GM model for all 11 relations. We thank Pinter and Eisenstein (2018) for their assistance running their code. We employ the TransE and ConvKB implementations provided by Nguyen et al. (2016b) and Nguyen et al. ("
N19-1226,D18-1201,0,0.0919518,"ppear in the KG into accounts. We rank the valid test triple and corrupted triples in descending order of their scores. We employ evaluation metrics: mean rank (MR), mean reciprocal rank (MRR) and Hits@10 (i.e., the proportion of the valid test triples ranking in top 10 predictions). Lower MR, higher MRR or higher Hits@10 indicate better performance. Final scores on the test set are reported for the model obtaining the highest Hits@10 on the validation set. Training protocol: We use the common Bernoulli strategy (Wang et al., 2014; Lin et al., 2015b) when sampling invalid triples. For WN18RR, Pinter and Eisenstein (2018)2 found a strong evidence to support the necessity of a WordNet-related semantic setup, in which they averaged pre-trained word embeddings for word surface forms within the WordNet to create synset embeddings, and then used these synset embeddings to initialize entity embeddings for training their TransE association model. We follow this evidence in using the pre-trained 100-dimensional Glove word embeddings (Pennington et al., 2014) to train a TransE model on WN18RR. 2 Pinter and Eisenstein (2018) considered WN18RR and evaluated their M3GM model only for 7 relations as they employed the inver"
N19-1226,W15-4007,0,0.13142,"hose length is used as a score for the triple. Finally, this score is used to predict whether the triple (s, r, o) is valid or not. In summary, our main contributions from this paper are as follows: • We propose an embedding model CapsE using the capsule network (Sabour et al., 2017) for modeling relationship triples. To our best of knowledge, our work is the first consideration of exploring the capsule network to knowledge graph completion and search personalization. • We evaluate our CapsE for knowledge graph completion on two benchmark datasets WN18RR (Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015). CapsE obtains the best mean rank on WN18RR and the highest mean reciprocal rank and highest Hits@10 on FB15k-237. • We restate the prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. We adapt our model to search personalization and evaluate on SEARCH17 (Vu et al., 2017) – a dataset of the web search query logs. Experimental results show that our CapsE achieves the new state-of-the-art results with significant improvements over strong baselines. 2 The proposed CapsE Let G be a collection of valid factual triples i"
P17-1121,J08-1001,0,0.360822,"distinguishes a coherent text from a random sequence of sentences is that it binds the sentences together to express a meaning as a whole — the interpretation of a sentence usually depends on the meaning of its neighbors. Coherence models that can distinguish a coherent from incoherent texts have a wide range of applications in text generation, summarization, and coherence scoring. Several formal theories of coherence have been proposed (Mann and Thompson, 1988a; Grosz et al., 1995; Asher and Lascarides, 2003), and their principles have inspired development of many existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014). Among these models, the entity grid (Barzilay and Lapata, 2008), which is based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years. As shown in Figure 1, the entity grid model represents a text by a grid that captures how ∗ Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models"
P17-1121,N10-1099,0,0.166112,"Missing"
P17-1121,P08-2011,0,0.258569,"removed in turn, and an insertion place is located for which the model gives the highest coherence score to the document. The insertion score is then computed as the average fraction of sentences per document reinserted in their actual position. Discrimination can be easier for longer documents, since a random permutation is likely to be different than the original one. Insertion is a much more difficult task since the candidate documents differ only by the position of one sentence. Dataset: For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by Elsner and Charniak (2008, 2011); Lin et al. (2011); Feng et al. (2014). Table 1 gives basic statistics about the dataset. Following previous works, we use 20 random permutations of each article, and we exclude permutations that match the original document.5 The fourth column (# Pairs) in Table 1 shows the resulting number of (original, permuted) pairs used for training our model and for testing in the discrimination task. Some previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively. Each of"
P17-1121,P11-2022,0,0.300747,"based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years. As shown in Figure 1, the entity grid model represents a text by a grid that captures how ∗ Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). While the entity grid and its extensions have been successful in many applications, they are limited in several ways. First, they use discrete representation for grammatical roles and features, which prevents the model from considering sufficiently long transitions (Bengio et al., 2003). Second, feature vector computation in existing models is decoupled from the target task, which limits the model’s capacity to learn task-specific features. In this paper, we propose a neural architecture for coherence assessm"
P17-1121,E12-1032,0,0.507614,"ata (2005, 2008) introduced the entity grid representation of discourse to model local coherence that captures the distribution of discourse entities across sentences in a text. They also introduced three tasks to evaluate the performance of coherence models: discrimination, summary coherence rating, and readability. 11 Since we do not have access to the output of their systems, we could not do a significance test for this task. A number of extensions of the basic entity grid model has been proposed. Elsner and Charniak (2011) included entity-specific features to distinguish between entities. Feng and Hirst (2012) used the basic grid representation, but improved its learning to rank scheme. Their model learns not only from original document and its permutations but also from ranking preferences among the permutations themselves. Guinaudeau and Strube (2013) convert a standard entity grid into a bipartite graph representing entity occurrences in sentences. To model local entity transition, the method constructs a directed projection graph representing the connection between adjacent sentences. Two sentences have a connected edge if they share at least one entity in common. The coherence score of the doc"
P17-1121,C14-1089,0,0.691858,"a number of improvements over the years. As shown in Figure 1, the entity grid model represents a text by a grid that captures how ∗ Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). While the entity grid and its extensions have been successful in many applications, they are limited in several ways. First, they use discrete representation for grammatical roles and features, which prevents the model from considering sufficiently long transitions (Bengio et al., 2003). Second, feature vector computation in existing models is decoupled from the target task, which limits the model’s capacity to learn task-specific features. In this paper, we propose a neural architecture for coherence assessment that can capture long range entity transitions along with arbitrary entityspecif"
P17-1121,W07-2321,0,0.109213,"Missing"
P17-1121,J95-2003,0,0.904306,"Missing"
P17-1121,P13-1010,0,0.68095,"ment with pairwise coherence rankings given by humans judges; see (Barzilay and Lapata, 2008) for details on the annotation method. There are 144 pairs of summaries for training and 80 pairs for testing. 5 Experiments In this section, we present our experiments — the models we compare, their settings, and the results. 5.1 Models Compared We compare our coherence model against a random baseline and several existing models. Random: The Random baseline makes a random decision for the evaluation tasks. Graph-based Model: This is the graph-based unsupervised model proposed by Guinaudeau and Strube (2013). We use the implementation from the cohere7 toolkit (Smith et al., 2016), and run it on the test set with syntactic projection (command line option ‘projection=3’) for graph construction. This setting yielded best scores for this model. Distributed Sentence Model: Li and Hovy (2014) proposed this neural model for measuring 6 7 https://bitbucket.org/melsner/browncoherence https://github.com/karins/CoherenceFramework text coherence. The model first encodes each sentence in a document into a fixed-length vector using a recurrent or a recursive neural network. Then it computes the coherence score"
P17-1121,P14-1062,0,0.00454977,"ion of k vectors in the lookup layer representing a transition of length k for entity ej in the grid, bt is a bias 1322 Figure 2: Neural architecture for modeling local coherence and the pairwise training method. term, and f is a nonlinear activation function, e.g., ReLU (Nair and Hinton, 2010) in our model. We apply this filter to each possible k-length transitions of different entities in the grid to generate a feature map, hi = [h1 , · · · , hm.n+k−1 ]. We repeat this process N times with N different filters to get N different feature maps (Figure 2). Notice that we use a wide convolution (Kalchbrenner et al., 2014), as opposed to narrow, to ensure that the filters reach entire columns of a grid, including the boundary entities. This is done by performing zero-padding, where out-of-range (i.e., for t &lt; 0 or t > {m, n}) vectors are assumed to be zero. Convolutional filters learn to compose local transition features of a grid into higher-level representations automatically. Since it operates over the distributed representation of grid entries, compared to traditional grid models, the transition length k can be sufficiently large (e.g., 5 − 8 in our experiments) to capture long-range transitional dependenci"
P17-1121,D14-1181,0,0.0121286,"dels, we assume there is no spatio-temporal relation between the entities in a document. In other words, columns in a grid are treated independently. • We are interested in modeling entity transitions of arbitrary lengths in a location-invariant way. This means, we aim to compose local patches of entity transitions into higher-level representations, while treating the patches independently of their position in the entity grid. Under these assumptions, the natural choice to tackle this problem is to use a convolutional approach, used previously to solve other NLP tasks (Collobert et al., 2011; Kim, 2014). Convolution layer: A convolution operation involves applying a filter w ∈ Rk.d (i.e., a vector of weight parameters) to each entity transition of length k to produce a new abstract feature ht = f (wT Lt:t+k−1,j + bt ) (2) where Lt:t+k−1,j denotes the concatenation of k vectors in the lookup layer representing a transition of length k for entity ej in the grid, bt is a bias 1322 Figure 2: Neural architecture for modeling local coherence and the pairwise training method. term, and f is a nonlinear activation function, e.g., ReLU (Nair and Hinton, 2010) in our model. We apply this filter to eac"
P17-1121,D14-1218,0,0.571069,"Missing"
P17-1121,D14-1220,0,0.0517117,"Missing"
P17-1121,P11-1100,0,0.719802,"Missing"
P17-1121,D12-1106,0,0.559092,"tect each NP as a label Lnp ∈ {new, old}. The coherenceQscore of the document is then estimated by np:N P s P (Lnp |np). In this work, they also estimate text coherence through pronoun coreference modeling. Lin et al. (2011) assume that a coherent text has certain discourse relation patterns. Instead of modeling entity transitions, they model discourse role transitions between sentences. In a follow up work, Feng et al. (2014) trained the same model but using features derived from deep discourse structures annotated with Rhetorical Structure Theory or RST (Mann and Thompson, 1988b) relations. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns. In recent years, there has been a growing interest in neuralizing traditional NLP approaches – language modeling (Bengio et al., 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al., 2013), and discourse parsing (Li et al., 2014), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models. Li and Hovy (2014) also proposed a 1327 neural framework to compute"
P17-1121,P10-1158,0,0.0129557,"1) proposed a number of improvements. They initially show significant improvement by including non-head nouns (i.e., nouns that do not head NPs) as entities in the grid.2 Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011). 2.1 Limitations of Entity Grid Models Despite its success, existing entity grid models are limited in several ways. • Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k with R different grammatical roles"
P17-1121,P10-1056,0,0.0199359,"w significant improvement by including non-head nouns (i.e., nouns that do not head NPs) as entities in the grid.2 Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011). 2.1 Limitations of Entity Grid Models Despite its success, existing entity grid models are limited in several ways. • Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k with R different grammatical roles, the basic entity grid model needs to compute Rk"
P17-1121,L16-1649,0,0.0300613,"Missing"
P17-1121,P13-1045,0,0.00971264,"n a follow up work, Feng et al. (2014) trained the same model but using features derived from deep discourse structures annotated with Rhetorical Structure Theory or RST (Mann and Thompson, 1988b) relations. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns. In recent years, there has been a growing interest in neuralizing traditional NLP approaches – language modeling (Bengio et al., 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al., 2013), and discourse parsing (Li et al., 2014), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models. Li and Hovy (2014) also proposed a 1327 neural framework to compute the coherence score of a document by estimating coherence probability for every window of L sentences (in their experiments, L = 3). First, they use a recurrent or a recursive neural network to compute the representation for each sentence in L from its words and their pre-trained embeddings. Then the concatenated vector is passed through a non-linear hidden layer, and finally the outpu"
P17-1121,P06-2103,0,0.0490742,"e grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011). 2.1 Limitations of Entity Grid Models Despite its success, existing entity grid models are limited in several ways. • Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k with R different grammatical roles, the basic entity grid model needs to compute Rk tran2 1321 They match the nouns to detect coreferent entities. sition probabilities from a grid. One can imagine that the estimated distri"
P17-1121,P05-1018,0,\N,Missing
P18-1052,P15-2113,1,0.864539,"Missing"
P18-1052,P13-1010,0,0.148961,"erformance. We present our coherence model for asynchronous conversation in the next section. 3.1 Neural Entity Grid Figure 1 depicts the neural grid model of Nguyen and Joty (2017). Given an entity grid E, they first transform each entry Ei,j (a grammatical role) into a distributed representation of d dimensions by looking up a shared embedding matrix M ∈ R|G|×d , where G is the vocabulary of possible grammatical roles, i.e., G = {S, O, X, −}. Formally, the look-up operation can be expressed as: h i L = M (E1,1 ) · · · M (Ei,j ) · · · M (EI,J ) (1) Other Existing Models Guinaudeau and Strube (2013) proposed a graphbased unsupervised method. They convert an entity grid into a bipartite graph consisting of two sets of nodes, representing sentences and entities, respectively. The edges are assigned weights based on the grammatical role of the entities in the respective sentences. They perform one-mode projections to transform the bipartite graph to a directed graph containing only sentence nodes. The coherence score of the document is then computed where M (Ei,j ) refers to the row in M that corresponds to grammatical role Ei,j , and I and J are 560 where entity grid Ei exhibits a higher d"
P18-1052,J08-1001,0,0.770588,"long entity transitions, it is still limited in that it does not consider any lexical information regarding the entities, thereby, fails to distinguish Introduction Sentences in a text or a conversation do not occur independently, rather they are connected to form a coherent discourse that is easy to comprehend. Coherence models are computational models that can distinguish a coherent discourse from incoherent ones. It has ranges of applications in text generation, summarization, and coherence scoring. Inspired by formal theories of discourse, a number of coherence models have been proposed (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Jurafsky, 2017). The entity grid model (Barzilay and Lapata, 2008) is one of the most popular coherence models that has received much attention over the years. As exemplified in Table 1, the model represents a text by a grid that captures how grammatical roles of different discourse entities (e.g., nouns) change from one sentence to ∗ All authors contributed equally. 558 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 558–568 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Lin"
P18-1052,P08-1041,0,0.0952387,"Missing"
P18-1052,P11-1118,0,0.711899,"ables, rather than directly by LDI Corp. s3 : LDI leases and sells data-processing, telecommunications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid"
P18-1052,D14-1218,0,0.377803,"at entity ej plays in sentence si , which can be one of: subject (S), object (O), other (X), or absent (–). In cases where an 559 as the average out-degree of sentence nodes. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns by assuming that sentences in a coherent text exhibit certain syntactic regularities. They propose a local coherence model that captures the co-occurrence of structural features in adjacent sentences, and a global model based on a hidden Markov model, which learns the global syntactic patterns from clusters of sentences with similar syntax. Li and Hovy (2014) proposed a neural framework to compute the coherence score of a document by estimating coherence probability for every window of three sentences. They encode each sentence in the window using either a recurrent or a recursive neural network. To get a documentlevel coherence score, they sum up the windowlevel log probabilities. Li and Jurafsky (2017) proposed two encoder-decoder models augmented with latent variables for both coherence evaluation and discourse generation. Their first model incorporates global discourse information (topics) by feeding the output of a sentence-level HMMLDA model"
P18-1052,P11-2022,0,0.72728,"ables, rather than directly by LDI Corp. s3 : LDI leases and sells data-processing, telecommunications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid"
P18-1052,D17-1019,0,0.631416,"in that it does not consider any lexical information regarding the entities, thereby, fails to distinguish Introduction Sentences in a text or a conversation do not occur independently, rather they are connected to form a coherent discourse that is easy to comprehend. Coherence models are computational models that can distinguish a coherent discourse from incoherent ones. It has ranges of applications in text generation, summarization, and coherence scoring. Inspired by formal theories of discourse, a number of coherence models have been proposed (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Jurafsky, 2017). The entity grid model (Barzilay and Lapata, 2008) is one of the most popular coherence models that has received much attention over the years. As exemplified in Table 1, the model represents a text by a grid that captures how grammatical roles of different discourse entities (e.g., nouns) change from one sentence to ∗ All authors contributed equally. 558 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 558–568 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tion for asynchronous conversatio"
P18-1052,E12-1032,0,0.34681,"LDI leases and sells data-processing, telecommunications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid model effectively captures long entity tr"
P18-1052,P11-1100,0,0.682096,"Missing"
P18-1052,C14-1089,0,0.742325,"nications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid model effectively captures long entity transitions, it is still limited in that it doe"
P18-1052,D15-1178,0,0.0876731,"Missing"
P18-1052,D12-1106,0,0.454999,"structure in the grid representation and subsequently in feature computation. For this, we propose a novel grid representa2.1 Traditional Entity Grid Models Introduced by Barzilay and Lapata (2008), the entity grid model represents a text by a twodimensional matrix. As shown in Table 1, the rows correspond to sentences, and the columns correspond to entities (noun phrases). Each entry Ei,j represents the syntactic role that entity ej plays in sentence si , which can be one of: subject (S), object (O), other (X), or absent (–). In cases where an 559 as the average out-degree of sentence nodes. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns by assuming that sentences in a coherent text exhibit certain syntactic regularities. They propose a local coherence model that captures the co-occurrence of structural features in adjacent sentences, and a global model based on a hidden Markov model, which learns the global syntactic patterns from clusters of sentences with similar syntax. Li and Hovy (2014) proposed a neural framework to compute the coherence score of a document by estimating coherence probability for every window of three sentences. They encode each sentence in the w"
P18-1052,J95-2003,0,0.957445,"Missing"
P18-1052,P17-1121,1,0.401043,"equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid model effectively captures long entity transitions, it is still limited in that it does not consider any lexical informa"
P18-1052,D14-1162,0,0.081676,"Missing"
P18-1052,prasad-etal-2008-penn,0,0.0276965,"ner and Charniak (2011b) show improvements to the grid model by including non-head nouns as entities. Instead of employing a coreference resolver, they match the nouns to detect coreferent entities. They demonstrate further improvements by extending the grid to distinguish between entities of different types. They do so by incorporating entity-specific features like named entity, noun class and modifiers. Lin et al. (2011) model transitions of discourse roles for entities as opposed to their grammatical roles. They instantiate discourse roles by discourse relations in Penn Discourse Treebank (Prasad et al., 2008). In a follow up work, Feng et al. (2014) trained the same model but using relations derived from deep discourse structures annotated with Rhetorical Structure Theory (Mann and Thompson, 1988). 2.2 3 Extending Neural Entity Grid In this section we first briefly describe the neural entity grid model proposed by Nguyen and Joty (2017). Then, we propose our extension to this model that leads to improved performance. We present our coherence model for asynchronous conversation in the next section. 3.1 Neural Entity Grid Figure 1 depicts the neural grid model of Nguyen and Joty (2017). Given an ent"
P18-1052,D15-1036,0,0.0345256,"Missing"
Q15-1022,D12-1039,0,0.0161468,"7 13,428 6,347 1,390 Finally, we also experimented on the publicly available Sanders Twitter corpus.8 This corpus consists of 5,512 Tweets grouped into four different topics (Apple, Google, Microsoft, and Twitter). Due to restrictions in Twitter’s Terms of Service, the actual Tweets need to be downloaded using 5,512 Tweet IDs. There are 850 Tweets not available to download. After removing the non-English Tweets, 3,115 Tweets remain. In addition to converting into lowercase and removing non-alphabetic characters, words were normalized by using a lexical normalization dictionary for microblogs (Han et al., 2012). We then removed stop-words, words shorter than 3 characters or appearing less than 3 times in the corpus. The four words apple, google, microsoft and twitter were removed as these four words occur in every Tweet in the corresponding topic. Moreover, words not found in both Google and Stanford vector lists were also removed.9 In all our experiments, after removing words from documents, any document with a zero word count was also removed from the corpus. For the Twitter corpus, this resulted in just 2,520 remaining Tweets. 4.1.3 General settings The hyper-parameter β used in baseline LDA and"
Q15-1022,P10-1117,1,0.214613,"ow that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents. 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus, and predict the probabilities of each word in each document belonging to each topic (Teh et al., 2006; Newman et al., 2006; Toutanova and Johnson, 2008; Porteous et al., 2008; Johnson, 2010; Xie and Xing, 2013; Hingmire et al., 2013). Conventional topic modeling algorithms such as these infer document-to-topic and topic-to-word distributions from the co-occurrence of words within documents. But when the training corpus of documents is small or when the documents are short, the resulting distributions might be based on little evidence. Sahami and Heilman (2006) and Phan et al. (2011) show that it helps to exploit external knowledge to improve the topic representations. Sahami and Heilman (2006) employed web search results to improve the information in short texts. Phan et al. (20"
Q15-1022,E14-1056,0,0.120704,"Missing"
Q15-1022,D11-1024,0,0.785259,"Missing"
Q15-1022,N10-1012,0,0.380251,"Missing"
Q15-1022,Q15-1004,0,0.0102457,"ent. This approximation is reasonably accurate for short documents. This distribution Q simplifies the coupling between zd and sd . This enables us to integrate out sd in Q. We first sample the document topic zd for document d using Q(zd ), marginalizing over sd : t,wd i i ¬d t ¬d i &gt; 3.5 Learning latent feature vectors for topics 4.1 To estimate the topic vectors after each Gibbs sampling iteration through the data, we apply regularized maximum likelihood estimation. Applying MAP estimation to learn log-linear models for topic models is also used in SAGE (Eisenstein et al., 2011) and SPRITE (Paul and Dredze, 2015). How4.1.1 Distributed word representations We experimented with two state-of-the-art sets of pre-trained word vectors here. 303 1 Experimental setup The L2 regularizer constant was set to µ = 0.01. We used the L-BFGS implementation from the Mallet toolkit (McCallum, 2002). 2 Google word vectors3 are pre-trained 300dimensional vectors for 3 million words and phrases. These vectors were trained on a 100 billion word subset of the Google News corpus by using the Google Word2Vec toolkit (Mikolov et al., 2013). Stanford vectors4 are pre-trained 300-dimensional vectors for 2 million words. These ve"
Q15-1022,D14-1162,0,0.125631,"pic representations in the small corpus. However, if the larger corpus has many irrelevant topics, this will “use up” the topic space of the model. In addition, Petterson et al. (2010) proposed an extension of LDA that uses external information about word similarity, such as thesauri and dictionaries, to smooth the topic-to-word distribution. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature (LF) vectors have been used for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). The combination of values permitted by latent features forms a high dimensional space which makes it is well suited to model topics of very large corpora. Rather than relying solely on a multinomial or latent feature model, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), we explore how to take advantage of both latent feature and multinomial models by using a latent feature representation trained on a large external corpus to supplement a multinomial topic model estimated from a smaller corpus. Our main contribution is that we propose two new latent fea"
Q15-1022,P13-1045,0,0.0165863,"to help shape the topic representations in the small corpus. However, if the larger corpus has many irrelevant topics, this will “use up” the topic space of the model. In addition, Petterson et al. (2010) proposed an extension of LDA that uses external information about word similarity, such as thesauri and dictionaries, to smooth the topic-to-word distribution. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature (LF) vectors have been used for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). The combination of values permitted by latent features forms a high dimensional space which makes it is well suited to model topics of very large corpora. Rather than relying solely on a multinomial or latent feature model, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), we explore how to take advantage of both latent feature and multinomial models by using a latent feature representation trained on a large external corpus to supplement a multinomial topic model estimated from a smaller corpus. Our main contribution is that we"
Q15-1022,D12-1087,0,0.169543,"Missing"
Q16-1016,W04-2214,0,0.0291821,"ken Coherence. Feature f11 (ent i , ent j ) measures the coherence between the token contexts of two entity candidates enti and entj : f11 (enti , ent j ) =  WO tok-cxt(ent i ), tok-cxt(ent j ) f11 allows us to establish cross-dependencies among labels in our graphical model. For example, the two entities David Beckham and Manchester United are highly coherent as they share many tokens in their contexts, such as “champions”, “league”, “premier”, “cup”, etc. Thus, they should mapped jointly. 4.2 Domain Features We use WordNet domains, created by Miller (1995), Magnini and Cavagli (2000), and Bentivogli et al. (2004), to construct a taxonomy of 46 domains, including Politics, Economy, Sports, Science, Medicine, Biology, Art, Music, etc. We combine the domains with semantic types (classes of entities) provided by YAGO2, by assigning them to their respective domains. This is based on the manual assignment of WordNet synsets to domains, introduced by Magnini and Cavagli (2000), and Bentivogli et al. (2004), and extends to additional types in YAGO2. For example, Singer is assigned to Music, and Football Player to Football, a sub-domain of Sports. These types include the standard NER types Person (PER), Organi"
Q16-1016,E06-1002,0,0.0133756,"3). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work of Dill et al. (2003), Bunescu and Pasca (2006), Cucerzan (2007), and Milne and Witten (2008). More recent advances led to open-source tools like the Wikipedia Miner Wikifier (Milne and Witten, 2013), the Illinois Wikifier (Ratinov et al., 2011), Spotlight (Mendes et al., 2011), Semanticizer (Meij et al., 2012), TagMe (Ferragina and Scaiella, 2010; Cornolti et al., 2014), and AIDA (Hoffart et al., 2011) with its improved variant AIDA-light (Nguyen et al., 2014). We choose some, namely, Spotlight, TagMe and AIDA-light, as baselines for our experiments. These are the best-performing, publicly available systems for news and web texts. Most of"
Q16-1016,D07-1074,0,0.0792189,"e based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work of Dill et al. (2003), Bunescu and Pasca (2006), Cucerzan (2007), and Milne and Witten (2008). More recent advances led to open-source tools like the Wikipedia Miner Wikifier (Milne and Witten, 2013), the Illinois Wikifier (Ratinov et al., 2011), Spotlight (Mendes et al., 2011), Semanticizer (Meij et al., 2012), TagMe (Ferragina and Scaiella, 2010; Cornolti et al., 2014), and AIDA (Hoffart et al., 2011) with its improved variant AIDA-light (Nguyen et al., 2014). We choose some, namely, Spotlight, TagMe and AIDA-light, as baselines for our experiments. These are the best-performing, publicly available systems for news and web texts. Most of these methods co"
Q16-1016,de-marneffe-etal-2006-generating,0,0.0331227,"Missing"
Q16-1016,Q14-1037,0,0.590502,"maintains the uncertainty of both mention candidates (i.e., token spans) and entity candidates for competing mention candidates, and makes joint decisions, as opposed to fixing mentions before reasoning on their disambiguation. We present experiments with three major datasets: the CoNLL’03 collection of newswire articles, the ACE’05 corpus of news and blogs, and the ClueWeb’09-FACC1 corpus of web pages. Baselines that we compare J-NERD with include AIDAlight (Nguyen et al., 2014), Spotlight (Daiber et al., 2013), and TagMe (Ferragina and Scaiella, 2010), and the recent joint NER/NED method of Durrett and Klein (2014). J-NERD consistently outperforms these competitors in terms of both precision and recall. 2 Related Work NER: Detecting the boundaries of text spans that denote named entities has been mostly addressed by supervised CRF’s over word sequences (McCallum and Li, 2003; Finkel et al., 2005). The work of Ratinov and Roth (2009) improved these techniques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features"
Q16-1016,P05-1045,0,0.358918,"ge base. We present experiments with different kinds of texts from the CoNLL’03, ACE’05, and ClueWeb’09-FACC1 corpora. J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1. 1 Introduction Motivation: Methods for Named Entity Recognition and Disambiguation, NERD for short, typically proceed in two stages: • At the NER stage, text spans of entity mentions are detected and tagged with coarse-grained types like Person, Organization, Location, etc. This is typically performed by a trained Conditional Random Field (CRF) over word sequences (e.g., Finkel et al. (2005)). • At the NED stage, mentions are mapped to entities in a knowledge base (KB) based on contextual similarity measures and the semantic coherence of the selected entities (e.g., Cucerzan (2014); Hoffart et al. (2011); Ratinov et al. (2011)). This two-stage approach has limitations. First, NER may produce false positives that can misguide NED. Second, NER may miss out on some entity mentions, and NED has no chance to compensate for these false negatives. Third, NED is not able to help NER, for example, by disambiguating “easy” mentions (e.g., of prominent entities with more or less unique name"
Q16-1016,C02-1130,0,0.0631354,"ditional output of the CRF’s are type tags for the recognized word spans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a K"
Q16-1016,magnini-cavaglia-2000-integrating,0,0.026281,"tok-cxt(tok i ) Entity-Entity Token Coherence. Feature f11 (ent i , ent j ) measures the coherence between the token contexts of two entity candidates enti and entj : f11 (enti , ent j ) =  WO tok-cxt(ent i ), tok-cxt(ent j ) f11 allows us to establish cross-dependencies among labels in our graphical model. For example, the two entities David Beckham and Manchester United are highly coherent as they share many tokens in their contexts, such as “champions”, “league”, “premier”, “cup”, etc. Thus, they should mapped jointly. 4.2 Domain Features We use WordNet domains, created by Miller (1995), Magnini and Cavagli (2000), and Bentivogli et al. (2004), to construct a taxonomy of 46 domains, including Politics, Economy, Sports, Science, Medicine, Biology, Art, Music, etc. We combine the domains with semantic types (classes of entities) provided by YAGO2, by assigning them to their respective domains. This is based on the manual assignment of WordNet synsets to domains, introduced by Magnini and Cavagli (2000), and Bentivogli et al. (2004), and extends to additional types in YAGO2. For example, Singer is assigned to Music, and Football Player to Football, a sub-domain of Sports. These types include the standard"
Q16-1016,W03-0430,0,0.0108238,"datasets: the CoNLL’03 collection of newswire articles, the ACE’05 corpus of news and blogs, and the ClueWeb’09-FACC1 corpus of web pages. Baselines that we compare J-NERD with include AIDAlight (Nguyen et al., 2014), Spotlight (Daiber et al., 2013), and TagMe (Ferragina and Scaiella, 2010), and the recent joint NER/NED method of Durrett and Klein (2014). J-NERD consistently outperforms these competitors in terms of both precision and recall. 2 Related Work NER: Detecting the boundaries of text spans that denote named entities has been mostly addressed by supervised CRF’s over word sequences (McCallum and Li, 2003; Finkel et al., 2005). The work of Ratinov and Roth (2009) improved these techniques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features and external dictionaries for further improvement. An alternative line of NER techniques is based on dictionaries of name-entity pairs, including nicknames, shorthand names, and paraphrases (e.g., “the first man on the moon”). The work of Ferragina and Scaiella (20"
Q16-1016,P13-1146,1,0.486139,"ed to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work of Dill et al. (2003), Bu"
Q16-1016,W14-1609,0,0.0189621,"hat are designed for the broader task of “Wikification” are not penalized by their (typically lower) performance on inputs other than proper entity mentions. 5.2 We compare J-NERD in its four variants (linear vs. tree and local vs. global) to various state-of-the-art NER/NED methods. For NER (i.e., mention boundaries and types) we use the recent version 3.4.1 of the Stanford NER Tagger7 (Finkel et al., 2005) and the recent version 2.8.4 of the Illinois Tagger8 (Ratinov and Roth, 2009) as baselines. These systems have NER benchmark results on CoNLL’03 that are as good as the result reported in Passos et al. (2014). We retrained this model by using the same corpus-specific training data that we use for J-NERD . For NED, we compared J-NERD against the following methods for which we obtained open-source software or could call a Web service: • Berkeley-entity (Durrett and Klein, 2014) uses a joint model for coreference resolution, NER and NED with linkage to Wikipedia. • AIDA-light (Nguyen et al., 2014) is an optimized variant of the AIDA system (Hoffart et al., 2011), based on YAGO2. It uses the Stanford tool for NER. • TagMe (Ferragina and Scaiella, 2010) is a Wikifier that maps mentions to entities or c"
Q16-1016,C10-1105,0,0.0148273,"s are type tags for the recognized word spans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and t"
Q16-1016,W09-1119,0,0.566404,"he ACE’05 corpus of news and blogs, and the ClueWeb’09-FACC1 corpus of web pages. Baselines that we compare J-NERD with include AIDAlight (Nguyen et al., 2014), Spotlight (Daiber et al., 2013), and TagMe (Ferragina and Scaiella, 2010), and the recent joint NER/NED method of Durrett and Klein (2014). J-NERD consistently outperforms these competitors in terms of both precision and recall. 2 Related Work NER: Detecting the boundaries of text spans that denote named entities has been mostly addressed by supervised CRF’s over word sequences (McCallum and Li, 2003; Finkel et al., 2005). The work of Ratinov and Roth (2009) improved these techniques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features and external dictionaries for further improvement. An alternative line of NER techniques is based on dictionaries of name-entity pairs, including nicknames, shorthand names, and paraphrases (e.g., “the first man on the moon”). The work of Ferragina and Scaiella (2010) and Mendes et al. (2011) are examples of dictionary-bas"
Q16-1016,P11-1138,0,0.311803,"n Motivation: Methods for Named Entity Recognition and Disambiguation, NERD for short, typically proceed in two stages: • At the NER stage, text spans of entity mentions are detected and tagged with coarse-grained types like Person, Organization, Location, etc. This is typically performed by a trained Conditional Random Field (CRF) over word sequences (e.g., Finkel et al. (2005)). • At the NED stage, mentions are mapped to entities in a knowledge base (KB) based on contextual similarity measures and the semantic coherence of the selected entities (e.g., Cucerzan (2014); Hoffart et al. (2011); Ratinov et al. (2011)). This two-stage approach has limitations. First, NER may produce false positives that can misguide NED. Second, NER may miss out on some entity mentions, and NED has no chance to compensate for these false negatives. Third, NED is not able to help NER, for example, by disambiguating “easy” mentions (e.g., of prominent entities with more or less unique names), and then using the entities and knowledge about them as enriched features for NER. Example: Consider the following sentences: David played for manu, real, and la galaxy. His wife posh performed with the spice girls. This is difficult fo"
Q16-1016,spitkovsky-chang-2012-cross,0,0.0238091,"niques by additional features from context aggregation and external lexical sources (gazetteers, etc.). Passos et 1 The J-NERD source is available at the URL http:// download.mpi-inf.mpg.de/d5/tk/jnerd-tacl.zip. 216 al. (2014) harnessed skip-gram features and external dictionaries for further improvement. An alternative line of NER techniques is based on dictionaries of name-entity pairs, including nicknames, shorthand names, and paraphrases (e.g., “the first man on the moon”). The work of Ferragina and Scaiella (2010) and Mendes et al. (2011) are examples of dictionary-based NER. The work of Spitkovsky and Chang (2012) is an example of a large-scale dictionary that can be harnessed by such methods. An additional output of the CRF’s are type tags for the recognized word spans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., poli"
Q16-1016,C12-2133,1,0.759465,"ans, typically limited to coarse-grained types like Person, Organization, and Location (and also Miscellaneous). The most widely used tool of this kind is the Stanford NER Tagger (Finkel et al., 2005). Many NED tools use the Stanford NER Tagger in their first stage of detecting mentions. Mention Typing: The specific NER task of inferring semantic types has been further refined and extended by various works on fine-grained typing (e.g., politicians, musicians, singers, guitarists) for entity mentions and general noun phrases (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012; Yosef et al., 2012; Nakashole et al., 2013). Most of these works are based on supervised classification, using linguistic features from mentions and their surrounding text. One exception is the work of Nakashole et al. (2013) which is based on text patterns that connect entities of specific types, acquired by sequence mining from the Wikipedia fulltext corpus. In contrast to our work, those are simple surface patterns, and the task addressed here is limited to typing noun phrases that likely denote emerging entities that are not yet registered in a KB. NED: Methods and tools for NED go back to the seminal work"
Q16-1016,D11-1072,1,\N,Missing
Q16-1016,W10-3503,0,\N,Missing
R11-1056,damljanovic-etal-2008-text,0,0.031166,"natural language question to an SQL query while TGEN (Text Generator) generates answers based on the query result tables. QTRAN uses limited context-free grammars to analyze user’s question into syntax tree via CYK algorithm. Recently, some question answering systems that used semantic annotations generated high results in natural language question analysis. A well known annotation based framework is GATE (Cunningham et al., 2002) which have been used in many question answering systems especially for the natural language question analysis module such as Aqualog (Lopez et al., 2007), QuestIO (Damljanovic et al., 2008), VnQAS (Nguyen et al., 2009). Aqualog and VnQAS are ontology-based question answering systems for English and Vietnamese respecIntroduction The goal of question answering systems is to give answers to the user’s questions instead of ranked lists of related documents as used by most current search engines (Hirschman and Gaizauskas, 2001). Natural language question analysis component is the first component in any question answering systems. This component creates an intermediate representation of the input question, which is expressed in natural language, to be utilized in the rest of the syste"
S17-1015,D15-1177,0,0.0439185,"Missing"
S17-1015,P16-1191,0,0.00990687,"Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that they assign the same weight to ever"
S17-1015,P16-1101,0,0.00456847,"ord. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word correspond"
S17-1015,J15-4004,0,0.0198179,"Missing"
S17-1015,P12-1092,0,0.0244217,"ptures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most suitable topic of a word"
S17-1015,P15-1010,0,0.0522639,"Nguyen1 , Dat Quoc Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,K16-1008,1,0.408695,"enses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense o"
S17-1015,W14-1606,1,0.616494,"is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense"
S17-1015,N15-1070,0,0.0357356,"Missing"
S17-1015,Q17-1003,1,0.820726,"model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinge"
S17-1015,D14-1113,0,0.0546105,"Missing"
S17-1015,Q15-1016,0,0.0359882,"he word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,t . 1 We use an unigram distribution raised"
S17-1015,Q15-1022,1,0.436169,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D15-1200,0,0.0397679,"t size. In addition, v tor representation of the word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,"
S17-1015,K17-3014,1,0.253101,"we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific"
S17-1015,U15-1014,1,0.855226,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D14-1162,0,0.126941,"ntal results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Co"
S17-1015,W13-3512,0,0.17894,"Missing"
S17-1015,C14-1016,0,0.133176,"Missing"
S17-1015,D16-1174,0,0.425139,"Missing"
S17-1015,C14-1015,0,0.0135853,"Missing"
S17-1015,N15-1058,0,0.0460639,"Missing"
S17-1015,N10-1013,0,0.0136891,"dding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most sui"
S17-1015,P15-1173,0,0.0914243,"Missing"
S17-1015,D15-1036,0,0.0294844,"Missing"
S17-1015,K15-1026,0,0.0326275,"c Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,D13-1170,0,0.00363313,"r words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, appr"
S17-1015,D14-1110,0,\N,Missing
S17-1015,P15-2003,0,\N,Missing
S17-1015,P14-1023,0,\N,Missing
S17-1015,L16-1046,0,\N,Missing
S18-1085,P11-1038,0,0.10536,"Missing"
S18-1085,D12-1039,0,0.0518716,"Missing"
S18-1085,W11-0705,0,0.0249049,"y metric and fifth using the F1 metric. Our code is available at: https://github.com/ NIHRIO/IronyDetectionInTwitter. 1 Subtask 1 (A): Ironic vs non-ironic This first subtask is a binary classification problem, in which we predict whether or not a tweet is ironic. For example, “I just love when you test my patience!! #not” is ironic, but “Had no sleep and have got school now #not happy” is non-ironic. Introduction Mining Twitter data has increasingly been attracting much research attention in many NLP applications such as in sentiment analysis (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Agarwal et al., 2011; Liu et al., 2012; Rosenthal et al., 2017; Cambria et al., 2018) and stock market prediction (Bollen et al., 2011; Vu et al., 2012; Bartov et al., 2015; Nofer and Hinz, 2015; Oliveira et al., 2017). Recently, Davidov et al. (2010) and Reyes et al. (2013) have shown that Twitter data includes a high volume of “ironic” tweets. For example, a user can use positive words in a Twitter message to her intended negative meaning (e.g., “It is awesome to go to bed at 3 am #not”). This especially results in a research challenge to assign correct sentiment labels for ironic tweets (Bosco et al., 2013; Gh"
S18-1085,E14-3007,0,0.106374,"et al. (2010) and Reyes et al. (2013) have shown that Twitter data includes a high volume of “ironic” tweets. For example, a user can use positive words in a Twitter message to her intended negative meaning (e.g., “It is awesome to go to bed at 3 am #not”). This especially results in a research challenge to assign correct sentiment labels for ironic tweets (Bosco et al., 2013; Ghosh et al., 2015; Far´ıas et al., 2016; Nozza et al., 2017; Kannangara, 2018). To handle that problem, much attention has been focused on automatic irony detection in Twitter (Davidov et al., 2010; Reyes et al., 2013; Barbieri and Saggion, 2014; Rajadesingan et al., 2015; Far´ıas et al., 2016; Sulis et al., 2016; Karoui et al., Subtask 2 (B): Different types of irony This second subtask is a multi-class classification problem, where we predict the correct label of a tweet from four classes: (1) non-irony, (2) verbal irony by means of a polarity contrast, (3) other verbal irony and (4) situational irony. The remainder of this paper is organized as follows: We describe the ironic tweet dataset provided by the SemEval-2018 Task 3 in Section 2. We then describe our system in Section 3. The experimental results and conclusion are detaile"
S18-1085,J92-4003,0,0.341051,"s seem not to well-capture that property. To handle this problem, we apply three approaches to compute tweet vector representations. Firstly, we employ 300-dimensional pre-trained word embeddings from GloVe (Pennington et al., 2014) to compute a tweet embedding as the average of the embeddings of words in the tweet. Secondly, we apply the latent semantic indexing (Papadimitriou et al., 1998) to capture the underlying semantics of the dataset. Here, each tweet is represented as a vector of 100 dimensions. Thirdly, we also extract tweet representation by applying the Brown clustering algorithm (Brown et al., 1992; Liang, 2005)2 —a hierarchical clustering algorithm which groups the words with similar meaning and syntactical function together. Applying the Brown clustering algorithm, we obtain a set of clusters, where each word belongs to only Polarity features: Motivated by the verbal irony by means of polarity contrast, such as “I really love this year’s summer; weeks and weeks of awful weather”, we use the number of polarity signals appearing in a tweet as the polarity features. The signals include positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon."
S18-1085,W10-2914,0,0.156636,"whether or not a tweet is ironic. For example, “I just love when you test my patience!! #not” is ironic, but “Had no sleep and have got school now #not happy” is non-ironic. Introduction Mining Twitter data has increasingly been attracting much research attention in many NLP applications such as in sentiment analysis (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Agarwal et al., 2011; Liu et al., 2012; Rosenthal et al., 2017; Cambria et al., 2018) and stock market prediction (Bollen et al., 2011; Vu et al., 2012; Bartov et al., 2015; Nofer and Hinz, 2015; Oliveira et al., 2017). Recently, Davidov et al. (2010) and Reyes et al. (2013) have shown that Twitter data includes a high volume of “ironic” tweets. For example, a user can use positive words in a Twitter message to her intended negative meaning (e.g., “It is awesome to go to bed at 3 am #not”). This especially results in a research challenge to assign correct sentiment labels for ironic tweets (Bosco et al., 2013; Ghosh et al., 2015; Far´ıas et al., 2016; Nozza et al., 2017; Kannangara, 2018). To handle that problem, much attention has been focused on automatic irony detection in Twitter (Davidov et al., 2010; Reyes et al., 2013; Barbieri and"
S18-1085,D14-1181,0,0.0127907,"Missing"
S18-1085,S17-2088,0,0.102556,"Missing"
S18-1085,S18-1005,0,0.0544399,"Missing"
S18-1085,W12-5503,0,0.0294026,"ronic vs non-ironic This first subtask is a binary classification problem, in which we predict whether or not a tweet is ironic. For example, “I just love when you test my patience!! #not” is ironic, but “Had no sleep and have got school now #not happy” is non-ironic. Introduction Mining Twitter data has increasingly been attracting much research attention in many NLP applications such as in sentiment analysis (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Agarwal et al., 2011; Liu et al., 2012; Rosenthal et al., 2017; Cambria et al., 2018) and stock market prediction (Bollen et al., 2011; Vu et al., 2012; Bartov et al., 2015; Nofer and Hinz, 2015; Oliveira et al., 2017). Recently, Davidov et al. (2010) and Reyes et al. (2013) have shown that Twitter data includes a high volume of “ironic” tweets. For example, a user can use positive words in a Twitter message to her intended negative meaning (e.g., “It is awesome to go to bed at 3 am #not”). This especially results in a research challenge to assign correct sentiment labels for ironic tweets (Bosco et al., 2013; Ghosh et al., 2015; Far´ıas et al., 2016; Nozza et al., 2017; Kannangara, 2018). To handle that problem, much attention has been focu"
S18-1085,E17-1026,0,0.0216452,"17; Cambria et al., 2018) and stock market prediction (Bollen et al., 2011; Vu et al., 2012; Bartov et al., 2015; Nofer and Hinz, 2015; Oliveira et al., 2017). Recently, Davidov et al. (2010) and Reyes et al. (2013) have shown that Twitter data includes a high volume of “ironic” tweets. For example, a user can use positive words in a Twitter message to her intended negative meaning (e.g., “It is awesome to go to bed at 3 am #not”). This especially results in a research challenge to assign correct sentiment labels for ironic tweets (Bosco et al., 2013; Ghosh et al., 2015; Far´ıas et al., 2016; Nozza et al., 2017; Kannangara, 2018). To handle that problem, much attention has been focused on automatic irony detection in Twitter (Davidov et al., 2010; Reyes et al., 2013; Barbieri and Saggion, 2014; Rajadesingan et al., 2015; Far´ıas et al., 2016; Sulis et al., 2016; Karoui et al., Subtask 2 (B): Different types of irony This second subtask is a multi-class classification problem, where we predict the correct label of a tweet from four classes: (1) non-irony, (2) verbal irony by means of a polarity contrast, (3) other verbal irony and (4) situational irony. The remainder of this paper is organized as fol"
S18-1085,pak-paroubek-2010-twitter,0,0.0284319,"s. In particular, we rank third using the accuracy metric and fifth using the F1 metric. Our code is available at: https://github.com/ NIHRIO/IronyDetectionInTwitter. 1 Subtask 1 (A): Ironic vs non-ironic This first subtask is a binary classification problem, in which we predict whether or not a tweet is ironic. For example, “I just love when you test my patience!! #not” is ironic, but “Had no sleep and have got school now #not happy” is non-ironic. Introduction Mining Twitter data has increasingly been attracting much research attention in many NLP applications such as in sentiment analysis (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Agarwal et al., 2011; Liu et al., 2012; Rosenthal et al., 2017; Cambria et al., 2018) and stock market prediction (Bollen et al., 2011; Vu et al., 2012; Bartov et al., 2015; Nofer and Hinz, 2015; Oliveira et al., 2017). Recently, Davidov et al. (2010) and Reyes et al. (2013) have shown that Twitter data includes a high volume of “ironic” tweets. For example, a user can use positive words in a Twitter message to her intended negative meaning (e.g., “It is awesome to go to bed at 3 am #not”). This especially results in a research challenge to assign correct sentiment l"
S18-1085,D14-1162,0,0.0878263,"d annotate part-of-speech tags (POS tags) for all tweets in the dataset. We then use all the POS tags with their corresponding tf-idf values as our syntactic features and feature values, respectively. Semantic features: A major challenge when dealing with the tweet data is that the lexicon used in a tweet is informal and much different from tweet to tweet. The lexical and syntactic features seem not to well-capture that property. To handle this problem, we apply three approaches to compute tweet vector representations. Firstly, we employ 300-dimensional pre-trained word embeddings from GloVe (Pennington et al., 2014) to compute a tweet embedding as the average of the embeddings of words in the tweet. Secondly, we apply the latent semantic indexing (Papadimitriou et al., 1998) to capture the underlying semantics of the dataset. Here, each tweet is represented as a vector of 100 dimensions. Thirdly, we also extract tweet representation by applying the Brown clustering algorithm (Brown et al., 1992; Liang, 2005)2 —a hierarchical clustering algorithm which groups the words with similar meaning and syntactical function together. Applying the Brown clustering algorithm, we obtain a set of clusters, where each w"
S18-1085,E17-1025,0,\N,Missing
U15-1014,N10-1083,0,0.0903286,"Missing"
U15-1014,P11-1026,0,0.020909,"ect to the j ele2013) to learn 25-dimensional word vectors on the ment of the vector for each topic z is: experimental dataset, using a local 10-word win  dow context.4 X X X ∂L = nd,w P(z |w, d) vw,j − vw0 ,j φz,w0 ∂µz,j The numbers of topics is set to 20. For varid∈D w∈W w0 ∈W ational inference LDA, we use Blei’s implemen− 2π2 µz,j − π1 sign(µz,j ) tation.5 For Gibbs sampling LDA, we use the (6) jLDADMM package6 (Nguyen, 2015) with comWe used OWL - QN1 (Andrew and Gao, 2007) to mon hyper-parameters β = 0.01 and α = 0.1 find the topic vector µz and the parameters ξd,z (Newman et al., 2009; Hu et al., 2011; Xie and and ψz,w that maximize L. Xing, 2013). We ran Gibbs sampling LDA for 2000 iterations and evaluated the topics assigned 4 Experiments to words in the last sample. We then used the To investigate the performance of our new apdocument-to-topic and topic-to-word distributions proach, we compared it with two baselines on from the last sample of Gibbs sampling LDA to topic coherence: 1) variational inference LDA initialize the parameters ξd,z and ψz,w while topic (Blei et al., 2003); and 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004)."
U15-1014,P10-1117,1,0.802126,"iminary results show that the word vectors induced from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus of documents and to infer document-to-topic and topicto-word distributions from the co-occurrence of words within the documents (Wallach, 2006; Blei and McAuliffe, 2008; Wang et al., 2007; Johnson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et"
U15-1014,E14-1056,0,0.0461841,"d 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004). The topic coherMAP learner. For our MAP approach, we set7 ence evaluation measures the coherence of the λ2 = π2 = 0.01, λ1 = π1 = 1.0e−6, 2 = 0.1 topic-to-word associations, i.e. it directly evaluand 1 = 0.01. We report the mean and standard ates how the high-probability words in each topic deviation of the results of ten repetitions of each are semantically coherent (Chang et al., 2009; experiment. Newman et al., 2010; Mimno et al., 2011; Stevens 4.2 Quantitative analysis et al., 2012; Lau et al., 2014; R¨oder et al., 2015). − 22 ψz,w − 1 sign(ψz,w ) 4.1 Experimental setup We conducted experiments on the standard benchmark 20-Newsgroups dataset.2 In addition to converting into lowercase and removing non-alphabetic characters, we removed stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We then removed words shorter than 3 characters or words appearing less than 10 times. Table 1 presents details of the experimental dataset. As pointed out in Levy and Goldberg (2014) and Pennington et al. (2014), the prediction-based methods and count-based methods for learning"
U15-1014,D11-1024,0,0.100948,"ize the parameters ξd,z and ψz,w while topic (Blei et al., 2003); and 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004). The topic coherMAP learner. For our MAP approach, we set7 ence evaluation measures the coherence of the λ2 = π2 = 0.01, λ1 = π1 = 1.0e−6, 2 = 0.1 topic-to-word associations, i.e. it directly evaluand 1 = 0.01. We report the mean and standard ates how the high-probability words in each topic deviation of the results of ten repetitions of each are semantically coherent (Chang et al., 2009; experiment. Newman et al., 2010; Mimno et al., 2011; Stevens 4.2 Quantitative analysis et al., 2012; Lau et al., 2014; R¨oder et al., 2015). − 22 ψz,w − 1 sign(ψz,w ) 4.1 Experimental setup We conducted experiments on the standard benchmark 20-Newsgroups dataset.2 In addition to converting into lowercase and removing non-alphabetic characters, we removed stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We then removed words shorter than 3 characters or words appearing less than 10 times. Table 1 presents details of the experimental dataset. As pointed out in Levy and Goldberg (2014) and Pennington et al. (2014),"
U15-1014,N10-1012,0,0.0604883,"inference LDA initialize the parameters ξd,z and ψz,w while topic (Blei et al., 2003); and 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004). The topic coherMAP learner. For our MAP approach, we set7 ence evaluation measures the coherence of the λ2 = π2 = 0.01, λ1 = π1 = 1.0e−6, 2 = 0.1 topic-to-word associations, i.e. it directly evaluand 1 = 0.01. We report the mean and standard ates how the high-probability words in each topic deviation of the results of ten repetitions of each are semantically coherent (Chang et al., 2009; experiment. Newman et al., 2010; Mimno et al., 2011; Stevens 4.2 Quantitative analysis et al., 2012; Lau et al., 2014; R¨oder et al., 2015). − 22 ψz,w − 1 sign(ψz,w ) 4.1 Experimental setup We conducted experiments on the standard benchmark 20-Newsgroups dataset.2 In addition to converting into lowercase and removing non-alphabetic characters, we removed stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We then removed words shorter than 3 characters or words appearing less than 10 times. Table 1 presents details of the experimental dataset. As pointed out in Levy and Goldberg (2014) and Penni"
U15-1014,Q15-1004,0,0.0167403,"g the MAP estimation for the LDA model has been suggested before. Chien and Wu (2008), Asuncion et al. (2009) and Taddy (2012) proposed EM algorithms for estimating θd,z and φz,w , while we use direct gradient-based optimization methods. Sontag and Roy (2011) optimized the MAP estimates of φz,w and θd,z in turn by integrating out θd,z and φz,w respectively. We, on the other hand, estimate all parameters jointly in a single optimization step. In addition to Taddy (2012)’s approach, applying MAP estimation to learn log-linear models for topic models is also found in Eisenstein et al. (2011) and Paul and Dredze (2015). Our MAP model is also defined in log-linear representation. However, unlike our MAP approach, those approaches do not use latent feature word vectors to characterize the topic-to-word distributions. Furthermore, Berg-Kirkpatrick et al. (2010) proposed a direct optimization approach of the objective function for Hidden Markov Model-like generative models. However, they applied the approach to various unsupervised NLP tasks, such as part-of-speech induction, grammar induction, word alignment, and word segmentation, but not to topic models. 117 3 Direct MAP estimation approach In this section,"
U15-1014,D14-1162,0,0.0776879,"., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). Rather than relying solely on word count information as the standard multinomial LDA does, or using only distributed feature representations, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), Nguyen et al. (2015) integrated pretrained latent feature word representations containing external information from very large corpora into existing topic models and obtained significant improvements on small document collections and short text datasets. However, their implementation is computationally quite expensive because they have to compute a MAP estimate in ea"
U15-1014,P13-1045,0,0.0251003,"nson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). Rather than relying solely on word count information as the standard multinomial LDA does, or using only distributed feature representations, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), Nguyen et al. (2015) integrated pretrained latent feature word representations containing external information from very large corpora into existing topic models and obtained significant improvements on small document collections and short text datasets. However, their implementation is computationally quite expensive because they have to co"
U15-1014,D12-1087,0,0.0477563,"Missing"
U15-1014,N15-1074,0,0.0190303,"d vectors induced from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus of documents and to infer document-to-topic and topicto-word distributions from the co-occurrence of words within the documents (Wallach, 2006; Blei and McAuliffe, 2008; Wang et al., 2007; Johnson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014)."
U15-1014,D15-1037,0,0.0136596,"from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus of documents and to infer document-to-topic and topicto-word distributions from the co-occurrence of words within the documents (Wallach, 2006; Blei and McAuliffe, 2008; Wang et al., 2007; Johnson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). Rather than relying"
U15-1014,Q15-1022,1,\N,Missing
U16-1017,P16-1231,0,0.0545678,"Missing"
U16-1017,W06-2920,0,0.344542,"Missing"
U16-1017,D14-1082,0,0.165994,"Missing"
U16-1017,P15-1038,0,0.113202,"Missing"
U16-1017,P15-1033,0,0.0502673,"Missing"
U16-1017,P11-2125,0,0.0214393,"uses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher scores than the traditional parsers (McDonald et al., 2005; Nivre et al., 2007b). More specifically, in each"
U16-1017,P82-1020,0,0.823,"Missing"
U16-1017,Q16-1032,0,0.0674042,"Missing"
U16-1017,Q16-1023,0,0.0898462,"Missing"
U16-1017,P13-2109,0,0.102004,"Missing"
U16-1017,J11-1007,0,0.169542,"Missing"
U16-1017,P05-1012,0,0.526091,"Missing"
U16-1017,E14-2005,1,0.927337,"results across many languages. Chen and Manning (2014), Weiss et al. (2015), Pei et al. (2015), and Andor et al. (2016) represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers, while Dyer et al. (2015), Kiperwasser and Goldberg (2016a), and Kiperwasser and Goldberg (2016b) propose novel neural network architectures to solve the feature-engineering problem. Dependency parsing for Vietnamese has not been actively explored. One main reason is because there is no manually labeled dependency treebank available. Thi et al. (2013) and Nguyen et al. (2014b) propose constituent-to-dependency conversion approaches to automatically translate the manually built constituent treebank for Vietnamese (Nguyen et al., 2009) to dependency treebanks. The converted dependency treebanks are then used in later works on Vietnamese dependency parsing, including Vu-Manh et al. (2015), Le-Hong et al. (2015) and Nguyen and Nguyen (2015). All of the previous research works use either the MSTparser (McDonald et al., 2005) or the Maltparser (Nivre et al., 2007b) for their parsing experiments. Among them, Nguyen et al. (2014b) report the highest results with LAS at 7"
U16-1017,P08-1108,0,0.0387999,"ttach the indexed3 word to be dependent to the indexed-2 word by the label nmod. This sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtai"
U16-1017,P15-1032,0,0.0567917,"Missing"
U16-1017,D08-1059,0,0.0257571,"be dependent to the indexed-2 word by the label nmod. This sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher s"
U16-1017,P11-2033,0,0.109133,"Missing"
U16-1017,P15-1031,0,0.0956167,"Missing"
U16-1017,D07-1111,0,0.0426341,"his sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher scores than the traditional parsers (McDonald et al., 200"
U16-1017,N10-1091,0,0.0484834,"Missing"
U16-1017,P81-1022,0,0.467235,"Missing"
U16-1017,P13-1104,0,\N,Missing
U17-1013,W03-0316,0,0.14222,"Missing"
U17-1013,P09-1058,0,0.0273785,"a written text “thu∏ thu nh™p cá nhân” (individualcá_nhân incomethu_nh™p taxthu∏ ) consisting of 5 syllables, the word segmenter returns a two-word phrase “thu∏_thu_nh™p cá_nhân.”1 Then given the input segmented text “thu∏_thu_nh™p cá_nhân”, the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese POS tagging from unsegmented text. In addition, we make a comparison between SOTA feature-based and neural networkbased m"
U17-1013,N16-1030,0,0.082684,"tion rule-based learning model which obtained the highest accuracy at the VLSP 2013 POS tagging shared task.4 • MarMoT (Mueller et al., 2013) is a generic CRF framework and a SOTA POS and morphological tagger.5 • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of the BiLSTMCRF, using CNN to derive character-based representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is another extension of the BiLSTM-CRF, using BiLSTM to derive the character-based representations (Lample et al., 2016). Here, for the pipeline strategy, we train these models to predict POS tags with respect to (w.r.t.) gold word segmentation. In addition, we also retrain the fast and accurate Vietnamese word segmenter RDRsegmenter (Nguyen et al., 2017b) using the training set of 27k sentences.6 3 The data was officially used for the Vietnamese POS tagging shared task at the second VLSP 2013 workshop. 4 http://rdrpostagger.sourceforge.net 5 http://cistern.cis.lmu.de/marmot 6 RDRsegmenter obtains a segmentation speed at 60k words per second, computed on a personal computer of Intel Core i7 2.2 GHz. RDRsegmente"
U17-1013,2010.jeptalnrecital-long.36,0,0.579301,"Missing"
U17-1013,D13-1032,0,0.167826,"Missing"
U17-1013,Y06-1028,0,0.805919,"Missing"
U17-1013,K17-3014,1,0.90322,"2008), Pham et al. (2009) and Tran et al. (2012) used the maximum matching method (NanYuan and YanBin, 1991) to generate all possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) applied ngram language model while Pham et al. (2009) employed POS information from an external POS tagger. Later, Liu and Lin (2014) and Nguyen and Le (2016) proposed approaches based on pointwise prediction, where a binary classifier is trained to identify whether or not there is a word boundary at each point between two syllables. Furthermore, Nguyen et al. (2017b) proposed a rule-based approach which gets the highest results to date in terms of both segmentation accuracy and speed. 2.2 POS tagging Regarding Vietnamese POS tagging, Dien and Kiem (2003) projected POS annotations from English to Vietnamese via a bilingual corpus of word alignments. As a standard sequence labeling task, previous research has applied the CRF, SVM or MaxEnt model to assign each word a POS tag (Nghiem et al., 2008; Tran et al., 2009; Le-Hong et al., 2010; Nguyen et al., 2010; Tran et al., 2010; Bach et al., 2013). In addition, Nguyen et al. (2011) proposed a rule-based appr"
U17-1013,E14-2005,1,0.665315,"labeling models on the syllable-based transformed corpus. 3.2 Dataset The Vietnamese treebank (Nguyen et al., 2009) is the largest annotated corpus for Vietnamese, providing a set of 27,870 manually POS-annotated sentences for training and development (about 23 words per sentence on average) and a test set of 2120 manually POS-annotated sentences (about 31 words per sentence).3 From the set of 27,870 sentences, we use the first 27k sentences for training and the last 870 sentences for development. 3.3 Models For both joint and pipeline strategies, we use the following models: • RDRPOSTagger (Nguyen et al., 2014a) is a transformation rule-based learning model which obtained the highest accuracy at the VLSP 2013 POS tagging shared task.4 • MarMoT (Mueller et al., 2013) is a generic CRF framework and a SOTA POS and morphological tagger.5 • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of the BiLSTMCRF, using CNN to derive character-based representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is another extension of the BiLSTM-CRF, using BiLSTM to derive the character-based"
U17-1013,I17-3010,0,0.256196,"Missing"
U17-1013,D17-1035,0,0.019357,"second, computed on a personal computer of Intel Core i7 2.2 GHz. RDRsegmenter is available at: https: //github.com/datquocnguyen/RDRsegmenter 110 Pipeline 100 100 150 Joint 200 250 250 Table 1: Optimal number of LSTM units. 3.4 Implementation details We use the original pure Java implementations of RDRPOSTagger and MarMoT with default hyperparameter settings in our experiments. Instead of using implementations independently provided by authors of BiLSTM-CRF, BiLSTM-CRF + CNNchar7 and BiLSTM-CRF + LSTM-char, we use a reimplementation which is optimized for performance of all these models from Reimers and Gurevych (2017).8 For three BiLSTM-CRF-based models, we use default hyper-parameters provided by Reimers and Gurevych (2017) with the following exceptions: we use a dropout rate at 0.5 (Ma and Hovy, 2016) with the frequency threshold of 5 for unknown word and syllable types. We initialize word and syllable embeddings with 100-dimensional pretrained embeddings,9 then learn them together with other model parameters during training by using Nadam (Dozat, 2016). For training, we run for 100 epochs. We perform a grid search of hyperparameters to select the number of BiLSTM layers from {1, 2, 3} and the number of"
U17-1013,I17-1018,0,0.0204126,"the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese POS tagging from unsegmented text. In addition, we make a comparison between SOTA feature-based and neural networkbased models, which, to the best of our knowledge, has not done in any prior work on Vietnamese. On the benchmark Vietnamese treebank (Nguyen et al., 2009), we show that the pipeline strategy produces better scores than the joint strategy. We"
U17-1013,N03-1033,0,0.183629,"Missing"
U17-1013,K17-3001,0,0.0615701,"Missing"
U17-1013,P08-1101,0,0.0409698,"ed the word-segmented text—which is the output of the word segmenter—as the input to a POS tagger. For example, given a written text “thu∏ thu nh™p cá nhân” (individualcá_nhân incomethu_nh™p taxthu∏ ) consisting of 5 syllables, the word segmenter returns a two-word phrase “thu∏_thu_nh™p cá_nhân.”1 Then given the input segmented text “thu∏_thu_nh™p cá_nhân”, the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese PO"
U19-1004,D15-1041,0,0.0607333,"Missing"
U19-1004,P18-1246,0,0.0445082,"Missing"
U19-1004,H05-1091,0,0.184131,"the first multi-task learning model for joint Vietnamese word segmentation, partof-speech (POS) tagging and dependency parsing. In particular, our model extends the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) with BiLSTMCRF-based neural layers (Huang et al., 2015) for word segmentation and POS tagging. On Vietnamese benchmark datasets, experimental results show that our joint model obtains stateof-the-art or competitive performances. 1 Introduction Dependency parsing (K¨ubler et al., 2009) is extremely useful in many downstream applications such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Galley and Manning, 2009). POS tags are essential features used in dependency parsing. In real-world parsing, most parsers are used in a pipeline process with a precursor POS tagging model for producing predicted POS tags. In English where white space is a strong word boundary indicator, POS tagging is considered to be the first important step towards dependency parsing (Ballesteros et al., 2015). Unlike English, for Vietnamese NLP, word segmentation is considered to be the key first step. This is because when written, white space is used in Vietnamese to separate syl"
U19-1004,K18-2005,0,0.0344938,"Missing"
U19-1004,N19-1423,0,0.0757212,"Missing"
U19-1004,K17-3002,0,0.0327726,"Missing"
U19-1004,C96-1058,0,0.542998,"tructed similarly to the BIST graph-based dependency parser from Kiperwasser and Goldberg (2016). A difference is that (DEP) we use FFNNs to split rj into head and dependent representations: (A - H) (DEP)  hj = FFNNArc-Head rj (10) (A - D) (DEP)  hj = FFNNArc-Dep rj (11) (L - H) (DEP)  (12) hj = FFNNLabel-Head rj  (L - D) (DEP) hj = FFNNLabel-Dep rj (13) To score a potential dependency arc, we use a FFNN (FFNNARC ) with a one-node output layer: (A - H) (A - D)  score(i, j) = FFNNARC hi ◦ hj (14) Given scores of word pairs, we predict the highest scoring projective parse tree by using the Eisner (1996) decoding algorithm. This unlabeled parsing model is trained with a margin-based hinge loss LARC (Kiperwasser and Goldberg, 2016). To label predicted arcs, we use another FFNN (FFNNLABEL ) with softmax output: (L - H) (L - D)  v (i,j) = FFNNLABEL hi ◦ hj (15) Based on vectors v (i,j) , a cross entropy loss LLABEL for dependency label prediction is computed when training, using the gold labeled tree. Joint multi-task learning: We train our model by summing LWS , LPOS , LARC and LLABEL losses prior to computing gradients. Model parameters are learned to minimize the sum of the losses. Discussio"
U19-1004,P09-1087,0,0.0139777,"amese word segmentation, partof-speech (POS) tagging and dependency parsing. In particular, our model extends the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) with BiLSTMCRF-based neural layers (Huang et al., 2015) for word segmentation and POS tagging. On Vietnamese benchmark datasets, experimental results show that our joint model obtains stateof-the-art or competitive performances. 1 Introduction Dependency parsing (K¨ubler et al., 2009) is extremely useful in many downstream applications such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Galley and Manning, 2009). POS tags are essential features used in dependency parsing. In real-world parsing, most parsers are used in a pipeline process with a precursor POS tagging model for producing predicted POS tags. In English where white space is a strong word boundary indicator, POS tagging is considered to be the first important step towards dependency parsing (Ballesteros et al., 2015). Unlike English, for Vietnamese NLP, word segmentation is considered to be the key first step. This is because when written, white space is used in Vietnamese to separate syllables that constitute words, in addition to markin"
U19-1004,D17-1206,0,0.0431532,"ge loss LARC (Kiperwasser and Goldberg, 2016). To label predicted arcs, we use another FFNN (FFNNLABEL ) with softmax output: (L - H) (L - D)  v (i,j) = FFNNLABEL hi ◦ hj (15) Based on vectors v (i,j) , a cross entropy loss LLABEL for dependency label prediction is computed when training, using the gold labeled tree. Joint multi-task learning: We train our model by summing LWS , LPOS , LARC and LLABEL losses prior to computing gradients. Model parameters are learned to minimize the sum of the losses. Discussion: Our model is inspired by stack propagation based methods (Zhang and Weiss, 2016; Hashimoto et al., 2017) which are joint models for POS tagging and dependency parsing. For dependency parsing, the Stack-propagation model (Zhang and Weiss, 2016) uses a transitionbased approach, and the joint multi-task model JMT (Hashimoto et al., 2017) uses a head selection based approach which produces a probability distribution over possible heads for each word (Zhang et al., 2017), while our model uses a graphbased approach. Our model can be viewed as an extension of the joint POS tagging and dependency parsing model jPTDP-v2 (Nguyen and Verspoor, 2018),2 where we incorporate a BiLSTM-CRF for word boundary pre"
U19-1004,P12-1110,0,0.0169315,"n. (e.g. “Tˆoi l`a sinh viˆen”) is provided as the input to the POS tagger, which automatically generates POS-annotated text (e.g. “Tˆoi/PRON l`a/VERB sinh viˆen/NOUN”) which is in turn fed to the parser. See Figure 1 for the final parsing output. However, Vietnamese word segmenters and POS taggers have a non-trivial error rate, thus leading to error propagation. A solution to these problems is to develop models for jointly learning word segmentation, POS tagging and dependency parsing, such as those that have been actively explored for Chinese. These include traditional feature-based models (Hatori et al., 2012; Qian and Liu, 2012; Zhang et al., 2014, 2015) and neural models (Kurita et al., 2017; Li et al., 2018). These models construct transition-based frameworks at character level. In this paper, we present a new multi-task learning model for joint word segmentation, POS tagging and dependency parsing. More specifically, our model can be viewed as an extension of the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016), that incorporates BiLSTM-CRF-based architectures (Huang et al., 2015) to predict the segmentation and POS tags. To the best of our knowledge, our model is the first"
U19-1004,W08-1301,0,0.0854158,"Missing"
U19-1004,D13-1032,0,0.0455956,"Missing"
U19-1004,U16-1017,1,0.931761,"edding epj . We create a sequence of vectors z1:n as input for the dependency parsing component, in which each zj is resulted by concatenating the word vector representation xj (from Equation 4) and the corresponding (P) POS tag embedding epj . The dependency parsing component uses a BiLSTM (BiLSTMDEP ) to learn latent feature representations from the input z1:n : zj = xj ◦ e(pPj) (DEP) rj (8) = BiLSTMDEP (z1:n , j) (9) (DEP) Based on latent feature vectors rj , either a transition-based or graph-based neural architecture can be applied for dependency parsing (Kiperwasser and Goldberg, 2016). Nguyen et al. (2016) show that in both neural network-based and traditional feature-based categories, graph-based parsers perform better than transition-based parsers for Vietnamese. Thus, our parsing component is constructed similarly to the BIST graph-based dependency parser from Kiperwasser and Goldberg (2016). A difference is that (DEP) we use FFNNs to split rj into head and dependent representations: (A - H) (DEP)  hj = FFNNArc-Head rj (10) (A - D) (DEP)  hj = FFNNArc-Dep rj (11) (L - H) (DEP)  (12) hj = FFNNLabel-Head rj  (L - D) (DEP) hj = FFNNLabel-Dep rj (13) To score a potential dependency arc, we u"
U19-1004,Q16-1023,0,0.164236,"ointly learning word segmentation, POS tagging and dependency parsing, such as those that have been actively explored for Chinese. These include traditional feature-based models (Hatori et al., 2012; Qian and Liu, 2012; Zhang et al., 2014, 2015) and neural models (Kurita et al., 2017; Li et al., 2018). These models construct transition-based frameworks at character level. In this paper, we present a new multi-task learning model for joint word segmentation, POS tagging and dependency parsing. More specifically, our model can be viewed as an extension of the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016), that incorporates BiLSTM-CRF-based architectures (Huang et al., 2015) to predict the segmentation and POS tags. To the best of our knowledge, our model is the first one which is proposed to jointly learn these three tasks for Vietnamese. Experiments on Vietnamese benchmark datasets show that our model produces state-of-the-art or competitive results. 2 Our proposed model As illustrated in Figure 1, our joint multi-task model can be viewed as a hierarchical mixture of three components: word segmentation, POS tagging and dependency parsing. In particular, our word segmentation component formal"
U19-1004,L18-1410,1,0.75848,"S tagging component also uses a BiLSTM-CRF to predict POS tags from the sequence of segmented words. Based on the input segmented words and their predicted POS tags, our dependency parsing component uses a graph-based architecture similarly to the one from Kiperwasser and Goldberg (2016) to decode dependency arcs and labels. Syllable vector representation: Given an input sentence S of m syllables s1 , s2 , ..., sm , we apply an initial word segmenter to produce initial BIO word-boundary tags b1 , b2 , ..., bm . Following the state-of-the-art Vietnamese word segmenter VnCoreNLP’s RDRsegmenter (Nguyen et al., 2018b), our initial word segmenter is based on the lexicon-based longest matching strategy (Poowarawan, 1986). We create a vector vi to represent each ith syllable in the input sentence S by (S) concatenating its syllable embedding esi and its (B) initial word-boundary tag embedding ebi : (B) vi = e(sSi ) ◦ ebi (1) Word segmentation (WSeg): The WSeg component uses a BiLSTM (BiLSTMWS ) to learn a latent feature vector representing the ith syllable from a sequence of vectors v1:m : (WS) ri = BiLSTMWS (v1:m , i) (2) The WSeg component then uses a single-layer feed-forward network (FFNNWS ) to perform"
U19-1004,K18-2008,1,0.844766,"d by stack propagation based methods (Zhang and Weiss, 2016; Hashimoto et al., 2017) which are joint models for POS tagging and dependency parsing. For dependency parsing, the Stack-propagation model (Zhang and Weiss, 2016) uses a transitionbased approach, and the joint multi-task model JMT (Hashimoto et al., 2017) uses a head selection based approach which produces a probability distribution over possible heads for each word (Zhang et al., 2017), while our model uses a graphbased approach. Our model can be viewed as an extension of the joint POS tagging and dependency parsing model jPTDP-v2 (Nguyen and Verspoor, 2018),2 where we incorporate a BiLSTM-CRF for word boundary prediction. Other improvements to jPTDP-v2 include: (i) instead of using ‘local’ single wordbased character-level embeddings, we use ‘global’ sentence-level context for learning word embeddings (see equations 2 and 5), (ii) we use a CRF layer for POS tagging instead of a softmax layer, and (iii) following Dozat and Manning (2017), we employ head and dependent projection representations (in Equations 10–13) as feature vectors for dependency parsing rather than the top recurrent states (in Equation 9). 3 Experimental setup Datasets: We follo"
U19-1004,P17-1111,0,0.0285037,"matically generates POS-annotated text (e.g. “Tˆoi/PRON l`a/VERB sinh viˆen/NOUN”) which is in turn fed to the parser. See Figure 1 for the final parsing output. However, Vietnamese word segmenters and POS taggers have a non-trivial error rate, thus leading to error propagation. A solution to these problems is to develop models for jointly learning word segmentation, POS tagging and dependency parsing, such as those that have been actively explored for Chinese. These include traditional feature-based models (Hatori et al., 2012; Qian and Liu, 2012; Zhang et al., 2014, 2015) and neural models (Kurita et al., 2017; Li et al., 2018). These models construct transition-based frameworks at character level. In this paper, we present a new multi-task learning model for joint word segmentation, POS tagging and dependency parsing. More specifically, our model can be viewed as an extension of the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016), that incorporates BiLSTM-CRF-based architectures (Huang et al., 2015) to predict the segmentation and POS tags. To the best of our knowledge, our model is the first one which is proposed to jointly learn these three tasks for Vietnamese. Experiments o"
U19-1004,U17-1013,1,0.606099,"Missing"
U19-1004,L18-1341,0,0.0327434,"t Vietnamese constituency treebank (Nguyen et al., 2009) into a dependency treebank. However, Thi et al. (2013) do not clarify how dependency labels are inferred; also, they ignore syntactic information encoded in grammatical function tags, and unable to deal with coordination and empty category cases.6 Nguyen et al. (2014) later present a new conversion method to tackle all those issues, producing the high quality dependency treebank VnDT which is then widely used in Vietnamese dependency parsing research (Nguyen and Nguyen, 2015, 2016; Nguyen et al., 2016, 2018a; Vu et al., 2018). Recently, Nguyen (2018) 6 Thi et al. (2013) reformed their dependency treebank with the UD annotation scheme to create a Vietnamese UD treebank in 2017. Note that the CoNLL 2017 & 2018 multilingual parsing shared tasks also provided F1 scores for word segmentation, POS tagging and dependency parsing on this Vietnamese UD treebank. However, this UD treebank is small (containing about 1,400 training sentences), thus it might not be ideal to draw a reliable conclusion. manually builds another Vietnamese dependency treebank—BKTreebank—consisting of about 7K sentences based on the Stanford Dependencies annotation scheme"
U19-1004,N16-1030,0,0.0359737,"which obtains the highest performance to date. Nguyen et al. (2017) briefly review word segmentation and POS tagging approaches for Vietnamese. In addition, Nguyen et al. (2017) also present an empirical comparison between state-of-the-art feature- and neural network-based models for Vietnamese POS tagging, and show that a conventional featurebased model performs better than neural networkbased models. In particular, on the VLSP 2013 POS tagging dataset, MarMoT (Mueller et al., 2013) obtains better accuracy than BiLSTM-CRFbased models with LSTM- and CNN-based character level word embeddings (Lample et al., 2016; Ma and Hovy, 2016). Vu et al. (2018) incorporate RDRsegmenter and MarMoT as the word segmentation and POS tagging components of VnCoreNLP, respectively. Thi et al. (2013) propose a conversion method to automatically convert the manually built Vietnamese constituency treebank (Nguyen et al., 2009) into a dependency treebank. However, Thi et al. (2013) do not clarify how dependency labels are inferred; also, they ignore syntactic information encoded in grammatical function tags, and unable to deal with coordination and empty category cases.6 Nguyen et al. (2014) later present a new conversion"
U19-1004,P16-1101,0,0.0389038,"ghest performance to date. Nguyen et al. (2017) briefly review word segmentation and POS tagging approaches for Vietnamese. In addition, Nguyen et al. (2017) also present an empirical comparison between state-of-the-art feature- and neural network-based models for Vietnamese POS tagging, and show that a conventional featurebased model performs better than neural networkbased models. In particular, on the VLSP 2013 POS tagging dataset, MarMoT (Mueller et al., 2013) obtains better accuracy than BiLSTM-CRFbased models with LSTM- and CNN-based character level word embeddings (Lample et al., 2016; Ma and Hovy, 2016). Vu et al. (2018) incorporate RDRsegmenter and MarMoT as the word segmentation and POS tagging components of VnCoreNLP, respectively. Thi et al. (2013) propose a conversion method to automatically convert the manually built Vietnamese constituency treebank (Nguyen et al., 2009) into a dependency treebank. However, Thi et al. (2013) do not clarify how dependency labels are inferred; also, they ignore syntactic information encoded in grammatical function tags, and unable to deal with coordination and empty category cases.6 Nguyen et al. (2014) later present a new conversion method to tackle all"
U19-1004,D12-1046,0,0.0264407,"nh viˆen”) is provided as the input to the POS tagger, which automatically generates POS-annotated text (e.g. “Tˆoi/PRON l`a/VERB sinh viˆen/NOUN”) which is in turn fed to the parser. See Figure 1 for the final parsing output. However, Vietnamese word segmenters and POS taggers have a non-trivial error rate, thus leading to error propagation. A solution to these problems is to develop models for jointly learning word segmentation, POS tagging and dependency parsing, such as those that have been actively explored for Chinese. These include traditional feature-based models (Hatori et al., 2012; Qian and Liu, 2012; Zhang et al., 2014, 2015) and neural models (Kurita et al., 2017; Li et al., 2018). These models construct transition-based frameworks at character level. In this paper, we present a new multi-task learning model for joint word segmentation, POS tagging and dependency parsing. More specifically, our model can be viewed as an extension of the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016), that incorporates BiLSTM-CRF-based architectures (Huang et al., 2015) to predict the segmentation and POS tags. To the best of our knowledge, our model is the first one which is propose"
U19-1004,N18-5012,1,0.767952,"diction. Other improvements to jPTDP-v2 include: (i) instead of using ‘local’ single wordbased character-level embeddings, we use ‘global’ sentence-level context for learning word embeddings (see equations 2 and 5), (ii) we use a CRF layer for POS tagging instead of a softmax layer, and (iii) following Dozat and Manning (2017), we employ head and dependent projection representations (in Equations 10–13) as feature vectors for dependency parsing rather than the top recurrent states (in Equation 9). 3 Experimental setup Datasets: We follow the setup used in the Vietnamese NLP toolkit VnCoreNLP (Vu et al., 2018). For word segmentation and POS tagging, we use standard datasets from the Vietnamese Language and Speech Processing (VLSP) 2013 shared tasks.3 To train the word segmentation layer, we use 75K manually word-segmented sentences in which 70K sentences are used for training and 5K sentences are used for development. For POS tagging, we use 27,870 manually word-segmented and POS-annotated sentences in which 27K and 870 sentences are used for training and development, respectively. For both tasks, the test set consists of 2120 manually word-segmented and POSannotated sentences. To train the depende"
U19-1004,K17-3001,0,0.0576976,"Missing"
U19-1004,K18-2001,0,0.0385459,"Missing"
U19-1004,P14-1125,0,0.013717,"ed as the input to the POS tagger, which automatically generates POS-annotated text (e.g. “Tˆoi/PRON l`a/VERB sinh viˆen/NOUN”) which is in turn fed to the parser. See Figure 1 for the final parsing output. However, Vietnamese word segmenters and POS taggers have a non-trivial error rate, thus leading to error propagation. A solution to these problems is to develop models for jointly learning word segmentation, POS tagging and dependency parsing, such as those that have been actively explored for Chinese. These include traditional feature-based models (Hatori et al., 2012; Qian and Liu, 2012; Zhang et al., 2014, 2015) and neural models (Kurita et al., 2017; Li et al., 2018). These models construct transition-based frameworks at character level. In this paper, we present a new multi-task learning model for joint word segmentation, POS tagging and dependency parsing. More specifically, our model can be viewed as an extension of the BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016), that incorporates BiLSTM-CRF-based architectures (Huang et al., 2015) to predict the segmentation and POS tags. To the best of our knowledge, our model is the first one which is proposed to jointly learn t"
U19-1004,E17-1063,0,0.0160388,"umming LWS , LPOS , LARC and LLABEL losses prior to computing gradients. Model parameters are learned to minimize the sum of the losses. Discussion: Our model is inspired by stack propagation based methods (Zhang and Weiss, 2016; Hashimoto et al., 2017) which are joint models for POS tagging and dependency parsing. For dependency parsing, the Stack-propagation model (Zhang and Weiss, 2016) uses a transitionbased approach, and the joint multi-task model JMT (Hashimoto et al., 2017) uses a head selection based approach which produces a probability distribution over possible heads for each word (Zhang et al., 2017), while our model uses a graphbased approach. Our model can be viewed as an extension of the joint POS tagging and dependency parsing model jPTDP-v2 (Nguyen and Verspoor, 2018),2 where we incorporate a BiLSTM-CRF for word boundary prediction. Other improvements to jPTDP-v2 include: (i) instead of using ‘local’ single wordbased character-level embeddings, we use ‘global’ sentence-level context for learning word embeddings (see equations 2 and 5), (ii) we use a CRF layer for POS tagging instead of a softmax layer, and (iii) following Dozat and Manning (2017), we employ head and dependent project"
U19-1004,N15-1005,0,0.0392893,"Missing"
U19-1004,P16-1147,0,0.0232822,"Missing"
U19-1014,N18-2075,0,0.0279898,"zed as part of CLEF-IP 2012 (Piroi et al., 2012). In the sub-task titled “Passage Retrieval Starting From Claims”, participants were required to extract passages from chemical patents that are relevant to a given claim. The difference between our task and theirs is that the output of our task is all chemical reactions mentioned in a given patent, independent of any claim. In addition, the CLEF-IP task does not require the identification of reaction spans. That is, they deal with each passage independently, ignoring ordering. The proposed task can also be viewed as a text segmentation problem. Koshorek et al. (2018) formulated the text segmentation task on general domain corpora such as Wikipedia as a supervised learning problem, and proposed a twolevel bidirectional LSTM model to learn to detect text spans. In particular, they used a softmax layer on top of a standard BiLTSM architecture for segmentation prediction. We experiment with a BiLTSM-CRF architecture as a document-level training method as described in Section 4, i.e. we use a CRF layer to obtain the document-level label sequence, instead of applying a softmax classifier on top of the BiLSTM. 3 3.1 Task and Dataset Task Formulation A patent doc"
U19-1014,P09-1113,0,0.0277244,"kground knowledge. Indeed, the BiLSTM-CRF model trained at the documentlevel performed much better than the paragraphlevel classification methods. The performance of the baseline methods presented in this paper is still not satisfactory considering the complex downstream tasks such as event extraction. We believe that both the models and the corpus have potential to be improved. As future work, we plan to explore more efficient document-level training methods, and, in particular, methods that work well on noisy training sets. For instance, techniques successfully used for distant supervision (Mintz et al., 2009) may be effective. Furthermore, although we used only textual information, patent documents contain substantial visual information (e.g., images of compounds, or tables) that may be helpful to properly understand a reaction description. Longer term, we will also tackle finer-grained information extraction for chemical reactions utilizing the output of this task. This step involves extracting the details of the detected reactions, that is, inferring the underlying structure of the reactions themselves. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. T"
U19-1014,N18-1202,0,0.0449915,"Missing"
U19-1014,E99-1023,0,0.134349,"copper(I)bromide dimethyl sulfide (2.17 g, 10.56 mmol) was dissolved ... I Figure 2: Illustration of our reaction span detection task. would not be able to detect reactions as a whole or capture reaction substructure. Figure 2 shows an example of an input and goldstandard output of the reaction span detection task. A patent document is given as a sequence of paragraphs. The task is to detect a span of contiguous paragraphs that describe a single chemical reaction. In our corpus, we provide paragraph-level label sequences over paragraphs in patent documents, following the IOB2 tagging scheme (Tjong et al., 1999). The definition of “reaction spans” in our dataset follows the extraction rules of the original database. In principle, a reaction is extracted from a patent if the requisite information about the reaction (e.g., starting materials, reaction conditions and target compounds) is provided within the patent document and there is no obvious error or inconsistency in the description. Typically a reaction constitutes an example section or a subsection beginning with a title paragraph such as Example 1, Step 1 and Preparation of [product name], as shown in Figure 1. However, it is also commonly the c"
U19-1014,W19-5035,1,0.833679,"xt of each paragraph as input, with a maximum length of 128 tokens.5 For tokenization, we used the OSCAR4 tokenizer (Jessop et al., 2011), as it is customized to chemical text mining. Equation (1) formulates the input token-level representation for the BiLSTM paragraph encoder in the form of (context-insensitive) word embeddings, contextualized word embeddings, and feature embeddings. For the word embeddings eWE wi and contextualized embeddings eCW wi |p , we employ Word2Vec (Mikolov et al., 2013) and ELMo (Peters et al., 2018), respectively, both pre-trained on chemical patent documents from Zhai et al. (2019). These embeddings are fixed during training. We denote our encoder employing only the pre-trained word and contextualized embeddings CW (i.e. ei = eWE wi ⊕ ewi |p ) as W 2 V +ELM O . We also explore additional learnable feature embeddings eFT fi (in Equation 1) based on the output of a chemical named entity recognizer (Zhai et al., 2019). This named entity recognizer was trained on a patent corpus named Reaxys® Gold data (Akhondi et al., 2019). For self-containment purpose we show the entity label set of Reaxys® Gold data in Table 4 in the Appendix. As the label set has two levels of granular"
W14-2621,P11-1015,0,0.302129,"independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128–135, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Linguistics predicted score as the rating-based feature for learning sentiment categorization. By combining the rating-based feature with unigrams, bigrams and trigrams, we then present the results from sentiment classification experiments on the benchmark datasets published by Pang and Lee (2004) and Maas et al. (2011). To sum up, the contributions of our study are: al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sent"
W14-2621,P06-2079,0,0.0289456,"9 3.2 N-gram Features of 25000 labeled reviews and a test set of 25000 labeled reviews, where training and test sets have 12500 positive reviews and 12500 negative reviews in each. In most related works, unigrams are considered as the most basic features, in which each document is represented as a collection of unique unigram words where each word is considered as an individual feature. In addition, we take into account bigrams and trigrams since a combination of unigram, bigram and trigram features (N-grams) could outperform a baseline performance based on unigram features as pointed out in (Ng et al., 2006; Martineau and Finin, 2009; Wang and Manning, 2012). We calculate the value of the N-gram feature ith by using term frequency - inverse document frequency (tf*idf) weighting scheme for the document D as follows: |{D}| N gramiD = log(1 + tfiD ) ∗ log dfi where tfiD is the occurrence frequency of the feature ith in document D, |{D} |is the number of documents in the data corpus {D}, and dfi is the number of documents containing the feature ith . We then normalize N-gram feature vector P of the document D as follows: −−−−−→ −−−−−−−→ ηN gramD = 4 Machine learning algorithm. We utilize SVM impleme"
W14-2621,I13-1114,1,0.462436,"hed by Pang and Lee (2004) and Maas et al. (2011). To sum up, the contributions of our study are: al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sentiment classifier applying reject option, where documents rejected at the first stage are forwarded to be classified at the second stage. • Propose a novel rating-based feature and describe regression models learned from the external dataset to predict the feature value for the reviews in the two experimental datasets. • Achieve state-of-the-art performances in the use of the rating-based feature for the sentiment polarity classification task on the two datasets. 3 We apply a supervised machine learning approach to handle the task of document-level sentiment polarit"
W14-2621,baccianella-etal-2010-sentiwordnet,0,0.0220094,"set consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, In this paper, we firstly introduce a novel rating"
W14-2621,P04-1035,0,0.506257,"earned from an external independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128–135, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Linguistics predicted score as the rating-based feature for learning sentiment categorization. By combining the rating-based feature with unigrams, bigrams and trigrams, we then present the results from sentiment classification experiments on the benchmark datasets published by Pang and Lee (2004) and Maas et al. (2011). To sum up, the contributions of our study are: al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) const"
W14-2621,W02-1011,0,0.0342326,"entations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classifying polarity sentiment. For a review with no associated score, we could predict a score for the review in the use of a regression model learned from an external independent dataset of reviews and their actual corresponding scores. We refer to the 128 Proc"
W14-2621,J11-2001,0,0.0186461,"ovie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Recently, most sentiment polarity classification systems (Whitelaw et al., 2005; Kennedy and Inkpen, 2006; Martineau and Finin, 2009; Maas et al., 2011; Tu et al., 2012; Wang and Manning, 2012; Nguyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, In this paper, we firstly introduce a novel rating-based feature for the"
W14-2621,P12-2066,0,0.0277513,"Missing"
W14-2621,P02-1053,0,0.0247522,"guyen et al., 2013) have obtained state-of-the-art results by employing machine learning techniques using combination of various features such as Ngrams, syntactic and semantic representations as well as exploiting lexicon resources (Wilson et al., 2005; Ng et al., 2006; Baccianella et al., 2010; Taboada et al., 2011). Introduction This paper focuses on document-level sentiment classification on polarity reviews. Specifically, the document-level sentiment analysis is to identify either a positive or negative opinion in a given opinionated review (Pang and Lee, 2008; Liu, 2010). In early work, Turney (2002) proposed an unsupervised learning algorithm to classify reviews by calculating the mutual information between a given phrase and reference words “excellent” and “poor”. Pang et al. (2002) applied supervised learners of Naive Bayes, In this paper, we firstly introduce a novel rating-based feature for the sentiment polarity classification task. Our rating-based feature can be seen by that the scores – which users employ to rate entities on review websites – could bring useful information for improving the performance of classifying polarity sentiment. For a review with no associated score, we c"
W14-2621,P12-2018,0,0.154165,"inguistics predicted score as the rating-based feature for learning sentiment categorization. By combining the rating-based feature with unigrams, bigrams and trigrams, we then present the results from sentiment classification experiments on the benchmark datasets published by Pang and Lee (2004) and Maas et al. (2011). To sum up, the contributions of our study are: al. (2011) introduced a model to catch sentiment information and word meanings. Tu et al. (2012) proposed an approach utilizing high-impact parse features for convolution kernels in document-level sentiment recognition. Meanwhile, Wang and Manning (2012) obtained a strong and robust performance by identifying simple NB and SVM variants. Dahl et al. (2012) applied the restricted Boltzmann machine to learn representations capturing meaningful syntactic and semantic properties of words. In addition, Nguyen et al. (2013) constructed a two-stage sentiment classifier applying reject option, where documents rejected at the first stage are forwarded to be classified at the second stage. • Propose a novel rating-based feature and describe regression models learned from the external dataset to predict the feature value for the reviews in the two experi"
W14-2621,H05-1044,0,0.188686,"Missing"
W14-5418,D10-1115,0,0.197781,"Missing"
W15-2813,P14-2135,0,0.0313736,"igure 1 for the answer). Besides the inherent coolness of the task, it has many potential applications. Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properIn cognitive science, there is a lively debate on whether abstract words have embodied representations, (Barsalou and Wiemer-Hastings, 2005; Lakoff and Johnson, 1999), an issue that has recently attracted the attention of the distributed semantics community (Hill and Korhonen, 2014; Kiela et al., 2014; Lazaridou et al., 2015). An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts. Recent work in neuroscience attempts to generate images of “what people think”, as encoded in vector-based representations of fMRI patterns (Naselaris et al., 2009; Nishimoto et al., 2011). With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of the same words. 81 Proceedings of the 2015 Workshop on Vision and Language (VL’15), pages 81–86, c Lisbon, Portugal, 18 September 2015."
W15-2813,N13-1016,0,0.0305393,"capturing much faster than from textual neighbour lists. For example, a more “topical” model might produce pictures depicting the wider scenes in which objects occur (a ball being dribbled by soccer players), whereas a model capturing strictly conceptual aspects might produce narrow views of the denoted objects (a close-up of the ball). Image synthesis could also be used to explore the effect of different input corpora on representations: e.g., given a historical corpus, generate images for the car word representations induced from early 20th-century vs. 21st-century texts. As a last example, Aletras and Stevenson (2013) proposed to examine the topics of Topic Models by associating them with images retrieved from the Web. Given that topics are represented by vectors, we could directly generate images representing these topics. We introduce the task of visualizing distributed semantic representations by generating images from word vectors. Given the corpus-based vector encoding the word broccoli, we convert it to a visual representation by means of a cross-modal mapping function, and then use the mapped representation to generate an image of broccoli as “dreamed” by the distributed model. We propose a baseline"
W15-2813,J99-4009,0,0.0671009,"ord looks like, according to the model. Given, say, the word2vec vector of broccoli, we want to know how broccoli looks like to word2vec (see Figure 1 for the answer). Besides the inherent coolness of the task, it has many potential applications. Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properIn cognitive science, there is a lively debate on whether abstract words have embodied representations, (Barsalou and Wiemer-Hastings, 2005; Lakoff and Johnson, 1999), an issue that has recently attracted the attention of the distributed semantics community (Hill and Korhonen, 2014; Kiela et al., 2014; Lazaridou et al., 2015). An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts. Recent work in neuroscience attempts to generate images of “what people think”, as encoded in vector-based representations of fMRI patterns (Naselaris et al., 2009; Nishimoto et al., 2011). With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of"
W15-2813,P14-1023,1,0.672833,"ges extracted from ImageNet (Deng et al., 2009) representing 5.1K distinct seen words. The dreamed word set includes 510 concrete, base-level concepts from the semantic norms of McRae et al. (2005) (we excluded 31 McRae concepts because they were marked as ambiguous there, or for technical reasons). Linguistic and Visual Representations For all seen and dreamed concepts, we build 300dimensional word vectors with the word2vec toolkit,1 choosing the CBOW method.2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks (Baroni et al., 2014). Word vectors are induced from a corpus of 2.8 billion words.3 The 500K images are represented by 4096-dimensional visual vectors, extracted with the pre-trained convolutional neural network model of Krizhevsky et al. (2012) through the Caffe toolkit (Jia et al., 2014). The main aim of this paper is to present proofof-concept evidence that the task is feasible. To this end, we rely on state-of-the-art word representation and cross-modality mapping methods, but we adopt an image synthesis strategy that could be seen as an interesting baseline to compare other approaches against. Briefly, our p"
W15-2813,P14-1132,1,0.919895,"nging. However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an attempt to understand the inner workings of visual feature extraction algorithms (Zeiler and Fergus, 2014). 2 General setup We refer to the words we generate images for as dreamed words, and to the corresponding images as dreams. We refer to the set of words that are associated to real pictures as seen words. The real picture set contains approximately 500K images extracted from ImageNet (Deng et al., 2009) representing 5.1K distinct seen wor"
W15-2813,N15-1016,1,0.656575,"er). Besides the inherent coolness of the task, it has many potential applications. Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properIn cognitive science, there is a lively debate on whether abstract words have embodied representations, (Barsalou and Wiemer-Hastings, 2005; Lakoff and Johnson, 1999), an issue that has recently attracted the attention of the distributed semantics community (Hill and Korhonen, 2014; Kiela et al., 2014; Lazaridou et al., 2015). An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts. Recent work in neuroscience attempts to generate images of “what people think”, as encoded in vector-based representations of fMRI patterns (Naselaris et al., 2009; Nishimoto et al., 2011). With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of the same words. 81 Proceedings of the 2015 Workshop on Vision and Language (VL’15), pages 81–86, c Lisbon, Portugal, 18 September 2015. 2015 Association for Comp"
W15-2813,N13-1090,0,0.220487,"d like to move beyond words, towards generating images depicting the meaning of phrases (e.g., an angry cat vs. a cute cat vs. a white cat) and sentences. This would nicely complement current work on generating verbal descriptions of images (Karpathy and FeiFei, 2015; Kiros et al., 2014) with the inverse task of generating images from verbal descriptions. Generating images from vectorial word representations is of course extremely challenging. However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an att"
W15-2813,D14-1162,0,0.0750838,"ds generating images depicting the meaning of phrases (e.g., an angry cat vs. a cute cat vs. a white cat) and sentences. This would nicely complement current work on generating verbal descriptions of images (Karpathy and FeiFei, 2015; Kiros et al., 2014) with the inverse task of generating images from verbal descriptions. Generating images from vectorial word representations is of course extremely challenging. However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an attempt to understand the inner workings"
W15-2813,Q14-1017,0,0.0349871,"relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an attempt to understand the inner workings of visual feature extraction algorithms (Zeiler and Fergus, 2014). 2 General setup We refer to the words we generate images for as dreamed words, and to the corresponding images as dreams. We refer to the set of words that are associated to real pictures as seen words. The real picture set contains approximately 500K images extracted from ImageNet (Deng et al., 2009) representing 5.1K distinct seen words. The dreamed word s"
W15-2813,D14-1032,0,\N,Missing
W18-2314,P05-1053,0,0.526405,"Missing"
W18-2314,D17-1191,0,0.0371973,"for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark BioCreative-V CDR corpu"
W18-2314,P16-1200,0,0.129137,"cations (Bach and Badaskar, 2007). Traditional approaches for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, an"
W18-2314,P16-1101,0,0.031943,"relations can be expressed across sentence boundaries; they can extend over distances of hundreds of word tokens. As LSTM models can be difficult to apply to very long word sequences (Bradbury et al., 2017), CNN models may be better suited for this task. New domain-specific terms arise frequently in biomedical text data, requiring the capture of unknown words in practical relation extraction applications in this context. Recent research has shown that character-based word embeddings enable capture of unknown words, helping to improve performance on many NLP tasks (dos Santos and Gatti, 2014; Ma and Hovy, 2016; Lample et al., 2016; Plank et al., 2016; Nguyen et al., 2017). This may be particularly relevant for terms such as gene or chemical names, which often have identifiable morphological structure (Krallinger et al., 2017). We investigate the value of character-based word embeddings in a standard CNN model for relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). To the best of our knowledge, We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM,"
W18-2314,J93-2004,0,0.0608846,"+CNNchar CNN+LSTMchar Linear+TK (Panyam et al., 2016) SVM (Peng et al., 2016) SVM (+dev.) (Peng et al., 2016) SVM (+dev.+18K) (Peng et al., 2016) SVM (+dev.) (Xu et al., 2016) SVM (+dev.) (Pons et al., 2016) P 62.0 59.3 64.9 55.6 60.9 55.7 55.6 53.2 54.8 57.0 56.8 63.6 62.1 68.2 71.1 65.8 73.1 R 55.1 62.3 49.3 68.4 59.5 68.1 70.8 69.7 69.0 68.6 68.8 59.8 64.2 66.0 72.6 68.6 67.6 F1 58.3 60.8 56.0 61.3 60.2 61.3 62.1 60.3 61.1 62.3 62.2 61.7 63.1 67.1 71.8 67.2 70.2 dency parser (Chen and Manning, 2014). However, this dependency parser was trained on the Penn Treebank (in the newswire domain) (Marcus et al., 1993); training on a domain-specific treebank such as CRAFT (Bada et al., 2012) should help to improve results (Verspoor et al., 2012). We also achieve slightly better scores than the more complex model BRAN (Verga et al., 2017), the Biaffine Relation Attention Network, based on the Transformer self-attention model (Vaswani et al., 2017). BRAN additionally uses byte pair encoding (Gage, 1994) to construct a vocabulary of subword units for tokenization. Using subword tokens to capture rare or unknown words has been demonstrated to be useful in machine translation (Sennrich et al., 2016) and likely c"
W18-2314,P16-2067,0,0.0267964,"nce boundaries; they can extend over distances of hundreds of word tokens. As LSTM models can be difficult to apply to very long word sequences (Bradbury et al., 2017), CNN models may be better suited for this task. New domain-specific terms arise frequently in biomedical text data, requiring the capture of unknown words in practical relation extraction applications in this context. Recent research has shown that character-based word embeddings enable capture of unknown words, helping to improve performance on many NLP tasks (dos Santos and Gatti, 2014; Ma and Hovy, 2016; Lample et al., 2016; Plank et al., 2016; Nguyen et al., 2017). This may be particularly relevant for terms such as gene or chemical names, which often have identifiable morphological structure (Krallinger et al., 2017). We investigate the value of character-based word embeddings in a standard CNN model for relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). To the best of our knowledge, We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM, to learn word vector representations from"
W18-2314,P09-1113,0,0.385071,"Missing"
W18-2314,P16-1105,0,0.0328705,"ised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark BioCreative-V CDR corpus for chemical-induced disease relation extraction (Li et al., 201"
W18-2314,E17-1110,0,0.187073,"Missing"
W18-2314,D15-1203,0,0.186853,"of practical applications (Bach and Badaskar, 2007). Traditional approaches for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-ba"
W18-2314,K17-1032,0,0.0180127,"ches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark BioCreative-V CDR corpus for chemical-induced disease relation extraction (Li et al., 2016a), obtaining state-of-theart results. 2 Convolutional layer: This layer uses different filters to extract features from the input matrix S = [v 1 , v 2 , ..., v n ]T"
W18-2314,C14-1220,0,0.898304,"unknown words in practical relation extraction applications in this context. Recent research has shown that character-based word embeddings enable capture of unknown words, helping to improve performance on many NLP tasks (dos Santos and Gatti, 2014; Ma and Hovy, 2016; Lample et al., 2016; Plank et al., 2016; Nguyen et al., 2017). This may be particularly relevant for terms such as gene or chemical names, which often have identifiable morphological structure (Krallinger et al., 2017). We investigate the value of character-based word embeddings in a standard CNN model for relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). To the best of our knowledge, We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM, to learn word vector representations from character embeddings. Through a task on the BioCreative-V CDR corpus, extracting relationships between chemicals and diseases, we show that models exploiting the character-based word representations improve on models that do not use this information, obtaining state-of-the-art result relative to previous"
W18-2314,P16-1162,0,0.0511875,"swire domain) (Marcus et al., 1993); training on a domain-specific treebank such as CRAFT (Bada et al., 2012) should help to improve results (Verspoor et al., 2012). We also achieve slightly better scores than the more complex model BRAN (Verga et al., 2017), the Biaffine Relation Attention Network, based on the Transformer self-attention model (Vaswani et al., 2017). BRAN additionally uses byte pair encoding (Gage, 1994) to construct a vocabulary of subword units for tokenization. Using subword tokens to capture rare or unknown words has been demonstrated to be useful in machine translation (Sennrich et al., 2016) and likely captures similar information to character embeddings. However, Verga et al. (2017) do not provide comparative results using only original word tokens. Therefore, it is difficult to assess the usefulness specifically of using byte-pair encoded subword tokens in the CID relation extraction task, as compared to the impact of the full model architecture. We also plan to explore the usefulness of subword tokens in the baseline CNN for future work, to enable comparison with the improvement when using the character-based word embeddings. It is worth noting that both CNN+CNNchar and CNN+LS"
W18-2314,D17-1186,0,0.157191,"ditional approaches for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark"
W18-2314,D17-1182,0,0.0342266,"Missing"
W18-2314,P04-1054,0,\N,Missing
W18-2314,D14-1082,0,\N,Missing
W18-2314,W15-1506,0,\N,Missing
W18-2314,C14-1008,0,\N,Missing
W18-2314,N16-1030,0,\N,Missing
W18-2314,W16-2922,0,\N,Missing
W18-2314,K17-3014,1,\N,Missing
W18-2314,P17-1085,0,\N,Missing
W18-2314,H05-1091,0,\N,Missing
W18-2314,N16-1034,0,\N,Missing
W18-2314,C16-1139,0,\N,Missing
W18-5605,P16-1101,0,0.379418,"ng state-of-art performance that outperformed traditional feature-based models. Luo et al. (2018) further improved on this result on a chemical NER task by adding an attention layer between the BiLSTM and CRF layers (Att-BiLSTM-CRF). In an experiment by Reimers and Gurevych (2017b), optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). However, the choice of CNN-based (Ma and Hovy, 2016) or LSTM-based character-level word embeddings (Lample et al., 2016) did not affect the performance significantly. Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred approach. In this paper, we implement and compare models with each type of word embedding to generate empirical results for the tasks of chemical and disease NER, using the BioCreative V CDR corpus (Li et al., 2016). These BNER categories are the most searched entities in the biomedical literature (Islamaj Dogan et al., 2009), and henc"
W18-5605,D17-1035,0,0.0949137,"morphology such as common prefixes (e.g., di-) or suffixes (e.g., -ase). Features that capture word-internal characteristics have been shown to be effective for BNER tasks in CRF models (Klinger et al., 2008). Lyu et al. (2017) applied a BiLSTM-CRF model with LSTM-based character-level word embeddings to a gene and protein NER task, demonstrating state-of-art performance that outperformed traditional feature-based models. Luo et al. (2018) further improved on this result on a chemical NER task by adding an attention layer between the BiLSTM and CRF layers (Att-BiLSTM-CRF). In an experiment by Reimers and Gurevych (2017b), optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). However, the choice of CNN-based (Ma and Hovy, 2016) or LSTM-based character-level word embeddings (Lample et al., 2016) did not affect the performance significantly. Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred"
W18-5605,N16-1030,0,0.797536,"based models. Luo et al. (2018) further improved on this result on a chemical NER task by adding an attention layer between the BiLSTM and CRF layers (Att-BiLSTM-CRF). In an experiment by Reimers and Gurevych (2017b), optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). However, the choice of CNN-based (Ma and Hovy, 2016) or LSTM-based character-level word embeddings (Lample et al., 2016) did not affect the performance significantly. Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred approach. In this paper, we implement and compare models with each type of word embedding to generate empirical results for the tasks of chemical and disease NER, using the BioCreative V CDR corpus (Li et al., 2016). These BNER categories are the most searched entities in the biomedical literature (Islamaj Dogan et al., 2009), and hence particularly important to study. The results show that models with"
W19-5035,D14-1012,0,0.023505,"tion performance of these systems and their ensembles (Habibi et al., 2016). The application of the tmChem model trained on chemical literature corpora of the BioCreative IV CHEMDNER task (Krallinger et al., 2015) and the ChemSpot model trained on a subset of the SCAI corpus (Klinger et al., 2008) resulted in a significant performance drop over chemical patent corpora. Zhang et al. (2016) compared the performance of CRF- and Support Vector Machine (SVM)based models on the CHEMDNER-patents corpus (Krallinger et al., 2015). The features constructed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to enc"
W19-5035,J92-4003,0,0.261664,"their ensembles (Habibi et al., 2016). The application of the tmChem model trained on chemical literature corpora of the BioCreative IV CHEMDNER task (Krallinger et al., 2015) and the ChemSpot model trained on a subset of the SCAI corpus (Klinger et al., 2008) resulted in a significant performance drop over chemical patent corpora. Zhang et al. (2016) compared the performance of CRF- and Support Vector Machine (SVM)based models on the CHEMDNER-patents corpus (Krallinger et al., 2015). The features constructed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et"
W19-5035,N19-1149,0,0.0159323,"extended version as EBC-CRF as illustrated in Figure 1. In particular, for EBC-CRF, we use a concatenation of pretrained word embeddings, CNN-based characterlevel word embeddings and ELMo-based contextualized word embeddings as the input of a BiLSTM encoder. The BiLSTM encoder learns a latent feature vector for each word in the input. Then each latent feature vector is linearly transformed before being fed into a linear-chain CRF layer (Lafferty et al., 2001) for NER tag prediction. We assume binary potential between tags and unary potential between tags and words. Pre-trained word embeddings Dai et al. (2019) showed that NER performance is significantly affected by the overlap between pretrained word embedding vocabulary and the target NER data. Therefore, we explore the effects of different sets of pre-trained word embeddings on the NER performance. We use 200-dimensional pre-trained PubMedPMC and Wiki-PubMed-PMC word embeddings (Pyysalo et al., 2013), which are widely used for NLP tasks in biomedical domain. Both the PubMed-PMC and Wiki-PubMed-PMC embeddings word embeddings were generated by training the Word2Vec skip-gram model (Mikolov et al., 2013) on a collection of PubMed abstracts and PubM"
W19-5035,N19-1423,0,0.0142134,"Hence, we fix the hyper-parameters shown in Table 2 to the suggested values in our experiments, which means that only models with 2stacked LSTM of size 250 are evaluated. In this study, we also consider the choice of tokenizer and word embedding source as hyperparameters. To compare the performance of different tokenizers, we tokenize the same split of datasets with different tokenizers and evaluate the overall F1 score over development set. After the best tokenizer for pre-processing patent corpus is determined, we use datasets tokenized by the best ELMo ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) can be used to generate contextualized word representations by combining internal states of different layers in neural language models. Contextualized word representation can help to improve performance in various NLP tasks by incorporating contextual information, essentially allowing for the same word to have distinct context-dependent meanings. This could be particularly powerful for chemical NER since generic chemical names (e.g. salts, acid) may have different meanings in other domains. We therefore explore the impact of using contextualized word representations for chemical patents. We t"
W19-5035,N16-1030,0,0.775717,"l., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et al. (2016) 329 CRF-based models (Section 3.3) with pre-trained word embeddings (Section 3.4), character-level word embeddings (Section 3.5), contextualized word embeddings (Section 3.6) and implementation details (Section 3.7). developed a LSTM-based approach for encoding character level information. Habibi et al. (2017) presented an empirical study comparing three NER models on a large collection of biomedical corpora including the BioSemantics patent corpus: (1) tmChem–the CRFbased model with hand-crafted features–used as the baseline; (2) a second CRF model based on CRFSuite (Okazaki, 2007) using pre"
W19-5035,D17-1035,0,0.0122379,"sets. We do not update weights for word embeddings if pre-trained word embeddings were used. Character-level representation The BiLSTM-CRF model with character-level word representations (Lample et al., 2016; Ma and Hovy, 2016) has been shown to have state-of-theart performance in NER tasks on chemical patent datasets (Habibi et al., 2017). It has been shown that the choice of using LSTM-based or CNNbased character-level word representation has little effect on final NER performance in both general and biomedical domain while the CNN-based approach has the advantage of reduced training time (Reimers and Gurevych, 2017b; Zhai et al., 2018). Hence, we use the CNN-based approach with the same hyper-parameter settings of Reimers and Gurevych (2017b) for capturing characterlevel information (see Table 2 for details). 3.6 Hyper-para. charEmbedSize filter length # of filters output size (a) BiLSTM-CRF Table 1: Statistics of the unannotated patent corpus used for training ChemPatent embeddings and ELMo. 3.5 Value Adam 0.001 16 1 [0.25, 0.25] 3.7 Implementation details Our NER model implementation is based on the AllenNLP system (Gardner et al., 2017). We learn model parameters using the training set, and we use th"
W19-5035,P16-1101,0,0.294108,"ed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et al. (2016) 329 CRF-based models (Section 3.3) with pre-trained word embeddings (Section 3.4), character-level word embeddings (Section 3.5), contextualized word embeddings (Section 3.6) and implementation details (Section 3.7). developed a LSTM-based approach for encoding character level information. Habibi et al. (2017) presented an empirical study comparing three NER models on a large collection of biomedical corpora including the BioSemantics patent corpus: (1) tmChem–the CRFbased model with hand-crafte"
W19-5035,D14-1162,0,0.0868215,"3) were also explored. The results showed that the BiLSTMCRF model with the combination of domainspecific pre-trained word embedding and LSTMbased character-level word embeddings outperformed the two CRF-based models on chemical NER tasks in both chemical literature and chemical patent corpora. However, this work used only a general tokenizer (i.e. OpenNLP) and word embeddings pre-trained on biomedical corpora. Corbett and Boyle (2018) presented word-level and character-level BiLSTM networks for chemical NER in literature domain. The word-level model employed word embeddings learned by GloVe (Pennington et al., 2014) on a corpus of patent titles and abstracts. The character-level model used two different transfer learning approaches to pre-train its character-level encoder. The first approach attempts to predict neighbor characters at each time step, while the other tries to predict whether a given character sequence is an entry in the chemical database ChEBI (Degtyarenko et al., 2007). Experimental results show that the character-level model can produce better NER performance than word-level model by leveraging transfer learning. In addition, for the wordlevel model, using pre-trained word embeddings lea"
W19-5035,N18-1202,0,0.19692,"ddings are fixed during training of the NER models. For a more concrete comparison, a set of 200-dimensional trainable word embeddings initialized from normal distribution is used as a baseline. The 200-dimensional baseline word embeddings contain all words in the vocabulary of the dataset and are initialized from a normal distribution, the baseline word embeddings are learned during training process. The vocabulary of models Models We use the BiLSTM-CNN-CRF model (Ma and Hovy, 2016) as our baseline. We extend the baseline by adding the contextualized word representations generated from ELMo (Peters et al., 2018). 1 NBIC UMLSGeneChemTokenizer is developed by the Netherlands Bioinformatics Center, available at https:// trac.nbic.nl/data-mining/wiki. 331 Patent Office AU CA EP GB IN US WO Total Document 7,743 1,962 19,274 918 1,913 41,131 11,135 84,076 Sentence 4,662,375 463,123 3,478,258 182,627 261,260 19,800,123 4,830,708 33,687,474 Hyper-para. Optimizer Learning rate Mini-batch size Clip Norm(L2) Dropout Tokens 156,137,670 16,109,776 117,992,191 6,038,837 9,015,238 628,256,609 159,286,325 1,092,836,646 Value 50 3 30 30 (b) CNN-char Table 2: Fixed hyper-parameter configurations. patents (detailed in"
W19-5035,W18-5605,1,0.80271,"ts for word embeddings if pre-trained word embeddings were used. Character-level representation The BiLSTM-CRF model with character-level word representations (Lample et al., 2016; Ma and Hovy, 2016) has been shown to have state-of-theart performance in NER tasks on chemical patent datasets (Habibi et al., 2017). It has been shown that the choice of using LSTM-based or CNNbased character-level word representation has little effect on final NER performance in both general and biomedical domain while the CNN-based approach has the advantage of reduced training time (Reimers and Gurevych, 2017b; Zhai et al., 2018). Hence, we use the CNN-based approach with the same hyper-parameter settings of Reimers and Gurevych (2017b) for capturing characterlevel information (see Table 2 for details). 3.6 Hyper-para. charEmbedSize filter length # of filters output size (a) BiLSTM-CRF Table 1: Statistics of the unannotated patent corpus used for training ChemPatent embeddings and ELMo. 3.5 Value Adam 0.001 16 1 [0.25, 0.25] 3.7 Implementation details Our NER model implementation is based on the AllenNLP system (Gardner et al., 2017). We learn model parameters using the training set, and we use the overall F1 score ov"
W19-5035,W18-2501,0,\N,Missing
